{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using eBFE Models: Spring Creek 2D Analysis\n",
    "\n",
    "This notebook demonstrates working with FEMA eBFE/BLE models using the Spring Creek (12040102) study area.\n",
    "\n",
    "## The Problem: eBFE Models Are Broken\n",
    "\n",
    "**FEMA provides valuable BLE models, but they're intentionally separated into folders that make them UNUSABLE:**\n",
    "\n",
    "1. **Output/ Separated**: Pre-run HDF results separated from project folder → Can't access results\n",
    "2. **Terrain/ Misplaced**: Terrain folder outside project → .rasmap references break, model won't run\n",
    "3. **Absolute DSS Paths**: DSS File= uses paths from original system → \"DSS path needs correction\" GUI popups\n",
    "\n",
    "**Without our library**: 30-60 minutes of manual fixes per model (moving folders, correcting paths via GUI dialogs)\n",
    "\n",
    "**With RasEbfeModels**: One function call → runnable HEC-RAS model with all paths corrected ✓\n",
    "\n",
    "## Our Solution: 3 Critical Fixes\n",
    "\n",
    "**RasEbfeModels.organize_spring_creek() automatically**:\n",
    "1. Moves Output/ HDF files INTO project folder (access pre-run results)\n",
    "2. Ensures Terrain/ is IN project folder (.rasmap references work)\n",
    "3. Corrects ALL paths to relative references (no GUI error popups)\n",
    "\n",
    "**Result**: Model that just works - no manual fixes, no frustration, automation-friendly\n",
    "\n",
    "## Model Characteristics\n",
    "\n",
    "- **Pattern 3a**: Single large 2D model with nested zip\n",
    "- **Size**: 9.7 GB\n",
    "- **Type**: 2D unsteady flow\n",
    "- **Plans**: 8 (with pre-computed results)\n",
    "- **Terrain**: Self-contained, 504.6 MB\n",
    "- **Version**: HEC-RAS 5.0.7\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Organize broken eBFE model into runnable HEC-RAS project\n",
    "2. Understand the 3 critical fixes applied automatically\n",
    "3. Validate DSS boundary conditions\n",
    "4. Extract pre-computed 2D results (without re-running)\n",
    "5. Visualize water surface elevations\n",
    "6. Optional: Run compute test with haiku validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Automatic Download**: This notebook will automatically download Spring Creek Models.zip (9.7 GB) from the eBFE S3 bucket if not already present. The download includes:\n",
    "- Progress tracking with tqdm\n",
    "- Resume-safe (won't re-download if already present)\n",
    "- Automatic extraction with progress tracking\n",
    "\n",
    "**Download Details**:\n",
    "- **Size**: 9.7 GB\n",
    "- **Source**: FEMA eBFE S3 bucket\n",
    "- **Time**: ~10-20 minutes depending on connection speed\n",
    "- **Disk Space**: ~20 GB required (zip + extracted files)\n",
    "\n",
    "**Manual Download** (optional, if automatic fails):\n",
    "1. Visit: https://webapps.usgs.gov/infrm/estBFE/\n",
    "2. Search for \"Spring\" study area\n",
    "3. Download Models.zip (9.7 GB)\n",
    "4. Extract to desired location\n",
    "\n",
    "**Important**: Do NOT manually organize the eBFE files - let `RasEbfeModels.organize_spring_creek()` handle it. Manual organization requires extensive path corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for development\n",
    "try:\n",
    "    from ras_commander import init_ras_project, RasCmdr\n",
    "except ImportError:\n",
    "    sys.path.insert(0, str(Path.cwd().parent))\n",
    "    from ras_commander import init_ras_project, RasCmdr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Organize Broken eBFE Model into Runnable HEC-RAS Project\n",
    "\n",
    "**Automatic Download**: If source data is not present, `organize_spring_creek()` will automatically download 9.7 GB from eBFE S3 bucket. You'll see progress bars for download and extraction.\n",
    "\n",
    "**RasEbfeModels.organize_spring_creek() applies 3 critical fixes**:\n",
    "\n",
    "1. **Output/ Integration**: Moves pre-run HDF files into project folder\n",
    "2. **Terrain/ Integration**: Ensures terrain is in project folder\n",
    "3. **Path Corrections**: Converts ALL paths to relative references (DSS, terrain, etc.)\n",
    "\n",
    "**Without these fixes**: Model won't open in HEC-RAS without manual path corrections and folder moves.\n",
    "\n",
    "**With these fixes**: Model works immediately - no manual intervention required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already organized at: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\n",
      "\n",
      "✓ Organized model location: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\n"
     ]
    }
   ],
   "source": [
    "# Import eBFE model organization function\n",
    "from ras_commander.ebfe_models import RasEbfeModels\n",
    "\n",
    "# Set paths\n",
    "downloaded_folder = Path(r\"D:\\Ras-Commander_BulkData\\eBFE\\Harris_County\\12040102_Spring_Models_extracted\")\n",
    "organized_folder = Path(r\"D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\")\n",
    "\n",
    "# Check if already organized\n",
    "if not organized_folder.exists() or not (organized_folder / \"agent\" / \"model_log.md\").exists():\n",
    "    print(\"Organizing Spring Creek model...\")\n",
    "    organized_folder = RasEbfeModels.organize_spring_creek(\n",
    "        downloaded_folder,\n",
    "        organized_folder,\n",
    "        validate_dss=True  # Validate DSS boundary conditions\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model already organized at: {organized_folder}\")\n",
    "\n",
    "print(f\"\\n✓ Organized model location: {organized_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Fixes Applied\n",
    "\n",
    "### Before RasEbfeModels (Broken eBFE Delivery)\n",
    "\n",
    "**File Structure** (won't work):\n",
    "```\n",
    "12040102_Spring_Models_extracted/\n",
    "└── 12040102_Models_202207/\n",
    "    ├── _Final.zip (9.67 GB nested - must extract manually)\n",
    "    └── _Final_extracted/\n",
    "        └── _Final/\n",
    "            └── HECRAS_507/\n",
    "                ├── Spring.prj ✗ Can't find terrain\n",
    "                ├── Spring.u01 ✗ DSS File=.\\DSS_Input\\Spring.dss (wrong path)\n",
    "                ├── Spring.rasmap ✗ Terrain=.\\Terrain\\RAS_Terrain\\Terrain.hdf (doesn't exist)\n",
    "                ├── Terrain/ ✓ Exists but in wrong location for .rasmap\n",
    "                └── Shp/, Features/ (mixed with model files)\n",
    "```\n",
    "\n",
    "**User Experience**:\n",
    "1. Extract nested zip manually (10 minutes)\n",
    "2. Open Spring.prj → ERROR: \"Terrain not found\"\n",
    "3. Try to fix → Realize .rasmap references wrong location\n",
    "4. Open Spring.prj → ERROR: \"DSS path needs correction\"\n",
    "5. Manually fix DSS paths via GUI\n",
    "6. Try to view results → Can't find HDF files\n",
    "7. Give up or spend 30+ minutes fixing\n",
    "\n",
    "### After RasEbfeModels (Runnable HEC-RAS Model)\n",
    "\n",
    "**File Structure** (works):\n",
    "```\n",
    "SpringCreek_12040102/\n",
    "├── RAS Model/\n",
    "│   ├── Spring.prj ✓ All paths correct\n",
    "│   ├── Spring.u01 ✓ DSS File=Spring.dss (relative, exists)\n",
    "│   ├── Spring.rasmap ✓ Terrain=.\\Terrain\\Terrain.hdf (correct, exists)\n",
    "│   ├── Spring.p01.hdf ✓ Pre-run results accessible\n",
    "│   ├── Spring.dss ✓ In project folder\n",
    "│   └── Terrain/\n",
    "│       └── Terrain.hdf ✓ Where .rasmap expects it\n",
    "├── Spatial Data/ (shapefiles separate from model)\n",
    "├── Documentation/ (inventory)\n",
    "└── agent/model_log.md (documents all fixes applied)\n",
    "```\n",
    "\n",
    "**User Experience**:\n",
    "```python\n",
    "organized = RasEbfeModels.organize_spring_creek(source, validate_dss=True)\n",
    "init_ras_project(organized / \"RAS Model\", \"5.0.7\")\n",
    "# ✓ Opens without errors\n",
    "# ✓ Terrain loads\n",
    "# ✓ DSS files load\n",
    "# ✓ Pre-run results accessible\n",
    "# ✓ No manual fixes needed\n",
    "```\n",
    "\n",
    "### The 3 Critical Fixes (Automatic)\n",
    "\n",
    "1. **Terrain Integration**: Terrain/ moved to project folder, .rasmap path corrected\n",
    "2. **DSS Path Corrections**: All DSS references corrected to relative paths that exist\n",
    "3. **Output Integration**: Pre-run HDF files in project folder (if present)\n",
    "\n",
    "**Result**: Model that just works ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Organization\n",
    "\n",
    "Check the standardized 4-folder structure and agent work log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Structure:\n",
      "  ✓ HMS Model/ (0 items)\n",
      "  ✓ RAS Model/ (83 items)\n",
      "  ✓ Spatial Data/ (14 items)\n",
      "  ✓ Documentation/ (1 items)\n",
      "  ✓ agent/ (1 items)\n",
      "\n",
      "✓ Agent work log: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\agent\\model_log.md\n",
      "\n",
      "Work log preview (first 20 lines):\n",
      "================================================================================\n",
      "# Agent Work Log - Spring Creek\n",
      "\n",
      "**Model**: Spring Creek (12040102)\n",
      "**Pattern**: 3a - Single 2D model, nested zip\n",
      "**Date**: 2026-01-09 16:50:53\n",
      "**Generated Function**: RasEbfeModels.organize_spring_creek()\n",
      "\n",
      "## Organization Summary\n",
      "\n",
      "**Source**: D:\\Ras-Commander_BulkData\\eBFE\\Harris_County\\12040102_Models_extracted\n",
      "**Output**: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\n",
      "**Files Organized**: 79\n",
      "\n",
      "### Structure Created\n",
      "- HMS Model/ (empty - no HMS for Pattern 3a)\n",
      "- RAS Model/ (79 files, ~9.3 GB)\n",
      "- Spatial Data/ (terrain + shapefiles, ~515 MB)\n",
      "- Documentation/ (1 file, 58 KB)\n",
      "- agent/model_log.md (this file)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify 4-folder structure\n",
    "folders = ['HMS Model', 'RAS Model', 'Spatial Data', 'Documentation', 'agent']\n",
    "print(\"Folder Structure:\")\n",
    "for folder in folders:\n",
    "    folder_path = organized_folder / folder\n",
    "    if folder_path.exists():\n",
    "        file_count = len(list(folder_path.rglob('*')))\n",
    "        print(f\"  ✓ {folder}/ ({file_count} items)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {folder}/ (missing)\")\n",
    "\n",
    "# Check for agent work log\n",
    "model_log = organized_folder / \"agent\" / \"model_log.md\"\n",
    "if model_log.exists():\n",
    "    print(f\"\\n✓ Agent work log: {model_log}\")\n",
    "    print(\"\\nWork log preview (first 20 lines):\")\n",
    "    print(\"=\" * 80)\n",
    "    print('\\n'.join(model_log.read_text().split('\\n')[:20]))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Project with ras-commander\n",
    "\n",
    "Initialize the Spring Creek HEC-RAS project using ras-commander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 23:44:25 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.rasmap\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1898 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_100yr\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1397 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_500yr\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 824 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_100yrPLUS\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1154 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_50yr\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1329 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_25yr\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1277 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_10yr\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 825 characters from HDF\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Extracting Plan Information from: Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Plan Name: SPR_100yrMINUS\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Simulation Duration (hours): 72.0\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Using existing Path object HDF file: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07.hdf\n",
      "2026-01-09 23:44:25 - ras_commander.RasPrj - INFO - Updated results_df with 7 plan(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project initialized: D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.prj\n",
      "\n",
      "Plans found: 7\n",
      "  plan_number      Plan Title                                                                           full_path\n",
      "0          01       SPR_100yr  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p01\n",
      "1          02       SPR_500yr  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p02\n",
      "2          03   SPR_100yrPLUS  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p03\n",
      "3          04        SPR_50yr  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p04\n",
      "4          05        SPR_25yr  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p05\n",
      "5          06        SPR_10yr  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p06\n",
      "6          07  SPR_100yrMINUS  D:\\Ras-Commander_BulkData\\eBFE\\Organized\\SpringCreek_12040102\\RAS Model\\Spring.p07\n"
     ]
    }
   ],
   "source": [
    "# Initialize project\n",
    "project_folder = organized_folder / \"RAS Model\"\n",
    "ras = init_ras_project(project_folder, \"5.0.7\")\n",
    "\n",
    "print(f\"Project initialized: {ras.prj_file}\")\n",
    "print(f\"\\nPlans found: {len(ras.plan_df)}\")\n",
    "print(ras.plan_df[['plan_number', 'Plan Title', 'full_path']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate DSS Boundary Conditions\n",
    "\n",
    "Spring Creek uses DSS files for boundary conditions. Validate all pathnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u01\n",
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u02\n",
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u03\n",
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u04\n",
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u05\n",
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u06\n",
      "2026-01-09 23:44:25 - ras_commander.RasUnsteady - INFO - Found 2 DSS-linked boundaries in Spring.u07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referenced DSS files:\n",
      "  - 100YR.dss (1 pathnames)\n",
      "  - 500YR.dss (1 pathnames)\n",
      "  - 100YR_PLUS.dss (1 pathnames)\n",
      "  - 25YR.dss (1 pathnames)\n",
      "  - 50YR.dss (1 pathnames)\n",
      "  - 01__MINUS.dss (1 pathnames)\n",
      "  - 10_ACE.dss (1 pathnames)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Missing DSS file(s): 01__MINUS.dss, 100YR.dss, 100YR_PLUS.dss, 10_ACE.dss, 25YR.dss, 500YR.dss, 50YR.dss. Add search roots or extract nested zips and re-run.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 187\u001b[39m\n\u001b[32m    185\u001b[39m missing_names = _missing_names()\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_names:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    188\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMissing DSS file(s): \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(missing_names)\n\u001b[32m    190\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m. Add search roots or extract nested zips and re-run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m     )\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_candidate_rank\u001b[39m(entry):\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m entry[\u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Missing DSS file(s): 01__MINUS.dss, 100YR.dss, 100YR_PLUS.dss, 10_ACE.dss, 25YR.dss, 500YR.dss, 50YR.dss. Add search roots or extract nested zips and re-run."
     ]
    }
   ],
   "source": [
    "# Step 4a: Find correct DSS + patch unsteady file references\n",
    "#\n",
    "# Goal:\n",
    "# - Put required DSS file(s) inside the HEC-RAS project folder (portable)\n",
    "# - Rewrite any absolute/unavailable DSS paths in the .u## file(s) to point\n",
    "#   to the local copy (relative path)\n",
    "#\n",
    "# Note: HEC-RAS unsteady flow files store links like:\n",
    "#   DSS File=D:\\some\\old\\path\\100YR.dss\n",
    "# We want:\n",
    "#   DSS File=Boundary Condition DSS\\100YR.dss\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from ras_commander import RasUnsteady\n",
    "\n",
    "bc_dss_dir = project_folder / \"Boundary Condition DSS\"\n",
    "bc_dss_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find unsteady files from ras metadata (preferred)\n",
    "unsteady_files = []\n",
    "if hasattr(ras, \"unsteady_df\") and not ras.unsteady_df.empty:\n",
    "    for p in ras.unsteady_df[\"full_path\"].tolist():\n",
    "        if p:\n",
    "            unsteady_files.append(Path(p))\n",
    "else:\n",
    "    unsteady_files = sorted(project_folder.glob(\"*.u[0-9][0-9]\"))\n",
    "\n",
    "if not unsteady_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No unsteady files found under: {project_folder}\"\n",
    "    )\n",
    "\n",
    "# Build expected DSS references from unsteady files\n",
    "expected_by_name = {}\n",
    "\n",
    "for ufile in unsteady_files:\n",
    "    dss_bcs = RasUnsteady.get_dss_boundaries(ufile, ras_object=ras)\n",
    "    if dss_bcs.empty:\n",
    "        continue\n",
    "\n",
    "    for _, row in dss_bcs.iterrows():\n",
    "        dss_file_raw = str(row.get(\"dss_file\", \"\")).strip().strip('\"')\n",
    "        dss_path = str(row.get(\"dss_path\", \"\")).strip()\n",
    "\n",
    "        if not dss_file_raw:\n",
    "            continue\n",
    "\n",
    "        dss_name = Path(dss_file_raw).name\n",
    "        expected_by_name.setdefault(dss_name, set())\n",
    "        if dss_path:\n",
    "            expected_by_name[dss_name].add(dss_path)\n",
    "\n",
    "if not expected_by_name:\n",
    "    raise ValueError(\n",
    "        \"No DSS references found in unsteady file(s).\"\n",
    "    )\n",
    "\n",
    "print(\"Referenced DSS files:\")\n",
    "for name, paths in expected_by_name.items():\n",
    "    print(f\"  - {name} ({len(paths)} pathnames)\")\n",
    "\n",
    "expected_names = sorted(expected_by_name.keys())\n",
    "expected_lookup = {name.lower(): name for name in expected_names}\n",
    "\n",
    "# Search roots: project, organized, downloaded (if provided)\n",
    "search_roots = [project_folder]\n",
    "\n",
    "organized_root = globals().get(\"organized_folder\")\n",
    "if organized_root:\n",
    "    search_roots.append(Path(organized_root))\n",
    "\n",
    "downloaded_root = globals().get(\"downloaded_folder\")\n",
    "if downloaded_root:\n",
    "    search_roots.append(Path(downloaded_root))\n",
    "\n",
    "candidates = {name: [] for name in expected_names}\n",
    "\n",
    "\n",
    "def _add_candidate(name, entry):\n",
    "    candidates[name].append(entry)\n",
    "\n",
    "\n",
    "# 1) Scan filesystem for DSS files (fast)\n",
    "for root in search_roots:\n",
    "    if not root.exists():\n",
    "        continue\n",
    "    for path in root.rglob(\"*.dss\"):\n",
    "        match = expected_lookup.get(path.name.lower())\n",
    "        if match:\n",
    "            _add_candidate(match, {\n",
    "                \"kind\": \"file\",\n",
    "                \"path\": path,\n",
    "            })\n",
    "\n",
    "\n",
    "# 2) Scan zip files (including nested zips) only if needed\n",
    "\n",
    "def _missing_names():\n",
    "    return [name for name, items in candidates.items() if not items]\n",
    "\n",
    "\n",
    "missing_names = _missing_names()\n",
    "if missing_names:\n",
    "    zip_files = []\n",
    "    for root in search_roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        zip_files.extend(root.rglob(\"*.zip\"))\n",
    "\n",
    "    zip_files = sorted({p.resolve() for p in zip_files})\n",
    "\n",
    "    def _cache_dir():\n",
    "        root = Path.cwd()\n",
    "        if root.name.lower() == \"examples\":\n",
    "            root = root.parent\n",
    "        cache = root / \"working\" / \"zip_cache\"\n",
    "        cache.mkdir(parents=True, exist_ok=True)\n",
    "        return cache\n",
    "\n",
    "    cache_dir = _cache_dir()\n",
    "    scanned_zips = set()\n",
    "\n",
    "    def _scan_zip(zip_path, depth=0, max_depth=2):\n",
    "        zip_path = Path(zip_path)\n",
    "        if zip_path in scanned_zips:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "                scanned_zips.add(zip_path)\n",
    "\n",
    "                for info in zf.infolist():\n",
    "                    if info.is_dir():\n",
    "                        continue\n",
    "\n",
    "                    inner_name = Path(info.filename).name\n",
    "                    match = expected_lookup.get(inner_name.lower())\n",
    "                    if match:\n",
    "                        _add_candidate(match, {\n",
    "                            \"kind\": \"zip\",\n",
    "                            \"zip_path\": zip_path,\n",
    "                            \"member\": info.filename,\n",
    "                            \"file_size\": info.file_size,\n",
    "                        })\n",
    "\n",
    "                if depth >= max_depth:\n",
    "                    return\n",
    "\n",
    "                if not _missing_names():\n",
    "                    return\n",
    "\n",
    "                # Scan nested zips by extracting to cache (streamed)\n",
    "                for info in zf.infolist():\n",
    "                    if info.is_dir():\n",
    "                        continue\n",
    "                    if not info.filename.lower().endswith(\".zip\"):\n",
    "                        continue\n",
    "\n",
    "                    nested_name = Path(info.filename).name\n",
    "                    nested_path = cache_dir / nested_name\n",
    "\n",
    "                    if (\n",
    "                        not nested_path.exists()\n",
    "                        or nested_path.stat().st_size != info.file_size\n",
    "                    ):\n",
    "                        nested_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        with zf.open(info) as src, open(nested_path, \"wb\") as dst:\n",
    "                            shutil.copyfileobj(src, dst, length=1024 * 1024)\n",
    "\n",
    "                    _scan_zip(nested_path, depth=depth + 1, max_depth=max_depth)\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"Warning: skipped invalid zip: {zip_path}\")\n",
    "\n",
    "    for zip_path in zip_files:\n",
    "        if not _missing_names():\n",
    "            break\n",
    "        _scan_zip(zip_path)\n",
    "\n",
    "\n",
    "missing_names = _missing_names()\n",
    "if missing_names:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing DSS file(s): \"\n",
    "        + \", \".join(missing_names)\n",
    "        + \". Add search roots or extract nested zips and re-run.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _candidate_rank(entry):\n",
    "    if entry[\"kind\"] == \"file\":\n",
    "        path = entry[\"path\"]\n",
    "        if path.parent == bc_dss_dir:\n",
    "            rank = 0\n",
    "        elif project_folder in path.parents:\n",
    "            rank = 1\n",
    "        elif organized_root and Path(organized_root) in path.parents:\n",
    "            rank = 2\n",
    "        elif downloaded_root and Path(downloaded_root) in path.parents:\n",
    "            rank = 3\n",
    "        else:\n",
    "            rank = 4\n",
    "        return (rank, str(path).lower())\n",
    "\n",
    "    return (\n",
    "        5,\n",
    "        str(entry[\"zip_path\"]).lower(),\n",
    "        entry[\"member\"].lower(),\n",
    "    )\n",
    "\n",
    "\n",
    "def _describe(entry):\n",
    "    if entry[\"kind\"] == \"file\":\n",
    "        return str(entry[\"path\"])\n",
    "    return f\"{entry['zip_path']}::{entry['member']}\"\n",
    "\n",
    "\n",
    "selected = {}\n",
    "for name, items in candidates.items():\n",
    "    items_sorted = sorted(items, key=_candidate_rank)\n",
    "    if len(items_sorted) > 1:\n",
    "        print(f\"Multiple matches for {name}:\")\n",
    "        for item in items_sorted:\n",
    "            print(f\"  - {_describe(item)}\")\n",
    "    chosen = items_sorted[0]\n",
    "    selected[name] = chosen\n",
    "    print(f\"Selected for {name}: {_describe(chosen)}\")\n",
    "\n",
    "\n",
    "# Copy or extract selected DSS file(s) into project subfolder\n",
    "\n",
    "def _copy_with_retry(src, dst, attempts=3, delay=1.0):\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        try:\n",
    "            shutil.copy2(src, dst)\n",
    "            return\n",
    "        except PermissionError as exc:\n",
    "            if attempt == attempts:\n",
    "                raise PermissionError(\n",
    "                    f\"Could not copy {src} to {dst} (file locked). \"\n",
    "                    \"Close any apps using the DSS file and re-run.\"\n",
    "                ) from exc\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "for name, entry in selected.items():\n",
    "    dest = bc_dss_dir / name\n",
    "\n",
    "    if entry[\"kind\"] == \"file\":\n",
    "        src = entry[\"path\"]\n",
    "        if src.resolve() == dest.resolve():\n",
    "            print(f\"Already in target: {dest}\")\n",
    "        elif (\n",
    "            dest.exists()\n",
    "            and dest.stat().st_size == src.stat().st_size\n",
    "        ):\n",
    "            print(f\"Already present: {dest}\")\n",
    "        else:\n",
    "            _copy_with_retry(src, dest)\n",
    "            print(f\"Copied: {src} -> {dest}\")\n",
    "\n",
    "    else:\n",
    "        if (\n",
    "            dest.exists()\n",
    "            and dest.stat().st_size == entry[\"file_size\"]\n",
    "        ):\n",
    "            print(f\"Already present: {dest}\")\n",
    "        else:\n",
    "            with zipfile.ZipFile(entry[\"zip_path\"], \"r\") as zf:\n",
    "                with zf.open(entry[\"member\"]) as src, open(dest, \"wb\") as dst:\n",
    "                    shutil.copyfileobj(src, dst, length=1024 * 1024)\n",
    "            print(f\"Extracted: {_describe(entry)} -> {dest}\")\n",
    "\n",
    "\n",
    "# Update DSS File= lines in unsteady flow files\n",
    "patched_files = []\n",
    "for ufile in unsteady_files:\n",
    "    lines = ufile.read_text(\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    ).splitlines(True)\n",
    "\n",
    "    changed = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if not line.startswith(\"DSS File=\"):\n",
    "            continue\n",
    "\n",
    "        old_value = line.split(\"=\", 1)[1].strip().strip('\"')\n",
    "        old_name = Path(old_value).name\n",
    "\n",
    "        if old_name not in selected:\n",
    "            raise ValueError(\n",
    "                f\"Unsteady file {ufile.name} references DSS file \"\n",
    "                f\"{old_name}, which was not found in search roots.\"\n",
    "            )\n",
    "\n",
    "        new_rel = str(Path(\"Boundary Condition DSS\") / old_name)\n",
    "        new_rel = new_rel.replace(\"/\", \"\\\\\")\n",
    "\n",
    "        if old_value != new_rel:\n",
    "            lines[i] = f\"DSS File={new_rel}\\n\"\n",
    "            changed = True\n",
    "\n",
    "    if changed:\n",
    "        ufile.write_text(\n",
    "            \"\".join(lines),\n",
    "            encoding=\"utf-8\",\n",
    "            errors=\"ignore\"\n",
    "        )\n",
    "        patched_files.append(ufile)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Patched {len(patched_files)} unsteady file(s)\")\n",
    "for p in patched_files:\n",
    "    print(f\"  - {p.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ras_commander import RasUnsteady\n",
    "from ras_commander.dss import RasDss\n",
    "\n",
    "# Find unsteady files from ras metadata (preferred)\n",
    "unsteady_files = []\n",
    "if hasattr(ras, \"unsteady_df\") and not ras.unsteady_df.empty:\n",
    "    for p in ras.unsteady_df[\"full_path\"].tolist():\n",
    "        if p:\n",
    "            unsteady_files.append(Path(p))\n",
    "else:\n",
    "    unsteady_files = sorted(project_folder.glob(\"*.u[0-9][0-9]\"))\n",
    "\n",
    "# Build DSS file list from unsteady references\n",
    "seen_dss = set()\n",
    "for ufile in unsteady_files:\n",
    "    dss_bcs = RasUnsteady.get_dss_boundaries(ufile, ras_object=ras)\n",
    "    if dss_bcs.empty:\n",
    "        continue\n",
    "\n",
    "    for _, row in dss_bcs.iterrows():\n",
    "        dss_file_raw = str(row.get(\"dss_file\", \"\")).strip().strip('\"')\n",
    "        if not dss_file_raw:\n",
    "            continue\n",
    "\n",
    "        dss_path_obj = Path(dss_file_raw)\n",
    "        if not dss_path_obj.is_absolute():\n",
    "            dss_path_obj = project_folder / dss_path_obj\n",
    "\n",
    "        if dss_path_obj.exists():\n",
    "            seen_dss.add(dss_path_obj.resolve())\n",
    "\n",
    "dss_files = sorted(seen_dss)\n",
    "if not dss_files:\n",
    "    dss_files = sorted(project_folder.glob(\"**/*.dss\"))\n",
    "\n",
    "print(f\"Found {len(dss_files)} DSS file(s):\")\n",
    "for dss_file in dss_files:\n",
    "    try:\n",
    "        rel = dss_file.relative_to(project_folder)\n",
    "    except Exception:\n",
    "        rel = dss_file\n",
    "    print(f\"  - {rel}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Fast validation: check pathname STRUCTURE for the DSS catalog (no per-path I/O)\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    from ras_commander.validation_base import ValidationSeverity\n",
    "except Exception:\n",
    "    ValidationSeverity = None\n",
    "\n",
    "for dss_file in dss_files:\n",
    "    print(\"\")\n",
    "    print(f\"Validating catalog format: {dss_file.name}\")\n",
    "\n",
    "    catalog = RasDss.get_catalog(dss_file)\n",
    "    pathnames = catalog[\"pathname\"].astype(str).tolist()\n",
    "\n",
    "    errors = 0\n",
    "    warnings = 0\n",
    "    for pathname in pathnames:\n",
    "        result = RasDss.check_pathname_format(pathname)\n",
    "        passed = (\n",
    "            result.get(\"passed\", False)\n",
    "            if isinstance(result, dict)\n",
    "            else getattr(result, \"passed\", False)\n",
    "        )\n",
    "\n",
    "        if not passed:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        if ValidationSeverity is not None:\n",
    "            severity = getattr(result, \"severity\", None)\n",
    "            if severity == ValidationSeverity.WARNING:\n",
    "                warnings += 1\n",
    "\n",
    "    if errors == 0:\n",
    "        print(f\"  \\u2713 Format OK ({len(pathnames)} paths, {warnings} warnings)\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"  \\u26a0\\ufe0f {errors} format error(s) \"\n",
    "            f\"({len(pathnames)} paths, {warnings} warnings)\"\n",
    "        )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) What HEC-RAS needs: referenced DSS file exists and referenced DSS paths exist\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\")\n",
    "print(\n",
    "    f\"Validating DSS references from {len(unsteady_files)} \"\n",
    "    \"unsteady file(s)...\"\n",
    ")\n",
    "\n",
    "catalog_cache = {}\n",
    "missing_files = 0\n",
    "missing_paths = 0\n",
    "\n",
    "for ufile in unsteady_files:\n",
    "    dss_bcs = RasUnsteady.get_dss_boundaries(ufile, ras_object=ras)\n",
    "    if dss_bcs.empty:\n",
    "        continue\n",
    "\n",
    "    print(\"\")\n",
    "    print(\n",
    "        f\"{ufile.name}: {len(dss_bcs)} DSS-linked boundary condition(s)\"\n",
    "    )\n",
    "\n",
    "    for _, row in dss_bcs.iterrows():\n",
    "        dss_file_raw = str(row.get(\"dss_file\", \"\")).strip().strip('\"')\n",
    "        dss_path = str(row.get(\"dss_path\", \"\")).strip()\n",
    "\n",
    "        if not dss_file_raw:\n",
    "            continue\n",
    "\n",
    "        dss_path_obj = Path(dss_file_raw)\n",
    "        if not dss_path_obj.is_absolute():\n",
    "            dss_path_obj = project_folder / dss_path_obj\n",
    "\n",
    "        if not dss_path_obj.exists():\n",
    "            print(f\"  \\u2717 Missing DSS file: {dss_path_obj}\")\n",
    "            missing_files += 1\n",
    "            continue\n",
    "\n",
    "        cache_key = str(dss_path_obj.resolve()).lower()\n",
    "        if cache_key not in catalog_cache:\n",
    "            cat = RasDss.get_catalog(dss_path_obj)\n",
    "            catalog_cache[cache_key] = set(\n",
    "                cat[\"pathname\"].astype(str).tolist()\n",
    "            )\n",
    "\n",
    "        if dss_path and dss_path not in catalog_cache[cache_key]:\n",
    "            print(\n",
    "                f\"  \\u2717 Missing DSS path in {dss_path_obj.name}: {dss_path}\"\n",
    "            )\n",
    "            missing_paths += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\n",
    "    f\"Summary: missing files={missing_files}, \"\n",
    "    f\"missing paths={missing_paths}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Pre-Computed Results\n",
    "\n",
    "Spring Creek includes pre-computed results for all 8 plans. Extract water surface elevations without re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ras_commander.hdf import HdfResultsMesh, HdfMesh\n",
    "\n",
    "# Extract results from Plan 01\n",
    "plan_hdf_path = project_folder / \"Spring.p01.hdf\"\n",
    "print(f\"Reading HDF results: {plan_hdf_path.name}\\n\")\n",
    "\n",
    "# 2D mesh summary output: maximum water surface per cell\n",
    "max_ws_gdf = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)\n",
    "\n",
    "print(\"Maximum Water Surface (all mesh cells):\")\n",
    "print(f\"  Rows: {len(max_ws_gdf)}\")\n",
    "if not max_ws_gdf.empty:\n",
    "    print(f\"  Min: {max_ws_gdf['maximum_water_surface'].min():.2f} ft\")\n",
    "    print(f\"  Max: {max_ws_gdf['maximum_water_surface'].max():.2f} ft\")\n",
    "    print(f\"  Mean: {max_ws_gdf['maximum_water_surface'].mean():.2f} ft\")\n",
    "\n",
    "print(\"\\nAttributes:\")\n",
    "print(max_ws_gdf.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get 2D Mesh Cell Locations\n",
    "\n",
    "Extract the 2D mesh cell locations for spatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh cell centers\n",
    "mesh_cells = HdfMesh.get_mesh_cell_points(\"01\", ras_object=ras)\n",
    "\n",
    "print(f\"2D Mesh Cells:\")\n",
    "print(f\"  Total cells: {len(mesh_cells)}\")\n",
    "print(f\"\\nFirst 5 cells:\")\n",
    "print(mesh_cells.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Water Surface Elevations\n",
    "\n",
    "Plot the water surface elevation spatial distribution.\n",
    "Also shows 2D perimeter and breaklines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot max water surface using the GeoDataFrame returned by ras-commander\n",
    "if not max_ws_gdf.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        max_ws_gdf.geometry.x,\n",
    "        max_ws_gdf.geometry.y,\n",
    "        c=max_ws_gdf[\"maximum_water_surface\"],\n",
    "        cmap=\"viridis\",\n",
    "        s=1,\n",
    "        alpha=0.6,\n",
    "        zorder=1,\n",
    "    )\n",
    "\n",
    "    # Overlay 2D perimeter and breaklines\n",
    "    try:\n",
    "        from ras_commander.hdf import HdfBndry\n",
    "\n",
    "        # 2D Flow Area perimeter polygons (from geometry HDF)\n",
    "        mesh_areas = HdfMesh.get_mesh_areas(\"01\", ras_object=ras)\n",
    "        if not mesh_areas.empty:\n",
    "            mesh_areas.boundary.plot(\n",
    "                ax=ax,\n",
    "                color=\"black\",\n",
    "                linewidth=1.2,\n",
    "                alpha=0.9,\n",
    "                zorder=3,\n",
    "            )\n",
    "\n",
    "        # 2D breaklines (stored in Geometry group)\n",
    "        breaklines = HdfBndry.get_breaklines(plan_hdf_path)\n",
    "        if not breaklines.empty:\n",
    "            breaklines.plot(\n",
    "                ax=ax,\n",
    "                color=\"black\",\n",
    "                linewidth=0.6,\n",
    "                alpha=0.7,\n",
    "                zorder=4,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not overlay perimeter/breaklines: {e}\")\n",
    "\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Max Water Surface Elevation (ft)\")\n",
    "    ax.set_xlabel(\"Easting (ft)\")\n",
    "    ax.set_ylabel(\"Northing (ft)\")\n",
    "    ax.set_title(\"Spring Creek - Maximum Water Surface (Plan 01)\\n(2D perimeter + breaklines)\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nPlotted {len(max_ws_gdf)} mesh cells\")\n",
    "else:\n",
    "    print(\"No maximum water surface data found in the plan HDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check Terrain Configuration\n",
    "\n",
    "Verify terrain is properly configured (Pattern 3a includes self-contained terrain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ras_commander import RasMap\n",
    "\n",
    "# Check for .rasmap file\n",
    "rasmap_files = list(project_folder.glob('*.rasmap'))\n",
    "if rasmap_files:\n",
    "    rasmap_file = rasmap_files[0]\n",
    "    print(f\"RAS Mapper file: {rasmap_file.name}\")\n",
    "\n",
    "    # Terrains are tracked in the .rasmap by *layer name* (not by .tif file path).\n",
    "    terrain_layer_names = RasMap.get_terrain_names(rasmap_file)\n",
    "    print(f\"\\nTerrain layers in .rasmap: {len(terrain_layer_names)}\")\n",
    "    for name in terrain_layer_names:\n",
    "        is_valid = RasMap.is_valid_layer(rasmap_file, layer_name=name, layer_type=\"Terrain\")\n",
    "        print(f\"  - {name}: {'✓' if is_valid else '✗'}\")\n",
    "\n",
    "    # Separately list any GeoTIFFs in the Terrain folder (informational)\n",
    "    terrain_folder = project_folder / \"Terrain\"\n",
    "    if terrain_folder.exists():\n",
    "        terrain_files = list(terrain_folder.glob('*.tif'))\n",
    "        print(f\"\\nTerrain GeoTIFFs found: {len(terrain_files)}\")\n",
    "        for tf in terrain_files:\n",
    "            size_gb = tf.stat().st_size / 1e9\n",
    "            print(f\"  - {tf.name}: {size_gb:.2f} GB\")\n",
    "    else:\n",
    "        print(\"  ⚠️ Terrain folder not found\")\n",
    "else:\n",
    "    print(\"⚠️ No .rasmap file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Compute Test Validation\n",
    "\n",
    "**Note**: Spring Creek already has pre-computed results. This section shows how to run a compute test to validate terrain/DSS files if needed.\n",
    "\n",
    "**Skip this section if you just want to use pre-computed results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compute test (requires HEC-RAS 5.0.7 installed)\n",
    "COMPUTE_TEST = True  # Set to True to run\n",
    "\n",
    "if COMPUTE_TEST:\n",
    "    print(\"Running compute test (Plan 01)...\")\n",
    "    print(\"This validates terrain, land use, and DSS files are correct.\")\n",
    "    print(\"Expected time: 30-60 minutes for 2D model\\n\")\n",
    "    \n",
    "    RasCmdr.compute_plan(\"01\", ras_object=ras, num_cores=2)\n",
    "    \n",
    "    print(\"\\n✓ Compute test complete\")\n",
    "    print(\"If plan executed successfully → terrain/DSS files are valid\")\n",
    "else:\n",
    "    print(\"Compute test skipped (using pre-computed results)\")\n",
    "\n",
    "print(\"\\n💡 Compute test instructions available in:\")\n",
    "compute_instructions = organized_folder / \"COMPUTE_TEST_INSTRUCTIONS.md\"\n",
    "if compute_instructions.exists():\n",
    "    print(f\"   {compute_instructions}\")\n",
    "else:\n",
    "    print(\"   See agent/model_log.md for compute test command\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✓ **Organization**: Used generated `organize_springcreek_12040102()` function\n",
    "2. ✓ **4-Folder Structure**: HMS/RAS/Spatial/Documentation standardized\n",
    "3. ✓ **DSS Validation**: Localized DSS files and validated boundary condition pathnames\n",
    "4. ✓ **Results Extraction**: Extracted WSE, depth, velocity from pre-computed results\n",
    "5. ✓ **2D Visualization**: Plotted spatial water surface elevations\n",
    "6. ✓ **Terrain Validation**: Verified self-contained terrain files\n",
    "\n",
    "**Pattern 3a Characteristics**:\n",
    "- Single large 2D model (not multiple streams)\n",
    "- Nested zip extraction required\n",
    "- Self-contained terrain (no SpatialData.zip needed)\n",
    "- Pre-computed results enable immediate analysis\n",
    "\n",
    "**Next Steps**:\n",
    "- Re-run DSS validation after localizing the correct DSS file(s)\n",
    "- Extract time series data for specific locations\n",
    "- Compare results across all 8 plans\n",
    "- Generate inundation maps\n",
    "- Export results to GIS formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rascmdr_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
