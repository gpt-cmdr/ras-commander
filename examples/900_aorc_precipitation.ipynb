{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# AORC Precipitation for HEC-RAS Rain-on-Grid Models\n",
    "\n",
    "This notebook demonstrates a complete workflow for using NOAA's Analysis of Record for Calibration (AORC) gridded precipitation data with HEC-RAS 2D rain-on-grid models.\n",
    "\n",
    "**Example Project**: BaldEagleCrkMulti2D\n",
    "\n",
    "## Workflow\n",
    "1. Extract example project and create a labeled working copy\n",
    "2. Get project bounds from geometry HDF (with buffer)\n",
    "3. Generate a storm catalog from historical AORC data\n",
    "4. Download precipitation, export hyetographs, and create HEC-RAS plans\n",
    "5. Execute all plans in parallel (2 cores, 3 workers)\n",
    "6. Extract and compare results\n",
    "\n",
    "## Data Export\n",
    "All precipitation data is exported to the `Precipitation/` subfolder:\n",
    "- `storm_catalog.csv` - Complete storm catalog with metadata\n",
    "- `storm_YYYYMMDD.nc` - NetCDF precipitation files\n",
    "- `hyetographs/` - PNG plots of each storm's precipitation\n",
    "- `storm_catalog_summary.png` - Overview plot of all storms\n",
    "\n",
    "## AORC Dataset Overview\n",
    "- **Coverage**: CONUS (1979-present), Alaska (1981-present)\n",
    "- **Resolution**: ~800 meters, hourly timesteps\n",
    "- **Format**: Cloud-optimized Zarr on AWS S3\n",
    "- **Access**: Anonymous (no authentication required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install ras-commander[precip]  # Includes xarray, zarr, s3fs, netCDF4, rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DEVELOPMENT MODE TOGGLE\n# =============================================================================\nUSE_LOCAL_SOURCE = False  # <-- TOGGLE THIS\n\nif USE_LOCAL_SOURCE:\n    import sys\n    from pathlib import Path\n    local_path = str(Path.cwd().parent)\n    if local_path not in sys.path:\n        sys.path.insert(0, local_path)\n    print(f\"ðŸ“ LOCAL SOURCE MODE: Loading from {local_path}/ras_commander\")\nelse:\n    print(\"ðŸ“¦ PIP PACKAGE MODE: Loading installed ras-commander\")\n\n# Import ras-commander\nfrom ras_commander import init_ras_project, RasExamples, RasPlan, RasUnsteady, RasCmdr\nfrom ras_commander.precip import PrecipAorc\nfrom ras_commander.hdf.HdfProject import HdfProject\n\n# Additional imports\nimport os\nimport shutil\nimport glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xarray as xr\n\n# Verify which version loaded\nimport ras_commander\nprint(f\"âœ“ Loaded: {ras_commander.__file__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Configure these values to customize the notebook for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS - Edit these to customize the notebook\n",
    "# =============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Project Configuration\n",
    "PROJECT_NAME = \"Muncie\"           # Example project to extract\n",
    "RAS_VERSION = \"6.6\"               # HEC-RAS version (6.3, 6.5, 6.6, etc.)\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUTS_DIR = Path(\"_outputs\") / \"900_aorc_precipitation\"  # Artifacts saved here\n",
    "\n",
    "# AORC Settings\n",
    "ONLINE = True                     # Enable network requests\n",
    "\n",
    "# Create output directory\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Outputs will be saved to: {OUTPUTS_DIR.absolute()}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 1: Extract Example Project and Create Working Copy\n",
    "\n",
    "We extract the BaldEagleCrkMulti2D example and create a labeled copy for our AORC analysis.\n",
    "**Note**: Existing AORC folders are deleted to ensure repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEAR = 2020  # Year to analyze\n",
    "TEMPLATE_PLAN = \"06\"  # Template plan with precipitation enabled (DSS-based, version 6.00)\n",
    "NUM_CORES = 2  # Cores per HEC-RAS instance\n",
    "MAX_WORKERS = 3  # Parallel worker processes\n",
    "\n",
    "# Extract base example project\n",
    "base_project = RasExamples.extract_project(\"BaldEagleCrkMulti2D\", output_path=\"example_projects_24_aorc_precipitation\")\n",
    "print(f\"Base project extracted to: {base_project}\")\n",
    "\n",
    "# Clean up any existing AORC folders (for repeatability)\n",
    "print(\"\\nCleaning up existing AORC folders...\")\n",
    "aorc_pattern = base_project.parent / \"BaldEagleCrkMulti2D_AORC_*\"\n",
    "existing_folders = list(base_project.parent.glob(\"BaldEagleCrkMulti2D_AORC_*\"))\n",
    "for folder in existing_folders:\n",
    "    if folder.is_dir():\n",
    "        print(f\"  Removing: {folder.name}\")\n",
    "        shutil.rmtree(folder)\n",
    "print(f\"  Removed {len(existing_folders)} existing folders\")\n",
    "\n",
    "# Create labeled working copy\n",
    "working_folder = base_project.parent / f\"BaldEagleCrkMulti2D_AORC_{YEAR}\"\n",
    "print(f\"\\nCreating working copy: {working_folder}\")\n",
    "shutil.copytree(base_project, working_folder)\n",
    "\n",
    "# Initialize project\n",
    "ras = init_ras_project(working_folder, \"6.6\")\n",
    "print(f\"\\nProject: {ras.project_name}\")\n",
    "print(f\"Location: {ras.project_folder}\")\n",
    "print(f\"Plans: {len(ras.plan_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 2: Get Project Bounds from Geometry HDF\n",
    "\n",
    "Use `HdfProject.get_project_bounds_latlon()` to extract proper bounds with buffering.\n",
    "This handles CRS transformation and ensures precipitation coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Get geometry HDF path from template plan\n# The 'Geom File' column contains just the number (e.g., \"09\"),\n# so we need to add the \"g\" prefix for the geometry file extension\ntemplate_row = ras.plan_df[ras.plan_df['plan_number'] == TEMPLATE_PLAN].iloc[0]\ngeom_number = template_row['Geom File']\ngeom_hdf = ras.project_folder / f\"{ras.project_name}.g{geom_number}.hdf\"\n\nprint(f\"Template plan {TEMPLATE_PLAN} uses geometry: g{geom_number}\")\nprint(f\"Geometry HDF: {geom_hdf}\")\nprint(f\"Exists: {geom_hdf.exists()}\")\n\n# Get bounds with 50% buffer (default)\n# This properly handles CRS transformation and includes 2D mesh, 1D elements, and storage areas\nbounds = HdfProject.get_project_bounds_latlon(\n    geom_hdf,\n    buffer_percent=50.0,  # 50% buffer ensures full coverage after reprojection\n    include_1d=True,\n    include_2d=True,\n    include_storage=True\n)\n\nwest, south, east, north = bounds\nprint(f\"\\nProject Bounds (WGS84 with 50% buffer):\")\nprint(f\"  West:  {west:.4f}\")\nprint(f\"  South: {south:.4f}\")\nprint(f\"  East:  {east:.4f}\")\nprint(f\"  North: {north:.4f}\")\nprint(f\"  Size:  {east-west:.4f} x {north-south:.4f} degrees\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 3: Generate Storm Catalog\n",
    "\n",
    "Analyze AORC data to identify all significant precipitation events for the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate storm catalog\n",
    "storm_catalog = PrecipAorc.get_storm_catalog(\n",
    "    bounds=bounds,\n",
    "    year=YEAR,\n",
    "    inter_event_hours=8.0,      # USGS standard for storm separation\n",
    "    min_depth_inches=0.75,      # Minimum significant precipitation\n",
    "    buffer_hours=48             # Simulation warmup buffer\n",
    ")\n",
    "\n",
    "print(f\"\\nStorm Catalog: {len(storm_catalog)} events for {YEAR}\")\n",
    "print(\"=\"*90)\n",
    "print(storm_catalog[['storm_id', 'start_time', 'end_time', 'total_depth_in', \n",
    "                     'peak_intensity_in_hr', 'duration_hours', 'rank']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Download AORC Data and Export Precipitation Records\n",
    "\n",
    "Download precipitation data, export storm catalog CSV, and generate hyetograph plots for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create precipitation folder structure\n",
    "precip_folder = ras.project_folder / \"Precipitation\"\n",
    "precip_folder.mkdir(exist_ok=True)\n",
    "hyetograph_folder = precip_folder / \"hyetographs\"\n",
    "hyetograph_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Export storm catalog to CSV\n",
    "catalog_csv = precip_folder / \"storm_catalog.csv\"\n",
    "storm_catalog.to_csv(catalog_csv, index=False)\n",
    "print(f\"Storm catalog exported to: {catalog_csv}\")\n",
    "\n",
    "# Download AORC data for all storms\n",
    "print(f\"\\nDownloading AORC precipitation data for {len(storm_catalog)} storms...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "precip_files = {}\n",
    "for idx, storm in storm_catalog.iterrows():\n",
    "    storm_id = storm['storm_id']\n",
    "    date_str = storm['start_time'].strftime('%Y%m%d')\n",
    "    precip_file = precip_folder / f\"storm_{date_str}.nc\"\n",
    "    \n",
    "    if not precip_file.exists():\n",
    "        print(f\"  Storm {storm_id:2d}: {storm['start_time'].strftime('%b %d')} - Downloading...\")\n",
    "        PrecipAorc.download(\n",
    "            bounds=bounds,\n",
    "            start_time=storm['sim_start'],\n",
    "            end_time=storm['sim_end'],\n",
    "            output_path=precip_file,\n",
    "            target_crs=\"EPSG:5070\",\n",
    "            resolution=2000.0\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  Storm {storm_id:2d}: {storm['start_time'].strftime('%b %d')} - Already downloaded\")\n",
    "    \n",
    "    precip_files[storm_id] = precip_file\n",
    "\n",
    "print(f\"\\nDownloaded {len(precip_files)} precipitation files to {precip_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save individual hyetograph plots for each storm\n",
    "print(f\"Generating hyetograph plots for {len(storm_catalog)} storms...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, storm in storm_catalog.iterrows():\n",
    "    storm_id = storm['storm_id']\n",
    "    date_str = storm['start_time'].strftime('%Y%m%d')\n",
    "    precip_file = precip_files[storm_id]\n",
    "    \n",
    "    # Load precipitation data\n",
    "    ds = xr.open_dataset(precip_file)\n",
    "    da = ds['APCP_surface']\n",
    "    hourly_mean = da.mean(dim=['x', 'y']).values\n",
    "    times = pd.to_datetime(da.time.values)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.bar(times, hourly_mean, width=0.03, color='steelblue', alpha=0.8, edgecolor='darkblue', linewidth=0.5)\n",
    "    \n",
    "    # Add storm info\n",
    "    ax.set_title(f\"Storm {storm_id}: {storm['start_time'].strftime('%B %d, %Y')}\\n\"\n",
    "                 f\"Total: {storm['total_depth_in']:.2f} in | Peak: {storm['peak_intensity_in_hr']:.3f} in/hr | \"\n",
    "                 f\"Duration: {storm['duration_hours']:.0f} hours\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date/Time', fontsize=11)\n",
    "    ax.set_ylabel('Precipitation Rate (mm/hr)', fontsize=11)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics box\n",
    "    stats_text = (f\"Max Rate: {hourly_mean.max():.2f} mm/hr\\n\"\n",
    "                  f\"Total: {hourly_mean.sum():.1f} mm\\n\"\n",
    "                  f\"Timesteps: {len(hourly_mean)}\")\n",
    "    ax.text(0.98, 0.95, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = hyetograph_folder / f\"storm_{date_str}_hyetograph.png\"\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    ds.close()\n",
    "    print(f\"  Storm {storm_id:2d}: {storm['start_time'].strftime('%b %d')} - Saved to {plot_path.name}\")\n",
    "\n",
    "print(f\"\\nHyetographs saved to: {hyetograph_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storm catalog summary plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Total depth by storm\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.bar(storm_catalog['storm_id'], storm_catalog['total_depth_in'], color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel('Storm ID')\n",
    "ax1.set_ylabel('Total Depth (inches)')\n",
    "ax1.set_title('Precipitation Totals')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, storm_catalog['total_depth_in']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{val:.2f}',\n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. Peak intensity\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.bar(storm_catalog['storm_id'], storm_catalog['peak_intensity_in_hr'], color='darkorange', alpha=0.8)\n",
    "ax2.set_xlabel('Storm ID')\n",
    "ax2.set_ylabel('Peak Intensity (in/hr)')\n",
    "ax2.set_title('Peak Intensity')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Duration\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.bar(storm_catalog['storm_id'], storm_catalog['duration_hours'], color='green', alpha=0.8)\n",
    "ax3.set_xlabel('Storm ID')\n",
    "ax3.set_ylabel('Duration (hours)')\n",
    "ax3.set_title('Storm Duration')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Timeline\n",
    "ax4 = axes[1, 1]\n",
    "for idx, storm in storm_catalog.iterrows():\n",
    "    ax4.barh(storm['storm_id'], storm['duration_hours'], left=storm['start_time'].dayofyear,\n",
    "             color='steelblue', alpha=0.7, height=0.6)\n",
    "ax4.set_xlabel(f'Day of Year ({YEAR})')\n",
    "ax4.set_ylabel('Storm ID')\n",
    "ax4.set_title('Storm Timeline')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle(f'Storm Catalog Summary - {YEAR}\\n({len(storm_catalog)} storms, '\n",
    "             f'{storm_catalog[\"total_depth_in\"].sum():.1f} inches total)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save summary plot\n",
    "summary_path = precip_folder / \"storm_catalog_summary.png\"\n",
    "plt.savefig(summary_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Summary plot saved to: {summary_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storm plans with precipitation data written to HDF\n",
    "print(f\"Creating HEC-RAS plans for {len(storm_catalog)} storms...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use create_storm_plans which calls set_gridded_precipitation\n",
    "# This writes precipitation data directly to the .u##.hdf file\n",
    "results = PrecipAorc.create_storm_plans(\n",
    "    storm_catalog=storm_catalog,\n",
    "    bounds=bounds,\n",
    "    template_plan=TEMPLATE_PLAN,\n",
    "    precip_folder=\"Precipitation\",\n",
    "    ras_object=ras,\n",
    "    download_data=False  # Already downloaded above\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nPlan Creation Results:\")\n",
    "print(results[['storm_id', 'start_time', 'total_depth_in', 'plan_number', 'status']].to_string(index=False))\n",
    "\n",
    "# Refresh plan list\n",
    "ras.plan_df = ras.get_plan_entries()\n",
    "print(f\"\\nTotal plans in project: {len(ras.plan_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Step 5: Execute Storm Plans in Parallel\n",
    "\n",
    "Run all storm plans using parallel execution with 2 cores per instance and 3 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of storm plan numbers\n",
    "storm_plan_numbers = results[results['status'] == 'success']['plan_number'].tolist()\n",
    "print(f\"Plans to execute: {storm_plan_numbers}\")\n",
    "print(f\"Execution config: {NUM_CORES} cores x {MAX_WORKERS} workers\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute plans in parallel\n",
    "import time\n",
    "\n",
    "print(f\"Starting parallel execution of {len(storm_plan_numbers)} plans...\")\n",
    "start_time = time.time()\n",
    "\n",
    "execution_results = RasCmdr.compute_parallel(\n",
    "    plan_number=storm_plan_numbers,\n",
    "    max_workers=MAX_WORKERS,\n",
    "    num_cores=NUM_CORES,\n",
    "    ras_object=ras,\n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Results summary\n",
    "success_count = sum(1 for success in execution_results.values() if success)\n",
    "fail_count = len(execution_results) - success_count\n",
    "\n",
    "print(f\"\\nExecution complete in {elapsed/60:.1f} minutes\")\n",
    "print(f\"  Successful: {success_count}\")\n",
    "print(f\"  Failed: {fail_count}\")\n",
    "print(f\"\\nResults copied to: {ras.project_folder.parent / (ras.project_folder.name + ' [Computed]')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 6: Extract and Compare Results\n",
    "\n",
    "Re-initialize from the computed folder and extract results summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize from computed folder\n",
    "computed_folder = ras.project_folder.parent / f\"{ras.project_folder.name} [Computed]\"\n",
    "if computed_folder.exists():\n",
    "    ras = init_ras_project(computed_folder, \"6.6\")\n",
    "    print(f\"Re-initialized from: {computed_folder}\")\n",
    "else:\n",
    "    print(f\"Warning: Computed folder not found\")\n",
    "\n",
    "# Extract results summary\n",
    "import h5py\n",
    "\n",
    "print(\"\\nStorm Execution Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "storm_results = []\n",
    "for idx, row in results[results['status'] == 'success'].iterrows():\n",
    "    storm_id = row['storm_id']\n",
    "    plan_num = row['plan_number']\n",
    "    exec_success = execution_results.get(plan_num, False)\n",
    "    \n",
    "    plan_row = ras.plan_df[ras.plan_df['plan_number'] == plan_num]\n",
    "    if len(plan_row) == 0:\n",
    "        continue\n",
    "    \n",
    "    plan_path = Path(plan_row.iloc[0]['full_path'])\n",
    "    hdf_path = Path(str(plan_path) + '.hdf')\n",
    "    \n",
    "    result = {\n",
    "        'storm_id': storm_id,\n",
    "        'plan_number': plan_num,\n",
    "        'start_time': row['start_time'],\n",
    "        'total_depth_in': row['total_depth_in'],\n",
    "        'exec_success': exec_success,\n",
    "        'hdf_exists': hdf_path.exists(),\n",
    "        'hdf_size_mb': hdf_path.stat().st_size / 1e6 if hdf_path.exists() else 0\n",
    "    }\n",
    "    \n",
    "    # Extract compute time if available\n",
    "    if hdf_path.exists():\n",
    "        with h5py.File(hdf_path, 'r') as f:\n",
    "                if 'Results/Summary/Compute Processes' in f:\n",
    "                    cp = f['Results/Summary/Compute Processes'][:]\n",
    "                    if len(cp) > 0:\n",
    "                        result['compute_time'] = cp[0]['Compute Time'].decode('utf-8').strip()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    storm_results.append(result)\n",
    "    status = 'OK' if exec_success and result['hdf_exists'] else 'FAILED'\n",
    "    print(f\"Storm {storm_id:2d} ({row['start_time'].strftime('%b %d')}): Plan {plan_num} - {status} - {result['hdf_size_mb']:.1f} MB\")\n",
    "\n",
    "print(f\"\\nCompleted: {sum(1 for r in storm_results if r['exec_success'])} of {len(storm_results)} storms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "if storm_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Precipitation totals\n",
    "    ax1 = axes[0]\n",
    "    storm_dates = [r['start_time'].strftime('%m/%d') for r in storm_results]\n",
    "    precip_totals = [r['total_depth_in'] for r in storm_results]\n",
    "    colors = ['green' if r['exec_success'] else 'red' for r in storm_results]\n",
    "    \n",
    "    bars = ax1.bar(range(len(storm_results)), precip_totals, color=colors, alpha=0.8)\n",
    "    ax1.set_xlabel('Storm')\n",
    "    ax1.set_ylabel('Total Precipitation (inches)')\n",
    "    ax1.set_title('Storm Precipitation Totals (green=success)')\n",
    "    ax1.set_xticks(range(len(storm_results)))\n",
    "    ax1.set_xticklabels(storm_dates, rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # HDF sizes\n",
    "    ax2 = axes[1]\n",
    "    hdf_sizes = [r['hdf_size_mb'] for r in storm_results]\n",
    "    ax2.bar(range(len(storm_results)), hdf_sizes, color=colors, alpha=0.8)\n",
    "    ax2.set_xlabel('Storm')\n",
    "    ax2.set_ylabel('HDF File Size (MB)')\n",
    "    ax2.set_title('Simulation Output Files')\n",
    "    ax2.set_xticks(range(len(storm_results)))\n",
    "    ax2.set_xticklabels(storm_dates, rotation=45)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle(f'AORC Storm Simulation Results - {YEAR}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Data Export Summary\n",
    "\n",
    "All precipitation data has been exported to the project's `Precipitation/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all exported files\n",
    "print(f\"Precipitation Data Export Summary\")\n",
    "print(f\"Location: {precip_folder}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count files\n",
    "nc_files = list(precip_folder.glob(\"storm_*.nc\"))\n",
    "png_files = list(hyetograph_folder.glob(\"*.png\"))\n",
    "csv_files = list(precip_folder.glob(\"*.csv\"))\n",
    "summary_files = list(precip_folder.glob(\"storm_catalog_summary.png\"))\n",
    "\n",
    "print(f\"\\nExported Files:\")\n",
    "print(f\"  Storm Catalog CSV:    {len(csv_files)} file(s)\")\n",
    "print(f\"  NetCDF Precip Files:  {len(nc_files)} file(s)\")\n",
    "print(f\"  Hyetograph PNGs:      {len(png_files)} file(s)\")\n",
    "print(f\"  Summary Plot:         {len(summary_files)} file(s)\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = sum(f.stat().st_size for f in nc_files + png_files + csv_files + summary_files)\n",
    "print(f\"\\nTotal Size: {total_size / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nFolder Structure:\")\n",
    "print(f\"  Precipitation/\")\n",
    "print(f\"    storm_catalog.csv\")\n",
    "print(f\"    storm_catalog_summary.png\")\n",
    "print(f\"    storm_YYYYMMDD.nc  (x{len(nc_files)})\")\n",
    "print(f\"    hyetographs/\")\n",
    "print(f\"      storm_YYYYMMDD_hyetograph.png  (x{len(png_files)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete AORC precipitation workflow:\n",
    "\n",
    "1. **Project Setup** - Extracted example and created labeled working copy (with cleanup)\n",
    "2. **Bounds Calculation** - Used `HdfProject.get_project_bounds_latlon()` with 50% buffer\n",
    "3. **Storm Catalog** - Generated catalog using USGS standard parameters\n",
    "4. **Precipitation Export** - Downloaded AORC data, exported CSV catalog and hyetograph plots\n",
    "5. **Plan Creation** - Created HEC-RAS plans with precipitation written to HDF\n",
    "6. **Parallel Execution** - Ran all plans with 2 cores and 3 workers\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `HdfProject.get_project_bounds_latlon()` | Get buffered bounds from geometry HDF |\n",
    "| `PrecipAorc.get_storm_catalog()` | Generate catalog of precipitation events |\n",
    "| `PrecipAorc.download()` | Download AORC data to NetCDF |\n",
    "| `PrecipAorc.create_storm_plans()` | Create plans with precipitation in HDF |\n",
    "| `RasCmdr.compute_parallel()` | Execute plans in parallel |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "# Optional: Process All Available AORC Years (1979-2024)\n",
    "\n",
    "The following cell processes storm catalogs for all available years in the AORC dataset.\n",
    "Each year gets its own project folder with full precipitation data export.\n",
    "\n",
    "**Warning**: This can take a very long time and generate many files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Process all AORC years\n",
    "# Set PROCESS_ALL_YEARS = True to run\n",
    "\n",
    "PROCESS_ALL_YEARS = False  # Set to True to run\n",
    "\n",
    "if PROCESS_ALL_YEARS:\n",
    "    import time\n",
    "    \n",
    "    # AORC is available from 1979-present for CONUS\n",
    "    ALL_YEARS = list(range(1979, 2025))  # 1979-2024\n",
    "    \n",
    "    print(f\"Processing {len(ALL_YEARS)} years of AORC data\")\n",
    "    print(f\"Configuration: {NUM_CORES} cores x {MAX_WORKERS} workers per year\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Clean up ALL existing AORC folders first\n",
    "    print(\"\\nCleaning up existing AORC folders...\")\n",
    "    existing_folders = list(base_project.parent.glob(\"BaldEagleCrkMulti2D_AORC_*\"))\n",
    "    for folder in existing_folders:\n",
    "        if folder.is_dir():\n",
    "            print(f\"  Removing: {folder.name}\")\n",
    "            shutil.rmtree(folder)\n",
    "    print(f\"  Removed {len(existing_folders)} existing folders\")\n",
    "    \n",
    "    all_year_results = {}\n",
    "    \n",
    "    for year in ALL_YEARS:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PROCESSING YEAR: {year}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create year-specific working folder\n",
    "        year_folder = base_project.parent / f\"BaldEagleCrkMulti2D_AORC_{year}\"\n",
    "            print(f\"  Creating: {year_folder.name}\")\n",
    "            shutil.copytree(base_project, year_folder)\n",
    "            \n",
    "            # Initialize\n",
    "            ras_year = init_ras_project(year_folder, \"6.6\")\n",
    "            \n",
    "            # Generate storm catalog\n",
    "            print(f\"  Generating storm catalog for {year}...\")\n",
    "            catalog = PrecipAorc.get_storm_catalog(\n",
    "                bounds=bounds,\n",
    "                year=year,\n",
    "                inter_event_hours=8.0,\n",
    "                min_depth_inches=0.75,\n",
    "                buffer_hours=48\n",
    "            )\n",
    "            print(f\"  Found {len(catalog)} storms\")\n",
    "            \n",
    "            if len(catalog) == 0:\n",
    "                print(f\"  No storms found for {year}, skipping\")\n",
    "                all_year_results[year] = {'storms': 0, 'plans': 0, 'success': 0, 'elapsed_min': 0}\n",
    "                continue\n",
    "            \n",
    "            # Create precipitation folder structure\n",
    "            year_precip_folder = year_folder / \"Precipitation\"\n",
    "            year_precip_folder.mkdir(exist_ok=True)\n",
    "            year_hyetograph_folder = year_precip_folder / \"hyetographs\"\n",
    "            year_hyetograph_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Export storm catalog to CSV\n",
    "            catalog.to_csv(year_precip_folder / \"storm_catalog.csv\", index=False)\n",
    "            print(f\"  Exported storm catalog CSV\")\n",
    "            \n",
    "            # Download precipitation and create plans\n",
    "            print(f\"  Downloading precipitation and creating plans...\")\n",
    "            year_plan_results = PrecipAorc.create_storm_plans(\n",
    "                storm_catalog=catalog,\n",
    "                bounds=bounds,\n",
    "                template_plan=TEMPLATE_PLAN,\n",
    "                precip_folder=\"Precipitation\",\n",
    "                ras_object=ras_year,\n",
    "                download_data=True\n",
    "            )\n",
    "            \n",
    "            # Generate hyetographs for each storm\n",
    "            print(f\"  Generating hyetograph plots...\")\n",
    "            for idx, storm in catalog.iterrows():\n",
    "                storm_id = storm['storm_id']\n",
    "                date_str = storm['start_time'].strftime('%Y%m%d')\n",
    "                precip_file = year_precip_folder / f\"storm_{date_str}.nc\"\n",
    "                \n",
    "                if precip_file.exists():\n",
    "                    ds = xr.open_dataset(precip_file)\n",
    "                    da = ds['APCP_surface']\n",
    "                    hourly_mean = da.mean(dim=['x', 'y']).values\n",
    "                    times = pd.to_datetime(da.time.values)\n",
    "                    \n",
    "                    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "                    ax.bar(times, hourly_mean, width=0.03, color='steelblue', alpha=0.8)\n",
    "                    ax.set_title(f\"Storm {storm_id}: {storm['start_time'].strftime('%B %d, %Y')}\\n\"\n",
    "                                 f\"Total: {storm['total_depth_in']:.2f} in | Peak: {storm['peak_intensity_in_hr']:.3f} in/hr\",\n",
    "                                 fontsize=12, fontweight='bold')\n",
    "                    ax.set_xlabel('Date/Time')\n",
    "                    ax.set_ylabel('Precipitation Rate (mm/hr)')\n",
    "                    ax.tick_params(axis='x', rotation=45)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(year_hyetograph_folder / f\"storm_{date_str}_hyetograph.png\", dpi=150, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    ds.close()\n",
    "            \n",
    "            # Create summary plot\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            axes[0].bar(catalog['storm_id'], catalog['total_depth_in'], color='steelblue', alpha=0.8)\n",
    "            axes[0].set_xlabel('Storm ID')\n",
    "            axes[0].set_ylabel('Total Depth (in)')\n",
    "            axes[0].set_title('Precipitation Totals')\n",
    "            axes[1].bar(catalog['storm_id'], catalog['peak_intensity_in_hr'], color='darkorange', alpha=0.8)\n",
    "            axes[1].set_xlabel('Storm ID')\n",
    "            axes[1].set_ylabel('Peak Intensity (in/hr)')\n",
    "            axes[1].set_title('Peak Intensity')\n",
    "            axes[2].bar(catalog['storm_id'], catalog['duration_hours'], color='green', alpha=0.8)\n",
    "            axes[2].set_xlabel('Storm ID')\n",
    "            axes[2].set_ylabel('Duration (hours)')\n",
    "            axes[2].set_title('Storm Duration')\n",
    "            plt.suptitle(f'Storm Catalog Summary - {year}', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(year_precip_folder / \"storm_catalog_summary.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Get plan numbers and execute\n",
    "            plan_numbers = year_plan_results[year_plan_results['status'] == 'success']['plan_number'].tolist()\n",
    "            print(f\"  Created {len(plan_numbers)} plans\")\n",
    "            \n",
    "            if len(plan_numbers) == 0:\n",
    "                print(f\"  No successful plans for {year}, skipping execution\")\n",
    "                all_year_results[year] = {'storms': len(catalog), 'plans': 0, 'success': 0, 'elapsed_min': 0}\n",
    "                continue\n",
    "            \n",
    "            # Execute in parallel\n",
    "            print(f\"  Executing {len(plan_numbers)} plans ({NUM_CORES} cores x {MAX_WORKERS} workers)...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            exec_results = RasCmdr.compute_parallel(\n",
    "                plan_number=plan_numbers,\n",
    "                max_workers=MAX_WORKERS,\n",
    "                num_cores=NUM_CORES,\n",
    "                ras_object=ras_year,\n",
    "                overwrite_dest=True\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            success = sum(1 for v in exec_results.values() if v)\n",
    "            \n",
    "            all_year_results[year] = {\n",
    "                'storms': len(catalog),\n",
    "                'plans': len(plan_numbers),\n",
    "                'success': success,\n",
    "                'elapsed_min': elapsed / 60\n",
    "            }\n",
    "            \n",
    "            print(f\"  Completed: {success}/{len(plan_numbers)} in {elapsed/60:.1f} minutes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {year}: {e}\")\n",
    "            all_year_results[year] = {'error': str(e)}\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ALL YEARS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_storms = 0\n",
    "    total_success = 0\n",
    "    for year, res in sorted(all_year_results.items()):\n",
    "        if 'error' in res:\n",
    "            print(f\"{year}: ERROR - {res['error']}\")\n",
    "        else:\n",
    "            total_storms += res['storms']\n",
    "            total_success += res['success']\n",
    "            print(f\"{year}: {res['storms']} storms, {res['success']}/{res['plans']} success, {res['elapsed_min']:.1f} min\")\n",
    "    \n",
    "    print(f\"\\nTOTAL: {total_storms} storms, {total_success} successful simulations\")\n",
    "else:\n",
    "    print(\"Set PROCESS_ALL_YEARS = True to process all AORC years (1979-2024)\")\n",
    "    print(\"Warning: This will take many hours and create ~50 project folders!\")\n",
    "    print(\"\\nEach year will include:\")\n",
    "    print(\"  - Precipitation/storm_catalog.csv\")\n",
    "    print(\"  - Precipitation/storm_catalog_summary.png\")\n",
    "    print(\"  - Precipitation/storm_YYYYMMDD.nc (per storm)\")\n",
    "    print(\"  - Precipitation/hyetographs/storm_YYYYMMDD_hyetograph.png (per storm)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}