{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated HEC-RAS Analysis for Multiple AEP Events\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for performing flood analysis with different Annual Exceedance Probability (AEP) events. We'll automate the following steps:\n",
    "\n",
    "1. Generate hyetographs from NOAA Atlas 14 data for different AEP events\n",
    "2. Download the Davis HEC-RAS project\n",
    "3. Clone and configure HEC-RAS plans and unsteady flow files for each AEP event\n",
    "4. Execute all plans in parallel\n",
    "5. Extract and visualize results\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "We'll use the following libraries:\n",
    "- `ras-commander`: For HEC-RAS automation\n",
    "- `pandas`, `numpy`: For data manipulation\n",
    "- `matplotlib`: For visualization\n",
    "- Standard libraries: `os`, `re`, `pathlib`, etc.\n",
    "\n",
    "Let's start by installing ras-commander (if needed) and importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander if needed (uncomment to run)\n",
    "# !pip install ras-commander\n",
    "\n",
    "# Import necessary libraries\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from math import log, exp\n",
    "from pathlib import Path\n",
    "import time\n",
    "import psutil  # For getting system CPU info\n",
    "from IPython import display\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Hyetographs from NOAA Atlas 14 Data\n",
    "\n",
    "First, let's define functions to generate hyetographs from NOAA Atlas 14 precipitation frequency data. These functions will:\n",
    "1. Parse duration strings from the CSV file\n",
    "2. Read and process precipitation frequency data\n",
    "3. Interpolate depths for each ARI (Annual Recurrence Interval)\n",
    "4. Compute incremental depths\n",
    "5. Apply the Alternating Block Method to generate hyetographs\n",
    "6. Save the hyetographs to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse duration strings and convert them to hours\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a duration string and converts it to hours.\n",
    "    Examples:\n",
    "        \"5-min:\" -> 0.0833 hours\n",
    "        \"2-hr:\" -> 2 hours\n",
    "        \"2-day:\" -> 48 hours\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "    value, unit = match.groups()\n",
    "    value = int(value)\n",
    "    unit = unit.lower()\n",
    "    if unit in ['min', 'minute', 'minutes']:\n",
    "        hours = value / 60.0\n",
    "    elif unit in ['hr', 'hour', 'hours']:\n",
    "        hours = value\n",
    "    elif unit in ['day', 'days']:\n",
    "        hours = value * 24\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
    "    return hours\n",
    "\n",
    "# Function to read and process the precipitation frequency CSV\n",
    "def read_precipitation_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the precipitation frequency CSV and returns a DataFrame\n",
    "    with durations in hours as the index and ARIs as columns.\n",
    "    This function dynamically locates the header line for the data table.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_line_idx = None\n",
    "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
    "\n",
    "    # Locate the header line\n",
    "    for idx, line in enumerate(lines):\n",
    "        if header_pattern.match(line.strip().lower()):\n",
    "            header_line_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_line_idx is None:\n",
    "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
    "\n",
    "    # Extract the ARI headers from the header line\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    headers = [item.strip() for item in header_line.split(',')]\n",
    "    \n",
    "    if len(headers) < 2:\n",
    "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
    "\n",
    "    aris = headers[1:]  # Exclude the first column which is the duration\n",
    "\n",
    "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
    "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
    "\n",
    "    # Initialize lists to store durations and corresponding depths\n",
    "    durations = []\n",
    "    depths = {ari: [] for ari in aris}\n",
    "\n",
    "    # Iterate over the lines following the header to extract data\n",
    "    for line in lines[header_line_idx + 1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if not duration_pattern.match(line):\n",
    "            break  # Stop if the line does not match the duration pattern\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if len(parts) != len(headers):\n",
    "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
    "        duration_str = parts[0]\n",
    "        try:\n",
    "            duration_hours = parse_duration(duration_str)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Skipping line due to error: {ve}\")\n",
    "            continue  # Skip lines with invalid duration formats\n",
    "        durations.append(duration_hours)\n",
    "        for ari, depth_str in zip(aris, parts[1:]):\n",
    "            try:\n",
    "                depth = float(depth_str)\n",
    "            except ValueError:\n",
    "                depth = np.nan  # Assign NaN for invalid depth values\n",
    "            depths[ari].append(depth)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(depths, index=durations)\n",
    "    df.index.name = 'Duration_hours'\n",
    "\n",
    "    # Drop any rows with NaN values (optional, based on data quality)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to perform log-log linear interpolation for each ARI\n",
    "def interpolate_depths(df, total_duration):\n",
    "    \"\"\"\n",
    "    Interpolates precipitation depths for each ARI on a log-log scale\n",
    "    for each hour up to the total storm duration.\n",
    "    \"\"\"\n",
    "    T = total_duration\n",
    "    t_hours = np.arange(1, T+1)\n",
    "    D = {}\n",
    "    for ari in df.columns:\n",
    "        durations = df.index.values\n",
    "        depths = df[ari].values\n",
    "        # Ensure all depths are positive\n",
    "        if np.any(depths <= 0):\n",
    "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
    "        # Log-log interpolation\n",
    "        log_durations = np.log(durations)\n",
    "        log_depths = np.log(depths)\n",
    "        log_t = np.log(t_hours)\n",
    "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
    "        D_t = np.exp(log_D_t)\n",
    "        D[ari] = D_t\n",
    "    return D\n",
    "\n",
    "# Function to compute incremental precipitation depths\n",
    "def compute_incremental_depths(D, total_duration):\n",
    "    \"\"\"\n",
    "    Computes incremental precipitation depths for each hour.\n",
    "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
    "    \"\"\"\n",
    "    incremental_depths = {}\n",
    "    for ari, D_t in D.items():\n",
    "        I_t = np.empty(total_duration)\n",
    "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
    "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
    "        incremental_depths[ari] = I_t\n",
    "    return incremental_depths\n",
    "\n",
    "# Function to assign incremental depths using the Alternating Block Method\n",
    "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
    "    \"\"\"\n",
    "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    hyetograph = [0.0] * T\n",
    "    hyetograph[central_index] = max_depth\n",
    "    remaining_depths = sorted_depths.copy()\n",
    "    remaining_depths.remove(max_depth)\n",
    "    left = central_index - 1\n",
    "    right = central_index + 1\n",
    "    toggle = True  # Start assigning to the right\n",
    "    for depth in remaining_depths:\n",
    "        if toggle and right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif not toggle and left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        elif right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        else:\n",
    "            print(\"Warning: Not all incremental depths assigned.\")\n",
    "            break\n",
    "        toggle = not toggle\n",
    "    return hyetograph\n",
    "\n",
    "# Function to generate the hyetograph for a given ARI\n",
    "def generate_hyetograph(incremental_depths, position_percent, T):\n",
    "    \"\"\"\n",
    "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    max_depth = np.max(incremental_depths)\n",
    "    incremental_depths_list = incremental_depths.tolist()\n",
    "    central_index = int(round(T * position_percent / 100)) - 1\n",
    "    central_index = max(0, min(central_index, T - 1))\n",
    "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
    "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
    "    return hyetograph\n",
    "\n",
    "# Function to save the hyetograph to a CSV file\n",
    "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
    "    \"\"\"\n",
    "    Saves the hyetograph to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Time_hour': np.arange(1, total_duration + 1),\n",
    "        'Precipitation_in': hyetograph\n",
    "    })\n",
    "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hyetographs for Different AEP Events\n",
    "\n",
    "Now, let's use the functions defined above to generate hyetographs for different AEP events.\n",
    "We'll download the NOAA Atlas 14 data for Davis, CA and generate hyetographs for various ARI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open NOAA Atlas 14 data from CSV file\n",
    "def open_noaa_atlas14_csv():\n",
    "    \"\"\"\n",
    "    Opens NOAA Atlas 14 data from CSV file in data directory.\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    data_dir = Path('data')\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    input_file = data_dir / 'PF_Depth_English_PDS_DavisCA.csv'\n",
    "    if input_file.exists():\n",
    "        logging.info(f\"NOAA Atlas 14 data file found: {input_file}\")\n",
    "        return str(input_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"NOAA Atlas 14 data file not found at {input_file}\")\n",
    "\n",
    "# Generate hyetographs\n",
    "def generate_all_hyetographs(input_csv, output_dir, ari_values, position_percent=50, total_duration=24):\n",
    "    \"\"\"\n",
    "    Generates hyetographs for specified ARI values.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv: Path to NOAA Atlas 14 CSV file\n",
    "    - output_dir: Directory to save hyetographs\n",
    "    - ari_values: List of ARI values to generate hyetographs for\n",
    "    - position_percent: Position percentage for peak intensity (default: 50%)\n",
    "    - total_duration: Total storm duration in hours (default: 24 hours)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping ARI values to hyetograph file paths\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Output directory is set to: {output_dir}\")\n",
    "    \n",
    "    # Read precipitation data\n",
    "    try:\n",
    "        df = read_precipitation_data(input_csv)\n",
    "        logging.info(\"Successfully read the input CSV file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading input CSV: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Display the first few rows of the DataFrame to verify\n",
    "    logging.info(\"\\nPrecipitation Frequency Data (first few rows):\")\n",
    "    display.display(df.head())\n",
    "    \n",
    "    # Interpolate depths\n",
    "    try:\n",
    "        D = interpolate_depths(df, total_duration)\n",
    "        logging.info(\"Successfully interpolated precipitation depths.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during interpolation: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Compute incremental depths\n",
    "    I = compute_incremental_depths(D, total_duration)\n",
    "    logging.info(\"Successfully computed incremental depths.\")\n",
    "    \n",
    "    # Generate and save hyetographs for each ARI\n",
    "    hyetograph_files = {}\n",
    "    for ari in ari_values:\n",
    "        ari_str = str(ari)\n",
    "        if ari_str in I:\n",
    "            incremental_depths = I[ari_str]\n",
    "            hyetograph = generate_hyetograph(incremental_depths, position_percent, total_duration)\n",
    "            hyetograph_file = save_hyetograph(hyetograph, ari_str, output_dir, position_percent, total_duration)\n",
    "            hyetograph_files[ari_str] = hyetograph_file\n",
    "        else:\n",
    "            logging.warning(f\"ARI {ari} not found in the input data. Skipping.\")\n",
    "    \n",
    "    logging.info(f\"\\nGenerated {len(hyetograph_files)} hyetographs.\")\n",
    "    return hyetograph_files\n",
    "\n",
    "# Plot multiple hyetographs\n",
    "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
    "    \"\"\"\n",
    "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    - aris (list of str or int): List of Annual Recurrence Intervals to plot\n",
    "    - position_percent (int): Position percentage for the maximum intensity\n",
    "    - total_duration (int): Total storm duration in hours\n",
    "    - output_dir (str): Directory where hyetograph CSV files are saved\n",
    "    \n",
    "    Returns:\n",
    "    - Plot object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    for ari in aris:\n",
    "        # Ensure ARI is a string for consistent filename formatting\n",
    "        ari_str = str(ari)\n",
    "        \n",
    "        # Construct the filename based on the naming convention\n",
    "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            logging.warning(f\"File '{filename}' does not exist in '{output_dir}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        try:\n",
    "            hyetograph_df = pd.read_csv(filepath)\n",
    "            logging.info(f\"Successfully read hyetograph data from '{filename}'.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading CSV file '{filename}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot the hyetograph\n",
    "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
    "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Time (Hour)', fontsize=14)\n",
    "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
    "    plt.title(f'Comparison of Hyetographs for Different ARIs\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 12)))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt_filename = f\"hyetograph_comparison_pos{position_percent}pct_{total_duration}hr.png\"\n",
    "    plt_filepath = os.path.join(output_dir, plt_filename)\n",
    "    plt.savefig(plt_filepath, dpi=300)\n",
    "    logging.info(f\"Saved comparison plot to {plt_filepath}\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Run the hyetograph generation process\n",
    "ari_values = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "position_percent = 50  # Position of peak intensity (50% = center of storm)\n",
    "total_duration = 24    # Total storm duration in hours\n",
    "\n",
    "# Open NOAA Atlas 14 data\n",
    "input_csv = open_noaa_atlas14_csv()\n",
    "\n",
    "# Generate hyetographs\n",
    "output_dir = 'hyetographs'\n",
    "hyetograph_files = generate_all_hyetographs(\n",
    "    input_csv=input_csv,\n",
    "    output_dir=output_dir,\n",
    "    ari_values=ari_values,\n",
    "    position_percent=position_percent,\n",
    "    total_duration=total_duration\n",
    ")\n",
    "\n",
    "# Plot the hyetographs\n",
    "plot = plot_multiple_hyetographs(\n",
    "    aris=ari_values,\n",
    "    position_percent=position_percent,\n",
    "    total_duration=total_duration,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Download and Prepare the Davis HEC-RAS Project\n",
    "\n",
    "Now, let's download the Davis project using RasExamples.extract_project() and prepare it for our analysis. \n",
    "We'll then create a new folder for our AEP analysis to keep the original project intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davis_path = RasExamples.extract_project(\"Davis\")\n",
    "\n",
    "# Create a new project folder for our AEP analysis\n",
    "def create_aep_project_folder(original_project_folder, new_project_name):\n",
    "    \"\"\"\n",
    "    Creates a new project folder for our AEP analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_project_folder: Path to the original project folder\n",
    "    - new_project_name: Name for the new project folder\n",
    "    \n",
    "    Returns:\n",
    "    - Path to the new project folder\n",
    "    \"\"\"\n",
    "    # Create path for the new project folder\n",
    "    new_project_folder = original_project_folder.parent / new_project_name\n",
    "    \n",
    "    # Remove the new project folder if it already exists\n",
    "    if new_project_folder.exists():\n",
    "        logging.info(f\"Removing existing folder: {new_project_folder}\")\n",
    "        shutil.rmtree(new_project_folder)\n",
    "    \n",
    "    # Copy the original project to the new folder\n",
    "    logging.info(f\"Copying project from {original_project_folder} to {new_project_folder}\")\n",
    "    shutil.copytree(original_project_folder, new_project_folder)\n",
    "    \n",
    "    logging.info(f\"Created new project folder: {new_project_folder}\")\n",
    "    return new_project_folder\n",
    "\n",
    "# Initialize the RAS project\n",
    "def initialize_ras_project(project_folder, ras_version=\"6.6\"):\n",
    "    \"\"\"\n",
    "    Initializes the RAS project and returns the RAS object.\n",
    "    \n",
    "    Parameters:\n",
    "    - project_folder: Path to the project folder\n",
    "    - ras_version: HEC-RAS version (default: \"6.6\")\n",
    "    \n",
    "    Returns:\n",
    "    - Initialized RAS object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ras_object = init_ras_project(project_folder, ras_version)\n",
    "        logging.info(f\"Initialized RAS project: {ras_object.project_name}\")\n",
    "        return ras_object\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing RAS project: {e}\")\n",
    "        raise\n",
    "\n",
    "# Download the Davis project\n",
    "davis_path = download_davis_project()\n",
    "\n",
    "# Create a new project folder for our AEP analysis\n",
    "aep_project_name = \"Davis_AEP_Analysis\"\n",
    "aep_project_folder = create_aep_project_folder(davis_path, aep_project_name)\n",
    "\n",
    "# Initialize the RAS project\n",
    "ras_object = initialize_ras_project(aep_project_folder)\n",
    "\n",
    "# Display project information\n",
    "print(\"\\nHEC-RAS Project Information:\")\n",
    "print(f\"Project Name: {ras_object.project_name}\")\n",
    "print(f\"Project Folder: {ras_object.project_folder}\")\n",
    "\n",
    "# Display available plans\n",
    "print(\"\\nAvailable Plans:\")\n",
    "display.display(ras_object.plan_df)\n",
    "\n",
    "# Display available unsteady flow files\n",
    "print(\"\\nAvailable Unsteady Flow Files:\")\n",
    "display.display(ras_object.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Clone Plans and Unsteady Flow Files for Each AEP Event\n",
    "\n",
    "Now, let's clone Plan 02 for each AEP event and update the plan and unsteady flow files with the appropriate hyetograph data. This will allow us to simulate different AEP events with HEC-RAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template plan and unsteady flow file numbers\n",
    "template_plan = \"02\"  # Use plan 02 as the template\n",
    "template_unsteady = \"01\"  # Use unsteady file 01 as the template\n",
    "\n",
    "# Function to create plans and unsteady flow files for each AEP event\n",
    "def create_aep_plans_and_unsteady_files(ras_object, template_plan, template_unsteady, ari_values, hyetograph_files):\n",
    "    \"\"\"\n",
    "    Creates plans and unsteady flow files for each AEP event.\n",
    "    \n",
    "    Parameters:\n",
    "    - ras_object: Initialized RAS object\n",
    "    - template_plan: Plan number to use as template\n",
    "    - template_unsteady: Unsteady flow file number to use as template\n",
    "    - ari_values: List of ARI values to create plans for\n",
    "    - hyetograph_files: Dictionary mapping ARI values to hyetograph file paths\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping ARI values to plan numbers\n",
    "    - Dictionary mapping ARI values to unsteady flow file numbers\n",
    "    \"\"\"\n",
    "    project_name = ras_object.project_name\n",
    "    project_folder = ras_object.project_folder\n",
    "    prj_file = ras_object.prj_file\n",
    "    \n",
    "    new_plan_numbers = {}\n",
    "    new_unsteady_numbers = {}\n",
    "    \n",
    "    for ari in ari_values:\n",
    "        ari_str = str(ari)\n",
    "        if ari_str not in hyetograph_files:\n",
    "            logging.warning(f\"ARI {ari} does not have a hyetograph file. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Create new plan file by cloning template\n",
    "        new_plan_number = RasPlan.clone_plan(template_plan, f\"ARI_{ari}_years\", ras_object)\n",
    "        logging.info(f\"Created new plan: {new_plan_number} for ARI {ari} years\")\n",
    "        new_plan_numbers[ari_str] = new_plan_number\n",
    "        \n",
    "        # Update plan description\n",
    "        plan_description = f\"Annual Exceedance Probability (AEP) Event\\nAnnual Recurrence Interval (ARI): {ari} years\\nExceedance Probability: {100/float(ari):.2f}%\"\n",
    "        # Get plan file path\n",
    "        plan_file_path = RasPlan.get_plan_path(new_plan_number, ras_object)\n",
    "        \n",
    "        # Update the plan file directly since update_plan_value doesn't exist\n",
    "        def update_description(lines):\n",
    "            updated_lines = []\n",
    "            in_description = False\n",
    "            description_updated = False\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.strip().startswith(\"Description=\"):\n",
    "                    updated_lines.append(f\"Description={plan_description}\\n\")\n",
    "                    description_updated = True\n",
    "                    in_description = True\n",
    "                elif in_description and line.strip() and not line.strip().startswith(\"Description=\"):\n",
    "                    in_description = False\n",
    "                    if not description_updated:\n",
    "                        updated_lines.append(line)\n",
    "                else:\n",
    "                    updated_lines.append(line)\n",
    "                    \n",
    "            return updated_lines\n",
    "        \n",
    "        # Use RasUtils to update the file\n",
    "        RasUtils.update_file(plan_file_path, update_description)\n",
    "        logging.info(f\"Updated plan description for ARI {ari} years\")\n",
    "        \n",
    "        # Create new unsteady flow file by cloning template\n",
    "        new_unsteady_number = RasPlan.clone_unsteady(template_unsteady, ras_object)\n",
    "        logging.info(f\"Created new unsteady flow file: {new_unsteady_number} for ARI {ari} years\")\n",
    "        new_unsteady_numbers[ari_str] = new_unsteady_number\n",
    "        \n",
    "        # Update unsteady flow file with hyetograph data\n",
    "        unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object)\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        hyetograph_file = hyetograph_files[ari_str]\n",
    "        hyetograph_data = pd.read_csv(hyetograph_file)\n",
    "        \n",
    "        # Update flow title in unsteady file\n",
    "        flow_title = f\"ARI_{ari}_years\"\n",
    "        RasUnsteady.update_flow_title(unsteady_file_path, flow_title, ras_object)\n",
    "        logging.info(f\"Updated flow title for ARI {ari} years\")\n",
    "        \n",
    "        # Apply the new unsteady file to the new plan\n",
    "        RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object)\n",
    "        logging.info(f\"Applied unsteady flow file {new_unsteady_number} to plan {new_plan_number}\")\n",
    "    \n",
    "    logging.info(f\"Created {len(new_plan_numbers)} plans and {len(new_unsteady_numbers)} unsteady flow files.\")\n",
    "    return new_plan_numbers, new_unsteady_numbers\n",
    "\n",
    "# Create plans and unsteady flow files for each AEP event\n",
    "new_plan_numbers, new_unsteady_numbers = create_aep_plans_and_unsteady_files(\n",
    "    ras_object=ras_object,\n",
    "    template_plan=template_plan,\n",
    "    template_unsteady=template_unsteady,\n",
    "    ari_values=ari_values,\n",
    "    hyetograph_files=hyetograph_files\n",
    ")\n",
    "\n",
    "# Display the new plans and unsteady flow files\n",
    "print(\"\\nNew Plans for AEP Events:\")\n",
    "for ari, plan_number in new_plan_numbers.items():\n",
    "    print(f\"ARI {ari} years: Plan {plan_number}\")\n",
    "\n",
    "print(\"\\nNew Unsteady Flow Files for AEP Events:\")\n",
    "for ari, unsteady_number in new_unsteady_numbers.items():\n",
    "    print(f\"ARI {ari} years: Unsteady Flow {unsteady_number}\")\n",
    "\n",
    "# Refresh the RAS object to see the new plans and unsteady flow files\n",
    "ras_object = initialize_ras_project(aep_project_folder)\n",
    "\n",
    "# Display all plans\n",
    "print(\"\\nAll Plans:\")\n",
    "display.display(ras_object.plan_df)\n",
    "\n",
    "# Display all unsteady flow files\n",
    "print(\"\\nAll Unsteady Flow Files:\")\n",
    "display.display(ras_object.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Execute All Plans in Parallel\n",
    "\n",
    "Now, let's execute all the plans we created in parallel using the RasCmdr.compute_parallel() function.\n",
    "This will allow us to efficiently run multiple HEC-RAS simulations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the optimal number of workers based on system resources\n",
    "def get_optimal_worker_count(cores_per_worker=2):\n",
    "    \"\"\"\n",
    "    Calculate the optimal number of workers based on available physical cores.\n",
    "    \n",
    "    Parameters:\n",
    "    - cores_per_worker: Number of cores to allocate to each worker (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal number of workers\n",
    "    \"\"\"\n",
    "    # Get physical CPU cores\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    if physical_cores is None:\n",
    "        physical_cores = psutil.cpu_count(logical=True) // 2  # Fallback estimate\n",
    "    \n",
    "    # Calculate optimal workers based on physical cores\n",
    "    optimal_workers = physical_cores // cores_per_worker\n",
    "    \n",
    "    # Ensure at least 1 worker\n",
    "    return max(1, optimal_workers)\n",
    "\n",
    "# Execute plans in parallel\n",
    "def execute_plans_in_parallel(ras_object, plan_numbers, compute_folder, cores_per_worker=2):\n",
    "    \"\"\"\n",
    "    Executes multiple HEC-RAS plans in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    - ras_object: Initialized RAS object\n",
    "    - plan_numbers: List of plan numbers to execute\n",
    "    - compute_folder: Folder to store computation results\n",
    "    - cores_per_worker: Number of cores to allocate to each worker (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of execution results\n",
    "    \"\"\"\n",
    "    # Check system resources\n",
    "    cpu_count = psutil.cpu_count(logical=True)\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    \n",
    "    logging.info(f\"System Resources:\")\n",
    "    logging.info(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores)\")\n",
    "    logging.info(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
    "    \n",
    "    # Calculate optimal number of workers\n",
    "    max_workers = get_optimal_worker_count(cores_per_worker)\n",
    "    logging.info(f\"Using {max_workers} workers with {cores_per_worker} cores per worker\")\n",
    "    \n",
    "    # Create compute folder if it doesn't exist\n",
    "    compute_folder = Path(compute_folder)\n",
    "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Compute folder: {compute_folder}\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute plans in parallel\n",
    "    logging.info(f\"Executing {len(plan_numbers)} plans in parallel...\")\n",
    "    results = RasCmdr.compute_parallel(\n",
    "        plan_number=plan_numbers,\n",
    "        max_workers=max_workers,\n",
    "        num_cores=cores_per_worker,\n",
    "        dest_folder=compute_folder,\n",
    "        overwrite_dest=True,\n",
    "        ras_object=ras_object\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    total_duration = end_time - start_time\n",
    "    \n",
    "    logging.info(f\"Parallel execution completed in {total_duration:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create compute folder\n",
    "compute_folder = aep_project_folder.parent / \"Davis_AEP_Compute\"\n",
    "\n",
    "# Get plan numbers to execute\n",
    "plan_numbers_to_execute = list(new_plan_numbers.values())\n",
    "print(f\"Executing {len(plan_numbers_to_execute)} plans: {plan_numbers_to_execute}\")\n",
    "\n",
    "# Execute plans in parallel\n",
    "execution_results = execute_plans_in_parallel(\n",
    "    ras_object=ras_object,\n",
    "    plan_numbers=plan_numbers_to_execute,\n",
    "    compute_folder=compute_folder,\n",
    "    cores_per_worker=2\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success, \"ARI\": next((ari for ari, p in new_plan_numbers.items() if p == plan), None)}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Sort by ARI\n",
    "results_df = results_df.sort_values(\"ARI\", key=lambda x: x.astype(float))\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)\n",
    "\n",
    "# Initialize a RAS project in the compute folder\n",
    "compute_ras_object = initialize_ras_project(compute_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Extract and Visualize Results\n",
    "\n",
    "Finally, let's extract and visualize the results from the HEC-RAS simulations. We'll use the ras-commander library to extract water surface elevation (WSEL) data for a specific cell in the 2D mesh and create comparison plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated an end-to-end workflow for performing flood analysis with different Annual Exceedance Probability (AEP) events using HEC-RAS and the ras-commander library. The key steps were:\n",
    "\n",
    "1. **Generate Hyetographs**: Created precipitation hyetographs for different ARI values using NOAA Atlas 14 data\n",
    "2. **Download and Prepare the HEC-RAS Project**: Downloaded the Davis project and created a new project folder for our analysis\n",
    "3. **Clone and Configure Plans**: Created new plans and unsteady flow files for each AEP event and updated them with the appropriate hyetographs\n",
    "4. **Execute Plans in Parallel**: Used parallel processing to efficiently run multiple HEC-RAS simulations simultaneously\n",
    "5. **Extract and Visualize Results**: Extracted water surface elevation data for a specific cell and created comparison plots\n",
    "\n",
    "This workflow demonstrates how the ras-commander library can be used to automate HEC-RAS workflows, making it easier to perform complex analyses with multiple scenarios. By using parallel processing, we can significantly reduce the overall computation time required for multi-scenario analyses.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To build on this analysis, you could:\n",
    "- Extract results for multiple cells or cross-sections to analyze spatial patterns of flooding\n",
    "- Create flood extent maps for different AEP events\n",
    "- Perform sensitivity analysis by varying parameters such as roughness or infiltration\n",
    "- Compare results with observed flood data for model calibration\n",
    "- Create animated visualizations of the flooding process\n",
    "\n",
    "These extensions would further enhance the value of the analysis and provide more comprehensive insights into flood behavior and risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
