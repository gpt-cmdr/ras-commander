{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USGS Gauge Data Integration for HEC-RAS\n",
    "\n",
    "This notebook demonstrates how to integrate USGS gauge data with HEC-RAS models using the `ras_commander.usgs` submodule.\n",
    "\n",
    "**Workflow covered:**\n",
    "1. Discover USGS gauges within/near a HEC-RAS project extent\n",
    "2. Retrieve flow and stage time series from USGS\n",
    "3. Set initial conditions using observed data\n",
    "4. Create boundary conditions from historic flow data (next phase)\n",
    "5. Validate model results against observations (next phase)\n",
    "\n",
    "**Example Projects:**\n",
    "- Balde Eagle Creek (1D model)\n",
    "- BaldEagleCrkMulti2D (1D/2D integrated model)\n",
    "\n",
    "**Target Event:**\n",
    "- Tropical Storm Lee (September 6-12, 2011) - Major flooding event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Development Mode, add the parent directory to the Python path\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current_file = Path(os.getcwd()).resolve()\n",
    "rascmdr_directory = current_file.parent\n",
    "\n",
    "# Use insert(0) instead of append() to give highest priority to local version\n",
    "if str(rascmdr_directory) not in sys.path:\n",
    "    sys.path.insert(0, str(rascmdr_directory))\n",
    "\n",
    "print(\"Loading ras-commander from local dev copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# ras-commander core imports\n",
    "from ras_commander import (\n",
    "    init_ras_project, ras, RasCmdr, RasPlan, RasExamples, RasPrj\n",
    ")\n",
    "from ras_commander.hdf import HdfProject\n",
    "\n",
    "print(\"Core ras-commander modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check USGS module dependencies\n",
    "from ras_commander.usgs import check_dependencies\n",
    "\n",
    "deps = check_dependencies()\n",
    "print(\"USGS Module Dependencies:\")\n",
    "for dep, available in deps.items():\n",
    "    status = \"Installed\" if available else \"NOT INSTALLED\"\n",
    "    print(f\"  {dep}: {status}\")\n",
    "\n",
    "if not deps['dataretrieval']:\n",
    "    print(\"\\n*** WARNING: dataretrieval is required for USGS data retrieval ***\")\n",
    "    print(\"Install with: pip install dataretrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dataretrieval if not available (uncomment to install)\n",
    "# !pip install dataretrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Example Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example projects\n",
    "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "bald_eagle_1d_path = extracted_paths[0]\n",
    "bald_eagle_2d_path = extracted_paths[1]\n",
    "\n",
    "print(f\"1D Model path: {bald_eagle_1d_path}\")\n",
    "print(f\"2D Model path: {bald_eagle_2d_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 1D project\n",
    "init_ras_project(bald_eagle_1d_path, \"6.5\")\n",
    "\n",
    "print(f\"Project: {ras.project_name}\")\n",
    "print(f\"Plans: {len(ras.plan_df)}\")\n",
    "print(f\"Geometries: {len(ras.geom_df)}\")\n",
    "print(f\"Unsteady files: {len(ras.unsteady_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View plan information\n",
    "ras.plan_df[['plan_number', 'Plan Title', 'Simulation Date', 'Computation Interval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View boundary conditions\n",
    "ras.boundaries_df[['river_reach_name', 'river_station', 'bc_type', 'Interval']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover USGS Gauges in Project Area\n",
    "\n",
    "We'll use the project's geographic bounds to find nearby USGS stream gauges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the geometry HDF file to get project bounds\n",
    "geom_hdf_files = list(bald_eagle_1d_path.glob(\"*.g*.hdf\"))\n",
    "print(f\"Found geometry HDF files: {geom_hdf_files}\")\n",
    "\n",
    "if geom_hdf_files:\n",
    "    geom_hdf = geom_hdf_files[0]\n",
    "    print(f\"Using: {geom_hdf.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get project bounds in lat/lon (WGS84)\n# Note: The Bald Eagle Creek example project doesn't have a CRS defined in the geometry HDF,\n# so we use known approximate bounds for the Lock Haven, PA area.\n\ntry:\n    bounds = HdfProject.get_project_bounds_latlon(geom_hdf, buffer_percent=50)\n    west, south, east, north = bounds\n    \n    # Check if bounds look like projected coordinates (large numbers) vs lat/lon\n    if abs(west) > 180 or abs(east) > 180:\n        raise ValueError(\"Bounds appear to be in projected coordinates, not lat/lon\")\n    \n    print(f\"Project Bounds (WGS84):\")\n    print(f\"  West:  {west:.6f}\")\n    print(f\"  South: {south:.6f}\")\n    print(f\"  East:  {east:.6f}\")\n    print(f\"  North: {north:.6f}\")\nexcept Exception as e:\n    print(f\"Note: {e}\")\n    # Use known approximate bounds for Bald Eagle Creek area (Lock Haven, PA)\n    west, south, east, north = -77.60, 40.90, -77.30, 41.15\n    print(f\"\\nUsing known bounds for Lock Haven, PA / Bald Eagle Creek area:\")\n    print(f\"  West:  {west:.6f}\")\n    print(f\"  South: {south:.6f}\")\n    print(f\"  East:  {east:.6f}\")\n    print(f\"  North: {north:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query USGS for stream gauges in the project area\n",
    "from dataretrieval import waterdata\n",
    "\n",
    "# Query monitoring locations in the bounding box\n",
    "gauges_df, metadata = waterdata.get_monitoring_locations(\n",
    "    bbox=[west, south, east, north],\n",
    "    site_type_code='ST'  # Stream sites only\n",
    ")\n",
    "\n",
    "print(f\"Found {len(gauges_df)} USGS stream gauges in the project area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display gauge information\n",
    "if not gauges_df.empty:\n",
    "    # Select relevant columns\n",
    "    display_cols = ['monitoring_location_id', 'monitoring_location_name']\n",
    "    if 'drain_area_va' in gauges_df.columns:\n",
    "        display_cols.append('drain_area_va')\n",
    "    \n",
    "    print(\"Available USGS Stream Gauges:\")\n",
    "    display(gauges_df[display_cols])\n",
    "else:\n",
    "    print(\"No gauges found in the project bounds.\")\n",
    "    print(\"Let's use the known gauges for Bald Eagle Creek:\")\n",
    "    print(\"  USGS-01547200: Bald Eagle Creek below Spring Creek at Milesburg, PA\")\n",
    "    print(\"  USGS-01548005: Bald Eagle Creek near Beech Creek Station, PA\")\n",
    "    print(\"  USGS-01548010: Bald Eagle Creek near Mill Hall, PA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Known Gauges for Bald Eagle Creek\n",
    "\n",
    "Based on watershed research, these are the key USGS gauges for Bald Eagle Creek:\n",
    "\n",
    "| Site ID | Name | Location | Best Use |\n",
    "|---------|------|----------|----------|\n",
    "| **01547200** | Bald Eagle Creek below Spring Creek at Milesburg, PA | Upstream | Upstream BC |\n",
    "| **01548005** | Bald Eagle Creek near Beech Creek Station, PA | Mid-reach | IC Point |\n",
    "| **01548010** | Bald Eagle Creek near Mill Hall, PA | Near Lock Haven | Downstream validation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the key gauges for our analysis\n",
    "target_gauges = {\n",
    "    'upstream': {\n",
    "        'site_id': '01547200',\n",
    "        'name': 'Bald Eagle Creek below Spring Creek at Milesburg, PA',\n",
    "        'use': 'Upstream boundary condition'\n",
    "    },\n",
    "    'midreach': {\n",
    "        'site_id': '01548005',\n",
    "        'name': 'Bald Eagle Creek near Beech Creek Station, PA',\n",
    "        'use': 'Initial condition / validation'\n",
    "    },\n",
    "    'downstream': {\n",
    "        'site_id': '01548010',\n",
    "        'name': 'Bald Eagle Creek near Mill Hall, PA',\n",
    "        'use': 'Downstream validation'\n",
    "    }\n",
    "}\n",
    "\n",
    "for location, info in target_gauges.items():\n",
    "    print(f\"{location.upper()}:\")\n",
    "    print(f\"  Site: USGS-{info['site_id']}\")\n",
    "    print(f\"  Name: {info['name']}\")\n",
    "    print(f\"  Use: {info['use']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Retrieve USGS Flow Data\n\nWe'll retrieve flow data for Tropical Storm Lee (September 2011), which caused major flooding in the region.\n\n**Note on data availability:** USGS instantaneous (15-min) data may not be available for historic events. \nIn such cases, we'll use daily values for historic analysis, or recent instantaneous data to demonstrate \nthe workflow for operational use."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the historic event\n",
    "event_name = \"Tropical_Storm_Lee_2011\"\n",
    "event_start = datetime(2011, 9, 5, 0, 0, 0)\n",
    "event_end = datetime(2011, 9, 13, 0, 0, 0)\n",
    "\n",
    "print(f\"Event: {event_name}\")\n",
    "print(f\"Period: {event_start} to {event_end}\")\n",
    "print(f\"Duration: {(event_end - event_start).days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data availability for the upstream gauge\n",
    "upstream_site = target_gauges['upstream']['site_id']\n",
    "\n",
    "# Format time range for dataretrieval\n",
    "time_range = f\"{event_start.strftime('%Y-%m-%d')}/{event_end.strftime('%Y-%m-%d')}\"\n",
    "print(f\"Checking data availability for USGS-{upstream_site}\")\n",
    "print(f\"Time range: {time_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve instantaneous flow data from upstream gauge\nfrom dataretrieval import nwis\n\nupstream_flow_df, upstream_metadata = nwis.get_iv(\n    sites=upstream_site,\n    parameterCd='00060',  # Discharge\n    start=event_start.strftime('%Y-%m-%d'),\n    end=event_end.strftime('%Y-%m-%d')\n)\n\n# Check if instantaneous data is available\nif len(upstream_flow_df) == 0:\n    print(\"No instantaneous (15-min) data available for this period.\")\n    print(\"Retrieving daily values instead...\")\n    \n    # Fall back to daily values\n    upstream_flow_df, upstream_metadata = nwis.get_dv(\n        sites=upstream_site,\n        parameterCd='00060',\n        start=event_start.strftime('%Y-%m-%d'),\n        end=event_end.strftime('%Y-%m-%d')\n    )\n    data_type = \"daily\"\n    flow_col = [c for c in upstream_flow_df.columns if '00060' in c][0]\nelse:\n    data_type = \"instantaneous\"\n    flow_col = [c for c in upstream_flow_df.columns if '00060' in c][0]\n\nprint(f\"Retrieved {len(upstream_flow_df)} {data_type} observations from upstream gauge\")\nif not upstream_flow_df.empty:\n    print(f\"Time range: {upstream_flow_df.index.min()} to {upstream_flow_df.index.max()}\")\n    print(f\"Peak flow: {upstream_flow_df[flow_col].max():.0f} cfs\")\n    print(f\"Min flow: {upstream_flow_df[flow_col].min():.0f} cfs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data structure\n",
    "upstream_flow_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the flow values into a clean DataFrame\n",
    "flow_col = [c for c in upstream_flow_df.columns if '00060' in c][0]\n",
    "\n",
    "# Create standardized DataFrame\n",
    "upstream_flow = pd.DataFrame({\n",
    "    'datetime': upstream_flow_df.index,\n",
    "    'value': upstream_flow_df[flow_col].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Remove NaN values\n",
    "upstream_flow = upstream_flow.dropna(subset=['value'])\n",
    "\n",
    "print(f\"Clean flow data: {len(upstream_flow)} records\")\n",
    "upstream_flow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the upstream flow hydrograph\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.plot(upstream_flow['datetime'], upstream_flow['value'], 'b-', linewidth=1)\n",
    "ax.fill_between(upstream_flow['datetime'], upstream_flow['value'], alpha=0.3)\n",
    "\n",
    "# Find and mark peak\n",
    "peak_idx = upstream_flow['value'].idxmax()\n",
    "peak_flow = upstream_flow.loc[peak_idx, 'value']\n",
    "peak_time = upstream_flow.loc[peak_idx, 'datetime']\n",
    "ax.axhline(y=peak_flow, color='r', linestyle='--', alpha=0.5)\n",
    "ax.annotate(f'Peak: {peak_flow:,.0f} cfs\\n{peak_time}', \n",
    "            xy=(peak_time, peak_flow),\n",
    "            xytext=(10, 10), textcoords='offset points',\n",
    "            fontsize=9, color='red')\n",
    "\n",
    "ax.set_xlabel('Date/Time')\n",
    "ax.set_ylabel('Flow (cfs)')\n",
    "ax.set_title(f'USGS-{upstream_site}: {target_gauges[\"upstream\"][\"name\"]}\\n{event_name}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis dates\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now retrieve data from the downstream gauge for validation\ndownstream_site = target_gauges['downstream']['site_id']\n\n# Try instantaneous first, fall back to daily\ndownstream_flow_df, downstream_metadata = nwis.get_iv(\n    sites=downstream_site,\n    parameterCd='00060',\n    start=event_start.strftime('%Y-%m-%d'),\n    end=event_end.strftime('%Y-%m-%d')\n)\n\nif len(downstream_flow_df) == 0:\n    print(\"No instantaneous data for downstream gauge. Trying daily values...\")\n    downstream_flow_df, downstream_metadata = nwis.get_dv(\n        sites=downstream_site,\n        parameterCd='00060',\n        start=event_start.strftime('%Y-%m-%d'),\n        end=event_end.strftime('%Y-%m-%d')\n    )\n    ds_data_type = \"daily\"\nelse:\n    ds_data_type = \"instantaneous\"\n\nprint(f\"Retrieved {len(downstream_flow_df)} {ds_data_type} observations from downstream gauge\")\n\nif not downstream_flow_df.empty:\n    flow_col_ds = [c for c in downstream_flow_df.columns if '00060' in c][0]\n    \n    downstream_flow = pd.DataFrame({\n        'datetime': downstream_flow_df.index,\n        'value': downstream_flow_df[flow_col_ds].values\n    }).reset_index(drop=True)\n    downstream_flow = downstream_flow.dropna(subset=['value'])\n    \n    print(f\"Clean flow data: {len(downstream_flow)} records\")\n    print(f\"Peak flow: {downstream_flow['value'].max():.0f} cfs\")\nelse:\n    print(\"No downstream flow data available for this period\")\n    downstream_flow = pd.DataFrame(columns=['datetime', 'value'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot both gauges for comparison (if data available)\nif not upstream_flow.empty and not downstream_flow.empty:\n    fig, ax = plt.subplots(figsize=(14, 6))\n\n    ax.plot(upstream_flow['datetime'], upstream_flow['value'], 'b-o' if data_type == 'daily' else 'b-', \n            linewidth=1.5, markersize=6, label=f'Upstream (USGS-{upstream_site})')\n    ax.plot(downstream_flow['datetime'], downstream_flow['value'], 'r-o' if ds_data_type == 'daily' else 'r-', \n            linewidth=1.5, markersize=6, label=f'Downstream (USGS-{downstream_site})')\n\n    ax.set_xlabel('Date/Time', fontsize=11)\n    ax.set_ylabel('Flow (cfs)', fontsize=11)\n    ax.set_title(f'Bald Eagle Creek Flow - {event_name}\\n({data_type} data)', fontsize=12)\n    ax.legend(loc='upper right')\n    ax.grid(True, alpha=0.3)\n\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n    ax.xaxis.set_major_locator(mdates.DayLocator())\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show()\nelif not upstream_flow.empty:\n    fig, ax = plt.subplots(figsize=(14, 6))\n    ax.plot(upstream_flow['datetime'], upstream_flow['value'], 'b-o' if data_type == 'daily' else 'b-', \n            linewidth=1.5, markersize=6, label=f'Upstream (USGS-{upstream_site})')\n    ax.set_xlabel('Date/Time', fontsize=11)\n    ax.set_ylabel('Flow (cfs)', fontsize=11)\n    ax.set_title(f'Bald Eagle Creek Flow - {event_name}\\n({data_type} data - upstream only)', fontsize=12)\n    ax.legend(loc='upper right')\n    ax.grid(True, alpha=0.3)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No flow data available for plotting\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve Stage Data\n",
    "\n",
    "Stage (gage height) data is useful for setting initial water surface elevations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve stage data from upstream gauge\n# Try instantaneous first, fall back to daily\nupstream_stage_df, stage_metadata = nwis.get_iv(\n    sites=upstream_site,\n    parameterCd='00065',  # Gage height\n    start=event_start.strftime('%Y-%m-%d'),\n    end=event_end.strftime('%Y-%m-%d')\n)\n\nif len(upstream_stage_df) == 0:\n    print(\"No instantaneous stage data. Trying daily values...\")\n    upstream_stage_df, stage_metadata = nwis.get_dv(\n        sites=upstream_site,\n        parameterCd='00065',\n        start=event_start.strftime('%Y-%m-%d'),\n        end=event_end.strftime('%Y-%m-%d')\n    )\n    stage_data_type = \"daily\"\nelse:\n    stage_data_type = \"instantaneous\"\n\nprint(f\"Retrieved {len(upstream_stage_df)} {stage_data_type} stage observations\")\n\nif not upstream_stage_df.empty:\n    stage_col = [c for c in upstream_stage_df.columns if '00065' in c][0]\n    \n    upstream_stage = pd.DataFrame({\n        'datetime': upstream_stage_df.index,\n        'value': upstream_stage_df[stage_col].values\n    }).reset_index(drop=True)\n    upstream_stage = upstream_stage.dropna(subset=['value'])\n    \n    print(f\"Clean stage data: {len(upstream_stage)} records\")\n    print(f\"Stage range: {upstream_stage['value'].min():.2f} to {upstream_stage['value'].max():.2f} ft\")\nelse:\n    print(\"No stage data available\")\n    upstream_stage = pd.DataFrame(columns=['datetime', 'value'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot flow and stage together (if both available)\nif not upstream_flow.empty and not upstream_stage.empty:\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\n    # Flow plot\n    marker = 'o' if data_type == 'daily' else ''\n    ax1.plot(upstream_flow['datetime'], upstream_flow['value'], f'b-{marker}', linewidth=1.5, markersize=6)\n    ax1.set_ylabel('Flow (cfs)', fontsize=11, color='blue')\n    ax1.tick_params(axis='y', labelcolor='blue')\n    ax1.grid(True, alpha=0.3)\n    ax1.set_title(f'USGS-{upstream_site}: Flow and Stage - {event_name}\\n({data_type} data)', fontsize=12)\n\n    # Stage plot\n    stage_marker = 'o' if stage_data_type == 'daily' else ''\n    ax2.plot(upstream_stage['datetime'], upstream_stage['value'], f'g-{stage_marker}', linewidth=1.5, markersize=6)\n    ax2.set_ylabel('Stage (ft)', fontsize=11, color='green')\n    ax2.tick_params(axis='y', labelcolor='green')\n    ax2.set_xlabel('Date/Time', fontsize=11)\n    ax2.grid(True, alpha=0.3)\n\n    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n    ax2.xaxis.set_major_locator(mdates.DayLocator())\n\n    plt.tight_layout()\n    plt.show()\nelif not upstream_flow.empty:\n    fig, ax = plt.subplots(figsize=(14, 5))\n    ax.plot(upstream_flow['datetime'], upstream_flow['value'], 'b-o' if data_type == 'daily' else 'b-', linewidth=1.5)\n    ax.set_ylabel('Flow (cfs)', fontsize=11)\n    ax.set_xlabel('Date/Time', fontsize=11)\n    ax.set_title(f'USGS-{upstream_site}: Flow - {event_name} (stage not available)', fontsize=12)\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No data available for plotting\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Conditions from USGS Data\n",
    "\n",
    "Now we'll use the USGS gauge data to set initial conditions for a HEC-RAS simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation start time (when we want initial conditions)\n",
    "simulation_start = datetime(2011, 9, 6, 0, 0, 0)  # Start at midnight on Sept 6\n",
    "\n",
    "print(f\"Simulation start time: {simulation_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the flow value closest to simulation start\nif not upstream_flow.empty:\n    # Convert simulation_start to timezone-aware if needed\n    upstream_flow['datetime'] = pd.to_datetime(upstream_flow['datetime'])\n\n    # Make simulation_start timezone-aware (UTC) to match USGS data\n    sim_start_utc = pd.Timestamp(simulation_start, tz='UTC')\n\n    # Calculate time differences\n    time_diffs = abs(upstream_flow['datetime'] - sim_start_utc)\n    nearest_idx = time_diffs.idxmin()\n\n    initial_flow = upstream_flow.loc[nearest_idx, 'value']\n    initial_time = upstream_flow.loc[nearest_idx, 'datetime']\n\n    print(f\"Initial condition for upstream gauge:\")\n    print(f\"  Target time: {simulation_start}\")\n    print(f\"  Nearest observation: {initial_time}\")\n    print(f\"  Time offset: {time_diffs[nearest_idx]}\")\n    print(f\"  Initial flow: {initial_flow:.0f} cfs\")\nelse:\n    print(\"No flow data available to determine initial conditions\")\n    initial_flow = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Also get initial stage\nif not upstream_stage.empty:\n    upstream_stage['datetime'] = pd.to_datetime(upstream_stage['datetime'])\n\n    stage_diffs = abs(upstream_stage['datetime'] - sim_start_utc)\n    nearest_stage_idx = stage_diffs.idxmin()\n\n    initial_stage = upstream_stage.loc[nearest_stage_idx, 'value']\n    initial_stage_time = upstream_stage.loc[nearest_stage_idx, 'datetime']\n\n    print(f\"Initial stage: {initial_stage:.2f} ft at {initial_stage_time}\")\nelse:\n    print(\"No stage data available for initial conditions\")\n    initial_stage = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Parse Existing Initial Conditions\n",
    "\n",
    "Let's look at the existing initial conditions in the model's unsteady flow file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the Multi2D project which has more IC examples\n",
    "multi_2d_project = RasPrj()\n",
    "init_ras_project(bald_eagle_2d_path, \"6.5\", ras_object=multi_2d_project)\n",
    "\n",
    "print(f\"Project: {multi_2d_project.project_name}\")\n",
    "print(f\"Unsteady files: {len(multi_2d_project.unsteady_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available unsteady files\n",
    "multi_2d_project.unsteady_df[['unsteady_number', 'Flow Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse initial conditions from an unsteady file\n",
    "from ras_commander.usgs import InitialConditions\n",
    "\n",
    "# Find an unsteady file to parse (u07 has IC examples based on the plan doc)\n",
    "unsteady_file = bald_eagle_2d_path / \"BaldEagleDamBrk.u07\"\n",
    "\n",
    "if unsteady_file.exists():\n",
    "    ic_df = InitialConditions.parse_initial_conditions(unsteady_file)\n",
    "    print(f\"Found {len(ic_df)} initial conditions in {unsteady_file.name}\")\n",
    "    display(ic_df)\n",
    "else:\n",
    "    print(f\"Unsteady file not found: {unsteady_file}\")\n",
    "    # List available files\n",
    "    print(\"Available unsteady files:\")\n",
    "    for uf in bald_eagle_2d_path.glob(\"*.u*\"):\n",
    "        if uf.suffix.startswith('.u') and not uf.suffix.endswith('.hdf'):\n",
    "            print(f\"  {uf.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try parsing from u01 which is more common\n",
    "unsteady_file_u01 = bald_eagle_2d_path / \"BaldEagleDamBrk.u01\"\n",
    "\n",
    "if unsteady_file_u01.exists():\n",
    "    ic_df = InitialConditions.parse_initial_conditions(unsteady_file_u01)\n",
    "    print(f\"Found {len(ic_df)} initial conditions in {unsteady_file_u01.name}\")\n",
    "    if not ic_df.empty:\n",
    "        display(ic_df)\n",
    "    else:\n",
    "        print(\"No initial conditions defined in this file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Create Initial Condition Lines from USGS Data\n",
    "\n",
    "Now we'll create IC lines using the USGS data we retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the gauge-to-model mapping for this project\n# Based on example_notebook_plan.md\nif initial_flow is not None:\n    gauge_model_mapping = [\n        {\n            'gauge_id': 'USGS-01547200',\n            'gauge_name': 'Milesburg',\n            'river': 'Bald Eagle Cr.',\n            'reach': 'Lock Haven',\n            'station': 137520,\n            'usgs_flow': initial_flow,\n            'ic_type': 'flow'\n        }\n    ]\n\n    print(\"Gauge-to-Model Mapping:\")\n    for mapping in gauge_model_mapping:\n        print(f\"  {mapping['gauge_id']} ({mapping['gauge_name']})\")\n        print(f\"    -> {mapping['river']}/{mapping['reach']}/Station {mapping['station']}\")\n        print(f\"    Flow: {mapping['usgs_flow']:.0f} cfs\")\nelse:\n    gauge_model_mapping = []\n    print(\"Cannot create gauge mapping - no initial flow data available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create IC line using the InitialConditions class\nif gauge_model_mapping:\n    for mapping in gauge_model_mapping:\n        ic_line = InitialConditions.create_ic_line(\n            ic_type=mapping['ic_type'],\n            river=mapping['river'],\n            reach=mapping['reach'],\n            station=mapping['station'],\n            value=mapping['usgs_flow']\n        )\n        print(f\"Generated IC line:\")\n        print(f\"  {ic_line}\")\nelse:\n    print(\"No gauge mapping available to create IC lines\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Data Cache\n",
    "\n",
    "Let's summarize what we've retrieved and cache the data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gauge_data directory\n",
    "gauge_data_dir = bald_eagle_1d_path / 'gauge_data'\n",
    "gauge_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "raw_dir = gauge_data_dir / 'raw'\n",
    "raw_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Gauge data directory: {gauge_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the retrieved data (if available)\nstart_str = event_start.strftime('%Y%m%d')\nend_str = event_end.strftime('%Y%m%d')\n\nsaved_files = []\n\n# Save upstream flow\nif not upstream_flow.empty:\n    upstream_flow_file = raw_dir / f\"USGS-{upstream_site}_{start_str}_{end_str}_flow.csv\"\n    upstream_flow.to_csv(upstream_flow_file, index=False)\n    print(f\"Saved: {upstream_flow_file.name}\")\n    saved_files.append(upstream_flow_file)\n\n# Save upstream stage\nif not upstream_stage.empty:\n    upstream_stage_file = raw_dir / f\"USGS-{upstream_site}_{start_str}_{end_str}_stage.csv\"\n    upstream_stage.to_csv(upstream_stage_file, index=False)\n    print(f\"Saved: {upstream_stage_file.name}\")\n    saved_files.append(upstream_stage_file)\n\n# Save downstream flow\nif not downstream_flow.empty:\n    downstream_flow_file = raw_dir / f\"USGS-{downstream_site}_{start_str}_{end_str}_flow.csv\"\n    downstream_flow.to_csv(downstream_flow_file, index=False)\n    print(f\"Saved: {downstream_flow_file.name}\")\n    saved_files.append(downstream_flow_file)\n\nif not saved_files:\n    print(\"No data files were saved\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary\nprint(\"=\" * 70)\nprint(\"USGS GAUGE DATA INTEGRATION - SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\nEvent: {event_name}\")\nprint(f\"Period: {event_start} to {event_end}\")\nprint(f\"\\nProject: {ras.project_name}\")\nprint(f\"Location: {bald_eagle_1d_path}\")\nprint(f\"\\nUSGS Gauges Used:\")\n\nif not upstream_flow.empty:\n    stage_count = len(upstream_stage) if not upstream_stage.empty else 0\n    print(f\"  Upstream:   USGS-{upstream_site} ({len(upstream_flow)} flow obs, {stage_count} stage obs) [{data_type}]\")\n    print(f\"  Peak Flow:  {upstream_flow['value'].max():,.0f} cfs\")\nelse:\n    print(f\"  Upstream:   USGS-{upstream_site} - No data available\")\n\nif not downstream_flow.empty:\n    print(f\"  Downstream: USGS-{downstream_site} ({len(downstream_flow)} flow obs) [{ds_data_type}]\")\n    print(f\"  Peak Flow:  {downstream_flow['value'].max():,.0f} cfs\")\nelse:\n    print(f\"  Downstream: USGS-{downstream_site} - No data available\")\n\nif initial_flow is not None:\n    print(f\"\\nInitial Condition at {simulation_start}:\")\n    print(f\"  Flow:  {initial_flow:.0f} cfs\")\n    if initial_stage is not None:\n        print(f\"  Stage: {initial_stage:.2f} ft\")\n\nprint(f\"\\nData saved to: {gauge_data_dir}\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Plan Simulation Dates\n",
    "\n",
    "Let's check the existing plan simulation dates to understand how to align USGS data with the simulation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to 1D project\n",
    "init_ras_project(bald_eagle_1d_path, \"6.5\")\n",
    "\n",
    "# View plan simulation dates\n",
    "print(\"Plan Simulation Dates:\")\n",
    "ras.plan_df[['plan_number', 'Plan Title', 'Simulation Date', 'Computation Interval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the simulation date from the existing plan\n",
    "sim_date_str = ras.plan_df.iloc[0]['Simulation Date']\n",
    "print(f\"Current simulation date string: {sim_date_str}\")\n",
    "\n",
    "# HEC-RAS format: DDMONYYYY,HHMM,DDMONYYYY,HHMM\n",
    "# Example: 18FEB1999,0000,24FEB1999,0500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check boundary condition intervals\n",
    "print(\"\\nBoundary Condition Intervals:\")\n",
    "ras.boundaries_df[['river_reach_name', 'bc_type', 'Interval']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many values we need for our event period at different intervals\n",
    "event_duration = event_end - event_start\n",
    "duration_hours = event_duration.total_seconds() / 3600\n",
    "\n",
    "intervals = {\n",
    "    '15MIN': 15/60,\n",
    "    '30MIN': 30/60,\n",
    "    '1HOUR': 1,\n",
    "    '2HOUR': 2,\n",
    "    '6HOUR': 6\n",
    "}\n",
    "\n",
    "print(f\"Event duration: {duration_hours:.0f} hours ({event_duration.days} days)\")\n",
    "print(f\"\\nValues needed for each interval:\")\n",
    "for interval, hours in intervals.items():\n",
    "    num_values = int(duration_hours / hours) + 1\n",
    "    print(f\"  {interval}: {num_values} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check USGS data interval\n",
    "time_diffs = upstream_flow['datetime'].diff().dropna()\n",
    "median_interval = time_diffs.median()\n",
    "print(f\"USGS data median interval: {median_interval}\")\n",
    "print(f\"\\nWe have {len(upstream_flow)} observations over {duration_hours:.0f} hours\")\n",
    "print(f\"This is approximately {len(upstream_flow) / duration_hours:.1f} observations per hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next phase, we will:\n",
    "\n",
    "1. **Resample USGS data** to match HEC-RAS boundary condition interval (1HOUR)\n",
    "2. **Generate fixed-width flow hydrograph table** for the unsteady file\n",
    "3. **Update the boundary condition** in the .u## file\n",
    "4. **Update plan simulation dates** to match the historic event\n",
    "5. **Run the simulation** using RasCmdr.compute_plan()\n",
    "6. **Validate results** against downstream USGS gauge"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Professional QAQC Visualizations\n\nThe following figures are designed for engineering QAQC review and inclusion in engineering reports. They include:\n\n- **Data Quality Dashboard**: Comprehensive assessment of data completeness, gaps, and temporal consistency\n- **Event Hydrograph**: Publication-quality hydrograph with dual axes and professional annotations\n- **Stage-Discharge Rating Curve**: Rating curve analysis with power law fit\n- **Summary Statistics Dashboard**: Single-page comprehensive data summary for reports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# PROFESSIONAL QAQC VISUALIZATION SETUP\n# =============================================================================\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.patches import Patch\nfrom scipy import stats\nfrom scipy.optimize import curve_fit\n\n# Configure matplotlib for publication-quality figures\nplt.rcParams.update({\n    'figure.dpi': 100,\n    'savefig.dpi': 150,\n    'font.family': 'sans-serif',\n    'font.size': 10,\n    'axes.titlesize': 12,\n    'axes.titleweight': 'bold',\n    'axes.labelsize': 11,\n    'xtick.labelsize': 9,\n    'ytick.labelsize': 9,\n    'legend.fontsize': 9,\n    'figure.titlesize': 14,\n    'axes.grid': True,\n    'grid.alpha': 0.3,\n    'grid.linestyle': '--',\n})\n\n# Define consistent color palette for QAQC figures\nCOLORS = {\n    'flow': '#1f77b4',        # Blue\n    'stage': '#2ca02c',       # Green  \n    'upstream': '#1f77b4',    # Blue\n    'downstream': '#d62728',  # Red\n    'peak': '#d62728',        # Red\n    'warning': '#ff7f0e',     # Orange\n    'good': '#2ca02c',        # Green\n    'bad': '#d62728',         # Red\n    'neutral': '#7f7f7f',     # Gray\n    'gap': '#ffcccc',         # Light red for gaps\n}\n\ndef create_stats_box(ax, text, loc='upper left', fontsize=9):\n    \"\"\"Create a standardized statistics annotation box.\"\"\"\n    props = dict(boxstyle='round,pad=0.5', facecolor='white', \n                 alpha=0.9, edgecolor='gray', linewidth=0.5)\n    \n    positions = {\n        'upper left': (0.02, 0.98, 'top', 'left'),\n        'upper right': (0.98, 0.98, 'top', 'right'),\n        'lower left': (0.02, 0.02, 'bottom', 'left'),\n        'lower right': (0.98, 0.02, 'bottom', 'right'),\n    }\n    x, y, va, ha = positions.get(loc, positions['upper left'])\n    \n    ax.text(x, y, text, transform=ax.transAxes, fontsize=fontsize, \n            fontfamily='monospace', verticalalignment=va, \n            horizontalalignment=ha, bbox=props)\n\ndef format_datetime_axis(ax, date_format='%m/%d', rotation=45):\n    \"\"\"Apply consistent datetime formatting to x-axis.\"\"\"\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(date_format))\n    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n    plt.setp(ax.xaxis.get_majorticklabels(), rotation=rotation, ha='right')\n\n# Create plots directory\nplots_dir = gauge_data_dir / 'plots'\nplots_dir.mkdir(exist_ok=True)\n\nprint(\"QAQC Visualization configuration loaded\")\nprint(f\"  - Figure DPI: {plt.rcParams['savefig.dpi']}\")\nprint(f\"  - Color palette: {len(COLORS)} colors defined\")\nprint(f\"  - Output directory: {plots_dir}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.1 Data Quality Dashboard\n\nA 4-panel dashboard for comprehensive data quality assessment:\n- Time series with gap highlighting\n- Data completeness by parameter\n- Distribution of observation intervals\n- Data presence timeline",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FIGURE A1: DATA QUALITY DASHBOARD (4-PANEL)\n# =============================================================================\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig.suptitle(f'USGS Gauge Data Quality Assessment\\n{event_name.replace(\"_\", \" \")}', \n             fontsize=14, fontweight='bold', y=0.98)\n\n# ==========================================================================\n# Panel 1: Time Series with Gap Highlighting (Top Left)\n# ==========================================================================\nax1 = axes[0, 0]\n\nax1.plot(upstream_flow['datetime'], upstream_flow['value'], \n         color=COLORS['flow'], linewidth=1, label='Flow')\n\n# Detect and highlight gaps (> 30 minutes for 15-min data)\ntime_diffs = upstream_flow['datetime'].diff()\ngap_threshold = pd.Timedelta(minutes=45)\ngaps = upstream_flow[time_diffs > gap_threshold].copy()\ngap_count = len(gaps)\n\nfor idx in gaps.index:\n    gap_start = upstream_flow.loc[idx - 1, 'datetime'] if idx > 0 else upstream_flow['datetime'].iloc[0]\n    gap_end = upstream_flow.loc[idx, 'datetime']\n    ax1.axvspan(gap_start, gap_end, alpha=0.3, color=COLORS['gap'], \n                label='Data Gap' if idx == gaps.index[0] else '')\n\nax1.set_xlabel('Date/Time')\nax1.set_ylabel('Flow (cfs)', color=COLORS['flow'])\nax1.set_title(f'USGS-{upstream_site}: Time Series with Gap Detection', fontweight='bold')\nax1.legend(loc='upper right')\nax1.grid(True, alpha=0.3)\nformat_datetime_axis(ax1)\n\n# Add gap statistics\nif gap_count > 0:\n    max_gap = time_diffs.max()\n    gap_text = f\"Gaps: {gap_count}\\nMax Gap: {max_gap}\"\nelse:\n    gap_text = \"No Gaps Detected\"\ncreate_stats_box(ax1, gap_text, loc='upper left')\n\n# ==========================================================================\n# Panel 2: Data Completeness Bar Chart (Top Right)\n# ==========================================================================\nax2 = axes[0, 1]\n\nevent_duration = (upstream_flow['datetime'].max() - upstream_flow['datetime'].min())\nexpected_records = int(event_duration.total_seconds() / (15 * 60)) + 1\n\ncompleteness = {\n    'Flow': len(upstream_flow) / expected_records * 100 if expected_records > 0 else 0,\n    'Stage': len(upstream_stage) / expected_records * 100 if expected_records > 0 else 0\n}\n\nbars = ax2.bar(completeness.keys(), completeness.values(), \n               color=[COLORS['flow'], COLORS['stage']], edgecolor='black', linewidth=0.5)\n\nax2.axhline(y=95, color=COLORS['good'], linestyle='--', linewidth=2, label='Target (95%)')\n\nfor bar, (param, pct) in zip(bars, completeness.items()):\n    if pct >= 95:\n        bar.set_facecolor(COLORS['good'])\n    elif pct >= 80:\n        bar.set_facecolor(COLORS['warning'])\n    else:\n        bar.set_facecolor(COLORS['bad'])\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n             f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n\nax2.set_ylabel('Data Completeness (%)')\nax2.set_title('Data Availability by Parameter', fontweight='bold')\nax2.set_ylim(0, 110)\nax2.legend(loc='lower right')\nax2.grid(True, alpha=0.3, axis='y')\n\n# ==========================================================================\n# Panel 3: Observation Interval Histogram (Bottom Left)\n# ==========================================================================\nax3 = axes[1, 0]\n\nintervals_min = time_diffs.dt.total_seconds().dropna() / 60\nbins = np.arange(0, max(61, intervals_min.max() + 5), 5)\ncounts, bins_out, patches = ax3.hist(intervals_min, bins=bins, \n                                      color=COLORS['flow'], edgecolor='black',\n                                      alpha=0.7, linewidth=0.5)\n\nax3.axvline(x=15, color=COLORS['good'], linestyle='--', linewidth=2, label='Expected (15 min)')\n\nfor patch, left_edge in zip(patches, bins_out[:-1]):\n    if left_edge > 20 or (left_edge > 0 and left_edge < 10):\n        patch.set_facecolor(COLORS['warning'])\n\nax3.set_xlabel('Observation Interval (minutes)')\nax3.set_ylabel('Frequency')\nax3.set_title('Distribution of Observation Intervals', fontweight='bold')\nax3.legend(loc='upper right')\nax3.grid(True, alpha=0.3, axis='y')\n\ninterval_stats = f\"Median: {intervals_min.median():.0f} min\\nStd: {intervals_min.std():.1f} min\"\ncreate_stats_box(ax3, interval_stats, loc='upper left')\n\n# ==========================================================================\n# Panel 4: Data Timeline Visualization (Bottom Right)\n# ==========================================================================\nax4 = axes[1, 1]\n\ndate_range = pd.date_range(upstream_flow['datetime'].min(), \n                            upstream_flow['datetime'].max(), freq='1H')\n\npresence = []\nfor dt in date_range:\n    has_data = any((upstream_flow['datetime'] >= dt) & \n                   (upstream_flow['datetime'] < dt + pd.Timedelta(hours=1)))\n    presence.append(1 if has_data else 0)\n\npresence_arr = np.array(presence).reshape(1, -1)\nax4.imshow(presence_arr, aspect='auto', cmap='RdYlGn', \n           extent=[0, len(presence), 0, 1], vmin=0, vmax=1)\n\nax4.set_yticks([])\nax4.set_xlabel('Hours from Event Start')\nax4.set_title('Data Presence Timeline (Hourly)', fontweight='bold')\n\nlegend_elements = [Patch(facecolor='green', label='Data Present'),\n                   Patch(facecolor='red', label='Data Missing')]\nax4.legend(handles=legend_elements, loc='upper right', ncol=2)\n\nplt.tight_layout(rect=[0, 0.02, 1, 0.96])\n\n# Footer\ntotal_records = len(upstream_flow)\nduration_days = event_duration.days + event_duration.seconds / 86400\nfooter_text = (f\"Total Records: {total_records:,} | \"\n               f\"Duration: {duration_days:.1f} days | \"\n               f\"Gauge: USGS-{upstream_site}\")\nfig.text(0.5, 0.01, footer_text, ha='center', fontsize=9, style='italic')\n\nfig.savefig(plots_dir / f'{event_name}_data_quality_dashboard.png', \n            dpi=150, bbox_inches='tight', facecolor='white')\nprint(f\"Saved: {plots_dir / f'{event_name}_data_quality_dashboard.png'}\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.2 Publication-Quality Event Hydrograph\n\nProfessional hydrograph with dual axes (flow and stage), peak annotations, initial condition marker, and comprehensive event statistics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FIGURE B1: PUBLICATION-QUALITY EVENT HYDROGRAPH\n# =============================================================================\n\nfig, ax1 = plt.subplots(figsize=(14, 7))\n\n# Primary Y-Axis: Flow\nline_flow, = ax1.plot(upstream_flow['datetime'], upstream_flow['value'], \n                      color=COLORS['flow'], linewidth=2, label='Flow (cfs)')\nax1.fill_between(upstream_flow['datetime'], upstream_flow['value'], \n                 alpha=0.15, color=COLORS['flow'])\n\nax1.set_xlabel('Date/Time', fontsize=11, fontweight='bold')\nax1.set_ylabel('Discharge (cfs)', color=COLORS['flow'], fontsize=11, fontweight='bold')\nax1.tick_params(axis='y', labelcolor=COLORS['flow'])\n\n# Secondary Y-Axis: Stage\nax2 = ax1.twinx()\nif not upstream_stage.empty:\n    line_stage, = ax2.plot(upstream_stage['datetime'], upstream_stage['value'],\n                           color=COLORS['stage'], linewidth=1.5, label='Stage (ft)')\n    ax2.set_ylabel('Stage (ft)', color=COLORS['stage'], fontsize=11, fontweight='bold')\n    ax2.tick_params(axis='y', labelcolor=COLORS['stage'])\n\n# Peak Flow Annotation\npeak_idx = upstream_flow['value'].idxmax()\npeak_flow = upstream_flow.loc[peak_idx, 'value']\npeak_time = upstream_flow.loc[peak_idx, 'datetime']\n\nax1.scatter([peak_time], [peak_flow], color=COLORS['peak'], s=100, \n            zorder=5, marker='*', edgecolors='black', linewidth=0.5)\n\nax1.annotate(\n    f'Peak: {peak_flow:,.0f} cfs\\n{peak_time.strftime(\"%m/%d %H:%M\")}',\n    xy=(peak_time, peak_flow),\n    xytext=(40, 30),\n    textcoords='offset points',\n    fontsize=10, fontweight='bold', color=COLORS['peak'],\n    arrowprops=dict(arrowstyle='->', color=COLORS['peak'], lw=1.5),\n    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', \n              edgecolor=COLORS['peak'], alpha=0.9)\n)\n\n# Initial Condition Point\nif initial_flow is not None:\n    sim_start_utc = pd.Timestamp(simulation_start, tz='UTC')\n    time_diffs_ic = abs(upstream_flow['datetime'] - sim_start_utc)\n    nearest_idx = time_diffs_ic.idxmin()\n    ic_flow = upstream_flow.loc[nearest_idx, 'value']\n    ic_time = upstream_flow.loc[nearest_idx, 'datetime']\n    \n    ax1.scatter([ic_time], [ic_flow], color=COLORS['good'], s=150,\n                zorder=5, marker='o', edgecolors='black', linewidth=1)\n    \n    ax1.annotate(\n        f'Initial Condition\\nQ = {ic_flow:,.0f} cfs',\n        xy=(ic_time, ic_flow),\n        xytext=(-60, -40),\n        textcoords='offset points',\n        fontsize=9, color=COLORS['good'],\n        arrowprops=dict(arrowstyle='->', color=COLORS['good'], lw=1.5),\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n                  edgecolor=COLORS['good'], alpha=0.9)\n    )\n\n# Calculate Event Statistics\nevent_start_time = upstream_flow['datetime'].min()\ntime_to_peak = peak_time - event_start_time\ntime_to_peak_hours = time_to_peak.total_seconds() / 3600\nevent_duration = upstream_flow['datetime'].max() - upstream_flow['datetime'].min()\nduration_days = event_duration.days + event_duration.seconds / 86400\nmin_flow = upstream_flow['value'].min()\nmean_flow = upstream_flow['value'].mean()\n\n# Statistics Box\nstats_text = (\n    f\"Event Statistics\\n\"\n    f\"{'─' * 20}\\n\"\n    f\"Duration:      {duration_days:.1f} days\\n\"\n    f\"Peak Flow:     {peak_flow:,.0f} cfs\\n\"\n    f\"Min Flow:      {min_flow:,.0f} cfs\\n\"\n    f\"Mean Flow:     {mean_flow:,.0f} cfs\\n\"\n    f\"Time to Peak:  {time_to_peak_hours:.0f} hrs\\n\"\n)\nif not upstream_stage.empty:\n    peak_stage = upstream_stage['value'].max()\n    min_stage = upstream_stage['value'].min()\n    stats_text += (\n        f\"{'─' * 20}\\n\"\n        f\"Peak Stage:    {peak_stage:.2f} ft\\n\"\n        f\"Stage Range:   {peak_stage - min_stage:.2f} ft\"\n    )\n\ncreate_stats_box(ax1, stats_text, loc='lower left', fontsize=9)\n\n# Formatting\nax1.set_title(\n    f'USGS-{upstream_site}: {target_gauges[\"upstream\"][\"name\"]}\\n{event_name.replace(\"_\", \" \")}',\n    fontsize=14, fontweight='bold', pad=15\n)\n\nax1.grid(True, which='major', alpha=0.4, linestyle='-')\nax1.grid(True, which='minor', alpha=0.2, linestyle=':')\nax1.minorticks_on()\n\nformat_datetime_axis(ax1, date_format='%m/%d\\n%H:%M', rotation=0)\n\n# Legend\nlines = [line_flow]\nlabels = ['Flow (cfs)']\nif not upstream_stage.empty:\n    lines.append(line_stage)\n    labels.append('Stage (ft)')\nax1.legend(lines, labels, loc='upper right', framealpha=0.9, edgecolor='gray')\n\nax1.set_ylim(0, peak_flow * 1.15)\nplt.tight_layout()\n\nfig.text(0.99, 0.01, f'Data Source: USGS National Water Information System',\n         ha='right', fontsize=8, style='italic', color='gray')\n\nfig.savefig(plots_dir / f'{event_name}_hydrograph.png', \n            dpi=150, bbox_inches='tight', facecolor='white')\nprint(f\"Saved: {plots_dir / f'{event_name}_hydrograph.png'}\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.3 Stage-Discharge Rating Curve\n\nRating curve analysis showing the stage-discharge relationship with power law fit and time-colored scatter points to identify hysteresis effects.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FIGURE D1: STAGE-DISCHARGE RATING CURVE\n# =============================================================================\n\nif not upstream_stage.empty:\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Merge flow and stage by nearest timestamp\n    merged = pd.merge_asof(\n        upstream_flow.sort_values('datetime'),\n        upstream_stage.sort_values('datetime'),\n        on='datetime',\n        direction='nearest',\n        tolerance=pd.Timedelta(minutes=30),\n        suffixes=('_flow', '_stage')\n    ).dropna()\n    \n    if len(merged) >= 10:\n        Q = merged['value_flow'].values\n        h = merged['value_stage'].values\n        \n        # Create time-based colors\n        times_numeric = (merged['datetime'] - merged['datetime'].min()).dt.total_seconds()\n        \n        # Scatter Plot with Time Colormap\n        scatter = ax.scatter(Q, h, c=times_numeric, cmap='viridis', \n                             s=40, alpha=0.7, edgecolors='black', linewidth=0.3)\n        \n        # Colorbar\n        cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\n        cbar.set_label('Time (hours from event start)', fontsize=10)\n        max_hours = times_numeric.max() / 3600\n        cbar.set_ticks(np.linspace(0, times_numeric.max(), 5))\n        cbar.set_ticklabels([f'{t:.0f}h' for t in np.linspace(0, max_hours, 5)])\n        \n        # Power Law Fit: Q = a * (h - h0)^b\n        def power_law(h, a, b, h0):\n            return a * np.maximum(h - h0, 0.001) ** b\n        \n        try:\n            h0_init = h.min() - 0.1\n            popt, pcov = curve_fit(power_law, h, Q, \n                                   p0=[100, 2.0, h0_init],\n                                   bounds=([0, 0.5, h.min()-1], [10000, 4.0, h.min()]),\n                                   maxfev=5000)\n            \n            a, b, h0 = popt\n            \n            h_fit = np.linspace(h.min(), h.max(), 100)\n            Q_fit = power_law(h_fit, a, b, h0)\n            \n            ax.plot(Q_fit, h_fit, 'r-', linewidth=2.5, \n                    label=f'Power Law Fit: Q = {a:.1f}(h-{h0:.2f})^{b:.2f}')\n            \n            # Calculate R-squared\n            Q_pred = power_law(h, a, b, h0)\n            ss_res = np.sum((Q - Q_pred) ** 2)\n            ss_tot = np.sum((Q - np.mean(Q)) ** 2)\n            r_squared = 1 - (ss_res / ss_tot)\n            \n            fit_text = (\n                f\"Rating Curve Fit\\n\"\n                f\"{'─' * 18}\\n\"\n                f\"Q = a(h-h0)^b\\n\"\n                f\"a = {a:.1f}\\n\"\n                f\"b = {b:.2f}\\n\"\n                f\"h0 = {h0:.2f} ft\\n\"\n                f\"R2 = {r_squared:.3f}\"\n            )\n            \n        except Exception as e:\n            print(f\"Power law fit failed: {e}\")\n            fit_text = \"Power law fit\\nnot available\"\n        \n        create_stats_box(ax, fit_text, loc='lower right', fontsize=9)\n        \n        ax.set_xlabel('Discharge (cfs)', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Stage (ft)', fontsize=11, fontweight='bold')\n        ax.set_title(f'USGS-{upstream_site}: Stage-Discharge Rating Curve\\n{target_gauges[\"upstream\"][\"name\"]}',\n                     fontsize=14, fontweight='bold', pad=15)\n        \n        ax.legend(loc='upper left', framealpha=0.9)\n        ax.grid(True, alpha=0.3)\n        \n        ax.set_xlim(0, Q.max() * 1.1)\n        ax.set_ylim(h.min() * 0.95, h.max() * 1.05)\n        \n        plt.tight_layout()\n        \n        fig.savefig(plots_dir / f'{event_name}_rating_curve.png', \n                    dpi=150, bbox_inches='tight', facecolor='white')\n        print(f\"Saved: {plots_dir / f'{event_name}_rating_curve.png'}\")\n        plt.show()\n    else:\n        print(f\"Insufficient paired data points ({len(merged)}) for rating curve\")\nelse:\n    print(\"Stage data not available for rating curve analysis\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.4 Summary Statistics Dashboard\n\nComprehensive single-page summary suitable for engineering reports, including statistics tables, mini hydrograph, distribution plots, and data quality metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FIGURE E1: COMPREHENSIVE SUMMARY STATISTICS DASHBOARD\n# =============================================================================\n\nfig = plt.figure(figsize=(16, 12))\n\ngs = gridspec.GridSpec(4, 4, figure=fig, height_ratios=[0.8, 2, 2, 1.5],\n                       hspace=0.3, wspace=0.3)\n\n# ==========================================================================\n# Header Row\n# ==========================================================================\nax_header = fig.add_subplot(gs[0, :])\nax_header.axis('off')\n\nheader_text = (\n    f\"USGS Gauge Data Summary Report\\n\"\n    f\"{'━' * 60}\\n\"\n    f\"Event: {event_name.replace('_', ' ')}    |    \"\n    f\"Period: {event_start.strftime('%Y-%m-%d')} to {event_end.strftime('%Y-%m-%d')}    |    \"\n    f\"Gauge: USGS-{upstream_site}\"\n)\nax_header.text(0.5, 0.5, header_text, ha='center', va='center',\n               fontsize=14, fontfamily='monospace', fontweight='bold')\n\n# ==========================================================================\n# Flow Statistics Table (Row 1, Left)\n# ==========================================================================\nax_flow_stats = fig.add_subplot(gs[1, 0:2])\nax_flow_stats.axis('off')\n\nflow_stats = {\n    'Count': f\"{len(upstream_flow):,}\",\n    'Min (cfs)': f\"{upstream_flow['value'].min():,.0f}\",\n    'Max (cfs)': f\"{upstream_flow['value'].max():,.0f}\",\n    'Mean (cfs)': f\"{upstream_flow['value'].mean():,.0f}\",\n    'Median (cfs)': f\"{upstream_flow['value'].median():,.0f}\",\n    'Std Dev (cfs)': f\"{upstream_flow['value'].std():,.0f}\",\n    'Skewness': f\"{stats.skew(upstream_flow['value']):.2f}\",\n    'Kurtosis': f\"{stats.kurtosis(upstream_flow['value']):.2f}\",\n}\n\ntable_data = [[k, v] for k, v in flow_stats.items()]\ntable = ax_flow_stats.table(cellText=table_data,\n                             colLabels=['Statistic', 'Value'],\n                             cellLoc='center', loc='center', colWidths=[0.5, 0.5])\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.5)\n\nfor (i, j), cell in table.get_celld().items():\n    if i == 0:\n        cell.set_facecolor('#4472C4')\n        cell.set_text_props(color='white', fontweight='bold')\n    elif i % 2 == 0:\n        cell.set_facecolor('#D9E2F3')\n    cell.set_edgecolor('white')\n\nax_flow_stats.set_title('Flow Statistics', fontweight='bold', fontsize=12, pad=10)\n\n# ==========================================================================\n# Mini Hydrograph (Row 1, Right)\n# ==========================================================================\nax_mini_hydro = fig.add_subplot(gs[1, 2:4])\n\nax_mini_hydro.plot(upstream_flow['datetime'], upstream_flow['value'],\n                   color=COLORS['flow'], linewidth=1.5)\nax_mini_hydro.fill_between(upstream_flow['datetime'], upstream_flow['value'],\n                           alpha=0.2, color=COLORS['flow'])\n\npeak_idx = upstream_flow['value'].idxmax()\npeak_flow_val = upstream_flow.loc[peak_idx, 'value']\npeak_time_val = upstream_flow.loc[peak_idx, 'datetime']\nax_mini_hydro.scatter([peak_time_val], [peak_flow_val], color=COLORS['peak'],\n                      s=100, marker='*', zorder=5, edgecolors='black')\n\nax_mini_hydro.set_title('Event Hydrograph', fontweight='bold', fontsize=12)\nax_mini_hydro.set_xlabel('Date')\nax_mini_hydro.set_ylabel('Flow (cfs)')\nax_mini_hydro.grid(True, alpha=0.3)\nformat_datetime_axis(ax_mini_hydro, date_format='%m/%d')\n\n# ==========================================================================\n# Flow Histogram (Row 2, Left)\n# ==========================================================================\nax_hist = fig.add_subplot(gs[2, 0:2])\n\nn, bins_hist, patches_hist = ax_hist.hist(upstream_flow['value'], bins=30,\n                                           color=COLORS['flow'], edgecolor='black',\n                                           alpha=0.7, linewidth=0.5)\n\nax_hist.axvline(upstream_flow['value'].mean(), color='red', linestyle='--',\n                linewidth=2, label=f'Mean: {upstream_flow[\"value\"].mean():,.0f}')\nax_hist.axvline(upstream_flow['value'].median(), color='green', linestyle=':',\n                linewidth=2, label=f'Median: {upstream_flow[\"value\"].median():,.0f}')\n\nax_hist.set_title('Flow Distribution', fontweight='bold', fontsize=12)\nax_hist.set_xlabel('Flow (cfs)')\nax_hist.set_ylabel('Frequency')\nax_hist.legend(loc='upper right', fontsize=9)\nax_hist.grid(True, alpha=0.3, axis='y')\n\n# ==========================================================================\n# Box Plot (Row 2, Right)\n# ==========================================================================\nax_box = fig.add_subplot(gs[2, 2:4])\n\nflow_norm = (upstream_flow['value'] - upstream_flow['value'].min()) / \\\n            (upstream_flow['value'].max() - upstream_flow['value'].min())\n\nbox_data = [flow_norm.dropna()]\nlabels = ['Flow\\n(normalized)']\n\nif not upstream_stage.empty:\n    stage_norm = (upstream_stage['value'] - upstream_stage['value'].min()) / \\\n                 (upstream_stage['value'].max() - upstream_stage['value'].min())\n    box_data.append(stage_norm.dropna())\n    labels.append('Stage\\n(normalized)')\n\nbp = ax_box.boxplot(box_data, labels=labels, patch_artist=True)\n\ncolors_box = [COLORS['flow'], COLORS['stage']]\nfor patch, color in zip(bp['boxes'], colors_box[:len(box_data)]):\n    patch.set_facecolor(color)\n    patch.set_alpha(0.6)\n\nax_box.set_title('Parameter Distributions (Normalized)', fontweight='bold', fontsize=12)\nax_box.set_ylabel('Normalized Value (0-1)')\nax_box.grid(True, alpha=0.3, axis='y')\n\n# ==========================================================================\n# Data Quality Bar (Bottom Row)\n# ==========================================================================\nax_quality = fig.add_subplot(gs[3, :])\n\nevent_duration_q = (upstream_flow['datetime'].max() - upstream_flow['datetime'].min())\nexpected_records_q = int(event_duration_q.total_seconds() / (15 * 60)) + 1\ncompleteness_q = len(upstream_flow) / expected_records_q * 100 if expected_records_q > 0 else 0\n\ntime_diffs_q = upstream_flow['datetime'].diff()\ngap_threshold_q = pd.Timedelta(minutes=45)\ngap_count_q = len(time_diffs_q[time_diffs_q > gap_threshold_q])\n\nmetrics = {\n    'Data Completeness': completeness_q,\n    'Temporal Consistency': 100 - (gap_count_q / len(upstream_flow) * 100) if len(upstream_flow) > 0 else 0,\n    'Value Range Check': 100 if upstream_flow['value'].min() >= 0 else 50,\n}\n\ny_pos = range(len(metrics))\nbars_q = ax_quality.barh(y_pos, metrics.values(), color=COLORS['flow'],\n                         edgecolor='black', linewidth=0.5, height=0.6)\n\nfor bar, val in zip(bars_q, metrics.values()):\n    if val >= 95:\n        bar.set_facecolor(COLORS['good'])\n    elif val >= 80:\n        bar.set_facecolor(COLORS['warning'])\n    else:\n        bar.set_facecolor(COLORS['bad'])\n    ax_quality.text(val + 1, bar.get_y() + bar.get_height()/2,\n                    f'{val:.1f}%', va='center', fontweight='bold')\n\nax_quality.set_yticks(y_pos)\nax_quality.set_yticklabels(metrics.keys())\nax_quality.set_xlim(0, 110)\nax_quality.set_xlabel('Score (%)')\nax_quality.set_title('Data Quality Metrics', fontweight='bold', fontsize=12)\nax_quality.axvline(95, color='gray', linestyle='--', linewidth=1, label='Target (95%)')\nax_quality.grid(True, alpha=0.3, axis='x')\nax_quality.legend(loc='lower right')\n\n# Footer\nfig.text(0.01, 0.01, f'Generated: {pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M\")}',\n         fontsize=8, color='gray')\nfig.text(0.99, 0.01, f'Data Source: USGS NWIS | {target_gauges[\"upstream\"][\"name\"]}',\n         ha='right', fontsize=8, color='gray')\n\nfig.savefig(plots_dir / f'{event_name}_summary_dashboard.png', \n            dpi=150, bbox_inches='tight', facecolor='white')\nprint(f\"Saved: {plots_dir / f'{event_name}_summary_dashboard.png'}\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.5 QAQC Visualization Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# QAQC VISUALIZATION SUMMARY\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"USGS GAUGE DATA QAQC VISUALIZATION - SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\nEvent: {event_name.replace('_', ' ')}\")\nprint(f\"Gauge: USGS-{upstream_site}\")\nprint(f\"Period: {event_start} to {event_end}\")\n\nprint(f\"\\nGenerated Figures:\")\nprint(f\"  [A1] Data Quality Dashboard - Gap detection, completeness, intervals\")\nprint(f\"  [B1] Event Hydrograph - Dual-axis with peak/IC annotations\")\nprint(f\"  [D1] Rating Curve - Stage-discharge with power law fit\")\nprint(f\"  [E1] Summary Dashboard - Comprehensive statistics for reports\")\n\nprint(f\"\\nOutput Directory: {plots_dir}\")\nprint(f\"\\nFigure Files:\")\nfor f in sorted(plots_dir.glob(f'{event_name}*.png')):\n    print(f\"  - {f.name}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"QAQC Visualization Complete - Figures ready for engineering reports\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}