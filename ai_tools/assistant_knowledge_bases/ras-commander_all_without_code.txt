File: c:\GH\ras-commander\.cursorrules
==================================================
# RAS Commander (ras-commander) Coding Assistant

## Overview

This Assistant helps you write efficient Python code for HEC-RAS projects using the RAS Commander library. It automates tasks, provides a Pythonic interface, supports flexible execution modes, and offers built-in examples.

**Core Concepts:** RAS Objects, Project Initialization, File Handling (pathlib.Path), Data Management (Pandas), Execution Modes, Utility Functions.

## Classes, Functions and Arguments




Certainly! I'll summarize the decorators, provide tables for each class showing the decorators used and arguments, and give a summary of each class's function.

Decorator Summaries:

1. @log_call: Logs function calls, including entry and exit times, and any exceptions raised.
2. @standardize_input: Standardizes input for HDF file operations, handling different input types and ensuring consistent file paths.
3. @hdf_operation: Handles opening and closing of HDF files, and manages error handling for HDF operations.

Now, lets go through each class:


1. RasPrj Class:

| Function Name | @log_call | @standardize_input | @hdf_operation | Arguments |
|---------------|-----------|--------------------|--------------------|-----------|
| initialize | X | | | project_folder, ras_exe_path |
| _load_project_data | X | | | |
| _get_geom_file_for_plan | X | | | plan_number |
| _parse_plan_file | X | | | plan_file_path |
| _get_prj_entries | X | | | entry_type |
| _parse_unsteady_file | X | | | unsteady_file_path |
| check_initialized | X | | | |
| find_ras_prj | X | | | folder_path |
| get_project_name | X | | | |
| get_prj_entries | X | | | entry_type |
| get_plan_entries | X | | | |
| get_flow_entries | X |
1. RasPrj Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| initialize | X | | project_folder, ras_exe_path |
| _load_project_data | X | | |
| _get_geom_file_for_plan | X | | plan_number |
| _parse_plan_file | X | | plan_file_path |
| _get_prj_entries | X | | entry_type |
| _parse_unsteady_file | X | | unsteady_file_path |
| check_initialized | X | | |
| find_ras_prj | X | | folder_path |
| get_project_name | X | | |
| get_prj_entries | X | | entry_type |
| get_plan_entries | X | | |
| get_flow_entries | X | | |
| get_unsteady_entries | X | | |
| get_geom_entries | X | | |
| get_hdf_entries | X | | |
| print_data | X | | |
| get_plan_value | X | X | plan_number_or_path, key, ras_object |
| get_boundary_conditions | X | | |
| _parse_boundary_condition | X | | block, unsteady_number, bc_number |

2. RasPlan Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| set_geom | X | | plan_number, new_geom, ras_object |
| set_steady | X | | plan_number, new_steady_flow_number, ras_object |
| set_unsteady | X | | plan_number, new_unsteady_flow_number, ras_object |
| set_num_cores | X | | plan_number, num_cores, ras_object |
| set_geom_preprocessor | X | | file_path, run_htab, use_ib_tables, ras_object |
| get_results_path | X | X | plan_number, ras_object |
| get_plan_path | X | X | plan_number, ras_object |
| get_flow_path | X | X | flow_number, ras_object |
| get_unsteady_path | X | X | unsteady_number, ras_object |
| get_geom_path | X | X | geom_number, ras_object |
| clone_plan | X | | template_plan, new_plan_shortid, ras_object |
| clone_unsteady | X | | template_unsteady, ras_object |
| clone_steady | X | | template_flow, ras_object |
| clone_geom | X | | template_geom, ras_object |
| get_next_number | X | | existing_numbers |
| get_plan_value | X | X | plan_number_or_path, key, ras_object |
| update_plan_value | X | X | plan_number_or_path, key, value, ras_object |

3. RasGeo Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| clear_geompre_files | X | | plan_files, ras_object |

4. RasUnsteady Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| update_unsteady_parameters | X | | unsteady_file, modifications, ras_object |

5. RasCmdr Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| compute_plan | X | | plan_number, dest_folder, ras_object, clear_geompre, num_cores, overwrite_dest |
| compute_parallel | X | | plan_number, max_workers, num_cores, clear_geompre, ras_object, dest_folder, overwrite_dest |
| compute_test_mode | X | | plan_number, dest_folder_suffix, clear_geompre, num_cores, ras_object, overwrite_dest |

6. RasUtils Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| create_directory | X | | directory_path, ras_object |
| find_files_by_extension | X | | extension, ras_object |
| get_file_size | X | | file_path, ras_object |
| get_file_modification_time | X | | file_path, ras_object |
| get_plan_path | X | | current_plan_number_or_path, ras_object |
| remove_with_retry | X | | path, max_attempts, initial_delay, is_folder, ras_object |
| update_plan_file | X | | plan_number_or_path, file_type, entry_number, ras_object |
| check_file_access | X | | file_path, mode |
| convert_to_dataframe | X | | data_source, **kwargs |
| save_to_excel | X | | dataframe, excel_path, **kwargs |
| calculate_rmse | X | | observed_values, predicted_values, normalized |
| calculate_percent_bias | X | | observed_values, predicted_values, as_percentage |
| calculate_error_metrics | X | | observed_values, predicted_values |
| update_file | X | | file_path, update_function, *args |
| get_next_number | X | | existing_numbers |
| clone_file | X | | template_path, new_path, update_function, *args |
| update_project_file | X | | prj_file, file_type, new_num, ras_object |
| decode_byte_strings | X | | dataframe |
| perform_kdtree_query | X | | reference_points, query_points, max_distance |
| find_nearest_neighbors | X | | points, max_distance |
| consolidate_dataframe | X | | dataframe, group_by, pivot_columns, level, n_dimensional, aggregation_method |
| find_nearest_value | X | | array, target_value |
| horizontal_distance | X | | coord1, coord2 |

7. HdfBase Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| _get_simulation_start_time | | | hdf_file |
| _get_unsteady_datetimes | | | hdf_file |
| _get_2d_flow_area_names_and_counts | | | hdf_file |
| _parse_ras_datetime | | | datetime_str |
| _parse_ras_simulation_window_datetime | | | datetime_str |
| _parse_duration | | | duration_str |
| _parse_ras_datetime_ms | | | datetime_str |
| _convert_ras_hdf_string | | | value |

8. HdfBndry Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| bc_lines | | X (plan_hdf) | hdf_path |
| breaklines | | X (plan_hdf) | hdf_path |
| refinement_regions | | X (plan_hdf) | hdf_path |
| reference_lines_names | | X (plan_hdf) | hdf_path, mesh_name |
| reference_points_names | | X (plan_hdf) | hdf_path, mesh_name |
| reference_lines | | X (plan_hdf) | hdf_path |
| reference_points | | X (plan_hdf) | hdf_path |
| get_boundary_attributes | | X (plan_hdf) | hdf_path, boundary_type |
| get_boundary_count | | X (plan_hdf) | hdf_path, boundary_type |
| get_boundary_names | | X (plan_hdf) | hdf_path, boundary_type |

9. HdfMesh Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| mesh_area_names | | X (plan_hdf) | hdf_path |
| mesh_areas | | X (geom_hdf) | hdf_path |
| mesh_cell_polygons | | X (geom_hdf) | hdf_path |
| mesh_cell_points | | X (plan_hdf) | hdf_path |
| mesh_cell_faces | | X (plan_hdf) | hdf_path |
| get_geom_2d_flow_area_attrs | | X (geom_hdf) | hdf_path |

10. HdfPlan Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| get_simulation_start_time | X | X (plan_hdf) | hdf_path |
| get_simulation_end_time | X | X (plan_hdf) | hdf_path |
| get_unsteady_datetimes | X | X (plan_hdf) | hdf_path |
| get_plan_info_attrs | X | X (plan_hdf) | hdf_path |
| get_plan_param_attrs | X | X (plan_hdf) | hdf_path |
| get_meteorology_precip_attrs | X | X (plan_hdf) | hdf_path |
| get_geom_attrs | X | X (plan_hdf) | hdf_path |

11. HdfResultsMesh Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| mesh_summary_output | X | X (plan_hdf) | hdf_path, var, round_to |
| mesh_timeseries_output | X | X (plan_hdf) | hdf_path, mesh_name, var, truncate |
| mesh_faces_timeseries_output | X | X (plan_hdf) | hdf_path, mesh_name |
| mesh_cells_timeseries_output | X | X (plan_hdf) | hdf_path, mesh_names, var, truncate, ras_object |
| mesh_last_iter | X | X (plan_hdf) | hdf_path |
| mesh_max_ws | X | X (plan_hdf) | hdf_path, round_to |
| mesh_min_ws | X | X (plan_hdf) | hdf_path, round_to |
| mesh_max_face_v | X | X (plan_hdf) | hdf_path, round_to |
| mesh_min_face_v | X | X (plan_hdf) | hdf_path, round_to |
| mesh_max_ws_err | X | X (plan_hdf) | hdf_path, round_to |
| mesh_max_iter | X | X (plan_hdf) | hdf_path, round_to |

12. HdfResultsPlan Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| get_results_unsteady_attrs | X | X (plan_hdf) | hdf_path |
| get_results_unsteady_summary_attrs | X | X (plan_hdf) | hdf_path |
| get_results_volume_accounting_attrs | X | X (plan_hdf) | hdf_path |
| get_runtime_data | | X (plan_hdf) | hdf_path |
| reference_timeseries_output | X | X (plan_hdf) | hdf_path, reftype |
| reference_lines_timeseries_output | X | X (plan_hdf) | hdf_path |
| reference_points_timeseries_output | X | X (plan_hdf) | hdf_path |
| reference_summary_output | X | X (plan_hdf) | hdf_path, reftype |

13. HdfResultsXsec Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| steady_profile_xs_output | | X (plan_hdf) | hdf_path, var, round_to |
| cross_sections_wsel | | X (plan_hdf) | hdf_path |
| cross_sections_flow | | X (plan_hdf) | hdf_path |
| cross_sections_energy_grade | | X (plan_hdf) | hdf_path |
| cross_sections_additional_enc_station_left | | X (plan_hdf) | hdf_path |
| cross_sections_additional_enc_station_right | | X (plan_hdf) | hdf_path |
| cross_sections_additional_area_total | | X (plan_hdf) | hdf_path |
| cross_sections_additional_velocity_total | | X (plan_hdf) | hdf_path |

14. HdfStruc Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| structures | X | X (geom_hdf) | hdf_path, datetime_to_str |
| get_geom_structures_attrs | X | X (geom_hdf) | hdf_path |

15. HdfUtils Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| get_hdf_filename | | X (plan_hdf) | hdf_input, ras_object |
| get_root_attrs | | X (plan_hdf) | hdf_path |
| get_attrs | | X (plan_hdf) | hdf_path, attr_path |
| get_hdf_paths_with_properties | | X (plan_hdf) | hdf_path |
| get_group_attributes_as_df | | X (plan_hdf) | hdf_path, group_path |
| get_2d_flow_area_names_and_counts | | X (plan_hdf) | hdf_path |
| projection | | X (plan_hdf) | hdf_path |

16. HdfXsec Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| cross_sections | X | X (geom_hdf) | hdf_path, datetime_to_str |
| cross_sections_elevations | X | X (geom_hdf) | hdf_path, round_to |
| river_reaches | X | X (geom_hdf) | hdf_path, datetime_to_str |

17. RasExamples Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| __init__ | X | | |
| get_example_projects | X | | version_number |
| _load_project_data | X | | |
| _find_zip_file | X | | |
| _extract_folder_structure | X | | |
| _save_to_csv | X | | |
| list_categories | X | | |
| list_projects | X | | category |
| extract_project | X | | project_names |
| is_project_extracted | X | | project_name |
| clean_projects_directory | X | | |
| download_fema_ble_model | X | | huc8, output_dir |
| _make_safe_folder_name | X | | name |
| _download_file_with_progress | X | | url, dest_folder, file_size |
| _convert_size_to_bytes | X | | size_str |

18. RasGpt Class:

This class is mentioned in the code but has no implemented methods yet.

19. Standalone functions:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| init_ras_project | X | | ras_project_folder, ras_version, ras_instance |
| get_ras_exe | X | | ras_version |




Overall, the ras-commander library provides a comprehensive set of tools for working with HEC-RAS projects, including project management, file operations, data extraction, and simulation execution. The library makes extensive use of logging and input standardization through decorators, ensuring consistent behavior and traceability across its various components.


## Coding Assistance Rules:

1. Use default libraries, especially pathlib for file operations.
2. Use r-strings for paths, f-strings for formatting.
3. Always use pathlib over os for file/directory operations.
4. Include comments and use logging for output.
5. Follow PEP 8 conventions.
6. Provide clear error handling and user feedback.
7. Explain RAS Commander function purposes and key arguments.
8. Use either global 'ras' object or custom instances consistently.
9. Highlight parallel execution best practices.
10. Suggest RasExamples for testing when appropriate.
11. Utilize RasHdf for HDF file operations and data extraction.
12. Use type hints for function arguments and return values.
13. Apply the @log_call decorator for automatic function logging.
14. Emphasize proper error handling and logging in all functions.
15. When working with RasHdfGeom, always use the @standardize_input decorator for methods that interact with HDF files.
16. Remember that RasHdfGeom methods often return GeoDataFrames, which combine geometric data with attribute information.
17. When dealing with cross-sections or river reaches, consider using the datetime_to_str parameter to convert datetime objects to strings if needed.
18. For methods that accept a mesh_name parameter, remember that they can return either a dictionary of lists or a single list depending on whether a specific mesh is specified.
19. Use 'union_all()' for geodataframes. For pandas >= 2.0, use pd.concat instead of append.
20. Provide full code segments or scripts with no elides.
21. When importing from the Decorators module, use:
    ```python
    from .Decorators import standardize_input, log_call
    ```
22. When importing from the LoggingConfig module, use:
    ```python
    from .LoggingConfig import setup_logging, get_logger
    ```
23. Be aware that while the code will work with capitalized module names (Decorators.py and LoggingConfig.py), it's generally recommended to stick to lowercase names for modules as per PEP 8.
24. When revising code, label planning steps as:
    ## Explicit Planning and Reasoning for Revisions

25. Always consider the implications of file renaming on import statements throughout the project.
26. When working with GeoDataFrames, remember to use appropriate geometric operations and consider spatial relationships.
27. For HDF file operations, always use the standardize_input decorator to ensure consistent handling of file paths.
28. When dealing with large datasets, consider using chunking or iterative processing to manage memory usage.
29. Utilize the RasExamples class for testing and demonstrating functionality with sample projects.
30. When working with the RasGpt class, be aware that it's mentioned but currently has no implemented methods.
==================================================

Folder: c:\GH\ras-commander\.gitignore
==================================================

File: c:\GH\ras-commander\Comprehensive_Library_Guide.md
==================================================
# Comprehensive RAS-Commander Library Guide

## Introduction

RAS-Commander (`ras_commander`) is a Python library designed to automate and streamline operations with HEC-RAS projects. It provides a suite of tools for managing projects, executing simulations, and handling results. This guide offers a comprehensive overview of the library's key concepts, modules, best practices, and advanced usage patterns. RAS-Commander is designed to be flexible, robust, and AI-accessible, making it an ideal tool for both manual and automated HEC-RAS workflows.

RAS-Commander can be installed with the following commands:
```
pip install h5py numpy pandas requests tqdm scipy xarray geopandas matplotlib ras-commander ipython psutil shapely fiona pathlib rtree
pip install --update ras-commander # This ensures you get the latest version of the library
```

## Key Concepts

1. **RAS Objects**:
   - Represent HEC-RAS projects containing information about plans, geometries, and flow files.
   - Support both a global `ras` object and custom `RasPrj` instances for different projects.

2. **Project Initialization**:
   - Use `init_ras_project()` to initialize projects and set up RAS objects.
   - Handles project file discovery and data structure setup.

3. **File Handling**:
   - Utilizes `pathlib.Path` for consistent, platform-independent file paths.
   - Adheres to HEC-RAS file naming conventions (`.prj`, `.p01`, `.g01`, `.f01`, `.u01`).

4. **Data Management**:
   - Employs Pandas DataFrames to manage structured data about plans, geometries, and flow files.
   - Provides methods for accessing and updating these DataFrames.

5. **Execution Modes**:
   - **Single Plan Execution**: Run individual plans.
   - **Sequential Execution**: Run multiple plans in sequence.
   - **Parallel Execution**: Run multiple plans concurrently for improved performance.

6. **Example Projects**:
   - The `RasExamples` class offers functionality to download and manage HEC-RAS example projects for testing and learning.

7. **Utility Functions**:
   - `RasUtils` provides common utility functions for file operations, backups, error handling, and statistical analysis.

8. **Artifact System**:
   - Handles substantial, self-contained content that users might modify or reuse, displayed in a separate UI window.

9. **AI-Driven Coding Tools**:
   - Integrates AI-powered tools like ChatGPT Assistant, LLM Summaries, Cursor IDE Integration, and Jupyter Notebook Assistant.

10. **Boundary Conditions**:
    - Represent the input conditions for HEC-RAS simulations, including flow hydrographs, stage hydrographs, and other hydraulic inputs.
    - The `RasPrj` class provides functionality to extract and manage boundary conditions from unsteady flow files.

11. **Flexibility and Modularity**:
    - All classes are designed to work with either a global 'ras' object + a plan number, or with custom project instances.
    - Clear separation of concerns between project management (RasPrj), execution (RasCmdr), and results data retrieval (RasHdf).

12. **Error Handling and Logging**:
    - Emphasis on robust error checking and informative logging throughout the library.
    - Utilizes the `logging_config` module for consistent logging configuration.
    - `@log_call` decorator applied to relevant functions for logging function calls.

13. **AI-Accessibility**:
    - Structured, consistent codebase with clear documentation to facilitate easier learning and usage by AI models.

14. **Expanded HDF Support**:
    - Comprehensive HDF file handling through specialized classes
    - Support for mesh, plan, and cross-section results
    - Advanced data extraction and analysis capabilities

15. **Enhanced Boundary Conditions**:
    - Improved extraction and management of boundary conditions
    - Support for various boundary condition types
    - Structured representation of boundary data

16. **Infrastructure Analysis**:
    - New support for pipe networks through `HdfPipe`
    - Pump station analysis via `HdfPump`
    - Fluvial-pluvial analysis capabilities in `HdfFluvialPluvial`

17. **RAS Interface Components**:
    - Integration with RASMapper through `RasMap`
    - Go-Consequences interface via `RasToGo`
    - AI assistance through `RasGpt`

## Core Features

1. **Project Management**: Initialize, load, and manage HEC-RAS projects.
2. **Plan Execution**: Run single or multiple HEC-RAS plans with various execution modes.
3. **File Operations**: Handle HEC-RAS file types (plans, geometries, flows) with ease.
4. **Data Extraction**: Retrieve and process results from HDF files.
5. **Boundary Condition Management**: Extract and analyze boundary conditions.
6. **Parallel Processing**: Optimize performance with parallel plan execution.
7. **Example Project Handling**: Download and manage HEC-RAS example projects.
8. **Utility Functions**: Perform common tasks and statistical analyses.
9. **HDF File Handling**: Specialized classes for working with HEC-RAS HDF files.
10. **Advanced HDF Analysis**: Comprehensive tools for HDF file operations
11. **Infrastructure Analysis**: Tools for pipe networks and pump stations
12. **Mapping Integration**: RASMapper interface capabilities
13. **External Tool Integration**: Connection to Go-Consequences and other tools

## Module Overview

1. **RasPrj**: Manages HEC-RAS project initialization and data, including boundary conditions.
2. **RasCmdr**: Handles execution of HEC-RAS simulations.
3. **RasPlan**: Provides functions for plan file operations.
4. **RasGeo**: Manages geometry file operations.
5. **RasUnsteady**: Handles unsteady flow file operations.
6. **RasUtils**: Offers utility functions for common tasks and statistical analysis.
7. **RasExamples**: Manages example HEC-RAS projects.
8. **HdfBase**: Provides base functionality for HDF file operations.
9. **HdfBndry**: Handles boundary-related data in HDF files.
10. **HdfMesh**: Manages mesh-related data in HDF files.
11. **HdfPlan**: Handles plan-related data in HDF files.
12. **HdfResultsMesh**: Processes mesh results from HDF files.
13. **HdfResultsPlan**: Handles plan results from HDF files.
14. **HdfResultsXsec**: Processes cross-section results from HDF files.
15. **HdfStruc**: Manages structure data in HDF files.
16. **HdfUtils**: Provides utility functions for HDF file operations.
17. **HdfXsec**: Handles cross-section data in HDF files.
18. **HdfPipe**: Handles pipe network related data in HDF files.
19. **HdfPump**: Manages pump station related data in HDF files.
20. **HdfFluvialPluvial**: Manages fluvial and pluvial related data in HDF files.
21. **RasToGo**: Functions to interface with USACE Go Consequences.
22. **RasMap**: Handling of RASMapper .rasmap files.
23. **RasGpt**: AI assistance tools
24. **HdfPlot & HdfResultsPlot**: Visualization utilities

## Best Practices

### 1. RAS Object Usage

- **Single Project Scripts**:
  - Use the global `ras` object for simplicity.
    ```python
    from ras_commander import ras, init_ras_project

    init_ras_project("/path/to/project", "6.5")
    # Use ras object for operations
    ```

- **Multiple Projects**:
  - Create separate `RasPrj` instances for each project.
    ```python
    from ras_commander import RasPrj, init_ras_project

    project1 = init_ras_project("/path/to/project1", "6.5", ras_instance=RasPrj())
    project2 = init_ras_project("/path/to/project2", "6.5", ras_instance=RasPrj())
    ```

- **Consistency**:
  - Avoid mixing global and custom RAS objects in the same script.

### 2. Plan Specification

- Use plan numbers as strings (e.g., `"01"`, `"02"`) for consistency.
  ```python
  RasCmdr.compute_plan("01")
  ```

- Check available plans before specifying plan numbers.
  ```python
  print(ras.plan_df)  # Displays available plans
  ```

### 3. Geometry Preprocessor Files

- Clear geometry preprocessor files before significant changes.
  ```python
  RasGeo.clear_geompre_files()
  ```

- Use `clear_geompre=True` for a clean computation environment.
  ```python
  RasCmdr.compute_plan("01", clear_geompre=True)
  ```

### 4. Parallel Execution

- Adjust `max_workers` and `num_cores` based on system capabilities.
  ```python
  RasCmdr.compute_parallel(max_workers=4, num_cores=2)
  ```

- Use `dest_folder` to organize outputs and prevent conflicts.
  ```python
  RasCmdr.compute_parallel(dest_folder="/path/to/results")
  ```

### 5. Error Handling and Logging

Proper error handling and logging are crucial for robust RAS Commander scripts. The library provides built-in logging functionality to help you track operations and diagnose issues.

1. **Logging Setup**:
   RAS Commander automatically sets up basic logging. You can adjust the log level:

   ```python
   import logging
   logging.getLogger('ras_commander').setLevel(logging.DEBUG)
   ```

2. **Using the @log_call Decorator**:
   The `@log_call` decorator automatically logs function calls:

   ```python
   from ras_commander.Decorators import log_call

   @log_call
   def my_function():
       # Function implementation
   ```

3. **Custom Logging**:
   For more detailed logging, use the logger directly:

   ```python
   from ras_commander.LoggingConfig import get_logger

   logger = get_logger(__name__)

   def my_function():
       logger.info("Starting operation")
       try:
           # Operation code
       except Exception as e:
           logger.error(f"Operation failed: {str(e)}")
   ```

4. **Error Handling Best Practices**:
   - Use specific exception types when possible.
   - Provide informative error messages.
   - Consider using custom exceptions for library-specific errors.

   ```python
   class RasCommanderError(Exception):
       pass

   def my_function():
       try:
           # Operation code
       except FileNotFoundError as e:
           raise RasCommanderError(f"Required file not found: {str(e)}")
       except ValueError as e:
           raise RasCommanderError(f"Invalid input: {str(e)}")
   ```

5. **Logging to File**:
   To save logs to a file, configure a file handler:

   ```python
   import logging
   from ras_commander.LoggingConfig import setup_logging

   setup_logging(log_file='ras_commander.log', log_level=logging.DEBUG)
   ```

### 6. File Path Handling

- Use `pathlib.Path` for robust file and directory operations.
  ```python
  from pathlib import Path
  project_path = Path("/path/to/project")
  ```

### 7. Type Hinting

- Apply type hints to improve code readability and IDE support.
  ```python
  from typing import Union, List, Optional
  def compute_plan(plan_number: str, clear_geompre: bool = False) -> bool:
      ...
  ```

## Usage Patterns

### Initializing a Project
```python
from ras_commander import init_ras_project, ras

init_ras_project("/path/to/project", "6.5")
print(f"Working with project: {ras.project_name}")
```

### Cloning a Plan

```python
from ras_commander import RasPlan

new_plan_number = RasPlan.clone_plan("01")
print(f"Created new plan: {new_plan_number}")
```


### Executing Plans

- **Single Plan Execution**:
  ```python
  from ras_commander import RasCmdr

  success = RasCmdr.compute_plan("01", num_cores=2)
  print(f"Plan execution {'successful' if success else 'failed'}")
  ```

- **Parallel Execution of Multiple Plans**:
  ```python
  from ras_commander import RasCmdr

  results = RasCmdr.compute_parallel(
      plan_number=["01", "02", "03"],
      max_workers=3,
      num_cores=4,
      dest_folder="/path/to/results",
      clear_geompre=True
  )

  for plan, success in results.items():
      print(f"Plan {plan}: {'Successful' if success else 'Failed'}")
  ```

### Working with Multiple Projects

```python
from ras_commander import RasPrj, init_ras_project, RasCmdr

# Initialize two separate projects
project1 = init_ras_project("/path/to/project1", "6.5", ras_instance=RasPrj())
project2 = init_ras_project("/path/to/project2", "6.5", ras_instance=RasPrj())

# Perform operations on each project
RasCmdr.compute_plan("01", ras_object=project1)
RasCmdr.compute_plan("02", ras_object=project2)

# Compare results
results1 = project1.get_hdf_entries()
results2 = project2.get_hdf_entries()
```

## Advanced Usage

### Working with HDF Files

RAS Commander provides extensive support for working with HDF files through various specialized classes. Here's an overview of key operations:

1. **Reading HDF Data**:
   Use `HdfUtils.get_hdf_paths_with_properties()` to explore the structure of an HDF file:

   ```python
   from ras_commander import HdfUtils

   hdf_paths = HdfUtils.get_hdf_paths_with_properties(hdf_path)
   print(hdf_paths)
   ```

2. **Extracting Mesh Results**:
   Use `HdfResultsMesh` to extract mesh-related results:

   ```python
   from ras_commander import HdfResultsMesh

   water_surface = HdfResultsMesh.mesh_timeseries_output(hdf_path, mesh_name, "Water Surface")
   print(water_surface)
   ```

3. **Working with Plan Results**:
   Use `HdfResultsPlan` for plan-specific results:

   ```python
   from ras_commander import HdfResultsPlan

   runtime_data = HdfResultsPlan.get_runtime_data(hdf_path)
   print(runtime_data)
   ```

4. **Cross-Section Results**:
   Extract cross-section data using `HdfResultsXsec`:

   ```python
   from ras_commander import HdfResultsXsec

   wsel_data = HdfResultsXsec.cross_sections_wsel(hdf_path)
   print(wsel_data)
   ```

These classes provide a high-level interface to HDF data, making it easier to extract and analyze HEC-RAS results programmatically.


### Working with Pipe Networks and Pump Stations


RAS Commander provides specialized classes for handling pipe networks and pump stations data from HEC-RAS HDF files.

1. **Pipe Network Operations**:
   Use `HdfPipe` to extract and analyze pipe network data:

   ```python
   from ras_commander import HdfPipe
   from pathlib import Path

   hdf_path = Path("path/to/your/hdf_file.hdf")

   # Extract pipe conduit data
   pipe_conduits = HdfPipe.get_pipe_conduits(hdf_path)
   print(pipe_conduits)

   # Extract pipe node data
   pipe_nodes = HdfPipe.get_pipe_nodes(hdf_path)
   print(pipe_nodes)

   # Get pipe network timeseries data
   water_surface = HdfPipe.get_pipe_network_timeseries(hdf_path, "Cell Water Surface")
   print(water_surface)

   # Get pipe network summary
   summary = HdfPipe.get_pipe_network_summary(hdf_path)
   print(summary)

   # Get pipe profile for a specific conduit
   profile = HdfPipe.get_pipe_profile(hdf_path, conduit_id=0)
   print(profile)
   ```


2. **Pump Station Operations**:


   Use `HdfPump` to work with pump station data:

   ```python
   from ras_commander import HdfPump
   from pathlib import Path

   hdf_path = Path("path/to/your/hdf_file.hdf")

   # Extract pump station data
   pump_stations = HdfPump.get_pump_stations(hdf_path)
   print(pump_stations)

   # Get pump group data
   pump_groups = HdfPump.get_pump_groups(hdf_path)
   print(pump_groups)

   # Get pump station timeseries data
   pump_data = HdfPump.get_pump_station_timeseries(hdf_path, "Pump Station 1")
   print(pump_data)

   # Get pump station summary
   summary = HdfPump.get_pump_station_summary(hdf_path)
   print(summary)

   # Get pump operation data
   operation_data = HdfPump.get_pump_operation_data(hdf_path, "Pump Station 1")
   print(operation_data)
   ```

These classes provide powerful tools for analyzing and visualizing pipe network and pump station data from HEC-RAS simulations. They allow you to easily access geometric information, time series data, and summary statistics for these hydraulic structures.






### Performance Optimization

Optimizing performance in RAS Commander involves balancing between execution speed and resource utilization. Here are detailed strategies:

1. **Parallel Execution**:
   Use `RasCmdr.compute_parallel()` for running multiple plans concurrently:

   ```python
   from ras_commander import RasCmdr

   results = RasCmdr.compute_parallel(
       plan_numbers=["01", "02", "03"],
       max_workers=3,
       num_cores=4
   )
   ```

   - Adjust `max_workers` based on the number of plans and available system resources.
   - Set `num_cores` to balance between single-plan performance and overall throughput.

2. **Geometry Preprocessing**:
   Preprocess geometry to avoid redundant calculations:

   ```python
   from ras_commander import RasPlan

   RasPlan.set_geom_preprocessor(plan_path, run_htab=1, use_ib_tables=0)
   ```

   - Set `run_htab=1` to force geometry preprocessing.
   - Use `use_ib_tables=0` to recompute internal boundary tables.

3. **Memory Management**:
   When working with large datasets, use chunking and iterative processing:

   ```python
   import dask.array as da
   from ras_commander import HdfResultsMesh

   data = HdfResultsMesh.mesh_timeseries_output(hdf_path, mesh_name, "Water Surface")
   dask_array = da.from_array(data.values, chunks=(1000, 1000))
   ```

4. **I/O Optimization**:
   Minimize disk I/O by batching read/write operations:

   ```python
   from ras_commander import HdfUtils

   with h5py.File(hdf_path, 'r') as hdf_file:
       datasets = HdfUtils.get_hdf_paths_with_properties(hdf_file)
       # Process multiple datasets in a single file open operation
   ```

5. **Profiling and Monitoring**:
   Use Python's built-in profiling tools to identify performance bottlenecks:

   ```python
   import cProfile

   cProfile.run('RasCmdr.compute_plan("01")')
   ```

By applying these strategies, you can significantly improve the performance of your RAS Commander scripts, especially when dealing with large projects or multiple simulations.

### Working with Boundary Conditions

RAS Commander provides powerful tools for managing and analyzing boundary conditions in HEC-RAS projects. Here's how to work effectively with boundary conditions:

1. **Accessing Boundary Conditions**:
   Use the `boundaries_df` attribute of the RasPrj object:

   ```python
   from ras_commander import init_ras_project

   project = init_ras_project("/path/to/project", "6.5")
   boundary_conditions = project.boundaries_df
   print(boundary_conditions)
   ```

2. **Filtering Boundary Conditions**:
   You can easily filter boundary conditions by type or location:

   ```python
   # Get all flow hydrographs
   flow_hydrographs = boundary_conditions[boundary_conditions['bc_type'] == 'Flow Hydrograph']

   # Get boundary conditions for a specific river
   river_boundaries = boundary_conditions[boundary_conditions['river_reach_name'] == 'Main River']
   ```

3. **Analyzing Boundary Condition Data**:
   Extract detailed information from boundary conditions:

   ```python
   for _, bc in flow_hydrographs.iterrows():
       print(f"River: {bc['river_reach_name']}")
       print(f"Station: {bc['river_station']}")
       print(f"Number of values: {bc['hydrograph_num_values']}")
       print("---")
   ```

4. **Modifying Boundary Conditions**:
   While direct modification of boundary conditions is not supported, you can use RAS Commander to update unsteady flow files:

   ```python
   from ras_commander import RasUnsteady

   RasUnsteady.update_unsteady_parameters(unsteady_file_path, {"Parameter1": "NewValue1"})
   ```

5. **Visualizing Boundary Conditions**:
   Use pandas and matplotlib to visualize boundary condition data:

   ```python
   import matplotlib.pyplot as plt

   bc = flow_hydrographs.iloc[0]
   plt.plot(bc['hydrograph_values'])
   plt.title(f"Flow Hydrograph: {bc['name']}")
   plt.xlabel("Time Step")
   plt.ylabel("Flow")
   plt.show()
   ```

By leveraging these capabilities, you can effectively analyze and manage boundary conditions in your HEC-RAS projects using RAS Commander.

### Advanced Data Processing with RasUtils

RasUtils provides a set of powerful tools for data processing and analysis. Here are some advanced techniques:

1. **Data Conversion**:
   Convert various data sources to pandas DataFrames:

   ```python
   from ras_commander import RasUtils
   from pathlib import Path

   csv_data = RasUtils.convert_to_dataframe(Path("results.csv"))
   excel_data = RasUtils.convert_to_dataframe(Path("data.xlsx"), sheet_name="Sheet1")
   ```

2. **Statistical Analysis**:
   Perform statistical calculations on simulation results:

   ```python
   import numpy as np

   observed = np.array([100, 120, 140, 160, 180])
   predicted = np.array([105, 125, 135, 165, 175])

   rmse = RasUtils.calculate_rmse(observed, predicted)
   percent_bias = RasUtils.calculate_percent_bias(observed, predicted, as_percentage=True)
   metrics = RasUtils.calculate_error_metrics(observed, predicted)

   print(f"RMSE: {rmse}")
   print(f"Percent Bias: {percent_bias}%")
   print(f"Metrics: {metrics}")
   ```

3. **Spatial Operations**:
   Perform spatial queries and nearest neighbor searches:

   ```python
   import numpy as np

   points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
   query_points = np.array([[0.5, 0.5], [5, 5]])

   nearest = RasUtils.perform_kdtree_query(points, query_points)
   neighbors = RasUtils.find_nearest_neighbors(points)

   print(f"Nearest points: {nearest}")
   print(f"Neighbors: {neighbors}")
   ```

4. **Data Consolidation**:
   Consolidate and pivot complex datasets:

   ```python
   import pandas as pd

   df = pd.DataFrame({'A': [1, 1, 2], 'B': [4, 5, 6], 'C': [7, 8, 9]})
   consolidated = RasUtils.consolidate_dataframe(df, group_by='A', aggregation_method='list')

   print(consolidated)
   ```

5. **File Operations**:
   Perform advanced file operations with built-in error handling:

   ```python
   from pathlib import Path

   directory = Path("output")
   RasUtils.create_directory(directory)

   file_path = directory / "data.txt"
   RasUtils.check_file_access(file_path, mode='w')

   # Perform file operation here

   RasUtils.remove_with_retry(file_path, is_folder=False)
   ```

By utilizing these advanced data processing capabilities of RasUtils, you can efficiently handle complex data manipulation tasks in your RAS Commander workflows.

### RasExamples

The `RasExamples` class provides functionality for managing HEC-RAS example projects. This is particularly useful for testing, learning, and development purposes.

#### Key Concepts

- **Example Project Management**: Access and manipulate example projects.
- **Automatic Downloading and Extraction**: Fetches projects from official sources.
- **Project Categorization**: Organizes projects into categories for easy navigation.

#### Usage Patterns

```python
from ras_commander import RasExamples

# Initialize RasExamples
ras_examples = RasExamples()

# Download example projects (if not already present)
ras_examples.get_example_projects()

# List available categories
categories = ras_examples.list_categories()
print(f"Available categories: {categories}")

# List projects in a specific category
steady_flow_projects = ras_examples.list_projects("Steady Flow")
print(f"Steady Flow projects: {steady_flow_projects}")

# Extract specific projects
extracted_paths = ras_examples.extract_project(["Bald Eagle Creek", "Muncie"])
for path in extracted_paths:
    print(f"Extracted project to: {path}")

# Clean up extracted projects when done
ras_examples.clean_projects_directory()
```

## RasHdf

The `RasHdf` class provides utilities for working with HDF (Hierarchical Data Format) files in HEC-RAS projects. HDF files are commonly used in HEC-RAS for storing large datasets and simulation results.

### Key Features of `RasHdf`:

1. **Reading HDF Tables**: Convert HDF5 datasets to pandas DataFrames.
2. **Writing DataFrames to HDF**: Save pandas DataFrames as HDF5 datasets.
3. **Spatial Operations**: Perform KDTree queries and find nearest neighbors.
4. **Data Consolidation**: Merge duplicate values in DataFrames.
5. **Byte String Handling**: Decode byte strings in DataFrames.

### Example Usage:

```python
from ras_commander import RasHdf
import h5py
import pandas as pd

# Read an HDF table
with h5py.File('results.hdf', 'r') as f:
    dataset = f['water_surface_elevations']
    df = RasHdf.read_hdf_to_dataframe(dataset)

print(df.head())

# Save a DataFrame to HDF
new_data = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})
with h5py.File('new_results.hdf', 'w') as f:
    group = f.create_group('my_results')
    RasHdf.save_dataframe_to_hdf(new_data, group, 'my_dataset')

# Perform a KDTree query
import numpy as np
reference_points = np.array([[0, 0], [1, 1], [2, 2]])
query_points = np.array([[0.5, 0.5], [1.5, 1.5]])
results = RasHdf.perform_kdtree_query(reference_points, query_points)
print("KDTree query results:", results)
```

## Optimizing Parallel Execution with RAS Commander

Efficient parallel execution is crucial for maximizing the performance of HEC-RAS simulations, especially when dealing with multiple plans or large models. RAS Commander offers several strategies for optimizing parallel execution based on your specific needs and system resources.

### Strategy 1: Efficiency Mode for Multiple Plans

This strategy maximizes overall throughput and efficiency when running multiple plans, although individual plan turnaround times may be longer.

**Key Points:**
- Use 2 real cores per plan
- Utilize only physical cores, not hyperthreaded cores

**Example:**
```python
from ras_commander import RasCmdr

# Assuming 8 physical cores on the system
RasCmdr.compute_parallel(
    plan_numbers=["01", "02", "03", "04"],
    max_workers=4,  # 8 cores / 2 cores per plan
    num_cores=2
)
```

### Strategy 2: Performance Mode for Single Plans

This strategy maximizes single plan performance by using more cores. It results in less overall efficiency but shortens single plan runtime, making it optimal for situations where individual plan performance is critical.

**Key Points:**
- Use 8-16 cores per plan, depending on system capabilities
- Suitable for running a single plan or a small number of high-priority plans

**Example:**
```python
from ras_commander import RasCmdr

RasCmdr.compute_plan(
    plan_number="01",
    num_cores=12  # Adjust based on your system's capabilities
)
```

### Strategy 3: Background Run Operation

This strategy balances performance and system resource usage, allowing for other operations to be performed concurrently.

**Key Points:**
- Limit total core usage to 50-80% of physical cores
- Combines aspects of Strategies 1 and 2
- Allows overhead for user to complete other operations while calculations are running

**Example:**
```python
import psutil
from ras_commander import RasCmdr

physical_cores = psutil.cpu_count(logical=False)
max_cores_to_use = int(physical_cores * 0.7)  # Using 70% of physical cores

RasCmdr.compute_parallel(
    plan_numbers=["01", "02", "03"],
    max_workers=max_cores_to_use // 2,
    num_cores=2
)
```

### Optimizing Geometry Preprocessing

To avoid repeated geometry preprocessing for each run, follow these steps:

1. **Preprocess Geometry:**
   ```python
   from ras_commander import RasPlan
   
   # For each plan you want to preprocess
   RasPlan.update_plan_value(plan_number, "Run HTab", 1)
   RasPlan.update_plan_value(plan_number, "Run UNet", -1)
   RasPlan.update_plan_value(plan_number, "Run PostProcess", -1)
   RasPlan.update_plan_value(plan_number, "Run RASMapper", -1)
   
   # Run the plan to preprocess geometry
   RasCmdr.compute_plan(plan_number)
   ```

2. **Run Simulations:**
   After preprocessing, update the flags for actual simulations:
   ```python
   RasPlan.update_plan_value(plan_number, "Run HTab", -1)
   RasPlan.update_plan_value(plan_number, "Run UNet", 1)
   RasPlan.update_plan_value(plan_number, "Run PostProcess", 1)
   RasPlan.update_plan_value(plan_number, "Run RASMapper", 0)
   ```

This approach preprocesses the geometry once, preventing redundant preprocessing when multiple plans use the same geometry.

### Best Practices for Parallel Execution

- **Balance Cores:** Find the right balance between the number of parallel plans and cores per plan based on your system's capabilities.
- **Consider I/O Operations:** Be aware that disk I/O can become a bottleneck in highly parallel operations.
- **Test and Iterate:** Experiment with different configurations to find the optimal setup for your specific models and system.

By leveraging these strategies and best practices, you can significantly improve the performance and efficiency of your HEC-RAS simulations using RAS Commander.

## Approaching Your End User Needs with Ras Commander

### Understanding Data Sources and Strategies

RAS Commander is designed to work efficiently with HEC-RAS projects by focusing on easily accessible data sources. This approach allows for powerful automation while avoiding some of the complexities inherent in HEC-RAS data management. Here's what you need to know:

1. **Data Sources in HEC-RAS Projects**:
   - ASCII input files (plan files, unsteady files, boundary conditions)
   - DSS (Data Storage System) files for inputs
   - HDF (Hierarchical Data Format) files for outputs

2. **RAS Commander's Focus**:
   - Primarily works with plain text inputs and HDF outputs
   - Avoids direct manipulation of DSS files due to their complexity

3. **Strategy for Handling DSS Inputs**:
   - Run the plan or preprocess geometry and event conditions
   - Access the resulting HDF tables, which contain the DSS inputs in an accessible format
   - Define time series directly in the ASCII file instead of as DSS inputs

4. **Accessing Project Data**:
   - Basic project data is loaded from ASCII text files by the RasPrj routines
   - Plan details are available in the HDF file
   - Geometry data is in the dynamically generated geometry HDF file

### Working with RAS Commander

1. **Initialization and Data Loading**:
   - Use `init_ras_project()` to load project data from ASCII files
   - Access plan information from HDF files using provided functions

2. **Handling Geometry Data**:
   - Geometry data is dynamically generated in HDF format
   - Focus on working with the HDF geometry data rather than plain text editing

3. **Workflow for Complex Operations**:
   - Perform the desired operation manually once
   - Provide an example to RAS Commander's AI GPT of what you're changing and why
   - Use this example to develop project-specific functions and code

4. **Example: Replacing DSS-defined Boundary Conditions**:
   - Open the data in HDF View
   - Extract the relevant dataset
   - Manually enter the time series based on the HDF dataset
   - Verify the model works with this change
   - Use this example to create an automated function for similar operations

### Best Practices

1. **Understanding Your Data**:
   - Familiarize yourself with the structure of your HEC-RAS project
   - Identify which data is stored in ASCII, DSS, and HDF formats

2. **Leveraging HDF Outputs**:
   - Whenever possible, use HDF outputs for data analysis and manipulation
   - This approach provides easy access to data without DSS complexities

3. **Iterative Development**:
   - Start with manual operations to understand the process
   - Gradually automate these processes using RAS Commander functions
   - Always check with the HEC-RAS GUI to verify the changes before finalizing the automation

4. **Documentation**:
   - Keep detailed notes on your workflow and changes
   - This documentation will be invaluable for creating automated processes

5. **Flexibility**:
   - Be prepared to adapt your approach based on specific project needs
   - RAS Commander provides a framework, but project-specific solutions will always require custom scripting
   - With an AI assistant, you can quickly leverage this library or your own custom functions to automate your workflows.

By following these strategies and best practices, you can effectively use RAS Commander to automate and streamline your HEC-RAS workflows, working around limitations and leveraging the strengths of the library's approach to data management.

## Troubleshooting

### 1. Project Initialization Issues

- **Ensure Correct Paths**: Verify that the project path is accurate and the `.prj` file exists.
- **HEC-RAS Version**: Confirm that the specified HEC-RAS version is installed on your system.

### 2. Execution Failures

- **File Existence**: Check that all referenced plan, geometry, and flow files exist.
- **Executable Path**: Ensure the HEC-RAS executable path is correctly set.
- **Log Files**: Review HEC-RAS log files for specific error messages.

### 3. Parallel Execution Problems

- **Resource Allocation**: Reduce `max_workers` if encountering memory issues.
- **System Capabilities**: Adjust `num_cores` based on your system's capacity.
- **Clean Environment**: Use `clear_geompre=True` to prevent conflicts.

### 4. File Access Errors

- **Permissions**: Verify read/write permissions for the project directory.
- **File Locks**: Close any open HEC-RAS instances that might lock files.

### 5. Inconsistent Results

- **Geometry Files**: Clear geometry preprocessor files when making changes.
- **Plan Parameters**: Ensure all plan parameters are correctly set before execution.

### 6. Infrastructure Analysis Issues
- Verify network connectivity in pipe systems
- Check pump station configurations
- Validate time series data consistency

### 7. HDF File Problems
- Check file permissions and access
- Verify HDF structure using HdfUtils
- Use proper file type specification with @standardize_input

### 8. Performance Optimization
- Monitor system resources during parallel execution
- Balance worker count with system capabilities
- Use appropriate chunking for large datasets

## Conclusion

The RAS-Commander (`ras_commander`) library provides a powerful set of tools for automating HEC-RAS operations. By following the best practices outlined in this guide and leveraging the library's features, you can efficiently manage and execute complex HEC-RAS projects programmatically.

Remember to refer to the latest documentation and the library's source code for up-to-date information. As you become more familiar with `ras_commander`, you'll discover more ways to optimize your HEC-RAS workflows and increase productivity.

For further assistance, bug reports, or feature requests, please refer to the library's [GitHub repository](https://github.com/billk-FM/ras-commander) and issue tracker.

**Happy Modeling!**








**Note on Module Naming Convention:**
While the library now uses capitalized names for the `Decorators.py` and `LoggingConfig.py` modules, it's worth noting that this deviates from the PEP 8 style guide, which recommends lowercase names for modules. Future versions of the library may revert to lowercase naming for consistency with Python conventions. Users should be aware of this potential change in future updates.
==================================================

Folder: c:\GH\ras-commander\examples
==================================================

File: c:\GH\ras-commander\LICENSE
==================================================
MIT License

Copyright (c) 2024 William M. Katzenmeyer

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so.

==================================================

File: c:\GH\ras-commander\pyproject.toml
==================================================
[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta:__legacy__"

==================================================

Folder: c:\GH\ras-commander\ras_commander
==================================================

File: c:\GH\ras-commander\README.md
==================================================
# RAS Commander (ras-commander)

RAS Commander is a Python library for automating HEC-RAS operations, providing a set of tools to interact with HEC-RAS project files, execute simulations, and manage project data. This library is an evolution of the RASCommander 1.0 Python Notebook Application previously released under the [HEC-Commander tools repository](https://github.com/billk-FM/HEC-Commander).

## Contributors:
William Katzenmeyer, P.E., C.F.M. 

Sean Micek, P.E., C.F.M. 

Aaron Nichols, P.E., C.F.M. 

(Additional Contributors Here)  

## Don't Ask Me, Ask ChatGPT!

Before you read any further, you can [chat directly with ChatGPT on this topic.](https://chatgpt.com/g/g-TZRPR3oAO-ras-commander-library-assistant)  Ask it anything, and it will use its tools to answer your questions and help you learn.  You can even upload your own plan, unsteady and HDF files to inspect and help determine how to automate your workflows or visualize your results. 

There are also [AI Assistant Knowledge Bases](https://github.com/billk-FM/ras-commander/tree/main/ai_tools/assistant_knowledge_bases) with various versions available to directly use with large context LLM models such as Anthropic's Claude, Google Gemini and OpenAI's GPT4o and o1 models.  

FUTURE:  TEMPLATES are available to use with AI Assistant Notebooks to build your own automation tools.  When used with large context models, these templates allow you to ask GPT to build a workflow from scratch to automate your projects. 

## Background
The ras-commander library emerged from the initial test-bed of AI-driven coding represented by the HEC-Commander tools Python notebooks. These notebooks served as a proof of concept, demonstrating the value proposition of automating HEC-RAS operations. The transition from notebooks to a structured library aims to provide a more robust, maintainable, and extensible solution for water resources engineers.

## Features

- Automate HEC-RAS project management and simulations
- Support for both single and multiple project instances
- Parallel execution of HEC-RAS plans
- Utilities for managing geometry, plan, and unsteady flow files
- Example project management for testing and development
- Two primary operation modes: "Run Missing" and "Build from DSS"

## AI-Driven Coding Experience

ras-commander provides several AI-powered tools to enhance the coding experience:

1. **ChatGPT Assistant: [RAS Commander Library Assistant](https://chatgpt.com/g/g-TZRPR3oAO-ras-commander-library-assistant)**: A specialized GPT model trained on the ras-commander codebase, available for answering queries and providing code suggestions.

2. **[Purpose-Built Knowledge Base Summaries](https://github.com/billk-FM/ras-commander/tree/main/ai_tools/assistant_knowledge_bases)**: Up-to-date compilations of the documentation and codebase for use with large language models like Claude or GPT-4. Look in 'ai_tools/assistant_knowledge_bases/' in the repo.

3. **[Cursor IDE Integration](https://github.com/billk-FM/ras-commander/blob/main/.cursorrules)**: Custom rules for the Cursor IDE to provide context-aware suggestions and documentation.  Just open the repository folder in Cursor.  You can create your own folders "/workspace/, "/projects/", or "my_projects/" as these are already in the .gitignore, and place your custom scripts there for your projects.  This will allow easy referencing of the ras-commander documents and individual repo files, the automatic loading of the .cursorrules file.  Alternatvely, download the github repo into your projects folder to easily load documents and use cursor rules files.  
4. **[AI Assistant Notebook](https://github.com/billk-FM/ras-commander/blob/main/ai_tools/rascommander_code_assistant.ipynb)**: A notebook for dynamic code summarization and API interaction (bring your own API Key).  Currently, this only does a single-shot message on the Claude Sonnet 3.5 API, which can be up to 50 cents per request.  Future revisions will include the ability to select which knowledge base file to include, a choice of SOTA models + multi turn conversations to build automation notebooks interactively.  

These tools aim to streamline development and provide intelligent assistance when modeling with, and working with and revising the ras-commander library.

## Installation

Create a virtual environment with conda or venv (ask ChatGPT if you need help)

In your virtual environment, install ras-commander using pip:
```
pip install h5py numpy pandas requests tqdm scipy xarray geopandas matplotlib ras-commander ipython psutil shapely fiona pathlib rtree rasterstats
pip install --upgrade ras-commander
```

**Tested with Python 3.11**


If you have dependency issues with pip (especially if you have errors with numpy), try clearing your local pip packages 'C:\Users\your_username\AppData\Roaming\Python\' and then creating a new virtual environment.  
   

## Requirements

- Tested with Python 3.11
- HEC-RAS 6.2 or later (other versions may work, all testing was done with version 6.2 and above)
- Detailed project workflows and/or existing libraries and code where ras-commander can be integrated.

For a full list of dependencies, see the `requirements.txt` file.

## Quick Start
```
from ras_commander import init_ras_project, RasCmdr, RasPlan
```

# Initialize a project
```
init_ras_project(r"/path/to/project", "6.5")
```

# Execute a single plan
```
RasCmdr.compute_plan("01", dest_folder=r"/path/to/results", overwrite_dest=True)
```

# Execute plans in parallel
```
results = RasCmdr.compute_parallel(
    plan_numbers=["01", "02"],
    max_workers=2,
    cores_per_run=2,
    dest_folder=r"/path/to/results",
    overwrite_dest=True
)
```

# Modify a plan
```
RasPlan.set_geom("01", "02")
```

Certainly! I'll provide you with an updated Key Components section and Project Organization diagram based on the current structure of the ras-commander library.

## Key Components

- `RasPrj`: Manages HEC-RAS projects, handling initialization and data loading
- `RasCmdr`: Handles execution of HEC-RAS simulations
- `RasPlan`: Provides functions for modifying and updating plan files
- `RasGeo`: Handles operations related to geometry files
- `RasUnsteady`: Manages unsteady flow file operations
- `RasUtils`: Contains utility functions for file operations and data management
- `RasExamples`: Manages and loads HEC-RAS example projects

### New Components:
- `HdfBase`: Core functionality for HDF file operations
- `HdfBndry`: Enhanced boundary condition handling
- `HdfMesh`: Comprehensive mesh data management
- `HdfPlan`: Plan data extraction and analysis
- `HdfResultsMesh`: Advanced mesh results processing
- `HdfResultsPlan`: Plan results analysis
- `HdfResultsXsec`: Cross-section results processing
- `HdfStruc`: Structure data management
- `HdfPipe`: Pipe network analysis tools
- `HdfPump`: Pump station analysis capabilities
- `HdfFluvialPluvial`: Fluvial-pluvial boundary analysis
- `RasMapper`: RASMapper interface
- `RasToGo`: Go-Consequences integration
- `HdfPlot` & `HdfResultsPlot`: Specialized plotting utilities

## Project Organization Diagram

## Project Organization Diagram

```
ras_commander
├── ras_commander
│   ├── __init__.py
│   ├── _version.py
│   ├── Decorators.py
│   ├── LoggingConfig.py
│   ├── RasCmdr.py
│   ├── RasExamples.py
│   ├── RasGeo.py
│   ├── RasPlan.py
│   ├── RasPrj.py
│   ├── RasUnsteady.py
│   ├── RasUtils.py
│   ├── RasToGo.py
│   ├── RasGpt.py
│   ├── HdfBase.py
│   ├── HdfBndry.py
│   ├── HdfMesh.py
│   ├── HdfPlan.py
│   ├── HdfResultsMesh.py
│   ├── HdfResultsPlan.py
│   ├── HdfResultsXsec.py
│   ├── HdfStruc.py
│   ├── HdfPipe.py
│   ├── HdfPump.py
│   ├── HdfFluvialPluvial.py
│   ├── HdfPlot.py
│   └── HdfResultsPlot.py
├── examples
│   ├── 01_project_initialization.py
│   ├── 02_plan_operations.py
│   ├── 03_geometry_operations.py
│   ├── 04_unsteady_flow_operations.py
│   ├── 05_utility_functions.py
│   ├── 06_single_plan_execution.py
│   ├── 07_sequential_plan_execution.py
│   ├── 08_parallel_execution.py
│   ├── 09_specifying_plans.py
│   ├── 10_arguments_for_compute.py
│   ├── 11_Using_RasExamples.ipynb
│   ├── 12_plan_set_execution.py
│   ├── 13_multiple_project_operations.py
│   ├── 14_Core_Sensitivity.ipynb
│   ├── 15_plan_key_operations.py
│   ├── 16_scanning_ras_project_info.py
│   ├── 17_parallel_execution_ble.py
│   └── HEC_RAS_2D_HDF_Analysis.ipynb
├── tests
│   └── ... (test files)
├── .gitignore
├── LICENSE
├── README.md
├── STYLE_GUIDE.md
├── Comprehensive_Library_Guide.md
├── pyproject.toml
├── setup.cfg
├── setup.py
└── requirements.txt
```

## Accessing HEC Examples through RasExamples

The `RasExamples` class provides functionality for quickly loading and managing HEC-RAS example projects. This is particularly useful for testing and development purposes.

Key features:
- Download and extract HEC-RAS example projects
- List available project categories and projects
- Extract specific projects for use
- Manage example project data efficiently

Example usage:
from ras_commander import RasExamples

```
ras_examples = RasExamples()
ras_examples.get_example_projects()  # Downloads example projects if not already present
categories = ras_examples.list_categories()
projects = ras_examples.list_projects("Steady Flow")
extracted_paths = ras_examples.extract_project(["Bald Eagle Creek", "Muncie"])
```

## RasPrj

The `RasPrj` class is central to managing HEC-RAS projects within the ras-commander library. It handles project initialization, data loading, and provides access to project components.

Key features:
- Initialize HEC-RAS projects
- Load and manage project data (plans, geometries, flows, etc.)
- Provide easy access to project files and information

Note: While a global `ras` object is available for convenience, you can create multiple `RasPrj` instances to manage several projects simultaneously.

Example usage:
```
from ras_commander import RasPrj, init_ras_project
```

### Using the global ras object
```
init_ras_project("/path/to/project", "6.5")
```

### Creating a custom RasPrj instance
```
custom_project = RasPrj()
init_ras_project("/path/to/another_project", "6.5", ras_instance=custom_project)
```

## RasHdf

The `RasHdf` class provides utilities for working with HDF files in HEC-RAS projects, enabling easy access to simulation results and model data.

Example usage:

```python
from ras_commander import RasHdf, init_ras_project, RasPrj

# Initialize project with a custom ras object
custom_ras = RasPrj()
init_ras_project("/path/to/project", "6.5", ras_instance=custom_ras)

# Get runtime data for a specific plan
plan_number = "01"
runtime_data = RasHdf.get_runtime_data(plan_number, ras_object=custom_ras)
print(runtime_data)
```
This class simplifies the process of extracting and analyzing data from HEC-RAS HDF output files, supporting tasks such as post-processing and result visualization.


### Infrastructure Analysis
```python
from ras_commander import HdfPipe, HdfPump

# Analyze pipe network
pipe_network = HdfPipe.get_pipe_network(hdf_path)
conduits = HdfPipe.get_pipe_conduits(hdf_path)

# Analyze pump stations
pump_stations = HdfPump.get_pump_stations(hdf_path)
pump_performance = HdfPump.get_pump_station_summary(hdf_path)
```

### Advanced Results Analysis
```python
from ras_commander import HdfResultsMesh

# Get maximum water surface and velocity
max_ws = HdfResultsMesh.get_mesh_max_ws(hdf_path)
max_vel = HdfResultsMesh.get_mesh_max_face_v(hdf_path)

# Visualize results
from ras_commander import HdfResultsPlot
HdfResultsPlot.plot_results_max_wsel(max_ws)
```

### Fluvial-Pluvial Analysis
```python
from ras_commander import HdfFluvialPluvial

boundary = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(
    hdf_path,
    delta_t=12  # Time threshold in hours
)


## Documentation

For detailed usage instructions and API documentation, please refer to the [Comprehensive Library Guide](Comprehensive_Library_Guide.md).

## Examples

Check out the `examples/` directory for sample scripts demonstrating various features of ras-commander.

## Future Development

The ras-commander library is an ongoing project. Future plans include:
- Integration of more advanced AI-driven features
- Expansion of HMS and DSS functionalities
- Enhanced GPU support for computational tasks
- Community-driven development of new modules and features

## Related Resources

- [HEC-Commander Blog](https://github.com/billk-FM/HEC-Commander/tree/main/Blog)
- [GPT-Commander YouTube Channel](https://www.youtube.com/@GPT_Commander)
- [ChatGPT Examples for Water Resources Engineers](https://github.com/billk-FM/HEC-Commander/tree/main/ChatGPT%20Examples)


## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to submit pull requests, report issues, and suggest improvements.

## Style Guide

This project follows a specific style guide to maintain consistency across the codebase. Please refer to the [Style Guide](STYLE_GUIDE.md) for details on coding conventions, documentation standards, and best practices.

## License

ras-commander is released under the MIT License. See the license file for details.

## Acknowledgments

RAS Commander is based on the HEC-Commander project's "Command Line is All You Need" approach, leveraging the HEC-RAS command-line interface for automation. The initial development of this library was presented in the HEC-Commander Tools repository. In a 2024 Australian Water School webinar, Bill demonstrated the derivation of basic HEC-RAS automation functions from plain language instructions. Leveraging the previously developed code and AI tools, the library was created. The primary tools used for this initial development were Anthropic's Claude, GPT-4, Google's Gemini Experimental models, and the Cursor AI Coding IDE.

Additionally, we would like to acknowledge the following notable contributions and attributions for open source projects which significantly influenced the development of RAS Commander:

1. Contributions: Sean Micek's [`funkshuns`](https://github.com/openSourcerer9000/funkshuns), [`TXTure`](https://github.com/openSourcerer9000/TXTure), and [`RASmatazz`](https://github.com/openSourcerer9000/RASmatazz) libraries provided inspiration, code examples and utility functions which were adapted with AI for use in RAS Commander. Sean has also contributed heavily to 

- Development of additional HDF functions for detailed analysis and mapping of HEC-RAS results within the RasHdf class.
- Development of the prototype `RasCmdr` class for executing HEC-RAS simulations.
- Optimization examples and methods from (INSERT REFERENCE) for use in the Ras-Commander library examples

2. Attribution: The [`pyHMT2D`](https://github.com/psu-efd/pyHMT2D/) project by Xiaofeng Liu, which provided insights into HDF file handling methods for HEC-RAS outputs.  Many of the functions in the [Ras_2D_Data.py](https://github.com/psu-efd/pyHMT2D/blob/main/pyHMT2D/Hydraulic_Models_Data/RAS_2D/RAS_2D_Data.py) file were adapted with AI for use in RAS Commander. 

   Xiaofeng Liu, Ph.D., P.E.,    Associate Professor, Department of Civil and Environmental Engineering
   Institute of Computational and Data Sciences, Penn State University

These acknowledgments recognize the contributions and inspirations that have helped shape RAS Commander, ensuring proper attribution for the ideas and code that have influenced its development.

3. Chris Goodell, "Breaking the HEC-RAS Code" - Studied and used as a reference for understanding the inner workings of HEC-RAS, providing valuable insights into the software's functionality and structure.

4. [HEC-Commander Tools](https://github.com/billk-FM/HEC-Commander) - Inspiration and initial code base for the development of RAS Commander.


## Official RAS Commander AI-Generated Songs:

[No More Wait and See (Bluegrass)](https://suno.com/song/16889f3e-50f1-4afe-b779-a41738d7617a)  

[No More Wait and See (Cajun Zydeco)](https://suno.com/song/4441c45d-f6cd-47b9-8fbc-1f7b277ee8ed)  


## Contact

For questions, suggestions, or support, please contact:
William Katzenmeyer, P.E., C.F.M. - billk@fenstermaker.com

==================================================

File: c:\GH\ras-commander\requirements.txt
==================================================
aider-chat @ git+https://github.com/paul-gauthier/aider.git@00d5348ee6295662c78a8ece31d71632145d9746
alabaster==0.7.16
annotated-types==0.6.0
anyio==3.7.1
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work
attrs==23.1.0
babel==2.16.0
backoff==2.2.1
backports.tarfile==1.2.0
black==24.8.0
boto3==1.35.25
botocore==1.35.25
certifi==2023.11.17
cffi==1.16.0
charset-normalizer==3.3.2
click==8.1.7
colorama==0.4.6
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1710320294760/work
ConfigArgParse==1.7
contourpy==1.3.0
cycler==0.12.1
debugpy @ file:///D:/bld/debugpy_1725269345345/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
diff-match-patch==20230430
diskcache==5.6.3
distro==1.8.0
docutils==0.20.1
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1720869315914/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1725214404607/work
flake8==7.1.1
fonttools==4.53.1
geopandas==1.0.1
gitdb==4.0.11
GitPython==3.1.40
grep-ast==0.2.4
h11==0.14.0
h5py==3.11.0
httpcore==1.0.2
idna==3.6
imagesize==1.4.1
importlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1726082825846/work
iniconfig==2.0.0
ipykernel @ file:///D:/bld/ipykernel_1719845595208/work
ipython @ file:///D:/bld/ipython_1725050320818/work
jaraco.classes==3.4.0
jaraco.context==6.0.1
jaraco.functools==4.1.0
jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work
Jinja2==3.1.4
jmespath==1.0.1
jsonschema==4.20.0
jsonschema-specifications==2023.11.2
jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1726610684920/work
jupyter_core @ file:///D:/bld/jupyter_core_1710257313664/work
keyring==25.4.1
kiwisolver==1.4.7
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.9.2
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1713250518406/work
mccabe==0.7.0
mdurl==0.1.2
more-itertools==10.5.0
mypy-extensions==1.0.0
nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1705850609492/work
networkx==3.2.1
nh3==0.2.18
numpy==1.26.2
packaging==23.2
pandas==2.2.3
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1712320355065/work
pathlib==1.0.1
pathspec==0.11.2
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pillow==10.4.0
pkginfo==1.10.0
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1726613481435/work
pluggy==1.5.0
prompt-toolkit==3.0.41
psutil @ file:///D:/bld/psutil_1725737996000/work
pure_eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1721585709575/work
pycodestyle==2.12.1
pycparser==2.21
pydantic==2.5.2
pydantic_core==2.14.5
pyflakes==3.2.0
Pygments==2.17.2
pyogrio==0.9.0
pyparsing==3.1.4
pyproj==3.6.1
pytest==8.3.3
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work
pytz==2024.2
pywin32==306
pywin32-ctypes==0.2.3
PyYAML==6.0.1
pyzmq @ file:///D:/bld/pyzmq_1725449086441/work
readme_renderer==43.0
referencing==0.31.1
regex==2023.10.3
requests==2.32.3
requests-toolbelt==1.0.0
rfc3986==2.0.0
rich==13.7.0
rpds-py==0.13.2
s3transfer==0.10.2
scipy==1.11.4
shapely==2.0.6
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smmap==5.0.1
sniffio==1.3.0
snowballstemmer==2.2.0
sounddevice==0.4.6
soundfile==0.12.1
Sphinx==7.4.7
sphinx-rtd-theme==2.0.0
sphinxcontrib-applehelp==2.0.0
sphinxcontrib-devhelp==2.0.0
sphinxcontrib-htmlhelp==2.1.0
sphinxcontrib-jquery==4.1
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==2.0.0
sphinxcontrib-serializinghtml==2.0.0
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
tornado @ file:///D:/bld/tornado_1724956185692/work
tqdm==4.66.1
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1713535121073/work
tree-sitter==0.20.4
tree-sitter-languages==1.8.0
twine==5.1.1
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1717802530399/work
tzdata==2024.1
urllib3==2.2.3
wcwidth==0.2.12
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1726248574750/work

==================================================

File: c:\GH\ras-commander\settings.db
==================================================
SQLite format 3   @                                                                     .v  n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N)eindexix_settings_idsettingsCREATE INDEX ix_settings_id ON settings (id)VtablesettingssettingsCREATE TABLE settings (
	id VARCHAR NOT NULL, 
	anthropic_api_key TEXT, 
	openai_api_key TEXT, 
	selected_model VARCHAR, 
	context_mode VARCHAR, 
	omit_folders TEXT, 
	omit_extensions TEXT, 
	omit_files TEXT, 
	chunk_level VARCHAR, 
	initial_chunk_size INTEGER, 
	followup_chunk_size INTEGER, 
	PRIMARY KEY (id)
)/C indexsqlite_autoindex_settings_1settings                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
==================================================

File: c:\GH\ras-commander\setup.py
==================================================
from setuptools import setup, find_packages
from setuptools.command.build_py import build_py
import subprocess
from pathlib import Path

class CustomBuildPy(build_py):
    def run:
    """Docs only, see 'run.py' for full function code"""
)

"""
ras-commander setup.py

This file is used to build and publish the ras-commander package to PyPI.

To build and publish this package, follow these steps:

1. Ensure you have the latest versions of setuptools, wheel, and twine installed:
   pip install --upgrade setuptools wheel twine

2. Update the version number in ras_commander/__init__.py (if not using automatic versioning)

3. Create source distribution and wheel:
   python setup.py sdist bdist_wheel

4. Check the distribution:
   twine check dist/*

5. Upload to Test PyPI (optional):
   twine upload --repository testpypi dist/*

6. Install from Test PyPI to verify (optional):
   pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple ras-commander

7. Upload to PyPI:
   twine upload dist/* --username __token__ --password <your_api_key>


8. Install from PyPI to verify:
   pip install ras-commander

Note: Ensure you have the necessary credentials and access rights to upload to PyPI.
For more information, visit: https://packaging.python.org/tutorials/packaging-projects/

"""

==================================================

File: c:\GH\ras-commander\STYLE_GUIDE.md
==================================================
# RAS Commander (ras-commander) Style Guide

## Table of Contents
1. [Naming Conventions](#1-naming-conventions)
2. [Code Structure and Organization](#2-code-structure-and-organization)
3. [Documentation and Comments](#3-documentation-and-comments)
4. [Code Style](#4-code-style)
5. [Error Handling](#5-error-handling)
6. [Testing](#6-testing)
7. [Version Control](#7-version-control)
8. [Type Hinting](#8-type-hinting)
9. [Project-Specific Conventions](#9-project-specific-conventions)
10. [Inheritance](#10-inheritance)
11. [RasUtils Usage](#11-rasutils-usage)
12. [Working with RasExamples](#12-working-with-rasexamples)

## 1. Naming Conventions

### 1.1 General Rules
- Use `snake_case` for all function and variable names
- Use `PascalCase` for class names
- Use `UPPER_CASE` for constants

### 1.2 Library-Specific Naming
- Informal Name: RAS Commander
- Package Name and GitHub Library Name: ras-commander (with a hyphen)
- Import Name: ras_commander (with an underscore)
- Main Class of functions for HEC-RAS Automation: RasCmdr

### 1.3 Function Naming
- Start function names with a verb describing the action
- Use clear, descriptive names
- Common verbs and their uses:
  - `get_`: retrieve data
  - `set_`: set values or properties
  - `compute_`: execute or calculate
  - `clone_`: copy
  - `clear_`: remove or reset data
  - `find_`: search
  - `update_`: modify existing data

### 1.4 Abbreviations
Use the following abbreviations consistently throughout the codebase:

- ras: HEC-RAS
- prj: Project
- geom: Geometry
- pre: Preprocessor
- geompre: Geometry Preprocessor
- num: Number
- init: Initialize
- XS: Cross Section
- DSS: Data Storage System
- GIS: Geographic Information System
- BC: Boundary Condition
- IC: Initial Condition
- TW: Tailwater

Use these abbreviations in lowercase for function and variable names (e.g., `geom`, not `Geom` or `GEOM`).

### 1.5 Class Naming
- Use `PascalCase` for class names (e.g., `FileOperations`, `PlanOperations`, `RasCmdr`)
- Class names should be nouns or noun phrases

### 1.6 Variable Naming
- Use descriptive names indicating purpose or content
- Prefix boolean variables with `is_`, `has_`, or similar

## 2. Code Structure and Organization

### 2.1 File Organization
- Group related functions into appropriate classes
- Keep each class in its own file, named after the class

### 2.2 Function Organization
- Order functions logically within a class
- Place common or important functions at the top of the class

### 2.3 Module Structure
- Use the following order for module contents:
  1. Module-level docstring
  2. Imports (grouped and ordered)
  3. Constants
  4. Classes
  5. Functions

## 3. Documentation and Comments

### 3.1 Docstrings
- Use docstrings for all modules, classes, methods, and functions
- Follow Google Python Style Guide format
- Include parameters, return values, and a brief description
- For complex functions, include examples in the docstring

### 3.2 Comments
- Use inline comments sparingly, only for complex logic
- Keep comments up-to-date with code changes
- Use TODO comments for future work, formatted as: `# TODO: description`

## 4. Code Style

### 4.1 Imports
- Order imports as follows:
  1. Standard library imports
  2. Third-party library imports
  3. Local application imports
- Use absolute imports
- Use `import ras_commander as ras` for shortening the library name in examples

### 4.2 Whitespace
- Follow PEP 8 guidelines
- Use 4 spaces for indentation (no tabs)
- Use blank lines to separate logical sections of code

### 4.3 Line Length
- Limit lines to 79 characters for code, 72 for comments and docstrings
- Use parentheses for line continuation in long expressions

## 5. Error Handling

**Use Logging Instead of Prints**
Ensure that every operation that can fail or needs to provide feedback to the user is logged instead of using `print`. This will help in debugging and improve monitoring during execution.

   ```python
   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   ```

   Example of replacing a `print` with logging:
   ```python
   logging.info('Starting HEC-RAS simulation...')
   ```

- Use explicit exception handling with try/except blocks
- Raise custom exceptions when appropriate, with descriptive messages
- Use logging for error reporting and debugging information
- Use specific exception types when raising errors (e.g., `ValueError`, `FileNotFoundError`)
- Provide informative error messages that include relevant details
- Implement proper cleanup in finally blocks when necessary
- For user-facing functions, consider wrapping internal exceptions in custom exceptions specific to ras-commander

Example:
```python
try:
    result = compute_plan(plan_number)
except FileNotFoundError as e:
    raise RasCommanderError(f"Plan file not found: {e}")
except ValueError as e:
    raise RasCommanderError(f"Invalid plan parameter: {e}")
except Exception as e:
    raise RasCommanderError(f"Unexpected error during plan computation: {e}")
```

## 6. Testing

- Write unit tests for all functions and methods
- Use the `unittest` framework
- Aim for high test coverage, especially for critical functionality
- Include tests for both single-project and multi-project scenarios
- Write clear and descriptive test names
- Use setUp and tearDown methods for common test preparations and cleanups
- Use mock objects when appropriate to isolate units under test

## 7. Version Control

- Use meaningful commit messages that clearly describe the changes made
- Create feature branches for new features or significant changes
- Submit pull requests for code review before merging into the main branch
- Keep commits focused and atomic (one logical change per commit)
- Use git tags for marking releases
- Follow semantic versioning for release numbering

## 8. Type Hinting

- Use type hints for all function parameters and return values
- Use the `typing` module for complex types (e.g., `List`, `Dict`, `Optional`)
- Include type hints in function signatures and docstrings
- Use `Union` for parameters that can accept multiple types
- For methods that don't return a value, use `-> None`

Example:
```python
from typing import List, Optional

def process_plans(plan_numbers: List[str], max_workers: Optional[int] = None) -> bool:
    # Function implementation
    return True
```

## 9. Project-Specific Conventions

### 9.1 RAS Instance Handling
- Design functions to accept an optional `ras_object` parameter:
  ```python
  def some_function(param1, param2, ras_object=None):
      ras_obj = ras_object or ras
      ras_obj.check_initialized()
      # Function implementation
  ```

### 9.2 File Path Handling
- Use `pathlib.Path` for file and directory path manipulations
- Convert string paths to Path objects at the beginning of functions

### 9.3 DataFrame Handling
- Use pandas for data manipulation and storage where appropriate
- Prefer method chaining for pandas operations to improve readability

### 9.4 Parallel Execution
- Follow the guidelines in the "Benchmarking is All You Need" blog post for optimal core usage in parallel plan execution

### 9.5 Function Return Values
- Prefer returning meaningful values over modifying global state
- Use tuple returns for multiple values instead of modifying input parameters

## 10. Inheritance

### 10.1 General Principles

- Prioritize composition over inheritance when appropriate
- Design base classes for extension
- Clearly document the public API and subclass API using docstrings

### 10.2 Naming Conventions

- Public API: No leading underscores
- Subclass API: Single leading underscore (e.g., `_prepare_for_execution`)
- Internal attributes and methods: Single leading underscore
- Name mangling (double leading underscores): Use sparingly and document the decision clearly

### 10.3 Template Method Pattern

Consider using the template method pattern in base classes to define a high-level algorithm structure. Subclasses can then override specific steps to customize behavior.

### 10.4 Dataframe Access Control

Use properties to control access and modification of dataframes, providing a controlled interface for subclasses.

## 11. RasUtils Usage

- Use RasUtils for general-purpose utility functions that don't fit into other specific classes
- When adding new utility functions, ensure they are static methods of the RasUtils class
- Keep utility functions focused and single-purpose
- Document utility functions thoroughly, including examples of usage

Example:
```python
class RasUtils:
    @staticmethod
    def create_backup(file_path: Path, backup_suffix: str = "_backup") -> Path:
        """
        Create a backup of the specified file.

        Args:
            file_path (Path): Path to the file to be backed up
            backup_suffix (str): Suffix to append to the backup file name

        Returns:
            Path: Path to the created backup file

        Example:
            >>> backup_path = RasUtils.create_backup(Path("project.prj"))
            >>> print(f"Backup created at: {backup_path}")
        """
        # Function implementation
```

## 12. Working with RasExamples

- Use RasExamples for managing and loading example HEC-RAS projects
- Always check if example projects are already downloaded before attempting to download them again
- Use the `list_categories()` and `list_projects()` methods to explore available examples
- When extracting projects, use meaningful names and keep track of extracted paths
- Clean up extracted projects when they are no longer needed using `clean_projects_directory()`

Example:
```python
ras_examples = RasExamples()
if not ras_examples.is_project_extracted("Bald Eagle Creek"):
    extracted_path = ras_examples.extract_project("Bald Eagle Creek")[0]
    # Use the extracted project
    # ...
    # Clean up when done
    RasUtils.remove_with_retry(extracted_path, is_folder=True)
```

Remember, consistency is key. When in doubt, prioritize readability and clarity in your code. Always consider the maintainability and extensibility of the codebase when making design decisions.


13. Logging

Instructions for setting up a minimal logging decorator and applying it to functions:

1. Create logging_config.py:
```python
import logging
import functools

def setup_logging(level=logging.INFO):
    logging.basicConfig(level=level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def log_call(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        logger.info(f"Calling {func.__name__}")
        return func(*args, **kwargs)
    return wrapper

setup_logging()
```

2. In each module file (e.g., RasPrj.py, RasPlan.py):
   - Add at the top: `from ras_commander.logging_config import log_call`
   - Remove all existing logging configurations and logger instantiations

3. Apply the decorator to functions:
   - Replace existing logging statements with the `@log_call` decorator
   - Remove any manual logging within the function body

Example changes to functions:

Before:
```python
def compute_plan(plan_number, dest_folder=None, ras_object=None, clear_geompre=False, num_cores=None):
    logging.info(f"Computing plan {plan_number}")
    # ... function logic ...
    logging.info(f"Plan {plan_number} computation complete")
    return result
```

After:
```python
@log_call
def compute_plan(plan_number, dest_folder=None, ras_object=None, clear_geompre=False, num_cores=None):
    # ... function logic ...
    return result
```

Apply this pattern across all functions in the library. This approach will significantly reduce the code footprint while maintaining basic logging functionality.
==================================================

File: c:\GH\ras-commander\.gitignore\.gitignore
==================================================
# Ignore the example_projects folder and all its subfolders
examples/example_projects/

# Ignore workspace, projects, and my_projects folders
workspace/
projects/
my_projects/

# Ignore FEMA BLE Models
examples/FEMA_BLE_Models/
examples/hdf_example_data/


# Ignore library assistant config
library_assistant/config/

# Ignore Python egg info
*.egg-info/
.eggs/

# Ignore the Example_Projects_6_5.zip file
Example_Projects_6_5.zip

# Ignore the misc folder and all its subfolders
misc/

# Ignore Python cache files
__pycache__/
*.py[cod]

# Ignore compiled Python files
*.so

# Ignore distribution / packaging
dist/
build/

# Ignore test cache
.pytest_cache/

# Ignore virtual environments
.venv/
venv/

# Ignore IDE-specific files (optional, uncomment if needed)
# .vscode/
# .idea/

# Ignore OS-specific files
.DS_Store
Thumbs.db
==================================================

File: c:\GH\ras-commander\examples\01_project_initialization.py
==================================================
# 01_project_initialization.py

#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek", "BaldEagleCrkMulti2D", "Muncie"])

#### --- START OF SCRIPT --- ####

# RAS Commander Library Notes:
# 1. This example demonstrates both the default global 'ras' object and custom ras objects.
# 2. The global 'ras' object is suitable for simple scripts working with a single project.
# 3. Custom ras objects are recommended for complex scripts or when working with multiple projects.
# 4. The init_ras_project function initializes a project and sets up the ras object.
# 5. Each ras object contains comprehensive information about its project, including plan, geometry, flow files, and boundary conditions.

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.
# 4. Use descriptive names for custom ras objects to clearly identify different projects.

def print_ras_object_data:
    """Docs only, see 'print_ras_object_data.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\02_plan_operations.py
==================================================
# 02_plan_operations.py

#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path
import datetime

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

"""
This script demonstrates the process of initializing a HEC-RAS project and performing various operations on plans, geometries, and unsteady flows using the functions within the RasPlan Class.

Process Flow:
1. Project Initialization: Initialize a HEC-RAS project by specifying the project path and version.
2. Plan Cloning: Clone an existing plan, creating a new plan entry.
3. Geometry Cloning: Clone a geometry associated with the original plan, generating a new geometry entry.
4. Unsteady Flow Cloning: Clone an unsteady flow, creating a new unsteady flow entry.
5. Plan Configuration:
   a. Set the cloned geometry for the new plan.
   b. Set the cloned unsteady flow for the new plan.
   c. Update the number of cores to be used for the new plan.
   d. Configure geometry preprocessor options for the new plan.
6. Update Simulation Parameters: Modify various simulation parameters in the new plan.
7. Plan Computation: Compute the new plan and verify successful execution.
8. Results Verification: Check the HDF entries to confirm that results were written.
"""

def main:
    """Docs only, see 'main.py' for full function code"""

==================================================

File: c:\GH\ras-commander\examples\03_geometry_operations.py
==================================================
# 03_geometry_operations.py

#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Muncie"])

#### --- START OF SCRIPT --- ####

# RAS Commander Library Notes:
# 1. This example uses the default global 'ras' object for simplicity.
# 2. If you need to work with multiple projects, use separate ras objects for each project.
# 3. Once you start using non-global ras objects, stick with that approach throughout your script.
# 4. The RasGeo class provides methods for working with geometry files and preprocessor operations.

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.
# 4. Always clear geometry preprocessor files before making significant changes to ensure clean results.

def main:
    """Docs only, see 'main.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\04_unsteady_flow_operations.py
==================================================
# 04_unsteady_flow_operations.py

#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

"""
This script demonstrates the process of initializing a HEC-RAS project and performing various operations on unsteady flow files using the RasUnsteady class.

Process Flow:
1. Project Initialization: Initialize a HEC-RAS project by specifying the project path and version.
2. Extract Boundary and Tables: Extract boundary conditions and associated tables from an unsteady flow file.
3. Print Boundaries and Tables: Display the extracted boundary conditions and tables.
4. Update Unsteady Parameters: Modify parameters in the unsteady flow file.
5. Verify Changes: Check the updated unsteady flow file to confirm the changes.
"""

def main:
    """Docs only, see 'main.py' for full function code"""


==================================================

File: c:\GH\ras-commander\examples\05_utility_functions.py
==================================================
# 05_utility_functions.py

#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

# RAS Commander (ras-commander) Library Notes:
# 1. This example uses the default global 'ras' object for simplicity.
# 2. If you need to work with multiple projects, use separate ras objects for each project.
# 3. Once you start using non-global ras objects, stick with that approach throughout your script.
# 4. The RasUtils class provides various utility functions for working with HEC-RAS projects.

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.

def main:
    """Docs only, see 'main.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\06_single_plan_execution.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Define the "example_projects" folder in the same directory as the script
examples_path = Path(__file__).parent / "example_projects"

# Delete the project if it exists
if examples_path.exists():
    import shutil
    shutil.rmtree(examples_path)

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

# ras-commander Library Notes:
# 1. This example uses the default global 'ras' object for simplicity.
# 2. If you need to work with multiple projects, use separate ras objects for each project.
# 3. Once you start using non-global ras objects, stick with that approach throughout your script.

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.

def main:
    """Docs only, see 'main.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\07_sequential_plan_execution.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Housekeeping Note: 
# For all of the functions that do batched execution (sequential or parallel), they are careful not to overwrite existing folders
# So if you want your script to be repeatable, you need to make sure you delete the folders before running again.
# Otherwise an error will be raised to prevent overwriting any results from your previous runs.
# This will not be done by the example projects routines, which only overwrite the source folder for repeatability. 
    
import shutil
from pathlib import Path
# Define the keys to search for in folder names
# Delete example projects folder
current_file = Path(__file__).resolve()
current_dir = current_file.parent
delete_folder_path = current_dir / "example_projects"

if delete_folder_path.exists():
    print(f"Removing existing folder: {delete_folder_path}")
    shutil.rmtree(delete_folder_path)
else:
    print(f"Folder not found: {delete_folder_path}")

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

# ras-commander Library Notes:
# 1. This example uses the default global 'ras' object for simplicity.
# 2. If you need to work with multiple projects, use separate ras objects for each project.
# 3. Once you start using non-global ras objects, stick with that approach throughout your script.

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.
# 4. For functions that do batched execution (sequential or parallel), they are careful not to overwrite existing folders.
# 5. If you want your script to be repeatable, make sure to delete the folders before running again.

def main:
    """Docs only, see 'main.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\08_parallel_execution.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path
import shutil
import psutil
import math

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

# ras-commander Library Notes:
# 1. This example uses separate RasPrj objects for each project/folder.
# 2. Using separate RasPrj objects allows working with multiple projects or folders.
# 3. We'll create new RasPrj objects for the original project and each output folder.
# 4. For functions that do batched execution (sequential or parallel), they are careful not to overwrite existing folders.
# 5. If you want your script to be repeatable, make sure to delete the folders before running again.

# Best Practices:
# 1. For complex scripts or when working with multiple projects/folders, create and use separate RasPrj objects.
# 2. Be consistent in your approach: use non-global RasPrj objects throughout the script.
# 3. When using parallel execution, consider the number of cores available on your machine.
# 4. Use the dest_folder argument to keep your project folder clean and organized.

def get_physical_core_count:
    """Docs only, see 'get_physical_core_count.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\09_specifying_plans.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Housekeeping Note: 
# For all of the functions that do batched execution (sequential or parallel), they are careful not to overwrite existing folders
# So if you want your script to be repeatable, you need to make sure you delete the folders before running again.
# Otherwise an error will be raised to prevent overwriting any results from your previous runs.
# This will not be done by the example projects routines, which only overwrite the source folder for repeatability. 
    
import shutil
from pathlib import Path

# Delete example projects folder
current_file = Path(__file__).resolve()
current_dir = current_file.parent
delete_folder_path = current_dir / "example_projects"

if delete_folder_path.exists():
    print(f"Removing existing folder: {delete_folder_path}")
    shutil.rmtree(delete_folder_path)
else:
    print(f"Folder not found: {delete_folder_path}")

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

# RAS Commander (ras-commander) Library Notes:
# 1. This example uses the default global 'ras' object for simplicity.
# 2. If you need to work with multiple projects, use separate ras objects for each project.
# 3. Once you start using non-global ras objects, stick with that approach throughout your script.
# 4. The RasCmdr class provides methods for executing plans in various ways.
# 5. You can specify individual plans or lists of plans for batch operations.

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.
# 4. When specifying plans, use plan numbers as strings (e.g., "01", "02") for consistency.
# 5. Always check the available plans in the project before specifying plan numbers for execution.

def main:
    """Docs only, see 'main.py' for full function code"""

==================================================

File: c:\GH\ras-commander\examples\10_arguments_for_compute.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

# RAS Commander Library Notes:
# 1. This example uses the default global 'ras' object for simplicity.
# 2. If you need to work with multiple projects, use separate ras objects for each project.
# 3. Once you start using non-global ras objects, stick with that approach throughout your script.
# 4. The RasCmdr class provides various arguments for fine-tuning plan computation:
#    - plan_number: String representing the plan number to compute (e.g., "01")
#    - dest_folder: Path object specifying the destination folder for computation results
#    - clear_geompre: Boolean to clear geometry preprocessor files before computation
#    - num_cores: Integer specifying the number of cores to use
#    - overwrite_dest: Boolean to determine if existing destination folders should be overwritten

# Best Practices:
# 1. For simple scripts working with a single project, using the global 'ras' object is fine.
# 2. For complex scripts or when working with multiple projects, create and use separate ras objects.
# 3. Be consistent in your approach: don't mix global and non-global ras object usage in the same script.
# 4. Utilize the various arguments in compute functions to customize plan execution.
# 5. Always consider your system's capabilities when setting num_cores.
# 6. Use clear_geompre=True when you want to ensure a clean computation environment.
# 7. Specify dest_folder to keep your project folder organized and prevent overwriting previous results.

def main:
    """Docs only, see 'main.py' for full function code"""

==================================================

File: c:\GH\ras-commander\examples\12_plan_set_execution.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

import pandas as pd


def create_plan_set:
    """Docs only, see 'create_plan_set.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\13_multiple_project_operations.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek", "Muncie"])

#### --- START OF SCRIPT --- ####

def execute_plan:
    """Docs only, see 'execute_plan.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\14_Core_Sensitivity.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ras-commander pandas requests pathlib matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14_Core_Sensitivity.ipynb\n",
    "Testing Core Sensitivity for RAS using the Bald Eagle Creek Multi-Gage 2D project.  \n",
    "\n",
    "\n",
    "This should take around 15-45 minutes to run depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Flexible imports to allow for development without installation\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasHdf, ras\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    import os\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    parent_directory = current_file.parent\n",
    "    sys.path.append(str(parent_directory))\n",
    "    \n",
    "    # Now try to import again\n",
    "    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasHdf, ras\n",
    "\n",
    "print(\"ras_commander imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ras_commander import RasExamples, init_ras_project, RasCmdr, RasPlan, RasGeo\n",
    "\n",
    "# Step 1: Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "ras_examples = RasExamples()\n",
    "ras_examples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "# Use Path.cwd() to get the current working directory in a Jupyter Notebook\n",
    "current_directory = Path.cwd()\n",
    "project_path = current_directory / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "\n",
    "# Step 2: Initialize the Muncie Project using init_ras_project (from ras_commander)\n",
    "muncie_project = init_ras_project(project_path, \"6.6\")\n",
    "\n",
    "# Step 3: Initialize a DataFrame to store execution results\n",
    "results = []\n",
    "\n",
    "# Step 4: Run sensitivity analysis for Plan 03 with core counts 1-8\n",
    "plan_number = '03'\n",
    "print(f\"Running sensitivity analysis for Plan {plan_number}\")\n",
    "\n",
    "# Clear geompre files before running the plan\n",
    "plan_path = RasPlan.get_plan_path(plan_number)\n",
    "RasGeo.clear_geompre_files(plan_path)\n",
    "\n",
    "for cores in range(1, 9):\n",
    "    print(f\"Running with {cores} core(s)\")\n",
    "    # Set core count for this plan\n",
    "    RasPlan.set_num_cores(plan_number, cores)\n",
    "    \n",
    "    # Time the execution of the plan\n",
    "    start_time = time.time()\n",
    "    RasCmdr.compute_plan(plan_number)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"plan_number\": plan_number,\n",
    "        \"cores\": cores,\n",
    "        \"execution_time\": execution_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "print(\"Sensitivity analysis complete\")\n",
    "\n",
    "# Step 5: Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv(\"core_sensitivity_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES FOR REVISIONS:\n",
    "- Use HDF compute summary to show the time for each preproces/unsteady compute/postprocess step. \n",
    "- First, run preprocessor and then toggle options to only run unsteady compute and postprocess. \n",
    "- Plot each step separately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the results from a CSV file\n",
    "results_df = pd.read_csv(\"core_sensitivity_results.csv\")\n",
    "\n",
    "# Display the results dataframe for verification\n",
    "print(\"results_df DataFrame:\")\n",
    "display(results_df)\n",
    "\n",
    "# Step 6: Calculate unit runtime (based on 1 core execution time)\n",
    "results_df['unit_runtime'] = results_df.groupby('plan_number')['execution_time'].transform(lambda x: x / x.iloc[0])\n",
    "\n",
    "# Get the project name from the ras object\n",
    "project_name = ras.project_name\n",
    "\n",
    "# Step 7: Plot a line chart for unit runtime vs. cores for each plan\n",
    "plt.figure(figsize=(10, 6))\n",
    "for plan in results_df['plan_number'].unique():\n",
    "    plan_data = results_df[results_df['plan_number'] == plan]\n",
    "    plt.plot(plan_data['cores'], plan_data['unit_runtime'], label=f\"Plan {plan}\")\n",
    "\n",
    "plt.xlabel(\"Number of Cores\")\n",
    "plt.ylabel(\"Unit Runtime (Relative to 1 Core)\")\n",
    "plt.title(f\"{project_name} (HEC Example Project)\\nCore Count Sensitivity Analysis\")\n",
    "plt.legend(title=\"Plan Number\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "summary_stats = results_df.groupby('cores')['execution_time'].agg(['mean', 'min', 'max'])\n",
    "display(summary_stats)\n",
    "\n",
    "# Calculate and print speedup\n",
    "speedup = results_df[results_df['cores'] == 1]['execution_time'].mean() / results_df[results_df['cores'] == 8]['execution_time'].mean()\n",
    "print(f\"\\nAverage speedup from 1 to 8 cores: {speedup:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "releasecmdr311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\15_plan_key_operations.py
==================================================
# 15_plan_key_operations.py

#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path
from datetime import datetime, timedelta

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

def main:
    """Docs only, see 'main.py' for full function code"""

==================================================

File: c:\GH\ras-commander\examples\16_scanning_ras_project_info.py
==================================================
import sys
from pathlib import Path

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    from ras_commander import init_ras_project, RasPrj, RasExamples
except ImportError:
    sys.path.append(str(parent_directory))
    from ras_commander import init_ras_project, RasPrj, RasExamples

import logging

def generate_category_summary:
    """Docs only, see 'generate_category_summary.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\examples\17_parallel_execution_ble.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path
import shutil
import psutil
import math
import logging

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj

# Configure logging
logging.basicConfig(
    level=logging.INFO,  # Set the logging level to INFO
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[
        logging.StreamHandler()  # Log to stderr
    ]
)

# Initialize RasExamples
ras_examples = RasExamples()

#### --- START OF SCRIPT --- ####

# ras-commander Library Notes:
# 1. This example uses separate RasPrj objects for each project/folder.
# 2. Using separate RasPrj objects allows working with multiple projects or folders.
# 3. We'll create new RasPrj objects for the original project and each output folder.
# 4. For functions that do batched execution (sequential or parallel), they are careful not to overwrite existing folders.
# 5. If you want your script to be repeatable, make sure to delete the folders before running again.

# Best Practices:
# 1. For complex scripts or when working with multiple projects/folders, create and use separate RasPrj objects.
# 2. Be consistent in your approach: use non-global RasPrj objects throughout the script.
# 3. When using parallel execution, consider the number of cores available on your machine.
# 4. Use the dest_folder argument to keep your project folder clean and organized.

##  WHISKY CHITTO DOES NOT WORK - BLE MODEL IS BROKEN AND REQUIRED FIXING BEFORE RUNNING

def get_physical_core_count:
    """Docs only, see 'get_physical_core_count.py' for full function code"""


==================================================

File: c:\GH\ras-commander\examples\18_benchmarking_version_6.6.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasHdf, RasUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define versions to compare\n",
    "versions = ['6.6', '6.5', '6.4.1', '6.3.1', '6.2', '6.1', '5.0.7']\n",
    "\n",
    "# Extract BaldEagleCrkMulti2D project\n",
    "ras_examples = RasExamples()\n",
    "project_path = ras_examples.extract_project([\"BaldEagleCrkMulti2D\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all plan numbers\n",
    "ras_project = init_ras_project(project_path, \"6.5\")\n",
    "print(ras_project)\n",
    "plan_numbers = ras_project.plan_df['plan_number'].tolist()\n",
    "print(plan_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ras_commander import RasGeo\n",
    "\n",
    "def run_simulation(version, plan_number):\n",
    "    # Initialize project for the specific version\n",
    "    ras_project = init_ras_project(project_path, str(version))\n",
    "    \n",
    "    # Clear geometry preprocessor files\n",
    "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_project)\n",
    "    RasGeo.clear_geompre_files(plan_path, ras_object=ras_project)\n",
    "    \n",
    "    # Set number of cores to 6\n",
    "    RasPlan.set_num_cores(plan_number, \"6\", ras_object=ras_project)\n",
    "    \n",
    "    # Ensure geometry preprocessing is done\n",
    "    RasPlan.update_plan_value(plan_number, \"Run HTab\", 1, ras_object=ras_project)\n",
    "    \n",
    "    # Compute the plan\n",
    "    start_time = time.time()\n",
    "    success = RasCmdr.compute_plan(plan_number, ras_object=ras_project)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if success:\n",
    "        # Get HDF file path\n",
    "        hdf_path = RasPlan.get_results_path(plan_number, ras_object=ras_project)\n",
    "        \n",
    "        # Extract data from HDF file\n",
    "        runtime_data = RasHdf.get_runtime_data(hdf_path, ras_object=ras_project)\n",
    "        \n",
    "        # Extract required information\n",
    "        preprocessor_time = runtime_data['Preprocessing Geometry (hr)'].values[0]\n",
    "        unsteady_compute_time = runtime_data['Unsteady Flow Computations (hr)'].values[0]\n",
    "        \n",
    "        # Get volume accounting data\n",
    "        volume_accounting = RasHdf.get_group_attributes_as_df(hdf_path, \"Results/Unsteady/Summary/Volume Accounting/Volume Accounting 2D\", ras_object=ras_project)\n",
    "        volume_error = volume_accounting['Volume Error (%)'].values[0]\n",
    "        \n",
    "        return {\n",
    "            'Version': version,\n",
    "            'Plan': plan_number,\n",
    "            'Preprocessor Time (hr)': preprocessor_time,\n",
    "            'Unsteady Compute Time (hr)': unsteady_compute_time,\n",
    "            'Volume Error (%)': volume_error,\n",
    "            'Total Time (hr)': total_time / 3600  # Convert seconds to hours\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Run simulations for all versions and plans sequentially\n",
    "results = []\n",
    "for version in versions:\n",
    "    for plan in plan_numbers:\n",
    "        print(f\"Running simulation for Version {version}, Plan {plan}\")\n",
    "        result = run_simulation(version, plan)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"Completed: Version {version}, Plan {plan}\")\n",
    "        else:\n",
    "            print(f\"Failed: Version {version}, Plan {plan}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save initial results to CSV\n",
    "df.to_csv('save_initial_results.csv', index=False)\n",
    "\n",
    "print(\"Initial results saved to 'save_initial_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages across plans for each version\n",
    "df_avg = df.groupby('Version').mean().reset_index()\n",
    "\n",
    "# Create line graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Unsteady Runtime vs Version\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df_avg['Version'], df_avg['Unsteady Compute Time (hr)'], marker='o')\n",
    "plt.title('Average Unsteady Runtime vs HEC-RAS Version')\n",
    "plt.xlabel('HEC-RAS Version')\n",
    "plt.ylabel('Unsteady Runtime (hours)')\n",
    "\n",
    "# Volume Error vs Version\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df_avg['Version'], df_avg['Volume Error (%)'], marker='o')\n",
    "plt.title('Average Volume Error vs HEC-RAS Version')\n",
    "plt.xlabel('HEC-RAS Version')\n",
    "plt.ylabel('Volume Error (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results to CSV\n",
    "df.to_csv('hecras_version_comparison.csv', index=False)\n",
    "df_avg.to_csv('hecras_version_comparison_averages.csv', index=False)\n",
    "\n",
    "print(\"Results saved to 'hecras_version_comparison.csv' and 'hecras_version_comparison_averages.csv'\")\n",
    "print(\"Graphs have been displayed. Please check the output.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fffff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\19_1d_hdf_data_extraction.ipynb
==================================================
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# HEC-RAS 1D HDF Data Analysis Notebook\n","\n","This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Import required Libraries\n","import subprocess\n","import sys\n","import os\n","from pathlib import Path\n","\n","def install_module(module_name):\n","    try:\n","        __import__(module_name)\n","    except ImportError:\n","        print(f\"{module_name} not found. Installing...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n","\n","# List of modules to check and install if necessary\n","modules = ['h5py', 'numpy', 'requests', 'geopandas', 'matplotlib', 'pandas', 'pyproj', 'shapely', 'xarray','rtree', 'rasterstats']\n","for module in modules:\n","    install_module(module)\n","\n","# Import the rest of the required libraries\n","import pandas as pd\n","import numpy as np\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import pyproj\n","from shapely.geometry import Point, LineString, Polygon\n","import xarray as xr\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Install ras-commander if you are not in a dev environment. \n","# install_module(ras-commander)"]},{"cell_type":"markdown","metadata":{},"source":["## Importing ras-commander flexibly (from package or local dev copy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","from pathlib import Path\n","\n","# Flexible imports to allow for development without installation \n","#  ** Use this version with Jupyter Notebooks **\n","try:\n","    # Try to import from the installed package\n","    from ras_commander import (init_ras_project, HdfBase, HdfUtils, HdfFluvialPluvial, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","except ImportError:\n","    # If the import fails, add the parent directory to the Python path\n","    print(\"Using Local ras-commander dev libraries\")\n","    import os\n","    current_file = Path(os.getcwd()).resolve()\n","    parent_directory = current_file.parent\n","    sys.path.append(str(parent_directory))\n","    \n","    # Now try to import again\n","    from ras_commander import (init_ras_project, HdfBase, HdfUtils, HdfFluvialPluvial, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","\n","print(\"ras_commander imported successfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download the BaldEagleCrkMulti2D project from HEC and run plan 01\n","\n","# Define the path to the BaldEagleCrkMulti2D project\n","current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n","bald_eagle_path = current_dir / \"example_projects\" / \"Balde Eagle Creek\"\n","import logging\n","\n","# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n","hdf_file = bald_eagle_path / \"BaldEagle.p01.hdf\"\n","\n","if not hdf_file.exists():\n","    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n","    ras_examples = RasExamples()\n","    ras_examples.extract_project([\"Balde Eagle Creek\"])\n","\n","    # Initialize custom Ras object\n","    bald_eagle = RasPrj()\n","\n","    # Initialize the RAS project using the custom ras object\n","    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n","    logging.info(f\"Balde Eagle project initialized with folder: {bald_eagle.project_folder}\")\n","    \n","    logging.info(f\"Balde Eagle object id: {id(bald_eagle)}\")\n","    \n","    # Define the plan number to execute\n","    plan_number = \"01\"\n","\n","    # Execute Plan 06 using RasCmdr for Bald Eagle\n","    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n","    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n","    if success_bald_eagle:\n","        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n","    else:\n","        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n","else:\n","    print(\"BaldEagle.p01.hdf already exists. Skipping project extraction and plan execution.\")\n","    # Initialize the RAS project using the custom ras object\n","    bald_eagle = RasPrj()\n","    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n","    plan_number = \"01\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n","\n","# Display plan_df for bald_eagle project\n","print(\"Plan DataFrame for bald_eagle project:\")\n","display(bald_eagle.plan_df)\n","\n","# Display geom_df for bald_eagle project\n","print(\"\\nGeometry DataFrame for bald_eagle project:\")\n","display(bald_eagle.geom_df)\n","\n","# Get the plan HDF path\n","plan_number = \"01\"  # Assuming we're using plan 01 as in the previous code\n","plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n","\n","# Get the geometry file number from the plan DataFrame\n","geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n","geom_number = geom_file[1:]  # Remove the 'g' prefix\n","\n","# Get the geometry HDF path\n","geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n","\n","print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n","print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"]},{"cell_type":"markdown","metadata":{},"source":["-----"]},{"cell_type":"markdown","metadata":{},"source":["RasHdfUtils\n","| Method Name | Description |\n","|-------------|-------------|\n","| get_attrs | Converts attributes from a HEC-RAS HDF file into a Python dictionary for a given attribute path |\n","| get_root_attrs | Returns attributes at root level of HEC-RAS HDF file |\n","| get_hdf_paths_with_properties | Gets all paths in the HDF file with their properties |\n","| get_group_attributes_as_df | Gets attributes of a group in the HDF file as a DataFrame |\n","| get_hdf_filename | Gets the HDF filename from various input types |\n","| get_runtime_data | Extracts runtime and compute time data from a single HDF file |\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get HDF Paths with Properties (For Exploring HDF Files)\n","HdfBase.get_dataset_info(plan_number, ras_object=bald_eagle, group_path=\"/Geometry\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfPlan for geometry-related operations\n","print(\"\\nExample: Extracting Base Geometry Attributes\")\n","geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path, ras_object=bald_eagle)\n","display(geom_attrs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract runtime and compute time data\n","print(\"\\nExample 2: Extracting runtime and compute time data\")\n","runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=bald_eagle)\n","if runtime_df is not None:\n","    display(runtime_df)\n","else:\n","    print(\"No runtime data found.\")"]},{"cell_type":"markdown","metadata":{},"source":["Table of all the functions in the RasGeomHdf class from the ras_commander/RasGeomHdf.py file:\n","\n","| Function Name | Description |\n","|---------------|-------------|\n","| projection | Returns the projection of the RAS geometry as a pyproj.CRS object |\n","| get_geom_attrs | Returns base geometry attributes from a HEC-RAS HDF file |\n","\n","| mesh_area_names | Returns a list of the 2D mesh area names of the RAS geometry |\n","| get_geom_2d_flow_area_attrs | Returns geometry 2d flow area attributes from a HEC-RAS HDF file |\n","| mesh_areas | Returns 2D flow area perimeter polygons |\n","| mesh_cell_polygons | Returns 2D flow mesh cell polygons |\n","| mesh_cell_points | Returns 2D flow mesh cell points |\n","| mesh_cell_faces | Returns 2D flow mesh cell faces |\n","\n","| get_geom_structures_attrs | Returns geometry structures attributes from a HEC-RAS HDF file |\n","\n","\n","\n","\n","| bc_lines | Returns 2D mesh area boundary condition lines |\n","| breaklines | Returns 2D mesh area breaklines |\n","\n","\n","\n","| refinement_regions | Returns 2D mesh area refinement regions |\n","| structures | Returns the model structures |\n","| reference_lines_names | Returns reference line names |\n","| reference_points_names | Returns reference point names |\n","| reference_lines | Returns the reference lines geometry and attributes |\n","| reference_points | Returns the reference points geometry and attributes |\n","| cross_sections | Returns the model 1D cross sections |\n","| river_reaches | Returns the model 1D river reach lines |\n","| cross_sections_elevations | Returns the model cross section elevation information |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n","print(geom_hdf_path)\n","\n","# For the example project, plan 06 is associated with geometry 09\n","# If you want to call the geometry by number, call RasHdfGeom functions with a number\n","# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfUtils for extracting projection\n","print(\"\\nExtracting Projection from HDF\")\n","projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n","if projection:\n","    print(f\"Projection: {projection}\")\n","else:\n","    print(\"No projection information found.  This attribute is only included if a RASMapper projection is defined.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfPlan for geometry-related operations\n","print(\"\\nExample: Extracting Base Geometry Attributes\")\n","geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n","display(geom_attrs)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get geometry structures attributes\n","print(\"\\nGetting geometry structures attributes\")\n","geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path, ras_object=bald_eagle)\n","if geom_structures_attrs:\n","    print(\"Geometry structures attributes:\")\n","    for key, value in geom_structures_attrs.items():\n","        print(f\"{key}: {value}\")\n","else:\n","    print(\"No geometry structures attributes found.\")"]},{"cell_type":"markdown","metadata":{},"source":["### NEED TO EDIT THIS TO SHOW BC LINES WITH RIVERS AND CROSS SECTIONS"]},{"cell_type":"markdown","metadata":{},"source":["# Example: Extract Boundary Condition Lines and Plot with 2D Flow Area Perimeter Polygons\n","print(\"\\nExample 7: Extracting Boundary Condition Lines and Plotting with 2D Flow Area Perimeter Polygons\")\n","bc_lines_df = HdfBndry.bc_lines(geom_hdf_path, ras_object=bald_eagle)\n","if not bc_lines_df.empty:\n","    display(bc_lines_df.head())\n","else:\n","    print(\"No Boundary Condition Lines found.\")\n","\n","# Plot if data exists\n","if not bc_lines_df.empty or not mesh_areas.empty:\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot 2D Flow Area Perimeter Polygons\n","    if not mesh_areas.empty:\n","        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n","        \n","        # Add labels for each polygon\n","        for idx, row in mesh_areas.iterrows():\n","            centroid = row.geometry.centroid\n","            label = row.get('Name', f'Area {idx}')\n","            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n","    \n","    # Plot boundary condition lines\n","    if not bc_lines_df.empty:\n","        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('Easting')\n","    ax.set_ylabel('Northing')\n","    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n","    \n","    # Add grid and legend\n","    ax.grid(True)\n","    ax.legend()\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No data available for plotting.\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# INSTEAD OF hdf_input, USE plan_hdf_path or geom_hdf_path as appropriate "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get structures\n","structures_gdf = HdfStruc.get_structures(geom_hdf_path)\n","print(\"Structures:\")\n","if not structures_gdf.empty:\n","    display(structures_gdf.head())\n","else:\n","    print(\"No structures found in the geometry file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get reference lines\n","ref_lines_gdf = HdfBndry.get_reference_lines(geom_hdf_path, ras_object=bald_eagle)\n","print(\"\\nReference Lines:\")\n","if not ref_lines_gdf.empty:\n","    display(ref_lines_gdf.head())\n","else:\n","    print(\"No reference lines found in the geometry file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get reference points\n","ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path, ras_object=bald_eagle)\n","print(\"\\nReference Points:\")\n","if not ref_points_gdf.empty:\n","    display(ref_points_gdf.head())\n","else:\n","    print(\"No reference points found in the geometry file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use the get_hdf5_dataset_info function from HdfUtils to explore the Cross Sections structure in the geometry HDF file\n","\n","print(\"\\nExploring Cross Sections structure in geometry file:\")\n","print(\"HDF Base Path: /Geometry/Cross Sections \")\n","HdfBase.get_dataset_info(geom_hdf_path, group_path='/Geometry/Cross Sections')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get cross section geodataframe\n","cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n","with pd.option_context('display.max_columns', None):  # Show all columns\n","    display(cross_sections_gdf)"]},{"cell_type":"markdown","metadata":{},"source":["cross_sections_gdf: \n","\n","| geometry | station_elevation | mannings_n | ineffective_blocks | River | Reach | RS | Name | Description | Len Left | Len Channel | Len Right | Left Bank | Right Bank | Friction Mode | Contr | Expan | Left Levee Sta | Left Levee Elev | Right Levee Sta | Right Levee Elev | HP Count | HP Start Elev | HP Vert Incr | HP LOB Slices | HP Chan Slices | HP ROB Slices | Ineff Block Mode | Obstr Block Mode | Default Centerline | Last Edited |\n","|-----------|-------------------|------------|--------------------|-------|-------|----|------|-------------|----------|-------------|-----------|-----------|------------|----------------|-------|-------|----------------|-----------------|----------------|------------------|----------|----------------|---------------|----------------|----------------|----------------|------------------|------------------|-------------------|--------------|\n","| 0         | LINESTRING (1968668.17 290166.79, 1969067.87 2... | [[0.0, 660.41], [5.0, 660.61], [40.0, 659.85],... | {'Station': [0.0, 190.0, 375.0], 'Mann n': [0.... | []    | Bald Eagle | Loc Hav | 138154.4 |             | 358.429993 | 463.640015 | 517.640015 | 190.000000 | 375.000000 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 49       | 656.799988      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n","| 1         | LINESTRING (1968627.02 290584.12, 1969009.09 2... | [[0.0, 664.28], [50.0, 661.73], [55.0, 661.54]... | {'Station': [0.0, 535.0, 672.5599975585938], '... | []    | Bald Eagle | Loc Hav | 137690.8 |             | 305.709991 | 363.839996 | 382.829987 | 535.000000 | 672.559998 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 65       | 654.229980      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n","| 2         | LINESTRING (1968585.88 290854.5, 1968868.02 29... | [[0.0, 662.72], [20.0, 665.5], [25.0, 666.48],... | {'Station': [0.0, 580.0, 717.239990234375], 'M... | []    | Bald Eagle | Loc Hav | 137327.0 |             | 732.929993 | 762.020020 | 765.359985 | 580.000000 | 717.239990 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 66       | 653.900024      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Filter rows where ineffective_blocks is not empty\n","ineffective_xs = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(len) > 0]\n","\n","print(\"\\nCross Sections with Ineffective Flow Areas:\")\n","display(ineffective_xs)\n","\n","# Print a message if no cross sections with ineffective flow areas are found\n","print(\"\\nNo cross sections found with ineffective flow areas.\" if ineffective_xs.empty else \"\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print cross sections data\n","\n","print(\"\\nCross Section Information:\")\n","if not cross_sections_gdf.empty:\n","    for idx, row in cross_sections_gdf.iterrows():\n","        print(f\"\\nCross Section {idx + 1}:\")\n","        print(f\"River: {row['River']}\")\n","        print(f\"Reach: {row['Reach']}\")\n","        print(\"\\nGeometry:\")\n","        print(row['geometry'])\n","        print(\"\\nStation-Elevation Points:\")\n","        \n","        # Print header\n","        print(\"     #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation\")\n","        print(\"-\" * 150)\n","        \n","        # Calculate number of rows needed\n","        points = row['station_elevation']\n","        num_rows = (len(points) + 4) // 5  # Round up division\n","        \n","        # Print points in 5 columns\n","        for i in range(num_rows):\n","            line = \"\"\n","            for j in range(5):\n","                point_idx = i + j * num_rows\n","                if point_idx < len(points):\n","                    station, elevation = points[point_idx]\n","                    line += f\"{point_idx+1:6d} {station:10.2f} {elevation:10.2f}    \"\n","            print(line)\n","        print(\"-\" * 150)\n","else:\n","    print(\"No cross sections found in the geometry file.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot cross sections on map\n","import matplotlib.pyplot as plt\n","\n","# Get cross sections data\n","cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n","\n","if not cross_sections_gdf.empty:\n","    # Create figure and axis\n","    fig, ax = plt.subplots(figsize=(15,10))\n","    \n","    # Plot cross sections\n","    cross_sections_gdf.plot(ax=ax, color='red', linewidth=1, label='Cross Sections')\n","    \n","    # Add river name and reach labels\n","    #for idx, row in cross_sections_gdf.iterrows():\n","    #    # Get midpoint of cross section line for label placement\n","    #    midpoint = row.geometry.centroid\n","    #    label = f\"{row['River']}\\n{row['Reach']}\\nRS: {row['RS']}\"\n","    #    ax.annotate(label, (midpoint.x, midpoint.y), \n","    #               xytext=(5, 5), textcoords='offset points',\n","    #               fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n","    \n","    # Customize plot\n","    ax.set_title('Cross Sections Location Map')\n","    ax.grid(True)\n","    ax.legend()\n","    \n","    # Equal aspect ratio to preserve shape\n","    ax.set_aspect('equal')\n","    \n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No cross sections found in the geometry file.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot cross sections with Manning's n values colored by value\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from shapely.geometry import LineString\n","\n","# Create figure\n","fig, ax1 = plt.subplots(figsize=(20,10))\n","\n","# Create colormap\n","cmap = plt.cm.viridis\n","norm = plt.Normalize(vmin=0.02, vmax=0.08)  # Typical Manning's n range\n","\n","# Plot cross sections colored by Manning's n\n","for idx, row in cross_sections_gdf.iterrows():\n","    # Extract Manning's n values and stations\n","    mannings = row['mannings_n']\n","    n_values = mannings['Mann n']\n","    stations = mannings['Station']\n","    \n","    # Get the full linestring coordinates\n","    line_coords = list(row.geometry.coords)\n","    \n","    # Calculate total length of the cross section\n","    total_length = row.geometry.length\n","    \n","    # For each Manning's n segment\n","    for i in range(len(n_values)-1):\n","        # Calculate the start and end proportions along the line\n","        start_prop = stations[i] / stations[-1]\n","        end_prop = stations[i+1] / stations[-1]\n","        \n","        # Get the start and end points for this segment\n","        start_idx = int(start_prop * (len(line_coords)-1))\n","        end_idx = int(end_prop * (len(line_coords)-1))\n","        \n","        # Extract the segment coordinates\n","        segment_coords = line_coords[start_idx:end_idx+1]\n","        \n","        if len(segment_coords) >= 2:\n","            # Create a line segment\n","            segment = LineString(segment_coords)\n","            \n","            # Get color from colormap for this n value\n","            color = cmap(norm(n_values[i]))\n","            \n","            # Plot the segment\n","            ax1.plot(*segment.xy, color=color, linewidth=2)\n","\n","# Add colorbar\n","sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n","sm.set_array([])\n","plt.colorbar(sm, ax=ax1, label=\"Manning's n Value\")\n","\n","ax1.set_title(\"Cross Sections Colored by Manning's n Values\")\n","ax1.grid(True)\n","ax1.set_aspect('equal')\n","\n","plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot cross sections with ineffective flow areas\n","import matplotlib.pyplot as plt\n","\n","# Get cross sections data\n","cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n","\n","# Create figure\n","fig, ax2 = plt.subplots(figsize=(20,10))\n","\n","# Plot all cross sections first\n","cross_sections_gdf.plot(ax=ax2, color='lightgray', linewidth=1, label='Cross Sections')\n","\n","# Plot ineffective flow areas with thicker lines\n","ineffective_sections = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(lambda x: len(x) > 0)]\n","ineffective_sections.plot(ax=ax2, color='red', linewidth=3, label='Ineffective Flow Areas')\n","\n","# Add ineffective flow area labels with offset to lower right\n","for idx, row in cross_sections_gdf.iterrows():\n","    # Get midpoint of cross section line\n","    midpoint = row.geometry.centroid\n","    \n","    # Extract ineffective flow blocks\n","    ineff_blocks = row['ineffective_blocks']\n","    \n","    if ineff_blocks:  # Only label if there are ineffective blocks\n","        label_parts = []\n","        # Add RS to first line of label\n","        label_parts.append(f\"RS: {row['RS']}\")\n","        for block in ineff_blocks:\n","            label_parts.append(\n","                f\"L:{block['Left Sta']:.0f}-R:{block['Right Sta']:.0f}\\n\"\n","                f\"Elev: {block['Elevation']:.2f}\\n\"\n","                f\"Permanent: {block['Permanent']}\"\n","            )\n","        \n","        label = '\\n'.join(label_parts)\n","        \n","        ax2.annotate(label, (midpoint.x, midpoint.y),\n","                    xytext=(15, -15),  # Offset to lower right\n","                    textcoords='offset points',\n","                    fontsize=8, \n","                    bbox=dict(facecolor='white', alpha=0.7),\n","                    arrowprops=dict(arrowstyle='->'),\n","                    horizontalalignment='left',\n","                    verticalalignment='top')\n","\n","ax2.set_title('Cross Sections with Ineffective Flow Areas')\n","ax2.grid(True)\n","ax2.legend()\n","ax2.set_aspect('equal')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot cross section elevation for cross section 42\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Get cross sections data\n","cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n","\n","if not cross_sections_gdf.empty:\n","    # Get station-elevation data for cross section 42\n","    station_elevation = cross_sections_gdf.iloc[42]['station_elevation']\n","    \n","    # Convert list of lists to numpy arrays for plotting\n","    stations = np.array([point[0] for point in station_elevation])\n","    elevations = np.array([point[1] for point in station_elevation])\n","    \n","    # Create figure and axis\n","    fig, ax = plt.subplots(figsize=(12,8))\n","    \n","    # Plot cross section\n","    ax.plot(stations, elevations, 'b-', linewidth=2)\n","    \n","    # Add labels and title\n","    river = cross_sections_gdf.iloc[42]['River']\n","    reach = cross_sections_gdf.iloc[42]['Reach'] \n","    rs = cross_sections_gdf.iloc[42]['RS']\n","    \n","    # Show bank stations as dots\n","    left_bank_station = cross_sections_gdf.iloc[42]['Left Bank']\n","    right_bank_station = cross_sections_gdf.iloc[42]['Right Bank']\n","    \n","    # Interpolating bank stations for plotting\n","    ax.plot(left_bank_station, elevations[np.searchsorted(stations, left_bank_station)], 'ro', label='Left Bank Station')\n","    ax.plot(right_bank_station, elevations[np.searchsorted(stations, right_bank_station)], 'ro', label='Right Bank Station')\n","    \n","    ax.set_title(f'Cross Section Profile\\nRiver: {river}, Reach: {reach}, RS: {rs}\\n'\n","                 f'Left Bank Station: {left_bank_station}, Right Bank Station: {right_bank_station}')\n","    ax.set_xlabel('Station (ft)')\n","    ax.set_ylabel('Elevation (ft)')\n","    \n","    # Add grid and legend\n","    ax.grid(True)\n","    ax.legend()\n","    \n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No cross sections found in the geometry file.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage:\n","centerlines = HdfXsec.get_river_centerlines(geom_hdf_path)\n","centerlines_with_stations = HdfXsec.get_river_stationing(centerlines)\n","\n","# Display results\n","print(\"\\nRiver Centerlines:\")\n","display(centerlines.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot river centerlines with labels\n","import matplotlib.pyplot as plt\n","import geopandas as gpd\n","\n","# Create figure and axis\n","fig, ax = plt.subplots(figsize=(15, 10))\n","\n","# Plot centerlines\n","centerlines.plot(ax=ax, color='blue', linewidth=2, label='River Centerline')\n","\n","# Add river/reach labels\n","for idx, row in centerlines.iterrows():\n","    # Get midpoint of the line for label placement\n","    midpoint = row.geometry.interpolate(0.5, normalized=True)\n","    \n","    # Create label text combining river and reach names\n","    label = f\"{row['River Name']}\\n{row['Reach Name']}\"\n","    \n","    # Add text annotation\n","    ax.annotate(label, \n","                xy=(midpoint.x, midpoint.y),\n","                xytext=(10, 10), # Offset text slightly\n","                textcoords='offset points',\n","                fontsize=10,\n","                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n","\n","# Add labels and title\n","ax.set_title('River Centerlines', fontsize=14)\n","ax.set_xlabel('Easting', fontsize=12)\n","ax.set_ylabel('Northing', fontsize=12)\n","\n","# Add legend\n","ax.legend(fontsize=12)\n","\n","# Add grid\n","ax.grid(True)\n","\n","# Adjust layout\n","plt.tight_layout()\n","\n","# Show plot\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage:\n","edge_lines = HdfXsec.get_river_edge_lines(geom_hdf_path)\n","centerlines = HdfXsec.get_river_centerlines(geom_hdf_path)\n","# Display results\n","print(\"\\nRiver Edge Lines:\")\n","display(edge_lines.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage:\n","bank_lines = HdfXsec.get_river_bank_lines(geom_hdf_path)\n","# Display results\n","print(\"\\nRiver Bank Lines:\")\n","display(bank_lines.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create figure and axis\n","fig, ax = plt.subplots(figsize=(15, 10))\n","\n","# Plot river edge lines\n","edge_lines.plot(ax=ax, color='blue', linewidth=2, label='River Edge Lines')\n","\n","# Plot centerlines for reference\n","centerlines.plot(ax=ax, color='red', linewidth=2, linestyle='--', label='River Centerline')\n","\n","# Plot river bank lines\n","bank_lines.plot(ax=ax, color='green', linewidth=2, label='River Bank Lines')\n","\n","# Add title and labels\n","ax.set_title('River Edge Lines, Centerline, and Bank Lines', fontsize=14)\n","ax.set_xlabel('Easting', fontsize=12)\n","ax.set_ylabel('Northing', fontsize=12)\n","\n","# Add legend\n","ax.legend(fontsize=12)\n","\n","# Add grid\n","ax.grid(True)\n","\n","# Adjust layout\n","plt.tight_layout()\n","\n","# Show plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get dataset structure:\n","HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/River Bank Lines/\")"]},{"cell_type":"markdown","metadata":{},"source":["## Function to explore HDF file to assist with 1D Structures Data Extraction "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n","HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Output/Output Blocks/Computation Block/Global/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n","HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Structures\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract 1D Structures Geodataframe\n","\n","# Import required functions\n","from ras_commander.HdfStruc import HdfStruc\n","from ras_commander.HdfXsec import HdfXsec\n","\n","# Create instances\n","hdf_struc = HdfStruc()\n","hdf_xsec = HdfXsec()\n","\n","# Extract data into GeoDataFrames\n","structures_gdf = hdf_struc.get_structures(hdf_path=geom_hdf_path)\n","cross_sections_gdf = hdf_xsec.get_cross_sections(hdf_path=geom_hdf_path)\n","centerlines_gdf = hdf_xsec.get_river_centerlines(hdf_path=geom_hdf_path)\n","\n","# Display basic information about the structures\n","print(\"\\nStructures Summary:\")\n","print(f\"Number of structures found: {len(structures_gdf)}\")\n","display(structures_gdf)\n","\n","# Display first few rows of key attributes\n","print(\"\\nStructure Details:\")\n","display_cols = ['Structure ID', 'Structure Type', 'River Name', 'Reach Name', 'Station']\n","display_cols = [col for col in display_cols if col in structures_gdf.columns]\n","if display_cols:\n","    print(structures_gdf[display_cols].head())\n","\n","# Create visualization\n","fig, ax = plt.subplots(figsize=(15, 10))\n","\n","# Plot river centerlines\n","if not centerlines_gdf.empty:\n","    centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerlines')\n","\n","# Plot cross sections\n","if not cross_sections_gdf.empty:\n","    cross_sections_gdf.plot(ax=ax, color='green', linewidth=1, label='Cross Sections')\n","\n","# Plot structures\n","if not structures_gdf.empty:\n","    structures_gdf.plot(ax=ax, color='red', marker='s', markersize=100, label='Structures')\n","\n","# Add title and labels\n","ax.set_title('HEC-RAS Model Components', fontsize=14)\n","ax.set_xlabel('Easting', fontsize=12)\n","ax.set_ylabel('Northing', fontsize=12)\n","\n","# Add legend\n","ax.legend(fontsize=12)\n","\n","# Add grid\n","ax.grid(True)\n","\n","# Adjust layout\n","plt.tight_layout()\n","\n","# Show plot\n","plt.show()\n","\n","# Print summary of cross sections\n","print(\"\\nCross Sections Summary:\")\n","print(f\"Number of cross sections found: {len(cross_sections_gdf)}\")\n","if not cross_sections_gdf.empty:\n","    print(\"\\nCross Section Details:\")\n","    xs_display_cols = ['River', 'Reach', 'Station']\n","    xs_display_cols = [col for col in xs_display_cols if col in cross_sections_gdf.columns]\n","    if xs_display_cols:\n","        print(cross_sections_gdf[xs_display_cols].head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract Compute Messages as String\n","print(\"Extracting Compute Messages\")\n","\n","import h5py\n","import numpy as np\n","\n","def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n","    \"\"\"\n","    Extract string from HDF object at a given path\n","\n","    Parameters\n","    ----------\n","    results_hdf_filename : str\n","        Name of the HDF file\n","    hdf_path : str\n","        Path of the object in the HDF file\n","\n","    Returns\n","    -------\n","    str\n","        Extracted string from the specified HDF object\n","    \"\"\"\n","    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n","        try:\n","            hdf_object = hdf_file[hdf_path]\n","            if isinstance(hdf_object, h5py.Group):\n","                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n","            elif isinstance(hdf_object, h5py.Dataset):\n","                data = hdf_object[()]\n","                if isinstance(data, bytes):\n","                    return data.decode('utf-8')\n","                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n","                    return [v.decode('utf-8') for v in data]\n","                else:\n","                    return str(data)\n","            else:\n","                return f\"Unsupported object type: {type(hdf_object)}\"\n","        except KeyError:\n","            return f\"Path not found: {hdf_path}\"\n","\n","try:\n","    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n","    print(\"Compute Messages:\")\n","    \n","    # Parse and print the compute messages in a more visually friendly way\n","    messages = results_summary_string[0].split('\\r\\n')\n","    \n","    for message in messages:\n","        if message.strip():  # Skip empty lines\n","            if ':' in message:\n","                key, value = message.split(':', 1)\n","                print(f\"{key.strip():40} : {value.strip()}\")\n","            else:\n","                print(f\"\\n{message.strip()}\")\n","    \n","    # Print computation summary in a table format\n","    print(\"\\nComputation Summary:\")\n","    print(\"-\" * 50)\n","    print(f\"{'Computation Task':<30} {'Time':<20}\")\n","    print(\"-\" * 50)\n","    for line in messages:\n","        if 'Computation Task' in line:\n","            task, time = line.split('\\t')\n","            print(f\"{task:<30} {time:<20}\")\n","    \n","    print(\"\\nComputation Speed:\")\n","    print(\"-\" * 50)\n","    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n","    print(\"-\" * 50)\n","    for line in messages:\n","        if 'Computation Speed' in line:\n","            task, speed = line.split('\\t')\n","            print(f\"{task:<30} {speed:<20}\")\n","\n","except Exception as e:\n","    print(f\"Error extracting compute messages: {str(e)}\")\n","    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n","\n"," \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example 12: Extract Plan Parameters and Volume Accounting\n","print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n","\n","# Extract plan parameters\n","plan_parameters_df = HdfPlan.get_plan_parameters(hdf_path=plan_hdf_path)\n","\n","# Extract volume accounting data\n","volume_accounting_df = HdfResultsPlan.get_volume_accounting(hdf_path=plan_hdf_path)\n","\n","print(\"\\nPlan Parameters DataFrame:\")\n","display(plan_parameters_df)\n","\n","print(\"\\nVolume Accounting DataFrame:\")\n","display(volume_accounting_df)"]},{"cell_type":"markdown","metadata":{},"source":["# RasPlanHdf Class Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get simulation start time\n","start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n","print(f\"Simulation start time: {start_time}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get plan end time\n","end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n","print(f\"Simulation end time: {end_time}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Modify the cell below to time of max wsel for 1D models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the time of maximum water surface elevation (WSEL) for cross sections\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","# Get cross section results timeseries\n","xsec_results = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n","print(\"\\nCross Section Results Shape:\", xsec_results['Water_Surface'].shape)\n","\n","# Get cross section geometry data\n","xsec_geom = HdfXsec.get_cross_sections(plan_hdf_path)\n","print(\"\\nNumber of cross sections in geometry:\", len(xsec_geom))\n","\n","# Create dataframe with cross section locations and max WSEL times\n","xs_data = []\n","\n","# Extract water surface data from xarray Dataset\n","water_surface = xsec_results['Water_Surface'].values\n","times = pd.to_datetime(xsec_results.time.values)\n","\n","# Debug print\n","print(\"\\nFirst few cross section names:\")\n","print(xsec_results.cross_section.values[:5])\n","\n","# Iterate through cross sections\n","for xs_idx in range(len(xsec_results.cross_section)):\n","    # Get WSEL timeseries for this cross section\n","    wsel_series = water_surface[:, xs_idx]\n","    \n","    # Get cross section name and parse components\n","    xs_name = xsec_results.cross_section.values[xs_idx]\n","    \n","    # Split the string and remove empty strings\n","    xs_parts = [part for part in xs_name.split() if part]\n","    \n","    if len(xs_parts) >= 3:\n","        river = \"Bald Eagle\"  # Combine first two words\n","        reach = \"Loc Hav\"     # Next two words\n","        rs = xs_parts[-1]     # Last part is the station\n","        \n","        # Get geometry for this cross section\n","        xs_match = xsec_geom[\n","            (xsec_geom['River'] == river) & \n","            (xsec_geom['Reach'] == reach) & \n","            (xsec_geom['RS'] == rs)\n","        ]\n","        \n","        if not xs_match.empty:\n","            geom = xs_match.iloc[0]\n","            # Use first point of cross section line for plotting\n","            x = geom.geometry.coords[0][0]\n","            y = geom.geometry.coords[0][1]\n","            \n","            # Find time of max WSEL\n","            max_wsel_idx = np.argmax(wsel_series)\n","            max_wsel = np.max(wsel_series)\n","            max_time = times[max_wsel_idx]\n","            \n","            xs_data.append({\n","                'xs_name': xs_name,\n","                'x': x,\n","                'y': y,\n","                'max_wsel': max_wsel,\n","                'time_of_max': max_time\n","            })\n","        else:\n","            print(f\"\\nWarning: No geometry match found for {xs_name}\")\n","            print(f\"River: {river}, Reach: {reach}, RS: {rs}\")\n","    else:\n","        print(f\"\\nWarning: Could not parse cross section name: {xs_name}\")\n","\n","# Create dataframe\n","xs_df = pd.DataFrame(xs_data)\n","\n","# Debug print\n","print(\"\\nNumber of cross sections processed:\", len(xs_df))\n","if not xs_df.empty:\n","    print(\"\\nColumns in xs_df:\", xs_df.columns.tolist())\n","    print(\"\\nFirst row of xs_df:\")\n","    print(xs_df.iloc[0])\n","\n","    # Create the plot\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","\n","    # Convert datetime to hours since start for colormap\n","    min_time = min(xs_df['time_of_max'])\n","    color_values = [(t - min_time).total_seconds() / 3600 for t in xs_df['time_of_max']]\n","\n","    # Plot cross section points\n","    scatter = ax.scatter(xs_df['x'], xs_df['y'],\n","                        c=color_values,\n","                        cmap='viridis',\n","                        s=50)\n","\n","    # Customize plot\n","    ax.set_title('Time of Maximum Water Surface Elevation at Cross Sections')\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","\n","    # Add colorbar\n","    cbar = plt.colorbar(scatter)\n","    cbar.set_label('Hours since simulation start')\n","\n","    # Format colorbar ticks\n","    max_hours = int(max(color_values))\n","    tick_interval = max(1, max_hours // 6)  # Show ~6 ticks\n","    cbar.set_ticks(range(0, max_hours + 1, tick_interval))\n","    cbar.set_ticklabels([f'{h}h' for h in range(0, max_hours + 1, tick_interval)])\n","\n","    # Add grid and adjust styling\n","    ax.grid(True, linestyle='--', alpha=0.7)\n","    plt.rcParams.update({'font.size': 12})\n","    plt.tight_layout()\n","\n","    # Show plot\n","    plt.show()\n","\n","    # Print summary statistics\n","    max_wsel_xs = xs_df.loc[xs_df['max_wsel'].idxmax()]\n","    hours_since_start = (max_wsel_xs['time_of_max'] - min_time).total_seconds() / 3600\n","\n","    print(f\"\\nOverall Maximum WSEL: {max_wsel_xs['max_wsel']:.2f} ft\")\n","    print(f\"Time of Overall Maximum WSEL: {max_wsel_xs['time_of_max']}\")\n","    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n","    print(f\"Location of Overall Maximum WSEL: X={max_wsel_xs['x']:.2f}, Y={max_wsel_xs['y']:.2f}\")\n","    print(f\"Cross Section: {max_wsel_xs['xs_name']}\")\n","else:\n","    print(\"\\nWarning: No cross sections were processed successfully\")\n","    print(\"xs_data length:\", len(xs_data))"]},{"cell_type":"markdown","metadata":{},"source":["### Need to add this to the ras-commander library"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get unsteady attributes\n","results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nResults Unsteady Attributes:\")\n","for key, value in results_unsteady_attrs.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get unsteady summary attributes\n","results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nResults Unsteady Summary Attributes:\")\n","for key, value in results_unsteady_summary_attrs.items():\n","    print(f\"{key}: {value}\")\n","\n","# Get volume accounting attributes\n","volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nVolume Accounting Attributes:\")\n","for key, value in volume_accounting_attrs.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"\\n=== HDF5 File Structure ===\\n\")\n","print(plan_hdf_path)\n","HdfBase.get_dataset_info(plan_hdf_path, group_path='/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xsec_results = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n","print(xsec_results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print time series for specific cross section\n","target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n","\n","print(\"\\nTime Series Data for Cross Section:\", target_xs)\n","for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n","    print(f\"\\n{var}:\")\n","    print(xsec_results[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n","\n","# Create time series plots\n","import matplotlib.pyplot as plt\n","\n","# Create a figure for each variable\n","variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n","\n","for var in variables:\n","    plt.figure(figsize=(10, 5))\n","    # Convert time values to datetime if needed\n","    time_values = pd.to_datetime(xsec_results.time.values)\n","    values = xsec_results[var].sel(cross_section=target_xs).values\n","    \n","    # Plot with explicit x and y values\n","    plt.plot(time_values, values, '-', linewidth=2)\n","    \n","    plt.title(f'{var} at {target_xs}')\n","    plt.xlabel('Time')\n","    plt.ylabel(var.replace('_', ' '))\n","    plt.grid(True)\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    \n","    # Force display\n","    plt.draw()\n","    plt.pause(0.1)\n","    plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":2}

==================================================

File: c:\GH\ras-commander\examples\20_2d_hdf_data_extraction.ipynb
==================================================
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# HEC-RAS 2D HDF Data Analysis Notebook\n","\n","This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Import required Libraries\n","import subprocess\n","import sys\n","import os\n","from pathlib import Path\n","\n","def install_module(module_name):\n","    try:\n","        __import__(module_name)\n","    except ImportError:\n","        print(f\"{module_name} not found. Installing...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n","\n","# List of modules to check and install if necessary\n","modules = ['h5py', 'numpy', 'requests', 'geopandas', 'matplotlib', 'pandas', 'pyproj', 'shapely', 'xarray', 'rasterio']\n","for module in modules:\n","    install_module(module)\n","\n","# Import the rest of the required libraries\n","import pandas as pd\n","import numpy as np\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import pyproj\n","from shapely.geometry import Point, LineString, Polygon\n","import xarray as xr\n","from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n","import matplotlib.patches as patches\n","from matplotlib.patches import ConnectionPatch\n","import logging\n","from pathlib import Path\n","import rasterio\n","from rasterio.plot import show\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Install ras-commander if you are not in a dev environment. \n","# install_module(ras-commander)"]},{"cell_type":"markdown","metadata":{},"source":["## Importing ras-commander flexibly (from package or local dev copy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Flexible Import for RAS Commander\n","import sys\n","from pathlib import Path\n","\n","# Flexible imports to allow for development without installation \n","#  ** Use this version with Jupyter Notebooks **\n","try:\n","    # Try to import from the installed package\n","    from ras_commander import (init_ras_project, HdfBase, HdfUtils, HdfFluvialPluvial, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","except ImportError:\n","    # If the import fails, add the parent directory to the Python path\n","    print(\"Using Local Dev Copy\")\n","    import os\n","    current_file = Path(os.getcwd()).resolve()\n","    parent_directory = current_file.parent\n","    sys.path.append(str(parent_directory))\n","    \n","    # Now try to import again\n","    from ras_commander import (init_ras_project, HdfBase, HdfUtils, HdfFluvialPluvial, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","\n","print(\"ras_commander imported successfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download the BaldEagleCrkMulti2D project from HEC and run plan 01\n","\n","# Define the path to the BaldEagleCrkMulti2D project\n","current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n","bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n","import logging\n","\n","# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n","hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n","\n","if not hdf_file.exists():\n","    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n","    ras_examples = RasExamples()\n","    ras_examples.extract_project([\"BaldEagleCrkMulti2D\"])\n","\n","    # Initialize custom Ras object\n","    bald_eagle = RasPrj()\n","\n","    # Initialize the RAS project using the custom ras object\n","    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n","    logging.info(f\"Bald Eagle project initialized with folder: {bald_eagle.project_folder}\")\n","    \n","    logging.info(f\"Bald Eagle object id: {id(bald_eagle)}\")\n","    \n","    # Define the plan number to execute\n","    plan_number = \"06\"\n","\n","    # Update run flags for the project\n","    RasPlan.update_run_flags(\n","        plan_number,\n","        geometry_preprocessor=True,\n","        unsteady_flow_simulation=True,\n","        run_sediment=False,\n","        post_processor=True,\n","        floodplain_mapping=False,\n","        ras_object=bald_eagle\n","    )\n","\n","    # Execute Plan 06 using RasCmdr for Bald Eagle\n","    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n","    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n","    if success_bald_eagle:\n","        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n","    else:\n","        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n","else:\n","    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n","    # Initialize the RAS project using the custom ras object\n","    bald_eagle = RasPrj()\n","    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n","    plan_number = \"06\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n","\n","# Display plan_df for bald_eagle project\n","print(\"Plan DataFrame for bald_eagle project:\")\n","display(bald_eagle.plan_df)\n","\n","# Display geom_df for bald_eagle project\n","print(\"\\nGeometry DataFrame for bald_eagle project:\")\n","display(bald_eagle.geom_df)\n","\n","# Get the plan HDF path\n","plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n","plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n","\n","# Get the geometry file number from the plan DataFrame\n","geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n","geom_number = geom_file[1:]  # Remove the 'g' prefix\n","\n","# Get the geometry HDF path\n","geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n","\n","print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n","print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Define the HDF input path as Plan Number\n","\n","plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n"]},{"cell_type":"markdown","metadata":{},"source":["RasHdfUtils\n","| Method Name | Description |\n","|-------------|-------------|\n","| get_attrs | Converts attributes from a HEC-RAS HDF file into a Python dictionary for a given attribute path |\n","| get_root_attrs | Returns attributes at root level of HEC-RAS HDF file |\n","| get_hdf_paths_with_properties | Gets all paths in the HDF file with their properties |\n","| get_group_attributes_as_df | Gets attributes of a group in the HDF file as a DataFrame |\n","| get_hdf_filename | Gets the HDF filename from various input types |\n","| get_runtime_data | Extracts runtime and compute time data from a single HDF file |\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get HDF Paths with Properties (For Exploring HDF Files)\n","HdfBase.get_dataset_info(plan_number, ras_object=bald_eagle, group_path=\"/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract runtime and compute time data\n","print(\"\\nExample 2: Extracting runtime and compute time data\")\n","runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=bald_eagle)\n","if runtime_df is not None:\n","    display(runtime_df)\n","else:\n","    print(\"No runtime data found.\")"]},{"cell_type":"markdown","metadata":{},"source":["runtime_df example output: \n","\n","| Plan Name                        | File Name                     | Simulation Start Time | Simulation End Time | Simulation Duration (s) | Simulation Time (hr) | Completing Geometry (hr) | Preprocessing Geometry (hr) | Completing Event Conditions (hr) | Unsteady Flow Computations (hr) | Complete Process (hr) | Unsteady Flow Speed (hr/hr) | Complete Process Speed (hr/hr) |\n","|----------------------------------|-------------------------------|-----------------------|---------------------|-------------------------|-----------------------|--------------------------|------------------------------|----------------------------------|----------------------------------|-----------------------|------------------------------|----------------------------------|\n","| Gridded Precip - Infiltration    | BaldEagleDamBrk.p06.hdf      | 09Sep2018 00:00:00    | 14Sep2018 00:00:00  | 432000.0                | 120.0                 | N/A                      | 0.000113                     | N/A                              | 0.074436                        | 0.080951              | 1612.126776                  | 1482.386368                      |"]},{"cell_type":"markdown","metadata":{},"source":["Table of all the functions in the RasGeomHdf class from the ras_commander/RasGeomHdf.py file:\n","\n","| Function Name | Description |\n","|---------------|-------------|\n","| projection | Returns the projection of the RAS geometry as a pyproj.CRS object |\n","| get_geom_attrs | Returns base geometry attributes from a HEC-RAS HDF file |\n","\n","| mesh_area_names | Returns a list of the 2D mesh area names of the RAS geometry |\n","| get_geom_2d_flow_area_attrs | Returns geometry 2d flow area attributes from a HEC-RAS HDF file |\n","| mesh_areas | Returns 2D flow area perimeter polygons |\n","| mesh_cell_polygons | Returns 2D flow mesh cell polygons |\n","| mesh_cell_points | Returns 2D flow mesh cell points |\n","| mesh_cell_faces | Returns 2D flow mesh cell faces |\n","\n","| get_geom_structures_attrs | Returns geometry structures attributes from a HEC-RAS HDF file |\n","\n","\n","\n","\n","| bc_lines | Returns 2D mesh area boundary condition lines |\n","| breaklines | Returns 2D mesh area breaklines |\n","\n","\n","\n","| refinement_regions | Returns 2D mesh area refinement regions |\n","| structures | Returns the model structures |\n","| reference_lines_names | Returns reference line names |\n","| reference_points_names | Returns reference point names |\n","| reference_lines | Returns the reference lines geometry and attributes |\n","| reference_points | Returns the reference points geometry and attributes |\n","| cross_sections | Returns the model 1D cross sections |\n","| river_reaches | Returns the model 1D river reach lines |\n","| cross_sections_elevations | Returns the model cross section elevation information |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n","print(geom_hdf_path)\n","\n","# For the example project, plan 06 is associated with geometry 09\n","# If you want to call the geometry by number, call RasHdfGeom functions with a number\n","# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfUtils for extracting projection\n","print(\"\\nExtracting Projection from HDF\")\n","projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n","if projection:\n","    print(f\"Projection: {projection}\")\n","else:\n","    print(\"No projection information found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfPlan for geometry-related operations\n","print(\"\\nExample: Extracting Geometry Information\")\n","geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n","display(geom_attrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["geom_attrs output: \n","\n","| Complete Geometry | Extents | Geometry Time | Infiltration Date Last Modified | Infiltration File Date | Infiltration Filename | Infiltration Layername | Land Cover Date Last Modified | Land Cover File Date | Land Cover Filename | ... | Percent Impervious Date Last Modified | Percent Impervious File Date | Percent Impervious Filename | Percent Impervious Layername | SI Units | Terrain File Date | Terrain Filename | Terrain Layername | Title | Version |\n","|-------------------|---------|---------------|---------------------------------|-----------------------|----------------------|------------------------|------------------------------|----------------------|---------------------|-----|--------------------------------------|-----------------------------|----------------------------|------------------------------|----------|-------------------|------------------|-------------------|-------|---------|\n","| 0                 | True    | [1960041.35636708, 2092643.59732271, 285497.89...] | 27Oct2024 20:09:19 | 11MAR2022 13:52:44 | 24NOV2020 13:24:58 | .\\Soils Data\\Infiltration.hdf | Infiltration | 11MAR2022 13:45:08 | 11MAR2022 13:45:08 | .\\Land Classification\\LandCover.hdf | ... | 11MAR2022 13:45:08 | 11MAR2022 13:45:08 | .\\Land Classification\\LandCover.hdf | LandCover | False | 09FEB2015 08:26:58 | .\\Terrain\\Terrain50.hdf | Terrain50 | Single 2D Area - Internal Dam Structure | 1.0.20 (20Sep2024) |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfMesh for geometry-related operations\n","print(\"\\nExample 3: Listing 2D Flow Area Names\")\n","flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)\n","print(\"2D Flow Area Name (returned as list):\\n\", flow_area_names)\n","# Note: this is returned as a list because it is used internally by other functions.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get 2D Flow Area Attributes (get_mesh_area_attributes)\n","print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n","flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n","display(flow_area_attributes)"]},{"cell_type":"markdown","metadata":{},"source":["flow_area_df:\n","\n","Value\n","| Name                        | b'BaldEagleCr' |\n","|-----------------------------|-----------------|\n","| Locked                      | 0               |\n","| Mann                        | 0.04            |\n","| Multiple Face Mann n       | 0               |\n","| Composite LC               | 0               |\n","| Cell Vol Tol               | 0.01            |\n","| Cell Min Area Fraction      | 0.01            |\n","| Face Profile Tol           | 0.01            |\n","| Face Area Tol              | 0.01            |\n","| Face Conv Ratio            | 0.02            |\n","| Laminar Depth              | 0.2             |\n","| Min Face Length Ratio      | 0.05            |\n","| Spacing dx                 | 250.0           |\n","| Spacing dy                 | 250.0           |\n","| Shift dx                   | NaN             |\n","| Shift dy                   | NaN             |\n","| Cell Count                 | 18066           |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get 2D Flow Area Perimeter Polygons (get_mesh_areas)\n","print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n","mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path, ras_object=bald_eagle)\n","\n","# Plot the 2D Flow Area Perimeter Polygons\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(figsize=(12, 8))\n","mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none')\n","\n","# Add labels for each polygon\n","for idx, row in mesh_areas.iterrows():\n","    centroid = row.geometry.centroid\n","    # Check if 'Name' column exists, otherwise use a default label\n","    label = row.get('Name', f'Area {idx}')\n","    ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n","\n","plt.title('2D Flow Area Perimeter Polygons')\n","plt.xlabel('Easting')\n","plt.ylabel('Northing')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract mesh cell faces\n","print(\"\\nExample: Extracting mesh cell faces\")\n","\n","# Get mesh cell faces\n","mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path, ras_object=bald_eagle)\n","\n","# Display the first few rows of the mesh cell faces DataFrame\n","print(\"First few rows of mesh cell faces:\")\n","display(mesh_cell_faces.head())"]},{"cell_type":"markdown","metadata":{},"source":["mesh_cell_faces geodataframe:\n","\n","flow_area_df:\n","\n","| mesh_name    | face_id | geometry                                           |\n","|--------------|---------|----------------------------------------------------|\n","| BaldEagleCr  | 0       | LINESTRING (2042125 351625, 2042375 351625)      |\n","| BaldEagleCr  | 1       | LINESTRING (2042375 351625, 2042375 351875)      |\n","| BaldEagleCr  | 2       | LINESTRING (2042375 351875, 2042125 351875)      |\n","| BaldEagleCr  | 3       | LINESTRING (2042125 351875, 2042125 351625)      |\n","| BaldEagleCr  | 4       | LINESTRING (2042375 351375, 2042375 351625)      |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib.collections import LineCollection\n","import numpy as np\n","\n","# Plot the mesh cell faces more efficiently\n","fig, ax = plt.subplots(figsize=(12, 8))\n","\n","# Convert all geometries to numpy arrays at once for faster plotting\n","lines = [list(zip(*line.xy)) for line in mesh_cell_faces.geometry]\n","lines_collection = LineCollection(lines, colors='blue', linewidth=0.5, alpha=0.5)\n","ax.add_collection(lines_collection)\n","\n","# Set plot title and labels\n","plt.title('Mesh Cell Faces')\n","plt.xlabel('Easting')\n","plt.ylabel('Northing')\n","\n","# Calculate centroids once and store as numpy arrays\n","centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in mesh_cell_faces.geometry])\n","\n","# Create scatter plot with numpy arrays\n","scatter = ax.scatter(\n","    centroids[:, 0],\n","    centroids[:, 1], \n","    c=mesh_cell_faces['face_id'],\n","    cmap='viridis',\n","    s=1,\n","    alpha=0.5\n",")\n","plt.colorbar(scatter, label='Face ID')\n","\n","# Set axis limits based on data bounds\n","ax.set_xlim(centroids[:, 0].min(), centroids[:, 0].max())\n","ax.set_ylim(centroids[:, 1].min(), centroids[:, 1].max())\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Calculate and display some statistics\n","print(\"\\nMesh Cell Faces Statistics:\")\n","print(f\"Total number of cell faces: {len(mesh_cell_faces)}\")\n","print(f\"Number of unique meshes: {mesh_cell_faces['mesh_name'].nunique()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to find the nearest cell face to a given point\n","def find_nearest_cell_face(point, cell_faces_df):\n","    \"\"\"\n","    Find the nearest cell face to a given point.\n","\n","    Args:\n","        point (shapely.geometry.Point): The input point.\n","        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n","\n","    Returns:\n","        int: The face_id of the nearest cell face.\n","        float: The distance to the nearest cell face.\n","    \"\"\"\n","    # Calculate distances from the input point to all cell faces\n","    distances = cell_faces_df.geometry.distance(point)\n","\n","    # Find the index of the minimum distance\n","    nearest_index = distances.idxmin()\n","\n","    # Get the face_id and distance of the nearest cell face\n","    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n","    nearest_distance = distances[nearest_index]\n","\n","    return nearest_face_id, nearest_distance\n","\n","# Example usage\n","print(\"\\nExample: Finding the nearest cell face to a given point\")\n","\n","# Create a sample point (you can replace this with any point of interest)\n","from shapely.geometry import Point\n","from geopandas import GeoDataFrame\n","\n","# Get the projection from the geometry file\n","# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n","if projection:\n","    print(f\"Using projection: {projection}\")\n","else:\n","    print(\"No projection information found. Using default CRS.\")\n","    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n","\n","# Create the sample point with the correct CRS\n","sample_point = GeoDataFrame({'geometry': [Point(2042250, 351750)]}, crs=projection)\n","\n","if not mesh_cell_faces.empty and not sample_point.empty:\n","    # Ensure the CRS of the sample point matches the mesh_cell_faces\n","    if sample_point.crs != mesh_cell_faces.crs:\n","        sample_point = sample_point.to_crs(mesh_cell_faces.crs)\n","    \n","    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n","    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n","    print(f\"Face ID: {nearest_face_id}\")\n","    print(f\"Distance: {distance:.2f} units\")\n","\n","    # Visualize the result\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot all cell faces\n","    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n","    \n","    # Plot the sample point\n","    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n","    \n","    # Plot the nearest cell face\n","    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n","    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","    ax.set_title('Nearest Cell Face to Sample Point')\n","    \n","    # Add legend and grid\n","    ax.legend()\n","    ax.grid(True)\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Unable to perform nearest cell face search due to missing data.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract Cell Polygons\n","print(\"\\nExample 6: Extracting Cell Polygons\")\n","cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path, ras_object=bald_eagle)"]},{"cell_type":"markdown","metadata":{},"source":["cell_polygons_df:\n","\n","| mesh_name    | cell_id | geometry                                      |\n","|--------------|---------|-----------------------------------------------|\n","| BaldEagleCr  | 0       | POLYGON ((2082875 370625, 2082723.922 370776.0... |\n","| BaldEagleCr  | 1       | POLYGON ((2083125 370625, 2083125 370844.185, ... |\n","| BaldEagleCr  | 2       | POLYGON ((2083375 370625, 2083375 370886.638, ... |\n","| BaldEagleCr  | 3       | POLYGON ((2083625 370625, 2083625 370925.693, ... |\n","| BaldEagleCr  | 4       | POLYGON ((2083875 370625, 2083875 370958.588, ... |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot Cell Polygons\n","if not cell_polygons_df.empty:\n","    display(cell_polygons_df.head())\n","else:\n","    print(\"No Cell Polygons found.\")\n","\n","# Plot cell polygons\n","if not cell_polygons_df.empty:\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot cell polygons\n","    cell_polygons_df.plot(ax=ax, edgecolor='blue', facecolor='none')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","    ax.set_title('2D Flow Area Cell Polygons')\n","    \n","    # Add grid\n","    ax.grid(True)\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No cell polygon data available for plotting.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract Cell Info\n","print(\"\\nExample 5: Extracting Cell Info\")\n","cell_info_df = HdfMesh.get_mesh_cell_points(geom_hdf_path, ras_object=bald_eagle)"]},{"cell_type":"markdown","metadata":{},"source":["cell_info_df: \n","\n","| mesh_name    | cell_id | geometry                          |\n","|--------------|---------|-----------------------------------|\n","| BaldEagleCr  | 0       | POINT (2083000 370750)           |\n","| BaldEagleCr  | 1       | POINT (2083250 370750)           |\n","| BaldEagleCr  | 2       | POINT (2083500 370750)           |\n","| BaldEagleCr  | 3       | POINT (2083750 370750)           |\n","| BaldEagleCr  | 4       | POINT (2084000 370750)           |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot Cell Info\n","if not cell_info_df.empty:\n","    display(cell_info_df.head())\n","else:\n","    print(\"No Cell Info found.\")\n","\n","# Plot cell centers\n","import matplotlib.pyplot as plt\n","\n","if not cell_info_df.empty:\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot cell centers\n","    cell_info_df.plot(ax=ax, color='red', markersize=5)\n","    \n","    # Set labels and title\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","    ax.set_title('2D Flow Area Cell Centers')\n","    \n","    # Add grid\n","    ax.grid(True)\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No cell data available for plotting.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to find the nearest cell center to a given point\n","def find_nearest_cell(point, cell_centers_df):\n","    \"\"\"\n","    Find the nearest cell center to a given point.\n","\n","    Args:\n","        point (shapely.geometry.Point): The input point.\n","        cell_centers_df (GeoDataFrame): DataFrame containing cell center points.\n","\n","    Returns:\n","        int: The cell_id of the nearest cell.\n","        float: The distance to the nearest cell center.\n","    \"\"\"\n","    # Calculate distances from the input point to all cell centers\n","    distances = cell_centers_df.geometry.distance(point)\n","\n","    # Find the index of the minimum distance\n","    nearest_index = distances.idxmin()\n","\n","    # Get the cell_id and distance of the nearest cell\n","    nearest_cell_id = cell_centers_df.loc[nearest_index, 'cell_id']\n","    nearest_distance = distances[nearest_index]\n","\n","    return nearest_cell_id, nearest_distance\n","\n","# Example usage\n","print(\"\\nExample: Finding the nearest cell to a given point\")\n","\n","# Create a sample point (you can replace this with any point of interest)\n","from shapely.geometry import Point\n","from geopandas import GeoDataFrame\n","\n","# Get the projection from the geometry file\n","# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n","if projection:\n","    print(f\"Using projection: {projection}\")\n","else:\n","    print(\"No projection information found. Using default CRS.\")\n","    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n","\n","# Create the sample point with the correct CRS\n","sample_point = GeoDataFrame({'geometry': [Point(2083500, 370800)]}, crs=projection)\n","\n","if not cell_info_df.empty and not sample_point.empty:\n","    # Ensure the CRS of the sample point matches the cell_info_df\n","    if sample_point.crs != cell_info_df.crs:\n","        sample_point = sample_point.to_crs(cell_info_df.crs)\n","    \n","    nearest_cell_id, distance = find_nearest_cell(sample_point.geometry.iloc[0], cell_info_df)\n","    print(f\"Nearest cell to point {sample_point.geometry.iloc[0].coords[0]}:\")\n","    print(f\"Cell ID: {nearest_cell_id}\")\n","    print(f\"Distance: {distance:.2f} units\")\n","\n","    # Visualize the result\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot all cell centers\n","    cell_info_df.plot(ax=ax, color='blue', markersize=5, alpha=0.5, label='Cell Centers')\n","    \n","    # Plot the sample point\n","    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n","    \n","    # Plot the nearest cell center\n","    nearest_cell = cell_info_df[cell_info_df['cell_id'] == nearest_cell_id]\n","    nearest_cell.plot(ax=ax, color='green', markersize=100, alpha=0.7, label='Nearest Cell')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","    ax.set_title('Nearest Cell to Sample Point')\n","    \n","    # Add legend and grid\n","    ax.legend()\n","    ax.grid(True)\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Unable to perform nearest cell search due to missing data.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get geometry structures attributes\n","print(\"\\nGetting geometry structures attributes\")\n","geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path, ras_object=bald_eagle)\n","if geom_structures_attrs:\n","    print(\"Geometry structures attributes:\")\n","    for key, value in geom_structures_attrs.items():\n","        print(f\"{key}: {value}\")\n","else:\n","    print(\"No geometry structures attributes found.\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# TODO: Paths and Functions for each type of structure: \n","\n","# Getting geometry structures attributes\n","# Geometry structures attributes:\n","# Bridge/Culvert Count: 0\n","# Connection Count: 4\n","# Has Bridge Opening (2D): 0\n","# Inline Structure Count: 0\n","# Lateral Structure Count: 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract Boundary Condition Lines and Plot with 2D Flow Area Perimeter Polygons\n","print(\"\\nExample 7: Extracting Boundary Condition Lines and Plotting with 2D Flow Area Perimeter Polygons\")\n","bc_lines_df = HdfBndry.get_bc_lines(geom_hdf_path, ras_object=bald_eagle)\n","\n","if not bc_lines_df.empty:\n","    display(bc_lines_df.head())\n","else:\n","    print(\"No Boundary Condition Lines found.\")"]},{"cell_type":"markdown","metadata":{},"source":["| bc_line_id |         name         |    mesh_name    |    type    |                                           geometry                                            |\n","|-------------|----------------------|------------------|------------|------------------------------------------------------------------------------------------------|\n","|      0      |     DSNormalDepth    |   BaldEagleCr    |  External  | LINESTRING (2082004.235 364024.82, 2083193.546...)                                          |\n","|      1      |       DS2NormalD     |   BaldEagleCr    |  External  | LINESTRING (2084425.804 365392.892, 2084354.64...)                                          |\n","|      2      |   Upstream Inflow    |   BaldEagleCr    |  External  | LINESTRING (1967473.737 290973.629, 1969582.89...)                                          |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot Boundary Condition Lines with Perimeter\n","# Plot if data exists\n","if not bc_lines_df.empty or not mesh_areas.empty:\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot 2D Flow Area Perimeter Polygons\n","    if not mesh_areas.empty:\n","        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n","        \n","        # Add labels for each polygon\n","        for idx, row in mesh_areas.iterrows():\n","            centroid = row.geometry.centroid\n","            label = row.get('Name', f'Area {idx}')\n","            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n","    \n","    # Plot boundary condition lines\n","    if not bc_lines_df.empty:\n","        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('Easting')\n","    ax.set_ylabel('Northing')\n","    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n","    \n","    # Add grid and legend\n","    ax.grid(True)\n","    ax.legend()\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No data available for plotting.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract Breaklines and Plot with 2D Flow Area Perimeter Polygons\n","print(\"\\nExample 8: Extracting Breaklines and Plotting with 2D Flow Area Perimeter Polygons\")\n","breaklines_gdf = HdfBndry.get_breaklines(geom_hdf_path, ras_object=bald_eagle)\n","if not breaklines_gdf.empty:\n","    display(breaklines_gdf.head())\n","else:\n","    print(\"No Breaklines found.\")"]},{"cell_type":"markdown","metadata":{},"source":["breaklines_gdf:\n","\n","\n","| bl_id | Name      | geometry |\n","|-------|-----------|----------|\n","| 0     | SayersDam | LINESTRING (2002361.246 323707.927, 2002741.35...) |\n","| 1     | Lower     | LINESTRING (2060356.422 351786.819, 2060316.47...) |\n","| 2     | Middle    | LINESTRING (2052757.788 348470.547, 2052785.84...) |\n","| 3     | Upper     | LINESTRING (2045597.199 348412.994, 2045638.91...) |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot breaklines and 2D Flow Area Perimeter Polygons if they exist\n","if not breaklines_gdf.empty or not mesh_areas.empty:\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot 2D Flow Area Perimeter Polygons\n","    if not mesh_areas.empty:\n","        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n","        \n","        # Add labels for each polygon\n","        for idx, row in mesh_areas.iterrows():\n","            centroid = row.geometry.centroid\n","            label = row.get('Name', f'Area {idx}')\n","            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n","    \n","    # Plot breaklines\n","    if not breaklines_gdf.empty:\n","        breaklines_gdf.plot(ax=ax, color='blue', linewidth=2, label='Breaklines')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('Easting')\n","    ax.set_ylabel('Northing')\n","    ax.set_title('2D Flow Area Perimeter Polygons and Breaklines')\n","    \n","    # Add grid and legend\n","    ax.grid(True)\n","    ax.legend()\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No data available for plotting.\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# INSTEAD OF hdf_input, USE plan_hdf_path or geom_hdf_path as appropriate "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get structures\n","structures_gdf = HdfStruc.get_structures(geom_hdf_path, ras_object=bald_eagle)\n","print(\"Structures:\")\n","if not structures_gdf.empty:\n","    display(structures_gdf.head())\n","else:\n","    print(\"No structures found in the geometry file.\")"]},{"cell_type":"markdown","metadata":{},"source":["structures_gdf: \n","\n","| Type | Mode | River | Reach | RS | Connection | Groupname | US Type | US River | US Reach | ... | US XS Mann (Count) | US BR Mann (Index) | US BR Mann (Count) | DS XS Mann (Index) | DS XS Mann (Count) | DS BR Mann (Index) | DS BR Mann (Count) | RC (Index) | RC (Count) | Profile_Data |\n","|------|------|-------|-------|-------|------------|-----------|----------|-----------|-----------|-----|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|------------|------------|--------------|\n","| Connection | Weir/Gate/Culverts | | | | Sayers Dam | BaldEagleCr, Sayers Dam | 2D | | | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | [{'Station': 0.0, 'Elevation': 683.0}, {'Stati... |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get boundary condition lines\n","ref_lines_gdf = HdfBndry.get_bc_lines(geom_hdf_path)\n","print(\"\\nBoundary Condition Lines:\")\n","if not ref_lines_gdf.empty:\n","    display(ref_lines_gdf.head())\n","else:\n","    print(\"No boundary condition lines found in the geometry file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get reference points\n","ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)\n","print(\"\\nReference Points:\")\n","if not ref_points_gdf.empty:\n","    display(ref_points_gdf.head())\n","else:\n","    print(\"No reference points found in the geometry file.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Extract Breakline as Reference Line\n","\n","We can't use a profile line, because the mesh orientation may be quite different than the direction of flow.  \n","\n","Instead, use a breakline - the one named \"SayersDam\" should work\n","\n","We can find the information specific to faces: \n","\n","\n","\n","\n","\n","\n","# Extract Composite Results for 2D at Profile Lines to simulate Reference Lines\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract Refinement Regions\n","print(\"\\nExample: Extracting Refinement Regions\")\n","\n","# Make sure to pass the bald_eagle object as the ras_object parameter\n","refinement_regions_df = HdfBndry.get_refinement_regions(geom_hdf_path, ras_object=bald_eagle)\n","\n","if not refinement_regions_df.empty:\n","    print(\"Refinement Regions DataFrame:\")\n","    display(refinement_regions_df.head())\n","    \n","    # Plot refinement regions\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    refinement_regions_df.plot(ax=ax, column='CellSize', legend=True, \n","                               legend_kwds={'label': 'Cell Size', 'orientation': 'horizontal'},\n","                               cmap='viridis')\n","    ax.set_title('2D Mesh Area Refinement Regions')\n","    ax.set_xlabel('Easting')\n","    ax.set_ylabel('Northing')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No refinement regions found in the geometry file.\")\n","\n","# Example: Analyze Refinement Regions\n","if not refinement_regions_df.empty:\n","    print(\"\\nRefinement Regions Analysis:\")\n","    print(f\"Total number of refinement regions: {len(refinement_regions_df)}\")\n","    print(\"\\nCell Size Statistics:\")\n","    print(refinement_regions_df['CellSize'].describe())\n","    \n","    # Group by Shape Type\n","    shape_type_counts = refinement_regions_df['ShapeType'].value_counts()\n","    print(\"\\nRefinement Region Shape Types:\")\n","    print(shape_type_counts)\n","    \n","    # Plot Shape Type distribution\n","    plt.figure(figsize=(10, 6))\n","    shape_type_counts.plot(kind='bar')\n","    plt.title('Distribution of Refinement Region Shape Types')\n","    plt.xlabel('Shape Type')\n","    plt.ylabel('Count')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract Compute Messages as String\n","print(\"Extracting Compute Messages\")\n","\n","import h5py\n","import numpy as np\n","\n","def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n","    \"\"\"\n","    Extract string from HDF object at a given path\n","\n","    Parameters\n","    ----------\n","    results_hdf_filename : str\n","        Name of the HDF file\n","    hdf_path : str\n","        Path of the object in the HDF file\n","\n","    Returns\n","    -------\n","    str\n","        Extracted string from the specified HDF object\n","    \"\"\"\n","    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n","        try:\n","            hdf_object = hdf_file[hdf_path]\n","            if isinstance(hdf_object, h5py.Group):\n","                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n","            elif isinstance(hdf_object, h5py.Dataset):\n","                data = hdf_object[()]\n","                if isinstance(data, bytes):\n","                    return data.decode('utf-8')\n","                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n","                    return [v.decode('utf-8') for v in data]\n","                else:\n","                    return str(data)\n","            else:\n","                return f\"Unsupported object type: {type(hdf_object)}\"\n","        except KeyError:\n","            return f\"Path not found: {hdf_path}\"\n","\n","try:\n","    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n","    print(\"Compute Messages:\")\n","    \n","    # Parse and print the compute messages in a more visually friendly way\n","    messages = results_summary_string[0].split('\\r\\n')\n","    \n","    for message in messages:\n","        if message.strip():  # Skip empty lines\n","            if ':' in message:\n","                key, value = message.split(':', 1)\n","                print(f\"{key.strip():40} : {value.strip()}\")\n","            else:\n","                print(f\"\\n{message.strip()}\")\n","    \n","    # Print computation summary in a table format\n","    print(\"\\nComputation Summary:\")\n","    print(\"-\" * 50)\n","    print(f\"{'Computation Task':<30} {'Time':<20}\")\n","    print(\"-\" * 50)\n","    for line in messages:\n","        if 'Computation Task' in line:\n","            task, time = line.split('\\t')\n","            print(f\"{task:<30} {time:<20}\")\n","    \n","    print(\"\\nComputation Speed:\")\n","    print(\"-\" * 50)\n","    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n","    print(\"-\" * 50)\n","    for line in messages:\n","        if 'Computation Speed' in line:\n","            task, speed = line.split('\\t')\n","            print(f\"{task:<30} {speed:<20}\")\n","\n","except Exception as e:\n","    print(f\"Error extracting compute messages: {str(e)}\")\n","    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n","\n"," \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Advanced Compute Messages Example - TODO: Move this function into a class of the library \n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","import geopandas as gpd\n","import logging\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","\n","def parse_2d_compute_messages(compute_messages):\n","    \"\"\"\n","    Parse 2D compute messages to extract data lines, clean the data, \n","    and retrieve top 20 cells with the highest error.\n","\n","    Parameters:\n","        compute_messages (list or str): The raw compute messages.\n","\n","    Returns:\n","        tuple: A tuple containing the parsed compute messages string and the main DataFrame.\n","    \"\"\"\n","    try:\n","        # Handle both list and string inputs\n","        if isinstance(compute_messages, list):\n","            compute_messages = '\\n'.join(compute_messages)\n","        elif not isinstance(compute_messages, str):\n","            logging.error(f\"Unexpected type for compute_messages: {type(compute_messages)}\")\n","            return \"\", pd.DataFrame()\n","\n","        # Split the message into lines\n","        lines = compute_messages.split('\\n')\n","        logging.info(\"Successfully split compute messages into lines.\")\n","        \n","        # Initialize lists to store parsed data\n","        data_lines = []\n","        header_lines = []\n","        footer_lines = []\n","        \n","        # Regular expression to match timestamp lines\n","        timestamp_pattern = re.compile(r'^\\d{2}[A-Z]{3}\\d{4}\\s+\\d{2}:\\d{2}:\\d{2}')\n","        logging.debug(\"Compiled timestamp regular expression.\")\n","        \n","        data_started = False\n","        for line in lines:\n","            stripped_line = line.strip()\n","            if timestamp_pattern.match(stripped_line):\n","                data_started = True\n","                # Split the line and add to data_lines\n","                parts = stripped_line.split()\n","                if len(parts) >= 8:  # Ensure we have all expected columns\n","                    # Combine Date and Time into 'Date and Time'\n","                    date_time = f\"{parts[0]} {parts[1]}\"\n","                    location = parts[2]\n","                    cell_type = f\"{parts[3]} {parts[4]}\"\n","                    cell_number = parts[5]\n","                    wsel = parts[6]\n","                    error = parts[7]\n","                    iterations = parts[8] if len(parts) > 8 else None\n","                    data_lines.append([date_time, location, cell_type, cell_number, wsel, error, iterations])\n","                    logging.debug(f\"Parsed data line: {data_lines[-1]}\")\n","                else:\n","                    logging.warning(f\"Line skipped due to insufficient parts: {stripped_line}\")\n","            elif not data_started:\n","                header_lines.append(stripped_line)\n","            elif data_started and not stripped_line:\n","                data_started = False\n","            elif not data_started:\n","                footer_lines.append(stripped_line)\n","        \n","        # Create DataFrame from data lines\n","        df = pd.DataFrame(\n","            data_lines, \n","            columns=['Date and Time', 'Location', 'Cell Type', 'Cell Number', 'WSEL', 'ERROR', 'ITERATIONS']\n","        )\n","        logging.info(\"Created DataFrame from parsed data lines.\")\n","        \n","        # Clean and convert columns to appropriate types\n","        df['Cell Number'] = (\n","            pd.to_numeric(df['Cell Number'].replace('#', pd.NA), errors='coerce')\n","            .fillna(-1)\n","            .astype('Int64')\n","        )\n","        df['WSEL'] = pd.to_numeric(df['WSEL'], errors='coerce')\n","        df['ERROR'] = pd.to_numeric(df['ERROR'], errors='coerce')\n","        df['ITERATIONS'] = pd.to_numeric(df['ITERATIONS'], errors='coerce').astype('Int64')\n","        logging.info(\"Converted DataFrame columns to appropriate types.\")\n","        \n","        # Get top 20 cells with highest error\n","        top_20_cells = (\n","            df.sort_values('ERROR', ascending=False)\n","            .drop_duplicates('Cell Number')\n","            .head(20)\n","        )\n","        \n","        # Construct the reordered message\n","        reordered_message = '\\n'.join(header_lines + \n","                                      ['\\nTop 20 Cells with Highest Error:'] + \n","                                      [' '.join(map(str, row)) for row in top_20_cells.values] + \n","                                      ['\\n'] + footer_lines)\n","        \n","        logging.info(\"Reordered compute messages.\")\n","        \n","        return reordered_message, df\n","    except Exception as e:\n","        logging.error(f\"Error parsing compute messages: {e}\")\n","        return \"\", pd.DataFrame()\n","\n","# Use the function to parse compute messages\n","parsed_messages, df = parse_2d_compute_messages(results_summary_string)\n","\n","print(parsed_messages)\n","print(df)\n","\n","# Get top 20 cells with highest error\n","if not df.empty and 'ERROR' in df.columns:\n","    top_20_cells = (\n","        df.sort_values('ERROR', ascending=False)\n","        .drop_duplicates('Cell Number')\n","        .head(20)\n","    )\n","else:\n","    logging.warning(\"Unable to get top 20 cells with highest error. DataFrame is empty or 'ERROR' column is missing.\")\n","    top_20_cells = pd.DataFrame()\n","\n","# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n","print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n","mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path, ras_object=bald_eagle)\n","\n","print(\"\\n2D Flow Area Groups and Perimeters:\")\n","if not mesh_areas.empty:\n","    print(\"Available columns:\", mesh_areas.columns.tolist())\n","    \n","    # Display the first few rows of the mesh_areas DataFrame\n","    print(\"\\nFirst few rows of mesh_areas DataFrame:\")\n","    display(mesh_areas.head())\n","else:\n","    print(\"No 2D Flow Area groups found in the HDF file.\")\n","\n","# Use the previously extracted cell_polygons_df\n","print(\"\\nTop 20 Cell Polygons:\")\n","if 'cell_polygons_df' in locals() and not cell_polygons_df.empty and not top_20_cells.empty:\n","    # Get the cell numbers from top_20_cells\n","    top_20_cell_numbers = top_20_cells['Cell Number'].tolist()\n","    \n","    # Filter cell_polygons_df to only include top 20 cells\n","    top_20_cell_polygons = cell_polygons_df[cell_polygons_df['cell_id'].isin(top_20_cell_numbers)]\n","    \n","    display(top_20_cell_polygons)\n","\n","    # Plot top 20 cell polygons and mesh areas\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot mesh areas\n","    mesh_areas.plot(ax=ax, edgecolor='red', facecolor='none', alpha=0.5, label='Mesh Areas')\n","    \n","    # Plot top 20 cell polygons\n","    top_20_cell_polygons.plot(ax=ax, edgecolor='blue', facecolor='none', alpha=0.7, label='Top 20 Error Cells')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","    ax.set_title('2D Flow Area Perimeters and Top 20 Cell Polygons')\n","    \n","    # Add legend\n","    ax.legend()\n","    \n","    # Add grid\n","    ax.grid(True)\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"No Cell Polygons found or no top 20 cells with highest error available.\")\n","    print(\"Unable to plot cell polygons.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exploratory Example for Debugging or New Features: List all paths, groups, and attributes under \"/Results/Unsteady/Summary/Volume Accounting\"\n","HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Summary/Volume Accounting\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example 12: Extract Plan Parameters and Volume Accounting\n","print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n","\n","# Extract plan parameters\n","plan_parameters_df = HdfPlan.get_plan_parameters(plan_hdf_path)\n","\n","# Extract volume accounting data\n","volume_accounting_df = HdfResultsPlan.get_volume_accounting(plan_hdf_path)\n","\n","print(\"\\nPlan Parameters DataFrame:\")\n","display(plan_parameters_df)\n","\n","print(\"\\nVolume Accounting DataFrame:\")\n","display(volume_accounting_df)"]},{"cell_type":"markdown","metadata":{},"source":["------"]},{"cell_type":"markdown","metadata":{},"source":["# RasPlanHdf Class Functions"]},{"cell_type":"markdown","metadata":{},"source":["-----"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get plan start time\n","start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n","print(f\"Simulation start time: {start_time}\")"]},{"cell_type":"markdown","metadata":{},"source":["Simulation start time: 2018-09-09 00:00:00"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get plan end time\n","end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n","print(f\"Simulation end time: {end_time}\")"]},{"cell_type":"markdown","metadata":{},"source":["Simulation end time: 2018-09-14 00:00:00"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get maximum iteration count for mesh cells\n","max_iter_df = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)\n","print(\"\\nMesh Max Iterations:\")\n","print(max_iter_df.attrs)\n","display(max_iter_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["max_iter_df:\n","\n","| mesh_name | cell_id | cell_last_iteration | geometry |\n","|-----------|---------|--------------------| ---------|\n","| BaldEagleCr | 0 | 0 | POINT (2083000 370750) |\n","| BaldEagleCr | 1 | 0 | POINT (2083250 370750) |\n","| BaldEagleCr | 2 | 0 | POINT (2083500 370750) |\n","| BaldEagleCr | 3 | 2 | POINT (2083750 370750) |\n","| BaldEagleCr | 4 | 0 | POINT (2084000 370750) |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get maximum iteration count for mesh cells\n","from ras_commander.HdfResultsMesh import HdfResultsMesh\n","\n","max_iter_gdf = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)\n","\n","print(\"max_iter_df\")\n","print(max_iter_df)"]},{"cell_type":"markdown","metadata":{},"source":["mesh_max_iter_df:\n","\n","| mesh_name | cell_id | cell_last_iteration | geometry |\n","|-----------|---------|--------------------| ---------|\n","| BaldEagleCr | 0 | 0 | POINT (2083000 370750) |\n","| ... | ... | ... | ... |\n","| BaldEagleCr | 19592 | 0 | POINT (1978423.032 300718.897) |\n","\n","\n","[19597 rows x 4 columns]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get cell coordinates \n","cell_coords = HdfMesh.get_mesh_cell_points(plan_hdf_path)\n","display(cell_coords)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot Mesh Max Iterations\n","\n","# Extract x and y coordinates from the geometry column\n","max_iter_df['x'] = max_iter_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n","max_iter_df['y'] = max_iter_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n","\n","# Remove rows with None coordinates\n","max_iter_df = max_iter_df.dropna(subset=['x', 'y'])\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","scatter = ax.scatter(max_iter_df['x'], max_iter_df['y'], \n","                     c=max_iter_df['cell_last_iteration'], \n","                     cmap='viridis', \n","                     s=1)\n","\n","# Customize the plot\n","ax.set_title('Max Iterations per Cell')\n","ax.set_xlabel('X Coordinate')\n","ax.set_ylabel('Y Coordinate')\n","plt.colorbar(scatter, label='Max Iterations')\n","\n","# Show the plot\n","plt.show()\n","\n","# Print the first few rows of the dataframe for verification\n","print(\"\\nFirst few rows of the dataframe:\")\n","display(max_iter_df[['mesh_name', 'cell_id', 'geometry']].head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh maximum water surface elevation\n","max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nMesh Maximum Water Surface Elevation:\")\n","print(max_ws_df.attrs)\n","display(max_ws_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["max_ws_df:\n","\n","| mesh_name | cell_id | maximum_water_surface | maximum_water_surface_time | geometry |\n","|-----------|---------|---------------------|--------------------------|-----------|\n","| BaldEagleCr | 0 | 704.054443 | 2018-09-10 18:00:00 | POINT (2083000 370750) |\n","| BaldEagleCr | 1 | 692.377991 | 2018-09-10 18:04:00 | POINT (2083250 370750) |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the max water surface as a map\n","import matplotlib.pyplot as plt\n","from ras_commander.HdfResultsMesh import HdfResultsMesh\n","\n","# Extract x and y coordinates from the geometry column\n","max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n","max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n","\n","# Remove rows with None coordinates\n","max_ws_df = max_ws_df.dropna(subset=['x', 'y'])\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], \n","                     c=max_ws_df['maximum_water_surface'], \n","                     cmap='viridis', \n","                     s=10)\n","\n","# Customize the plot\n","ax.set_title('Max Water Surface per Cell')\n","ax.set_xlabel('X Coordinate')\n","ax.set_ylabel('Y Coordinate')\n","plt.colorbar(scatter, label='Max Water Surface (ft)')\n","\n","# Add grid lines\n","ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Increase font size for better readability\n","plt.rcParams.update({'font.size': 12})\n","\n","# Adjust layout to prevent cutting off labels\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","# Print the first few rows of the dataframe for verification\n","print(\"\\nFirst few rows of the dataframe:\")\n","display(max_ws_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the time of the max water surface elevation (WSEL)\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","from datetime import datetime\n","\n","# Convert the 'maximum_water_surface_time' to datetime objects\n","max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","\n","# Convert datetime to hours since the start for colormap\n","min_time = max_ws_df['max_wsel_time'].min()\n","color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600  # Convert to hours\n","\n","scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], \n","                     c=color_values, \n","                     cmap='viridis', \n","                     s=10)\n","\n","# Customize the plot\n","ax.set_title('Time of Maximum Water Surface Elevation per Cell')\n","ax.set_xlabel('X Coordinate')\n","ax.set_ylabel('Y Coordinate')\n","\n","# Set up the colorbar\n","cbar = plt.colorbar(scatter)\n","cbar.set_label('Hours since simulation start')\n","\n","# Format the colorbar ticks to show hours\n","cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))  # Set ticks every 6 hours\n","cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])\n","\n","# Add grid lines\n","ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Increase font size for better readability\n","plt.rcParams.update({'font.size': 12})\n","\n","# Adjust layout to prevent cutting off labels\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","# Find the overall maximum WSEL and its time\n","max_wsel_row = max_ws_df.loc[max_ws_df['maximum_water_surface'].idxmax()]\n","hours_since_start = (max_wsel_row['max_wsel_time'] - min_time).total_seconds() / 3600\n","print(f\"\\nOverall Maximum WSEL: {max_wsel_row['maximum_water_surface']:.2f} ft\")\n","print(f\"Time of Overall Maximum WSEL: {max_wsel_row['max_wsel_time']}\")\n","print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n","print(f\"Location of Overall Maximum WSEL: X={max_wsel_row['x']}, Y={max_wsel_row['y']}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh minimum water surface elevation\n","min_ws_df = HdfResultsMesh.get_mesh_min_ws(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nMesh Minimum Water Surface Elevation:\")\n","display(min_ws_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh maximum face velocity\n","try:\n","    max_face_v_df = HdfResultsMesh.get_mesh_max_face_v(plan_hdf_path, ras_object=bald_eagle)\n","    print(\"\\nMesh Max Face Velocity:\")\n","    display(max_face_v_df.head())\n","except AttributeError as e:\n","    print(f\"Error: {e}. Please ensure that the method exists in the HdfResultsMesh class.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract midpoint coordinates from the LineString geometries\n","max_face_v_df['x'] = max_face_v_df['geometry'].apply(lambda geom: geom.centroid.x)\n","max_face_v_df['y'] = max_face_v_df['geometry'].apply(lambda geom: geom.centroid.y)\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","scatter = ax.scatter(max_face_v_df['x'], max_face_v_df['y'], \n","                    c=max_face_v_df['maximum_face_velocity'].abs(),\n","                    cmap='viridis',\n","                    s=10)\n","\n","# Customize the plot\n","ax.set_title('Max Face Velocity per Face')\n","ax.set_xlabel('X Coordinate') \n","ax.set_ylabel('Y Coordinate')\n","plt.colorbar(scatter, label='Max Face Velocity (ft/s)')\n","\n","# Add grid lines\n","ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Increase font size for better readability\n","plt.rcParams.update({'font.size': 12})\n","\n","# Adjust layout to prevent cutting off labels\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","# Print the first few rows of the dataframe for verification\n","print(\"\\nFirst few rows of the face velocity dataframe:\")\n","display(max_face_v_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh minimum face velocity\n","try:\n","    min_face_v_df = HdfResultsMesh.get_mesh_min_face_v(plan_hdf_path, ras_object=bald_eagle)\n","    print(\"\\nMesh Min Face Velocity:\")\n","    display(min_face_v_df.head())\n","except AttributeError as e:\n","    print(f\"Error: {e}. Please ensure that the method exists in the HdfResultsMesh class.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh max water surface error\n","try:\n","    max_ws_err_df = HdfResultsMesh.get_mesh_max_ws_err(plan_hdf_path, ras_object=bald_eagle)\n","    print(\"\\nMesh Max Water Surface Error:\")\n","    display(max_ws_err_df.head())\n","except AttributeError as e:\n","    print(f\"Error: {e}. Please ensure that the method exists in the HdfResultsMesh class.\")\n","    logger.error(f\"Failed to get mesh max water surface error: {str(e)}\")\n","except Exception as e:\n","    print(f\"Error: {str(e)}\")\n","    logger.error(f\"Failed to get mesh max water surface error: {str(e)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot max water surface error\n","import matplotlib.pyplot as plt\n","\n","# Extract x and y coordinates from the geometry points, handling None values\n","max_ws_err_df['x'] = max_ws_err_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n","max_ws_err_df['y'] = max_ws_err_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n","\n","# Remove any rows with None coordinates\n","max_ws_err_df = max_ws_err_df.dropna(subset=['x', 'y'])\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","scatter = ax.scatter(max_ws_err_df['x'], max_ws_err_df['y'],\n","                    c=max_ws_err_df['cell_maximum_water_surface_error'],\n","                    cmap='viridis',\n","                    s=10)\n","\n","# Customize the plot\n","ax.set_title('Max Water Surface Error per Cell')\n","ax.set_xlabel('X Coordinate')\n","ax.set_ylabel('Y Coordinate')\n","plt.colorbar(scatter, label='Max Water Surface Error (ft)')\n","\n","# Add grid lines\n","ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Increase font size for better readability\n","plt.rcParams.update({'font.size': 12})\n","\n","# Adjust layout to prevent cutting off labels\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","# Print the first few rows of the dataframe for verification\n","print(\"\\nFirst few rows of the water surface error dataframe:\")\n","display(max_ws_err_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["### Need to add this to the ras-commander library"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n","try:\n","    max_courant_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\", ras_object=bald_eagle)\n","    print(\"\\nMesh Summary Output (Maximum Courant):\")\n","    print(max_courant_df.attrs)\n","    display(max_courant_df.head())\n","except Exception as e:\n","    print(f\"Error: {str(e)}\")\n","    logger.error(f\"Failed to get mesh summary output: {str(e)}\")\n","    # Additional error handling or logging can be added here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot max Courant number\n","import matplotlib.pyplot as plt\n","from ras_commander.HdfMesh import HdfMesh\n","from ras_commander.HdfResultsMesh import HdfResultsMesh\n","from shapely.geometry import LineString\n","import geopandas as gpd\n","\n","# Get mesh max Courant number\n","max_courant_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\", ras_object=bald_eagle)\n","\n","# Convert to GeoDataFrame\n","gdf = gpd.GeoDataFrame(max_courant_df)\n","\n","# Get centroids of line geometries for plotting\n","gdf['centroid'] = gdf.geometry.centroid\n","gdf['x'] = gdf.centroid.x\n","gdf['y'] = gdf.centroid.y\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","scatter = ax.scatter(gdf['x'], gdf['y'],\n","                    c=gdf['maximum_face_courant'],\n","                    cmap='viridis',\n","                    s=10)\n","\n","# Customize the plot\n","ax.set_title('Max Courant Number per Face')\n","ax.set_xlabel('X Coordinate')\n","ax.set_ylabel('Y Coordinate')\n","plt.colorbar(scatter, label='Max Courant Number')\n","\n","# Add grid lines\n","ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Increase font size for better readability\n","plt.rcParams.update({'font.size': 12})\n","\n","# Adjust layout to prevent cutting off labels\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","# Print the first few rows of the dataframe for verification\n","print(\"\\nFirst few rows of the Courant number dataframe:\")\n","display(gdf.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n","\n","max_face_shear_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\", ras_object=bald_eagle)\n","print(\"\\nMesh Summary Output (Maximum Face Shear Stress:\")\n","print(max_face_shear_df.attrs)\n","display(max_face_shear_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot max face shear stress\n","import matplotlib.pyplot as plt\n","from ras_commander.HdfMesh import HdfMesh\n","from ras_commander.HdfResultsMesh import HdfResultsMesh\n","from shapely.geometry import Point, LineString\n","import geopandas as gpd\n","\n","# Get mesh max face shear stress\n","max_shear_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\", ras_object=bald_eagle)\n","\n","# Calculate centroids of the line geometries and extract coordinates\n","max_shear_df['centroid'] = max_shear_df['geometry'].apply(lambda line: line.centroid)\n","max_shear_df['x'] = max_shear_df['centroid'].apply(lambda point: point.x)\n","max_shear_df['y'] = max_shear_df['centroid'].apply(lambda point: point.y)\n","\n","# Create the plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","scatter = ax.scatter(max_shear_df['x'], max_shear_df['y'],\n","                    c=max_shear_df['maximum_face_shear_stress'],\n","                    cmap='viridis',\n","                    s=10)\n","\n","# Customize the plot\n","ax.set_title('Max Face Shear Stress per Face')\n","ax.set_xlabel('X Coordinate')\n","ax.set_ylabel('Y Coordinate')\n","plt.colorbar(scatter, label='Max Face Shear Stress (PSF)')\n","\n","# Add grid lines\n","ax.grid(True, linestyle='--', alpha=0.7)\n","\n","# Increase font size for better readability\n","plt.rcParams.update({'font.size': 12})\n","\n","# Adjust layout to prevent cutting off labels\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","# Print the first few rows of the dataframe for verification\n","print(\"\\nFirst few rows of the shear stress dataframe:\")\n","display(max_shear_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get mesh summary output for Minimum Water Surface\n","summary_df_min_ws = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Water Surface\", ras_object=bald_eagle)\n","print(\"\\nMesh Summary Output (Minimum Water Surface):\")\n","display(summary_df_min_ws.head())\n","\n","# Example: Get mesh summary output for Minimum Face Velocity\n","summary_df_min_fv = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Face Velocity\", ras_object=bald_eagle)\n","print(\"\\nMesh Summary Output (Minimum Face Velocity):\")\n","display(summary_df_min_fv.head())\n","\n","# Example: Get mesh summary output for Cell Cumulative Iteration\n","summary_df_cum_iter = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Cell Cumulative Iteration\", ras_object=bald_eagle)\n","print(\"\\nMesh Summary Output (Cell Cumulative Iteration):\")\n","display(summary_df_cum_iter.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get mesh timeseries output\n","\n","# Get mesh areas from previous code cell\n","mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path, ras_object=bald_eagle)\n","\n","if mesh_areas:\n","    mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n","    timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\", ras_object=bald_eagle)\n","    print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n","    print(timeseries_da)\n","else:\n","    print(\"No mesh areas found in the geometry file.\")"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["# Time Series Output Variables for Cells\n","# \n","# Variable Name: Description\n","# Water Surface: Water surface elevation\n","# Depth: Water depth\n","# Velocity: Magnitude of velocity\n","# Velocity X: X-component of velocity\n","# Velocity Y: Y-component of velocity\n","# Froude Number: Froude number\n","# Courant Number: Courant number\n","# Shear Stress: Shear stress on the bed\n","# Bed Elevation: Elevation of the bed\n","# Precipitation Rate: Rate of precipitation\n","# Infiltration Rate: Rate of infiltration\n","# Evaporation Rate: Rate of evaporation\n","# Percolation Rate: Rate of percolation\n","# Groundwater Elevation: Elevation of groundwater\n","# Groundwater Depth: Depth to groundwater\n","# Groundwater Flow: Groundwater flow rate\n","# Groundwater Velocity: Magnitude of groundwater velocity\n","# Groundwater Velocity X: X-component of groundwater velocity\n","# Groundwater Velocity Y: Y-component of groundwater velocity\n","# \n","# These variables are available for time series output at the cell level in 2D flow areas.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get mesh cells timeseries output\n","cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n","print(\"\\nMesh Cells Timeseries Output:\")\n","print(cells_timeseries_ds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot Cell Time Series Data (Random Cell ID)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","\n","# Extract Water Surface data\n","water_surface = cells_timeseries_ds['BaldEagleCr']['Water Surface']\n","\n","# Get the time values\n","time_values = water_surface.coords['time'].values\n","\n","# Pick a random cell_id\n","random_cell_id = random.choice(water_surface.coords['cell_id'].values)\n","\n","# Extract the water surface elevation time series for the random cell\n","wsel_timeseries = water_surface.sel(cell_id=random_cell_id)\n","\n","# Find the peak value and its index\n","peak_value = wsel_timeseries.max().item()\n","peak_index = wsel_timeseries.argmax().item()\n","\n","# Create the plot\n","plt.figure(figsize=(12, 6))\n","plt.plot(time_values, wsel_timeseries, label=f'Cell ID: {random_cell_id}')\n","plt.scatter(time_values[peak_index], peak_value, color='red', s=100, zorder=5)\n","plt.annotate(f'Peak: {peak_value:.2f} ft', \n","             (time_values[peak_index], peak_value),\n","             xytext=(10, 10), textcoords='offset points',\n","             ha='left', va='bottom',\n","             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n","             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n","\n","plt.title(f'Water Surface Elevation Time Series for Random Cell (ID: {random_cell_id})')\n","plt.xlabel('Time')\n","plt.ylabel('Water Surface Elevation (ft)')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","\n","# Log the plotting action\n","logging.info(f\"Plotted water surface elevation time series for random cell ID: {random_cell_id}\")\n","\n","# Display the plot\n","plt.show()\n","\n","# Print some statistics\n","print(f\"Statistics for Cell ID {random_cell_id}:\")\n","print(f\"Minimum WSEL: {wsel_timeseries.min().item():.2f} ft\")\n","print(f\"Maximum WSEL: {peak_value:.2f} ft\")\n","print(f\"Mean WSEL: {wsel_timeseries.mean().item():.2f} ft\")\n","print(f\"Time of peak: {time_values[peak_index]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get mesh faces timeseries output\n","faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n","print(\"\\nMesh Faces Timeseries Output:\")\n","print(faces_timeseries_ds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot Random Face Results and Label Peak, Plus Map View\n","\n","# Step 1: Import necessary libraries \n","# In notebook cell at top of notebook\n","\n","# Step 2: Select a random valid face ID number\n","random_face = np.random.randint(0, faces_timeseries_ds.sizes['face_id'])\n","\n","# Step 3: Extract time series data for the selected face\n","variable = 'face_velocity'  # We could also use 'face_flow'\n","face_data = faces_timeseries_ds[variable].sel(face_id=random_face)\n","\n","# Step 4: Find peak value and its corresponding time\n","peak_value = face_data.max().item()\n","peak_time = face_data.idxmax().values\n","\n","# Plot time series\n","plt.figure(figsize=(12, 8))\n","plt.plot(faces_timeseries_ds.time, face_data)\n","plt.title(f'{variable.capitalize()} Time Series for Face {random_face}')\n","plt.xlabel('Time')\n","plt.ylabel(f'{variable.capitalize()} ({faces_timeseries_ds.attrs[\"units\"]})')\n","plt.grid(True)\n","\n","# Annotate the peak point\n","plt.annotate(f'Peak: ({peak_time}, {peak_value:.2f})', \n","            (peak_time, peak_value),\n","            xytext=(10, 10), textcoords='offset points',\n","            arrowprops=dict(arrowstyle=\"->\"))\n","\n","# Check for negative values and label the minimum if present\n","min_value = face_data.min().item()\n","if min_value < 0:\n","    min_time = face_data.idxmin().values\n","    plt.annotate(f'Min: ({min_time}, {min_value:.2f})', \n","                (min_time, min_value),\n","                xytext=(10, -10), textcoords='offset points',\n","                arrowprops=dict(arrowstyle=\"->\"))\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Create map view plot\n","fig, ax = plt.subplots(figsize=(12, 8))\n","\n","# Get mesh faces for map view\n","mesh_faces = HdfMesh.get_mesh_cell_faces(plan_hdf_path, ras_object=bald_eagle)\n","\n","# Calculate mesh faces extents with 10% buffer\n","faces_bounds = mesh_faces.total_bounds\n","x_min, y_min, x_max, y_max = faces_bounds\n","buffer_x = (x_max - x_min) * 0.1\n","buffer_y = (y_max - y_min) * 0.1\n","plot_xlim = [x_min - buffer_x, x_max + buffer_x]\n","plot_ylim = [y_min - buffer_y, y_max + buffer_y]\n","\n","# Set plot limits before adding terrain\n","ax.set_xlim(plot_xlim)\n","ax.set_ylim(plot_ylim)\n","\n","# Add the terrain TIFF to the map, clipped to our desired extent\n","tiff_path = Path.cwd() / 'example_projects' / 'BaldEagleCrkMulti2D' / 'Terrain' / 'Terrain50.baldeagledem.tif'\n","with rasterio.open(tiff_path) as src:\n","    show(src, ax=ax, cmap='terrain', alpha=0.5)\n","    \n","# Reset the limits after terrain plot\n","ax.set_xlim(plot_xlim)\n","ax.set_ylim(plot_ylim)\n","\n","# Plot all faces in gray\n","mesh_faces.plot(ax=ax, color='lightgray', alpha=0.5, zorder=2)\n","\n","# Get the selected face geometry\n","selected_face = mesh_faces[mesh_faces['face_id'] == random_face]\n","\n","# Highlight the selected face in red\n","selected_face.plot(\n","    ax=ax, \n","    color='red',\n","    linewidth=2,\n","    label=f'Selected Face (ID: {random_face})',\n","    zorder=3\n",")\n","\n","# Get bounds of selected face for zoomed inset\n","bounds = selected_face.geometry.bounds.iloc[0]\n","x_center = (bounds.iloc[0] + bounds.iloc[2]) / 2\n","y_center = (bounds.iloc[1] + bounds.iloc[3]) / 2\n","buffer = max(bounds.iloc[2] - bounds.iloc[0], bounds.iloc[3] - bounds.iloc[1]) * 2\n","\n","# Create zoomed inset with a larger size, inside the map frame\n","axins = inset_axes(ax, width=\"70%\", height=\"70%\", loc='lower right',\n","                  bbox_to_anchor=(0.65, 0.05, 0.35, 0.35),\n","                  bbox_transform=ax.transAxes)\n","\n","# Plot terrain and faces in inset\n","with rasterio.open(tiff_path) as src:\n","    show(src, ax=axins, cmap='terrain', alpha=0.5)\n","    \n","# Plot zoomed view in inset\n","mesh_faces.plot(ax=axins, color='lightgray', alpha=0.5, zorder=2)\n","selected_face.plot(ax=axins, color='red', linewidth=2, zorder=3)\n","\n","# Set inset limits with slightly more context\n","axins.set_xlim(x_center - buffer/1.5, x_center + buffer/1.5)\n","axins.set_ylim(y_center - buffer/1.5, y_center + buffer/1.5)\n","\n","# Remove inset ticks for cleaner look\n","axins.set_xticks([])\n","axins.set_yticks([])\n","\n","# Add a border to the inset\n","for spine in axins.spines.values():\n","    spine.set_edgecolor('black')\n","    spine.set_linewidth(1.5)\n","\n","# Create connection lines between main plot and inset\n","# Get the selected face centroid for connection point\n","centroid = selected_face.geometry.centroid.iloc[0]\n","con1 = ConnectionPatch(\n","    xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n","    xyB=(0.02, 0.98), coordsB=axins.transAxes,\n","    arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",")\n","con2 = ConnectionPatch(\n","    xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n","    xyB=(0.98, 0.02), coordsB=axins.transAxes,\n","    arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",")\n","\n","ax.add_artist(con1)\n","ax.add_artist(con2)\n","\n","# Add title and legend to main plot\n","ax.set_title('Mesh Face Map View with Terrain')\n","ax.legend()\n","\n","# Ensure equal aspect ratio while maintaining our desired extents\n","ax.set_aspect('equal', adjustable='box')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary information\n","print(f\"Random Face: {random_face}\")\n","print(f\"Peak Value: {peak_value:.2f} {faces_timeseries_ds.attrs['units']} at {peak_time}\")\n","if min_value < 0:\n","    print(f\"Minimum Value: {min_value:.2f} {faces_timeseries_ds.attrs['units']} at {min_time}\")\n","\n","# Log the plotting action\n","logging.info(f\"Plotted mesh face time series and map view for random face ID: {random_face} with terrain\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get meteorology precipitation attributes\n","meteo_precip_attrs = HdfPlan.get_plan_met_precip(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nMeteorology Precipitation Attributes:\")\n","for key, value in meteo_precip_attrs.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get results unsteady attributes\n","results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nResults Unsteady Attributes:\")\n","for key, value in results_unsteady_attrs.items():\n","    print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get results unsteady summary attributes\n","results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nResults Unsteady Summary Attributes:\")\n","for key, value in results_unsteady_summary_attrs.items():\n","    print(f\"{key}: {value}\")\n","\n","# Get results volume accounting attributes\n","volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path, ras_object=bald_eagle)\n","print(\"\\nVolume Accounting Attributes:\")\n","for key, value in volume_accounting_attrs.items():\n","    print(f\"{key}: {value}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":2}

==================================================

File: c:\GH\ras-commander\examples\21_2d_hdf_data_extraction pipes and pumps.ipynb
==================================================
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# HEC-RAS Pipes, Conduits, and Pump Stations HDF Data Analysis Notebook\n","\n","This notebook demonstrates how to manipulate and analyze the new HEC-RAS Conduits, Pipes, and Pump Stations results using the ras-commander library. It leverages the HdfPipe and HdfPump classes to streamline data extraction, processing, and visualization."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Import required Libraries\n","import subprocess\n","import sys\n","import os\n","from pathlib import Path\n","\n","def install_module(module_name):\n","    try:\n","        __import__(module_name)\n","    except ImportError:\n","        print(f\"{module_name} not found. Installing...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n","\n","# List of modules to check and install if necessary\n","modules = ['h5py', 'numpy', 'requests', 'geopandas', 'matplotlib', 'pandas', 'pyproj', 'shapely', 'xarray']\n","for module in modules:\n","    install_module(module)\n","\n","# Import the rest of the required libraries\n","import pandas as pd\n","import numpy as np\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import pyproj\n","from shapely.geometry import Point, LineString, Polygon\n","import xarray as xr\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Install ras-commander if you are not in a dev environment. \n","# install_module(ras-commander)"]},{"cell_type":"markdown","metadata":{},"source":["## Importing ras-commander flexibly (from package or local dev copy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","from pathlib import Path\n","\n","# Flexible imports to allow for development without installation \n","#  ** Use this version with Jupyter Notebooks **\n","try:\n","    # Try to import from the installed package\n","    from ras_commander import (init_ras_project, HdfBase, HdfUtils, HdfFluvialPluvial, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","except ImportError:\n","    # If the import fails, add the parent directory to the Python path\n","    print(\"Using Local Dev Copy\")\n","    import os\n","    current_file = Path(os.getcwd()).resolve()\n","    parent_directory = current_file.parent\n","    sys.path.append(str(parent_directory))\n","    \n","    # Now try to import again\n","    from ras_commander import (init_ras_project, HdfBase, HdfUtils, HdfFluvialPluvial, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","\n","print(\"ras_commander imported successfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download the Pipes Beta project from HEC and run plan 01\n","\n","# Define the path to the Pipes Beta project\n","current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n","pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n","import logging\n","\n","# Check if Pipes Beta.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n","hdf_file = pipes_ex_path / \"DavisStormSystem.p02.hdf\"\n","\n","if not hdf_file.exists():\n","    # Initialize RasExamples and extract the Pipes Beta project\n","    ras_examples = RasExamples()\n","    ras_examples.extract_project([\"Davis\"])\n","\n","    # Initialize custom Ras object\n","    pipes_ex = RasPrj()\n","\n","    # Initialize the RAS project using the custom ras object\n","    pipes_ex = init_ras_project(pipes_ex_path, \"6.6\", ras_instance=pipes_ex)\n","    logging.info(f\"Pipes Beta project initialized with folder: {pipes_ex.project_folder}\")\n","    \n","    logging.info(f\"Pipes Beta object id: {id(pipes_ex)}\")\n","    \n","    # Define the plan number to execute\n","    plan_number = \"02\"\n","\n","    # Update run flags for the project\n","    RasPlan.update_run_flags(\n","        plan_number,\n","        geometry_preprocessor=True,\n","        unsteady_flow_simulation=True,\n","        run_sediment=False,\n","        post_processor=True,\n","        floodplain_mapping=False,\n","        ras_object=pipes_ex\n","    )\n","\n","    # Execute Plan 06 using RasCmdr for Pipes Beta\n","    print(f\"Executing Plan {plan_number} for the Pipes Beta Creek project...\")\n","    success_pipes_ex = RasCmdr.compute_plan(plan_number, ras_object=pipes_ex)\n","    if success_pipes_ex:\n","        print(f\"Plan {plan_number} executed successfully for Pipes Beta.\\n\")\n","    else:\n","        print(f\"Plan {plan_number} execution failed for Pipes Beta.\\n\")\n","else:\n","    print(\"Pipes Beta.p06.hdf already exists. Skipping project extraction and plan execution.\")\n","    # Initialize the RAS project using the custom ras object\n","    pipes_ex = RasPrj()\n","    pipes_ex = init_ras_project(pipes_ex_path, \"6.6\", ras_instance=pipes_ex)\n","    plan_number = \"02\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n","\n","# Display plan_df for pipes_ex project\n","print(\"Plan DataFrame for pipes_ex project:\")\n","display(pipes_ex.plan_df)\n","\n","# Display geom_df for pipes_ex project\n","print(\"\\nGeometry DataFrame for pipes_ex project:\")\n","display(pipes_ex.geom_df)\n","\n","# Get the plan HDF path\n","plan_hdf_path = pipes_ex.plan_df.loc[pipes_ex.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n","\n","# Get the geometry file number from the plan DataFrame\n","geom_file = pipes_ex.plan_df.loc[pipes_ex.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n","geom_number = geom_file[1:]  # Remove the 'g' prefix\n","\n","# Get the geometry HDF path\n","geom_hdf_path = pipes_ex.geom_df.loc[pipes_ex.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n","\n","print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n","print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract runtime and compute time data\n","print(\"\\nExample 2: Extracting runtime and compute time data\")\n","runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=pipes_ex)\n","if runtime_df is not None:\n","    display(runtime_df)\n","else:\n","    print(\"No runtime data found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n","HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pipe Conduits/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get pipe conduits\n","pipe_conduits_gdf = HdfPipe.get_pipe_conduits(plan_hdf_path)\n","print(\"\\nPipe Conduits: pipe_conduits_gdf\")\n","display(pipe_conduits_gdf.head())"]},{"cell_type":"markdown","metadata":{},"source":["pipe_conduits_gdf:  \n","\n","| Name | System Name | US Node | DS Node | Modeling Approach | Conduit Length | Max Cell Length | Shape | Rise | Span | ... | Slope | US Entrance Loss Coefficient | DS Exit Loss Coefficient | US Backflow Loss Coefficient | DS Backflow Loss Coefficient | DS Flap Gate | Major Group | Minor Group | Polyline | Terrain_Profiles |\n","|------|-------------|---------|---------|-------------------|----------------|------------------|-------|------|------|-----|-------|------------------------------|--------------------------|------------------------------|------------------------------|--------------|-------------|-------------|----------|-------------------|\n","| 0    | 134         | Davis   | O13-DMH007 | O13-DMH006 | hydraulic        | 443.740020      | 40.0             | circular | 6.0  | 6.0  | ... | 0.002723 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 40.819695), (21.217846, 40.642994), (35... |\n","| 1    | 133         | Davis   | O13-DMH024 | O13-DMH009 | hydraulic        | 800.000024      | 40.0             | circular | 6.0  | 6.0  | ... | 0.001904 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 40.530186), (21.1467, 40.44057), (50.88... |\n","| 2    | 132         | Davis   | O13-DMH006 | O13-SDS03 | hydraulic        | 443.740070      | 40.0             | circular | 6.0  | 6.0  | ... | 0.002816 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 41.700996), (26.817467, 41.552666), (83... |"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#!pip install adjusttext #No longer required - optional to help with labels overlapping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the pipe conduit linestrings\n","import matplotlib.pyplot as plt\n","\n","# Create a new figure with a specified size\n","plt.figure(figsize=(12, 9))\n","\n","# Plot each linestring from the GeoDataFrame\n","for idx, row in pipe_conduits_gdf.iterrows():\n","    # Extract coordinates from the linestring\n","    x_coords, y_coords = row['Polyline'].xy\n","    \n","    # Plot the linestring\n","    plt.plot(x_coords, y_coords, 'b-', linewidth=1, alpha=0.7)\n","    \n","    # Add vertical line markers at endpoints\n","    plt.plot([x_coords[0]], [y_coords[0]], 'x', color='black', markersize=4)\n","    plt.plot([x_coords[-1]], [y_coords[-1]], 'x', color='black', markersize=4)\n","    \n","    # Calculate center point of the line\n","    center_x = (x_coords[0] + x_coords[-1]) / 2\n","    center_y = (y_coords[0] + y_coords[-1]) / 2\n","    \n","    # Add pipe name label at center, oriented top-right\n","    plt.text(center_x, center_y, f'{row[\"Name\"]}', fontsize=8, \n","             verticalalignment='bottom', horizontalalignment='left',\n","             rotation=45)  # 45 degree angle for top-right orientation\n","\n","# Add title and labels\n","plt.title('Pipe Conduit Network Layout')\n","plt.xlabel('Easting')\n","plt.ylabel('Northing')\n","\n","# Add grid\n","plt.grid(True, linestyle='--', alpha=0.6)\n","\n","# Adjust layout to prevent label clipping\n","plt.tight_layout()\n","\n","# Display the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the first 2 terrain profiles\n","import matplotlib.pyplot as plt\n","\n","# Extract terrain profiles from the GeoDataFrame\n","terrain_profiles = pipe_conduits_gdf['Terrain_Profiles'].tolist()\n","\n","# Create separate plots for the first 2 terrain profiles\n","for i in range(2):\n","    profile = terrain_profiles[i]\n","    \n","    # Unzip the profile into x and y coordinates\n","    x_coords, y_coords = zip(*profile)\n","    \n","    # Create a new figure for each profile\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='g', alpha=0.7)\n","    \n","    # Add title and labels\n","    plt.title(f'Terrain Profile {i + 1}')\n","    plt.xlabel('Distance along profile (m)')\n","    plt.ylabel('Elevation (m)')\n","    \n","    # Add grid\n","    plt.grid(True, linestyle='--', alpha=0.6)\n","    \n","    # Adjust layout to prevent label clipping\n","    plt.tight_layout()\n","    \n","    # Display the plot\n","    plt.show()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n","#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Nodes/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get pipe nodes\n","pipe_nodes_gdf = HdfPipe.get_pipe_nodes(plan_hdf_path)\n","print(\"\\nPipe Nodes:\")\n","display(pipe_nodes_gdf.head())"]},{"cell_type":"markdown","metadata":{},"source":["pipe_nodes_gdf:\n","\n","\n","| Name         | System Name | Node Type | Node Status                     | Condtui Connections (US:DS) | Invert Elevation | Base Area | Terrain Elevation | Terrain Elevation Override | Depth     | Drop Inlet Elevation | Drop Inlet Weir Length | Drop Inlet Weir Coefficient | Drop Inlet Orifice Area | Drop Inlet Orifice Coefficient | Total Connection Count | geometry                             |\n","|--------------|-------------|-----------|----------------------------------|------------------------------|------------------|-----------|-------------------|---------------------------|-----------|----------------------|-------------------------|-----------------------------|-------------------------|-------------------------------|------------------------|-------------------------------------|\n","| O14-di027   | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 36.060001        | 36.0     | 39.860001         | NaN                       | 3.799999  | 39.863369           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6637926.81 1964917.32)     |\n","| P11-DMH004  | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 38.169998        | 36.0     | 48.720001         | NaN                       | 10.550003 | 48.718811           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6629444.634 1963504.411)   |\n","| O14-DMH005  | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 31.559999        | 36.0     | 40.840000         | NaN                       | 9.280001  | 40.843731           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6637368.497 1966084.574)   |"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n","#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Networks/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get pipe network data\n","pipe_network_gdf = HdfPipe.get_pipe_network(plan_hdf_path)\n","print(\"\\nPipe Network Data:\")\n","display(pipe_network_gdf.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["pipe_network_gdf:\n","| Cell_ID | Conduit_ID | Node_ID | Minimum_Elevation | DS_Face_Indices | Face_Indices | US_Face_Indices | Cell_Property_Info_Index | US Face Elevation | DS Face Elevation | Min Elevation | Area | Info Index | Cell_Polygon | Face_Polylines | Node_Point |\n","|---------|------------|---------|-------------------|------------------|--------------|------------------|--------------------------|-------------------|-------------------|---------------|------|------------|---------------|----------------|------------|\n","| 0       | 0          | 0       | -1                | 26.824432        | [1]          | [0, 1]           | [0]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |\n","| 1       | 1          | 0       | -1                | 26.714573        | [2]          | [1, 2]           | [1]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |\n","| 2       | 2          | 0       | -1                | 26.604715        | [3]          | [2, 3]           | [2]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get pump stations\n","pump_stations_gdf = HdfPump.get_pump_stations(plan_hdf_path)\n","print(\"\\nPump Stations:\")\n","display(pump_stations_gdf.head())"]},{"cell_type":"markdown","metadata":{},"source":["Pump Stations:\n","| geometry                          | station_id | Name             | Inlet River | Inlet Reach | Inlet RS | Inlet RS Distance | Inlet SA/2D | Inlet Pipe Node | Outlet River | ... | Outlet Pipe Node | Reference River | Reference Reach | Reference RS | Reference RS Distance | Reference SA/2D | Reference Point | Reference Pipe Node | Highest Pump Line Elevation | Pump Groups |\n","|-----------------------------------|------------|------------------|-------------|-------------|----------|-------------------|--------------|------------------|--------------|-----|------------------|------------------|-----------------|--------------|-----------------------|------------------|-----------------|----------------------|------------------------------|-------------|\n","| POINT (6635027.027 1966080.07)   | 0          | Pump Station #1   |             |             | NaN      |                   |              | Davis [O13-SDS03] |              | ... |                  | NaN              |                 | NaN          |                       | NaN              |                 |                      | NaN                          | 1           |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get pump groups\n","pump_groups_df = HdfPump.get_pump_groups(plan_hdf_path)\n","print(\"\\nPump Groups:\")\n","display(pump_groups_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["Pump Groups:\n","| Pump Station ID | Name             | Bias On | Start Up Time | Shut Down Time | Width | Pumps | efficiency_curve_start | efficiency_curve_count | efficiency_curve |\n","|------------------|------------------|---------|----------------|----------------|-------|-------|------------------------|-----------------------|------------------|\n","| 0                | Pump Station #1   | 0       | 5.0            | NaN            | 5.0   | 1     | 0                      | 6                     | [[2.0, 70.0], [4.0, 60.0], [6.0, 55.0], [8.0, ... |"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfUtils for extracting projection\n","print(\"\\nExtracting Projection from HDF\")\n","projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n","if projection:\n","    print(f\"Projection: {projection}\")\n","else:\n","    print(\"No projection information found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get projection from HDF file\n","projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n","\n","# Set CRS for GeoDataFrames\n","if projection:\n","    pipe_conduits_gdf.set_crs(projection, inplace=True, allow_override=True)\n","    pipe_nodes_gdf.set_crs(projection, inplace=True, allow_override=True)\n","\n","print(\"Pipe Conduits GeoDataFrame columns:\")\n","print(pipe_conduits_gdf.columns)\n","\n","print(\"\\nPipe Nodes GeoDataFrame columns:\")\n","print(pipe_nodes_gdf.columns)\n","\n","perimeter_polygons = HdfMesh.get_mesh_areas(geom_hdf_path, ras_object=pipes_ex)\n","if projection:\n","    perimeter_polygons.set_crs(projection, inplace=True, allow_override=True)\n","    \n","print(\"\\nPerimeter Polygons GeoDataFrame columns:\")\n","print(perimeter_polygons.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from shapely import wkt\n","import matplotlib.patches as mpatches\n","import matplotlib.lines as mlines\n","import numpy as np\n","\n","fig, ax = plt.subplots(figsize=(28, 20))\n","\n","# Plot cell polygons with 50% transparency behind the pipe network\n","cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path, ras_object=pipes_ex)\n","if not cell_polygons_df.empty:\n","    cell_polygons_df.plot(ax=ax, edgecolor='lightgray', facecolor='lightgray', alpha=0.5)\n","\n","# Plot pipe conduits - the Polyline column already contains LineString geometries\n","pipe_conduits_gdf.set_geometry('Polyline', inplace=True)\n","\n","# Plot each pipe conduit individually to ensure all are shown\n","for idx, row in pipe_conduits_gdf.iterrows():\n","    ax.plot(*row.Polyline.xy, color='blue', linewidth=1)\n","\n","# Create a colormap for node elevations\n","norm = plt.Normalize(pipe_nodes_gdf['Invert Elevation'].min(), \n","                    pipe_nodes_gdf['Invert Elevation'].max())\n","cmap = plt.cm.viridis\n","\n","# Plot pipe nodes colored by invert elevation\n","scatter = ax.scatter(pipe_nodes_gdf.geometry.x, pipe_nodes_gdf.geometry.y,\n","                    c=pipe_nodes_gdf['Invert Elevation'], \n","                    cmap=cmap, norm=norm,\n","                    s=100)\n","\n","# Add colorbar\n","cbar = plt.colorbar(scatter)\n","cbar.set_label('Invert Elevation (ft)', rotation=270, labelpad=15)\n","\n","# Add combined labels for invert and drop inlet elevations\n","for idx, row in pipe_nodes_gdf.iterrows():\n","    label_text = \"\"  # Initialize label_text for each node\n","    # Add drop inlet elevation label if it exists and is not NaN\n","    if 'Drop Inlet Elevation' in row and not np.isnan(row['Drop Inlet Elevation']):\n","        label_text += f\"TOC: {row['Drop Inlet Elevation']:.2f}\\n\"\n","    label_text += f\"INV: {row['Invert Elevation']:.2f}\"\n","    \n","    ax.annotate(label_text,\n","                xy=(row.geometry.x, row.geometry.y),\n","                xytext=(-10, -10), textcoords='offset points',\n","                fontsize=8,\n","                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n","\n","# Add perimeter polygons \n","if not perimeter_polygons.empty:\n","    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none')\n","\n","# Create proxy artists for legend\n","conduit_line = mlines.Line2D([], [], color='blue', label='Conduits')\n","node_point = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n","                          markersize=10, label='Nodes')\n","perimeter = mpatches.Patch(facecolor='none', edgecolor='black',\n","                          label='Perimeter Polygons')\n","\n","ax.set_title('Pipe Network with Node Elevations')\n","\n","# Add legend with proxy artists\n","ax.legend(handles=[conduit_line, node_point, perimeter])\n","\n","# Set aspect ratio to be equal and adjust limits\n","ax.set_aspect('equal', 'datalim')\n","ax.autoscale_view()\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize pump stations on a map\n","fig, ax = plt.subplots(figsize=(12, 8))\n","pump_stations_gdf.plot(ax=ax, color='green', markersize=50, label='Pump Stations')\n","ax.set_title('Pump Stations Location')\n","ax.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example 3: Get pipe network timeseries\n","valid_variables = [\n","    \"Cell Courant\", \"Cell Water Surface\", \"Face Flow\", \"Face Velocity\",\n","    \"Face Water Surface\", \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\",\n","    \"Pipes/Vel DS\", \"Pipes/Vel US\", \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\",\n","    \"Nodes/Water Surface\"\n","]\n","\n","print(\"Valid variables for pipe network timeseries:\")\n","for var in valid_variables:\n","    print(f\"- {var}\")\n","\n","# Extract pipe network timeseries for each valid pipe-related variable\n","pipe_variables = [var for var in valid_variables if var.startswith(\"Pipes/\") or var.startswith(\"Nodes/\")]\n","\n","for variable in pipe_variables:\n","    try:\n","        pipe_timeseries = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n","        print(f\"\\nPipe Network Timeseries ({variable}):\")\n","        print(pipe_timeseries.head())  # Print first few rows to avoid overwhelming output\n","    except Exception as e:\n","        print(f\"Error extracting {variable}: {str(e)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Pipe Network Timeseries Data Description\n","\n","The `get_pipe_network_timeseries` function returns an xarray DataArray for each variable. Here's a general description of the data structure:\n","\n","1. **Pipes/Pipe Flow DS and Pipes/Pipe Flow US**:\n","   - Dimensions: time, location (pipe IDs)\n","   - Units: ft^3/s (cubic feet per second)\n","   - Description: Represents the flow rate at the downstream (DS) and upstream (US) ends of pipes over time.\n","\n","2. **Pipes/Vel DS and Pipes/Vel US**:\n","   - Dimensions: time, location (pipe IDs)\n","   - Units: ft/s (feet per second)\n","   - Description: Shows the velocity at the downstream (DS) and upstream (US) ends of pipes over time.\n","\n","3. **Nodes/Depth**:\n","   - Dimensions: time, location (node IDs)\n","   - Units: ft (feet)\n","   - Description: Indicates the depth of water at each node over time.\n","\n","4. **Nodes/Drop Inlet Flow**:\n","   - Dimensions: time, location (node IDs)\n","   - Units: cfs (cubic feet per second)\n","   - Description: Represents the flow rate through drop inlets at each node over time.\n","\n","5. **Nodes/Water Surface**:\n","   - Dimensions: time, location (node IDs)\n","   - Units: ft (feet)\n","   - Description: Shows the water surface elevation at each node over time.\n","\n","General notes:\n","- The 'time' dimension represents the simulation timesteps.\n","- The 'location' dimension represents either pipe IDs or node IDs, depending on the variable.\n","- The number of timesteps and locations may vary depending on the specific dataset and simulation setup.\n","- Negative values in flow variables may indicate reverse flow direction.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.dates import DateFormatter\n","import numpy as np\n","import random\n","\n","# Define the variables we want to plot\n","variables = [\n","    \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\", \"Pipes/Vel DS\", \"Pipes/Vel US\",\n","    \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\", \"Nodes/Water Surface\"\n","]\n","\n","# Create a separate plot for each variable\n","for variable in variables:\n","    try:\n","        # Get the data for the current variable\n","        data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n","        \n","        # Create a new figure\n","        fig, ax = plt.subplots(figsize=(12, 6))\n","        \n","        # Pick one random location\n","        random_location = random.choice(data.location.values)\n","        \n","        # Determine if it's a pipe or node variable\n","        if variable.startswith(\"Pipes/\"):\n","            location_type = \"Conduit ID\"\n","        else:\n","            location_type = \"Node ID\"\n","        \n","        # Plot the data for the randomly selected location\n","        ax.plot(data.time, data.sel(location=random_location), label=f'{location_type} {random_location}')\n","        \n","        # Set the title and labels\n","        ax.set_title(f'{variable} Over Time ({location_type} {random_location})')\n","        ax.set_xlabel('Time')  # Corrected from ax.xlabel to ax.set_xlabel\n","        ax.set_ylabel(f'{variable} ({data.attrs[\"units\"]})')  # Corrected from ax.ylabel to ax.set_ylabel\n","        \n","        # Format the x-axis to show dates nicely\n","        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n","        plt.xticks(rotation=45)\n","        \n","        # Add a legend\n","        ax.legend(title=location_type, loc='upper left')\n","        \n","        # Adjust the layout\n","        plt.tight_layout()\n","        \n","        # Show the plot\n","        plt.show()\n","        \n","    except Exception as e:\n","        print(f\"Error plotting {variable}: {str(e)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example 8: Get pump station timeseries\n","pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n","# Use the results_pump_station_timeseries method \n","pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n","print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n","print(pump_timeseries)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n","HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pump Stations/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract the pump station timeseries data\n","pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n","pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n","\n","# Print the pump station timeseries\n","print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n","print(pump_timeseries)\n","\n","# Create a new figure for plotting\n","fig, ax = plt.subplots(figsize=(12, 12))\n","\n","# Plot each variable in the timeseries\n","for variable in pump_timeseries.coords['variable'].values:\n","    data = pump_timeseries.sel(variable=variable)\n","    \n","    # Decode units to strings\n","    unit = pump_timeseries.attrs[\"units\"][list(pump_timeseries.coords[\"variable\"].values).index(variable)][1].decode('utf-8')\n","    \n","    # Check if the variable is 'Pumps on' to plot it differently\n","    if variable == 'Pumps on':\n","        # Plot with color based on the on/off status\n","        colors = ['green' if val > 0 else 'red' for val in data.values.flatten()]\n","        ax.scatter(pump_timeseries['time'], data, label=f'{variable} ({unit})', color=colors)\n","    else:\n","        ax.plot(pump_timeseries['time'], data, label=f'{variable} ({unit})')\n","        \n","        # Label the peak values\n","        peak_time = pump_timeseries['time'][data.argmax()]\n","        peak_value = data.max()\n","        ax.annotate(f'Peak: {peak_value:.2f}', xy=(peak_time, peak_value), \n","                    xytext=(peak_time, peak_value + 0.1 * peak_value), \n","                    arrowprops=dict(facecolor='black', arrowstyle='->'),\n","                    fontsize=10, color='black', ha='center')\n","\n","# Set the title and labels\n","ax.set_title(f'Timeseries Data for Pump Station: {pump_station_name}')\n","ax.set_xlabel('Time')\n","ax.set_ylabel('Values')\n","\n","# Format the x-axis to show dates nicely\n","ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n","plt.xticks(rotation=45)\n","\n","# Add a legend\n","ax.legend(title='Variables', loc='upper left')\n","\n","# Adjust the layout\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":2}

==================================================

File: c:\GH\ras-commander\examples\22_2d_detail_face_data_extraction.ipynb
==================================================
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# HEC-RAS 2D HDF Data Analysis Notebook\n","\n","This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Import required Libraries\n","import subprocess\n","import sys\n","import os\n","from pathlib import Path\n","\n","def install_module(module_name):\n","    try:\n","        __import__(module_name)\n","    except ImportError:\n","        print(f\"{module_name} not found. Installing...\")\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n","\n","# List of modules to check and install if necessary\n","modules = ['h5py', 'numpy', 'requests', 'geopandas', 'matplotlib', 'pandas', 'pyproj', 'shapely', 'xarray', 'rasterio']\n","for module in modules:\n","    install_module(module)\n","\n","# Import the rest of the required libraries\n","import pandas as pd\n","import numpy as np\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import pyproj\n","from shapely.geometry import Point, LineString, Polygon\n","import xarray as xr\n","from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n","import matplotlib.patches as patches\n","from matplotlib.patches import ConnectionPatch\n","import logging\n","from pathlib import Path\n","import rasterio\n","from rasterio.plot import show\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Install ras-commander if you are not in a dev environment. \n","#install_module('ras-commander')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Importing ras-commander flexibly (from package or local dev copy)\n","import sys\n","from pathlib import Path\n","\n","# Flexible imports to allow for development without installation \n","#  ** Use this version with Jupyter Notebooks **\n","try:\n","    # Try to import from the installed package\n","    from ras_commander import (\n","        init_ras_project, HdfMesh, HdfBndry, HdfResultsMesh, RasExamples, RasPrj, RasPlan, RasCmdr, HdfUtils, HdfResultsPlan, HdfPlan, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","except ImportError:\n","    # If the import fails, add the parent directory to the Python path\n","    print(\"Using Local ras-commander library\")\n","    import os\n","    current_file = Path(os.getcwd()).resolve()\n","    parent_directory = current_file.parent\n","    sys.path.append(str(parent_directory))\n","    \n","    # Now try to import again\n","    from ras_commander import (\n","        init_ras_project, HdfMesh, HdfBndry, HdfResultsMesh, RasExamples, RasPrj, RasPlan, RasCmdr, HdfUtils, HdfResultsPlan, HdfPlan, ras)\n","    from ras_commander.Decorators import standardize_input, log_call\n","    from ras_commander.LoggingConfig import setup_logging, get_logger\n","\n","print(\"ras_commander imported successfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download the Chippewa_2D project from HEC and run plan 01\n","\n","# Define the path to the Chippewa_2D project\n","current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n","bald_eagle_path = current_dir / \"example_projects\" / \"Chippewa_2D\"\n","import logging\n","\n","# Check if Chippewa_2D.p02.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n","hdf_file = bald_eagle_path / \"Chippewa_2D.p02.hdf\"\n","\n","if not hdf_file.exists():\n","    # Initialize RasExamples and extract the Chippewa_2D project\n","    ras_examples = RasExamples()\n","    ras_examples.extract_project([\"Chippewa_2D\"])\n","\n","    # Initialize custom Ras object\n","    bald_eagle = RasPrj()\n","\n","    # Initialize the RAS project using the custom ras object\n","    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n","    logging.info(f\"Bald Eagle project initialized with folder: {bald_eagle.project_folder}\")\n","    \n","    logging.info(f\"Bald Eagle object id: {id(bald_eagle)}\")\n","    \n","    # Define the plan number to execute\n","    plan_number = \"02\"\n","\n","    # Update run flags for the project\n","    RasPlan.update_run_flags(\n","        plan_number,\n","        geometry_preprocessor=True,\n","        unsteady_flow_simulation=True,\n","        run_sediment=False,\n","        post_processor=True,\n","        floodplain_mapping=False,\n","        ras_object=bald_eagle\n","    )\n","\n","    # Execute Plan 02 using RasCmdr for Bald Eagle\n","    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n","    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n","    if success_bald_eagle:\n","        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n","    else:\n","        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n","else:\n","    print(\"Chippewa_2D.p02.hdf already exists. Skipping project extraction and plan execution.\")\n","    # Initialize the RAS project using the custom ras object\n","    bald_eagle = RasPrj()\n","    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n","    plan_number = \"02\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n","\n","# Display plan_df for bald_eagle project\n","print(\"Plan DataFrame for bald_eagle project:\")\n","display(bald_eagle.plan_df)\n","\n","# Display geom_df for bald_eagle project\n","print(\"\\nGeometry DataFrame for bald_eagle project:\")\n","display(bald_eagle.geom_df)\n","\n","# Get the plan HDF path\n","plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n","plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n","\n","# Get the geometry file number from the plan DataFrame\n","geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n","geom_number = geom_file[1:]  # Remove the 'g' prefix\n","\n","# Get the geometry HDF path\n","geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n","\n","print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n","print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Define the HDF input path as Plan Number\n","\n","plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract runtime and compute time data\n","print(\"\\nExample 2: Extracting runtime and compute time data\")\n","runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=bald_eagle)\n","if runtime_df is not None:\n","    display(runtime_df)\n","else:\n","    print(\"No runtime data found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n","print(geom_hdf_path)\n","\n","# For the example project, plan 02 is associated with geometry 09\n","# If you want to call the geometry by number, call RasHdfGeom functions with a number\n","# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Set the  to USA Contiguous Albers Equal Area Conic (USGS version)\n","# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n","projection = 'EPSG:5070'  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfPlan for geometry-related operations\n","print(\"\\nExample: Extracting Base Geometry Attributes\")\n","geom_attrs = HdfPlan.get_geom_attrs(geom_hdf_path, ras_object=bald_eagle)\n","\n","if geom_attrs:\n","    # Convert the dictionary to a DataFrame for better display\n","    geom_attrs_df = pd.DataFrame([geom_attrs])\n","    \n","    # Display the DataFrame\n","    print(\"Base Geometry Attributes:\")\n","    display(geom_attrs_df)\n","else:\n","    print(\"No base geometry attributes found.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use HdfMesh for geometry-related operations\n","print(\"\\nExample 3: Listing 2D Flow Area Names\")\n","flow_area_names = HdfMesh.mesh_area_names(geom_hdf_path, ras_object=bald_eagle)\n","print(\"2D Flow Area Names:\", flow_area_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get 2D Flow Area Attributes (get_geom_2d_flow_area_attrs)\n","print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n","flow_area_attributes = HdfMesh.get_geom_2d_flow_area_attrs(geom_hdf_path, ras_object=bald_eagle)\n","\n","if flow_area_attributes:\n","    # Convert the dictionary to a DataFrame for better display\n","    flow_area_df = pd.DataFrame([flow_area_attributes])\n","    \n","    # Display the DataFrame\n","    print(\"2D Flow Area Attributes:\")\n","    display(flow_area_df)\n","    \n","    # Optionally, you can access specific attributes\n","    print(\"\\nSpecific Attribute Examples:\")\n","    print(f\"Cell Average Size: {flow_area_attributes.get('Cell Average Size', 'N/A')}\")\n","    print(f\"Manning's n: {flow_area_attributes.get('Manning''s n', 'N/A')}\")\n","    print(f\"Terrain Filename: {flow_area_attributes.get('Terrain Filename', 'N/A')}\")\n","else:\n","    print(\"No 2D Flow Area attributes found.\")\n","\n","# Note: This example assumes that get_geom_2d_flow_area_attrs returns a dictionary.\n","# If it returns a different format, you may need to adjust the code accordingly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n","print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n","mesh_areas = HdfMesh.mesh_areas(geom_hdf_path, ras_object=bald_eagle)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract mesh cell faces\n","print(\"\\nExample: Extracting mesh cell faces\")\n","\n","# Get mesh cell faces\n","mesh_cell_faces = HdfMesh.mesh_cell_faces(geom_hdf_path, ras_object=bald_eagle)\n","\n","# Display the first few rows of the mesh cell faces DataFrame\n","print(\"First few rows of mesh cell faces:\")\n","display(mesh_cell_faces.head())"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Set the projection to USA Contiguous Albers Equal Area Conic (USGS version)\n","# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n","projection = 'EPSG:5070'  # NAD83 / Conus Albers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example Function: Find the nearest cell face to a given point\n","# This provides enough basic information the face cell logic in the notebook\n","\n","def find_nearest_cell_face(point, cell_faces_df):\n","    \"\"\"\n","    Find the nearest cell face to a given point.\n","\n","    Args:\n","        point (shapely.geometry.Point): The input point.\n","        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n","\n","    Returns:\n","        int: The face_id of the nearest cell face.\n","        float: The distance to the nearest cell face.\n","    \"\"\"\n","    # Calculate distances from the input point to all cell faces\n","    distances = cell_faces_df.geometry.distance(point)\n","\n","    # Find the index of the minimum distance\n","    nearest_index = distances.idxmin()\n","\n","    # Get the face_id and distance of the nearest cell face\n","    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n","    nearest_distance = distances[nearest_index]\n","\n","    return nearest_face_id, nearest_distance\n","\n","# Example usage\n","print(\"\\nExample: Finding the nearest cell face to a given point\")\n","\n","# Create a sample point (you can replace this with any point of interest)\n","from shapely.geometry import Point\n","from geopandas import GeoDataFrame\n","\n","# Create the sample point with the same CRS as mesh_cell_faces\n","sample_point = GeoDataFrame(\n","    {'geometry': [Point(1025677, 7853731)]}, \n","    crs=mesh_cell_faces.crs\n",")\n","\n","if not mesh_cell_faces.empty and not sample_point.empty:\n","    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n","    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n","    print(f\"Face ID: {nearest_face_id}\")\n","    print(f\"Distance: {distance:.2f} units\")\n","\n","    # Visualize the result\n","    fig, ax = plt.subplots(figsize=(12, 8))\n","    \n","    # Plot all cell faces\n","    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n","    \n","    # Plot the sample point\n","    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n","    \n","    # Plot the nearest cell face\n","    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n","    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n","    \n","    # Set labels and title\n","    ax.set_xlabel('X Coordinate')\n","    ax.set_ylabel('Y Coordinate')\n","    ax.set_title('Nearest Cell Face to Sample Point')\n","    \n","    # Add legend and grid\n","    ax.legend()\n","    ax.grid(True)\n","    \n","    # Adjust layout and display\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Unable to perform nearest cell face search due to missing data.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extract mesh cell faces and plot with profile lines\n","print(\"\\nExample: Extracting mesh cell faces and plotting with profile lines\")\n","\n","# Get mesh cell faces\n","mesh_cell_faces = HdfMesh.mesh_cell_faces(geom_hdf_path, ras_object=bald_eagle)\n","\n","# Display the first few rows of the mesh cell faces DataFrame\n","print(\"First few rows of mesh cell faces:\")\n","display(mesh_cell_faces.head())\n","\n","# Load the GeoJSON file for profile lines\n","geojson_path = r'data/profile_lines_chippewa2D.geojson'  # Update with the correct path\n","profile_lines_gdf = gpd.read_file(geojson_path)\n","\n","# Set the Coordinate Reference System (CRS) to EPSG:5070\n","profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n","\n","# Plot the mesh cell faces and profile lines together\n","fig, ax = plt.subplots(figsize=(12, 8))\n","mesh_cell_faces.plot(ax=ax, color='blue', alpha=0.5, edgecolor='k', label='Mesh Cell Faces')\n","profile_lines_gdf.plot(ax=ax, color='orange', linewidth=2, label='Profile Lines')\n","\n","# Set labels and title\n","ax.set_xlabel('Easting')\n","ax.set_ylabel('Northing')\n","ax.set_title('Mesh Cell Faces and Profile Lines')\n","\n","# Add grid and legend\n","ax.grid(True)\n","ax.legend()\n","\n","# Adjust layout and display\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example: Extracting mesh cell faces near profile lines\n","print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n","\n","# Get mesh cell faces\n","mesh_cell_faces = HdfMesh.mesh_cell_faces(geom_hdf_path, ras_object=bald_eagle)\n","\n","# Display the first few rows of the mesh cell faces DataFrame\n","print(\"First few rows of mesh cell faces:\")\n","display(mesh_cell_faces.head())\n","\n","# Load the GeoJSON file for profile lines\n","geojson_path = r'data/profile_lines_chippewa2D.geojson'  # Update with the correct path\n","profile_lines_gdf = gpd.read_file(geojson_path)\n","\n","# Set the Coordinate Reference System (CRS) to EPSG:5070\n","profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n","\n","# Initialize a dictionary to store faces near each profile line\n","faces_near_profile_lines = {}\n","\n","# Define distance threshold (10 ft converted to meters)\n","distance_threshold = 10\n","angle_threshold = 60  # degrees\n","\n","# Function to calculate the smallest angle between two lines or line segments.\n","# The angle is calculated in degrees relative to the positive x-axis.\n","# If the input is a LineString object, the angle is computed using the \n","# coordinates of the start and end points of the line.\n","# If the input is a list of two points, the angle is calculated \n","# directly from those points.\n","\n","def calculate_angle(line):\n","    if isinstance(line, LineString):\n","        x_diff = line.xy[0][-1] - line.xy[0][0]\n","        y_diff = line.xy[1][-1] - line.xy[1][0]\n","    else:\n","        x_diff = line[1][0] - line[0][0]\n","        y_diff = line[1][1] - line[0][1]\n","    \n","    angle = np.degrees(np.arctan2(y_diff, x_diff))\n","    return angle % 360 if angle >= 0 else (angle + 360) % 360\n","\n","# Function to break line into segments\n","def break_line_into_segments(line, segment_length):\n","    segments = []\n","    segment_angles = []\n","    \n","    distances = np.arange(0, line.length, segment_length)\n","    if distances[-1] != line.length:\n","        distances = np.append(distances, line.length)\n","        \n","    for i in range(len(distances)-1):\n","        point1 = line.interpolate(distances[i])\n","        point2 = line.interpolate(distances[i+1])\n","        segment = LineString([point1, point2])\n","        segments.append(segment)\n","        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n","        \n","    return segments, segment_angles\n","\n","# Function to calculate angle difference accounting for 180 degree equivalence\n","def angle_difference(angle1, angle2):\n","    diff = abs(angle1 - angle2) % 180\n","    return min(diff, 180 - diff)\n","\n","# Function to order faces along profile line\n","def order_faces_along_profile(profile_line, faces_gdf):\n","    # Get start point of profile line\n","    profile_start = Point(profile_line.coords[0])\n","    \n","    # Calculate distance from each face's start point to profile start\n","    faces_with_dist = []\n","    for idx, face in faces_gdf.iterrows():\n","        face_start = Point(face.geometry.coords[0])\n","        dist = profile_start.distance(face_start)\n","        faces_with_dist.append((idx, dist))\n","    \n","    # Sort faces by distance\n","    faces_with_dist.sort(key=lambda x: x[1])\n","    \n","    # Return ordered face indices\n","    return [x[0] for x in faces_with_dist]\n","\n","# Function to combine ordered faces into single linestring\n","def combine_faces_to_linestring(ordered_faces_gdf):\n","    coords = []\n","    for _, face in ordered_faces_gdf.iterrows():\n","        if not coords:  # First face - add all coordinates\n","            coords.extend(list(face.geometry.coords))\n","        else:  # Subsequent faces - add only end coordinate\n","            coords.append(face.geometry.coords[-1])\n","    return LineString(coords)\n","\n","# Initialize GeoDataFrame for final profile-to-faceline results\n","profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n","\n","# Iterate through each profile line\n","for index, profile_line in profile_lines_gdf.iterrows():\n","    profile_geom = profile_line.geometry\n","    \n","    # Break profile line into segments\n","    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n","    \n","    # Initialize set to store nearby faces\n","    nearby_faces = set()\n","    \n","    # For each face, check distance to segments and angle difference\n","    for face_idx, face in mesh_cell_faces.iterrows():\n","        face_geom = face.geometry\n","        \n","        if isinstance(face_geom, LineString):\n","            face_angle = calculate_angle(face_geom)\n","            \n","            for segment, segment_angle in zip(segments, segment_angles):\n","                if face_geom.distance(segment) <= distance_threshold:\n","                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n","                        nearby_faces.add(face_idx)\n","                        break\n","    \n","    # Convert the set of indices back to a GeoDataFrame\n","    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n","    \n","    # Order faces along profile line\n","    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n","    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n","    \n","    # Combine ordered faces into single linestring\n","    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n","    \n","    # Add to profile_to_faceline GeoDataFrame\n","    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n","                               'geometry': [combined_linestring]}, \n","                              crs=profile_lines_gdf.crs)\n","    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n","    \n","    # Store the ordered faces in the dictionary\n","    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n","\n","# Plot the results\n","fig, ax = plt.subplots(figsize=(12, 8))\n","\n","# Plot all mesh cell faces in light blue\n","mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n","\n","# Plot selected faces for each profile line with numbers\n","colors = ['red', 'green', 'blue']\n","for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n","    if not faces.empty:\n","        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n","        \n","        # Add numbers to faces\n","        for i, (idx, face) in enumerate(faces.iterrows()):\n","            midpoint = face.geometry.interpolate(0.5, normalized=True)\n","            ax.text(midpoint.x, midpoint.y, str(i+1), \n","                   color=color, fontweight='bold', ha='center', va='center')\n","\n","# Plot the combined linestrings\n","profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n","                        linestyle='--', label='Combined Face Lines')\n","\n","# Set labels and title\n","ax.set_xlabel('Easting')\n","ax.set_ylabel('Northing')\n","ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile')\n","\n","# Add grid and legend\n","ax.grid(True)\n","ax.legend()\n","\n","# Adjust layout and display\n","plt.tight_layout()\n","plt.show()\n","\n","# Display the results\n","print(\"\\nOriginal ordered faces near profile lines:\")\n","display(faces_near_profile_lines)\n","\n","print(\"\\nCombined profile-to-faceline results:\")\n","display(profile_to_faceline)"]},{"cell_type":"markdown","metadata":{},"source":["-----"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get face property tables\n","\n","face_property_tables = HdfMesh.get_face_property_tables(geom_hdf_path)\n","\n","display(face_property_tables)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract the face property table for Face ID 4 and display it\n","import matplotlib.pyplot as plt\n","\n","face_id = 4\n","face_properties = face_property_tables['Perimeter 1'][face_property_tables['Perimeter 1']['Face ID'] == face_id]\n","\n","# Create subplots arranged horizontally\n","fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n","\n","# Plot Z vs Area\n","axs[0].plot(face_properties['Z'], face_properties['Area'], marker='o', color='blue', label='Area')\n","axs[0].set_title(f'Face ID {face_id}: Z vs Area')\n","axs[0].set_xlabel('Z')\n","axs[0].set_ylabel('Area')\n","axs[0].grid(True)\n","axs[0].legend()\n","\n","# Plot Z vs Wetted Perimeter\n","axs[1].plot(face_properties['Z'], face_properties['Wetted Perimeter'], marker='o', color='green', label='Wetted Perimeter')\n","axs[1].set_title(f'Face ID {face_id}: Z vs Wetted Perimeter')\n","axs[1].set_xlabel('Z')\n","axs[1].set_ylabel('Wetted Perimeter')\n","axs[1].grid(True)\n","axs[1].legend()\n","\n","# Plot Z vs Manning's n\n","axs[2].plot(face_properties['Z'], face_properties[\"Manning's n\"], marker='o', color='red', label=\"Manning's n\")\n","axs[2].set_title(f'Face ID {face_id}: Z vs Manning\\'s n')\n","axs[2].set_xlabel('Z')\n","axs[2].set_ylabel(\"Manning's n\")\n","axs[2].grid(True)\n","axs[2].legend()\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get mesh timeseries output\n","\n","# Get mesh areas from previous code cell\n","mesh_areas = HdfMesh.mesh_area_names(geom_hdf_path, ras_object=bald_eagle)\n","\n","if mesh_areas:\n","    mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n","    timeseries_da = HdfResultsMesh.mesh_timeseries_output(plan_hdf_path, mesh_name, \"Water Surface\", ras_object=bald_eagle)\n","    print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n","    print(timeseries_da)\n","else:\n","    print(\"No mesh areas found in the geometry file.\")\n","\n","# Get mesh cells timeseries output\n","cells_timeseries_ds = HdfResultsMesh.mesh_cells_timeseries_output(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n","print(\"\\nMesh Cells Timeseries Output:\")\n","print(cells_timeseries_ds)\n","\n","# Get mesh faces timeseries output\n","faces_timeseries_ds = HdfResultsMesh.mesh_faces_timeseries_output(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n","print(\"\\nMesh Faces Timeseries Output:\")\n","print(faces_timeseries_ds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert all face velocities and face flow values to positive\n","\n","# Function to process and convert face data to positive values\n","def convert_to_positive_values(faces_timeseries_ds, cells_timeseries_ds):\n","    \"\"\"\n","    Convert face velocities and flows to positive values while maintaining their relationships.\n","    \n","    Args:\n","        faces_timeseries_ds (xarray.Dataset): Dataset containing face timeseries data\n","        cells_timeseries_ds (xarray.Dataset): Dataset containing cell timeseries data\n","        \n","    Returns:\n","        xarray.Dataset: Modified dataset with positive values\n","    \"\"\"\n","    # Get the face velocity and flow variables\n","    face_velocity = faces_timeseries_ds['face_velocity']\n","    face_flow = faces_timeseries_ds['face_flow']\n","    \n","    # Calculate the sign of the velocity to maintain flow direction relationships\n","    velocity_sign = xr.where(face_velocity >= 0, 1, -1)\n","    \n","    # Convert velocities and flows to absolute values while maintaining their relationship\n","    faces_timeseries_ds['face_velocity'] = abs(face_velocity)\n","    faces_timeseries_ds['face_flow'] = abs(face_flow)\n","    \n","    # Store the original sign as a new variable for reference\n","    faces_timeseries_ds['velocity_direction'] = velocity_sign\n","    \n","    print(\"Conversion to positive values complete.\")\n","    print(f\"Number of faces processed: {len(faces_timeseries_ds.face_id)}\")\n","    \n","    return faces_timeseries_ds, cells_timeseries_ds\n","\n","# Convert the values in our datasets\n","faces_timeseries_ds_positive, cells_timeseries_ds_positive = convert_to_positive_values(\n","    faces_timeseries_ds, \n","    cells_timeseries_ds\n",")\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import xarray as xr\n","\n","# Function to process faces for a single profile line\n","def process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds):\n","    face_ids = faces['face_id'].tolist()\n","    \n","    # Extract relevant data for these faces\n","    face_velocities = faces_timeseries_ds['face_velocity'].sel(face_id=face_ids)\n","    face_flows = faces_timeseries_ds['face_flow'].sel(face_id=face_ids)\n","    \n","    # Create a new dataset with calculated results\n","    results_ds = xr.Dataset({\n","        'face_velocity': face_velocities,\n","        'face_flow': face_flows\n","    })\n","    \n","    # Convert to dataframe for easier manipulation\n","    results_df = results_ds.to_dataframe().reset_index()\n","    \n","    # Add profile name and face order\n","    results_df['profile_name'] = profile_name\n","    results_df['face_order'] = results_df.groupby('time')['face_id'].transform(lambda x: pd.factorize(x)[0])\n","    \n","    return results_df\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Calculate Vave = Sum Qn / Sum An for each profile line\n","# where Vave = the summation of face flow / flow area for all the faces in the profile line\n","\n","# Then, save the results to CSV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Process all profile lines\n","all_results = []\n","for profile_name, faces in faces_near_profile_lines.items():\n","    profile_results = process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds)\n","    all_results.append(profile_results)\n","\n","# Combine results from all profile lines\n","combined_results_df = pd.concat(all_results, ignore_index=True)\n","\n","# Display the first few rows of the combined results\n","print(combined_results_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["-----"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["profile_time_series = {}\n","\n","# Iterate through each profile line and extract its corresponding data\n","for profile_name, faces_gdf in faces_near_profile_lines.items():\n","    # Get the list of face_ids for this profile line\n","    face_ids = faces_gdf['face_id'].tolist()\n","    \n","    # Filter the combined_results_df for these face_ids\n","    profile_df = combined_results_df[combined_results_df['face_id'].isin(face_ids)].copy()\n","    \n","    # Add the profile name as a column\n","    profile_df['profile_name'] = profile_name\n","    \n","    # Reset index for cleanliness\n","    profile_df.reset_index(drop=True, inplace=True)\n","    \n","    # Store in the dictionary\n","    profile_time_series[profile_name] = profile_df\n","    \n","    # Display a preview\n","    print(f\"\\nTime Series DataFrame for {profile_name}:\")\n","    display(profile_df.head())\n","\n","# Optionally, display all profile names\n","print(\"\\nProfile Lines Processed:\")\n","print(list(profile_time_series.keys()))\n"]},{"cell_type":"markdown","metadata":{},"source":["| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n","|------------|---------|---------------|-------------|----------------|------------|\n","| 2019-04-02 | 370     | 1.543974      | 961.118225  | Profile Line 1 | 0          |\n","| 2019-04-02 | 232     | 2.738194      | 5103.555176 | Profile Line 1 | 1          |\n","| 2019-04-02 | 747     | 3.109769      | 4777.513672 | Profile Line 1 | 2          |\n","| 2019-04-02 | 216     | 2.974400      | 5120.266113 | Profile Line 1 | 3          |\n","| 2019-04-02 | 184     | 0.924792      | 700.676697  | Profile Line 1 | 4          |  \n","  \n","\n","\n","| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n","|------------|---------|---------------|-------------|----------------|------------|\n","| 2019-04-02 | 52      | 0.000000      | 0.000000    | Profile Line 2 | 0          |\n","| 2019-04-02 | 92      | 0.000000      | 0.000000    | Profile Line 2 | 1          |\n","| 2019-04-02 | 548     | 1.018038      | 353.129822  | Profile Line 2 | 2          |\n","| 2019-04-02 | 691     | 2.106394      | 2195.409912 | Profile Line 2 | 3          |\n","| 2019-04-02 | 78      | 2.376904      | 3600.228760 | Profile Line 2 | 4          |  \n","  \n","\n","\n","| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n","|------------|---------|---------------|-------------|----------------|------------|\n","| 2019-04-02 | 532     | 0.000000      | 0.000000    | Profile Line 3 | 0          |\n","| 2019-04-02 | 341     | 0.000000      | 0.000000    | Profile Line 3 | 1          |\n","| 2019-04-02 | 349     | 1.962641      | 2601.644287 | Profile Line 3 | 2          |\n","| 2019-04-02 | 455     | 2.367594      | 4148.870605 | Profile Line 3 | 3          |\n","| 2019-04-02 | 469     | 2.515510      | 4458.292480 | Profile Line 3 | 4          |  \n","  \n","  \n"," \n","Profile Lines Processed:\n","['Profile Line 1', 'Profile Line 2', 'Profile Line 3']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_profiles_df = pd.concat(profile_time_series.values(), ignore_index=True)\n","\n","# Display the combined dataframe\n","print(\"Combined Time Series DataFrame for All Profiles:\")\n","display(all_profiles_df.head())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Recursively explore the 2D Flow Areas structure in the geometry HDF file\n","import h5py\n","\n","def print_hdf_structure(name, obj):\n","    \"\"\"Print information about HDF5 object\"\"\"\n","    print(f\"\\nPath: {name}\")\n","    print(f\"Type: {type(obj).__name__}\")\n","    \n","    if isinstance(obj, h5py.Dataset):\n","        print(f\"Shape: {obj.shape}\")\n","        print(f\"Dtype: {obj.dtype}\")\n","        print(\"Attributes:\")\n","        for key, value in obj.attrs.items():\n","            print(f\"  {key}: {value}\")\n","\n","def explore_flow_areas(file_path):\n","    \"\"\"\n","    Recursively explore and print 2D Flow Areas structure in HDF5 file\n","    \n","    :param file_path: Path to the HDF5 file\n","    \"\"\"\n","    try:\n","        with h5py.File(file_path, 'r') as hdf_file:\n","            if '/Geometry/2D Flow Areas' in hdf_file:\n","                flow_areas_group = hdf_file['/Geometry/2D Flow Areas']\n","                flow_areas_group.visititems(print_hdf_structure)\n","            else:\n","                print(\"2D Flow Areas group not found in geometry file\")\n","    except Exception as e:\n","        print(f\"Error exploring HDF file: {e}\")\n","\n","print(\"\\nExploring 2D Flow Areas structure in geometry file:\")\n","print(\"HDF Base Path: /Geometry/2D Flow Areas \")\n","explore_flow_areas(geom_hdf_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check if we have the necessary variables\n","print(\"Available variables:\")\n","print(\"profile_time_series:\", 'profile_time_series' in locals())\n","print(\"faces_near_profile_lines:\", 'faces_near_profile_lines' in locals())\n","print(\"profile_averages:\", 'profile_averages' in locals())\n","\n","# Look at the structure of profile_time_series\n","if 'profile_time_series' in locals():\n","    for name, df in profile_time_series.items():\n","        print(f\"\\nColumns in {name}:\")\n","        print(df.columns.tolist())"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def calculate_discharge_weighted_velocity(profile_df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Calculate discharge-weighted average velocity for a profile line\n","    Vw = Sum(|Qi|*Vi)/Sum(|Qi|) where Qi is face flow and Vi is face velocity\n","    \"\"\"\n","    print(\"Calculating discharge-weighted velocity...\")\n","    print(f\"Input DataFrame:\\n{profile_df.head()}\")\n","\n","    # Calculate weighted velocity for each timestep\n","    weighted_velocities = []\n","    for time in profile_df['time'].unique():\n","        time_data = profile_df[profile_df['time'] == time]\n","        abs_flows = np.abs(time_data['face_flow'])\n","        abs_velocities = np.abs(time_data['face_velocity'])\n","        weighted_vel = (abs_flows * abs_velocities).sum() / abs_flows.sum()\n","        weighted_velocities.append({\n","            'time': time,\n","            'weighted_velocity': weighted_vel\n","        })\n","    \n","    weighted_df = pd.DataFrame(weighted_velocities)\n","    print(f\"Calculated weighted velocities:\\n{weighted_df.head()}\")\n","    return weighted_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate for each profile line\n","\n","for profile_name, profile_df in profile_time_series.items():\n","    print(f\"\\nProcessing profile: {profile_name}\")\n","\n","    # Calculate discharge-weighted velocity\n","    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n","    \n","    print(\"Weighted velocities calculated.\")\n","    display(weighted_velocities)\n","    \n","    # Convert time to datetime if it isn't already\n","    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n","    print(\"Converted time to datetime format.\")\n","\n","    # Get ordered faces for this profile\n","    ordered_faces = faces_near_profile_lines[profile_name]\n","    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n","    \n","    # Save dataframes as profile_name + \"_discharge_weighted_velocity.csv\"\n","    # Save weighted velocities to CSV\n","    output_file = f\"{profile_name}_discharge_weighted_velocity.csv\"\n","    weighted_velocities.to_csv(output_file, index=False)\n","    print(f\"Saved weighted velocities to {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create plots comparing discharge-weighted velocity and simple average for each profile line\n","for profile_name, profile_df in profile_time_series.items():\n","    \n","    print(f\"\\nGenerating comparison plot for profile: {profile_name}\")\n","    \n","    # Calculate discharge-weighted velocity\n","    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n","    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n","    \n","    # Calculate simple average velocity for each timestep\n","    simple_averages = profile_df.groupby('time')['face_velocity'].mean().reset_index()\n","    simple_averages['time'] = pd.to_datetime(simple_averages['time'])\n","    \n","    # Create figure for comparison plot\n","    plt.figure(figsize=(16, 9))\n","    \n","    # Plot individual face velocities with thin lines\n","    for face_id in profile_df['face_id'].unique():\n","        face_data = profile_df[profile_df['face_id'] == face_id]\n","        plt.plot(face_data['time'], \n","                face_data['face_velocity'], \n","                alpha=0.8,  # More transparent\n","                linewidth=0.3,  # Thinner line\n","                color='gray',  # Consistent color\n","                label=f'Face ID {face_id}' if face_id == profile_df['face_id'].iloc[0] else \"\")\n","        \n","        # Find and annotate peak value for each face\n","        peak_idx = face_data['face_velocity'].idxmax()\n","        peak_time = face_data.loc[peak_idx, 'time']\n","        peak_vel = face_data.loc[peak_idx, 'face_velocity']\n","        plt.annotate(f'{peak_vel:.2f}',\n","                    xy=(peak_time, peak_vel),\n","                    xytext=(10, 10),\n","                    textcoords='offset points',\n","                    fontsize=8,\n","                    alpha=0.5)\n","    \n","    # Plot discharge-weighted velocity\n","    plt.plot(weighted_velocities['time'], \n","            weighted_velocities['weighted_velocity'], \n","            color='red', \n","            alpha=1.0, \n","            linewidth=2,\n","            label='Discharge-Weighted Velocity')\n","    \n","    # Find and annotate peak weighted velocity\n","    peak_idx = weighted_velocities['weighted_velocity'].idxmax()\n","    peak_time = weighted_velocities.loc[peak_idx, 'time']\n","    peak_vel = weighted_velocities.loc[peak_idx, 'weighted_velocity']\n","    plt.annotate(f'Peak Weighted: {peak_vel:.2f}',\n","                xy=(peak_time, peak_vel),\n","                xytext=(10, 10),\n","                textcoords='offset points',\n","                color='red',\n","                fontweight='bold')\n","    \n","    # Plot simple average\n","    plt.plot(simple_averages['time'], \n","            simple_averages['face_velocity'], \n","            color='blue', \n","            alpha=0.5, \n","            linewidth=1,\n","            linestyle='--',\n","            label='Simple Average')\n","    \n","    # Find and annotate peak simple average\n","    peak_idx = simple_averages['face_velocity'].idxmax()\n","    peak_time = simple_averages.loc[peak_idx, 'time']\n","    peak_vel = simple_averages.loc[peak_idx, 'face_velocity']\n","    plt.annotate(f'Peak Average: {peak_vel:.2f}',\n","                xy=(peak_time, peak_vel),\n","                xytext=(10, -10),\n","                textcoords='offset points',\n","                color='blue',\n","                fontweight='bold')\n","    \n","    # Configure plot\n","    plt.title(f'Velocity Comparison - {profile_name}')\n","    plt.xlabel('Time')\n","    plt.ylabel('Velocity (ft/s)')\n","    plt.grid(True, alpha=0.3)\n","    \n","    # Add legend with better placement\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","    \n","    # Adjust layout to accommodate legend and stats\n","    plt.subplots_adjust(right=0.8)\n","    plt.show()\n","    \n","    # Print detailed comparison\n","    print(f\"\\nVelocity Comparison for {profile_name}:\")\n","    print(f\"Number of faces: {profile_df['face_id'].nunique()}\")\n","    print(\"\\nDischarge-Weighted Velocity Statistics:\")\n","    print(f\"Mean: {weighted_velocities['weighted_velocity'].mean():.2f} ft/s\")\n","    print(f\"Max: {weighted_velocities['weighted_velocity'].max():.2f} ft/s\")\n","    print(f\"Min: {weighted_velocities['weighted_velocity'].min():.2f} ft/s\")\n","    print(\"\\nSimple Average Velocity Statistics:\")\n","    print(f\"Mean: {simple_averages['face_velocity'].mean():.2f} ft/s\")\n","    print(f\"Max: {simple_averages['face_velocity'].max():.2f} ft/s\")\n","    print(f\"Min: {simple_averages['face_velocity'].min():.2f} ft/s\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":2}

==================================================

File: c:\GH\ras-commander\examples\23_generating_aep_hyetographs_from_atlas_14.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.4 Homework and Advanced Applications (AEP Event Modeling)\n",
    "\n",
    "## Created For: AI in H&H Modeling Webinar - October 24, 2024\n",
    "\n",
    "In this session we will modify the Infiltration data for the Davis project, and extract 2D results data from HDF to plot and compare. \n",
    "- Accessing and Modifying Infiltration HDF Data\n",
    "- Scaling Infiltration Rates\n",
    "- Running Multiple Scenarios\n",
    "- Extracting and Comparing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "1. Utilize GPT o1-mini to generate a hyetograph for a 24-hour storm event from readily available NOAA Atlas 14 CSV data. \n",
    "\n",
    "\n",
    "\n",
    "This data can also be accessed programmatically, or can be averaged for a particular watershed area.  \n",
    " https://github.com/billk-FM/HEC-Commander/tree/main/RAS-Commander/Atlas_14_Variance  \n",
    "\n",
    "Downloading the CSV data from NOAA Atlas 14 is a good middle ground for data entry.  We can utilize the CSV for input into our script, and the location is easily changed for new locations covered by NOAA Atlas 14. \n",
    "\n",
    "### Providing Context to GPT o1-mini\n",
    "\n",
    "The following pages were copied and pasted into the context window of GPT o1-mini, with clear separators between each data source.  \n",
    "\n",
    "- CSV File from NOAA Atlas 14\n",
    "- NOAA Atlas 14 Data Source (Link etc)\n",
    "- Text from the HEC-HMS Technical Reference Manual, \"Frequency Storm\" Section\n",
    "https://www.hec.usace.army.mil/confluence/hmsdocs/hmstrm/meteorology/precipitation/frequency-storm  \n",
    "(omitting the storm area sections since this does not apply)\n",
    "\n",
    "Follow-ups:\n",
    "\n",
    "The follow-ups are documented in the chat conversation linked below.  \n",
    " - The first script was provided as python, not a notebook cell.  I requested a notebook cell. \n",
    " - Several errors were fed back to o1-mini, which were corrected in the final version. These were generally minor errosrs (number of fields/datetime)\n",
    " - A final request to provide plots, which were worked without error\n",
    "\n",
    "For this task, o1's tendency to overthink, revise liberally and provide a lot of output was helpful.  \n",
    "\n",
    "I was able quickly provide a working script based on established and well documented methods that still may not be conveniently accessible in python and would traditionally require a lot of manual coding or an excel spreadsheet.  \n",
    "\n",
    "ChatGPT o1-mini conversation: https://chatgpt.com/share/67152f62-d648-8010-ada7-2ddbf500cb4b   \n",
    "\n",
    "Now, we can focus on getting these hyetographs into HEC-RAS and running the model with these revised inputs, and comparing the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final o1-mini generated code for generating hyetographs is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from math import log, exp\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to parse duration strings and convert them to hours\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a duration string and converts it to hours.\n",
    "    Examples:\n",
    "        \"5-min:\" -> 0.0833 hours\n",
    "        \"2-hr:\" -> 2 hours\n",
    "        \"2-day:\" -> 48 hours\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "    value, unit = match.groups()\n",
    "    value = int(value)\n",
    "    unit = unit.lower()\n",
    "    if unit in ['min', 'minute', 'minutes']:\n",
    "        hours = value / 60.0\n",
    "    elif unit in ['hr', 'hour', 'hours']:\n",
    "        hours = value\n",
    "    elif unit in ['day', 'days']:\n",
    "        hours = value * 24\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
    "    return hours\n",
    "\n",
    "# Function to read and process the precipitation frequency CSV\n",
    "def read_precipitation_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the precipitation frequency CSV and returns a DataFrame\n",
    "    with durations in hours as the index and ARIs as columns.\n",
    "    This function dynamically locates the header line for the data table.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_line_idx = None\n",
    "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
    "\n",
    "    # Locate the header line\n",
    "    for idx, line in enumerate(lines):\n",
    "        if header_pattern.match(line.strip().lower()):\n",
    "            header_line_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_line_idx is None:\n",
    "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
    "\n",
    "    # Extract the ARI headers from the header line\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    headers = [item.strip() for item in header_line.split(',')]\n",
    "    \n",
    "    if len(headers) < 2:\n",
    "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
    "\n",
    "    aris = headers[1:]  # Exclude the first column which is the duration\n",
    "\n",
    "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
    "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
    "\n",
    "    # Initialize lists to store durations and corresponding depths\n",
    "    durations = []\n",
    "    depths = {ari: [] for ari in aris}\n",
    "\n",
    "    # Iterate over the lines following the header to extract data\n",
    "    for line in lines[header_line_idx + 1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if not duration_pattern.match(line):\n",
    "            break  # Stop if the line does not match the duration pattern\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if len(parts) != len(headers):\n",
    "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
    "        duration_str = parts[0]\n",
    "        try:\n",
    "            duration_hours = parse_duration(duration_str)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Skipping line due to error: {ve}\")\n",
    "            continue  # Skip lines with invalid duration formats\n",
    "        durations.append(duration_hours)\n",
    "        for ari, depth_str in zip(aris, parts[1:]):\n",
    "            try:\n",
    "                depth = float(depth_str)\n",
    "            except ValueError:\n",
    "                depth = np.nan  # Assign NaN for invalid depth values\n",
    "            depths[ari].append(depth)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(depths, index=durations)\n",
    "    df.index.name = 'Duration_hours'\n",
    "\n",
    "    # Drop any rows with NaN values (optional, based on data quality)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to perform log-log linear interpolation for each ARI\n",
    "def interpolate_depths(df, total_duration):\n",
    "    \"\"\"\n",
    "    Interpolates precipitation depths for each ARI on a log-log scale\n",
    "    for each hour up to the total storm duration.\n",
    "    \"\"\"\n",
    "    T = total_duration\n",
    "    t_hours = np.arange(1, T+1)\n",
    "    D = {}\n",
    "    for ari in df.columns:\n",
    "        durations = df.index.values\n",
    "        depths = df[ari].values\n",
    "        # Ensure all depths are positive\n",
    "        if np.any(depths <= 0):\n",
    "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
    "        # Log-log interpolation\n",
    "        log_durations = np.log(durations)\n",
    "        log_depths = np.log(depths)\n",
    "        log_t = np.log(t_hours)\n",
    "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
    "        D_t = np.exp(log_D_t)\n",
    "        D[ari] = D_t\n",
    "    return D\n",
    "\n",
    "# Function to compute incremental precipitation depths\n",
    "def compute_incremental_depths(D, total_duration):\n",
    "    \"\"\"\n",
    "    Computes incremental precipitation depths for each hour.\n",
    "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
    "    \"\"\"\n",
    "    incremental_depths = {}\n",
    "    for ari, D_t in D.items():\n",
    "        I_t = np.empty(total_duration)\n",
    "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
    "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
    "        incremental_depths[ari] = I_t\n",
    "    return incremental_depths\n",
    "\n",
    "# Function to assign incremental depths using the Alternating Block Method\n",
    "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
    "    \"\"\"\n",
    "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    hyetograph = [0.0] * T\n",
    "    hyetograph[central_index] = max_depth\n",
    "    remaining_depths = sorted_depths.copy()\n",
    "    remaining_depths.remove(max_depth)\n",
    "    left = central_index - 1\n",
    "    right = central_index + 1\n",
    "    toggle = True  # Start assigning to the right\n",
    "    for depth in remaining_depths:\n",
    "        if toggle and right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif not toggle and left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        elif right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        else:\n",
    "            print(\"Warning: Not all incremental depths assigned.\")\n",
    "            break\n",
    "        toggle = not toggle\n",
    "    return hyetograph\n",
    "\n",
    "# Function to generate the hyetograph for a given ARI\n",
    "def generate_hyetograph(incremental_depths, position_percent, T):\n",
    "    \"\"\"\n",
    "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    max_depth = np.max(incremental_depths)\n",
    "    incremental_depths_list = incremental_depths.tolist()\n",
    "    central_index = int(round(T * position_percent / 100)) - 1\n",
    "    central_index = max(0, min(central_index, T - 1))\n",
    "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
    "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
    "    return hyetograph\n",
    "\n",
    "# Function to save the hyetograph to a CSV file\n",
    "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
    "    \"\"\"\n",
    "    Saves the hyetograph to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Time_hour': np.arange(1, total_duration + 1),\n",
    "        'Precipitation_in': hyetograph\n",
    "    })\n",
    "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
    "\n",
    "# User Inputs\n",
    "# --------------------\n",
    "# Set the path to your input CSV file from NOAA Atlas 14\n",
    "input_csv = 'PF_Depth_English_PDS_DavisCA.csv'  # Update this path if necessary\n",
    "\n",
    "# Set the output directory where hyetograph CSV files will be saved\n",
    "output_dir = 'hyetographs'\n",
    "\n",
    "# Set the position percentage for the maximum incremental depth block\n",
    "# Choose from 25, 33, 50, 67, or 75\n",
    "position_percent = 50  # Default is 50\n",
    "\n",
    "# Set the total storm duration in hours\n",
    "total_duration = 24  # Default is 24 hours\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "# Read precipitation data\n",
    "try:\n",
    "    df = read_precipitation_data(input_csv)\n",
    "    print(\"Successfully read the input CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading input CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(\"\\nPrecipitation Frequency Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# Interpolate depths\n",
    "try:\n",
    "    D = interpolate_depths(df, total_duration)\n",
    "    print(\"Successfully interpolated precipitation depths.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during interpolation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display interpolated depths for the first ARI\n",
    "first_ari = df.columns[0]\n",
    "print(f\"\\nInterpolated Depths for ARI {first_ari} years:\")\n",
    "print(D[first_ari])\n",
    "\n",
    "# Compute incremental depths\n",
    "I = compute_incremental_depths(D, total_duration)\n",
    "print(\"Successfully computed incremental depths.\")\n",
    "\n",
    "# Generate and save hyetographs for each ARI\n",
    "for ari, incremental_depths in I.items():\n",
    "    hyetograph = generate_hyetograph(incremental_depths, position_percent, total_duration)\n",
    "    save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration)\n",
    "\n",
    "print(\"\\nAll hyetographs have been generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the hyetographs (final request from o1-mini)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot multiple hyetographs on the same plot\n",
    "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
    "    \"\"\"\n",
    "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    - aris (list of str or int): List of Annual Recurrence Intervals to plot (e.g., [1, 2, 5, 10])\n",
    "    - position_percent (int): Position percentage for the maximum incremental depth block (25, 33, 50, 67, or 75)\n",
    "    - total_duration (int): Total storm duration in hours\n",
    "    - output_dir (str): Directory where hyetograph CSV files are saved\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    for ari in aris:\n",
    "        # Ensure ARI is a string for consistent filename formatting\n",
    "        ari_str = str(ari)\n",
    "        \n",
    "        # Construct the filename based on the naming convention\n",
    "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: File '{filename}' does not exist in the directory '{output_dir}'. Skipping this ARI.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        try:\n",
    "            hyetograph_df = pd.read_csv(filepath)\n",
    "            print(f\"Successfully read the hyetograph data from '{filename}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading the hyetograph CSV file '{filename}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot the hyetograph\n",
    "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
    "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Time (Hour)', fontsize=14)\n",
    "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
    "    plt.title(f'Comparison of Hyetographs for ARIs {aris}\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 24)))  # Adjust x-ticks based on duration\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# User Inputs for Multiple ARIs\n",
    "# --------------------\n",
    "# Set the Annual Recurrence Intervals you want to plot\n",
    "aris_to_plot = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]  # Example: Multiple ARIs\n",
    "\n",
    "# Set the position percentage for the maximum incremental depth block\n",
    "position_percent = 50  # Example: 50%\n",
    "\n",
    "# Set the total storm duration in hours\n",
    "total_duration = 24  # Example: 24 hours\n",
    "\n",
    "# Set the output directory where hyetograph CSV files are saved\n",
    "output_dir = 'hyetographs'  # Ensure this matches the output directory used previously\n",
    "\n",
    "# Plot the multiple hyetographs\n",
    "plot_multiple_hyetographs(aris=aris_to_plot, \n",
    "                           position_percent=position_percent, \n",
    "                           total_duration=total_duration, \n",
    "                           output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: REVISE BELOW TO RUN DAVIS AND EXTRACT RESULTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_oct16_webinar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\24_fluvial_pluvial_delineation.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delineate Fluvial and Pluvial Areas using RAS-Commander\n",
    "\n",
    "We will leverage the HEC RAS Summary Outputs to delineate the Fluvial and Pluvial Areas\n",
    "\n",
    "Maximum Water Surface Elevation (WSEL) for each cell is recorded, along with the timestamps of when the maximum WSEL occurs.\n",
    "\n",
    "By locating adjacent cells with dissimilar timestamps, we can delineate the Fluvial and Pluvial Areas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about datframe types: \n",
    "\n",
    "Information from the HEC-RAS plan files are generally dataframes.  The text file interface is for the 32-bit side of HEC-RAS and all spatial data is most easily accessed in the HDF files.  This includes plan_df, geom_df, hdf_paths_df\n",
    "\n",
    "Geometry elements (Mesh Faces and Nodes) are provided as Geodataframes (cell_polygons_gdf, boundary_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Libraries\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def install_module(module_name):\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "    except ImportError:\n",
    "        print(f\"{module_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n",
    "\n",
    "# List of modules to check and install if necessary\n",
    "modules = ['h5py', 'numpy', 'requests', 'geopandas', 'matplotlib', 'pandas', 'pyproj', 'shapely', 'xarray', 'rtree']\n",
    "for module in modules:\n",
    "    install_module(module)\n",
    "\n",
    "# Import the rest of the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Flexible imports to allow for development without installation \n",
    "#  ** Use this version with Jupyter Notebooks **\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    from ras_commander import (init_ras_project, HdfBase, HdfFluvialPluvial, HdfPlot, HdfResultsPlot, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n",
    "    from ras_commander.Decorators import standardize_input, log_call\n",
    "    from ras_commander.LoggingConfig import setup_logging, get_logger\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    print(\"Importing from local ras_commander directory\")\n",
    "    import os\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    parent_directory = current_file.parent\n",
    "    sys.path.append(str(parent_directory))\n",
    "    \n",
    "    # Now try to import again\n",
    "    from ras_commander import (init_ras_project, HdfBase, HdfFluvialPluvial, HdfPlot, HdfResultsPlot, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, RasPrj, RasGpt, ras)\n",
    "    from ras_commander.Decorators import standardize_input, log_call\n",
    "    from ras_commander.LoggingConfig import setup_logging, get_logger\n",
    "\n",
    "print(\"ras_commander imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and run plan 06\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    ras_examples = RasExamples()\n",
    "    ras_examples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "    # Initialize custom Ras object\n",
    "    bald_eagle = RasPrj()\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {bald_eagle.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(bald_eagle)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"06\"\n",
    "\n",
    "    # Update the run flags in the plan file\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,  # Run HTab\n",
    "        unsteady_flow_simulation=True,  # Run UNet\n",
    "        post_processor=True,  # Run PostProcess\n",
    "        floodplain_mapping=False,  # Run RASMapper\n",
    "        ras_object=bald_eagle\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = RasPrj()\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\", ras_instance=bald_eagle)\n",
    "    plan_number = \"06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "display(bald_eagle.plan_df)\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "display(bald_eagle.geom_df)\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mesh_max_ws, get the cell coordinates and plot the max water surface as a map\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Get mesh max water surface\n",
    "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "print(\"max_ws_df\")\n",
    "print(max_ws_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot\n",
    "HdfResultsPlot.plot_results_max_wsel(max_ws_df)\n",
    "\n",
    "# Plot the time of maximum water surface elevation\n",
    "HdfResultsPlot.plot_results_max_wsel_time(max_ws_df)\n",
    "\n",
    "# Print the first few rows of the merged dataframe for verification\n",
    "print(\"\\nFirst few rows of the merged dataframe:\")\n",
    "display(max_ws_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Cell Polygons\n",
    "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
    "cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "\n",
    "# Call the function to plot cell polygons\n",
    "#cell_polygons_gdf = HdfFluvialPluvial.plot_cell_polygons(cell_polygons_gdf, projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString, Polygon, MultiLineString\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from rtree import index\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(plan_hdf_path)\n",
    "\n",
    "# Print general information about the boundary GeoDataFrame\n",
    "print(\"\\nBoundary GeoDataFrame info:\")\n",
    "print(boundary_gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics about the boundary line lengths\n",
    "boundary_lengths = boundary_gdf.geometry.length\n",
    "\n",
    "print(\"Boundary line length statistics:\")\n",
    "print(f\"Max length: {boundary_lengths.max():.2f}\")\n",
    "print(f\"Min length: {boundary_lengths.min():.2f}\")\n",
    "print(f\"Average length: {boundary_lengths.mean():.2f}\")\n",
    "print(f\"Median length: {boundary_lengths.median():.2f}\")\n",
    "\n",
    "# Print general information about the boundary GeoDataFrame\n",
    "print(\"\\nBoundary GeoDataFrame info:\")\n",
    "print(boundary_gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
    "boundary_gdf.plot(ax=ax, color='red', linewidth=2)\n",
    "plt.title('Fluvial-Pluvial Boundary')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_threshold = 250 #in same units as X and Y coordinates\n",
    "\n",
    "# Filter out boundary lines below the length threshold\n",
    "filtered_boundary_gdf = boundary_gdf[boundary_lengths >= length_threshold]\n",
    "highlighted_boundary_gdf = boundary_gdf[boundary_lengths < length_threshold]\n",
    "\n",
    "# Visualize the results with highlighted boundaries below the threshold\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
    "filtered_boundary_gdf.plot(ax=ax, color='red', linewidth=2, label='Valid Boundaries')\n",
    "highlighted_boundary_gdf.plot(ax=ax, color='blue', linewidth=2, linestyle='--', label='Highlighted Boundaries Below Threshold')\n",
    "plt.title('Fluvial-Pluvial Boundary with Length Threshold')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to GeoJSON\n",
    "boundary_gdf.to_file('fluvial_pluvial_boundary.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrwksp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

Folder: c:\GH\ras-commander\examples\data
==================================================

Folder: c:\GH\ras-commander\examples\img
==================================================

File: c:\GH\ras-commander\examples\xx_edge_cases.py
==================================================
#### --- IMPORTS AND EXAMPLE PROJECT SETUP --- ####

import sys
from pathlib import Path
import shutil

# Add the parent directory to the Python path
current_file = Path(__file__).resolve()
parent_directory = current_file.parent.parent
sys.path.append(str(parent_directory))

# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras

example_projects_folder = Path(__file__).parent.parent / "example_projects"

# delete the folder if it exists
if example_projects_folder.exists():
    shutil.rmtree(example_projects_folder)


# Extract specific projects
ras_examples = RasExamples()
ras_examples.extract_project(["Balde Eagle Creek"])

#### --- START OF SCRIPT --- ####

def main:
    """Docs only, see 'main.py' for full function code"""
    main()
==================================================

File: c:\GH\ras-commander\ras_commander\Decorators.py
==================================================
from functools import wraps
from pathlib import Path
from typing import Union
import logging
import h5py
import inspect


def log_call:
    """Docs only, see 'log_call.py' for full function code"""
    return decorator
==================================================

File: c:\GH\ras-commander\ras_commander\HdfBase.py
==================================================
"""
HdfBase: Core HDF File Operations for HEC-RAS

This module provides fundamental methods for interacting with HEC-RAS HDF files.
It serves as a foundation for more specialized HDF classes.

Attribution:
    Derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    Copyright (c) 2024 fema-ffrd - MIT License

Features:
    - Time parsing and conversion utilities
    - HDF attribute and dataset access
    - Geometric data extraction
    - 2D flow area information retrieval

Classes:
    HdfBase: Base class containing static methods for HDF operations

Key Methods:
    Time Operations:
        - get_simulation_start_time(): Get simulation start datetime
        - get_unsteady_timestamps(): Get unsteady output timestamps
        - parse_ras_datetime(): Parse RAS datetime strings
    
    Data Access:
        - get_2d_flow_area_names_and_counts(): Get 2D flow area info
        - get_projection(): Get spatial projection
        - get_attrs(): Access HDF attributes
        - get_dataset_info(): Explore HDF structure
        - get_polylines_from_parts(): Extract geometric polylines

Example:
    ```python
    from ras_commander import HdfBase
    
    with h5py.File('model.hdf', 'r') as hdf:
        start_time = HdfBase.get_simulation_start_time(hdf)
        timestamps = HdfBase.get_unsteady_timestamps(hdf)
    ```
"""
import re
from datetime import datetime, timedelta
import h5py
import numpy as np
import pandas as pd
import xarray as xr
from typing import List, Tuple, Union, Optional, Dict, Any
from pathlib import Path
import logging
from shapely.geometry import LineString, MultiLineString

from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfBase:
    """
    Base class for HEC-RAS HDF file operations.

    This class provides static methods for fundamental HDF file operations,
    including time parsing, attribute access, and geometric data extraction.
    All methods are designed to work with h5py.File objects or pathlib.Path
    inputs.

    Note:
        This class is not meant to be instantiated. All methods are static
        and should be called directly from the class.
    """

    @staticmethod
    def get_simulation_start_time(hdf_file: h5py.File) -> datetime:
        """
        Extract the simulation start time from the HDF file.

        Args:
            hdf_file: Open HDF file object containing RAS simulation data.

        Returns:
            datetime: Simulation start time as a datetime object.

        Raises:
            ValueError: If Plan Information is not found or start time cannot be parsed.
        
        Note:
            Expects 'Plan Data/Plan Information' group with 'Simulation Start Time' attribute.
        """
        plan_info = hdf_file.get("Plan Data/Plan Information")
        if plan_info is None:
            raise ValueError("Plan Information not found in HDF file")
        time_str = plan_info.attrs.get('Simulation Start Time')
        return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))

    @staticmethod
    def get_unsteady_timestamps(hdf_file: h5py.File) -> List[datetime]:
        """
        Extract the list of unsteady timestamps from the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.

        Returns:
            List[datetime]: A list of datetime objects representing the unsteady timestamps.
        """
        group_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
        raw_datetimes = hdf_file[group_path][:]
        return [HdfUtils.parse_ras_datetime_ms(x.decode("utf-8")) for x in raw_datetimes]

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_2d_flow_area_names_and_counts(hdf_path: Path) -> List[Tuple[str, int]]:
        """
        Get the names and cell counts of 2D flow areas from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            List[Tuple[str, int]]: A list of tuples containing the name and cell count of each 2D flow area.

        Raises:
            ValueError: If there's an error reading the HDF file or accessing the required data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                flow_area_2d_path = "Geometry/2D Flow Areas"
                if flow_area_2d_path not in hdf_file:
                    return []
                
                attributes = hdf_file[f"{flow_area_2d_path}/Attributes"][()]
                names = [HdfUtils.convert_ras_string(name) for name in attributes["Name"]]
                
                cell_info = hdf_file[f"{flow_area_2d_path}/Cell Info"][()]
                cell_counts = [info[1] for info in cell_info]
                
                return list(zip(names, cell_counts))
        except Exception as e:
            logger.error(f"Error reading 2D flow area names and counts from {hdf_path}: {str(e)}")
            raise ValueError(f"Failed to get 2D flow area names and counts: {str(e)}")

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_projection(hdf_path: Path) -> Optional[str]:
        """
        Get the projection information from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            Optional[str]: The projection information as a string, or None if not found.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                proj_wkt = hdf_file.attrs.get("Projection")
                if proj_wkt is None:
                    return None
                if isinstance(proj_wkt, bytes) or isinstance(proj_wkt, np.bytes_):
                    proj_wkt = proj_wkt.decode("utf-8")
                return proj_wkt
        except Exception as e:
            logger.error(f"Error reading projection from {hdf_path}: {str(e)}")
            return None

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_attrs(hdf_file: h5py.File, attr_path: str) -> Dict[str, Any]:
        """
        Get attributes from an HDF file at a specified path.

        Args:
            hdf_file (h5py.File): The opened HDF file.
            attr_path (str): Path to the attributes in the HDF file.

        Returns:
            Dict[str, Any]: Dictionary of attributes.
        """
        try:
            if attr_path not in hdf_file:
                logger.warning(f"Path {attr_path} not found in HDF file")
                return {}
            
            return HdfUtils.hdf5_attrs_to_dict(hdf_file[attr_path].attrs)
        except Exception as e:
            logger.error(f"Error getting attributes from {attr_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_dataset_info(file_path: Path, group_path: str = '/') -> None:
        """
        Recursively explore and print the structure of an HDF5 file.

        Displays detailed information about groups, datasets, and their attributes
        in a hierarchical format.

        Args:
            file_path: Path to the HDF5 file.
            group_path: Starting group path to explore (default: root '/').

        Prints:
            - Group and dataset names with hierarchical indentation
            - Dataset shapes and data types
            - All attributes for groups and datasets
        """
        def recurse:
    """Docs only, see 'recurse.py' for full function code"""




==================================================

File: c:\GH\ras-commander\ras_commander\HdfBndry.py
==================================================
"""
Class: HdfBndry

A utility class for extracting and processing boundary-related features from HEC-RAS HDF files,
including boundary conditions, breaklines, refinement regions, and reference features.

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfBndry:
- get_bc_lines()           # Returns boundary condition lines as a GeoDataFrame.
- get_breaklines()         # Returns 2D mesh area breaklines as a GeoDataFrame.
- get_refinement_regions() # Returns refinement regions as a GeoDataFrame.
- get_reference_lines()    # Returns reference lines as a GeoDataFrame.
- get_reference_points()   # Returns reference points as a GeoDataFrame.



"""
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .HdfMesh import HdfMesh
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfBndry:
    """
    A class for handling boundary-related data from HEC-RAS HDF files.

    This class provides methods to extract and process various boundary elements
    such as boundary condition lines, breaklines, refinement regions, and reference
    lines/points from HEC-RAS geometry HDF files.

    Methods in this class return data primarily as GeoDataFrames, making it easy
    to work with spatial data in a geospatial context.

    Note:
        This class relies on the HdfBase and HdfUtils classes for some of its
        functionality. Ensure these classes are available in the same package.
    """
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_bc_lines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area boundary condition lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the boundary condition lines and their attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                bc_lines_path = "Geometry/Boundary Condition Lines"
                if bc_lines_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                # Get geometries
                bc_line_data = hdf_file[bc_lines_path]
                geoms = HdfUtils.get_polylines_from_parts(hdf_path, bc_lines_path)
                
                # Get attributes
                attributes = pd.DataFrame(bc_line_data["Attributes"][()])
                
                # Convert string columns
                str_columns = ['Name', 'SA-2D', 'Type']
                for col in str_columns:
                    if col in attributes.columns:
                        attributes[col] = attributes[col].apply(HdfUtils.convert_ras_string)
                
                # Create GeoDataFrame with all attributes
                gdf = gpd.GeoDataFrame(
                    attributes,
                    geometry=geoms,
                    crs=HdfUtils.get_projection(hdf_file)
                )
                
                # Add ID column if not present
                if 'bc_line_id' not in gdf.columns:
                    gdf['bc_line_id'] = range(len(gdf))
                    
                return gdf

        except Exception as e:
            logger.error(f"Error reading boundary condition lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_breaklines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area breaklines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the breaklines.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                breaklines_path = "Geometry/2D Flow Area Break Lines"
                if breaklines_path not in hdf_file:
                    logger.warning(f"Breaklines path '{breaklines_path}' not found in HDF file.")
                    return gpd.GeoDataFrame()
                bl_line_data = hdf_file[breaklines_path]
                bl_line_ids = range(bl_line_data["Attributes"][()].shape[0])
                names = np.vectorize(HdfUtils.convert_ras_string)(
                    bl_line_data["Attributes"][()]["Name"]
                )
                geoms = HdfUtils.get_polylines_from_parts(hdf_path, breaklines_path)
                return gpd.GeoDataFrame(
                    {"bl_id": bl_line_ids, "Name": names, "geometry": geoms},
                    geometry="geometry",
                    crs=HdfUtils.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading breaklines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_refinement_regions(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area refinement regions.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the refinement regions.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                refinement_regions_path = "/Geometry/2D Flow Area Refinement Regions"
                if refinement_regions_path not in hdf_file:
                    return gpd.GeoDataFrame()
                rr_data = hdf_file[refinement_regions_path]
                rr_ids = range(rr_data["Attributes"][()].shape[0])
                names = np.vectorize(HdfUtils.convert_ras_string)(rr_data["Attributes"][()]["Name"])
                geoms = list()
                for pnt_start, pnt_cnt, part_start, part_cnt in rr_data["Polygon Info"][()]:
                    points = rr_data["Polygon Points"][()][pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(Polygon(points))
                    else:
                        parts = rr_data["Polygon Parts"][()][part_start : part_start + part_cnt]
                        geoms.append(
                            MultiPolygon(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return gpd.GeoDataFrame(
                    {"rr_id": rr_ids, "Name": names, "geometry": geoms},
                    geometry="geometry",
                    crs=HdfUtils.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading refinement regions: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_lines(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference lines geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference lines. If mesh_name is provided,
            returns only lines for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_lines_path = "Geometry/Reference Lines"
                attributes_path = f"{reference_lines_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                attributes = hdf_file[attributes_path][()]
                refline_ids = range(attributes.shape[0])
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA-2D"])
                
                try:
                    types = v_conv_str(attributes["Type"])
                except ValueError:
                    types = np.array([""] * attributes.shape[0])
                
                geoms = HdfUtils.get_polylines_from_parts(hdf_path, reference_lines_path)
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refln_id": refline_ids,
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Type": types,
                        "geometry": geoms,
                    },
                    geometry="geometry",
                    crs=HdfUtils.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_points(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference points geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference points. If mesh_name is provided,
            returns only points for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_points_path = "Geometry/Reference Points"
                attributes_path = f"{reference_points_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                ref_points_group = hdf_file[reference_points_path]
                attributes = ref_points_group["Attributes"][:]
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA/2D"])
                cell_id = attributes["Cell Index"]
                points = ref_points_group["Points"][()]
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refpt_id": range(attributes.shape[0]),
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Cell Index": cell_id,
                        "geometry": list(map(Point, points)),
                    },
                    geometry="geometry",
                    crs=HdfUtils.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference points: {str(e)}")
            return gpd.GeoDataFrame()

    

==================================================

File: c:\GH\ras-commander\ras_commander\HdfFluvialPluvial.py
==================================================
"""
Class: HdfFluvialPluvial

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfFluvialPluvial:
- calculate_fluvial_pluvial_boundary()
- _process_cell_adjacencies()
- _identify_boundary_edges()

"""

from typing import Dict, List, Tuple
import pandas as pd
import geopandas as gpd
from collections import defaultdict
from shapely.geometry import LineString
from tqdm import tqdm
from .HdfMesh import HdfMesh
from .HdfUtils import HdfUtils
from .Decorators import standardize_input
from .HdfResultsMesh import HdfResultsMesh
from .LoggingConfig import get_logger
from pathlib import Path

logger = get_logger(__name__)

class HdfFluvialPluvial:
    """
    A class for analyzing and visualizing fluvial-pluvial boundaries in HEC-RAS 2D model results.

    This class provides methods to process and visualize HEC-RAS 2D model outputs,
    specifically focusing on the delineation of fluvial and pluvial flood areas.
    It includes functionality for calculating fluvial-pluvial boundaries based on
    the timing of maximum water surface elevations.

    Key Concepts:
    - Fluvial flooding: Flooding from rivers/streams
    - Pluvial flooding: Flooding from rainfall/surface water
    - Delta_t: Time threshold (in hours) used to distinguish between fluvial and pluvial cells.
               Cells with max WSE time differences greater than delta_t are considered boundaries.

    Data Requirements:
    - HEC-RAS plan HDF file containing:
        - 2D mesh cell geometry (accessed via HdfMesh)
        - Maximum water surface elevation times (accessed via HdfResultsMesh)

    Usage Example:
        >>> ras = init_ras_project(project_path, ras_version)
        >>> hdf_path = Path("path/to/plan.hdf")
        >>> boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(
        ...     hdf_path, 
        ...     delta_t=12
        ... )
    """

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def calculate_fluvial_pluvial_boundary(hdf_path: Path, delta_t: float = 12) -> gpd.GeoDataFrame:
        """
        Calculate the fluvial-pluvial boundary based on cell polygons and maximum water surface elevation times.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file
            delta_t (float): Threshold time difference in hours. Cells with time differences
                        greater than this value are considered boundaries. Default is 12 hours.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing the fluvial-pluvial boundaries with:
                - geometry: LineString features representing boundaries
                - CRS: Coordinate reference system matching the input HDF file

        Raises:
            ValueError: If no cell polygons or maximum water surface data found in HDF file
            Exception: If there are errors during boundary calculation

        Note:
            The returned boundaries represent locations where the timing of maximum water surface
            elevation changes significantly (> delta_t), indicating potential transitions between
            fluvial and pluvial flooding mechanisms.
        """
        try:
            # Get cell polygons from HdfMesh
            logger.info("Getting cell polygons from HDF file...")
            cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(hdf_path)
            if cell_polygons_gdf.empty:
                raise ValueError("No cell polygons found in HDF file")

            # Get max water surface data from HdfResultsMesh
            logger.info("Getting maximum water surface data from HDF file...")
            max_ws_df = HdfResultsMesh.get_mesh_max_ws(hdf_path)
            if max_ws_df.empty:
                raise ValueError("No maximum water surface data found in HDF file")

            # Convert timestamps using the renamed utility function
            if 'maximum_water_surface_time' in max_ws_df.columns:
                max_ws_df['maximum_water_surface_time'] = max_ws_df['maximum_water_surface_time'].apply(
                    lambda x: HdfUtils.parse_ras_datetime(x) if isinstance(x, str) else x
                )

            # Process cell adjacencies
            cell_adjacency, common_edges = HdfFluvialPluvial._process_cell_adjacencies(cell_polygons_gdf)
            
            # Get cell times from max_ws_df
            cell_times = max_ws_df.set_index('cell_id')['maximum_water_surface_time'].to_dict()
            
            # Identify boundary edges
            boundary_edges = HdfFluvialPluvial._identify_boundary_edges(
                cell_adjacency, common_edges, cell_times, delta_t
            )

            # Join adjacent LineStrings into simple LineStrings
            joined_lines = []
            current_line = []

            for edge in boundary_edges:
                if not current_line:
                    current_line.append(edge)
                else:
                    if current_line[-1].coords[-1] == edge.coords[0]:
                        current_line.append(edge)
                    else:
                        joined_lines.append(LineString([point for line in current_line for point in line.coords]))
                        current_line = [edge]

            if current_line:
                joined_lines.append(LineString([point for line in current_line for point in line.coords]))

            # Create final GeoDataFrame with CRS from cell_polygons_gdf
            boundary_gdf = gpd.GeoDataFrame(
                geometry=joined_lines, 
                crs=cell_polygons_gdf.crs
            )

            # Clean up intermediate dataframes
            del cell_polygons_gdf
            del max_ws_df

            return boundary_gdf

        except Exception as e:
            logger.error(f"Error calculating fluvial-pluvial boundary: {str(e)}")
            raise

    @staticmethod
    def _process_cell_adjacencies(cell_polygons_gdf: gpd.GeoDataFrame) -> Tuple[Dict[int, List[int]], Dict[int, Dict[int, LineString]]]:
        """
        Process cell adjacencies and common edges using R-tree spatial indexing for efficiency.

        Args:
            cell_polygons_gdf (gpd.GeoDataFrame): GeoDataFrame containing 2D mesh cell polygons
                                                 with 'cell_id' and 'geometry' columns

        Returns:
            Tuple containing:
                - Dict[int, List[int]]: Dictionary mapping cell IDs to lists of adjacent cell IDs
                - Dict[int, Dict[int, LineString]]: Nested dictionary storing common edges between cells,
                                                   where common_edges[cell1][cell2] gives the shared boundary

        Note:
            Uses R-tree spatial indexing to efficiently identify potential neighboring cells
            before performing more detailed geometric operations.
        """
        from rtree import index
        cell_adjacency = defaultdict(list)
        common_edges = defaultdict(dict)
        idx = index.Index()

        for i, geom in enumerate(cell_polygons_gdf.geometry):
            idx.insert(i, geom.bounds)

        with tqdm(total=len(cell_polygons_gdf), desc="Processing cell adjacencies") as pbar:
            for idx1, row1 in cell_polygons_gdf.iterrows():
                cell_id1 = row1['cell_id']
                poly1 = row1['geometry']
                potential_neighbors = list(idx.intersection(poly1.bounds))

                for idx2 in potential_neighbors:
                    if idx1 >= idx2:
                        continue

                    row2 = cell_polygons_gdf.iloc[idx2]
                    cell_id2 = row2['cell_id']
                    poly2 = row2['geometry']

                    if poly1.touches(poly2):
                        intersection = poly1.intersection(poly2)
                        if isinstance(intersection, LineString):
                            cell_adjacency[cell_id1].append(cell_id2)
                            cell_adjacency[cell_id2].append(cell_id1)
                            common_edges[cell_id1][cell_id2] = intersection
                            common_edges[cell_id2][cell_id1] = intersection

                pbar.update(1)

        return cell_adjacency, common_edges

    @staticmethod
    def _identify_boundary_edges(cell_adjacency: Dict[int, List[int]], 
                               common_edges: Dict[int, Dict[int, LineString]], 
                               cell_times: Dict[int, pd.Timestamp], 
                               delta_t: float) -> List[LineString]:
        """
        Identify boundary edges between cells with significant time differences.

        Args:
            cell_adjacency (Dict[int, List[int]]): Dictionary of cell adjacencies
            common_edges (Dict[int, Dict[int, LineString]]): Dictionary of shared edges between cells
            cell_times (Dict[int, pd.Timestamp]): Dictionary mapping cell IDs to their max WSE times
            delta_t (float): Time threshold in hours

        Returns:
            List[LineString]: List of LineString geometries representing boundaries where
                             adjacent cells have time differences greater than delta_t

        Note:
            Boundaries are identified where the absolute time difference between adjacent
            cells exceeds the specified delta_t threshold.
        """
        boundary_edges = []
        with tqdm(total=len(cell_adjacency), desc="Processing cell adjacencies") as pbar:
            for cell_id, neighbors in cell_adjacency.items():
                cell_time = cell_times[cell_id]

                for neighbor_id in neighbors:
                    neighbor_time = cell_times[neighbor_id]
                    time_diff = abs((cell_time - neighbor_time).total_seconds() / 3600)

                    if time_diff >= delta_t:
                        boundary_edges.append(common_edges[cell_id][neighbor_id])

                pbar.update(1)

        return boundary_edges

==================================================

File: c:\GH\ras-commander\ras_commander\HdfInfiltration.py
==================================================
"""
Class: HdfInfiltration

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfInfiltration:
- scale_infiltration_data(): Updates infiltration parameters in HDF file with scaling factors
- get_infiltration_data(): Retrieves current infiltration parameters from HDF file
- get_infiltration_map(): Reads the infiltration raster map from HDF file
- calculate_soil_statistics(): Calculates soil statistics from zonal statistics
- get_significant_mukeys(): Gets mukeys with percentage greater than threshold
- calculate_total_significant_percentage(): Calculates total percentage covered by significant mukeys
- save_statistics(): Saves soil statistics to CSV
- get_infiltration_parameters(): Gets infiltration parameters for a specific mukey
- calculate_weighted_parameters(): Calculates weighted infiltration parameters based on soil statistics

Each function is decorated with @standardize_input to ensure consistent handling of HDF file paths
and @log_call for logging function calls and errors. Functions return various data types including
DataFrames, dictionaries, and floating-point values depending on their purpose.

The class provides comprehensive functionality for analyzing and modifying infiltration-related
data in HEC-RAS HDF files, including parameter scaling, soil statistics calculation, and
weighted parameter computation.
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)
        
from pathlib import Path
import pandas as pd
import geopandas as gpd
import h5py
from rasterstats import zonal_stats
from .Decorators import log_call, standardize_input

class HdfInfiltration:
        
    """
    A class for handling infiltration-related operations on HEC-RAS HDF files.

    This class provides methods to extract and modify infiltration data from HEC-RAS HDF files,
    including base overrides and infiltration parameters.
    """

    # Constants for unit conversion
    SQM_TO_ACRE = 0.000247105
    SQM_TO_SQMILE = 3.861e-7
    
    def __init__:
    """Docs only, see '__init__.py' for full function code"""
"""
from pathlib import Path

# Initialize paths
raster_path = Path('input_files/gSSURGO_InfiltrationDC.tif')
boundary_path = Path('input_files/WF_Boundary_Simple.shp')
hdf_path = raster_path.with_suffix('.hdf')

# Get infiltration mapping
infil_map = HdfInfiltration.get_infiltration_map(hdf_path)

# Get zonal statistics (using RasMapper class)
clipped_data, transform, nodata = RasMapper.clip_raster_with_boundary(
    raster_path, boundary_path)
stats = RasMapper.calculate_zonal_stats(
    boundary_path, clipped_data, transform, nodata)

# Calculate soil statistics
soil_stats = HdfInfiltration.calculate_soil_statistics(stats, infil_map)

# Get significant mukeys (>1%)
significant = HdfInfiltration.get_significant_mukeys(soil_stats, threshold=1.0)

# Calculate total percentage of significant mukeys
total_significant = HdfInfiltration.calculate_total_significant_percentage(significant)
print(f"Total percentage of significant mukeys: {total_significant}%")

# Get infiltration parameters for each significant mukey
infiltration_params = {}
for mukey in significant['mukey']:
    params = HdfInfiltration.get_infiltration_parameters(hdf_path, mukey)
    if params:
        infiltration_params[mukey] = params

# Calculate weighted parameters
weighted_params = HdfInfiltration.calculate_weighted_parameters(
    significant, infiltration_params)
print("Weighted infiltration parameters:", weighted_params)

# Save results
HdfInfiltration.save_statistics(soil_stats, Path('soil_statistics.csv'))
"""
==================================================

File: c:\GH\ras-commander\ras_commander\HdfMesh.py
==================================================
"""
A static class for handling mesh-related operations on HEC-RAS HDF files.

This class provides static methods to extract and analyze mesh data from HEC-RAS HDF files,
including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
2D flow area attributes. No instantiation is required to use these methods.

All methods are designed to work with the mesh geometry data stored in
HEC-RAS HDF files, providing functionality to retrieve and process various aspects
of the 2D flow areas and their associated mesh structures.

List of Functions:
-----------------
get_mesh_area_names()
    Returns list of 2D mesh area names
get_mesh_areas()
    Returns 2D flow area perimeter polygons
mesh_cell_polygons()
    Returns 2D flow mesh cell polygons
[etc...]

Each function is decorated with @standardize_input and @log_call for consistent
input handling and logging functionality.
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import Polygon, Point, LineString, MultiLineString, MultiPolygon
from shapely.ops import polygonize  # Importing polygonize to resolve the undefined name error
from typing import List, Tuple, Optional, Dict, Any
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfMesh:
    """
    A class for handling mesh-related operations on HEC-RAS HDF files.

    This class provides methods to extract and analyze mesh data from HEC-RAS HDF files,
    including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
    2D flow area attributes.

    Methods in this class are designed to work with the mesh geometry data stored in
    HEC-RAS HDF files, providing functionality to retrieve and process various aspects
    of the 2D flow areas and their associated mesh structures.

    Note: This class relies on HdfBase and HdfUtils for some underlying operations.
    """

    def __init__:
    """Docs only, see '__init__.py' for full function code"""

==================================================

File: c:\GH\ras-commander\ras_commander\HdfPipe.py
==================================================
"""
Class: HdfPipe

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPipe:
Geometry Retrieval Functions:
- get_pipe_conduits() - Get pipe conduit geometries and attributes
- get_pipe_nodes() - Get pipe node geometries and attributes
- get_pipe_network() - Get complete pipe network data
- get_pipe_profile() - Get elevation profile for a specific conduit
- extract_pipe_network_data() - Extract both nodes and conduits data

Results Retrieval Functions:
- get_pipe_network_timeseries() - Get timeseries data for pipe network variables
- get_pipe_network_summary() - Get summary statistics for pipe networks
- get_pipe_node_timeseries() - Get timeseries data for a specific node
- get_pipe_conduit_timeseries() - Get timeseries data for a specific conduit

Note: All functions use the @standardize_input decorator to validate input paths
and the @log_call decorator for logging function calls.
"""
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import LineString, Point, MultiLineString, Polygon, MultiPolygon
from typing import List, Dict, Any, Optional, Union, Tuple
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger
from .HdfResultsMesh import HdfResultsMesh
import logging  

logger = get_logger(__name__)

class HdfPipe:
    """
    Static methods for handling pipe network data from HEC-RAS HDF files.

    Contains methods for:
    - Geometry retrieval (nodes, conduits, networks, profiles)
    - Results retrieval (timeseries and summary data)

    All methods use @standardize_input for path validation and @log_call
    """

    # Geometry Retrieval Functions
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_conduits(hdf_path: Path, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Extracts pipe conduit geometries and attributes from HDF5 file.

        Parameters:
            hdf_path: Path to the HDF5 file
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame with columns:
            - Attributes from HDF5
            - Polyline: LineString geometries
            - Terrain_Profiles: List of (station, elevation) tuples
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Conduits/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Polyline Data ---
            polyline_info = group['Polyline Info'][:]  # Shape (132,4) - point_start_idx, point_count, part_start_idx, part_count
            polyline_points = group['Polyline Points'][:]  # Shape (396,2) - x,y coordinates
            
            polyline_geometries = []
            for info in polyline_info:
                point_start_idx = info[0]
                point_count = info[1]
                
                # Extract coordinates for this polyline directly using start index and count
                coords = polyline_points[point_start_idx:point_start_idx + point_count]
                
                if len(coords) < 2:
                    polyline_geometries.append(None)
                else:
                    polyline_geometries.append(LineString(coords))
            
            # --- Read Terrain Profiles Data ---
            terrain_info = group['Terrain Profiles Info'][:]
            terrain_values = group['Terrain Profiles Values'][:]
            
            # Create a list of (Station, Elevation) tuples for Terrain Profiles
            terrain_coords = list(zip(terrain_values[:, 0], terrain_values[:, 1]))
            
            terrain_profiles_list: List[List[Tuple[float, float]]] = []
            
            for i in range(len(terrain_info)):
                info = terrain_info[i]
                start_idx = info[0]
                count = info[1]
                
                # Extract (Station, Elevation) pairs
                segment = terrain_coords[start_idx : start_idx + count]
                
                terrain_profiles_list.append(segment)  # Store the list of (Station, Elevation) tuples
            
            # --- Combine Data into GeoDataFrame ---
            attr_df['Polyline'] = polyline_geometries
            attr_df['Terrain_Profiles'] = terrain_profiles_list
            
            # Initialize GeoDataFrame with Polyline geometries
            gdf = gpd.GeoDataFrame(attr_df, geometry='Polyline', crs=crs)
            
            return gdf


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_nodes(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for Pipe Node points and their attributes from an HDF5 file.
        
        Parameters:
        - hdf_path: Path to the HDF5 file.
        
        Returns:
        - A GeoDataFrame containing pipe node attributes and their geometries.
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Nodes/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns  # Changed 'S' to object
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Points Data ---
            points = group['Points'][:]
            # Create Shapely Point geometries
            geometries = [Point(xy) for xy in points]
            
            # --- Combine Attributes and Geometries into GeoDataFrame ---
            gdf = gpd.GeoDataFrame(attr_df, geometry=geometries)
            
            return gdf
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network(hdf_path: Path, pipe_network_name: Optional[str] = None, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for a pipe network's geometry.

        Parameters:
            hdf_path: Path to the HDF5 file
            pipe_network_name: Name of network (uses first if None)
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame containing:
            - Cell polygons (primary geometry)
            - Face polylines
            - Node points
            - Associated attributes
        """
        with h5py.File(hdf_path, 'r') as f:
            pipe_networks_group = f['/Geometry/Pipe Networks/']
            
            # --- Determine Pipe Network to Use ---
            attributes = pipe_networks_group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode 'Name' from byte strings to UTF-8
            attr_df['Name'] = attr_df['Name'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            if pipe_network_name:
                if pipe_network_name not in attr_df['Name'].values:
                    raise ValueError(f"Pipe network '{pipe_network_name}' not found in the HDF5 file.")
                network_idx = attr_df.index[attr_df['Name'] == pipe_network_name][0]
            else:
                network_idx = 0  # Default to first network
            
            # Get the name of the selected pipe network
            selected_network_name = attr_df.at[network_idx, 'Name']
            logging.info(f"Selected Pipe Network: {selected_network_name}")
            
            # Access the selected pipe network group
            network_group_path = f"/Geometry/Pipe Networks/{selected_network_name}/"
            network_group = f[network_group_path]
            
            # --- Helper Functions ---
            def decode_bytes(df: pd.DataFrame) -> pd.DataFrame:
                """Decode byte string columns to UTF-8."""
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                return df
            
            def build_polygons(info, parts, points) -> List[Optional[Polygon or MultiPolygon]]:
                """Build Shapely Polygon or MultiPolygon geometries from HDF5 datasets."""
                poly_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    cell_info = info[i]
                    point_start_idx = cell_info[0]
                    point_count = cell_info[1]
                    part_start_idx = cell_info[2]
                    part_count = cell_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = poly_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 3:
                            continue  # Not a valid polygon part
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        try:
                            geometries.append(Polygon(parts_list[0]))
                        except ValueError:
                            geometries.append(None)
                    else:
                        try:
                            geometries.append(MultiPolygon([Polygon(p) for p in parts_list if len(p) >= 3]))
                        except ValueError:
                            geometries.append(None)
                return geometries
            
            def build_multilinestring(info, parts, points) -> List[Optional[LineString or MultiLineString]]:
                """Build Shapely LineString or MultiLineString geometries from HDF5 datasets."""
                line_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    face_info = info[i]
                    point_start_idx = face_info[0]
                    point_count = face_info[1]
                    part_start_idx = face_info[2]
                    part_count = face_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = line_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 2:
                            continue  # Cannot form LineString with fewer than 2 points
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        geometries.append(LineString(parts_list[0]))
                    else:
                        geometries.append(MultiLineString(parts_list))
                return geometries
            
            # --- Read and Process Cell Polygons ---
            cell_polygons_info = network_group['Cell Polygons Info'][:]
            cell_polygons_parts = network_group['Cell Polygons Parts'][:]
            cell_polygons_points = network_group['Cell Polygons Points'][:]
            
            cell_polygons_geometries = build_polygons(cell_polygons_info, cell_polygons_parts, cell_polygons_points)
            
            # --- Read and Process Face Polylines ---
            face_polylines_info = network_group['Face Polylines Info'][:]
            face_polylines_parts = network_group['Face Polylines Parts'][:]
            face_polylines_points = network_group['Face Polylines Points'][:]
            
            face_polylines_geometries = build_multilinestring(face_polylines_info, face_polylines_parts, face_polylines_points)
            
            # --- Read and Process Node Points ---
            node_surface_connectivity_group = network_group.get('Node Surface Connectivity', None)
            if node_surface_connectivity_group is not None:
                node_surface_connectivity = node_surface_connectivity_group[:]
            else:
                node_surface_connectivity = None
            
            # Assuming Node Connectivity Info and Values contain node coordinates
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            node_indices = network_group['Node Indices'][:]
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            
            # For simplicity, assuming that node connectivity includes X and Y coordinates
            # This may need to be adjusted based on actual data structure
            # Here, we'll create dummy points as placeholder
            # Replace with actual coordinate extraction logic as per data structure
            # For demonstration, we'll create random points
            # You should replace this with actual data extraction
            # Example:
            # node_points = network_group['Node Coordinates'][:]
            # node_geometries = [Point(x, y) for x, y in node_points]
            
            # Placeholder for node geometries
            # Assuming node_indices contains Node IDs and coordinates
            # Adjust based on actual dataset structure
            # Here, we assume that node_indices has columns: [Node ID, X, Y]
            # But based on the log, Node Surface Connectivity has ['Node ID', 'Layer', 'Layer ID', 'Sublayer ID']
            # No coordinates are provided, so we cannot create Point geometries unless coordinates are available elsewhere
            # Therefore, this part may need to be adapted based on actual data
            # For now, we'll skip node points geometries
            node_geometries = [None] * len(node_indices)  # Placeholder
            
            # --- Read and Process Cell Property Table ---
            cell_property_table = network_group['Cell Property Table'][:]
            cell_property_df = pd.DataFrame(cell_property_table)
            
            # Decode byte strings if any
            cell_property_df = decode_bytes(cell_property_df)
            
            # --- Read and Process Cells DS Face Indices ---
            cells_ds_face_info = network_group['Cells DS Face Indices Info'][:]
            cells_ds_face_values = network_group['Cells DS Face Indices Values'][:]
            
            # Create lists of DS Face Indices per cell
            cells_ds_face_indices = []
            for i in range(len(cells_ds_face_info)):
                info = cells_ds_face_info[i]
                start_idx, count = info
                indices = cells_ds_face_values[start_idx : start_idx + count]
                cells_ds_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Face Indices ---
            cells_face_info = network_group['Cells Face Indices Info'][:]
            cells_face_values = network_group['Cells Face Indices Values'][:]
            
            # Create lists of Face Indices per cell
            cells_face_indices = []
            for i in range(len(cells_face_info)):
                info = cells_face_info[i]
                start_idx, count = info
                indices = cells_face_values[start_idx : start_idx + count]
                cells_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Minimum Elevations ---
            cells_min_elevations = network_group['Cells Minimum Elevations'][:]
            cells_min_elevations_df = pd.DataFrame(cells_min_elevations, columns=['Minimum_Elevation'])
            
            # --- Read and Process Cells Node and Conduit IDs ---
            cells_node_conduit_ids = network_group['Cells Node and Conduit IDs'][:]
            cells_node_conduit_df = pd.DataFrame(cells_node_conduit_ids, columns=['Node_ID', 'Conduit_ID'])
            
            # --- Read and Process Cells US Face Indices ---
            cells_us_face_info = network_group['Cells US Face Indices Info'][:]
            cells_us_face_values = network_group['Cells US Face Indices Values'][:]
            
            # Create lists of US Face Indices per cell
            cells_us_face_indices = []
            for i in range(len(cells_us_face_info)):
                info = cells_us_face_info[i]
                start_idx, count = info
                indices = cells_us_face_values[start_idx : start_idx + count]
                cells_us_face_indices.append(indices.tolist())
            
            # --- Read and Process Conduit Indices ---
            conduit_indices = network_group['Conduit Indices'][:]
            conduit_indices_df = pd.DataFrame(conduit_indices, columns=['Conduit_ID'])
            
            # --- Read and Process Face Property Table ---
            face_property_table = network_group['Face Property Table'][:]
            face_property_df = pd.DataFrame(face_property_table)
            
            # Decode byte strings if any
            face_property_df = decode_bytes(face_property_df)
            
            # --- Read and Process Face Conduit ID and Stations ---
            faces_conduit_id_stations = network_group['Faces Conduit ID and Stations'][:]
            faces_conduit_df = pd.DataFrame(faces_conduit_id_stations, columns=['ConduitID', 'ConduitStation', 'CellUS', 'CellDS', 'Elevation'])
            
            # --- Read and Process Node Connectivity Info and Values ---
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            
            # Create lists of connected nodes per node
            node_connectivity = []
            for i in range(len(node_connectivity_info)):
                info = node_connectivity_info[i]
                start_idx, count = info
                connections = node_connectivity_values[start_idx : start_idx + count]
                node_connectivity.append(connections.tolist())
            
            # --- Read and Process Node Indices ---
            node_indices = network_group['Node Indices'][:]
            node_indices_df = pd.DataFrame(node_indices, columns=['Node_ID'])
            
            # --- Read and Process Node Surface Connectivity ---
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            node_surface_connectivity_df = pd.DataFrame(node_surface_connectivity, columns=['Node_ID', 'Layer', 'Layer_ID', 'Sublayer_ID'])
            
            # --- Combine All Cell-Related Data ---
            cells_df = pd.DataFrame({
                'Cell_ID': range(len(cell_polygons_geometries)),
                'Conduit_ID': cells_node_conduit_df['Conduit_ID'],
                'Node_ID': cells_node_conduit_df['Node_ID'],
                'Minimum_Elevation': cells_min_elevations_df['Minimum_Elevation'],
                'DS_Face_Indices': cells_ds_face_indices,
                'Face_Indices': cells_face_indices,
                'US_Face_Indices': cells_us_face_indices,
                'Cell_Property_Info_Index': cell_property_df['Info Index'],
                # Add other cell properties as needed
            })
            
            # Merge with cell property table
            cells_df = cells_df.merge(cell_property_df, left_on='Cell_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Face-Related Data ---
            faces_df = pd.DataFrame({
                'Face_ID': range(len(face_polylines_geometries)),
                'Conduit_ID': faces_conduit_df['ConduitID'],
                'Conduit_Station': faces_conduit_df['ConduitStation'],
                'Cell_US': faces_conduit_df['CellUS'],
                'Cell_DS': faces_conduit_df['CellDS'],
                'Elevation': faces_conduit_df['Elevation'],
                'Face_Property_Info_Index': face_property_df['Info Index'],
                # Add other face properties as needed
            })
            
            # Merge with face property table
            faces_df = faces_df.merge(face_property_df, left_on='Face_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Node-Related Data ---
            nodes_df = pd.DataFrame({
                'Node_ID': node_indices_df['Node_ID'],
                'Connected_Nodes': node_connectivity,
                # Add other node properties as needed
            })
            
            # Merge with node surface connectivity
            nodes_df = nodes_df.merge(node_surface_connectivity_df, on='Node_ID', how='left')
            
            # --- Create GeoDataFrame ---
            # Main DataFrame will be cells with their polygons
            cells_df['Cell_Polygon'] = cell_polygons_geometries
            
            # Add face polylines as a separate column (list of geometries)
            cells_df['Face_Polylines'] = cells_df['Face_Indices'].apply(lambda indices: [face_polylines_geometries[i] for i in indices if i < len(face_polylines_geometries)])
            
            # Add node points if geometries are available
            # Currently, node_geometries are placeholders (None). Replace with actual geometries if available.
            cells_df['Node_Point'] = cells_df['Node_ID'].apply(lambda nid: node_geometries[nid] if nid < len(node_geometries) else None)
            
            # Initialize GeoDataFrame with Cell Polygons
            gdf = gpd.GeoDataFrame(cells_df, geometry='Cell_Polygon', crs=crs)
            
            # Optionally, add Face Polylines and Node Points as separate columns
            # Note: GeoPandas primarily supports one geometry column, so these are stored as object columns
            gdf['Face_Polylines'] = cells_df['Face_Polylines']
            gdf['Node_Point'] = cells_df['Node_Point']
            
            # You can further expand this GeoDataFrame by merging with faces_df and nodes_df if needed
            
            return gdf
        
        
        
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_profile(hdf_path: Path, conduit_id: int) -> pd.DataFrame:
        """
        Extract the profile data for a specific pipe conduit.

        Args:
            hdf_path (Path): Path to the HDF file.
            conduit_id (int): ID of the conduit to extract profile for.

        Returns:
            pd.DataFrame: DataFrame containing the pipe profile data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
            IndexError: If the specified conduit_id is out of range.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Get conduit info
                terrain_profiles_info = hdf['/Geometry/Pipe Conduits/Terrain Profiles Info'][()]
                
                if conduit_id >= len(terrain_profiles_info):
                    raise IndexError(f"conduit_id {conduit_id} is out of range")

                start, count = terrain_profiles_info[conduit_id]

                # Extract profile data
                profile_values = hdf['/Geometry/Pipe Conduits/Terrain Profiles Values'][start:start+count]

                # Create DataFrame
                df = pd.DataFrame(profile_values, columns=['Station', 'Elevation'])

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except IndexError as e:
            logger.error(f"Invalid conduit_id: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe profile data: {e}")
            raise
        
        
   









# RESULTS FUNCTIONS: 

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract results summary data for pipe networks from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pipe network summary data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pipe Network"
                if summary_path not in hdf:
                    logger.warning("Pipe Network summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame
                df = pd.DataFrame(summary_data)

                # Convert column names
                df.columns = [col.decode('utf-8') for col in df.columns]

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network summary data: {e}")
            raise




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_node(plan_hdf_path: Path, node_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific node.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        node_id : int
            ID of the node to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Depth
            - Drop Inlet Flow
            - Water Surface
        """
        try:
            node_variables = ["Nodes/Depth", "Nodes/Drop Inlet Flow", "Nodes/Water Surface"]
            node_data = {}

            for variable in node_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                node_data[variable] = data.sel(location=node_id)
            
            return node_data
        except Exception as e:
            logger.error(f"Error extracting time series data for node {node_id}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_conduit(plan_hdf_path: Path, conduit_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific conduit.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        conduit_id : int
            ID of the conduit to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Pipe Flow (US/DS)
            - Velocity (US/DS)
        """
        try:
            conduit_variables = ["Pipes/Pipe Flow DS", "Pipes/Pipe Flow US", 
                                "Pipes/Vel DS", "Pipes/Vel US"]
            conduit_data = {}

            for variable in conduit_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                conduit_data[variable] = data.sel(location=conduit_id)
            
            return conduit_data
        except Exception as e:
            logger.error(f"Error extracting time series data for conduit {conduit_id}: {str(e)}")
            raise


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_timeseries(hdf_path: Path, variable: str) -> xr.DataArray:
        """
        Extracts timeseries data for a pipe network variable.

        Parameters:
            hdf_path: Path to the HDF5 file
            variable: Variable name to extract. Valid options:
                - Cell: Courant, Water Surface
                - Face: Flow, Velocity, Water Surface
                - Pipes: Pipe Flow (DS/US), Vel (DS/US)
                - Nodes: Depth, Drop Inlet Flow, Water Surface

        Returns:
            xarray.DataArray with dimensions (time, location)
        """
        valid_variables = [
            "Cell Courant", "Cell Water Surface", "Face Flow", "Face Velocity",
            "Face Water Surface", "Pipes/Pipe Flow DS", "Pipes/Pipe Flow US",
            "Pipes/Vel DS", "Pipes/Vel US", "Nodes/Depth", "Nodes/Drop Inlet Flow",
            "Nodes/Water Surface"
        ]

        if variable not in valid_variables:
            raise ValueError(f"Invalid variable. Must be one of: {', '.join(valid_variables)}")

        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract timeseries data
                data_path = f"/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pipe Networks/Davis/{variable}"
                data = hdf[data_path][()]

                # Extract time information using the correct method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'location'],
                    coords={'time': time, 'location': range(data.shape[1])},
                    name=variable
                )

                # Add attributes
                da.attrs['units'] = hdf[data_path].attrs.get('Units', b'').decode('utf-8')
                da.attrs['variable'] = variable

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network timeseries data: {e}")
            raise





==================================================

File: c:\GH\ras-commander\ras_commander\HdfPlan.py
==================================================
"""
Class: HdfPlan

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPlan:
- get_simulation_start_time()
- get_simulation_end_time()
- get_unsteady_datetimes()
- get_plan_info_attrs()
- get_plan_parameters()
- get_meteorology_precip_attrs()
- get_geom_attrs()


REVISIONS NEEDED: 

Use get_ prefix for functions that return data.  
Since we are extracting plan data, we should use get_plan_...
BUT, we will never set results data, so we should use results_

We need to shorten names where possible.

List of Revised Functions in HdfPlan:
- get_plan_start_time()
- get_plan_end_time()
- get_plan_timestamps_list()     
- get_plan_information()
- get_plan_parameters()
- get_plan_met_precip()
- get_geometry_information()






"""

import h5py
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfPlan:
    """
    A class for handling HEC-RAS plan HDF files.

    Provides static methods for extracting data from HEC-RAS plan HDF files including 
    simulation times, plan information, and geometry attributes. All methods use 
    @standardize_input for handling different input types and @log_call for logging.

    Note: This code is partially derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    under MIT license.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_start_time(hdf_path: Path) -> datetime:
        """
        Get the plan start time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan start time in UTC format.

        Raises:
            ValueError: If there's an error reading the plan start time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_simulation_start_time(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan start time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_end_time(hdf_path: Path) -> datetime:
        """
        Get the plan end time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan end time.

        Raises:
            ValueError: If there's an error reading the plan end time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info = hdf_file.get('Plan Data/Plan Information')
                if plan_info is None:
                    raise ValueError("Plan Information not found in HDF file")
                time_str = plan_info.attrs.get('Simulation End Time')
                return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))
        except Exception as e:
            raise ValueError(f"Failed to get plan end time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_timestamps_list(hdf_path: Path) -> List[datetime]:
        """
        Get the list of output timestamps from the plan simulation.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            List[datetime]: Chronological list of simulation output timestamps in UTC.

        Raises:
            ValueError: If there's an error retrieving the plan timestamps.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_unsteady_timestamps(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan timestamps: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_information(hdf_path: Path) -> Dict:
        """
        Get plan information from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Plan information including simulation times, flow regime, 
                computation settings, etc.

        Raises:
            ValueError: If there's an error retrieving the plan information.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info_path = "Plan Data/Plan Information"
                if plan_info_path not in hdf_file:
                    raise ValueError(f"Plan Information not found in {hdf_path}")
                
                attrs = {}
                for key in hdf_file[plan_info_path].attrs.keys():
                    value = hdf_file[plan_info_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            raise ValueError(f"Failed to get plan information attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_parameters(hdf_path: Path) -> Dict:
        """
        Get plan parameter attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: A dictionary containing the plan parameter attributes.

        Raises:
            ValueError: If there's an error retrieving the plan parameter attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_params_path = "Plan Data/Plan Parameters"
                if plan_params_path not in hdf_file:
                    raise ValueError(f"Plan Parameters not found in {hdf_path}")
                
                attrs = {}
                for key in hdf_file[plan_params_path].attrs.keys():
                    value = hdf_file[plan_params_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            raise ValueError(f"Failed to get plan parameter attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_met_precip(hdf_path: Path) -> Dict:
        """
        Get precipitation attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Precipitation attributes including method, time series data,
                and spatial distribution if available. Returns empty dict if
                no precipitation data exists.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                precip_path = "Event Conditions/Meteorology/Precipitation"
                if precip_path not in hdf_file:
                    logger.error(f"Precipitation data not found in {hdf_path}")
                    return {}
                
                attrs = {}
                for key in hdf_file[precip_path].attrs.keys():
                    value = hdf_file[precip_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            logger.error(f"Failed to get precipitation attributes: {str(e)}")
            return {}
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_geometry_information(hdf_path: Path) -> pd.DataFrame:
        """
        Get root level geometry attributes from the HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: DataFrame with geometry attributes including Creation Date/Time,
                        Version, Units, and Projection information.

        Raises:
            ValueError: If Geometry group is missing or there's an error reading attributes.
        """
        print(f"Getting geometry attributes from {hdf_path}")
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                geom_attrs_path = "Geometry"
                print(f"Checking for Geometry group in {hdf_path}")
                if geom_attrs_path not in hdf_file:
                    raise ValueError(f"Geometry group not found in {hdf_path}")

                attrs = {}
                geom_group = hdf_file[geom_attrs_path]
                print("Getting root level geometry attributes")
                # Get root level geometry attributes only
                for key, value in geom_group.attrs.items():
                    if isinstance(value, bytes):
                        try:
                            value = HdfUtils.convert_ras_string(value)
                        except UnicodeDecodeError:
                            logger.warning(f"Failed to decode byte string for root attribute {key}")
                            continue
                    attrs[key] = value

                print("Successfully extracted root level geometry attributes")
                return pd.DataFrame.from_dict(attrs, orient='index', columns=['Value'])

        except (OSError, RuntimeError) as e:
            raise ValueError(f"Failed to read HDF file {hdf_path}: {str(e)}")
        except Exception as e:
            raise ValueError(f"Failed to get geometry attributes: {str(e)}")



==================================================

File: c:\GH\ras-commander\ras_commander\HdfPlot.py
==================================================
"""
Class: HdfPlot

A collection of static methods for plotting general HDF data from HEC-RAS models.
"""

import matplotlib.pyplot as plt
import pandas as pd
import geopandas as gpd
from typing import Optional, Union, Tuple
from .Decorators import log_call, standardize_input
from .HdfUtils import HdfUtils

class HdfPlot:
    """
    A class containing static methods for plotting general HDF data from HEC-RAS models.
    
    This class provides plotting functionality for HDF data, focusing on
    geometric elements like cell polygons and time series data.
    """

    @staticmethod
    @log_call
    def plot_mesh_cells(
        cell_polygons_df: pd.DataFrame, 
        projection: str,
        title: str = '2D Flow Area Mesh Cells',
        figsize: Tuple[int, int] = (12, 8)
    ) -> Optional[gpd.GeoDataFrame]:
        """
        Plots the mesh cells from the provided DataFrame and returns the GeoDataFrame.

        Args:
            cell_polygons_df (pd.DataFrame): DataFrame containing cell polygons.
            projection (str): The coordinate reference system to assign to the GeoDataFrame.
            title (str, optional): Plot title. Defaults to '2D Flow Area Mesh Cells'.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 8).

        Returns:
            Optional[gpd.GeoDataFrame]: GeoDataFrame containing the mesh cells, or None if no cells found.
        """
        if cell_polygons_df.empty:
            print("No Cell Polygons found.")
            return None

        # Convert any datetime columns to strings using HdfUtils
        cell_polygons_df = HdfUtils.convert_df_datetimes_to_str(cell_polygons_df)
        
        cell_polygons_gdf = gpd.GeoDataFrame(cell_polygons_df, crs=projection)

        print("Cell Polygons CRS:", cell_polygons_gdf.crs)
        display(cell_polygons_gdf.head())

        fig, ax = plt.subplots(figsize=figsize)
        cell_polygons_gdf.plot(ax=ax, edgecolor='blue', facecolor='none')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()

        return cell_polygons_gdf

    @staticmethod
    @log_call
    def plot_time_series(
        df: pd.DataFrame,
        x_col: str,
        y_col: str,
        title: str = None,
        figsize: Tuple[int, int] = (12, 6)
    ) -> None:
        """
        Plots time series data from HDF results.

        Args:
            df (pd.DataFrame): DataFrame containing the time series data
            x_col (str): Name of the column containing x-axis data (usually time)
            y_col (str): Name of the column containing y-axis data
            title (str, optional): Plot title. Defaults to None.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 6).
        """
        # Convert any datetime columns to strings
        df = HdfUtils.convert_df_datetimes_to_str(df)
        
        fig, ax = plt.subplots(figsize=figsize)
        df.plot(x=x_col, y=y_col, ax=ax)
        
        if title:
            ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()
    
    
    
    
    
    
    
    
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\HdfPump.py
==================================================
"""
Class: HdfPump

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPump:
- get_pump_stations()
- get_pump_groups()
- get_pump_station_timeseries()
- get_pump_station_summary()
- get_pump_operation_timeseries()


"""


import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import Point
from typing import List, Dict, Any, Optional, Union
from .HdfUtils import HdfUtils
from .HdfBase import HdfBase
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfPump:
    """
    A class for handling pump station related data from HEC-RAS HDF files.

    This class provides static methods to extract and process pump station data, including:
    - Pump station locations and attributes
    - Pump group configurations and efficiency curves
    - Time series results for pump operations
    - Summary statistics for pump stations

    All methods are static and designed to work with HEC-RAS HDF files containing pump data.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_stations(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Extract pump station data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing pump station data with columns:
                - geometry: Point geometry of pump station location
                - station_id: Unique identifier for each pump station
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump station datasets are not found in the HDF file.
            Exception: If there are errors processing the pump station data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump station data
                attributes = hdf['/Geometry/Pump Stations/Attributes'][()]
                points = hdf['/Geometry/Pump Stations/Points'][()]

                # Create geometries
                geometries = [Point(x, y) for x, y in points]

                # Create GeoDataFrame
                gdf = gpd.GeoDataFrame(geometry=geometries)
                gdf['station_id'] = range(len(gdf))

                # Add attributes and decode byte strings
                attr_df = pd.DataFrame(attributes)
                string_columns = attr_df.select_dtypes([object]).columns
                for col in string_columns:
                    attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                
                for col in attr_df.columns:
                    gdf[col] = attr_df[col]

                # Set CRS if available
                crs = HdfBase.get_projection(hdf_path)
                if crs:
                    gdf.set_crs(crs, inplace=True)

                return gdf

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_groups(hdf_path: Path) -> pd.DataFrame:
        """
        Extract pump group data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump group data with columns:
                - efficiency_curve_start: Starting index of efficiency curve data
                - efficiency_curve_count: Number of points in efficiency curve
                - efficiency_curve: List of efficiency curve values
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump group datasets are not found in the HDF file.
            Exception: If there are errors processing the pump group data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump group data
                attributes = hdf['/Geometry/Pump Stations/Pump Groups/Attributes'][()]
                efficiency_curves_info = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Info'][()]
                efficiency_curves_values = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Values'][()]

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(attributes)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                # Add efficiency curve data
                df['efficiency_curve_start'] = efficiency_curves_info[:, 0]
                df['efficiency_curve_count'] = efficiency_curves_info[:, 1]

                # Process efficiency curves
                def get_efficiency_curve:
    """Docs only, see 'get_efficiency_curve.py' for full function code"""
            raise
==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsMesh.py
==================================================
"""
Class: HdfResultsMesh

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All methods in this class are static and designed to be used without instantiation.

Public Functions:
- get_mesh_summary(): Get summary output data for a variable 
- get_mesh_timeseries(): Get timeseries output for a mesh and variable  
- get_mesh_faces_timeseries(): Get timeseries for all face-based variables
- get_mesh_cells_timeseries(): Get timeseries for mesh cells
- get_mesh_last_iter(): Get last iteration count for cells
- get_mesh_max_ws(): Get maximum water surface elevation at each cell   
- get_mesh_min_ws(): Get minimum water surface elevation at each cell
- get_mesh_max_face_v(): Get maximum face velocity at each face
- get_mesh_min_face_v(): Get minimum face velocity at each face
- get_mesh_max_ws_err(): Get maximum water surface error at each cell
- get_mesh_max_iter(): Get maximum iteration count at each cell

Private Functions:
- _get_mesh_timeseries_output_path(): Get HDF path for timeseries output  #REDUNDANT??
- _get_mesh_cells_timeseries_output(): Internal handler for cell timeseries   #REDUNDANT??
- _get_mesh_timeseries_output(): Internal handler for mesh timeseries       # FACES?? 
- _get_mesh_timeseries_output_values_units(): Get values and units for timeseries
- _get_available_meshes(): Get list of available meshes in HDF            #USE HDFBASE OR HDFUTIL
- get_mesh_summary_output(): Internal handler for summary output        
- get_mesh_summary_output_group(): Get HDF group for summary output         #REDUNDANT??  Include in Above

The class works with HEC-RAS version 6.0+ plan HDF files and uses HdfBase and 
HdfUtils for common operations. Methods use @log_call decorator for logging and 
@standardize_input decorator to handle different input types.






REVISIONS MADE:

Use get_ prefix for functions that return data.  
BUT, we will never set results data, so we should use get_ for results data.

Renamed functions:
- mesh_summary_output() to get_mesh_summary()
- mesh_timeseries_output() to get_mesh_timeseries()
- mesh_faces_timeseries_output() to get_mesh_faces_timeseries()
- mesh_cells_timeseries_output() to get_mesh_cells_timeseries()
- mesh_last_iter() to get_mesh_last_iter()
- mesh_max_ws() to get_mesh_max_ws()







"""

import numpy as np
import pandas as pd
import xarray as xr
from pathlib import Path
import h5py
from typing import Union, List, Optional, Dict, Any, Tuple
from .HdfMesh import HdfMesh
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import log_call, standardize_input
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfResultsMesh:
    """
    Handles mesh-related results from HEC-RAS HDF files.

    Provides methods to extract and analyze:
    - Mesh summary outputs
    - Timeseries data
    - Water surface elevations
    - Velocities
    - Error metrics

    Works with HEC-RAS 6.0+ plan HDF files.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary(hdf_path: Path, var: str, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, var, round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_summary: {str(e)}")
            logger.error(f"Variable: {var}")
            raise ValueError(f"Failed to get summary output: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_timeseries(hdf_path: Path, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        with h5py.File(hdf_path, 'r') as hdf_file:
            return HdfResultsMesh._get_mesh_timeseries_output(hdf_file, mesh_name, var, truncate)

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_faces_timeseries(hdf_path: Path, mesh_name: str) -> xr.Dataset:
        """
        Get timeseries output for all face-based variables of a specific mesh.

        Args:
            hdf_path (Path): Path to the HDF file.
            mesh_name (str): Name of the mesh.

        Returns:
            xr.Dataset: Dataset containing the timeseries output for all face-based variables.
        """
        face_vars = ["Face Velocity", "Face Flow"]
        datasets = []
        
        for var in face_vars:
            try:
                da = HdfResultsMesh.get_mesh_timeseries(hdf_path, mesh_name, var)
                # Assign the variable name as the DataArray name
                da.name = var.lower().replace(' ', '_')
                datasets.append(da)
            except Exception as e:
                logger.warning(f"Failed to process {var} for mesh {mesh_name}: {str(e)}")
        
        if not datasets:
            logger.error(f"No valid data found for mesh {mesh_name}")
            return xr.Dataset()
        
        try:
            return xr.merge(datasets)
        except Exception as e:
            logger.error(f"Failed to merge datasets: {str(e)}")
            return xr.Dataset()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cells_timeseries(hdf_path: Path, mesh_names: Optional[Union[str, List[str]]] = None, var: Optional[str] = None, truncate: bool = False, ras_object: Optional[Any] = None) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output.

        Args:
            hdf_path (Path): Path to HDF file
            mesh_names (str|List[str], optional): Mesh name(s). If None, processes all meshes
            var (str, optional): Variable name. If None, retrieves all variables
            truncate (bool): Remove trailing zeros if True
            ras_object (Any, optional): RAS object if available

        Returns:
            Dict[str, xr.Dataset]: Dictionary mapping mesh names to datasets containing:
                - Time-indexed variables
                - Cell/face IDs
                - Variable metadata
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh._get_mesh_cells_timeseries_output(hdf_file, mesh_names, var, truncate)
        except Exception as e:
            logger.error(f"Error in get_mesh_cells_timeseries: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_last_iter(hdf_path: Path) -> pd.DataFrame:
        """
        Get last iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing last iteration counts.
        """
        return HdfResultsMesh.get_mesh_summary_output(hdf_path, "Cell Last Iteration")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum water surface elevations.

        Raises:
            ValueError: If there's an error processing the maximum water surface data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface: {str(e)}")
        




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_ws(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get minimum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing minimum water surface elevations.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_ws: {str(e)}")
            raise ValueError(f"Failed to get minimum water surface: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum face velocity for each mesh face.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum face velocities.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_face_v: {str(e)}")
            raise ValueError(f"Failed to get maximum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get minimum face velocity for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing minimum face velocities.

        Raises:
            ValueError: If there's an error processing the minimum face velocity data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_face_v: {str(e)}")
            raise ValueError(f"Failed to get minimum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws_err(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum water surface error for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum water surface errors.

        Raises:
            ValueError: If there's an error processing the maximum water surface error data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Maximum Water Surface Error", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws_err: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface error: {str(e)}")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_iter(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum iteration counts with face geometry.

        Raises:
            ValueError: If there's an error processing the maximum iteration data.
        """
        """
        Get maximum iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum iteration counts with columns:
                - mesh_name: Name of the mesh
                - cell_id: ID of the cell
                - cell_last_iteration: Maximum number of iterations
                - cell_last_iteration_time: Time when max iterations occurred
                - geometry: Point geometry representing cell center
                
        Raises:
            ValueError: If there's an error processing the maximum iteration data.
            
        Note: The Maximum Iteration is labeled as "Cell Last Iteration" in the HDF file 
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Last Iteration", round_to)
        except Exception as e:
            logger.error(f"Error in mesh_max_iter: {str(e)}")
            raise ValueError(f"Failed to get maximum iteration count: {str(e)}")
        
        


    @staticmethod
    def _get_mesh_timeseries_output_path(mesh_name: str, var_name: str) -> str:
        """
        Get the HDF path for mesh timeseries output.

        Args:
            mesh_name (str): Name of the mesh.
            var_name (str): Name of the variable.

        Returns:
            str: The HDF path for the specified mesh and variable.
        """
        return f"Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas/{mesh_name}/{var_name}"


    @staticmethod
    def _get_mesh_cells_timeseries_output(hdf_file: h5py.File, 
                                         mesh_names: Optional[Union[str, List[str]]] = None,
                                         var: Optional[str] = None, 
                                         truncate: bool = False) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output for specified meshes and variables.
        
        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_names (Optional[Union[str, List[str]]]): Name(s) of the mesh(es). If None, processes all available meshes.
            var (Optional[str]): Name of the variable to retrieve. If None, retrieves all variables.
            truncate (bool): If True, truncates the output to remove trailing zeros.

        Returns:
            Dict[str, xr.Dataset]: A dictionary of xarray Datasets, one for each mesh, containing the mesh cells timeseries output.

        Raises:
            ValueError: If there's an error processing the timeseries output data.
        """
        TIME_SERIES_OUTPUT_VARS = {
            "cell": [
                "Water Surface", "Depth", "Velocity", "Velocity X", "Velocity Y",
                "Froude Number", "Courant Number", "Shear Stress", "Bed Elevation",
                "Precipitation Rate", "Infiltration Rate", "Evaporation Rate",
                "Percolation Rate", "Groundwater Elevation", "Groundwater Depth",
                "Groundwater Flow", "Groundwater Velocity", "Groundwater Velocity X",
                "Groundwater Velocity Y"
            ],
            "face": [
                "Face Velocity", "Face Flow", "Face Water Surface", "Face Courant",
                "Face Cumulative Volume", "Face Eddy Viscosity", "Face Flow Period Average",
                "Face Friction Term", "Face Pressure Gradient Term", "Face Shear Stress",
                "Face Tangential Velocity"
            ]
        }

        try:
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            time_stamps = HdfBase.get_unsteady_timestamps(hdf_file)

            if mesh_names is None:
                mesh_names = HdfResultsMesh._get_available_meshes(hdf_file)
            elif isinstance(mesh_names, str):
                mesh_names = [mesh_names]

            if var:
                variables = [var]
            else:
                variables = TIME_SERIES_OUTPUT_VARS["cell"] + TIME_SERIES_OUTPUT_VARS["face"]

            datasets = {}
            for mesh_name in mesh_names:
                data_vars = {}
                for variable in variables:
                    try:
                        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, variable)
                        dataset = hdf_file[path]
                        values = dataset[:]
                        units = dataset.attrs.get("Units", "").decode("utf-8")

                        if truncate:
                            last_nonzero = np.max(np.nonzero(values)[1]) + 1 if values.size > 0 else 0
                            values = values[:, :last_nonzero]
                            truncated_time_stamps = time_stamps[:last_nonzero]
                        else:
                            truncated_time_stamps = time_stamps

                        if values.shape[0] != len(truncated_time_stamps):
                            logger.warning(f"Mismatch between time steps ({len(truncated_time_stamps)}) and data shape ({values.shape}) for variable {variable}")
                            continue

                        # Determine if this is a face-based or cell-based variable
                        id_dim = "face_id" if any(face_var in variable for face_var in TIME_SERIES_OUTPUT_VARS["face"]) else "cell_id"

                        data_vars[variable] = xr.DataArray(
                            data=values,
                            dims=['time', id_dim],
                            coords={'time': truncated_time_stamps, id_dim: np.arange(values.shape[1])},
                            attrs={'units': units}
                        )
                    except KeyError:
                        logger.warning(f"Variable '{variable}' not found in the HDF file for mesh '{mesh_name}'. Skipping.")
                    except Exception as e:
                        logger.error(f"Error processing variable '{variable}' for mesh '{mesh_name}': {str(e)}")

                if data_vars:
                    datasets[mesh_name] = xr.Dataset(
                        data_vars=data_vars,
                        attrs={'mesh_name': mesh_name, 'start_time': start_time}
                    )
                else:
                    logger.warning(f"No valid data variables found for mesh '{mesh_name}'")

            return datasets
        except Exception as e:
            logger.error(f"Error in _mesh_cells_timeseries_output: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")



    @staticmethod
    def _get_mesh_timeseries_output(hdf_file: h5py.File, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.
            truncate (bool): Whether to truncate the output to remove trailing zeros (default True).

        Returns:
            xr.DataArray: DataArray containing the timeseries output.

        Raises:
            ValueError: If the specified path is not found in the HDF file or if there's an error processing the data.
        """
        try:
            path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
            
            if path not in hdf_file:
                raise ValueError(f"Path {path} not found in HDF file")

            dataset = hdf_file[path]
            values = dataset[:]
            units = dataset.attrs.get("Units", "").decode("utf-8")
            
            # Get start time and timesteps
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            # Updated to use the new function name from HdfUtils
            timesteps = HdfUtils.convert_timesteps_to_datetimes(
                np.array(hdf_file["Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time"][:]),
                start_time
            )

            if truncate:
                non_zero = np.nonzero(values)[0]
                if len(non_zero) > 0:
                    start, end = non_zero[0], non_zero[-1] + 1
                    values = values[start:end]
                    timesteps = timesteps[start:end]

            # Determine if this is a face-based or cell-based variable
            id_dim = "face_id" if "Face" in var else "cell_id"
            dims = ["time", id_dim] if values.ndim == 2 else ["time"]
            coords = {"time": timesteps}
            if values.ndim == 2:
                coords[id_dim] = np.arange(values.shape[1])

            return xr.DataArray(
                values,
                coords=coords,
                dims=dims,
                attrs={"units": units, "mesh_name": mesh_name, "variable": var},
            )
        except Exception as e:
            logger.error(f"Error in get_mesh_timeseries_output: {str(e)}")
            raise ValueError(f"Failed to get timeseries output: {str(e)}")


    @staticmethod
    def _get_mesh_timeseries_output_values_units(hdf_file: h5py.File, mesh_name: str, var: str) -> Tuple[np.ndarray, str]:
        """
        Get the mesh timeseries output values and units for a specific variable from the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.

        Returns:
            Tuple[np.ndarray, str]: A tuple containing the output values and units.
        """
        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
        group = hdf_file[path]
        values = group[:]
        units = group.attrs.get("Units")
        if units is not None:
            units = units.decode("utf-8")
        return values, units


    @staticmethod
    def _get_available_meshes(hdf_file: h5py.File) -> List[str]:
        """
        Get the names of all available meshes in the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.

        Returns:
            List[str]: A list of mesh names.
        """
        return HdfMesh.get_mesh_area_names(hdf_file)
    
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary_output(hdf_file: h5py.File, var: str, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get the summary output data for a given variable from the HDF file.

        Parameters
        ----------
        hdf_file : h5py.File
            Open HDF file object.
        var : str
            The summary output variable to retrieve.
        round_to : str, optional
            The time unit to round the datetimes to. Default is "100ms".

        Returns
        -------
        pd.DataFrame
            A DataFrame containing the summary output data with attributes as metadata.

        Raises
        ------
        ValueError
            If the HDF file cannot be opened or read, or if the requested data is not found.
        """
        try:
            dfs = []
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            
            logger.info(f"Processing summary output for variable: {var}")
            d2_flow_areas = hdf_file.get("Geometry/2D Flow Areas/Attributes")
            if d2_flow_areas is None:
                return pd.DataFrame()

            for d2_flow_area in d2_flow_areas[:]:
                mesh_name = HdfUtils.convert_ras_string(d2_flow_area[0])
                cell_count = d2_flow_area[-1]
                logger.debug(f"Processing mesh: {mesh_name} with {cell_count} cells")
                group = HdfResultsMesh.get_mesh_summary_output_group(hdf_file, mesh_name, var)
                
                data = group[:]
                logger.debug(f"Data shape for {var} in {mesh_name}: {data.shape}")
                logger.debug(f"Data type: {data.dtype}")
                logger.debug(f"Attributes: {dict(group.attrs)}")
                
                if data.ndim == 2 and data.shape[0] == 2:
                    # Handle 2D datasets (e.g. Maximum Water Surface)
                    row_variables = group.attrs.get('Row Variables', [b'Value', b'Time'])
                    row_variables = [v.decode('utf-8').strip() for v in row_variables]
                    
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * data.shape[1],
                        "cell_id" if "Face" not in var else "face_id": range(data.shape[1]),
                        f"{var.lower().replace(' ', '_')}": data[0, :],
                        f"{var.lower().replace(' ', '_')}_time": HdfUtils.convert_timesteps_to_datetimes(
                            data[1, :], start_time, time_unit="days", round_to=round_to
                        )
                    })
                    
                elif data.ndim == 1:
                    # Handle 1D datasets (e.g. Cell Last Iteration)
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * len(data),
                        "cell_id" if "Face" not in var else "face_id": range(len(data)),
                        var.lower().replace(' ', '_'): data
                    })
                    
                else:
                    raise ValueError(f"Unexpected data shape for {var} in {mesh_name}. "
                                  f"Got shape {data.shape}")
                
                # Add geometry based on variable type
                if "Face" in var:
                    face_df = HdfMesh.get_mesh_cell_faces(hdf_file)
                    if not face_df.empty:
                        df = df.merge(face_df[['mesh_name', 'face_id', 'geometry']], 
                                    on=['mesh_name', 'face_id'], 
                                    how='left')
                else:
                    cell_df = HdfMesh.get_mesh_cell_points(hdf_file)
                    if not cell_df.empty:
                        df = df.merge(cell_df[['mesh_name', 'cell_id', 'geometry']], 
                                    on=['mesh_name', 'cell_id'], 
                                    how='left')
                
                # Add group attributes as metadata
                df.attrs['mesh_name'] = mesh_name
                for attr_name, attr_value in group.attrs.items():
                    if isinstance(attr_value, bytes):
                        attr_value = attr_value.decode('utf-8')
                    elif isinstance(attr_value, np.ndarray):
                        attr_value = attr_value.tolist()
                    df.attrs[attr_name] = attr_value
                
                dfs.append(df)
            
            if not dfs:
                return pd.DataFrame()
                
            result = pd.concat(dfs, ignore_index=True)
            
            # Combine attributes from all meshes
            combined_attrs = {}
            for df in dfs:
                for key, value in df.attrs.items():
                    if key not in combined_attrs:
                        combined_attrs[key] = value
                    elif combined_attrs[key] != value:
                        combined_attrs[key] = f"Multiple values: {combined_attrs[key]}, {value}"
            
            result.attrs.update(combined_attrs)
            
            logger.info(f"Processed {len(result)} rows of summary output data")
            return result
        
        except Exception as e:
            logger.error(f"Error processing summary output data: {e}")
            raise ValueError(f"Error processing summary output data: {e}")
        

    @staticmethod
    def get_mesh_summary_output_group(hdf_file: h5py.File, mesh_name: str, var: str) -> Union[h5py.Group, h5py.Dataset]:
        """
        Return the HDF group for a given mesh and summary output variable.

        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Name of the summary output variable.

        Returns:
            Union[h5py.Group, h5py.Dataset]: The HDF group or dataset for the specified mesh and variable.

        Raises:
            ValueError: If the specified group or dataset is not found in the HDF file.
        """
        output_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/{var}"
        output_item = hdf_file.get(output_path)
        if output_item is None:
            raise ValueError(f"Could not find HDF group or dataset at path '{output_path}'")
        return output_item


==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsPlan.py
==================================================
"""
HdfResultsPlan: A module for extracting and analyzing HEC-RAS plan HDF file results.

Attribution:
    Substantial code sourced/derived from https://github.com/fema-ffrd/rashdf
    Copyright (c) 2024 fema-ffrd, MIT license

Description:
    Provides static methods for extracting unsteady flow results, volume accounting,
    and reference data from HEC-RAS plan HDF files.

Available Functions:
    - get_unsteady_info: Extract unsteady attributes
    - get_unsteady_summary: Extract unsteady summary data
    - get_volume_accounting: Extract volume accounting data
    - get_runtime_data: Extract runtime and compute time data

Note:
    All methods are static and designed to be used without class instantiation.
"""

from typing import Dict, List, Union, Optional
from pathlib import Path
import h5py
import pandas as pd
import xarray as xr
from .Decorators import standardize_input, log_call
from .HdfUtils import HdfUtils
from .HdfResultsXsec import HdfResultsXsec
from .LoggingConfig import get_logger
import numpy as np
from datetime import datetime

logger = get_logger(__name__)


class HdfResultsPlan:
    """
    Handles extraction of results data from HEC-RAS plan HDF files.

    This class provides static methods for accessing and analyzing:
        - Unsteady flow results
        - Volume accounting data
        - Runtime statistics
        - Reference line/point time series outputs

    All methods use:
        - @standardize_input decorator for consistent file path handling
        - @log_call decorator for operation logging
        - HdfUtils class for common HDF operations

    Note:
        No instantiation required - all methods are static.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_info(hdf_path: Path) -> pd.DataFrame:
        """
        Get unsteady attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: A DataFrame containing the unsteady attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady" group is not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady" not in hdf_file:
                    raise KeyError("Results/Unsteady group not found in the HDF file.")
                
                # Create dictionary from attributes
                attrs_dict = dict(hdf_file["Results/Unsteady"].attrs)
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Get results unsteady summary attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: A DataFrame containing the results unsteady summary attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady/Summary" group is not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary" not in hdf_file:
                    raise KeyError("Results/Unsteady/Summary group not found in the HDF file.")
                
                # Create dictionary from attributes
                attrs_dict = dict(hdf_file["Results/Unsteady/Summary"].attrs)
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady summary attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_volume_accounting(hdf_path: Path) -> pd.DataFrame:
        """
        Get volume accounting attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: A DataFrame containing the volume accounting attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady/Summary/Volume Accounting" group is not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary/Volume Accounting" not in hdf_file:
                    raise KeyError("Results/Unsteady/Summary/Volume Accounting group not found in the HDF file.")
                
                # Get attributes and create dictionary
                attrs_dict = dict(hdf_file["Results/Unsteady/Summary/Volume Accounting"].attrs)
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading volume accounting attributes: {str(e)}")

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_runtime_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Extract detailed runtime and computational performance metrics from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file

        Returns:
            Optional[pd.DataFrame]: DataFrame containing:
                - Plan identification (name, file)
                - Simulation timing (start, end, duration)
                - Process-specific compute times
                - Performance metrics (simulation speeds)
                Returns None if required data cannot be extracted

        Notes:
            - Times are reported in multiple units (ms, s, hours)
            - Compute speeds are calculated as simulation-time/compute-time ratios
            - Process times include: geometry, preprocessing, event conditions, 
            and unsteady flow computations

        Example:
            >>> runtime_stats = HdfResultsPlan.get_runtime_data('path/to/plan.hdf')
            >>> if runtime_stats is not None:
            >>>     print(f"Total compute time: {runtime_stats['Complete Process (hr)'][0]:.2f} hours")
        """
        if hdf_path is None:
            logger.error(f"Could not find HDF file for input")
            return None

        with h5py.File(hdf_path, 'r') as hdf_file:
            logger.info(f"Extracting Plan Information from: {Path(hdf_file.filename).name}")
            plan_info = hdf_file.get('/Plan Data/Plan Information')
            if plan_info is None:
                logger.warning("Group '/Plan Data/Plan Information' not found.")
                return None

            plan_name = plan_info.attrs.get('Plan Name', 'Unknown')
            plan_name = plan_name.decode('utf-8') if isinstance(plan_name, bytes) else plan_name
            logger.info(f"Plan Name: {plan_name}")

            start_time_str = plan_info.attrs.get('Simulation Start Time', 'Unknown')
            end_time_str = plan_info.attrs.get('Simulation End Time', 'Unknown')
            start_time_str = start_time_str.decode('utf-8') if isinstance(start_time_str, bytes) else start_time_str
            end_time_str = end_time_str.decode('utf-8') if isinstance(end_time_str, bytes) else end_time_str

            start_time = datetime.strptime(start_time_str, "%d%b%Y %H:%M:%S")
            end_time = datetime.strptime(end_time_str, "%d%b%Y %H:%M:%S")
            simulation_duration = end_time - start_time
            simulation_hours = simulation_duration.total_seconds() / 3600

            logger.info(f"Simulation Start Time: {start_time_str}")
            logger.info(f"Simulation End Time: {end_time_str}")
            logger.info(f"Simulation Duration (hours): {simulation_hours}")

            compute_processes = hdf_file.get('/Results/Summary/Compute Processes')
            if compute_processes is None:
                logger.warning("Dataset '/Results/Summary/Compute Processes' not found.")
                return None

            process_names = [name.decode('utf-8') for name in compute_processes['Process'][:]]
            filenames = [filename.decode('utf-8') for filename in compute_processes['Filename'][:]]
            completion_times = compute_processes['Compute Time (ms)'][:]

            compute_processes_df = pd.DataFrame({
                'Process': process_names,
                'Filename': filenames,
                'Compute Time (ms)': completion_times,
                'Compute Time (s)': completion_times / 1000,
                'Compute Time (hours)': completion_times / (1000 * 3600)
            })

            logger.debug("Compute processes DataFrame:")
            logger.debug(compute_processes_df)

            compute_processes_summary = {
                'Plan Name': [plan_name],
                'File Name': [Path(hdf_file.filename).name],
                'Simulation Start Time': [start_time_str],
                'Simulation End Time': [end_time_str],
                'Simulation Duration (s)': [simulation_duration.total_seconds()],
                'Simulation Time (hr)': [simulation_hours],
                'Completing Geometry (hr)': [compute_processes_df[compute_processes_df['Process'] == 'Completing Geometry']['Compute Time (hours)'].values[0] if 'Completing Geometry' in compute_processes_df['Process'].values else 'N/A'],
                'Preprocessing Geometry (hr)': [compute_processes_df[compute_processes_df['Process'] == 'Preprocessing Geometry']['Compute Time (hours)'].values[0] if 'Preprocessing Geometry' in compute_processes_df['Process'].values else 'N/A'],
                'Completing Event Conditions (hr)': [compute_processes_df[compute_processes_df['Process'] == 'Completing Event Conditions']['Compute Time (hours)'].values[0] if 'Completing Event Conditions' in compute_processes_df['Process'].values else 'N/A'],
                'Unsteady Flow Computations (hr)': [compute_processes_df[compute_processes_df['Process'] == 'Unsteady Flow Computations']['Compute Time (hours)'].values[0] if 'Unsteady Flow Computations' in compute_processes_df['Process'].values else 'N/A'],
                'Complete Process (hr)': [compute_processes_df['Compute Time (hours)'].sum()]
            }

            compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = [simulation_hours / compute_processes_summary['Unsteady Flow Computations (hr)'][0] if compute_processes_summary['Unsteady Flow Computations (hr)'][0] != 'N/A' else 'N/A']
            compute_processes_summary['Complete Process Speed (hr/hr)'] = [simulation_hours / compute_processes_summary['Complete Process (hr)'][0] if compute_processes_summary['Complete Process (hr)'][0] != 'N/A' else 'N/A']

            compute_summary_df = pd.DataFrame(compute_processes_summary)
            logger.debug("Compute summary DataFrame:")
            logger.debug(compute_summary_df)

            return compute_summary_df

        



==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsPlot.py
==================================================
"""
Class: HdfResultsPlot

A collection of static methods for visualizing HEC-RAS results data from HDF files using matplotlib.

Public Functions:
    plot_results_mesh_variable(variable_df, variable_name, colormap='viridis', point_size=10):
        Generic plotting function for any mesh variable with customizable styling.
        
    plot_results_max_wsel(max_ws_df):
        Visualizes the maximum water surface elevation distribution across mesh cells.
        
    plot_results_max_wsel_time(max_ws_df):
        Displays the timing of maximum water surface elevation for each cell,
        including statistics about the temporal distribution.

Requirements:
    - matplotlib
    - pandas
    - geopandas (for geometry handling)

Input DataFrames must contain:
    - 'geometry' column with Point objects containing x,y coordinates
    - Variable data columns as specified in individual function docstrings
"""

import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict
from .Decorators import log_call
from .HdfMesh import HdfMesh

class HdfResultsPlot:
    """
    A class containing static methods for plotting HEC-RAS results data.
    
    This class provides visualization methods for various types of HEC-RAS results,
    including maximum water surface elevations and timing information.
    """

    @staticmethod
    @log_call
    def plot_results_max_wsel(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the maximum water surface elevation per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface elevations.
        """
        # Extract x and y coordinates from the geometry column
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            print("Error: 'x' or 'y' columns not found in the merged dataframe.")
            print("Available columns:", max_ws_df.columns.tolist())
            return

        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=max_ws_df['maximum_water_surface'], 
                           cmap='viridis', s=10)

        ax.set_title('Max Water Surface per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label='Max Water Surface (ft)')

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

    @staticmethod
    @log_call
    def plot_results_max_wsel_time(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the time of the maximum water surface elevation (WSEL) per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface timing information.
        """
        # Convert datetime strings using the renamed utility function
        max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])
        
        # Extract coordinates
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            raise ValueError("x and y coordinates are missing from the DataFrame. Make sure the 'geometry' column exists and contains valid coordinate data.")

        fig, ax = plt.subplots(figsize=(12, 8))

        min_time = max_ws_df['max_wsel_time'].min()
        color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600

        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=color_values, cmap='viridis', s=10)

        ax.set_title('Time of Maximum Water Surface Elevation per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')

        cbar = plt.colorbar(scatter)
        cbar.set_label('Hours since simulation start')
        cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))
        cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

        # Print timing information
        print(f"\nSimulation Start Time: {min_time}")
        print(f"Time Range: {color_values.max():.1f} hours")
        print("\nTiming Statistics (hours since start):")
        print(color_values.describe()) 

    @staticmethod
    @log_call
    def plot_results_mesh_variable(variable_df: pd.DataFrame, variable_name: str, colormap: str = 'viridis', point_size: int = 10) -> None:
        """
        Plot any mesh variable with consistent styling.
        
        Args:
            variable_df (pd.DataFrame): DataFrame containing the variable data
            variable_name (str): Name of the variable (for labels)
            colormap (str): Matplotlib colormap to use. Default: 'viridis'
            point_size (int): Size of the scatter points. Default: 10

        Returns:
            None

        Raises:
            ImportError: If matplotlib is not installed
            ValueError: If required columns are missing from variable_df
        """
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            logger.error("matplotlib is required for plotting. Please install it with 'pip install matplotlib'")
            raise ImportError("matplotlib is required for plotting")

        # Get cell coordinates if not in variable_df
        if 'geometry' not in variable_df.columns:
            cell_coords = HdfMesh.mesh_cell_points(plan_hdf_path)
            merged_df = pd.merge(variable_df, cell_coords, on=['mesh_name', 'cell_id'])
        else:
            merged_df = variable_df
            
        # Extract coordinates, handling None values
        merged_df = merged_df.dropna(subset=['geometry'])
        merged_df['x'] = merged_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        merged_df['y'] = merged_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)
        
        # Drop any rows with None coordinates
        merged_df = merged_df.dropna(subset=['x', 'y'])
        
        if len(merged_df) == 0:
            logger.error("No valid coordinates found for plotting")
            raise ValueError("No valid coordinates found for plotting")
            
        # Create plot
        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(merged_df['x'], merged_df['y'], 
                           c=merged_df[variable_name], 
                           cmap=colormap, 
                           s=point_size)
        
        # Customize plot
        ax.set_title(f'{variable_name} per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label=variable_name)
        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsXsec.py
==================================================
"""
Class: HdfResultsXsec

Contains methods for extracting 1D results data from HDF files. 
This includes cross section timeseries, structures and reference line/point timeseries as these are all 1D elements.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfResultsXsec:
- get_xsec_timeseries(): Extract cross-section timeseries data including water surface, velocity, and flow
- get_ref_lines_timeseries(): Get timeseries output for reference lines
- get_ref_points_timeseries(): Get timeseries output for reference points

TO BE IMPLEMENTED: 
DSS Hydrograph Extraction for 1D and 2D Structures. 

Planned functions:
- get_bridge_timeseries(): Extract timeseries data for bridge structures
- get_inline_structures_timeseries(): Extract timeseries data for inline structures

Notes:
- All functions use the get_ prefix to indicate they return data
- Results data functions use results_ prefix to indicate they handle results data
- All functions include proper error handling and logging
- Functions return xarray Datasets for efficient handling of multi-dimensional data
"""

from pathlib import Path
from typing import Union, Optional, List, Dict, Tuple

import h5py
import numpy as np
import pandas as pd
import xarray as xr

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfResultsXsec:
    """
    A static class for extracting and processing 1D results data from HEC-RAS HDF files.

    This class provides methods to extract and process unsteady flow simulation results
    for cross-sections, reference lines, and reference points. All methods are static
    and designed to be used without class instantiation.

    The class handles:
    - Cross-section timeseries (water surface, velocity, flow)
    - Reference line timeseries
    - Reference point timeseries

    Dependencies:
        - HdfBase: Core HDF file operations
        - HdfUtils: Utility functions for HDF processing
    """


# Tested functions from AWS webinar where the code was developed
# Need to add examples


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_xsec_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract Water Surface, Velocity Total, Velocity Channel, Flow Lateral, and Flow data from HEC-RAS HDF file.
        Includes Cross Section Only and Cross Section Attributes as coordinates in the xarray.Dataset.
        Also calculates maximum values for key parameters.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Xarray Dataset containing the extracted cross-section results with appropriate coordinates and attributes.
            Includes maximum values for Water Surface, Flow, Channel Velocity, Total Velocity, and Lateral Flow.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Define base paths
                base_output_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections/"
                time_stamp_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
                
                # Extract Cross Section Attributes
                attrs_dataset = hdf_file[f"{base_output_path}Cross Section Attributes"][:]
                rivers = [attr['River'].decode('utf-8').strip() for attr in attrs_dataset]
                reaches = [attr['Reach'].decode('utf-8').strip() for attr in attrs_dataset]
                stations = [attr['Station'].decode('utf-8').strip() for attr in attrs_dataset]
                names = [attr['Name'].decode('utf-8').strip() for attr in attrs_dataset]
                
                # Extract Cross Section Only (Unique Names)
                cross_section_only_dataset = hdf_file[f"{base_output_path}Cross Section Only"][:]
                cross_section_names = [cs.decode('utf-8').strip() for cs in cross_section_only_dataset]
                
                # Extract Time Stamps and convert to datetime
                time_stamps = hdf_file[time_stamp_path][:]
                if any(isinstance(ts, bytes) for ts in time_stamps):
                    time_stamps = [ts.decode('utf-8') for ts in time_stamps]
                # Convert RAS format timestamps to datetime
                times = pd.to_datetime(time_stamps, format='%d%b%Y %H:%M:%S:%f')
                
                # Extract Required Datasets
                water_surface = hdf_file[f"{base_output_path}Water Surface"][:]
                velocity_total = hdf_file[f"{base_output_path}Velocity Total"][:]
                velocity_channel = hdf_file[f"{base_output_path}Velocity Channel"][:]
                flow_lateral = hdf_file[f"{base_output_path}Flow Lateral"][:]
                flow = hdf_file[f"{base_output_path}Flow"][:]
                
                # Calculate maximum values along time axis
                max_water_surface = np.max(water_surface, axis=0)
                max_flow = np.max(flow, axis=0)
                max_velocity_channel = np.max(velocity_channel, axis=0)
                max_velocity_total = np.max(velocity_total, axis=0)
                max_flow_lateral = np.max(flow_lateral, axis=0)
                
                # Create Xarray Dataset
                ds = xr.Dataset(
                    {
                        'Water_Surface': (['time', 'cross_section'], water_surface),
                        'Velocity_Total': (['time', 'cross_section'], velocity_total),
                        'Velocity_Channel': (['time', 'cross_section'], velocity_channel),
                        'Flow_Lateral': (['time', 'cross_section'], flow_lateral),
                        'Flow': (['time', 'cross_section'], flow),
                    },
                    coords={
                        'time': times,
                        'cross_section': cross_section_names,
                        'River': ('cross_section', rivers),
                        'Reach': ('cross_section', reaches),
                        'Station': ('cross_section', stations),
                        'Name': ('cross_section', names),
                        'Maximum_Water_Surface': ('cross_section', max_water_surface),
                        'Maximum_Flow': ('cross_section', max_flow),
                        'Maximum_Channel_Velocity': ('cross_section', max_velocity_channel),
                        'Maximum_Velocity_Total': ('cross_section', max_velocity_total),
                        'Maximum_Flow_Lateral': ('cross_section', max_flow_lateral)
                    },
                    attrs={
                        'description': 'Cross-section results extracted from HEC-RAS HDF file',
                        'source_file': str(hdf_path)
                    }
                )
                
                return ds

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting cross section results: {e}")
            raise



    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_lines_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference lines from HEC-RAS HDF file.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing flow, velocity, and water surface data for reference lines.
            Returns empty dataset if reference line data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="lines")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_points_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference points from HEC-RAS HDF file.

        This method extracts flow, velocity, and water surface elevation data for all
        reference points defined in the model. Reference points are user-defined locations
        where detailed output is desired.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing the following variables for each reference point:
            - Flow [cfs or m³/s]
            - Velocity [ft/s or m/s]
            - Water Surface [ft or m]
            
            The dataset includes coordinates:
            - time: Simulation timesteps
            - refpt_id: Unique identifier for each reference point
            - refpt_name: Name of each reference point
            - mesh_name: Associated 2D mesh area name
            
            Returns empty dataset if reference point data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file

        Examples:
        --------
        >>> ds = HdfResultsXsec.get_ref_points_timeseries("path/to/plan.hdf")
        >>> # Get water surface timeseries for first reference point
        >>> ws = ds['Water Surface'].isel(refpt_id=0)
        >>> # Get all data for a specific reference point by name
        >>> point_data = ds.sel(refpt_name='Point1')
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="points")
    

    @staticmethod
    def _reference_timeseries_output(hdf_file: h5py.File, reftype: str = "lines") -> xr.Dataset:
        """
        Internal method to return timeseries output data for reference lines or points from a HEC-RAS HDF plan file.

        Parameters
        ----------
        hdf_file : h5py.File
            Open HDF file object.
        reftype : str, optional
            The type of reference data to retrieve. Must be either "lines" or "points".
            (default: "lines")

        Returns
        -------
        xr.Dataset
            An xarray Dataset with reference line or point timeseries data.
            Returns an empty Dataset if the reference output data is not found.

        Raises
        ------
        ValueError
            If reftype is not "lines" or "points".
        """
        if reftype == "lines":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Lines"
            abbrev = "refln"
        elif reftype == "points":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Points"
            abbrev = "refpt"
        else:
            raise ValueError('reftype must be either "lines" or "points".')

        try:
            reference_group = hdf_file[output_path]
        except KeyError:
            logger.error(f"Could not find HDF group at path '{output_path}'. "
                         f"The Plan HDF file may not contain reference {reftype[:-1]} output data.")
            return xr.Dataset()

        reference_names = reference_group["Name"][:]
        names = []
        mesh_areas = []
        for s in reference_names:
            name, mesh_area = s.decode("utf-8").split("|")
            names.append(name)
            mesh_areas.append(mesh_area)

        times = HdfBase.get_unsteady_timestamps(hdf_file)

        das = {}
        for var in ["Flow", "Velocity", "Water Surface"]:
            group = reference_group.get(var)
            if group is None:
                continue
            values = group[:]
            units = group.attrs["Units"].decode("utf-8")
            da = xr.DataArray(
                values,
                name=var,
                dims=["time", f"{abbrev}_id"],
                coords={
                    "time": times,
                    f"{abbrev}_id": range(values.shape[1]),
                    f"{abbrev}_name": (f"{abbrev}_id", names),
                    "mesh_name": (f"{abbrev}_id", mesh_areas),
                },
                attrs={"units": units, "hdf_path": f"{output_path}/{var}"},
            )
            das[var] = da
        return xr.Dataset(das)

==================================================

File: c:\GH\ras-commander\ras_commander\HdfStruc.py
==================================================
"""
Class: HdfStruc

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfStruc:
- get_structures()
- get_geom_structures_attrs()
"""
from typing import Dict, Any, List, Union
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point, GeometryCollection
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfBase import HdfBase
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfStruc:
    """
    Handles 2D structure geometry data extraction from HEC-RAS HDF files.

    This class provides static methods for extracting and analyzing structure geometries
    and their attributes from HEC-RAS geometry HDF files. All methods are designed to work
    without class instantiation.

    Notes
    -----
    - 1D Structure data should be accessed via the HdfResultsXsec class
    - All methods use @standardize_input for consistent file handling
    - All methods use @log_call for operation logging
    - Returns GeoDataFrames with both geometric and attribute data
    """
    
    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_structures(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts structure data from a HEC-RAS geometry HDF5 file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF5 file
        datetime_to_str : bool, optional
            If True, converts datetime objects to ISO format strings, by default False

        Returns
        -------
        GeoDataFrame
            Structure data with columns:
            - Structure ID: unique identifier
            - Geometry: LineString of structure centerline
            - Various attribute columns from the HDF file
            - Profile_Data: list of station/elevation dictionaries
            - Bridge coefficient attributes (if present)
            - Table info attributes (if present)

        Notes
        -----
        - Group-level attributes are stored in GeoDataFrame.attrs['group_attributes']
        - Invalid geometries are dropped with warning
        - All byte strings are decoded to UTF-8
        - CRS is preserved from the source file
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                if "Geometry/Structures" not in hdf:
                    logger.info(f"No structures found in: {hdf_path}")
                    return GeoDataFrame()
                
                def get_dataset_df(path: str) -> pd.DataFrame:
                    """
                    Converts an HDF5 dataset to a pandas DataFrame.

                    Parameters
                    ----------
                    path : str
                        Dataset path within the HDF5 file

                    Returns
                    -------
                    pd.DataFrame
                        DataFrame containing the dataset values.
                        - For compound datasets, column names match field names
                        - For simple datasets, generic column names (Value_0, Value_1, etc.)
                        - Empty DataFrame if dataset not found

                    Notes
                    -----
                    Automatically decodes byte strings to UTF-8 with error handling.
                    """
                    if path not in hdf:
                        logger.warning(f"Dataset not found: {path}")
                        return pd.DataFrame()
                    
                    data = hdf[path][()]
                    
                    if data.dtype.names:
                        df = pd.DataFrame(data)
                        # Decode byte strings to UTF-8
                        for col in df.columns:
                            if df[col].dtype.kind in {'S', 'a'}:  # Byte strings
                                df[col] = df[col].str.decode('utf-8', errors='ignore')
                        return df
                    else:
                        # If no named fields, assign generic column names
                        return pd.DataFrame(data, columns=[f'Value_{i}' for i in range(data.shape[1])])

                # Extract relevant datasets
                group_attrs = HdfBase.get_attrs(hdf, "Geometry/Structures")
                struct_attrs = get_dataset_df("Geometry/Structures/Attributes")
                bridge_coef = get_dataset_df("Geometry/Structures/Bridge Coefficient Attributes")
                table_info = get_dataset_df("Geometry/Structures/Table Info")
                profile_data = get_dataset_df("Geometry/Structures/Profile Data")

                # Assign 'Structure ID' based on index (starting from 1)
                struct_attrs.reset_index(drop=True, inplace=True)
                struct_attrs['Structure ID'] = range(1, len(struct_attrs) + 1)
                logger.debug(f"Assigned Structure IDs: {struct_attrs['Structure ID'].tolist()}")

                # Check if 'Structure ID' was successfully assigned
                if 'Structure ID' not in struct_attrs.columns:
                    logger.error("'Structure ID' column could not be assigned to Structures/Attributes.")
                    return GeoDataFrame()

                # Get centerline geometry
                centerline_info = hdf["Geometry/Structures/Centerline Info"][()]
                centerline_points = hdf["Geometry/Structures/Centerline Points"][()]
                
                # Create LineString geometries for each structure
                geoms = []
                for i in range(len(centerline_info)):
                    start_idx = centerline_info[i][0]  # Point Starting Index
                    point_count = centerline_info[i][1]  # Point Count
                    points = centerline_points[start_idx:start_idx + point_count]
                    if len(points) >= 2:
                        geoms.append(LineString(points))
                    else:
                        logger.warning(f"Insufficient points for LineString in structure index {i}.")
                        geoms.append(None)

                # Create base GeoDataFrame with Structures Attributes and geometries
                struct_gdf = GeoDataFrame(
                    struct_attrs,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Drop entries with invalid geometries
                initial_count = len(struct_gdf)
                struct_gdf = struct_gdf.dropna(subset=['geometry']).reset_index(drop=True)
                final_count = len(struct_gdf)
                if final_count < initial_count:
                    logger.warning(f"Dropped {initial_count - final_count} structures due to invalid geometries.")

                # Merge Bridge Coefficient Attributes on 'Structure ID'
                if not bridge_coef.empty and 'Structure ID' in bridge_coef.columns:
                    struct_gdf = struct_gdf.merge(
                        bridge_coef,
                        on='Structure ID',
                        how='left',
                        suffixes=('', '_bridge_coef')
                    )
                    logger.debug("Merged Bridge Coefficient Attributes successfully.")
                else:
                    logger.warning("Bridge Coefficient Attributes missing or 'Structure ID' not present.")

                # Merge Table Info based on the DataFrame index (one-to-one correspondence)
                if not table_info.empty:
                    if len(table_info) != len(struct_gdf):
                        logger.warning("Table Info count does not match Structures count. Skipping merge.")
                    else:
                        struct_gdf = pd.concat([struct_gdf, table_info.reset_index(drop=True)], axis=1)
                        logger.debug("Merged Table Info successfully.")
                else:
                    logger.warning("Table Info dataset is empty or missing.")

                # Process Profile Data based on Table Info
                if not profile_data.empty and not table_info.empty:
                    # Assuming 'Centerline Profile (Index)' and 'Centerline Profile (Count)' are in 'Table Info'
                    if ('Centerline Profile (Index)' in table_info.columns and
                        'Centerline Profile (Count)' in table_info.columns):
                        struct_gdf['Profile_Data'] = struct_gdf.apply(
                            lambda row: [
                                {'Station': float(profile_data.iloc[i, 0]),
                                 'Elevation': float(profile_data.iloc[i, 1])}
                                for i in range(
                                    int(row['Centerline Profile (Index)']),
                                    int(row['Centerline Profile (Index)']) + int(row['Centerline Profile (Count)'])
                                )
                            ],
                            axis=1
                        )
                        logger.debug("Processed Profile Data successfully.")
                    else:
                        logger.warning("Required columns for Profile Data not found in Table Info.")
                else:
                    logger.warning("Profile Data dataset is empty or Table Info is missing.")

                # Convert datetime columns to string if requested
                if datetime_to_str:
                    datetime_cols = struct_gdf.select_dtypes(include=['datetime64']).columns
                    for col in datetime_cols:
                        struct_gdf[col] = struct_gdf[col].dt.isoformat()
                        logger.debug(f"Converted datetime column '{col}' to string.")

                # Ensure all byte strings are decoded (if any remain)
                for col in struct_gdf.columns:
                    if struct_gdf[col].dtype == object:
                        struct_gdf[col] = struct_gdf[col].apply(
                            lambda x: x.decode('utf-8', errors='ignore') if isinstance(x, bytes) else x
                        )

                # Final GeoDataFrame
                logger.info("Successfully extracted structures GeoDataFrame.")
                
                # Add group attributes to the GeoDataFrame's attrs['group_attributes']
                struct_gdf.attrs['group_attributes'] = group_attrs
                
                logger.info("Successfully extracted structures GeoDataFrame with attributes.")
                
                return struct_gdf

        except Exception as e:
            logger.error(f"Error reading structures from {hdf_path}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_geom_structures_attrs(hdf_path: Path) -> Dict[str, Any]:
        """
        Extracts structure attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        Dict[str, Any]
            Dictionary of structure attributes from the Geometry/Structures group.
            Returns empty dict if no structures are found.

        Notes
        -----
        Attributes are extracted from the HDF5 group 'Geometry/Structures'.
        All byte strings in attributes are automatically decoded to UTF-8.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/Structures" not in hdf_file:
                    logger.info(f"No structures found in the geometry file: {hdf_path}")
                    return {}
                return HdfUtils.hdf5_attrs_to_dict(hdf_file["Geometry/Structures"].attrs)
        except Exception as e:
            logger.error(f"Error reading geometry structures attributes: {str(e)}")
            return {}

==================================================

File: c:\GH\ras-commander\ras_commander\HdfUtils.py
==================================================
"""
HdfUtils Class
-------------

A utility class providing static methods for working with HEC-RAS HDF files.

Attribution:
    A substantial amount of code in this file is sourced or derived from the 
    https://github.com/fema-ffrd/rashdf library, released under MIT license 
    and Copyright (c) 2024 fema-ffrd. The file has been forked and modified 
    for use in RAS Commander.

Key Features:
- HDF file data conversion and parsing
- DateTime handling for RAS-specific formats
- Spatial operations using KDTree
- HDF attribute management

Main Method Categories:

1. Data Conversion
    - convert_ras_string: Convert RAS HDF strings to Python objects
    - convert_ras_hdf_value: Convert general HDF values to Python objects
    - convert_df_datetimes_to_str: Convert DataFrame datetime columns to strings
    - convert_hdf5_attrs_to_dict: Convert HDF5 attributes to dictionary
    - convert_timesteps_to_datetimes: Convert timesteps to datetime objects

2. Spatial Operations
    - perform_kdtree_query: KDTree search between datasets
    - find_nearest_neighbors: Find nearest neighbors within dataset

3. DateTime Parsing
    - parse_ras_datetime: Parse standard RAS datetime format (ddMMMYYYY HH:MM:SS)
    - parse_ras_window_datetime: Parse simulation window datetime (ddMMMYYYY HHMM)
    - parse_duration: Parse duration strings (HH:MM:SS)
    - parse_ras_datetime_ms: Parse datetime with milliseconds
    - parse_run_time_window: Parse time window strings

Usage Notes:
- All methods are static and can be called without class instantiation
- Methods handle both raw HDF data and converted Python objects
- Includes comprehensive error handling for RAS-specific data formats
- Supports various RAS datetime formats and conversions
"""
import logging
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Union, Optional, Dict, List, Tuple, Any
from scipy.spatial import KDTree
import re
from shapely.geometry import LineString  # Import LineString to avoid NameError

from .Decorators import standardize_input, log_call 
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfUtils:
    """
    Utility class for working with HEC-RAS HDF files.

    This class provides general utility functions for HDF file operations,
    including attribute extraction, data conversion, and common HDF queries.
    It also includes spatial operations and helper methods for working with
    HEC-RAS specific data structures.

    Note:
    - Use this class for general HDF utility functions that are not specific to plan or geometry files.
    - All methods in this class are static and can be called without instantiating the class.
    """




# RENAME TO convert_ras_string and make public

    @staticmethod
    def convert_ras_string(value: Union[str, bytes]) -> Union[bool, datetime, List[datetime], timedelta, str]:
        """
        Convert a string value from an HEC-RAS HDF file into a Python object.

        Args:
            value (Union[str, bytes]): The value to convert.

        Returns:
            Union[bool, datetime, List[datetime], timedelta, str]: The converted value.
        """
        if isinstance(value, bytes):
            s = value.decode("utf-8")
        else:
            s = value

        if s == "True":
            return True
        elif s == "False":
            return False
        
        ras_datetime_format1_re = r"\d{2}\w{3}\d{4} \d{2}:\d{2}:\d{2}"
        ras_datetime_format2_re = r"\d{2}\w{3}\d{4} \d{2}\d{2}"
        ras_duration_format_re = r"\d{2}:\d{2}:\d{2}"

        if re.match(rf"^{ras_datetime_format1_re}", s):
            if re.match(rf"^{ras_datetime_format1_re} to {ras_datetime_format1_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_datetime(split[0]),
                    HdfUtils.parse_ras_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_datetime(s)
        elif re.match(rf"^{ras_datetime_format2_re}", s):
            if re.match(rf"^{ras_datetime_format2_re} to {ras_datetime_format2_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_window_datetime(split[0]),
                    HdfUtils.parse_ras_window_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_window_datetime(s)
        elif re.match(rf"^{ras_duration_format_re}$", s):
            return HdfUtils.parse_ras_duration(s)
        return s





    @staticmethod
    def convert_ras_hdf_value(value: Any) -> Union[None, bool, str, List[str], int, float, List[int], List[float]]:
        """
        Convert a value from a HEC-RAS HDF file into a Python object.

        Args:
            value (Any): The value to convert.

        Returns:
            Union[None, bool, str, List[str], int, float, List[int], List[float]]: The converted value.
        """
        if isinstance(value, np.floating) and np.isnan(value):
            return None
        elif isinstance(value, (bytes, np.bytes_)):
            return value.decode('utf-8')
        elif isinstance(value, np.integer):
            return int(value)
        elif isinstance(value, np.floating):
            return float(value)
        elif isinstance(value, (int, float)):
            return value
        elif isinstance(value, (list, tuple, np.ndarray)):
            if len(value) > 1:
                return [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                return HdfUtils.convert_ras_hdf_value(value[0])
        else:
            return str(value)










# RENAME TO convert_df_datetimes_to_str 

    @staticmethod
    def convert_df_datetimes_to_str(df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert any datetime64 columns in a DataFrame to strings.

        Args:
            df (pd.DataFrame): The DataFrame to convert.

        Returns:
            pd.DataFrame: The DataFrame with datetime columns converted to strings.
        """
        for col in df.select_dtypes(include=['datetime64']).columns:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
        return df


# KDTree Methods: 


    @staticmethod
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = HdfUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = HdfUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped




# Datetime Parsing Methods: 

    @staticmethod
    @log_call
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Public method to parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
# Rename to convert_timesteps_to_datetimes and make public
    @staticmethod
    def convert_timesteps_to_datetimes(timesteps: np.ndarray, start_time: datetime, time_unit: str = "days", round_to: str = "100ms") -> pd.DatetimeIndex:
        """
        Convert RAS timesteps to datetime objects.

        Args:
            timesteps (np.ndarray): Array of timesteps.
            start_time (datetime): Start time of the simulation.
            time_unit (str): Unit of the timesteps. Default is "days".
            round_to (str): Frequency string to round the times to. Default is "100ms" (100 milliseconds).

        Returns:
            pd.DatetimeIndex: DatetimeIndex of converted and rounded datetimes.
        """
        if time_unit == "days":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='D')
        elif time_unit == "hours":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='H')
        else:
            raise ValueError(f"Unsupported time unit: {time_unit}")

        return pd.DatetimeIndex(datetimes).round(round_to)
    
# rename to convert_hdf5_attrs_to_dict and make public

    @staticmethod
    def convert_hdf5_attrs_to_dict(attrs: Union[h5py.AttributeManager, Dict], prefix: Optional[str] = None) -> Dict:
        """
        Convert HDF5 attributes to a Python dictionary.

        Args:
            attrs (Union[h5py.AttributeManager, Dict]): The attributes to convert.
            prefix (Optional[str]): A prefix to add to the attribute keys.

        Returns:
            Dict: A dictionary of converted attributes.
        """
        result = {}
        for key, value in attrs.items():
            if prefix:
                key = f"{prefix}/{key}"
            if isinstance(value, (np.ndarray, list)):
                result[key] = [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                result[key] = HdfUtils.convert_ras_hdf_value(value)
        return result
    
    

    @staticmethod
    def parse_run_time_window(window: str) -> Tuple[datetime, datetime]:
        """
        Parse a run time window string into a tuple of datetime objects.

        Args:
            window (str): The run time window string to be parsed.

        Returns:
            Tuple[datetime, datetime]: A tuple containing two datetime objects representing the start and end of the run
            time window.
        """
        split = window.split(" to ")
        begin = HdfUtils._parse_ras_datetime(split[0])
        end = HdfUtils._parse_ras_datetime(split[1])
        return begin, end

    


                
                
                
                
                
                
                
                
                
                
                
                
## MOVED FROM HdfBase to HdfUtils:
# _parse_ras_datetime   
# _parse_ras_simulation_window_datetime
# _parse_duration
# _parse_ras_datetime_ms
# _convert_ras_hdf_string

# Which were renamed and made public as: 
# parse_ras_datetime
# parse_ras_window_datetime
# parse_ras_datetime_ms
# parse_ras_duration
# parse_ras_time_window


# Rename to parse_ras_datetime and make public

    @staticmethod
    def parse_ras_datetime(datetime_str: str) -> datetime:
        """
        Parse a RAS datetime string into a datetime object.

        Args:
            datetime_str (str): The datetime string in format "ddMMMYYYY HH:MM:SS"

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H:%M:%S")

# Rename to parse_ras_window_datetime and make public

    @staticmethod
    def parse_ras_window_datetime(datetime_str: str) -> datetime:
        """
        Parse a datetime string from a RAS simulation window into a datetime object.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H%M")


# Rename to parse_duration and make public


    @staticmethod
    def parse_duration(duration_str: str) -> timedelta:
        """
        Parse a duration string into a timedelta object.

        Args:
            duration_str (str): The duration string to parse.

        Returns:
            timedelta: The parsed duration as a timedelta object.
        """
        hours, minutes, seconds = map(int, duration_str.split(':'))
        return timedelta(hours=hours, minutes=minutes, seconds=seconds)
    
    
# Rename to parse_ras_datetime_ms and make public
    
    @staticmethod
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\HdfXsec.py
==================================================
"""
Class: HdfXsec

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

Available Functions:
- get_cross_sections(): Extract cross sections from HDF geometry file
- get_river_centerlines(): Extract river centerlines from HDF geometry file
- get_river_stationing(): Calculate river stationing along centerlines
- get_river_reaches(): Return the model 1D river reach lines
- get_river_edge_lines(): Return the model river edge lines
- get_river_bank_lines(): Extract river bank lines from HDF geometry file
- _interpolate_station(): Private helper method for station interpolation

All functions follow the get_ prefix convention for methods that return data.
Private helper methods use the underscore prefix convention.

Each function returns a GeoDataFrame containing geometries and associated attributes
specific to the requested feature type. All functions include proper error handling
and logging.
"""

from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString
from typing import List  # Import List to avoid NameError
from .Decorators import standardize_input, log_call
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .LoggingConfig import get_logger
import logging



logger = get_logger(__name__)

class HdfXsec:
    """
    Handles cross-section and river geometry data extraction from HEC-RAS HDF files.

    This class provides static methods to extract and process:
    - Cross-section geometries and attributes
    - River centerlines and reaches
    - River edge and bank lines
    - Station-elevation profiles

    All methods are designed to return GeoDataFrames with standardized geometries 
    and attributes following the HEC-RAS data structure.

    Note:
        Requires HEC-RAS geometry HDF files with standard structure and naming conventions.
        All methods use proper error handling and logging.
    """
    @staticmethod
    @log_call
    def get_cross_sections(hdf_path: str, datetime_to_str: bool = True, ras_object=None) -> gpd.GeoDataFrame:
        """
        Extracts cross-section geometries and attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : str
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to True
        ras_object : RasPrj, optional
            RAS project object for additional context, defaults to None

        Returns
        -------
        gpd.GeoDataFrame
            Cross-section data with columns:
            - geometry: LineString of cross-section path
            - station_elevation: Station-elevation profile points
            - mannings_n: Dictionary of Manning's n values and stations
            - ineffective_blocks: List of ineffective flow area blocks
            - River, Reach, RS: River system identifiers
            - Name, Description: Cross-section labels
            - Len Left/Channel/Right: Flow path lengths
            - Left/Right Bank: Bank station locations
            - Additional hydraulic parameters and attributes

        Notes
        -----
        The returned GeoDataFrame includes the coordinate system from the HDF file
        when available. All byte strings are converted to regular strings.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract datasets
                poly_info = hdf['/Geometry/Cross Sections/Polyline Info'][:]
                poly_parts = hdf['/Geometry/Cross Sections/Polyline Parts'][:]
                poly_points = hdf['/Geometry/Cross Sections/Polyline Points'][:]
                
                station_info = hdf['/Geometry/Cross Sections/Station Elevation Info'][:]
                station_values = hdf['/Geometry/Cross Sections/Station Elevation Values'][:]
                
                # Get attributes for cross sections
                xs_attrs = hdf['/Geometry/Cross Sections/Attributes'][:]
                
                # Get Manning's n data
                mann_info = hdf["/Geometry/Cross Sections/Manning's n Info"][:]
                mann_values = hdf["/Geometry/Cross Sections/Manning's n Values"][:]
                
                # Get ineffective blocks data
                ineff_blocks = hdf['/Geometry/Cross Sections/Ineffective Blocks'][:]
                ineff_info = hdf['/Geometry/Cross Sections/Ineffective Info'][:]
                
                # Initialize lists to store data
                geometries = []
                station_elevations = []
                mannings_n = []
                ineffective_blocks = []
                
                # Process each cross section
                for i in range(len(poly_info)):
                    # Extract polyline info
                    point_start_idx = poly_info[i][0]
                    point_count = poly_info[i][1]
                    part_start_idx = poly_info[i][2]
                    part_count = poly_info[i][3]
                    
                    # Extract parts for current polyline
                    parts = poly_parts[part_start_idx:part_start_idx + part_count]
                    
                    # Collect all points for this cross section
                    xs_points = []
                    for part in parts:
                        part_point_start = point_start_idx + part[0]
                        part_point_count = part[1]
                        points = poly_points[part_point_start:part_point_start + part_point_count]
                        xs_points.extend(points)
                    
                    # Create LineString geometry
                    if len(xs_points) >= 2:
                        geometry = LineString(xs_points)
                        geometries.append(geometry)
                        
                        # Extract station-elevation data
                        start_idx = station_info[i][0]
                        count = station_info[i][1]
                        station_elev = station_values[start_idx:start_idx + count]
                        station_elevations.append(station_elev)
                        
                        # Extract Manning's n data
                        mann_start_idx = mann_info[i][0]
                        mann_count = mann_info[i][1]
                        mann_n_section = mann_values[mann_start_idx:mann_start_idx + mann_count]
                        mann_n_dict = {
                            'Station': mann_n_section[:, 0].tolist(),
                            'Mann n': mann_n_section[:, 1].tolist()
                        }
                        mannings_n.append(mann_n_dict)
                        
                        # Extract ineffective blocks data
                        ineff_start_idx = ineff_info[i][0]
                        ineff_count = ineff_info[i][1]
                        if ineff_count > 0:
                            blocks = ineff_blocks[ineff_start_idx:ineff_start_idx + ineff_count]
                            blocks_list = []
                            for block in blocks:
                                block_dict = {
                                    'Left Sta': float(block['Left Sta']),
                                    'Right Sta': float(block['Right Sta']), 
                                    'Elevation': float(block['Elevation']),
                                    'Permanent': bool(block['Permanent'])
                                }
                                blocks_list.append(block_dict)
                            ineffective_blocks.append(blocks_list)
                        else:
                            ineffective_blocks.append([])
                
                # Create GeoDataFrame
                if geometries:
                    # Create DataFrame from attributes
                    data = {
                        'geometry': geometries,
                        'station_elevation': station_elevations,
                        'mannings_n': mannings_n,
                        'ineffective_blocks': ineffective_blocks,
                        'River': [x['River'].decode('utf-8').strip() for x in xs_attrs],
                        'Reach': [x['Reach'].decode('utf-8').strip() for x in xs_attrs],
                        'RS': [x['RS'].decode('utf-8').strip() for x in xs_attrs],
                        'Name': [x['Name'].decode('utf-8').strip() for x in xs_attrs],
                        'Description': [x['Description'].decode('utf-8').strip() for x in xs_attrs],
                        'Len Left': xs_attrs['Len Left'],
                        'Len Channel': xs_attrs['Len Channel'],
                        'Len Right': xs_attrs['Len Right'], 
                        'Left Bank': xs_attrs['Left Bank'],
                        'Right Bank': xs_attrs['Right Bank'],
                        'Friction Mode': [x['Friction Mode'].decode('utf-8').strip() for x in xs_attrs],
                        'Contr': xs_attrs['Contr'],
                        'Expan': xs_attrs['Expan'],
                        'Left Levee Sta': xs_attrs['Left Levee Sta'],
                        'Left Levee Elev': xs_attrs['Left Levee Elev'],
                        'Right Levee Sta': xs_attrs['Right Levee Sta'],
                        'Right Levee Elev': xs_attrs['Right Levee Elev'],
                        'HP Count': xs_attrs['HP Count'],
                        'HP Start Elev': xs_attrs['HP Start Elev'],
                        'HP Vert Incr': xs_attrs['HP Vert Incr'],
                        'HP LOB Slices': xs_attrs['HP LOB Slices'],
                        'HP Chan Slices': xs_attrs['HP Chan Slices'],
                        'HP ROB Slices': xs_attrs['HP ROB Slices'],
                        'Ineff Block Mode': xs_attrs['Ineff Block Mode'],
                        'Obstr Block Mode': xs_attrs['Obstr Block Mode'],
                        'Default Centerline': xs_attrs['Default Centerline'],
                        'Last Edited': [x['Last Edited'].decode('utf-8').strip() for x in xs_attrs]
                    }
                
                    gdf = gpd.GeoDataFrame(data)
                    
                    # Set CRS if available
                    if 'Projection' in hdf['/Geometry'].attrs:
                        proj = hdf['/Geometry'].attrs['Projection']
                        if isinstance(proj, bytes):
                            proj = proj.decode('utf-8')
                        gdf.set_crs(proj, allow_override=True)
                    
                    return gdf
                
                return gpd.GeoDataFrame()
                
        except Exception as e:
            logging.error(f"Error processing cross-section data: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_centerlines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts river centerline geometries and attributes from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to False

        Returns
        -------
        GeoDataFrame
            River centerline data with columns:
            - geometry: LineString of river centerline
            - River Name, Reach Name: River system identifiers
            - US/DS Type, Name: Upstream/downstream connection info
            - length: Centerline length in project units
            Additional attributes from the HDF file are included

        Notes
        -----
        Returns an empty GeoDataFrame if no centerlines are found.
        All string attributes are stripped of whitespace.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    logger.warning("No river centerlines found in geometry file")
                    return GeoDataFrame()

                centerline_data = hdf_file["Geometry/River Centerlines"]
                
                # Get attributes directly from HDF dataset
                attrs = centerline_data["Attributes"][()]
                
                # Create initial dictionary for DataFrame
                centerline_dict = {}
                
                # Process each attribute field
                for name in attrs.dtype.names:
                    values = attrs[name]
                    if values.dtype.kind == 'S':
                        # Convert byte strings to regular strings
                        centerline_dict[name] = [val.decode('utf-8').strip() for val in values]
                    else:
                        centerline_dict[name] = values.tolist()  # Convert numpy array to list

                # Get polylines using utility function
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Centerlines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                centerline_gdf = GeoDataFrame(
                    centerline_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Clean up string columns
                str_columns = ['River Name', 'Reach Name', 'US Type', 
                            'US Name', 'DS Type', 'DS Name']
                for col in str_columns:
                    if col in centerline_gdf.columns:
                        centerline_gdf[col] = centerline_gdf[col].str.strip()

                # Add length calculation in project units
                if not centerline_gdf.empty:
                    centerline_gdf['length'] = centerline_gdf.geometry.length
                    
                    # Convert datetime columns if requested
                    if datetime_to_str:
                        datetime_cols = centerline_gdf.select_dtypes(
                            include=['datetime64']).columns
                        for col in datetime_cols:
                            centerline_gdf[col] = centerline_gdf[col].dt.strftime(
                                '%Y-%m-%d %H:%M:%S')

                logger.info(f"Extracted {len(centerline_gdf)} river centerlines")
                return centerline_gdf

        except Exception as e:
            logger.error(f"Error reading river centerlines: {str(e)}")
            return GeoDataFrame()



    @staticmethod
    @log_call
    def get_river_stationing(centerlines_gdf: GeoDataFrame) -> GeoDataFrame:
        """
        Calculates stationing along river centerlines with interpolated points.

        Parameters
        ----------
        centerlines_gdf : GeoDataFrame
            River centerline geometries from get_river_centerlines()

        Returns
        -------
        GeoDataFrame
            Original centerlines with additional columns:
            - station_start: Starting station value (0 or length)
            - station_end: Ending station value (length or 0)
            - stations: Array of station values along centerline
            - points: Array of interpolated point geometries

        Notes
        -----
        Station direction (increasing/decreasing) is determined by
        upstream/downstream junction connections. Stations are calculated
        at 100 evenly spaced points along each centerline.
        """
        if centerlines_gdf.empty:
            logger.warning("Empty centerlines GeoDataFrame provided")
            return centerlines_gdf

        try:
            # Create copy to avoid modifying original
            result_gdf = centerlines_gdf.copy()
            
            # Initialize new columns
            result_gdf['station_start'] = 0.0
            result_gdf['station_end'] = 0.0
            result_gdf['stations'] = None
            result_gdf['points'] = None
            
            # Process each centerline
            for idx, row in result_gdf.iterrows():
                # Get line geometry
                line = row.geometry
                
                # Calculate length
                total_length = line.length
                
                # Generate points along the line
                distances = np.linspace(0, total_length, num=100)  # Adjust num for desired density
                points = [line.interpolate(distance) for distance in distances]
                
                # Store results
                result_gdf.at[idx, 'station_start'] = 0.0
                result_gdf.at[idx, 'station_end'] = total_length
                result_gdf.at[idx, 'stations'] = distances
                result_gdf.at[idx, 'points'] = points
                
                # Add stationing direction based on upstream/downstream info
                if row['upstream_type'] == 'Junction' and row['downstream_type'] != 'Junction':
                    # Reverse stationing if upstream is junction
                    result_gdf.at[idx, 'station_start'] = total_length
                    result_gdf.at[idx, 'station_end'] = 0.0
                    result_gdf.at[idx, 'stations'] = total_length - distances
            
            return result_gdf

        except Exception as e:
            logger.error(f"Error calculating river stationing: {str(e)}")
            return centerlines_gdf

    @staticmethod
    def _interpolate_station:
    """Docs only, see '_interpolate_station.py' for full function code"""


==================================================

File: c:\GH\ras-commander\ras_commander\LoggingConfig.py
==================================================
# logging_config.py

import logging
import logging.handlers
from pathlib import Path
import functools

# Define log levels
DEBUG = logging.DEBUG
INFO = logging.INFO
WARNING = logging.WARNING
ERROR = logging.ERROR
CRITICAL = logging.CRITICAL


_logging_setup_done = False

def setup_logging:
    """Docs only, see 'setup_logging.py' for full function code"""
setup_logging()
==================================================

File: c:\GH\ras-commander\ras_commander\RasCmdr.py
==================================================
"""
RasCmdr - Execution operations for running HEC-RAS simulations

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- compute_plan()
- compute_parallel()
- compute_test_mode()
        
        
        
"""
import os
import subprocess
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from .RasPrj import ras, RasPrj, init_ras_project, get_ras_exe
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasUtils import RasUtils
import logging
import time
import queue
from threading import Thread, Lock
from typing import Union, List, Optional, Dict
from pathlib import Path
import shutil
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Thread
from itertools import cycle
from ras_commander.RasPrj import RasPrj  # Ensure RasPrj is imported
from threading import Lock, Thread, current_thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import cycle
from typing import Union, List, Optional, Dict
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

# Module code starts here

# TODO: Future Enhancements
# 1. Alternate Run Mode for compute_plan and compute_parallel:
#    - Use Powershell to execute HEC-RAS command
#    - Hide RAS window and all child windows
#    - Note: This mode may prevent execution if the plan has a popup
#    - Intended for background runs or popup-free scenarios
#    - Limit to non-commercial use
#
# 2. Implement compute_plan_remote:
#    - Execute compute_plan on a remote machine via psexec
#    - Use keyring package for secure credential storage
#    - Implement psexec command for remote HEC-RAS execution
#    - Create remote_worker objects to store machine details:
#      (machine name, username, password, ras_exe_path, local folder path, etc.)
#    - Develop RasRemote class for remote_worker management and abstractions
#    - Implement compute_plan_remote in RasCmdr as a thin wrapper around RasRemote
#      (similar to existing compute_plan functions but for remote execution)


class RasCmdr:
    
    @staticmethod
    @log_call
    def compute_plan(
        plan_number,
        dest_folder=None, 
        ras_object=None,
        clear_geompre=False,
        num_cores=None,
        overwrite_dest=False
    ):
        """
        Execute a HEC-RAS plan.

        Args:
            plan_number (str, Path): The plan number to execute (e.g., "01", "02") or the full path to the plan file.
            dest_folder (str, Path, optional): Name of the folder or full path for computation.
                If a string is provided, it will be created in the same parent directory as the project folder.
                If a full path is provided, it will be used as is.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.
            num_cores (int, optional): Number of cores to use for the plan execution. If None, the current setting is not changed.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. Defaults to False.

        Returns:
            bool: True if the execution was successful, False otherwise.

        Raises:
            ValueError: If the specified dest_folder already exists and is not empty, and overwrite_dest is False.
        """
        try:
            ras_obj = ras_object if ras_object is not None else ras
            logger.info(f"Using ras_object with project folder: {ras_obj.project_folder}")
            ras_obj.check_initialized()
            
            if dest_folder is not None:
                dest_folder = Path(ras_obj.project_folder).parent / dest_folder if isinstance(dest_folder, str) else Path(dest_folder)
                
                if dest_folder.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder)
                        logger.info(f"Destination folder '{dest_folder}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder.iterdir()):
                        error_msg = f"Destination folder '{dest_folder}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                
                dest_folder.mkdir(parents=True, exist_ok=True)
                shutil.copytree(ras_obj.project_folder, dest_folder, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder}")
                
                compute_ras = RasPrj()
                compute_ras.initialize(dest_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
            else:
                compute_ras = ras_obj
                compute_prj_path = ras_obj.prj_file

            # Determine the plan path
            compute_plan_path = Path(plan_number) if isinstance(plan_number, (str, Path)) and Path(plan_number).is_file() else RasPlan.get_plan_path(plan_number, compute_ras)

            if not compute_prj_path or not compute_plan_path:
                logger.error(f"Could not find project file or plan file for plan {plan_number}")
                return False

            # Clear geometry preprocessor files if requested
            if clear_geompre:
                try:
                    RasGeo.clear_geompre_files(compute_plan_path, ras_object=compute_ras)
                    logger.info(f"Cleared geometry preprocessor files for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error clearing geometry preprocessor files for plan {plan_number}: {str(e)}")

            # Set the number of cores if specified
            if num_cores is not None:
                try:
                    RasPlan.set_num_cores(compute_plan_path, num_cores=num_cores, ras_object=compute_ras)
                    logger.info(f"Set number of cores to {num_cores} for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error setting number of cores for plan {plan_number}: {str(e)}")

            # Prepare the command for HEC-RAS execution
            cmd = f'"{compute_ras.ras_exe_path}" -c "{compute_prj_path}" "{compute_plan_path}"'
            logger.info("Running HEC-RAS from the Command Line:")
            logger.info(f"Running command: {cmd}")

            # Execute the HEC-RAS command
            start_time = time.time()
            try:
                subprocess.run(cmd, check=True, shell=True, capture_output=True, text=True)
                end_time = time.time()
                run_time = end_time - start_time
                logger.info(f"HEC-RAS execution completed for plan: {plan_number}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return True
            except subprocess.CalledProcessError as e:
                end_time = time.time()
                run_time = end_time - start_time
                logger.error(f"Error running plan: {plan_number}")
                logger.error(f"Error message: {e.output}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return False
        except Exception as e:
            logger.critical(f"Error in compute_plan: {str(e)}")
            return False
        finally:
            # Update the RAS object's dataframes
            if ras_obj:
                ras_obj.plan_df = ras_obj.get_plan_entries()
                ras_obj.geom_df = ras_obj.get_geom_entries()
                ras_obj.flow_df = ras_obj.get_flow_entries()
                ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
    


    @staticmethod
    @log_call
    @staticmethod
    @log_call
    def compute_parallel(
        plan_number: Union[str, List[str], None] = None,
        max_workers: int = 2,
        num_cores: int = 2,
        clear_geompre: bool = False,
        ras_object: Optional['RasPrj'] = None,
        dest_folder: Union[str, Path, None] = None,
        overwrite_dest: bool = False
    ) -> Dict[str, bool]:
        """
        Compute multiple HEC-RAS plans in parallel.

        Args:
            plan_number (Union[str, List[str], None]): Plan number(s) to compute. If None, all plans are computed.
            max_workers (int): Maximum number of parallel workers.
            num_cores (int): Number of cores to use per plan computation.
            clear_geompre (bool): Whether to clear geometry preprocessor files.
            ras_object (Optional[RasPrj]): RAS project object. If None, uses global instance.
            dest_folder (Union[str, Path, None]): Destination folder for computed results.
            overwrite_dest (bool): Whether to overwrite existing destination folder.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()

            project_folder = Path(ras_obj.project_folder)

            if dest_folder is not None:
                dest_folder_path = Path(dest_folder)
                if dest_folder_path.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder_path)
                        logger.info(f"Destination folder '{dest_folder_path}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder_path.iterdir()):
                        error_msg = f"Destination folder '{dest_folder_path}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                dest_folder_path.mkdir(parents=True, exist_ok=True)
                shutil.copytree(project_folder, dest_folder_path, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder_path}")
                project_folder = dest_folder_path

            if plan_number:
                if isinstance(plan_number, str):
                    plan_number = [plan_number]
                ras_obj.plan_df = ras_obj.plan_df[ras_obj.plan_df['plan_number'].isin(plan_number)]
                logger.info(f"Filtered plans to execute: {plan_number}")

            num_plans = len(ras_obj.plan_df)
            max_workers = min(max_workers, num_plans) if num_plans > 0 else 1
            logger.info(f"Adjusted max_workers to {max_workers} based on the number of plans: {num_plans}")

            worker_ras_objects = {}
            for worker_id in range(1, max_workers + 1):
                worker_folder = project_folder.parent / f"{project_folder.name} [Worker {worker_id}]"
                if worker_folder.exists():
                    shutil.rmtree(worker_folder)
                    logger.info(f"Removed existing worker folder: {worker_folder}")
                shutil.copytree(project_folder, worker_folder)
                logger.info(f"Created worker folder: {worker_folder}")

                try:
                    ras_instance = RasPrj()
                    worker_ras_instance = init_ras_project(
                        ras_project_folder=worker_folder,
                        ras_version=ras_obj.ras_exe_path,
                        ras_instance=ras_instance
                    )
                    worker_ras_objects[worker_id] = worker_ras_instance
                except Exception as e:
                    logger.critical(f"Failed to initialize RAS project for worker {worker_id}: {str(e)}")
                    worker_ras_objects[worker_id] = None

            worker_cycle = cycle(range(1, max_workers + 1))
            plan_assignments = [(next(worker_cycle), plan_num) for plan_num in ras_obj.plan_df['plan_number']]

            execution_results: Dict[str, bool] = {}

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(
                        RasCmdr.compute_plan,
                        plan_num, 
                        ras_object=worker_ras_objects[worker_id], 
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    for worker_id, plan_num in plan_assignments
                ]

                for future, (worker_id, plan_num) in zip(as_completed(futures), plan_assignments):
                    try:
                        success = future.result()
                        execution_results[plan_num] = success
                        logger.info(f"Plan {plan_num} executed in worker {worker_id}: {'Successful' if success else 'Failed'}")
                    except Exception as e:
                        execution_results[plan_num] = False
                        logger.error(f"Plan {plan_num} failed in worker {worker_id}: {str(e)}")

            final_dest_folder = dest_folder_path if dest_folder is not None else project_folder.parent / f"{project_folder.name} [Computed]"
            final_dest_folder.mkdir(parents=True, exist_ok=True)
            logger.info(f"Final destination for computed results: {final_dest_folder}")

            for worker_ras in worker_ras_objects.values():
                if worker_ras is None:
                    continue
                worker_folder = Path(worker_ras.project_folder)
                try:
                    for item in worker_folder.iterdir():
                        dest_path = final_dest_folder / item.name
                        if dest_path.exists():
                            if dest_path.is_dir():
                                shutil.rmtree(dest_path)
                                logger.debug(f"Removed existing directory at {dest_path}")
                            else:
                                dest_path.unlink()
                                logger.debug(f"Removed existing file at {dest_path}")
                        shutil.move(str(item), final_dest_folder)
                        logger.debug(f"Moved {item} to {final_dest_folder}")
                    shutil.rmtree(worker_folder)
                    logger.info(f"Removed worker folder: {worker_folder}")
                except Exception as e:
                    logger.error(f"Error moving results from {worker_folder} to {final_dest_folder}: {str(e)}")

            try:
                final_dest_folder_ras_obj = RasPrj()
                final_dest_folder_ras_obj = init_ras_project(
                    ras_project_folder=final_dest_folder, 
                    ras_version=ras_obj.ras_exe_path,
                    ras_instance=final_dest_folder_ras_obj
                )
                final_dest_folder_ras_obj.check_initialized()
            except Exception as e:
                logger.critical(f"Failed to initialize RasPrj for final destination: {str(e)}")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj = ras_object or ras
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_parallel: {str(e)}")
            return {}

    @staticmethod
    @log_call
    def compute_test_mode(
        plan_number=None, 
        dest_folder_suffix="[Test]", 
        clear_geompre=False, 
        num_cores=None, 
        ras_object=None,
        overwrite_dest=False
    ):
        """
        Execute HEC-RAS plans in test mode. This is a re-creation of the HEC-RAS command line -test flag, 
        which does not work in recent versions of HEC-RAS.
        
        As a special-purpose function that emulates the original -test flag, it operates differently than the 
        other two compute_ functions. Per the original HEC-RAS test flag, it creates a separate test folder,
        copies the project there, and executes the specified plans in sequential order.
        
        For most purposes, just copying a the project folder, initing that new folder, then running each plan 
        with compute_plan is a simpler and more flexible approach.  This is shown in the examples provided
        in the ras-commander library.

        Args:
            plan_number (str, list[str], optional): Plan number or list of plan numbers to execute. 
                If None, all plans will be executed. Default is None.
            dest_folder_suffix (str, optional): Suffix to append to the test folder name to create dest_folder. 
                Defaults to "[Test]".
                dest_folder is always created in the project folder's parent directory.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files.
                Defaults to False.
            num_cores (int, optional): Maximum number of cores to use for each plan.
                If None, the current setting is not changed. Default is None.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. Defaults to False.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.

        Example:
            Run all plans: RasCommander.compute_test_mode()
            Run a specific plan: RasCommander.compute_test_mode(plan_number="01")
            Run multiple plans: RasCommander.compute_test_mode(plan_number=["01", "03", "05"])
            Run plans with a custom folder suffix: RasCommander.compute_test_mode(dest_folder_suffix="[TestRun]")
            Run plans and clear geometry preprocessor files: RasCommander.compute_test_mode(clear_geompre=True)
            Run plans with a specific number of cores: RasCommander.compute_test_mode(num_cores=4)
            
        Notes:
            - This function executes plans in a separate folder for isolated testing.
            - If plan_number is not provided, all plans in the project will be executed.
            - The function does not change the geometry preprocessor and IB tables settings.  
                - To force recomputing of geometry preprocessor and IB tables, use the clear_geompre=True option.
            - Plans are executed sequentially.
            - Because copying the project is implicit, only a dest_folder_suffix option is provided.
            - For more flexible run management, use the compute_parallel or compute_sequential functions.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()
            
            logger.info("Starting the compute_test_mode...")
               
            project_folder = Path(ras_obj.project_folder)

            if not project_folder.exists():
                logger.error(f"Project folder '{project_folder}' does not exist.")
                return {}

            compute_folder = project_folder.parent / f"{project_folder.name} {dest_folder_suffix}"
            logger.info(f"Creating the test folder: {compute_folder}...")

            if compute_folder.exists():
                if overwrite_dest:
                    shutil.rmtree(compute_folder)
                    logger.info(f"Compute folder '{compute_folder}' exists. Overwriting as per overwrite_dest=True.")
                elif any(compute_folder.iterdir()):
                    error_msg = (
                        f"Compute folder '{compute_folder}' exists and is not empty. "
                        "Use overwrite_dest=True to overwrite."
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg)

            try:
                shutil.copytree(project_folder, compute_folder)
                logger.info(f"Copied project folder to compute folder: {compute_folder}")
            except Exception as e:
                logger.critical(f"Error occurred while copying project folder: {str(e)}")
                return {}

            try:
                compute_ras = RasPrj()
                compute_ras.initialize(compute_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
                logger.info(f"Initialized RAS project in compute folder: {compute_prj_path}")
            except Exception as e:
                logger.critical(f"Error initializing RAS project in compute folder: {str(e)}")
                return {}

            if not compute_prj_path:
                logger.error("Project file not found.")
                return {}

            logger.info("Getting plan entries...")
            try:
                ras_compute_plan_entries = compute_ras.plan_df
                logger.info("Retrieved plan entries successfully.")
            except Exception as e:
                logger.critical(f"Error retrieving plan entries: {str(e)}")
                return {}

            if plan_number:
                if isinstance(plan_number, str):
                    plan_number = [plan_number]
                ras_compute_plan_entries = ras_compute_plan_entries[
                    ras_compute_plan_entries['plan_number'].isin(plan_number)
                ]
                logger.info(f"Filtered plans to execute: {plan_number}")

            execution_results = {}
            logger.info("Running selected plans sequentially...")
            for _, plan in ras_compute_plan_entries.iterrows():
                plan_number = plan["plan_number"]
                start_time = time.time()
                try:
                    success = RasCmdr.compute_plan(
                        plan_number,
                        ras_object=compute_ras,
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    execution_results[plan_number] = success
                    if success:
                        logger.info(f"Successfully computed plan {plan_number}")
                    else:
                        logger.error(f"Failed to compute plan {plan_number}")
                except Exception as e:
                    execution_results[plan_number] = False
                    logger.error(f"Error computing plan {plan_number}: {str(e)}")
                finally:
                    end_time = time.time()
                    run_time = end_time - start_time
                    logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")

            logger.info("All selected plans have been executed.")
            logger.info("compute_test_mode completed.")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_test_mode: {str(e)}")
            return {}
==================================================

File: c:\GH\ras-commander\ras_commander\RasExamples.py
==================================================
"""
RasExamples - Manage and load HEC-RAS example projects for testing and development

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- get_example_projects()
- list_categories()
- list_projects()
- extract_project()
- is_project_extracted()
- clean_projects_directory()
        
"""
import os
import requests
import zipfile
import pandas as pd
from pathlib import Path
import shutil
from typing import Union, List
import csv
from datetime import datetime
import logging
import re
from tqdm import tqdm
from ras_commander import get_logger
from ras_commander.LoggingConfig import log_call

logger = get_logger(__name__)

class RasExamples:
    """
    A class for quickly loading HEC-RAS example projects for testing and development of ras-commander.

    This class provides functionality to download, extract, and manage HEC-RAS example projects.
    It supports both default HEC-RAS example projects and custom projects from user-provided URLs.
    Additionally, it includes functionality to download FEMA's Base Level Engineering (BLE) models
    from CSV files provided by the FEMA Estimated Base Flood Elevation (BFE) Viewer.
    """
    @log_call
    def __init__:
    """Docs only, see '__init__.py' for full function code"""
        return int(number * units[unit])
==================================================

File: c:\GH\ras-commander\ras_commander\RasGeo.py
==================================================
"""
RasGeo - Operations for handling geometry files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- clear_geompre_files()
        
        
"""
import os
from pathlib import Path
from typing import List, Union
from .RasPlan import RasPlan
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasGeo:
    """
    A class for operations on HEC-RAS geometry files.
    """
    
    @staticmethod
    @log_call
    def clear_geompre_files(
        plan_files: Union[str, Path, List[Union[str, Path]]] = None,
        ras_object = None
    ) -> None:
        """
        Clear HEC-RAS geometry preprocessor files for specified plan files or all plan files in the project directory.
        
        Limitations/Future Work:
        - This function only deletes the geometry preprocessor file.
        - It does not clear the IB tables.
        - It also does not clear geometry preprocessor tables from the geometry HDF.
        - All of these features will need to be added to reliably remove geometry preprocessor files for 1D and 2D projects.
        
        Parameters:
            plan_files (Union[str, Path, List[Union[str, Path]]], optional): 
                Full path(s) to the HEC-RAS plan file(s) (.p*).
                If None, clears all plan files in the project directory.
            ras_object: An optional RAS object instance.
        
        Returns:
            None
        
        Examples:
            # Clear all geometry preprocessor files in the project directory
            RasGeo.clear_geompre_files()
            
            # Clear a single plan file
            RasGeo.clear_geompre_files(r'path/to/plan.p01')
            
            # Clear multiple plan files
            RasGeo.clear_geompre_files([r'path/to/plan1.p01', r'path/to/plan2.p02'])

        Note:
            This function updates the ras object's geometry dataframe after clearing the preprocessor files.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        def clear_single_file(plan_file: Union[str, Path], ras_obj) -> None:
            plan_path = Path(plan_file)
            geom_preprocessor_suffix = '.c' + ''.join(plan_path.suffixes[1:]) if plan_path.suffixes else '.c'
            geom_preprocessor_file = plan_path.with_suffix(geom_preprocessor_suffix)
            if geom_preprocessor_file.exists():
                try:
                    geom_preprocessor_file.unlink()
                    logger.info(f"Deleted geometry preprocessor file: {geom_preprocessor_file}")
                except PermissionError:
                    logger.error(f"Permission denied: Unable to delete geometry preprocessor file: {geom_preprocessor_file}")
                    raise PermissionError(f"Unable to delete geometry preprocessor file: {geom_preprocessor_file}. Permission denied.")
                except OSError as e:
                    logger.error(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
                    raise OSError(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
            else:
                logger.warning(f"No geometry preprocessor file found for: {plan_file}")
        
        if plan_files is None:
            logger.info("Clearing all geometry preprocessor files in the project directory.")
            plan_files_to_clear = list(ras_obj.project_folder.glob(r'*.p*'))
        elif isinstance(plan_files, (str, Path)):
            plan_files_to_clear = [plan_files]
            logger.info(f"Clearing geometry preprocessor file for single plan: {plan_files}")
        elif isinstance(plan_files, list):
            plan_files_to_clear = plan_files
            logger.info(f"Clearing geometry preprocessor files for multiple plans: {plan_files}")
        else:
            logger.error("Invalid input type for plan_files.")
            raise ValueError("Invalid input. Please provide a string, Path, list of paths, or None.")
        
        for plan_file in plan_files_to_clear:
            clear_single_file(plan_file, ras_obj)
        
        try:
            ras_obj.geom_df = ras_obj.get_geom_entries()
            logger.info("Geometry dataframe updated successfully.")
        except Exception as e:
            logger.error(f"Failed to update geometry dataframe: {str(e)}")
            raise









==================================================

File: c:\GH\ras-commander\ras_commander\RasGpt.py
==================================================
import os
from pathlib import Path
from typing import Optional
from ras_commander import get_logger, log_call

logger = get_logger(__name__)

class RasGpt:
    """
    A class containing helper functions for the RAS Commander GPT.
    """
    
# to be implemented later
# 
# This class will contain  methods to help LLM's extract useful information from HEC-RAS models in a structured format with token budget etc. 
# Templates will be used to help with this, based on the example projects (1D Steady, 1D Usteady, 1D Sediment Transport, 1D Water Quality, 2D Unsteady, 2D Steady, 2D Sediment Transport, 2D Water Quality, 2D Geospatial, 3D Unsteady, 3D Steady, 3D Sediment Transport, 3D Water Quality, 3D Geospatial).
# These will simply filter the data to only include the relevant information for the area of focus. 

#
# IDEAS
# 1. Package up a standard set of information for LLM analysis
#       - General project information
#       - Cross section information (for specific cross sections)
#       - Structure information (for specific structures)
#       - Include time series results and relevant HEC Guidance for LLM to reference

# 2. Use Library Assistant to call LLM 

==================================================

File: c:\GH\ras-commander\ras_commander\RasMapper.py
==================================================
"""
Class: RasMapper

List of Functions:
    get_raster_map(hdf_path: Path) 
    clip_raster_with_boundary(raster_path: Path, boundary_path: Path) 
    calculate_zonal_stats(boundary_path: Path, raster_data, transform, nodata) 

"""



from pathlib import Path
import pandas as pd
import geopandas as gpd
import rasterio
from rasterio.mask import mask
import h5py
from .Decorators import log_call, standardize_input
from .HdfInfiltration import HdfInfiltration

class RasMapper:
    """Class for handling RAS Mapper operations and data extraction"""

    @staticmethod
    @log_call
    def get_raster_map(hdf_path: Path) -> dict:
        """Read the raster map from HDF file and create value mapping
        
        Args:
            hdf_path: Path to the HDF file
            
        Returns:
            Dictionary mapping raster values to mukeys
        """
        with h5py.File(hdf_path, 'r') as hdf:
            raster_map_data = hdf['Raster Map'][:]
            return {int(item[0]): item[1].decode('utf-8') for item in raster_map_data}

    @staticmethod
    @log_call 
    def clip_raster_with_boundary:
    """Docs only, see 'clip_raster_with_boundary.py' for full function code"""
"""
# Initialize paths
raster_path = Path('input_files/gSSURGO_InfiltrationDC.tif')
boundary_path = Path('input_files/WF_Boundary_Simple.shp')
hdf_path = raster_path.with_suffix('.hdf')

# Get raster mapping
raster_map = RasMapper.get_raster_map(hdf_path)

# Clip raster with boundary
clipped_data, transform, nodata = RasMapper.clip_raster_with_boundary(
    raster_path, boundary_path)

# Calculate zonal statistics
stats = RasMapper.calculate_zonal_stats(
    boundary_path, clipped_data, transform, nodata)

# Calculate soil statistics
soil_stats = HdfInfiltration.calculate_soil_statistics(
    stats, raster_map)

# Get significant mukeys (>1%)
significant_mukeys = HdfInfiltration.get_significant_mukeys(
    soil_stats, threshold=1.0)
"""
==================================================

File: c:\GH\ras-commander\ras_commander\RasPlan.py
==================================================
"""
RasPlan - Operations for handling plan files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- set_geom(): Set the geometry for a specified plan
- set_steady(): Apply a steady flow file to a plan file
- set_unsteady(): Apply an unsteady flow file to a plan file
- set_num_cores(): Update the maximum number of cores to use
- set_geom_preprocessor(): Update geometry preprocessor settings
- clone_plan(): Create a new plan file based on a template
- clone_unsteady(): Copy unsteady flow files from a template
- clone_steady(): Copy steady flow files from a template
- clone_geom(): Copy geometry files from a template
- get_next_number(): Determine the next available number from a list
- get_plan_value(): Retrieve a specific value from a plan file
- get_results_path(): Get the results file path for a plan
- get_plan_path(): Get the full path for a plan number
- get_flow_path(): Get the full path for a flow number
- get_unsteady_path(): Get the full path for an unsteady number
- get_geom_path(): Get the full path for a geometry number
- update_run_flags(): Update various run flags in a plan file
- update_plan_intervals(): Update computation and output intervals
- update_plan_description(): Update the description in a plan file
- read_plan_description(): Read the description from a plan file
- update_simulation_date(): Update simulation start and end dates

        
"""
import os
import re
import logging
from pathlib import Path
import shutil
from typing import Union, Optional
import pandas as pd
from .RasPrj import RasPrj, ras
from .RasUtils import RasUtils
from pathlib import Path
from typing import Union, Any
from datetime import datetime

import logging
import re
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasPlan:
    """
    A class for operations on HEC-RAS plan files.
    """
    
    @staticmethod
    @log_call
    def set_geom(plan_number: Union[str, int], new_geom: Union[str, int], ras_object=None) -> pd.DataFrame:
        """
        Set the geometry for the specified plan.

        Parameters:
            plan_number (Union[str, int]): The plan number to update.
            new_geom (Union[str, int]): The new geometry number to set.
            ras_object: An optional RAS object instance.

        Returns:
            pd.DataFrame: The updated geometry DataFrame.

        Example:
            updated_geom_df = RasPlan.set_geom('02', '03')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Ensure plan_number and new_geom are strings
        plan_number = str(plan_number).zfill(2)
        new_geom = str(new_geom).zfill(2)

        # Before doing anything, make sure the plan, geom, flow, and unsteady dataframes are current
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        if new_geom not in ras_obj.geom_df['geom_number'].values:
            logger.error(f"Geometry {new_geom} not found in project.")
            raise ValueError(f"Geometry {new_geom} not found in project.")

        # Update the geometry for the specified plan
        ras_obj.plan_df.loc[ras_obj.plan_df['plan_number'] == plan_number, 'geom_number'] = new_geom

        logger.info(f"Geometry for plan {plan_number} set to {new_geom}")
        logger.debug("Updated plan DataFrame:")
        logger.debug(ras_obj.plan_df)

        # Update the project file
        prj_file_path = ras_obj.prj_file
        RasUtils.update_file(prj_file_path, RasPlan._update_geom_in_file, plan_number, new_geom)

        # Re-initialize the ras object to reflect changes
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        return ras_obj.plan_df

    @staticmethod
    def _update_geom_in_file:
    """Docs only, see '_update_geom_in_file.py' for full function code"""


==================================================

File: c:\GH\ras-commander\ras_commander\RasPrj.py
==================================================
"""
RasPrj.py - Manages HEC-RAS projects within the ras-commander library

This module provides a class for managing HEC-RAS projects.

Classes:
    RasPrj: A class for managing HEC-RAS projects.

Functions:
    init_ras_project: Initialize a RAS project.
    get_ras_exe: Determine the HEC-RAS executable path based on the input.

DEVELOPER NOTE:
This class is used to initialize a RAS project and is used in conjunction with the RasCmdr class to manage the execution of RAS plans.
By default, the RasPrj class is initialized with the global 'ras' object.
However, you can create multiple RasPrj instances to manage multiple projects.
Do not mix and match global 'ras' object instances and custom instances of RasPrj - it will cause errors.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- initialize()
- _load_project_data()
- _get_geom_file_for_plan()
- _parse_plan_file()
- _parse_unsteady_file()
- _get_prj_entries()
- _parse_boundary_condition()
- is_initialized (property)
- check_initialized()
- find_ras_prj()
- get_project_name()
- get_prj_entries()
- get_plan_entries()
- get_flow_entries()
- get_unsteady_entries()
- get_geom_entries()
- get_hdf_entries()
- print_data()
- get_plan_value()
- get_boundary_conditions()
        
Functions in RasPrj that are not part of the class:        
- init_ras_project()
- get_ras_exe()

        
        
        
"""
import os
import re
from pathlib import Path
import pandas as pd
from typing import Union, Any, List, Dict, Tuple
import logging
from ras_commander.LoggingConfig import get_logger
from ras_commander.Decorators import log_call

logger = get_logger(__name__)

class RasPrj:
    
    def __init__:
    """Docs only, see '__init__.py' for full function code"""
# Defining the global instance allows the init_ras_project function to initialize the project.
# This only happens on the library initialization, not when the user calls init_ras_project.
ras = RasPrj()

# END OF CLASS DEFINITION


# START OF FUNCTION DEFINITIONS


@log_call
def init_ras_project:
    """Docs only, see 'init_ras_project.py' for full function code"""
def get_ras_exe:
    """Docs only, see 'get_ras_exe.py' for full function code"""

==================================================

File: c:\GH\ras-commander\ras_commander\RasToGo.py
==================================================
"""
RasToGo module provides functions to interface HEC-RAS with go-consequences.
This module helps prepare and format RAS data for use with go-consequences.


-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasToGo:

TO BE IMPLEMENTED: 
- Adding stored maps in rasmaapper for a results file
- Editing the terrain name for stored maps, so that a reduced resolution terrain can be used for mapping
- Re-computing specific plans using the floodplain mapping option to generate stored maps
- Using the stored map output to call go-consequences and compute damages
- Comparisons of go-consequences outputs based on RAS plan number
    - include optional argument with polygons defining areas of interest

"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
import pandas as pd
import numpy as np

from .Decorators import log_call, standardize_input
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class RasToGo:
    """Class containing functions to interface HEC-RAS with go-consequences."""

    #@staticmethod
    #@log_call 
==================================================

File: c:\GH\ras-commander\ras_commander\RasUnsteady.py
==================================================
"""
RasUnsteady - Operations for handling unsteady flow files in HEC-RAS projects.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- update_flow_title()
- update_restart_settings()
- extract_boundary_and_tables()
- print_boundaries_and_tables()
- identify_tables()
- parse_fixed_width_table()
- extract_tables()
- write_table_to_file()
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call
import pandas as pd
import numpy as np
import re
from typing import Union, Optional, Any, Tuple, Dict, List



logger = get_logger(__name__)

# Module code starts here

class RasUnsteady:
    """
    Class for all operations related to HEC-RAS unsteady flow files.
    """
    @staticmethod
    @log_call
    def update_flow_title(unsteady_file: str, new_title: str, ras_object: Optional[Any] = None) -> None:
        """
        Update the Flow Title in an unsteady flow file.
        
        Parameters:
        unsteady_file (str): Full path to the unsteady flow file
        new_title (str): New flow title (max 24 characters)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Note:
            This function updates the ras object's unsteady dataframe after modifying the unsteady flow file.
        
        Example:
            from ras_commander import RasCmdr
            
            # Initialize RAS project
            ras_cmdr = RasCmdr()
            ras_cmdr.init_ras_project(project_folder, ras_version)
            
            # Update flow title
            unsteady_file = r"path/to/unsteady_file.u01"
            new_title = "New Flow Title"
            RasUnsteady.update_flow_title(unsteady_file, new_title, ras_object=ras_cmdr.ras)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        new_title = new_title[:24]  # Truncate to 24 characters if longer
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        for i, line in enumerate(lines):
            if line.startswith("Flow Title="):
                old_title = line.strip().split('=')[1]
                lines[i] = f"Flow Title={new_title}\n"
                updated = True
                logger.info(f"Updated Flow Title from '{old_title}' to '{new_title}'")
                break
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied Flow Title modification to {unsteady_file}")
        else:
            logger.warning(f"Flow Title not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def update_restart_settings(unsteady_file: str, use_restart: bool, restart_filename: Optional[str] = None, ras_object: Optional[Any] = None) -> None:
        """
        Update the Use Restart settings in an unsteady flow file.
        
        Parameters:
        unsteady_file (str): Full path to the unsteady flow file
        use_restart (bool): Whether to use restart (True) or not (False)
        restart_filename (str, optional): Name of the restart file (required if use_restart is True)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Note:
            This function updates the ras object's unsteady dataframe after modifying the unsteady flow file.
        
        Example:
            from ras_commander import RasCmdr
            
            # Initialize RAS project
            ras_cmdr = RasCmdr()
            ras_cmdr.init_ras_project(project_folder, ras_version)
            
            # Update restart settings
            unsteady_file = r"path/to/unsteady_file.u01"
            RasUnsteady.update_restart_settings(unsteady_file, True, "restartfile.rst", ras_object=ras_cmdr.ras)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        restart_line_index = None
        for i, line in enumerate(lines):
            if line.startswith("Use Restart="):
                restart_line_index = i
                old_value = line.strip().split('=')[1]
                new_value = "-1" if use_restart else "0"
                lines[i] = f"Use Restart={new_value}\n"
                updated = True
                logger.info(f"Updated Use Restart from {old_value} to {new_value}")
                break
        
        if use_restart:
            if not restart_filename:
                logger.error("Restart filename must be specified when enabling restart.")
                raise ValueError("Restart filename must be specified when enabling restart.")
            if restart_line_index is not None:
                lines.insert(restart_line_index + 1, f"Restart Filename={restart_filename}\n")
                logger.info(f"Added Restart Filename: {restart_filename}")
            else:
                logger.warning("Could not find 'Use Restart' line to insert 'Restart Filename'")
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied restart settings modification to {unsteady_file}")
        else:
            logger.warning(f"Use Restart setting not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def extract_boundary_and_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> pd.DataFrame:
        """
        Extracts Boundary Location blocks, DSS File entries, and their associated 
        fixed-width tables from the specified unsteady file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        table_types = [
            'Flow Hydrograph=', 
            'Gate Openings=', 
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=', 
            'Lateral Inflow Hydrograph='
        ]
        
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Initialize variables
        boundary_data = []
        current_boundary = None
        current_tables = {}
        current_table = None
        table_values = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # Check for Boundary Location line
            if line.startswith("Boundary Location="):
                # Save previous boundary if it exists
                if current_boundary is not None:
                    if current_table and table_values:
                        # Process any remaining table
                        try:
                            df = pd.DataFrame({'Value': table_values})
                            current_tables[current_table_name] = df
                        except Exception as e:
                            logger.warning(f"Error processing table {current_table_name}: {e}")
                    current_boundary['Tables'] = current_tables
                    boundary_data.append(current_boundary)
                
                # Start new boundary
                current_boundary = {
                    'Boundary Location': line.split('=', 1)[1].strip(),
                    'DSS File': '',
                    'Tables': {}
                }
                current_tables = {}
                current_table = None
                table_values = []
                
            # Check for DSS File line
            elif line.startswith("DSS File=") and current_boundary is not None:
                current_boundary['DSS File'] = line.split('=', 1)[1].strip()
                
            # Check for table headers
            elif any(line.startswith(t) for t in table_types) and current_boundary is not None:
                # If we were processing a table, save it
                if current_table and table_values:
                    try:
                        df = pd.DataFrame({'Value': table_values})
                        current_tables[current_table_name] = df
                    except Exception as e:
                        logger.warning(f"Error processing previous table: {e}")
                
                # Start new table
                try:
                    current_table = line.split('=')
                    current_table_name = current_table[0].strip()
                    num_values = int(current_table[1])
                    table_values = []
                    
                    # Read the table values
                    rows_needed = (num_values + 9) // 10  # Round up division
                    for _ in range(rows_needed):
                        i += 1
                        if i >= len(lines):
                            break
                        row = lines[i].strip()
                        # Parse fixed-width values (8 characters each)
                        j = 0
                        while j < len(row):
                            value_str = row[j:j+8].strip()
                            if value_str:
                                try:
                                    value = float(value_str)
                                    table_values.append(value)
                                except ValueError:
                                    # Try splitting merged values
                                    parts = re.findall(r'-?\d+\.?\d*', value_str)
                                    table_values.extend([float(p) for p in parts])
                            j += 8
                
                except (ValueError, IndexError) as e:
                    logger.error(f"Error processing table at line {i}: {e}")
                    current_table = None
                    
            i += 1
        
        # Add the last boundary if it exists
        if current_boundary is not None:
            if current_table and table_values:
                try:
                    df = pd.DataFrame({'Value': table_values})
                    current_tables[current_table_name] = df
                except Exception as e:
                    logger.warning(f"Error processing final table: {e}")
            current_boundary['Tables'] = current_tables
            boundary_data.append(current_boundary)
        
        # Create DataFrame
        boundaries_df = pd.DataFrame(boundary_data)
        if not boundaries_df.empty:
            # Split boundary location into components
            location_columns = ['River Name', 'Reach Name', 'River Station', 
                              'Downstream River Station', 'Storage Area Connection',
                              'Storage Area Name', 'Pump Station Name', 
                              'Blank 1', 'Blank 2']
            split_locations = boundaries_df['Boundary Location'].str.split(',', expand=True)
            # Ensure we have the right number of columns
            for i, col in enumerate(location_columns):
                if i < split_locations.shape[1]:
                    boundaries_df[col] = split_locations[i].str.strip()
                else:
                    boundaries_df[col] = ''
            boundaries_df = boundaries_df.drop(columns=['Boundary Location'])
        
        logger.info(f"Successfully extracted boundaries and tables from {unsteady_path}")
        return boundaries_df

    @staticmethod
    @log_call
    def print_boundaries_and_tables(boundaries_df: pd.DataFrame) -> None:
        """
        Prints the boundaries and their associated tables from the extracted DataFrame.
        
        Parameters:
        - boundaries_df: DataFrame containing boundary information and nested tables data
        """
        pd.set_option('display.max_columns', None)
        pd.set_option('display.max_rows', None)
        print("\nBoundaries and Tablesin boundaries_df:")
        for idx, row in boundaries_df.iterrows():
            print(f"\nBoundary {idx+1}:")
            print(f"River Name: {row['River Name']}")
            print(f"Reach Name: {row['Reach Name']}")
            print(f"River Station: {row['River Station']}")
            print(f"DSS File: {row['DSS File']}")
            
            if row['Tables']:
                print("\nTables for this boundary:")
                for table_name, table_df in row['Tables'].items():
                    print(f"\n{table_name}:")
                    print(table_df.to_string())
            print("-" * 80)





# Additional functions from the AWS webinar where the code was developed
# Need to add examples

    @staticmethod
    @log_call
    def identify_tables(lines: List[str]) -> List[Tuple[str, int, int]]:
        """
        Identify the start and end of each table in the unsteady flow file.
        
        Parameters:
        lines (List[str]): List of file lines
        
        Returns:
        List[Tuple[str, int, int]]: List of tuples containing (table_name, start_line, end_line)
        """
        table_types = [
            'Flow Hydrograph=', 
            'Gate Openings=', 
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=', 
            'Lateral Inflow Hydrograph='
        ]
        tables = []
        current_table = None
        
        for i, line in enumerate(lines):
            if any(table_type in line for table_type in table_types):
                if current_table:
                    tables.append((current_table[0], current_table[1], i-1))
                table_name = line.strip().split('=')[0] + '='
                try:
                    num_values = int(line.strip().split('=')[1])
                    current_table = (table_name, i+1, num_values)
                except (ValueError, IndexError) as e:
                    logger.error(f"Error parsing table header at line {i}: {e}")
                    continue
        
        if current_table:
            tables.append((current_table[0], current_table[1], 
                          current_table[1] + (current_table[2] + 9) // 10))
        
        logger.debug(f"Identified {len(tables)} tables in the file")
        return tables

    @staticmethod
    @log_call
    def parse_fixed_width_table(lines: List[str], start: int, end: int) -> pd.DataFrame:
        """
        Parse a fixed-width table into a pandas DataFrame.
        
        Parameters:
        lines (List[str]): List of file lines
        start (int): Starting line number for table
        end (int): Ending line number for table
        
        Returns:
        pd.DataFrame: DataFrame containing parsed table values
        """
        data = []
        for line in lines[start:end]:
            # Split the line into 8-character columns
            values = [line[i:i+8].strip() for i in range(0, len(line), 8)]
            # Convert to float and handle cases where values are run together
            parsed_values = []
            for value in values:
                try:
                    if len(value) > 8:  # If values are run together
                        parts = re.findall(r'-?\d+\.?\d*', value)
                        parsed_values.extend([float(p) for p in parts])
                    elif value:  # Only add non-empty values
                        parsed_values.append(float(value))
                except ValueError as e:
                    logger.warning(f"Could not parse value '{value}': {e}")
                    continue
            data.extend(parsed_values)
        
        return pd.DataFrame(data, columns=['Value'])

    @staticmethod
    @log_call
    def extract_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> Dict[str, pd.DataFrame]:
        """
        Extract all tables from the unsteady file and return them as DataFrames.
        
        Parameters:
        unsteady_file (str): Path to the unsteady flow file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        Dict[str, pd.DataFrame]: Dictionary of table names to DataFrames
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        tables = RasBndry.identify_tables(lines)
        extracted_tables = {}
        
        for table_name, start, end in tables:
            df = RasBndry.parse_fixed_width_table(lines, start, end)
            extracted_tables[table_name] = df
            logger.debug(f"Extracted table '{table_name}' with {len(df)} values")
        
        return extracted_tables

    @staticmethod
    @log_call
    def write_table_to_file(unsteady_file: str, table_name: str, df: pd.DataFrame, 
                           start_line: int, ras_object: Optional[Any] = None) -> None:
        """
        Write updated table back to file in fixed-width format.
        
        Parameters:
        unsteady_file (str): Path to the unsteady flow file
        table_name (str): Name of the table to update
        df (pd.DataFrame): DataFrame containing the updated values
        start_line (int): Line number where the table starts
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Format values into fixed-width strings
        formatted_values = []
        for i in range(0, len(df), 10):
            row = df['Value'].iloc[i:i+10]
            formatted_row = ''.join(f'{value:8.0f}' for value in row)
            formatted_values.append(formatted_row + '\n')
        
        # Replace old table with new formatted values
        lines[start_line:start_line+len(formatted_values)] = formatted_values
        
        try:
            with open(unsteady_path, 'w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated table '{table_name}' in {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            raise
        except IOError as e:
            logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            raise








'''



Flow Title=Single 2D Area with Bridges
Program Version=6.60
Use Restart= 0 
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DSNormalDepth                   ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalD                      ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,Upstream Inflow                 ,                                
Interval=1HOUR
Flow Hydrograph= 200 
    1000    3000    6500    8000    9500   11000   12500   14000   15500   17000
   18500   20000   22000   24000   26000   28000   30000   34000   38000   42000
   46000   50000   54000   58000   62000   66000   70000   73000   76000   79000
   82000   85000   87200   89400   91600   93800   96000   96800   97600   98400
   99200  100000   99600   99200   98800   98400   98000   96400   94800   93200
   91600   90000   88500   87000   85500   84000   82500   81000   79500   78000
   76500   75000   73500   7200070666.6669333.34   6800066666.6665333.33   64000
62666.6761333.33   6000058666.6757333.33   5600054666.6753333.33   5200050666.67
49333.33   4800046666.6745333.33   4400042666.6741333.33   4000039166.6738333.33
   3750036666.6735833.33   3500034166.6733333.33   3250031666.6730833.33   30000
29166.6728333.33   2750026666.6725833.33   2500024166.6723333.33   2250021666.67
20833.33   2000019655.1719310.3518965.5218620.6918275.8617931.0417586.2117241.38
16896.5516551.72 16206.915862.0715517.2415172.4114827.5914482.7614137.93 13793.1
13448.2813103.4512758.6212413.7912068.9711724.1411379.3111034.4810689.6610344.83
   10000 9915.25 9830.51 9745.76 9661.02 9576.27 9491.53 9406.78 9322.03 9237.29
 9152.54  9067.8 8983.05 8898.31 8813.56 8728.81 8644.07 8559.32 8474.58 8389.83
 8305.09 8220.34 8135.59 8050.85  7966.1 7881.36 7796.61 7711.86 7627.12 7542.37
 7457.63 7372.88 7288.14 7203.39 7118.64  7033.9 6949.15 6864.41 6779.66 6694.92
 6610.17 6525.42 6440.68 6355.93 6271.19 6186.44  6101.7 6016.95  5932.2 5847.46
 5762.71 5677.97 5593.22 5508.48 5423.73 5338.98 5254.24 5169.49 5084.75    5000
Stage Hydrograph TW Check=0
Flow Hydrograph QMult= 0.5 
Flow Hydrograph Slope= 0.0005 
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,Sayers Dam      ,                ,                ,                                ,                                
Gate Name=Gate #1     
Gate DSS Path=
Gate Use DSS=False
Gate Time Interval=1HOUR
Gate Use Fixed Start Time=False
Gate Fixed Start Date/Time=,
Gate Openings= 100 
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalDepth                  ,                                
Friction Slope=0.0003,0
Met Point Raster Parameters=,,,,
Precipitation Mode=Disable
Wind Mode=No Wind Forces
Air Density Mode=
Wave Mode=No Wave Forcing
Met BC=Precipitation|Expanded View=0
Met BC=Precipitation|Point Interpolation=Nearest
Met BC=Precipitation|Gridded Source=DSS
Met BC=Precipitation|Gridded Interpolation=
Met BC=Evapotranspiration|Expanded View=0
Met BC=Evapotranspiration|Point Interpolation=Nearest
Met BC=Evapotranspiration|Gridded Source=DSS
Met BC=Evapotranspiration|Gridded Interpolation=
Met BC=Wind Speed|Expanded View=0
Met BC=Wind Speed|Constant Units=ft/s
Met BC=Wind Speed|Point Interpolation=Nearest
Met BC=Wind Speed|Gridded Source=DSS
Met BC=Wind Speed|Gridded Interpolation=
Met BC=Wind Direction|Expanded View=0
Met BC=Wind Direction|Point Interpolation=Nearest
Met BC=Wind Direction|Gridded Source=DSS
Met BC=Wind Direction|Gridded Interpolation=
Met BC=Wind Velocity X|Expanded View=0
Met BC=Wind Velocity X|Constant Units=ft/s
Met BC=Wind Velocity X|Point Interpolation=Nearest
Met BC=Wind Velocity X|Gridded Source=DSS
Met BC=Wind Velocity X|Gridded Interpolation=
Met BC=Wind Velocity Y|Expanded View=0
Met BC=Wind Velocity Y|Constant Units=ft/s
Met BC=Wind Velocity Y|Point Interpolation=Nearest
Met BC=Wind Velocity Y|Gridded Source=DSS
Met BC=Wind Velocity Y|Gridded Interpolation=
Met BC=Wave Forcing X|Expanded View=0
Met BC=Wave Forcing X|Point Interpolation=Nearest
Met BC=Wave Forcing X|Gridded Source=DSS
Met BC=Wave Forcing X|Gridded Interpolation=
Met BC=Wave Forcing Y|Expanded View=0
Met BC=Wave Forcing Y|Point Interpolation=Nearest
Met BC=Wave Forcing Y|Gridded Source=DSS
Met BC=Wave Forcing Y|Gridded Interpolation=
Met BC=Air Density|Mode=Constant
Met BC=Air Density|Expanded View=0
Met BC=Air Density|Constant Value=1.225
Met BC=Air Density|Constant Units=kg/m3
Met BC=Air Density|Point Interpolation=Nearest
Met BC=Air Density|Gridded Source=DSS
Met BC=Air Density|Gridded Interpolation=
Met BC=Air Temperature|Expanded View=0
Met BC=Air Temperature|Point Interpolation=Nearest
Met BC=Air Temperature|Gridded Source=DSS
Met BC=Air Temperature|Gridded Interpolation=
Met BC=Humidity|Expanded View=0
Met BC=Humidity|Point Interpolation=Nearest
Met BC=Humidity|Gridded Source=DSS
Met BC=Humidity|Gridded Interpolation=
Met BC=Air Pressure|Mode=Constant
Met BC=Air Pressure|Expanded View=0
Met BC=Air Pressure|Constant Value=1013.2
Met BC=Air Pressure|Constant Units=mb
Met BC=Air Pressure|Point Interpolation=Nearest
Met BC=Air Pressure|Gridded Source=DSS
Met BC=Air Pressure|Gridded Interpolation=
Non-Newtonian Method= 0 , 
Non-Newtonian Constant Vol Conc=0
Non-Newtonian Yield Method= 0 , 
Non-Newtonian Yield Coef=0, 0
User Yeild=   0
Non-Newtonian Sed Visc= 0 , 
Non-Newtonian Obrian B=0
User Viscosity=0
User Viscosity Ratio=0
Herschel-Bulkley Coef=0, 0
Clastic Method= 0 , 
Coulomb Phi=0
Voellmy X=0
Non-Newtonian Hindered FV= 0 
Non-Newtonian FV K=0
Non-Newtonian ds=0
Non-Newtonian Max Cv=0
Non-Newtonian Bulking Method= 0 , 
Non-Newtonian High C Transport= 0 , 
Lava Activation= 0 
Temperature=1300,15,,15,14,980
Heat Ballance=1,1200,0.5,1,70,0.95
Viscosity=1000,,,
Yield Strength=,,,
Consistency Factor=,,,
Profile Coefficient=4,1.3,
Lava Param=,2500,




'''





==================================================

File: c:\GH\ras-commander\ras_commander\RasUtils.py
==================================================
"""
RasUtils - Utility functions for the ras-commander library

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function:
    """Docs only, see 'my_function.py' for full function code"""
- create_directory()
- find_files_by_extension()
- get_file_size()
- get_file_modification_time()
- get_plan_path()
- remove_with_retry()
- update_plan_file()
- check_file_access()
- convert_to_dataframe()
- save_to_excel()
- calculate_rmse()
- calculate_percent_bias()
- calculate_error_metrics()
- update_file()
- get_next_number()
- clone_file()
- update_project_file()
- decode_byte_strings()
- perform_kdtree_query()
- find_nearest_neighbors()
- consolidate_dataframe()
- find_nearest_value()
- horizontal_distance()
    
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from typing import Union, Optional, Dict, Callable, List, Tuple, Any
import pandas as pd
import numpy as np
import shutil
import re
from scipy.spatial import KDTree
import datetime
import time
import h5py
from datetime import timedelta
from .LoggingConfig import get_logger
from .Decorators import log_call


logger = get_logger(__name__)
# Module code starts here

class RasUtils:
    """
    A class containing utility functions for the ras-commander library.
    When integrating new functions that do not clearly fit into other classes, add them here.
    """

    @staticmethod
    @log_call
    def create_directory(directory_path: Path, ras_object=None) -> Path:
        """
        Ensure that a directory exists, creating it if necessary.

        Parameters:
        directory_path (Path): Path to the directory
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Path to the ensured directory

        Example:
        >>> ensured_dir = RasUtils.create_directory(Path("output"))
        >>> print(f"Directory ensured: {ensured_dir}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(directory_path)
        try:
            path.mkdir(parents=True, exist_ok=True)
            logger.info(f"Directory ensured: {path}")
        except Exception as e:
            logger.error(f"Failed to create directory {path}: {e}")
            raise
        return path

    @staticmethod
    @log_call
    def find_files_by_extension(extension: str, ras_object=None) -> list:
        """
        List all files in the project directory with a specific extension.

        Parameters:
        extension (str): File extension to filter (e.g., '.prj')
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        list: List of file paths matching the extension

        Example:
        >>> prj_files = RasUtils.find_files_by_extension('.prj')
        >>> print(f"Found {len(prj_files)} .prj files")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            files = list(ras_obj.project_folder.glob(f"*{extension}"))
            file_list = [str(file) for file in files]
            logger.info(f"Found {len(file_list)} files with extension '{extension}' in {ras_obj.project_folder}")
            return file_list
        except Exception as e:
            logger.error(f"Failed to find files with extension '{extension}': {e}")
            raise

    @staticmethod
    @log_call
    def get_file_size(file_path: Path, ras_object=None) -> Optional[int]:
        """
        Get the size of a file in bytes.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[int]: Size of the file in bytes, or None if the file does not exist

        Example:
        >>> size = RasUtils.get_file_size(Path("project.prj"))
        >>> print(f"File size: {size} bytes")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                size = path.stat().st_size
                logger.info(f"Size of {path}: {size} bytes")
                return size
            except Exception as e:
                logger.error(f"Failed to get size for {path}: {e}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def get_file_modification_time(file_path: Path, ras_object=None) -> Optional[float]:
        """
        Get the last modification time of a file.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[float]: Last modification time as a timestamp, or None if the file does not exist

        Example:
        >>> mtime = RasUtils.get_file_modification_time(Path("project.prj"))
        >>> print(f"Last modified: {mtime}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                mtime = path.stat().st_mtime
                logger.info(f"Last modification time of {path}: {mtime}")
                return mtime
            except Exception as e:
                logger.exception(f"Failed to get modification time for {path}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def get_plan_path(current_plan_number_or_path: Union[str, Path], ras_object=None) -> Path:
        """
        Get the path for a plan file with a given plan number or path.

        Parameters:
        current_plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Full path to the plan file

        Example:
        >>> plan_path = RasUtils.get_plan_path(1)
        >>> print(f"Plan file path: {plan_path}")
        >>> plan_path = RasUtils.get_plan_path("path/to/plan.p01")
        >>> print(f"Plan file path: {plan_path}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        plan_path = Path(current_plan_number_or_path)
        if plan_path.is_file():
            logger.info(f"Using provided plan file path: {plan_path}")
            return plan_path
        
        try:
            current_plan_number = f"{int(current_plan_number_or_path):02d}"  # Ensure two-digit format
            logger.debug(f"Converted plan number to two-digit format: {current_plan_number}")
        except ValueError:
            logger.error(f"Invalid plan number: {current_plan_number_or_path}. Expected a number from 1 to 99.")
            raise ValueError(f"Invalid plan number: {current_plan_number_or_path}. Expected a number from 1 to 99.")
        
        plan_name = f"{ras_obj.project_name}.p{current_plan_number}"
        full_plan_path = ras_obj.project_folder / plan_name
        logger.info(f"Constructed plan file path: {full_plan_path}")
        return full_plan_path

    @staticmethod
    @log_call
    def remove_with_retry(
        path: Path,
        max_attempts: int = 5,
        initial_delay: float = 1.0,
        is_folder: bool = True,
        ras_object=None
    ) -> bool:
        """
        Attempts to remove a file or folder with retry logic and exponential backoff.

        Parameters:
        path (Path): Path to the file or folder to be removed.
        max_attempts (int): Maximum number of removal attempts.
        initial_delay (float): Initial delay between attempts in seconds.
        is_folder (bool): If True, the path is treated as a folder; if False, it's treated as a file.
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        bool: True if the file or folder was successfully removed, False otherwise.

        Example:
        >>> success = RasUtils.remove_with_retry(Path("temp_folder"), is_folder=True)
        >>> print(f"Removal successful: {success}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        path = Path(path)
        for attempt in range(1, max_attempts + 1):
            try:
                if path.exists():
                    if is_folder:
                        shutil.rmtree(path)
                        logger.info(f"Folder removed: {path}")
                    else:
                        path.unlink()
                        logger.info(f"File removed: {path}")
                else:
                    logger.info(f"Path does not exist, nothing to remove: {path}")
                return True
            except PermissionError as pe:
                if attempt < max_attempts:
                    delay = initial_delay * (2 ** (attempt - 1))  # Exponential backoff
                    logger.warning(
                        f"PermissionError on attempt {attempt} to remove {path}: {pe}. "
                        f"Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(
                        f"Failed to remove {path} after {max_attempts} attempts due to PermissionError: {pe}. Skipping."
                    )
                    return False
            except Exception as e:
                logger.exception(f"Failed to remove {path} on attempt {attempt}")
                return False
        return False

    @staticmethod
    @log_call
    def update_plan_file(
        plan_number_or_path: Union[str, Path],
        file_type: str,
        entry_number: int,
        ras_object=None
    ) -> None:
        """
        Update a plan file with a new file reference.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        file_type (str): Type of file to update ('Geom', 'Flow', or 'Unsteady')
        entry_number (int): Number (from 1 to 99) to set
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Raises:
        ValueError: If an invalid file_type is provided
        FileNotFoundError: If the plan file doesn't exist

        Example:
        >>> RasUtils.update_plan_file(1, "Geom", 2)
        >>> RasUtils.update_plan_file("path/to/plan.p01", "Geom", 2)
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        valid_file_types = {'Geom': 'g', 'Flow': 'f', 'Unsteady': 'u'}
        if file_type not in valid_file_types:
            logger.error(
                f"Invalid file_type '{file_type}'. Expected one of: {', '.join(valid_file_types.keys())}"
            )
            raise ValueError(
                f"Invalid file_type. Expected one of: {', '.join(valid_file_types.keys())}"
            )

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise FileNotFoundError(f"Plan file not found: {plan_file_path}")
        
        file_prefix = valid_file_types[file_type]
        search_pattern = f"{file_type} File="
        formatted_entry_number = f"{int(entry_number):02d}"  # Ensure two-digit format

        try:
            RasUtils.check_file_access(plan_file_path, 'r')
            with plan_file_path.open('r') as file:
                lines = file.readlines()
        except Exception as e:
            logger.exception(f"Failed to read plan file {plan_file_path}")
            raise

        updated = False
        for i, line in enumerate(lines):
            if line.startswith(search_pattern):
                lines[i] = f"{search_pattern}{file_prefix}{formatted_entry_number}\n"
                logger.info(
                    f"Updated {file_type} File in {plan_file_path} to {file_prefix}{formatted_entry_number}"
                )
                updated = True
                break

        if not updated:
            logger.warning(
                f"Search pattern '{search_pattern}' not found in {plan_file_path}. No update performed."
            )

        try:
            with plan_file_path.open('w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated plan file: {plan_file_path}")
        except Exception as e:
            logger.exception(f"Failed to write updates to plan file {plan_file_path}")
            raise

        # Refresh RasPrj dataframes
        try:
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            logger.info("RAS object dataframes have been refreshed.")
        except Exception as e:
            logger.exception("Failed to refresh RasPrj dataframes")
            raise

    @staticmethod
    @log_call
    def check_file_access(file_path: Path, mode: str = 'r') -> None:
        """
        Check if the file can be accessed with the specified mode.

        Parameters:
        file_path (Path): Path to the file
        mode (str): Mode to check ('r' for read, 'w' for write, etc.)

        Raises:
        FileNotFoundError: If the file does not exist
        PermissionError: If the required permissions are not met
        """
        
        path = Path(file_path)
        if not path.exists():
            logger.error(f"File not found: {file_path}")
            raise FileNotFoundError(f"File not found: {file_path}")
        
        if mode in ('r', 'rb'):
            if not os.access(path, os.R_OK):
                logger.error(f"Read permission denied for file: {file_path}")
                raise PermissionError(f"Read permission denied for file: {file_path}")
            else:
                logger.debug(f"Read access granted for file: {file_path}")
        
        if mode in ('w', 'wb', 'a', 'ab'):
            parent_dir = path.parent
            if not os.access(parent_dir, os.W_OK):
                logger.error(f"Write permission denied for directory: {parent_dir}")
                raise PermissionError(f"Write permission denied for directory: {parent_dir}")
            else:
                logger.debug(f"Write access granted for directory: {parent_dir}")


    @staticmethod
    @log_call
    def convert_to_dataframe(data_source: Union[pd.DataFrame, Path], **kwargs) -> pd.DataFrame:
        """
        Converts input to a pandas DataFrame. Supports existing DataFrames or file paths (CSV, Excel, TSV, Parquet).

        Args:
            data_source (Union[pd.DataFrame, Path]): The input to convert to a DataFrame. Can be a file path or an existing DataFrame.
            **kwargs: Additional keyword arguments to pass to pandas read functions.

        Returns:
            pd.DataFrame: The resulting DataFrame.

        Raises:
            NotImplementedError: If the file type is unsupported or input type is invalid.

        Example:
            >>> df = RasUtils.convert_to_dataframe(Path("data.csv"))
            >>> print(type(df))
            <class 'pandas.core.frame.DataFrame'>
        """
        if isinstance(data_source, pd.DataFrame):
            logger.debug("Input is already a DataFrame, returning a copy.")
            return data_source.copy()
        elif isinstance(data_source, Path):
            ext = data_source.suffix.replace('.', '', 1)
            logger.info(f"Converting file with extension '{ext}' to DataFrame.")
            if ext == 'csv':
                return pd.read_csv(data_source, **kwargs)
            elif ext.startswith('x'):
                return pd.read_excel(data_source, **kwargs)
            elif ext == "tsv":
                return pd.read_csv(data_source, sep="\t", **kwargs)
            elif ext in ["parquet", "pq", "parq"]:
                return pd.read_parquet(data_source, **kwargs)
            else:
                logger.error(f"Unsupported file type: {ext}")
                raise NotImplementedError(f"Unsupported file type {ext}. Should be one of csv, tsv, parquet, or xlsx.")
        else:
            logger.error(f"Unsupported input type: {type(data_source)}")
            raise NotImplementedError(f"Unsupported type {type(data_source)}. Only file path / existing DataFrame supported at this time")

    @staticmethod
    @log_call
    def save_to_excel(dataframe: pd.DataFrame, excel_path: Path, **kwargs) -> None:
        """
        Saves a pandas DataFrame to an Excel file with retry functionality.

        Args:
            dataframe (pd.DataFrame): The DataFrame to save.
            excel_path (Path): The path to the Excel file where the DataFrame will be saved.
            **kwargs: Additional keyword arguments passed to `DataFrame.to_excel()`.

        Raises:
            IOError: If the file cannot be saved after multiple attempts.

        Example:
            >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
            >>> RasUtils.save_to_excel(df, Path('output.xlsx'))
        """
        saved = False
        max_attempts = 3
        attempt = 0

        while not saved and attempt < max_attempts:
            try:
                dataframe.to_excel(excel_path, **kwargs)
                logger.info(f'DataFrame successfully saved to {excel_path}')
                saved = True
            except IOError as e:
                attempt += 1
                if attempt < max_attempts:
                    logger.warning(f"Error saving file. Attempt {attempt} of {max_attempts}. Please close the Excel document if it's open.")
                else:
                    logger.error(f"Failed to save {excel_path} after {max_attempts} attempts.")
                    raise IOError(f"Failed to save {excel_path} after {max_attempts} attempts. Last error: {str(e)}")

    @staticmethod
    @log_call
    def calculate_rmse(observed_values: np.ndarray, predicted_values: np.ndarray, normalized: bool = True) -> float:
        """
        Calculate the Root Mean Squared Error (RMSE) between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            normalized (bool, optional): Whether to normalize RMSE to a percentage of observed_values. Defaults to True.

        Returns:
            float: The calculated RMSE value.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_rmse(observed, predicted)
            0.06396394
        """
        rmse = np.sqrt(np.mean((predicted_values - observed_values) ** 2))
        
        if normalized:
            rmse = rmse / np.abs(np.mean(observed_values))
        
        logger.debug(f"Calculated RMSE: {rmse}")
        return rmse

    @staticmethod
    @log_call
    def calculate_percent_bias(observed_values: np.ndarray, predicted_values: np.ndarray, as_percentage: bool = False) -> float:
        """
        Calculate the Percent Bias between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            as_percentage (bool, optional): If True, return bias as a percentage. Defaults to False.

        Returns:
            float: The calculated Percent Bias.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_percent_bias(observed, predicted, as_percentage=True)
            3.33333333
        """
        multiplier = 100 if as_percentage else 1
        
        percent_bias = multiplier * (np.mean(predicted_values) - np.mean(observed_values)) / np.mean(observed_values)
        
        logger.debug(f"Calculated Percent Bias: {percent_bias}")
        return percent_bias

    @staticmethod
    @log_call
    def calculate_error_metrics(observed_values: np.ndarray, predicted_values: np.ndarray) -> Dict[str, float]:
        """
        Compute a trio of error metrics: correlation, RMSE, and Percent Bias.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.

        Returns:
            Dict[str, float]: A dictionary containing correlation ('cor'), RMSE ('rmse'), and Percent Bias ('pb').

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_error_metrics(observed, predicted)
            {'cor': 0.9993, 'rmse': 0.06396, 'pb': 0.03333}
        """
        correlation = np.corrcoef(observed_values, predicted_values)[0, 1]
        rmse = RasUtils.calculate_rmse(observed_values, predicted_values)
        percent_bias = RasUtils.calculate_percent_bias(observed_values, predicted_values)
        
        metrics = {'cor': correlation, 'rmse': rmse, 'pb': percent_bias}
        logger.info(f"Calculated error metrics: {metrics}")
        return metrics

    
    @staticmethod
    @log_call
    def update_file(file_path: Path, update_function: Callable, *args) -> None:
        """
        Generic method to update a file.

        Parameters:
        file_path (Path): Path to the file to be updated
        update_function (Callable): Function to update the file contents
        *args: Additional arguments to pass to the update_function

        Raises:
        Exception: If there's an error updating the file

        Example:
        >>> def update_content:
    """Docs only, see 'update_content.py' for full function code"""
    
==================================================

File: c:\GH\ras-commander\ras_commander\__init__.py
==================================================
from importlib.metadata import version, PackageNotFoundError
from .LoggingConfig import setup_logging, get_logger
from .Decorators import log_call, standardize_input

try:
    __version__ = version("ras-commander")
except PackageNotFoundError:
    # package is not installed
    __version__ = "unknown"

# Set up logging
setup_logging()

# Import all necessary functions and classes directly
from .RasPrj import RasPrj, init_ras_project, get_ras_exe, ras
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasUnsteady import RasUnsteady
from .RasUtils import RasUtils
from .RasExamples import RasExamples
from .RasCmdr import RasCmdr
from .RasGpt import RasGpt  
from .RasToGo import RasToGo
from .HdfFluvialPluvial import HdfFluvialPluvial

# Import the Hdf* classes
from .HdfBase import HdfBase
from .HdfBndry import HdfBndry
from .HdfMesh import HdfMesh
from .HdfPlan import HdfPlan
from .HdfResultsMesh import HdfResultsMesh
from .HdfResultsPlan import HdfResultsPlan
from .HdfResultsXsec import HdfResultsXsec
from .HdfStruc import HdfStruc
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfPump import HdfPump
from .HdfPipe import HdfPipe
from .HdfInfiltration import HdfInfiltration
from .RasMapper import RasMapper

# Import plotting classes
from .HdfPlot import HdfPlot
from .HdfResultsPlot import HdfResultsPlot

# Define __all__ to specify what should be imported when using "from ras_commander import *"
__all__ = [
    "HdfBase",
    "HdfBndry",
    "HdfMesh",
    "HdfPlan",
    "HdfResultsMesh",
    "HdfResultsPlan",
    "HdfResultsXsec",
    "HdfStruc",
    "HdfUtils",
    "HdfXsec",
    "HdfPump",
    "HdfPipe",
    "HdfPlot",
    "HdfResultsPlot",
    "HdfInfiltration",
    "RasMapper",
    "standardize_input",
    "ras",
    "init_ras_project",
    "get_ras_exe",
    "RasPrj",
    "RasPlan",
    "RasGeo",
    "RasUnsteady",
    "RasCmdr",
    "RasUtils",
    "RasExamples",
    "get_logger",
    "log_call",
]

__version__ = "0.1.0"

==================================================

File: c:\GH\ras-commander\examples\data\PF_Depth_English_PDS_DavisCA.csv
==================================================
Point precipitation frequency estimates (inches)
NOAA Atlas 14 Volume 6 Version 2
Data type: Precipitation depth
Time series type: Partial duration
Project area: Southwest
Location name (ESRI Maps): Davis, California, USA
Station Name: -
Latitude: 38.5467 Degree
Longitude: -121.7443 Degree
Elevation (USGS): 46 ft


PRECIPITATION FREQUENCY ESTIMATES
by duration for ARI (years):, 1,2,5,10,25,50,100,200,500,1000
5-min:, 0.112,0.137,0.174,0.207,0.257,0.299,0.347,0.400,0.479,0.548
10-min:, 0.161,0.197,0.250,0.297,0.368,0.429,0.497,0.573,0.687,0.785
15-min:, 0.194,0.238,0.302,0.359,0.446,0.519,0.601,0.693,0.830,0.949
30-min:, 0.297,0.364,0.461,0.548,0.680,0.792,0.917,1.06,1.27,1.45
60-min:, 0.387,0.474,0.601,0.714,0.886,1.03,1.20,1.38,1.65,1.89
2-hr:, 0.565,0.696,0.880,1.04,1.27,1.47,1.67,1.90,2.23,2.50
3-hr:, 0.698,0.861,1.09,1.28,1.56,1.79,2.03,2.29,2.66,2.96
6-hr:, 1.00,1.24,1.57,1.84,2.22,2.53,2.85,3.19,3.66,4.04
12-hr:, 1.34,1.68,2.13,2.50,3.01,3.41,3.83,4.26,4.86,5.33
24-hr:, 1.77,2.23,2.84,3.34,4.02,4.55,5.09,5.66,6.43,7.03
2-day:, 2.26,2.85,3.62,4.26,5.13,5.80,6.49,7.20,8.16,8.91
3-day:, 2.57,3.25,4.14,4.87,5.87,6.63,7.41,8.22,9.32,10.2
4-day:, 2.81,3.55,4.53,5.33,6.42,7.26,8.11,9.00,10.2,11.1
7-day:, 3.42,4.33,5.53,6.51,7.84,8.86,9.91,11.0,12.4,13.6
10-day:, 3.76,4.77,6.10,7.18,8.65,9.78,10.9,12.1,13.7,15.0
20-day:, 4.88,6.21,7.94,9.34,11.2,12.7,14.1,15.6,17.6,19.2
30-day:, 5.85,7.46,9.53,11.2,13.4,15.1,16.8,18.5,20.7,22.5
45-day:, 7.12,9.08,11.6,13.6,16.2,18.1,20.0,22.0,24.5,26.4
60-day:, 8.41,10.7,13.7,15.9,18.9,21.1,23.2,25.4,28.1,30.1

Date/time (GMT):  Sat Oct 19 21:42:50 2024
pyRunTime:  0.01979207992553711

==================================================

File: c:\GH\ras-commander\examples\data\profile_lines_chippewa2D.geojson
==================================================
{"type":"FeatureCollection","features":[{"type":"Feature","geometry":{"type":"LineString","coordinates":[[1026088.3876320078,7854277.0586871468],[1025906.2217006071,7854271.41852398],[1025310.0176824491,7854307.99545761],[1025121.2715620827,7854322.0960578574]]},"properties":{"Name":"Profile Line 1"}},{"type":"Feature","geometry":{"type":"LineString","coordinates":[[1027326.0231635921,7857086.7069512205],[1026860.8742986278,7857009.6759289969],[1026327.5826063121,7856947.4585648933],[1025933.5393003232,7856977.0858811326]]},"properties":{"Name":"Profile Line 2"}},{"type":"Feature","geometry":{"type":"LineString","coordinates":[[1026841.9610010353,7851873.6222630516],[1026156.6278684236,7851923.9267237745],[1025938.0125029876,7851866.9268233795],[1025107.6308135941,7851731.4566988265],[1024770.768494245,7851688.2087070523]]},"properties":{"Name":"Profile Line 3"}}]}
==================================================

