File: C:\GH\ras-commander\library_assistant\.cursorrules
==================================================
Library Assistant: AI-Powered Tool for Managing and Querying Library Content

Project Structure:
library_assistant/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ anthropic.py
â”‚   â””â”€â”€ openai.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ database/
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_handling.py
â”‚   â”œâ”€â”€ cost_estimation.py
â”‚   â”œâ”€â”€ conversation.py
â”‚   â””â”€â”€ context_processing.py
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ routes.py
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ static/
â””â”€â”€ assistant.py

Program Features:
1. Integration with Anthropic and OpenAI APIs for AI-powered responses
2. Context-aware processing using full context or RAG (Retrieval-Augmented Generation) modes
3. Dynamic file handling and content processing
4. Cost estimation for API calls
5. Conversation management and history saving
6. Web-based user interface using FastAPI
7. Customizable settings for model selection, context mode, and file handling
8. Error handling and user guidance

General Coding Rules and Guidelines:
1. Follow PEP 8 style guidelines for Python code
2. Use type hints and docstrings for improved code readability
3. Implement proper error handling and logging
4. Maintain separation of concerns between modules
5. Use asynchronous programming where appropriate for improved performance
6. Implement unit tests for critical functions
7. Keep sensitive information (e.g., API keys) secure and out of version control
8. Use meaningful variable and function names
9. Optimize for readability and maintainability
10. Regularly update dependencies and address security vulnerabilities

The Library Assistant operates by:
1. Processing user queries through a web interface
2. Preparing context based on the selected mode (full context or RAG)
3. Sending prepared prompts to the chosen AI model (Anthropic or OpenAI)
4. Streaming and processing AI responses
5. Estimating and displaying costs for API calls
6. Managing conversation history and allowing for conversation saving
7. Providing user guidance and error handling as needed

The AI assistant interacting with the Library Assistant is a helpful expert with experience in:
- Python programming
- FastAPI web framework
- SQLAlchemy ORM
- Anthropic and OpenAI APIs
- Natural Language Processing (NLP) techniques
- Retrieval-Augmented Generation (RAG)
- Asynchronous programming
- RESTful API design
- Database management
- Cost optimization for API usage
- Web development (HTML, CSS, JavaScript)
- Git version control

The assistant should provide accurate, context-aware, and helpful responses while adhering to the Library Assistant's capabilities and limitations. It should offer guidance on effective use of the tool, including query formulation and settings management, while maintaining a professional and knowledgeable persona throughout all interactions.

==================================================

Folder: C:\GH\ras-commander\library_assistant\api
==================================================

File: C:\GH\ras-commander\library_assistant\assistant.py
==================================================
"""
Main entry point for the Library Assistant application.

This module initializes the FastAPI application, sets up the necessary routes,
and provides functions to open the browser and run the application.

Functions:
- open_browser(): Opens the default web browser to the application URL.
- run_app(): Starts the FastAPI application using uvicorn.
"""

import os
import uvicorn
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from web.routes import router
import webbrowser
from utils.context_processing import initialize_rag_context

# Initialize FastAPI application
print("Initializing FastAPI")
app = FastAPI(
    title="Library Assistant",
    description="An AI-powered assistant for managing and querying library content.",
    version="1.0.0"
)

# Create necessary directories if they don't exist
os.makedirs("web/templates", exist_ok=True)
os.makedirs("web/static", exist_ok=True)

# Mount the static files directory
app.mount("/static", StaticFiles(directory="web/static"), name="static")

# Include the router from web/routes.py
app.include_router(router)

def open_browser():
    """
    Opens the default web browser to the application URL.
    
    This function is called when the application starts to provide
    easy access to the web interface.
    """
    webbrowser.open("http://127.0.0.1:8000")

def run_app():
    """
    Starts the FastAPI application using uvicorn.
    
    This function configures and runs the uvicorn server with the
    FastAPI application.
    """
    uvicorn.run(app, host="127.0.0.1", port=8000)

if __name__ == "__main__":
    # Initialize RAG context at startup
    try:
        initialize_rag_context()
        print("RAG context initialized successfully.")
    except Exception as e:
        print(f"Failed to initialize RAG context: {e}")

    # Open the browser
    open_browser()

    # Run the app
    run_app()

==================================================

Folder: C:\GH\ras-commander\library_assistant\config
==================================================

Folder: C:\GH\ras-commander\library_assistant\database
==================================================

File: C:\GH\ras-commander\library_assistant\Library_Assistant_REAME.md
==================================================
# Library Assistant

Library Assistant is an AI-powered tool for managing and querying library content, leveraging both Anthropic's Claude and OpenAI's GPT models. It provides a web interface for interacting with AI models while maintaining context awareness of your codebase or documentation.

## Features

- **Dual AI Provider Support**: Integration with both Anthropic (Claude) and OpenAI (GPT) models
- **Context-Aware Processing**: Two modes of operation:
  - Full Context: Uses complete codebase/documentation context
  - RAG (Retrieval-Augmented Generation): Dynamically retrieves relevant context
- **Web Interface**: Clean, intuitive web UI built with FastAPI and Bootstrap
- **Real-Time Cost Estimation**: Estimates API costs for each interaction
- **Conversation Management**: Save and export chat histories
- **Customizable Settings**: Configure API keys, models, and context handling
- **File Processing**:
  - Intelligent handling of Python and Markdown files
  - Configurable file/folder exclusions
  - Code stripping options for reduced token usage

## Installation

1. Clone the repository:
```bash
# Start of Selection
git clone https://github.com/billk-FM/ras-commander.git
# End of Selection
cd library-assistant
```

2. Install dependencies:
```bash
pip install fastapi uvicorn sqlalchemy jinja2 pandas anthropic openai tiktoken astor markdown python-multipart requests python-dotenv
```

3. Set up your environment:
   - Obtain API keys from [Anthropic](https://www.anthropic.com/) and/or [OpenAI](https://openai.com/)
   - Configure your settings through the web interface

## Usage

1. Start the application:
```bash
python assistant.py
```

2. Open your web browser to `http://127.0.0.1:8000`

3. Configure your settings:
   - Select your preferred AI model
   - Enter your API key(s)
   - Choose context handling mode
   - Adjust RAG parameters if using RAG mode

4. Start chatting with the AI assistant about your codebase or documentation

## Configuration

### Available Models

- **Anthropic**:
  - Claude 3.5 Sonnet

- **OpenAI**:
  - GPT-4o
  - GPT-4o-mini
  - o1-mini

### Context Modes

1. **Full Context**:
   - Provides complete codebase context to the AI
   - Best for smaller codebases
   - Higher token usage

2. **RAG Mode**:
   - Dynamically retrieves relevant context
   - More efficient for large codebases
   - Configurable chunk sizes

### File Processing Options

Configure exclusions in settings:
```python
omit_folders = [
    "__pycache__",
    ".git",
    "venv",
    # Add custom folders
]

omit_extensions = [
    ".jpg", ".png", ".pdf",
    # Add custom extensions
]

omit_files = [
    "specific_file.txt",
    # Add specific files
]
```

## Project Structure

```
library_assistant/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ anthropic.py    # Anthropic API integration
â”‚   â””â”€â”€ openai.py       # OpenAI API integration
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py       # Configuration management
â”œâ”€â”€ database/
â”‚   â””â”€â”€ models.py       # Database models
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_handling.py       # File processing utilities
â”‚   â”œâ”€â”€ cost_estimation.py     # API cost calculations
â”‚   â”œâ”€â”€ conversation.py        # Chat history management
â”‚   â””â”€â”€ context_processing.py  # Context handling
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ routes.py      # FastAPI routes
â”‚   â”œâ”€â”€ templates/     # HTML templates
â”‚   â””â”€â”€ static/        # Static assets
â””â”€â”€ assistant.py       # Main application entry point
```

## Error Handling

The application includes comprehensive error handling:
- API errors
- File processing errors
- Invalid settings
- Connection issues

Errors are logged and displayed in the web interface with appropriate messages.

## Development

### Adding New Features

1. Follow the existing project structure
2. Implement proper error handling
3. Update the web interface as needed
4. Document new features

### Code Style

- Follow PEP 8 guidelines
- Include docstrings for all functions
- Use type hints where appropriate
- Implement proper error handling
- Keep functions focused and modular

## Performance Considerations

- RAG mode is recommended for large codebases
- Adjust chunk sizes based on your needs
- Consider token limits of your chosen model
- Monitor API costs through the interface

## Limitations

- Maximum context window varies by model
- API rate limits apply
- Token costs vary by provider and model
- Some file types are excluded by default

## Support

For issues, questions, or contributions:
1. Check the existing documentation
2. Review the codebase for similar functionality
3. Open an issue for bugs or feature requests
4. Submit pull requests with improvements

## License

This project is licensed under the MIT License - see the LICENSE file for details.
==================================================

File: C:\GH\ras-commander\library_assistant\main.py
==================================================
from fastapi import FastAPI
from api.logging import router as logging_router

app = FastAPI()

# Include the logging router
app.include_router(logging_router, prefix="/api") 
==================================================

File: C:\GH\ras-commander\library_assistant\package-lock.json
==================================================
{
  "name": "library_assistant",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "dependencies": {
        "react-dropdown-tree-select": "^2.8.0"
      }
    },
    "node_modules/array.partial": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/array.partial/-/array.partial-1.0.5.tgz",
      "integrity": "sha512-nkHH1dU6JXrwppCqdUD5M1R85vihgBqhk9miq+3WFwwRayNY1ggpOT6l99PppqYQ1Hcjv2amFfUzhe25eAcYfA==",
      "license": "MIT",
      "engines": {
        "node": ">=6"
      }
    },
    "node_modules/js-tokens": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz",
      "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==",
      "license": "MIT",
      "peer": true
    },
    "node_modules/loose-envify": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz",
      "integrity": "sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "js-tokens": "^3.0.0 || ^4.0.0"
      },
      "bin": {
        "loose-envify": "cli.js"
      }
    },
    "node_modules/object-assign": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz",
      "integrity": "sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==",
      "license": "MIT",
      "peer": true,
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/prop-types": {
      "version": "15.8.1",
      "resolved": "https://registry.npmjs.org/prop-types/-/prop-types-15.8.1.tgz",
      "integrity": "sha512-oj87CgZICdulUohogVAR7AjlC0327U4el4L6eAvOqCeudMDVU0NThNaV+b9Df4dXgSP1gXMTnPdhfe/2qDH5cg==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "loose-envify": "^1.4.0",
        "object-assign": "^4.1.1",
        "react-is": "^16.13.1"
      }
    },
    "node_modules/react": {
      "version": "18.3.1",
      "resolved": "https://registry.npmjs.org/react/-/react-18.3.1.tgz",
      "integrity": "sha512-wS+hAgJShR0KhEvPJArfuPVN1+Hz1t0Y6n5jLrGQbkb4urgPE/0Rve+1kMB1v/oWgHgm4WIcV+i7F2pTVj+2iQ==",
      "license": "MIT",
      "peer": true,
      "dependencies": {
        "loose-envify": "^1.1.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/react-dropdown-tree-select": {
      "version": "2.8.0",
      "resolved": "https://registry.npmjs.org/react-dropdown-tree-select/-/react-dropdown-tree-select-2.8.0.tgz",
      "integrity": "sha512-Nu8Aur/qgNgJgxeW10dkyKHGBzyhyjIZBngmuM6gW1oNBs3UE9IcYUlUz2c6rLBEcAQYtkkp4pvu749mXKE0Dw==",
      "license": "MIT",
      "dependencies": {
        "array.partial": "^1.0.5",
        "react-infinite-scroll-component": "^4.0.2"
      },
      "peerDependencies": {
        "react": "^16.3.0 || ^17 || ^18"
      }
    },
    "node_modules/react-infinite-scroll-component": {
      "version": "4.5.3",
      "resolved": "https://registry.npmjs.org/react-infinite-scroll-component/-/react-infinite-scroll-component-4.5.3.tgz",
      "integrity": "sha512-8O0PIeYZx0xFVS1ChLlLl/1obn64vylzXeheLsm+t0qUibmet7U6kDaKFg6jVRQJwDikWBTcyqEFFsxrbFCO5w==",
      "license": "MIT",
      "peerDependencies": {
        "prop-types": "^15.0.0",
        "react": ">=0.14.0"
      }
    },
    "node_modules/react-is": {
      "version": "16.13.1",
      "resolved": "https://registry.npmjs.org/react-is/-/react-is-16.13.1.tgz",
      "integrity": "sha512-24e6ynE2H+OKt4kqsOvNd8kBpV65zoxbA4BVsEOB3ARVWQki/DHzaUoC5KuON/BiccDaCCTZBuOcfZs70kR8bQ==",
      "license": "MIT",
      "peer": true
    }
  }
}

==================================================

File: C:\GH\ras-commander\library_assistant\package.json
==================================================
{
  "dependencies": {
    "react-dropdown-tree-select": "^2.8.0"
  }
}

==================================================

File: C:\GH\ras-commander\library_assistant\requirements.txt
==================================================
# Web framework and ASGI server
fastapi==0.68.0
uvicorn==0.15.0

# Database ORM
sqlalchemy==1.4.23

# Template engine
jinja2==3.0.1

# Data manipulation
pandas==1.3.3

# AI/ML libraries
anthropic==0.2.7
openai==0.27.0
tiktoken==0.3.0

# Code parsing and manipulation
astor==0.8.1

# Markdown processing
markdown==3.3.4

# Form handling for FastAPI
python-multipart==0.0.5

# HTTP client for Python
requests==2.26.0

# Environment variable management
python-dotenv==0.19.0

# YAML parsing (if needed for configuration)
pyyaml==5.4.1

# Date and time manipulation
python-dateutil==2.8.2

# Progress bars (optional, for long-running tasks)
tqdm==4.62.3

# Testing framework
pytest==6.2.5

# Linting and code formatting
flake8==3.9.2
black==21.9b0

# Type checking
mypy==0.910

# Documentation generation
sphinx==4.2.0

==================================================

File: C:\GH\ras-commander\library_assistant\settings.db
==================================================
SQLite format 3   @                                                                     .v  n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N)eindexix_settings_idsettingsCREATE INDEX ix_settings_id ON settings (id)VtablesettingssettingsCREATE TABLE settings (
	id VARCHAR NOT NULL, 
	anthropic_api_key TEXT, 
	openai_api_key TEXT, 
	selected_model VARCHAR, 
	context_mode VARCHAR, 
	omit_folders TEXT, 
	omit_extensions TEXT, 
	omit_files TEXT, 
	chunk_level VARCHAR, 
	initial_chunk_size INTEGER, 
	followup_chunk_size INTEGER, 
	PRIMARY KEY (id)
)/C indexsqlite_autoindex_settings_1settings          w w                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             eUA%+Isingleton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jiowS6M1bPJl-sSEAp_1yCIrTFwcXO5ucclkKkiCuxmp73biN3T5k3hVJYuYwtgcAclaude-3-5-sonnet-20240620full_context["Bald Eagle Creek", "__pycache__", ".git", ".github", "tests", "build", "dist", "ras_commander.egg-info", "venv", "example_projects", "llm_summary", "misc", "future", "ai_tools", "docsExample_Projects_6_6"][".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff", ".webp", ".svg", ".ico", ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".zip", ".rar", ".7z", ".tar", ".gz", ".exe", ".dll", ".so", ".dylib", ".pyc", ".pyo", ".pyd", ".class", ".log", ".tmp", ".bak", ".swp", ".bat", ".sh"]["FunctionList.md", "DS_Store", "Thumbs.db", "llmsummarize", "example_projects.zip", "11_accessing_example_projects.ipynb", "Example_Projects_6_5.zip", "github_code_assistant.ipynb", "example_projects.ipynb", "11_Using_RasExamples.ipynb", "example_projects.csv", "rascommander_code_assistant.ipynb", "RasExamples.py"]function} >
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	singleton
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	singleton
==================================================

Folder: C:\GH\ras-commander\library_assistant\utils
==================================================

Folder: C:\GH\ras-commander\library_assistant\web
==================================================

File: C:\GH\ras-commander\library_assistant\api\anthropic.py
==================================================
"""
Anthropic API integration for the Library Assistant.
"""

from anthropic import AsyncAnthropic, Anthropic, APIError, AuthenticationError
from typing import AsyncGenerator, List, Optional, Union

async def anthropic_stream_response(
    client: Union[AsyncAnthropic, Anthropic], 
    prompt: str, 
    max_tokens: int = 8000,
    model: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """
    Streams a response from the Anthropic API using the Claude model.

    Args:
        client: An initialized Anthropic client (sync or async)
        prompt: The prompt to send to the API
        max_tokens: The maximum number of tokens to generate (default: 8000)
        model: The model to use (default: claude-3-5-sonnet-20240620)

    Yields:
        str: Chunks of the response text from the API

    Raises:
        APIError: If there's an error with the API call
        AuthenticationError: If authentication fails
    """
    try:
        model = model or "claude-3-5-sonnet-20240620"
        
        # Convert to async client if needed
        async_client = client if isinstance(client, AsyncAnthropic) else AsyncAnthropic(api_key=client.api_key)
        
        stream = await async_client.messages.create(
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
            model=model,
            stream=True
        )
        
        # Process the stream events
        current_line = []
        
        async for message in stream:
            try:
                if message.type == "content_block_delta" and message.delta and message.delta.text:
                    # Clean and normalize the chunk text
                    chunk = message.delta.text.replace('\r', '')
                    
                    # Accumulate text until we get a natural break
                    current_line.append(chunk)
                    
                    # Check for natural breaks (end of sentence or paragraph)
                    if any(chunk.endswith(end) for end in ['.', '!', '?', '\n']):
                        complete_line = ''.join(current_line)
                        if complete_line.strip():
                            yield complete_line
                        current_line = []
                        
            except Exception as e:
                print(f"Error processing message chunk: {str(e)}")
                continue
        
        # Yield any remaining text
        if current_line:
            remaining = ''.join(current_line)
            if remaining.strip():
                yield remaining
                
    except Exception as e:
        raise APIError(f"Unexpected error in Anthropic API call: {str(e)}")

def get_anthropic_client(api_key: str, async_client: bool = False) -> Union[Anthropic, AsyncAnthropic]:
    """
    Creates and returns an Anthropic client.

    Args:
        api_key: The Anthropic API key
        async_client: Whether to return an async client (default: False)

    Returns:
        An initialized Anthropic client (sync or async)

    Raises:
        ValueError: If the API key is not provided or invalid
    """
    if not api_key or not isinstance(api_key, str):
        raise ValueError("Valid Anthropic API key must be provided")
    return AsyncAnthropic(api_key=api_key) if async_client else Anthropic(api_key=api_key)

async def validate_anthropic_api_key(api_key: str) -> bool:
    """
    Validates the Anthropic API key by making a test API call.

    Args:
        api_key: The Anthropic API key to validate

    Returns:
        True if the API key is valid, False otherwise
    """
    try:
        client = get_anthropic_client(api_key, async_client=True)
        await client.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=1,
            messages=[{"role": "user", "content": "Test"}],
            stream=False
        )
        return True
    except (anthropic.APIError, anthropic.AuthenticationError, ValueError):
        return False

def get_anthropic_models() -> List[str]:
    """
    Returns a list of available Anthropic models.

    Returns:
        List of strings representing available Anthropic model names
    """
    return ["claude-3-5-sonnet-20240620"]

async def stream_response(
    client: Union[AsyncAnthropic, Anthropic], 
    prompt: str, 
    max_tokens: int = 8000,
    model: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """
    Streams a response from the Anthropic API.

    This function is a wrapper around anthropic_stream_response to provide
    a consistent interface across different API providers.

    Args:
        client: An initialized Anthropic client (sync or async)
        prompt: The prompt to send to the API
        max_tokens: The maximum number of tokens to generate (default: 8000)
        model: The model to use (optional)

    Returns:
        An async generator yielding response chunks
    """
    async for chunk in anthropic_stream_response(client, prompt, max_tokens, model):
        yield chunk

==================================================

File: C:\GH\ras-commander\library_assistant\api\logging.py
==================================================
from fastapi import APIRouter, Request
import logging
import os

router = APIRouter()

# Determine the path for the log folder relative to assistant.py
log_folder_path = os.path.join(os.path.dirname(__file__), '..', 'log_folder')
os.makedirs(log_folder_path, exist_ok=True)

# Configure logging
log_file_path = os.path.join(log_folder_path, 'client_logs.log')

# Create a logger
logger = logging.getLogger("clientLogger")
logger.setLevel(logging.INFO)

# Create handlers
file_handler = logging.FileHandler(log_file_path)
console_handler = logging.StreamHandler()

# Set level for handlers
file_handler.setLevel(logging.INFO)
console_handler.setLevel(logging.INFO)

# Create a logging format
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# Add handlers to the logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)

@router.post("/log")
async def log_message(request: Request):
    data = await request.json()
    message = data.get("message", "")
    logger.info(message)
    return {"status": "success"} 
==================================================

File: C:\GH\ras-commander\library_assistant\api\openai.py
==================================================
"""
OpenAI API integration for the Library Assistant.
"""

from openai import OpenAI, OpenAIError
from typing import AsyncGenerator, List, Optional, Dict, Any

async def openai_stream_response(
    client: OpenAI,
    model: str,
    messages: List[Dict[str, str]],
    max_tokens: int = 16000
) -> AsyncGenerator[str, None]:
    """
    Streams a response from the OpenAI API using the specified model.

    Args:
        client: An initialized OpenAI client
        model: The name of the OpenAI model to use
        messages: A list of message dictionaries to send to the API
        max_tokens: The maximum number of tokens to generate (default: 16000)

    Yields:
        str: Chunks of the response text from the API

    Raises:
        OpenAIError: If there's an error with the API call
    """
    try:
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                # Clean and normalize the chunk text
                text = chunk.choices[0].delta.content.replace('\r', '')
                if text.strip():  # Only yield non-empty chunks
                    yield text

    except OpenAIError as e:
        raise OpenAIError(f"OpenAI API error: {str(e)}")
    except Exception as e:
        raise OpenAIError(f"Unexpected error in OpenAI API call: {str(e)}")

def get_openai_client(api_key: str) -> OpenAI:
    """
    Creates and returns an OpenAI client.

    Args:
        api_key: The OpenAI API key

    Returns:
        OpenAI: An initialized OpenAI client

    Raises:
        ValueError: If the API key is not provided
    """
    if not api_key:
        raise ValueError("OpenAI API key not provided")
    return OpenAI(api_key=api_key)

async def validate_openai_api_key(api_key: str) -> bool:
    """
    Validates the OpenAI API key by making a test API call.

    Args:
        api_key: The OpenAI API key to validate

    Returns:
        bool: True if the API key is valid, False otherwise
    """
    try:
        client = get_openai_client(api_key)
        # Make a minimal API call to test the key
        client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=1,
            stream=False
        )
        return True
    except (OpenAIError, ValueError):
        return False

def get_openai_models() -> List[str]:
    """
    Returns a list of available OpenAI models.

    Returns:
        List[str]: A list of strings representing available OpenAI model names
    """
    return ["gpt-4o-2024-08-06", "gpt-4o-mini", "o1-mini"]

==================================================

File: C:\GH\ras-commander\library_assistant\config\config.py
==================================================
"""
Configuration module for the Library Assistant.

This module provides functions for loading and updating settings,
as well as defining default settings for the application.

Functions:
- load_settings(): Loads the current settings from the database or initializes with defaults.
- update_settings(data): Updates the settings in the database with new values.

Constants:
- DEFAULT_SETTINGS: A dictionary containing the default settings for the application.
"""

import json
from sqlalchemy.orm import sessionmaker
from database.models import Settings, engine

# Create a SessionLocal class for database sessions
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Default settings for the application
DEFAULT_SETTINGS = {
    "anthropic_api_key": "",
    "openai_api_key": "",
    "selected_model": "",
    "context_mode": "",
    "omit_folders": [
        "Bald Eagle Creek", 
        "__pycache__", 
        ".git", 
        ".github", 
        "tests", 
        "build", 
        "dist", 
        "ras_commander.egg-info", 
        "venv", 
        "example_projects", 
        "llm_summary", 
        "misc", 
        "future", 
        "ai_tools",
        "docs"
        "Example_Projects_6_6"
        "html"
        "data"
    ],
    "omit_extensions": [
        '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.svg', '.ico',
        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
        '.zip', '.rar', '.7z', '.tar', '.gz',
        '.exe', '.dll', '.so', '.dylib',
        '.pyc', '.pyo', '.pyd',
        '.class',
        '.log', '.tmp', '.bak', '.swp',
        '.bat', '.sh', '.html',
    ],
    "omit_files": [
        'FunctionList.md',
        'DS_Store',
        'Thumbs.db',
        'llmsummarize',
        'example_projects.zip',
        '11_accessing_example_projects.ipynb',
        'Example_Projects_6_5.zip',
        'github_code_assistant.ipynb',
        'example_projects.ipynb',
        '11_Using_RasExamples.ipynb',
        'example_projects.csv',
        'rascommander_code_assistant.ipynb',
        'RasExamples.py'
    ],
    "chunk_level": "function",
    "initial_chunk_size": 32000,
    "followup_chunk_size": 16000
}

def load_settings():
    """
    Loads the current settings from the database or initializes with defaults.

    This function queries the database for existing settings. If no settings are found,
    it initializes the database with the default settings. The settings are stored
    as a singleton record in the database.

    Returns:
        Settings: An instance of the Settings model containing the current settings.
    """
    db = SessionLocal()
    settings = db.query(Settings).filter(Settings.id == "singleton").first()
    if not settings:
        # Initialize with default settings
        settings = Settings(
            id="singleton",
            **{key: json.dumps(value) if isinstance(value, list) else value 
               for key, value in DEFAULT_SETTINGS.items()}
        )
        db.add(settings)
        db.commit()
        db.refresh(settings)
    db.close()
    return settings

def update_settings(data):
    """
    Updates the settings in the database with new values.

    This function takes a dictionary of settings to update, queries the database
    for the existing settings, and updates the values accordingly. For list-type
    settings (omit_folders, omit_extensions, omit_files), the values are JSON-encoded
    before storage.

    Args:
        data (dict): A dictionary containing the settings to update.
                     Keys should match the attribute names in the Settings model.

    Note:
        This function does not return any value. It directly updates the database.
    """
    db = SessionLocal()
    settings = db.query(Settings).filter(Settings.id == "singleton").first()
    for key, value in data.items():
        if key in ["omit_folders", "omit_extensions", "omit_files"]:
            setattr(settings, key, json.dumps(value))
        else:
            setattr(settings, key, value)
    db.commit()
    db.close()

==================================================

File: C:\GH\ras-commander\library_assistant\database\models.py
==================================================
"""
Database models for the Library Assistant.

This module defines the SQLAlchemy ORM models used for storing application settings.

Classes:
- Base: The declarative base class for SQLAlchemy models.
- Settings: The model representing application settings.

Constants:
- DATABASE_URL: The URL for the SQLite database.
- engine: The SQLAlchemy engine instance.
"""

from sqlalchemy import create_engine, Column, String, Text, Integer
from sqlalchemy.orm import declarative_base

# Define the database URL
DATABASE_URL = "sqlite:///./settings.db"

# Create the SQLAlchemy engine
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})

# Create the declarative base class
Base = declarative_base()

class Settings(Base):
    """
    SQLAlchemy ORM model for storing application settings.

    This class represents a single row in the settings table, which stores
    all configuration options for the Library Assistant application.

    Attributes:
        id (str): Primary key, set to "singleton" as there's only one settings record.
        anthropic_api_key (str): API key for Anthropic services.
        openai_api_key (str): API key for OpenAI services.
        selected_model (str): The currently selected AI model.
        context_mode (str): The context handling mode (e.g., 'full_context' or 'rag').
        omit_folders (str): JSON string of folders to omit from processing.
        omit_extensions (str): JSON string of file extensions to omit from processing.
        omit_files (str): JSON string of specific files to omit from processing.
        chunk_level (str): The level at which to chunk text (e.g., 'function').
        initial_chunk_size (int): The initial size of text chunks for processing.
        followup_chunk_size (int): The size of follow-up chunks for processing.
    """

    __tablename__ = "settings"

    id = Column(String, primary_key=True, index=True, default="singleton")
    anthropic_api_key = Column(Text, nullable=True)
    openai_api_key = Column(Text, nullable=True)
    selected_model = Column(String, nullable=True)
    context_mode = Column(String, nullable=True)
    omit_folders = Column(Text, nullable=True)
    omit_extensions = Column(Text, nullable=True)
    omit_files = Column(Text, nullable=True)
    chunk_level = Column(String, nullable=True)
    initial_chunk_size = Column(Integer, default=32000)
    followup_chunk_size = Column(Integer, default=16000)

# Create the database tables
Base.metadata.create_all(bind=engine)

==================================================

File: C:\GH\ras-commander\library_assistant\utils\context_processing.py
==================================================
"""
Utility functions for context processing in the Library Assistant.
"""

import tiktoken
from pathlib import Path
from config.config import load_settings
from utils.file_handling import combine_files, read_system_message, set_context_folder
import json

# Initialize global variables for context
preprocessed_context = ""
preprocessed_rag_context = ""
conversation_context = {}  # Store context for each conversation

def initialize_rag_context():
    """
    Initializes both RAG and full context processing.
    """
    global preprocessed_context, preprocessed_rag_context
    
    try:
        # Load settings
        settings = load_settings()
        context_folder = set_context_folder()
        
        # Get settings as Python objects
        omit_folders = json.loads(settings.omit_folders)
        omit_extensions = json.loads(settings.omit_extensions)
        omit_files = json.loads(settings.omit_files)
        
        # Combine files with current settings
        combined_text, total_token_count, file_token_counts = combine_files(
            summarize_subfolder=context_folder,
            omit_folders=omit_folders,
            omit_extensions=omit_extensions,
            omit_files=omit_files,
            strip_code=True,
            chunk_level=settings.chunk_level
        )
        
        # Store full context
        preprocessed_context = combined_text
        
        # Process RAG context
        preprocessed_rag_context = prepare_context(
            text=combined_text,
            mode='rag',
            initial_chunk_size=settings.initial_chunk_size,
            followup_chunk_size=settings.followup_chunk_size
        )
        
        # Store token counts for files
        global file_token_mapping
        file_token_mapping = file_token_counts
        
        return True
    except Exception as e:
        print(f"Error initializing context: {str(e)}")
        raise

def prepare_full_prompt(user_query: str, selected_files=None, conversation_id=None) -> str:
    """
    Prepares the full prompt for the AI model, including context and conversation history.
    
    Args:
        user_query (str): The user's query
        selected_files (list): List of files to include in context
        conversation_id (str): Unique identifier for the conversation
    
    Returns:
        str: The complete prompt including system message, context, and conversation history
    """
    settings = load_settings()
    context_mode = settings.context_mode
    system_message = read_system_message()
    
    try:
        # Get or initialize conversation context
        if conversation_id not in conversation_context:
            conversation_context[conversation_id] = {
                'selected_files': selected_files,
                'history': []
            }
        
        conv_data = conversation_context[conversation_id]
        
        # Update selected files if they've changed
        if selected_files != conv_data['selected_files']:
            conv_data['selected_files'] = selected_files
        
        if context_mode == 'full_context':
            if selected_files:
                # Get content of selected files
                context_folder = set_context_folder()
                combined_text, _, _ = combine_files(
                    summarize_subfolder=context_folder,
                    omit_folders=[],
                    omit_extensions=[],
                    omit_files=[],
                    strip_code=True,
                    chunk_level='file',
                    selected_files=selected_files
                )
                context = combined_text
            else:
                context = preprocessed_context
                
            # Format prompt with conversation history
            prompt = (f"{system_message}\n\n"
                     f"Files from RAS-Commander Repository for Context:\n{context}\n\n"
                     "Previous Conversation:\n")
            
            # Add conversation history
            for msg in conv_data['history']:
                prompt += f"{msg['role'].capitalize()}: {msg['content']}\n\n"
            
            # Add current query
            prompt += f"User Query: {user_query}"
            
            return prompt
            
        else:  # RAG mode
            prompt = (f"{system_message}\n\n"
                     "<context>\nFiles from RAS-Commander Repository for Context:\n"
                     f"{preprocessed_rag_context}\n</context>\n\n"
                     "Previous Conversation:\n")
            
            # Add conversation history
            for msg in conv_data['history']:
                prompt += f"{msg['role'].capitalize()}: {msg['content']}\n\n"
            
            prompt += f"Using the context above, please respond to this query:\n\n"
            prompt += f"User Query: {user_query}"
            
            return prompt
            
    except Exception as e:
        print(f"Error preparing prompt: {str(e)}")
        return f"{system_message}\n\nUser Query: {user_query}"

def update_conversation_history(conversation_id: str, role: str, content: str):
    """
    Updates the conversation history for a given conversation.
    
    Args:
        conversation_id (str): Unique identifier for the conversation
        role (str): Role of the message sender ('user' or 'assistant')
        content (str): Content of the message
    """
    if conversation_id not in conversation_context:
        conversation_context[conversation_id] = {
            'selected_files': None,
            'history': []
        }
    
    conversation_context[conversation_id]['history'].append({
        'role': role,
        'content': content
    })

def clear_conversation_history(conversation_id: str):
    """
    Clears the conversation history for a given conversation.
    
    Args:
        conversation_id (str): Unique identifier for the conversation
    """
    if conversation_id in conversation_context:
        conversation_context[conversation_id]['history'] = []

def prepare_context(text="", mode='full_context', selected_files=None, initial_chunk_size=32000, followup_chunk_size=16000):
    """
    Prepares context based on the specified mode.
    
    Args:
        text (str): The input text to process
        mode (str): Context preparation mode ('full_context' or 'rag')
        selected_files (list): List of files to include in context (for full_context mode)
        initial_chunk_size (int): Size for initial RAG chunks
        followup_chunk_size (int): Size for follow-up RAG chunks
    
    Returns:
        str: The prepared context
    """
    if mode == 'full_context':
        if selected_files:
            # Filter context to only include selected files
            filtered_text = ""
            current_file = None
            for line in text.split('\n'):
                if line.startswith("----- ") and " - " in line:
                    current_file = line.split(" - ")[0].replace("----- ", "")
                    if current_file in selected_files:
                        filtered_text += line + "\n"
                elif current_file in selected_files:
                    filtered_text += line + "\n"
            return filtered_text
        return text
    elif mode == 'rag':
        chunks = chunk_text(text, initial_chunk_size)
        ranked_chunks = rank_chunks(chunks)
        prepared_context = ""
        current_size = 0
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        for chunk in ranked_chunks:
            chunk_size = len(enc.encode(chunk))
            if current_size + chunk_size <= followup_chunk_size:
                prepared_context += chunk + "\n\n"
                current_size += chunk_size
            else:
                break
        return prepared_context
    else:
        raise ValueError("Invalid mode. Choose 'full_context' or 'rag'")

def rank_chunks(chunks):
    """
    Ranks chunks of text based on potential relevance.
    Currently returns chunks in original order, but could be enhanced with
    more sophisticated ranking algorithms.

    Args:
        chunks (list): A list of text chunks to be ranked.

    Returns:
        list: The ranked chunks.
    """
    return chunks

def chunk_text(text, chunk_size):
    """
    Splits text into chunks while maintaining context boundaries.

    Args:
        text (str): The text to be chunked.
        chunk_size (int): Target size for each chunk in tokens.

    Returns:
        list: List of text chunks.
    """
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
    chunks = []
    current_chunk = ""
    current_tokens = 0

    for line in text.split('\n'):
        line_tokens = len(enc.encode(line))
        
        if current_tokens + line_tokens > chunk_size:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = line
            current_tokens = line_tokens
        else:
            current_chunk += '\n' + line if current_chunk else line
            current_tokens += line_tokens

    if current_chunk:
        chunks.append(current_chunk)

    return chunks
==================================================

File: C:\GH\ras-commander\library_assistant\utils\conversation.py
==================================================
"""
Utility functions for conversation handling in the Library Assistant.

This module provides functions for managing conversation history,
including adding messages, retrieving the full conversation,
and saving the conversation to a file.

Functions:
- add_to_history(role, content): Adds a message to the conversation history.
- get_full_conversation(): Retrieves the full conversation history as a string.
- save_conversation(): Saves the current conversation history to a file.
"""

from datetime import datetime
import os

# Initialize conversation history
conversation_history = []

def add_to_history(role, content):
    """
    Adds a message to the conversation history.

    Args:
        role (str): The role of the message sender (e.g., 'user' or 'assistant').
        content (str): The content of the message.
    """
    conversation_history.append({"role": role, "content": content})

def get_full_conversation():
    """
    Retrieves the full conversation history as a formatted string.

    Returns:
        str: A string representation of the entire conversation history.
    """
    return "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])

def save_conversation():
    """
    Saves the current conversation history to a file.

    This function creates a text file with a timestamp in its name,
    containing the full conversation history.

    Returns:
        str: The file path of the saved conversation history.

    Raises:
        IOError: If there's an error writing to the file.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = f"conversation_history_{timestamp}.txt"
    file_path = os.path.join(os.getcwd(), file_name)
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for message in conversation_history:
                f.write(f"{message['role'].capitalize()}: {message['content']}\n\n")
        return file_path
    except IOError as e:
        raise IOError(f"Error saving conversation history: {str(e)}")

def clear_conversation_history():
    """
    Clears the current conversation history.

    This function removes all messages from the conversation history,
    effectively resetting it to an empty state.
    """
    global conversation_history
    conversation_history = []

def get_conversation_length():
    """
    Returns the number of messages in the current conversation history.

    Returns:
        int: The number of messages in the conversation history.
    """
    return len(conversation_history)

def get_last_message():
    """
    Retrieves the last message from the conversation history.

    Returns:
        dict: A dictionary containing the role and content of the last message,
              or None if the conversation history is empty.
    """
    if conversation_history:
        return conversation_history[-1]
    return None

==================================================

File: C:\GH\ras-commander\library_assistant\utils\cost_estimation.py
==================================================
"""
Utility functions for cost estimation in the Library Assistant.

This module provides functions for creating pricing dataframes and
estimating the cost of API calls based on token usage.

Functions:
- create_pricing_df(model): Creates a pricing dataframe for a given model.
- estimate_cost(input_tokens, output_tokens, pricing_df): Estimates the cost of an API call.
"""

import pandas as pd

def create_pricing_df(model):
    """
    Creates a pricing dataframe for a given model.

    This function returns a dictionary containing a pandas DataFrame with pricing information
    and the provider (OpenAI or Anthropic) for the specified model.

    Args:
        model (str): The name of the model (e.g., 'gpt-4', 'claude-3-5-sonnet-20240620').

    Returns:
        dict: A dictionary containing:
            - 'pricing_df': A pandas DataFrame with columns 'Model', 'Input ($/1M Tokens)',
                            'Output ($/1M Tokens)', 'Context Window (Tokens)', and 'Response Max Tokens'.
            - 'provider': A string indicating the provider ('openai' or 'anthropic').

    Raises:
        ValueError: If an unsupported model is specified.
    """
    if model.startswith("claude"):
        provider = "anthropic"
        pricing_data = {
            "Model": ["Claude 3.5 Sonnet"],
            "Input ($/1M Tokens)": [3],
            "Output ($/1M Tokens)": [15],
            "Context Window (Tokens)": [200000],
            "Response Max Tokens": [8192]
        }
    elif model.startswith("gpt") or model.startswith("o1"):
        provider = "openai"
        if model == "gpt-4o-2024-08-06":
            pricing_data = {
                "Model": ["gpt-4o-2024-08-06"],
                "Input ($/1M Tokens)": [2.50],
                "Output ($/1M Tokens)": [10.00],
                "Context Window (Tokens)": [128000],
                "Response Max Tokens": [16000]
            }
        elif model == "gpt-4o-mini":
            pricing_data = {
                "Model": ["GPT-4o-mini"],
                "Input ($/1M Tokens)": [0.150],
                "Output ($/1M Tokens)": [0.600],
                "Context Window (Tokens)": [128000],
                "Response Max Tokens": [16000]
            }
        elif model == "o1-mini":
            pricing_data = {
                "Model": ["o1-mini"],
                "Input ($/1M Tokens)": [3.00],
                "Output ($/1M Tokens)": [12.00],
                "Context Window (Tokens)": [128000],
                "Response Max Tokens": [16000]
            }
        else:
            raise ValueError(f"Unsupported OpenAI model selected: {model}")
    else:
        raise ValueError(f"Unsupported model: {model}")
    
    pricing_df = pd.DataFrame(pricing_data)
    return {"pricing_df": pricing_df, "provider": provider}

def estimate_cost(input_tokens, output_tokens, pricing_df):
    """
    Estimates the cost of an API call based on input and output tokens.

    Args:
        input_tokens (int): The number of input tokens used in the API call.
        output_tokens (int): The number of output tokens generated by the API call.
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        float: The estimated cost of the API call in dollars.
    """
    input_cost = (input_tokens / 1e6) * pricing_df['Input ($/1M Tokens)'].iloc[0]
    output_cost = (output_tokens / 1e6) * pricing_df['Output ($/1M Tokens)'].iloc[0]
    return input_cost + output_cost

def get_max_tokens(pricing_df):
    """
    Retrieves the maximum number of tokens allowed for a response.

    Args:
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        int: The maximum number of tokens allowed for a response.
    """
    return pricing_df['Response Max Tokens'].iloc[0]

def get_context_window(pricing_df):
    """
    Retrieves the context window size in tokens.

    Args:
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        int: The context window size in tokens.
    """
    return pricing_df['Context Window (Tokens)'].iloc[0]

==================================================

File: C:\GH\ras-commander\library_assistant\utils\file_handling.py
==================================================
"""
Utility functions for file handling in the Library Assistant.

This module provides functions for reading API keys, system messages,
and processing various file types for the Library Assistant application.

Functions:
- read_api_key(file_path): Reads an API key from a file.
- read_system_message(): Reads the system message from .cursorrules file.
- set_context_folder(): Sets the context folder for file processing.
- strip_code_from_functions(content): Strips code from function bodies.
- handle_python_file(content, filepath, strip_code, chunk_level='function'): Processes Python files.
- handle_markdown_file(content, filepath): Processes Markdown files.
- combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False, chunk_level='function', selected_files=None): Combines and processes multiple files.
"""

import os
import json
import re
import ast
import astor
from pathlib import Path
import tiktoken

def read_api_key(file_path):
    """
    Reads an API key from a file.

    Args:
        file_path (str): Path to the file containing the API key.

    Returns:
        str: The API key.

    Raises:
        FileNotFoundError: If the API key file is not found.
    """
    try:
        with open(file_path, 'r') as file:
            return file.read().strip()
    except FileNotFoundError:
        raise FileNotFoundError(f"API key file not found: {file_path}")

def read_system_message():
    """
    Reads the system message from .cursorrules file.

    Returns:
        str: The system message.

    Raises:
        FileNotFoundError: If the .cursorrules file is not found.
        ValueError: If no system message is found in the file.
    """
    current_dir = Path.cwd()
    cursor_rules_path = current_dir.parent / '.cursorrules'

    if not cursor_rules_path.exists():
        raise FileNotFoundError("This script expects to be in a directory within the ras_commander repo which has a .cursorrules file in its parent directory.")

    with open(cursor_rules_path, 'r') as f:
        system_message = f.read().strip()

    if not system_message:
        raise ValueError("No system message found in .cursorrules file.")

    return system_message

def set_context_folder():
    """
    Sets the context folder for file processing.

    Returns:
        Path: The path to the context folder.
    """
    return Path.cwd().parent

class FunctionStripper(ast.NodeTransformer):
    """AST NodeTransformer to strip code from function bodies."""
    def visit_FunctionDef(self, node):
        new_node = ast.FunctionDef(
            name=node.name,
            args=node.args,
            body=[ast.Pass()],
            decorator_list=node.decorator_list,
            returns=node.returns
        )
        if (node.body and isinstance(node.body[0], ast.Expr) and
            isinstance(node.body[0].value, ast.Str)):
            new_node.body = [node.body[0], ast.Pass()]
        return new_node

def strip_code_from_functions(content):
    """
    Strips code from function bodies, leaving only function signatures and docstrings.

    Args:
        content (str): The Python code content.

    Returns:
        str: The code with function bodies stripped.
    """
    try:
        tree = ast.parse(content)
        stripped_tree = FunctionStripper().visit(tree)
        return astor.to_source(stripped_tree)
    except SyntaxError:
        return content

def handle_python_file(content, filepath, strip_code, chunk_level='function'):
    """
    Processes Python files, optionally stripping code and chunking content.

    Args:
        content (str): The content of the Python file
        filepath (Path): The path to the Python file
        strip_code (bool): Whether to strip code from function bodies
        chunk_level (str): The level at which to chunk the content ('function' or 'file')

    Returns:
        str: The processed content of the Python file
    """
    # Extract header (imports and module docstring)
    header_end = content.find("class ") if "class " in content else content.find("def ") if "def " in content else len(content)
    header = content[:header_end].strip()
    
    if not header:
        return ""
        
    processed_content = [f"\n\n----- {filepath.name} - header -----\n\n{header}\n\n----- End of header -----\n\n"]
    
    if chunk_level == 'function':
        # Improved regex to better handle nested functions and class methods
        function_pattern = r"(?:^|\n)(?:async\s+)?def\s+[^()]+\([^)]*\)\s*(?:->[^:]+)?:\s*(?:[^\n]*\n\s+[^\n]+)*"
        function_chunks = re.finditer(function_pattern, content[header_end:], re.MULTILINE)
        
        for match in function_chunks:
            chunk = match.group(0)
            if strip_code:
                chunk = strip_code_from_functions(chunk)
            processed_content.append(
                f"\n\n----- {filepath.name} - chunk -----\n\n{chunk.strip()}\n\n----- End of chunk -----\n\n"
            )
    else:
        remaining_content = strip_code_from_functions(content[header_end:]) if strip_code else content[header_end:]
        if remaining_content.strip():
            processed_content.append(
                f"\n\n----- {filepath.name} - full_file -----\n\n{remaining_content.strip()}\n\n----- End of full_file -----\n\n"
            )
    
    return "".join(processed_content)

def handle_markdown_file(content, filepath):
    """
    Processes Markdown files, splitting them into sections.

    Args:
        content (str): The content of the Markdown file.
        filepath (Path): The path to the Markdown file.

    Returns:
        str: The processed content of the Markdown file.
    """
    if filepath.name in ["Comprehensive_Library_Guide.md", "STYLE_GUIDE.md"]:
        return f"\n\n----- {filepath.name} - full_file -----\n\n{content}\n\n----- End of {filepath.name} -----\n\n"
    
    sections = re.split(r'\n#+ ', content)
    processed_content = ""
    for section in sections:
        processed_content += f"\n\n----- {filepath.name} - section -----\n\n# {section}\n\n----- End of section -----\n\n"
    return processed_content

def combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False, chunk_level='function', selected_files=None):
    """
    Combines and processes multiple files, respecting omission rules and file selection.
    
    Args:
        summarize_subfolder (Path): The root folder to process
        omit_folders (list): List of folder names to omit
        omit_extensions (list): List of file extensions to omit
        omit_files (list): List of specific file names to omit
        strip_code (bool): Whether to strip code from function bodies
        chunk_level (str): The level at which to chunk content
        selected_files (list): Optional list of specific files to include
    
    Returns:
        tuple: (combined_text, total_token_count, file_token_counts)
    """
    combined_text = []
    file_token_counts = {}
    total_token_count = 0
    
    this_script = Path(__file__).name
    summarize_subfolder = Path(summarize_subfolder)
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")

    # Convert selected_files paths to relative paths for comparison
    selected_file_paths = None
    if selected_files:
        selected_file_paths = {Path(f).as_posix() for f in selected_files}

    for filepath in sorted(summarize_subfolder.rglob('*')):
        # Skip directories and filtered items
        if not filepath.is_file() or filepath.name == this_script:
            continue
            
        if (any(omit_folder in filepath.parts for omit_folder in omit_folders) or
            filepath.suffix.lower() in omit_extensions or
            any(omit_file in filepath.name for omit_file in omit_files)):
            continue

        # Check if file is in selected files list
        if selected_file_paths:
            relative_path = filepath.relative_to(summarize_subfolder).as_posix()
            if relative_path not in selected_file_paths:
                continue

        try:
            content = filepath.read_text(encoding='utf-8')
        except UnicodeDecodeError:
            content = filepath.read_bytes().decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"Error reading {filepath}: {e}")
            continue

        processed_content = ""
        if filepath.suffix.lower() == '.py':
            processed_content = handle_python_file(content, filepath, strip_code, chunk_level)
        elif filepath.suffix.lower() == '.md':
            processed_content = handle_markdown_file(content, filepath)
        else:
            processed_content = f"\n\n----- {filepath.name} - full_file -----\n\n{content}\n\n----- End of {filepath.name} -----\n\n"
        
        if processed_content:
            combined_text.append(processed_content)
            file_tokens = len(enc.encode(processed_content))
            file_token_counts[str(filepath)] = file_tokens
            total_token_count += file_tokens

    return "".join(combined_text), total_token_count, file_token_counts

==================================================

File: C:\GH\ras-commander\library_assistant\web\routes.py
==================================================
"""
Web routes for the Library Assistant.

This module defines the FastAPI routes for the web interface of the Library Assistant.
It handles user interactions, API calls, and serves the HTML template.

Routes:
- GET /: Serves the main page of the application.
- POST /chat: Handles chat interactions with the AI model.
- POST /submit: Handles form submissions for updating settings.
- POST /save_conversation: Saves the current conversation history to a file.
- GET /get_file_tree: Returns the file tree structure with token counts.
- POST /refresh_context: Refreshes the context processing for both RAG and full context modes.
"""

from fastapi import APIRouter, Request, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from config.config import load_settings, update_settings
from utils.conversation import add_to_history, get_full_conversation, save_conversation
from utils.context_processing import prepare_full_prompt, update_conversation_history, initialize_rag_context
from utils.cost_estimation import create_pricing_df, estimate_cost
from utils.file_handling import set_context_folder
from api.anthropic import anthropic_stream_response, get_anthropic_client
from api.openai import openai_stream_response, get_openai_client
import json
import tiktoken
import os
from pathlib import Path
import uuid
from datetime import datetime

router = APIRouter()

# Set up Jinja2 templates
templates = Jinja2Templates(directory="web/templates")

# Set up static files
static_files = StaticFiles(directory="web/static")

@router.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """
    Serves the main page of the application.

    Args:
        request (Request): The incoming request object.

    Returns:
        TemplateResponse: The rendered HTML template with current settings.
    """
    settings = load_settings()
    return templates.TemplateResponse("index.html", {
        "request": request,
        "settings": settings
    })

@router.post("/chat")
async def chat(request: Request, message: dict):
    """
    Handles chat interactions with the AI model, supporting streaming responses.
    """
    try:
        # Validate input
        user_message = message.get("message")
        if not user_message:
            raise HTTPException(status_code=400, detail="No message provided.")
            
        # Get selected files for context if provided
        selected_files = message.get("selectedFiles", [])
        
        # Add user message to history
        add_to_history("user", user_message)
        
        # Load settings and prepare context
        settings = load_settings()
        selected_model = settings.selected_model
        
        # Get or generate conversation ID
        conversation_id = message.get("conversation_id", str(uuid.uuid4()))
        
        # Prepare prompt with conversation history
        full_prompt = prepare_full_prompt(
            user_message,
            selected_files if settings.context_mode == 'full_context' else None,
            conversation_id
        )
        
        # Update conversation history
        update_conversation_history(conversation_id, "user", user_message)
        
        # Estimate token usage and cost
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        input_tokens = len(enc.encode(full_prompt))
        output_tokens = 1000  # Assuming average response length
        
        # Get pricing info
        pricing_info = create_pricing_df(selected_model)
        pricing_df = pricing_info["pricing_df"]
        provider = pricing_info["provider"]
        
        estimated_cost = estimate_cost(input_tokens, output_tokens, pricing_df)

        async def stream_response():
            """Generator for streaming the AI response"""
            accumulated_response = []
            try:
                if provider == "anthropic":
                    client = get_anthropic_client(settings.anthropic_api_key)
                    async for chunk in anthropic_stream_response(client, full_prompt):
                        accumulated_response.append(chunk)
                        yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                        
                elif provider == "openai":
                    client = get_openai_client(settings.openai_api_key)
                    messages = [
                        {"role": "system", "content": "You are a helpful AI assistant."},
                        {"role": "user", "content": full_prompt}
                    ]
                    async for chunk in openai_stream_response(client, selected_model, messages):
                        accumulated_response.append(chunk)
                        yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                else:
                    raise HTTPException(status_code=400, detail="Unsupported provider selected.")
                
                # Add complete response to history
                complete_response = "".join(accumulated_response)
                add_to_history("assistant", complete_response)
                
                # Update history with assistant response
                update_conversation_history(conversation_id, "assistant", complete_response)
                
                # Send the cost estimate as a final message
                yield f"data: {json.dumps({'cost': estimated_cost, 'provider': provider})}\n\n"
                
            except Exception as e:
                error_msg = f"Error during streaming: {str(e)}"
                yield f"data: {json.dumps({'error': error_msg})}\n\n"

        return StreamingResponse(
            stream_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@router.post("/submit")
async def handle_submit(
    request: Request,
    anthropic_api_key: str = Form(None),
    openai_api_key: str = Form(None),
    selected_model: str = Form(None),
    context_mode: str = Form(None),
    initial_chunk_size: int = Form(None),
    followup_chunk_size: int = Form(None),
):
    """
    Handles form submissions for updating settings.

    This function updates the application settings based on form data submitted by the user.

    Args:
        request (Request): The incoming request object.
        anthropic_api_key (str, optional): The Anthropic API key.
        openai_api_key (str, optional): The OpenAI API key.
        selected_model (str, optional): The selected AI model.
        context_mode (str, optional): The context handling mode.
        initial_chunk_size (int, optional): The initial chunk size for RAG mode.
        followup_chunk_size (int, optional): The followup chunk size for RAG mode.

    Returns:
        JSONResponse: A JSON object indicating the success status of the update.
    """
    updated_data = {}
    if anthropic_api_key is not None:
        updated_data["anthropic_api_key"] = anthropic_api_key
    if openai_api_key is not None:
        updated_data["openai_api_key"] = openai_api_key
    if selected_model is not None:
        updated_data["selected_model"] = selected_model
    if context_mode is not None:
        updated_data["context_mode"] = context_mode
    if initial_chunk_size is not None:
        updated_data["initial_chunk_size"] = initial_chunk_size
    if followup_chunk_size is not None:
        updated_data["followup_chunk_size"] = followup_chunk_size
    
    update_settings(updated_data)
    return JSONResponse({"status": "success"})

@router.post("/save_conversation")
async def save_conversation_endpoint():
    """
    Saves the current conversation history to a file.

    This function triggers the saving of the conversation history and returns the file
    for download.

    Returns:
        FileResponse: The saved conversation history file for download.

    Raises:
        HTTPException: If there's an error saving the conversation.
    """
    try:
        file_path = save_conversation()
        return FileResponse(file_path, filename=os.path.basename(file_path))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save conversation: {str(e)}")

@router.get("/get_file_tree")
async def get_file_tree():
    """Get the file tree structure for the project."""
    try:
        root_dir = Path(__file__).parent.parent.parent
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        def count_tokens(content):
            return len(enc.encode(content))
        
        def build_tree(path):
            """Recursively build the file tree structure."""
            if path.name == '__pycache__':
                return None
                
            item = {
                "name": path.name,
                "path": str(path.relative_to(root_dir)),
                "type": "directory" if path.is_dir() else "file"
            }
            
            if path.is_dir():
                children = []
                total_tokens = 0
                
                cursorrules = path / '.cursorrules'
                if cursorrules.exists():
                    cursorrules_item = build_tree(cursorrules)
                    if cursorrules_item:
                        children.append(cursorrules_item)
                        total_tokens += cursorrules_item.get("tokens", 0)
                
                for child in path.iterdir():
                    if child.name != '.cursorrules':
                        child_item = build_tree(child)
                        if child_item:
                            children.append(child_item)
                            total_tokens += child_item.get("tokens", 0)
                
                item["children"] = sorted(children, key=lambda x: (x["type"] == "file", x["name"]))
                item["tokens"] = total_tokens
            else:
                try:
                    content = path.read_text(encoding='utf-8')
                    item["tokens"] = count_tokens(content)
                except (UnicodeDecodeError, OSError):
                    item["tokens"] = 0
            
            return item
            
        tree = build_tree(root_dir)
        return {"fileTree": tree}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error building file tree: {str(e)}")

@router.get("/get_file_content")
async def get_file_content(path: str):
    """Get the content of a specific file."""
    try:
        # Get the project root directory
        root_dir = Path(__file__).parent.parent.parent
        file_path = root_dir / path
        
        # Validate the path is within the project directory
        if not str(file_path.resolve()).startswith(str(root_dir.resolve())):
            raise HTTPException(status_code=403, detail="Access denied")
            
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="File not found")
            
        # Read and return the file content
        try:
            content = file_path.read_text(encoding='utf-8')
            return {"content": content}
        except UnicodeDecodeError:
            return {"content": "Binary file - cannot display content"}
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading file: {str(e)}")

@router.post("/refresh_context")
async def refresh_context():
    """
    Refreshes the context processing for both RAG and full context modes.
    """
    try:
        await initialize_rag_context()
        return JSONResponse({
            "status": "success",
            "message": "Context refreshed successfully",
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        raise HTTPException(
            status_code=500, 
            detail=f"Failed to refresh context: {str(e)}"
        )
==================================================

Folder: C:\GH\ras-commander\library_assistant\web\static
==================================================

Folder: C:\GH\ras-commander\library_assistant\web\templates
==================================================

File: C:\GH\ras-commander\library_assistant\web\static\fileTree.js
==================================================
import React, { useState, useEffect } from 'react';

// Override console.log to send logs to the server
(function() {
    const originalLog = console.log;
    console.log = function(...args) {
        originalLog.apply(console, args);
        fetch('/api/log', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({ message: args.join(' ') })
        }).catch(error => originalLog('Error sending log:', error));
    };
})();

// File node component
const FileNode = ({ file, path, onSelect, isSelected }) => (
  <div className="file ps-3">
    <div className="d-flex align-items-center py-1 hover-bg-light">
      <input 
        type="checkbox" 
        className="form-check-input me-2"
        checked={isSelected}
        onChange={(e) => onSelect(path, file.tokens, e.target.checked)}
      />
      <span className="me-2">ðŸ“„</span>
      <span>{file.name}</span>
      <small className="text-muted ms-2">({file.tokens?.toLocaleString() || 0} tokens)</small>
    </div>
  </div>
);

// Folder node component
const FolderNode = ({ folder, path, expanded, onToggle, children }) => (
  <div className="folder ps-3">
    <div className="d-flex align-items-center py-1 hover-bg-light">
      <button 
        className="btn btn-sm p-0 me-2"
        onClick={() => onToggle(path)}
      >
        {expanded ? 'â–¼' : 'â–¶'}
      </button>
      <span className="me-2">ðŸ“</span>
      <span>{folder.name}</span>
      <small className="text-muted ms-2">({folder.tokens?.toLocaleString() || 0} tokens)</small>
    </div>
    {expanded && <div className="children ps-3">{children}</div>}
  </div>
);

// Stats display component
const StatsDisplay = ({ stats, tokens }) => (
  <div className="p-3 bg-gray-100 rounded border sticky-bottom">
    <div className="mb-2 border-bottom pb-2">
      <div className="d-flex justify-content-between">
        <span>Selected Files:</span>
        <strong>{stats.fileCount} files</strong>
      </div>
      <div className="d-flex justify-content-between">
        <span>Selected Tokens:</span>
        <strong>{stats.tokens.toLocaleString()}</strong>
      </div>
    </div>
    <div>
      <div className="fw-bold mb-1">Estimated Context Costs:</div>
      <div className="d-flex justify-content-between">
        <span>Claude 3.5:</span>
        <strong>${stats.costs.claude.toFixed(4)}</strong>
      </div>
      <div className="d-flex justify-content-between">
        <span>GPT-4:</span>
        <strong>${stats.costs.gpt4.toFixed(4)}</strong>
      </div>
      <div className="d-flex justify-content-between">
        <span>GPT-4 Mini:</span>
        <strong>${stats.costs.gpt4mini.toFixed(4)}</strong>
      </div>
      <div className="mt-2 pt-2 border-top">
        <div className="d-flex justify-content-between text-muted">
          <small>Current Conversation:</small>
          <small>{tokens.conversation.toLocaleString()} tokens</small>
        </div>
        <div className="d-flex justify-content-between text-muted">
          <small>Message Being Typed:</small>
          <small>{tokens.currentMessage.toLocaleString()} tokens</small>
        </div>
      </div>
    </div>
  </div>
);

// Main FileTreeViewer component
const FileTreeViewer = ({ initialData }) => {
  const [fileData, setFileData] = useState(initialData);
  const [selectedFiles, setSelectedFiles] = useState(new Set());
  const [expandedFolders, setExpandedFolders] = useState(new Set(['library_assistant']));
  const [fileContents, setFileContents] = useState(new Map());
  const [tokens, setTokens] = useState({ conversation: 0, currentMessage: 0 });
  const [statsUpdate, setStatsUpdate] = useState(0);

  // Handle file selection and deselection
  const handleFileSelect = async (path, tokens, selected) => {
    console.group(`handleFileSelect: ${path}`);
    console.log('Selected:', selected);
    console.log('Tokens:', tokens);
    
    if (selected) {
        try {
            console.log('Fetching file content...');
            const content = await getFileContent(path);
            console.log('Content received:', !!content);
            
            if (content) {
                setSelectedFiles(prev => {
                    const next = new Set([...prev, path]);
                    console.log('Updated selected files:', Array.from(next));
                    return next;
                });
                setFileContents(prev => new Map(prev).set(path, content));
            }
        } catch (error) {
            console.error('Error loading file:', error);
        }
    } else {
        setSelectedFiles(prev => {
            const next = new Set(prev);
            next.delete(path);
            console.log('Updated selected files after removal:', Array.from(next));
            return next;
        });
    }
    
    setStatsUpdate(prev => prev + 1);
    console.groupEnd();
  };

  // Handle folder expansion
  const handleFolderToggle = (path) => {
    setExpandedFolders(prev => {
      const next = new Set(prev);
      if (next.has(path)) {
        next.delete(path);
      } else {
        next.add(path);
      }
      return next;
    });
  };

  // Fetch file content
  const getFileContent = async (path) => {
    if (fileContents.has(path)) {
      return fileContents.get(path);
    }

    const response = await fetch(`/get_file_content?path=${encodeURIComponent(path)}`);
    if (!response.ok) throw new Error('Failed to fetch file content');
    
    return await response.json();
  };

  // Render the file tree
  const renderTree = (item, path = '') => {
    const fullPath = path ? `${path}/${item.name}` : item.name;
    console.group(`renderTree: ${fullPath}`);
    console.log('Item:', item);
    console.log('Current path:', path);
    console.log('Full path:', fullPath);
    
    let result;
    if (item.type === 'directory') {
        const isExpanded = expandedFolders.has(fullPath);
        console.log('Directory is expanded:', isExpanded);
        result = (
            <FolderNode
                key={fullPath}
                folder={item}
                path={fullPath}
                expanded={isExpanded}
                onToggle={handleFolderToggle}
            >
                {isExpanded && item.children?.map(child => renderTree(child, fullPath))}
            </FolderNode>
        );
    } else {
        const isSelected = selectedFiles.has(fullPath);
        console.log('File is selected:', isSelected);
        result = (
            <FileNode
                key={fullPath}
                file={item}
                path={fullPath}
                isSelected={isSelected}
                onSelect={handleFileSelect}
            />
        );
    }
    
    console.groupEnd();
    return result;
  };

  useEffect(() => {
    console.group('FileTreeViewer Initialization');
    console.log('Initial data:', initialData);
    console.log('File data structure:', fileData);
    console.groupEnd();
  }, [initialData, fileData]);

  const updateDisplays = () => {
    document.getElementById('selected-files-count').textContent = `${selectedFiles.size} files`;
    document.getElementById('selected-tokens-count').textContent = calculateStats().tokens.toLocaleString();
    // Update cost displays
    document.getElementById('claude-cost').textContent = `$${calculateStats().costs.claude.toFixed(4)}`;
    document.getElementById('gpt4-cost').textContent = `$${calculateStats().costs.gpt4.toFixed(4)}`;
    document.getElementById('gpt4-mini-cost').textContent = `$${calculateStats().costs.gpt4mini.toFixed(4)}`;
  };

  useEffect(() => {
    updateDisplays();
  }, [selectedFiles, statsUpdate]);

  return (
    <div className="file-tree-container">
      <div className="file-tree-header">
        <div className="d-flex justify-content-between align-items-center w-100">
          <h5 className="mb-0">Project Files</h5>
          <button className="btn btn-sm btn-primary" onClick={() => window.location.reload()}>
            <span className="refresh-icon">â†»</span> Refresh Context
          </button>
        </div>
      </div>
      <div className="file-tree">
        {fileData && renderTree(fileData)}
      </div>
      <StatsDisplay key={statsUpdate} stats={calculateStats()} tokens={tokens} />
    </div>
  );
};

export default FileTreeViewer;
==================================================

File: C:\GH\ras-commander\library_assistant\web\static\styles.css
==================================================
/* Add these styles to your existing CSS */
.file-tree-dropdown {
    width: 100%;
    border: 1px solid #dee2e6;
    border-radius: 0.25rem;
}

.tree-directory {
    font-weight: bold;
}

.tree-file {
    padding-left: 1rem;
}

.file-tag {
    background-color: #e3f2fd;
    border-radius: 0.25rem;
    padding: 0.25rem 0.5rem;
    margin: 0.25rem;
    display: inline-block;
}

/* Customize dropdown appearance */
.dropdown-content {
    max-height: 400px;
    overflow-y: auto;
}

/* Style the checkboxes */
.checkbox-item {
    margin-right: 0.5rem;
}

/* Style partially selected nodes */
.node.partial .checkbox-item:indeterminate {
    background-color: #86b7fe;
} 
==================================================

File: C:\GH\ras-commander\library_assistant\web\templates\index.html
==================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAS Commander Library Assistant</title>
    <!-- Link to Bootstrap CSS for styling -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Include marked.js for markdown rendering -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/4.3.0/marked.min.js"></script>
    <style>
        /* Basic styling for the body */
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f8f9fa; }
        /* Container styling for layout */
        .container { max-width: 1800px; margin: auto; width: 98%; }
        .form-group { margin-bottom: 15px; }
        label { display: block; margin-bottom: 5px; }
        input, select, textarea { width: 100%; padding: 8px; box-sizing: border-box; }
        /* Chat box styling */
        .chat-box { border: 1px solid #ced4da; padding: 10px; height: 400px; overflow-y: scroll; background-color: #ffffff; border-radius: 5px; }
        .message { margin: 10px 0; }
        .user { color: #0d6efd; }
        .assistant { color: #198754; }
        .error { color: red; }
        /* Button styling */
        .btn { padding: 10px 20px; background-color: #0d6efd; color: white; border: none; cursor: pointer; border-radius: 5px; }
        .btn:hover { background-color: #0b5ed7; }
        .cost { margin-top: 10px; font-weight: bold; }
        /* Copy button styling */
        .copy-btn {
            margin-left: 10px;
            padding: 5px 10px;
            background-color: #0d6efd;
            color: white;
            border: none;
            cursor: pointer;
            border-radius: 5px;
        }
        .copy-btn:hover {
            background-color: #0b5ed7;
        }
        .hidden { display: none; }
        /* Slider Styling */
        input[type=range] {
            -webkit-appearance: none;
            width: 100%;
            height: 10px;
            border-radius: 5px;
            background: #d3d3d3;
            outline: none;
            margin-top: 10px;
        }
        input[type=range]::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #0d6efd;
            cursor: pointer;
        }
        input[type=range]::-moz-range-thumb {
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #0d6efd;
            cursor: pointer;
        }
        /* File tree styling */
        #file-tree {
            max-height: 600px;
            overflow-y: auto;
        }
        
        .file-tree-item {
            padding: 4px 8px;
            cursor: pointer;
            display: flex;
            align-items: center;
        }
        
        .file-tree-item:hover {
            background-color: #f0f0f0;
        }
        
        .file-tree-toggle {
            margin-right: 5px;
            width: 16px;
            height: 16px;
            text-align: center;
            line-height: 16px;
        }
        
        .file-tree-indent {
            margin-left: 20px;
        }
        
        .file-tree-selected {
            background-color: #e3f2fd;
        }

        /* Enhanced file tree container styling */
        .file-tree-container {
            width: 800px;
            flex-shrink: 0;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            background: white;
            display: flex;
            flex-direction: column;
        }

        .file-tree-header {
            padding: 10px;
            background: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .file-tree {
            padding: 10px;
            max-height: 600px;
            overflow-y: auto;
        }

        .file-item {
            display: flex;
            align-items: center;
            padding: 4px 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.2s;
            width: 100%;
        }

        .file-item:hover {
            background-color: #f0f0f0;
        }

        .file-item.selected {
            background-color: #e3f2fd;
        }

        .file-icon {
            margin-right: 8px;
            width: 16px;
            text-align: center;
        }

        .folder-toggle {
            cursor: pointer;
            transition: transform 0.2s;
        }

        .folder-toggle.open {
            transform: rotate(90deg);
        }

        .file-children {
            margin-left: 20px;
        }

        /* Main container layout */
        .main-container {
            display: flex;
            gap: 20px;
            margin-top: 20px;
            height: calc(100vh - 200px);
            min-height: 400px;
            width: 100%;
        }

        .chat-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            min-width: 400px;
        }

        .file-tree-header {
            padding: 10px;
            background: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-shrink: 0;
        }

        .file-tree {
            padding: 10px;
            overflow-y: auto;
            flex-grow: 1;
        }

        .chat-box {
            flex-grow: 1;
            overflow-y: auto;
            border: 1px solid #ced4da;
            border-radius: 5px;
            padding: 10px;
            background-color: #ffffff;
            margin-bottom: 10px;
        }

        /* Responsive adjustments */
        @media (max-width: 1400px) {
            .file-tree-container {
                width: 600px;
            }
        }

        @media (max-width: 768px) {
            .main-container {
                flex-direction: column;
                height: auto;
            }
            
            .file-tree-container {
                width: 100%;
                height: 300px;
                margin-bottom: 20px;
            }
            
            .chat-container {
                width: 100%;
            }
        }

        /* Checkbox styling */
        .file-checkbox {
            margin-right: 8px;
            cursor: pointer;
            width: 16px;
            height: 16px;
        }

        .file-item {
            display: flex;
            align-items: center;
            padding: 4px 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.2s;
        }

        .file-item:hover {
            background-color: #f0f0f0;
        }

        .file-item.selected {
            background-color: #e3f2fd;
        }

        .file-icon {
            margin-right: 8px;
            width: 16px;
            text-align: center;
        }

        .file-item span {
            cursor: pointer;
        }

        .file-checkbox:indeterminate {
            background-color: #86b7fe;
            border-color: #86b7fe;
        }

        /* Folder toggle styling */
        .folder-toggle {
            display: inline-block;
            width: 16px;
            height: 16px;
            line-height: 14px;
            text-align: center;
            cursor: pointer;
            font-family: monospace;
            font-weight: bold;
            font-size: 16px;
            color: #666;
            user-select: none;
        }

        .folder-toggle:hover {
            color: #000;
        }

        .file-icon {
            margin-right: 8px;
            width: 16px;
            text-align: center;
            cursor: pointer;
        }

        .file-item {
            display: flex;
            align-items: center;
            padding: 4px 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.2s;
        }

        .file-children {
            margin-left: 0; /* Remove left margin since we're using padding */
        }

        /* Token count display styling */
        .token-count {
            margin-left: auto;
            color: #666;
            font-size: 0.85em;
            padding-left: 10px;
        }

        .total-tokens-display {
            padding: 10px;
            background-color: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            font-weight: bold;
            color: #0d6efd;
        }

        .refresh-icon {
            display: inline-block;
            transition: transform 0.5s;
        }
        
        .refresh-icon.spinning {
            animation: spin 1s linear infinite;
        }
        
        @keyframes spin {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        /* Advanced Settings Popover Styling */
        .advanced-settings-popover {
            min-width: 300px;
            padding: 10px;
        }

        .advanced-settings-popover .form-group {
            margin-bottom: 15px;
        }

        .advanced-settings-popover label {
            font-weight: 500;
            margin-bottom: 5px;
        }

        .popover {
            max-width: 400px;
        }

        .popover-body {
            padding: 15px;
        }

        /* Add these new styles */
        .sticky-bottom {
            position: sticky;
            bottom: 0;
            background-color: #f8f9fa;
            border-top: 1px solid #dee2e6;
            z-index: 1000;
        }
        
        .file-tree-container {
            display: flex;
            flex-direction: column;
            max-height: calc(100vh - 250px);
        }
        
        .file-tree {
            flex: 1;
            overflow-y: auto;
            min-height: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="d-flex justify-content-between align-items-center mt-4 mb-4">
            <h1>RAS Commander Library Assistant</h1>
            <button class="btn btn-outline-secondary" 
                    type="button" 
                    data-bs-toggle="collapse" 
                    data-bs-target="#settingsCollapse"
                    aria-expanded="false"
                    aria-controls="settingsCollapse">
                âš™ï¸ Settings
            </button>
        </div>

        <!-- Settings collapse panel -->
        <div class="collapse mb-4" id="settingsCollapse">
            <div class="card">
                <div class="card-body">
                    <form id="settings-form">
                        <div class="form-group">
                            <label for="selected_model">Select Model:</label>
                            <select id="selected_model" name="selected_model" class="form-select" required>
                                <option value="" disabled>Select a model</option>
                                <option value="claude-3-5-sonnet-20240620" {% if settings.selected_model == 'claude-3-5-sonnet-20240620' %}selected{% endif %}>Claude 3.5 Sonnet (Anthropic)</option>
                                <option value="gpt-4o-2024-08-06" {% if settings.selected_model == 'gpt-4o-2024-08-06' %}selected{% endif %}>GPT-4o (OpenAI)</option>
                                <option value="gpt-4o-mini" {% if settings.selected_model == 'gpt-4o-mini' %}selected{% endif %}>GPT-4o-mini (OpenAI)</option>
                                <option value="o1-mini" {% if settings.selected_model == 'o1-mini' %}selected{% endif %}>o1-mini (OpenAI)</option>
                            </select>
                        </div>
                        <!-- API key input for Anthropic -->
                        <div id="anthropic_api_key_group" class="form-group hidden">
                            <label for="anthropic_api_key">Anthropic API Key:</label>
                            <input type="password" id="anthropic_api_key" name="anthropic_api_key" class="form-control" value="{{ settings.anthropic_api_key }}">
                        </div>
                        <!-- API key input for OpenAI -->
                        <div id="openai_api_key_group" class="form-group hidden">
                            <label for="openai_api_key">OpenAI API Key:</label>
                            <input type="password" id="openai_api_key" name="openai_api_key" class="form-control" value="{{ settings.openai_api_key }}">
                        </div>
                        <div class="form-group">
                            <label for="context_mode">Context Handling Mode:</label>
                            <select id="context_mode" name="context_mode" class="form-select" required>
                                <option value="" disabled>Select a mode</option>
                                <option value="full_context" {% if settings.context_mode == 'full_context' %}selected{% endif %}>Full Context</option>
                                <option value="rag" {% if settings.context_mode == 'rag' %}selected{% endif %}>Retrieval-Augmented Generation (RAG)</option>
                            </select>
                        </div>
                        <!-- RAG context size sliders -->
                        <div id="rag_sliders" class="hidden">
                            <div class="form-group">
                                <label for="initial_chunk_size">Initial RAG Context Size (tokens):</label>
                                <input type="range" id="initial_chunk_size" name="initial_chunk_size" min="16000" max="96000" step="1000" value="{{ settings.initial_chunk_size }}">
                                <span id="initial_chunk_size_value">{{ settings.initial_chunk_size }}</span>
                            </div>
                            <div class="form-group">
                                <label for="followup_chunk_size">Follow-up RAG Context Size (tokens):</label>
                                <input type="range" id="followup_chunk_size" name="followup_chunk_size" min="16000" max="64000" step="1000" value="{{ settings.followup_chunk_size }}">
                                <span id="followup_chunk_size_value">{{ settings.followup_chunk_size }}</span>
                            </div>
                        </div>
                    </form>
                </div>
                <div class="card-footer text-end">
                    <button type="button" 
                            class="btn btn-secondary" 
                            id="advanced-settings-btn"
                            data-bs-toggle="popover" 
                            data-bs-placement="bottom" 
                            data-bs-html="true"
                            data-bs-title="Advanced Settings">
                        Advanced Settings
                    </button>
                </div>
            </div>
        </div>
        
        <div class="main-container">
            <!-- File Tree Container -->
            <div id="file-tree-container" class="file-tree-container" style="display: none;">
                <div class="file-tree-header">
                    <div class="d-flex justify-content-between align-items-center w-100">
                        <h5 class="mb-0">Project Files</h5>
                        <div class="d-flex gap-2">
                            <button class="btn btn-sm btn-primary" id="refresh-context">
                                <span class="refresh-icon">â†»</span> Refresh Context
                            </button>
                        </div>
                    </div>
                    <div id="refresh-status" class="text-muted small mt-1" style="display: none;"></div>
                    <div id="refresh-error" class="alert alert-danger small mt-1" style="display: none;"></div>
                </div>
                <div class="file-tree" id="file-tree"></div>
                
                <!-- Cost Estimation Panel -->
                <div class="p-3 bg-gray-100 rounded border sticky-bottom">
                    <div class="mb-2 border-bottom pb-2">
                        <div class="d-flex justify-content-between">
                            <span>Selected Files:</span>
                            <strong id="selected-files-count">0 files</strong>
                        </div>
                        <div class="d-flex justify-content-between">
                            <span>Selected Tokens:</span>
                            <strong id="selected-tokens-count">0</strong>
                        </div>
                    </div>
                    <div>
                        <div class="fw-bold mb-1">Estimated Context Costs:</div>
                        <div class="d-flex justify-content-between">
                            <span>Claude 3.5:</span>
                            <strong id="claude-cost">$0.0000</strong>
                        </div>
                        <div class="d-flex justify-content-between">
                            <span>GPT-4:</span>
                            <strong id="gpt4-cost">$0.0000</strong>
                        </div>
                        <div class="d-flex justify-content-between">
                            <span>GPT-4 Mini:</span>
                            <strong id="gpt4-mini-cost">$0.0000</strong>
                        </div>
                        <div class="mt-2 pt-2 border-top">
                            <div class="d-flex justify-content-between text-muted">
                                <small>Current Conversation:</small>
                                <small id="conversation-tokens">0 tokens</small>
                            </div>
                            <div class="d-flex justify-content-between text-muted">
                                <small>Message Being Typed:</small>
                                <small id="current-message-tokens">0 tokens</small>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            

            
            <!-- Chat Container -->
            <div class="chat-container">
                <!-- Add header similar to file tree -->
                <div class="file-tree-header">
                    <div class="d-flex justify-content-between align-items-center w-100">
                        <h5 class="mb-0">Chat Window</h5>
                        <div class="d-flex gap-2">
                            <button class="btn btn-sm btn-primary" id="reset-chat">
                                Reset Conversation
                            </button>
                        </div>
                    </div>
                </div>
                <div class="chat-box" id="chat-box"></div>
                <form id="chat-form" class="mt-3">
                    <div class="form-group">
                        <label for="user-input">Your message:</label>
                        <textarea id="user-input" class="form-control" rows="3" required></textarea>
                    </div>
                    <div class="mt-2 d-flex justify-content-between align-items-center">
                        <div class="d-flex align-items-center gap-3">
                            <button type="submit" class="btn btn-primary">Send</button>
                            <div id="cost-display" class="text-muted"></div>
                        </div>
                        <button type="button" onclick="saveConversation()" class="btn btn-outline-secondary btn-sm">
                            Save Conversation
                        </button>
                    </div>
                </form>
            </div>
        </div>
    </div>
    
    <!-- Include Bootstrap JS for functionality -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        // FileTreeViewer class to manage the file tree display and interactions
        class FileTreeViewer {
            constructor(container) {
                this.container = container; // Container for the file tree
                this.selectedFiles = new Set(); // Set to track selected files
                this.onSelectionChange = null; // Callback for selection changes
                this.totalTokensDisplay = document.createElement('div'); // Display for total tokens
                this.totalTokensDisplay.className = 'total-tokens-display';
                container.parentElement.insertBefore(this.totalTokensDisplay, container); // Insert total tokens display before the container
                this.expandedFolders = [
                    'ras-commander',           // Root project folder
                    'ras-commander/examples',  // Examples folder
                    'ras-commander/ras_commander'  // Main package folder
                ];
            }

            // Update the total tokens display based on selected files
            updateTotalTokens() {
                let totalTokens = 0; // Initialize total tokens
                this.selectedFiles.forEach(path => {
                    const element = this.container.querySelector(`[data-path="${path}"]`); // Find the file element
                    if (element) {
                        totalTokens += parseInt(element.dataset.tokens || 0); // Add tokens from the selected file
                    }
                });
                this.totalTokensDisplay.textContent = `Selected: ${totalTokens.toLocaleString()} tokens`; // Update display
            }

            // Render a node (file or directory) in the file tree
            renderNode(node, level = 0) {
                const item = document.createElement('div'); // Create a new div for the item
                item.className = 'file-item'; // Set class for styling
                item.style.paddingLeft = `${level * 20}px`; // Indent based on level
                item.dataset.path = node.path; // Store the path in a data attribute
                item.dataset.tokens = node.tokens; // Store the token count in a data attribute
                
                const checkbox = document.createElement('input'); // Create a checkbox for selection
                checkbox.type = 'checkbox';
                checkbox.className = 'file-checkbox me-2'; // Add class for styling
                
                if (node.type === 'directory') {
                    // Directory rendering
                    const toggleBtn = document.createElement('span'); // Button to toggle folder open/close
                    toggleBtn.className = 'folder-toggle me-2';
                    
                    const icon = document.createElement('span'); // Icon for folder
                    icon.className = 'file-icon';
                    
                    const nameSpan = document.createElement('span'); // Name of the file or folder
                    nameSpan.textContent = node.name;
                    
                    const tokenCount = document.createElement('span'); // Display token count
                    tokenCount.className = 'token-count';
                    tokenCount.textContent = `${node.tokens.toLocaleString()} tokens`;
                    
                    // Append elements to the item
                    item.appendChild(toggleBtn);
                    item.appendChild(checkbox);
                    item.appendChild(icon);
                    item.appendChild(nameSpan);
                    item.appendChild(tokenCount);
                    
                    const content = document.createElement('div'); // Container for child items
                    content.className = 'file-children';
                    
                    // Check if this folder should be expanded by default
                    const shouldExpand = this.expandedFolders.some(path => {
                        const normalizedPath = path.replace(/\\/g, '/');
                        const nodePath = node.path.replace(/\\/g, '/');
                        return normalizedPath === nodePath || normalizedPath.startsWith(nodePath + '/');
                    });
                    
                    content.style.display = shouldExpand ? 'block' : 'none'; // Set display based on expansion
                    toggleBtn.textContent = shouldExpand ? 'âˆ’' : '+'; // Set toggle button text
                    icon.textContent = shouldExpand ? 'ðŸ“‚' : 'ðŸ“'; // Set icon based on expansion
                    
                    // Function to handle expand/collapse
                    const toggleFolder = (e) => {
                        e.stopPropagation(); // Prevent event bubbling
                        const isExpanded = content.style.display !== 'none'; // Check if currently expanded
                        content.style.display = isExpanded ? 'none' : 'block'; // Toggle display
                        toggleBtn.textContent = isExpanded ? '+' : 'âˆ’'; // Update toggle button text
                        icon.textContent = isExpanded ? 'ðŸ“' : 'ðŸ“‚'; // Update icon
                    };
                    
                    // Add click handlers for toggle button, icon, and name
                    toggleBtn.onclick = toggleFolder;
                    icon.onclick = toggleFolder;
                    nameSpan.onclick = toggleFolder;
                    
                    // Handle directory checkbox click
                    checkbox.onclick = (e) => {
                        e.stopPropagation(); // Prevent event bubbling
                        this.toggleDirectory(node, checkbox.checked, content); // Toggle directory selection
                    };
                    
                    // Render child nodes if they exist
                    if (node.children) {
                        node.children.forEach(child => {
                            const childElement = this.renderNode(child, level + 1); // Render child node
                            content.appendChild(childElement); // Append child to content
                        });
                    }
                    
                    const container = document.createElement('div'); // Create a container for the item and its children
                    container.appendChild(item);
                    container.appendChild(content);
                    return container; // Return the complete node structure
                    
                } else {
                    // File rendering
                    const emptyToggle = document.createElement('span'); // Placeholder for folder toggle
                    emptyToggle.className = 'folder-toggle me-2';
                    emptyToggle.style.visibility = 'hidden'; // Hide placeholder
                    
                    const icon = document.createElement('span'); // Icon for file
                    icon.className = 'file-icon';
                    icon.textContent = 'ðŸ“„'; // File icon
                    
                    const nameSpan = document.createElement('span'); // Name of the file
                    nameSpan.textContent = node.name;
                    
                    const tokenCount = document.createElement('span'); // Display token count
                    tokenCount.className = 'token-count';
                    tokenCount.textContent = `${node.tokens.toLocaleString()} tokens`;
                    
                    checkbox.checked = this.selectedFiles.has(node.path); // Check if file is selected
                    
                    // Append elements to the item
                    item.appendChild(emptyToggle);
                    item.appendChild(checkbox);
                    item.appendChild(icon);
                    item.appendChild(nameSpan);
                    item.appendChild(tokenCount);
                    
                    // Update checkbox handler to include token counting
                    checkbox.onclick = (e) => {
                        e.stopPropagation(); // Prevent event bubbling
                        if (checkbox.checked) {
                            this.selectedFiles.add(node.path); // Add file to selected set
                        } else {
                            this.selectedFiles.delete(node.path); // Remove file from selected set
                        }
                        
                        if (this.onSelectionChange) {
                            this.onSelectionChange(Array.from(this.selectedFiles)); // Call selection change callback
                        }
                        
                        this.updateParentCheckboxes(item); // Update parent checkboxes
                        this.updateTotalTokens(); // Update total tokens display
                    };
                    
                    // Handle file name click (view content)
                    const viewContent = async (e) => {
                        e.stopPropagation(); // Prevent event bubbling
                        
                        const selected = document.querySelector('.file-tree-selected'); // Find currently selected item
                        if (selected) {
                            selected.classList.remove('file-tree-selected'); // Remove selection from previous item
                        }
                        
                        item.classList.add('file-tree-selected'); // Mark this item as selected
                        
                        try {
                            const response = await fetch(`/get_file_content?path=${encodeURIComponent(node.path)}`); // Fetch file content
                            if (!response.ok) {
                                throw new Error(`HTTP error! status: ${response.status}`); // Handle HTTP errors
                            }
                            const data = await response.json(); // Parse JSON response
                            
                            const userInput = document.getElementById('user-input'); // Get user input textarea
                            userInput.value = `Please help me with this file:\n\n${data.content}`; // Populate input with file content
                        } catch (error) {
                            console.error('Error loading file content:', error); // Log error
                            alert('Error loading file content: ' + error.message); // Alert user
                        }
                    };
                    
                    // Add click handlers for both icon and name
                    icon.onclick = viewContent;
                    nameSpan.onclick = viewContent;
                    
                    return item; // Return the file item
                }
            }

            // Toggle selection for a directory and its children
            toggleDirectory(node, checked, contentElement) {
                const checkboxes = contentElement.querySelectorAll('.file-checkbox'); // Get all checkboxes in the directory
                checkboxes.forEach(checkbox => {
                    checkbox.checked = checked; // Set checkbox state
                    
                    const fileItem = checkbox.closest('.file-item'); // Find the closest file item
                    const nameSpan = fileItem.querySelector('span:not(.file-icon):not(.token-count)'); // Get the name span
                    const path = this.getNodePath(node, nameSpan.textContent); // Get the full path
                    
                    if (checked) {
                        this.selectedFiles.add(path); // Add path to selected files
                    } else {
                        this.selectedFiles.delete(path); // Remove path from selected files
                    }
                });

                if (this.onSelectionChange) {
                    this.onSelectionChange(Array.from(this.selectedFiles)); // Call selection change callback
                }
                
                this.updateTotalTokens(); // Update total tokens display
            }

            // Construct the full path for a file within a directory
            getNodePath(node, fileName) {
                return node.path + '/' + fileName; // Return full path
            }

            // Update parent directory checkboxes based on child selections
            updateParentCheckboxes(element) {
                let parent = element.parentElement; // Start with the parent element
                while (parent) {
                    const parentCheckbox = parent.querySelector(':scope > .file-item > .file-checkbox'); // Find the parent checkbox
                    if (parentCheckbox) {
                        const childCheckboxes = parent.querySelectorAll('.file-children .file-checkbox'); // Get all child checkboxes
                        const allChecked = Array.from(childCheckboxes).every(cb => cb.checked); // Check if all are checked
                        const someChecked = Array.from(childCheckboxes).some(cb => cb.checked); // Check if some are checked
                        
                        parentCheckbox.checked = allChecked; // Set parent checkbox state
                        parentCheckbox.indeterminate = someChecked && !allChecked; // Set indeterminate state if some are checked
                    }
                    parent = parent.parentElement; // Move up to the next parent
                }
            }

            // Initialize the file tree with data
            initialize(data) {
                this.container.innerHTML = ''; // Clear existing content
                if (data) {
                    const rootElement = this.renderNode(data); // Render the root node
                    this.container.appendChild(rootElement); // Append the root element to the container
                }
            }

            // Get the currently selected files
            getSelectedFiles() {
                return Array.from(this.selectedFiles); // Return selected files as an array
            }

            async loadProcessedFiles() {
                try {
                    const response = await fetch('/get_file_tree');
                    if (!response.ok) {
                        throw new Error(`HTTP error! status: ${response.status}`);
                    }
                    const data = await response.json();
                    
                    // Clear current selection
                    this.selectedFiles.clear();
                    
                    // Reinitialize with new data
                    this.initialize(data.fileTree);
                    
                    // Update total tokens display
                    this.updateTotalTokens();
                    
                } catch (error) {
                    console.error('Error loading processed files:', error);
                    throw error;
                }
            }
        }

        // Wait for the DOM to be fully loaded
        document.addEventListener('DOMContentLoaded', function() {
            const selectedModel = document.getElementById('selected_model'); // Model selection dropdown
            const anthropicGroup = document.getElementById('anthropic_api_key_group'); // Anthropic API key input group
            const openaiGroup = document.getElementById('openai_api_key_group'); // OpenAI API key input group
            const contextMode = document.getElementById('context_mode'); // Context mode selection dropdown
            const fileTreeContainer = document.getElementById('file-tree-container'); // File tree container
            const fileTree = document.getElementById('file-tree'); // File tree element
            const chatContainer = document.querySelector('.chat-container'); // Chat container
            const ragSliders = document.getElementById('rag_sliders'); // RAG sliders container

            // Initialize file tree viewer
            const fileTreeViewer = new FileTreeViewer(fileTree); // Ensure this is defined before use

            // Load the file tree from the server
            async function loadFileTree() {
                try {
                    const response = await fetch('/get_file_tree'); // Fetch file tree data
                    if (!response.ok) {
                        throw new Error(`HTTP error! status: ${response.status}`); // Handle HTTP errors
                    }
                    const data = await response.json(); // Parse JSON response
                    fileTreeViewer.initialize(data.fileTree); // Initialize file tree viewer with data
                } catch (error) {
                    console.error('Error loading file tree:', error); // Log error
                    alert('Error loading file tree: ' + error.message); // Alert user
                }
            }

            // Toggle the visibility of the file tree based on context mode
            function toggleFileTree() {
                if (contextMode.value === 'full_context') {
                    fileTreeContainer.style.display = 'flex'; // Show file tree
                    loadFileTree(); // Load file tree data
                } else {
                    fileTreeContainer.style.display = 'none'; // Hide file tree
                }
            }

            // Toggle visibility of API key input fields based on selected model
            function toggleApiKeys() {
                const model = selectedModel.value; // Get selected model
                if (model.startsWith('claude')) {
                    anthropicGroup.classList.remove('hidden'); // Show Anthropic API key input
                    openaiGroup.classList.add('hidden'); // Hide OpenAI API key input
                } else if (model.startsWith('gpt') || model.startsWith('o1')) {
                    openaiGroup.classList.remove('hidden'); // Show OpenAI API key input
                    anthropicGroup.classList.add('hidden'); // Hide Anthropic API key input
                } else {
                    anthropicGroup.classList.add('hidden'); // Hide both API key inputs
                    openaiGroup.classList.add('hidden');
                }
            }

            // Toggle visibility of RAG sliders based on context mode
            function toggleRagSliders() {
                if (contextMode.value === 'rag') {
                    ragSliders.classList.remove('hidden'); // Show RAG sliders
                } else {
                    ragSliders.classList.add('hidden'); // Hide RAG sliders
                }
            }

            // Add event listeners for context mode and model selection changes
            contextMode.addEventListener('change', () => {
                toggleFileTree(); // Toggle file tree visibility
                toggleRagSliders(); // Toggle RAG sliders visibility
                autoSaveSettings(); // Auto-save settings
            });

            selectedModel.addEventListener('change', () => {
                toggleApiKeys(); // Toggle API key visibility
                autoSaveSettings(); // Auto-save settings
            });

            // Initialize UI state
            toggleApiKeys(); // Set initial API key visibility
            toggleRagSliders(); // Set initial RAG sliders visibility

            // Force initial file tree display if in full_context mode
            if (contextMode.value === 'full_context') {
                fileTreeContainer.style.display = 'flex'; // Show file tree
                loadFileTree(); // Load file tree data
            }

            // Handle chat form submission
            const chatForm = document.getElementById('chat-form'); // Chat form element
            const chatBox = document.getElementById('chat-box'); // Chat box element
            const userInput = document.getElementById('user-input'); // User input textarea
            const costDisplay = document.getElementById('cost-display'); // Cost display element

            chatForm.addEventListener('submit', async (e) => {
                e.preventDefault(); // Prevent default form submission
                const message = userInput.value.trim(); // Get user input
                if (message === '') return; // Do nothing if input is empty

                // Get selected files if in full_context mode
                const selectedFiles = contextMode.value === 'full_context' ? 
                    fileTreeViewer.getSelectedFiles() : []; // Get selected files

                // Display user message with markdown rendering
                const userMessage = document.createElement('div'); // Create user message element
                userMessage.className = 'message user'; // Set class for styling
                userMessage.innerHTML = 'User: ' + marked.parse(message); // Render message as markdown
                chatBox.appendChild(userMessage); // Append user message to chat box

                // Clear input
                userInput.value = ''; // Reset user input

                // Create assistant message container
                const assistantMessage = document.createElement('div'); // Create assistant message element
                assistantMessage.className = 'message assistant'; // Set class for styling
                assistantMessage.innerHTML = 'Assistant: '; // Initial assistant message
                const responseText = document.createElement('span'); // Span for response text
                assistantMessage.appendChild(responseText); // Append response text to assistant message
                chatBox.appendChild(assistantMessage); // Append assistant message to chat box

                // Scroll to bottom
                chatBox.scrollTop = chatBox.scrollHeight; // Scroll to the bottom of the chat box

                try {
                    const response = await fetch('/chat', { // Send chat message to server
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json', // Set content type
                            'Accept': 'text/event-stream', // Accept server-sent events
                        },
                        body: JSON.stringify({ 
                            message: message, // Send user message
                            selectedFiles: selectedFiles // Send selected files
                        })
                    });

                    const reader = response.body.getReader(); // Get reader for response body
                    const decoder = new TextDecoder(); // Create a text decoder
                    let buffer = ''; // Buffer for accumulating response
                    let fullResponse = ''; // Full response text

                    while (true) {
                        const {value, done} = await reader.read(); // Read from response
                        if (done) break; // Exit loop if done
                        
                        buffer += decoder.decode(value, {stream: true}); // Decode and accumulate response
                        const lines = buffer.split('\n'); // Split buffer into lines
                        
                        // Process all complete lines
                        buffer = lines.pop() || '';  // Keep any incomplete line in the buffer
                        
                        for (const line of lines) {
                            if (line.startsWith('data: ')) { // Check for data lines
                                try {
                                    const data = JSON.parse(line.slice(6)); // Parse JSON data
                                    
                                    if (data.chunk) {
                                        // Accumulate the complete response
                                        fullResponse += data.chunk; // Append chunk to full response
                                        // Render the accumulated response
                                        responseText.innerHTML = marked.parse(fullResponse); // Update response text
                                    } else if (data.cost) {
                                        costDisplay.textContent = `Estimated Cost: $${data.cost.toFixed(6)} (${data.provider})`; // Display cost
                                        
                                        // Add copy button after streaming is complete
                                        const copyButton = document.createElement('button'); // Create copy button
                                        copyButton.textContent = 'Copy Response'; // Set button text
                                        copyButton.className = 'copy-btn btn btn-sm'; // Set class for styling
                                        copyButton.onclick = () => copyToClipboard(fullResponse); // Set click handler
                                        assistantMessage.appendChild(copyButton); // Append copy button to assistant message
                                    } else if (data.error) {
                                        const errorDiv = document.createElement('div'); // Create error message element
                                        errorDiv.className = 'error'; // Set class for styling
                                        errorDiv.textContent = data.error; // Set error message
                                        assistantMessage.appendChild(errorDiv); // Append error message to assistant
                                    }
                                    
                                    // Scroll to bottom as content is added
                                    chatBox.scrollTop = chatBox.scrollHeight; // Scroll to the bottom
                                } catch (e) {
                                    console.error('Error parsing SSE data:', e); // Log parsing error
                                }
                            }
                        }
                    }

                } catch (error) {
                    console.error('Error:', error); // Log error
                    const errorMessage = document.createElement('div'); // Create error message element
                    errorMessage.className = 'message assistant error'; // Set class for styling
                    errorMessage.textContent = 'Error: ' + error.message; // Set error message
                    chatBox.appendChild(errorMessage); // Append error message to chat box
                    chatBox.scrollTop = chatBox.scrollHeight; // Scroll to the bottom
                }
            });

            // Function to copy text to clipboard
            function copyToClipboard(text) {
                navigator.clipboard.writeText(text).then(() => {
                    alert('Response copied to clipboard!'); // Alert user on success
                }, (err) => {
                    console.error('Could not copy text: ', err); // Log error
                });
            }

            // Function to auto-save settings
            function autoSaveSettings() {
                const formData = new FormData(document.getElementById('settings-form')); // Get form data
                fetch('/submit', {
                    method: 'POST',
                    body: formData // Send form data to server
                })
                .then(response => response.json()) // Parse JSON response
                .then(data => {
                    console.log('Settings saved:', data); // Log success
                })
                .catch(error => {
                    console.error('Error saving settings:', error); // Log error
                });
            }

            const refreshBtn = document.getElementById('refresh-context');
            const refreshIcon = refreshBtn.querySelector('.refresh-icon');
            const refreshStatus = document.getElementById('refresh-status');
            const refreshError = document.getElementById('refresh-error');
            
            async function refreshContext() {
                refreshBtn.disabled = true;
                refreshIcon.classList.add('spinning');
                refreshError.style.display = 'none';
                refreshStatus.style.display = 'none';
                
                try {
                    const response = await fetch('/refresh_context', {
                        method: 'POST'
                    });
                    
                    if (!response.ok) {
                        throw new Error(`Failed to refresh context: ${response.statusText}`);
                    }
                    
                    const data = await response.json();
                    
                    // Show success status
                    refreshStatus.textContent = `Last refreshed: ${new Date().toLocaleTimeString()}`;
                    refreshStatus.style.display = 'block';
                    
                    // Reload file tree if it exists
                    if (fileTreeViewer) {
                        await fileTreeViewer.loadProcessedFiles();
                    }
                    
                } catch (error) {
                    console.error('Error refreshing context:', error);
                    refreshError.textContent = error.message;
                    refreshError.style.display = 'block';
                } finally {
                    refreshBtn.disabled = false;
                    refreshIcon.classList.remove('spinning');
                }
            }
            
            // Add click handler for refresh button
            refreshBtn.addEventListener('click', refreshContext);

            const resetChatBtn = document.getElementById('reset-chat');
            resetChatBtn.addEventListener('click', () => {
                if (confirm('Are you sure you want to reset the conversation? This cannot be undone.')) {
                    chatBox.innerHTML = ''; // Clear chat history
                    // You may also want to send a request to the server to clear the conversation state
                    fetch('/reset_conversation', { method: 'POST' })
                        .catch(error => console.error('Error resetting conversation:', error));
                }
            });

            // Initialize popover for advanced settings
            const advancedSettingsBtn = document.getElementById('advanced-settings-btn');
            const advancedSettingsContent = `
                <div class="advanced-settings-popover">
                    <div class="form-group mb-3">
                        <label for="temperature">Temperature:</label>
                        <input type="range" class="form-range" id="temperature" min="0" max="1" step="0.1" value="0.7">
                        <span id="temperature-value">0.7</span>
                    </div>
                    <div class="form-group mb-3">
                        <label for="max-tokens">Max Tokens:</label>
                        <input type="number" class="form-control" id="max-tokens" value="2000" min="100" max="4000">
                    </div>
                    <div class="form-group">
                        <label for="system-prompt">System Prompt:</label>
                        <textarea class="form-control" id="system-prompt" rows="3"></textarea>
                    </div>
                </div>
            `;

            // Initialize the popover
            const popover = new bootstrap.Popover(advancedSettingsBtn, {
                content: advancedSettingsContent,
                trigger: 'click',
                sanitize: false
            });

            // Handle popover shown event to initialize range input display
            advancedSettingsBtn.addEventListener('shown.bs.popover', () => {
                const temperatureInput = document.getElementById('temperature');
                const temperatureValue = document.getElementById('temperature-value');
                
                if (temperatureInput && temperatureValue) {
                    temperatureInput.addEventListener('input', () => {
                        temperatureValue.textContent = temperatureInput.value;
                    });
                }
            });

            // Close popover when clicking outside
            document.addEventListener('click', (event) => {
                if (!event.target.closest('#advanced-settings-btn') && 
                    !event.target.closest('.popover')) {
                    popover.hide();
                }
            });

            // Set selection change callback for file tree viewer
            fileTreeViewer.setSelectionChangeCallback((selectedFiles) => {
                const selectedStats = fileTreeViewer.getSelectedStats(); // Get selected stats
                updateContextCostDisplay(selectedStats); // Update cost display
            });
        });

        // Function to save conversation history
        function saveConversation() {
            fetch('/save_conversation', {
                method: 'POST' // Send request to save conversation
            })
            .then(response => {
                if (response.ok) {
                    return response.blob(); // Return response as blob
                } else {
                    throw new Error('Failed to save conversation history.'); // Handle error
                }
            })
            .then(blob => {
                const url = window.URL.createObjectURL(blob); // Create URL for blob
                const a = document.createElement('a'); // Create anchor element
                a.style.display = 'none'; // Hide anchor
                a.href = url; // Set href to blob URL
                a.download = 'conversation_history.txt'; // Set download filename
                document.body.appendChild(a); // Append anchor to body
                a.click(); // Trigger download
                window.URL.revokeObjectURL(url); // Clean up URL
            })
            .catch(error => {
                console.error('Error:', error); // Log error
                alert(error.message); // Alert user
            });
        }

        // Function to update context cost display
        function updateContextCostDisplay(selectedStats) {
            const costDisplay = document.getElementById('context-cost-display'); // Get cost display element
            const costsSpan = document.getElementById('context-costs'); // Get costs span
            
            if (selectedStats.totalTokens > 0) {
                costsSpan.innerHTML = `
                    <div class="mt-1">
                        <div>Tokens: ${selectedStats.totalTokens.toLocaleString()}</div>
                        <div>Claude 3.5: $${selectedStats.costs.claude.toFixed(4)}</div>
                        <div>GPT-4: $${selectedStats.costs.gpt4.toFixed(4)}</div>
                        <div>GPT-4 Mini: $${selectedStats.costs.gpt4mini.toFixed(4)}</div>
                    </div>
                `;
                costDisplay.style.display = 'block'; // Show cost display
            } else {
                costDisplay.style.display = 'none'; // Hide cost display
            }
        }
    </script>
</body>
</html>
==================================================

