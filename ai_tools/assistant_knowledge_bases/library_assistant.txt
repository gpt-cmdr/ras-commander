File: c:\GH\ras-commander\library_assistant\.cursorrules
==================================================
Library Assistant: AI-Powered Tool for Managing and Querying Library Content

Project Structure:
library_assistant/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ anthropic.py
â”‚   â””â”€â”€ openai.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ database/
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_handling.py
â”‚   â”œâ”€â”€ cost_estimation.py
â”‚   â”œâ”€â”€ conversation.py
â”‚   â””â”€â”€ context_processing.py
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ routes.py
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ static/
â””â”€â”€ assistant.py

Program Features:
1. Integration with Anthropic and OpenAI APIs for AI-powered responses
2. Context-aware processing using full context or RAG (Retrieval-Augmented Generation) modes
3. Dynamic file handling and content processing
4. Cost estimation for API calls
5. Conversation management and history saving
6. Web-based user interface using FastAPI
7. Customizable settings for model selection, context mode, and file handling
8. Error handling and user guidance

General Coding Rules and Guidelines:
1. Follow PEP 8 style guidelines for Python code
2. Use type hints and docstrings for improved code readability
3. Implement proper error handling and logging
4. Maintain separation of concerns between modules
5. Use asynchronous programming where appropriate for improved performance
6. Implement unit tests for critical functions
7. Keep sensitive information (e.g., API keys) secure and out of version control
8. Use meaningful variable and function names
9. Optimize for readability and maintainability
10. Regularly update dependencies and address security vulnerabilities

The Library Assistant operates by:
1. Processing user queries through a web interface
2. Preparing context based on the selected mode (full context or RAG)
3. Sending prepared prompts to the chosen AI model (Anthropic or OpenAI)
4. Streaming and processing AI responses
5. Estimating and displaying costs for API calls
6. Managing conversation history and allowing for conversation saving
7. Providing user guidance and error handling as needed

The AI assistant interacting with the Library Assistant is a helpful expert with experience in:
- Python programming
- FastAPI web framework
- SQLAlchemy ORM
- Anthropic and OpenAI APIs
- Natural Language Processing (NLP) techniques
- Retrieval-Augmented Generation (RAG)
- Asynchronous programming
- RESTful API design
- Database management
- Cost optimization for API usage
- Web development (HTML, CSS, JavaScript)
- Git version control

The assistant should provide accurate, context-aware, and helpful responses while adhering to the Library Assistant's capabilities and limitations. It should offer guidance on effective use of the tool, including query formulation and settings management, while maintaining a professional and knowledgeable persona throughout all interactions.

==================================================

Folder: c:\GH\ras-commander\library_assistant\api
==================================================

File: c:\GH\ras-commander\library_assistant\assistant.py
==================================================
"""
Main entry point for the Library Assistant application.

This module initializes the FastAPI application, sets up the necessary routes,
and provides functions to open the browser and run the application.

Functions:
- open_browser(): Opens the default web browser to the application URL.
- run_app(): Starts the FastAPI application using uvicorn.
"""

import os
import uvicorn
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from web.routes import router
import webbrowser
from utils.context_processing import initialize_rag_context

# Initialize FastAPI application
print("Initializing FastAPI")
app = FastAPI(
    title="Library Assistant",
    description="An AI-powered assistant for managing and querying library content.",
    version="1.0.0"
)

# Create necessary directories if they don't exist
os.makedirs("web/templates", exist_ok=True)
os.makedirs("web/static", exist_ok=True)

# Mount the static files directory
app.mount("/static", StaticFiles(directory="web/static"), name="static")

# Include the router from web/routes.py
app.include_router(router)

def open_browser():
    """
    Opens the default web browser to the application URL.
    
    This function is called when the application starts to provide
    easy access to the web interface.
    """
    webbrowser.open("http://127.0.0.1:8000")

def run_app():
    """
    Starts the FastAPI application using uvicorn.
    
    This function configures and runs the uvicorn server with the
    FastAPI application.
    """
    uvicorn.run(app, host="127.0.0.1", port=8000)

if __name__ == "__main__":
    # Initialize RAG context at startup
    try:
        initialize_rag_context()
        print("RAG context initialized successfully.")
    except Exception as e:
        print(f"Failed to initialize RAG context: {e}")

    # Open the browser
    open_browser()

    # Run the app
    run_app()

==================================================

Folder: c:\GH\ras-commander\library_assistant\config
==================================================

File: c:\GH\ras-commander\library_assistant\conversation_history_20241106_011820.txt
==================================================

==================================================

File: c:\GH\ras-commander\library_assistant\conversation_history_20241106_123226.txt
==================================================

==================================================

File: c:\GH\ras-commander\library_assistant\conversation_history_20241106_153542.txt
==================================================
User: summarize detail face data extraction

Assistant: Certainly! I'll provide a summary of the 2D detail face data extraction process as demonstrated in the "22_2d_detail_face_data_extraction.ipynb" notebook. This process is used to analyze HEC-RAS 2D model results, particularly focusing on extracting and processing data related to mesh faces.

Summary of 2D Detail Face Data Extraction:

1. Setup and Data Loading:
   - The notebook begins by importing necessary libraries and setting up the RAS Commander environment.
   - It loads a HEC-RAS project (Chippewa_2D) and executes a simulation if results don't already exist.

2. HDF File Handling:
   - The script accesses HDF files containing geometry and results data.
   - It uses various HDF-related classes from RAS Commander (e.g., HdfMesh, HdfResultsMesh) to extract data.

3. Mesh and Face Data Extraction:
   - Extracts mesh cell faces and their properties from the geometry HDF file.
   - Retrieves timeseries output for mesh cells and faces from the results HDF file.

4. Profile Line Analysis:
   - Loads predefined profile lines from a GeoJSON file.
   - Identifies mesh faces that intersect with these profile lines.
   - Orders the faces along each profile line for consistent analysis.

5. Data Processing:
   - Converts face velocities and flows to positive values while maintaining their relationships.
   - Processes face data for each profile line, including velocity and flow information.

6. Discharge-Weighted Velocity Calculation:
   - Implements a function to calculate discharge-weighted average velocity for each profile line.
   - This provides a more representative velocity measure compared to simple averages.

7. Visualization:
   - Creates plots comparing discharge-weighted velocity and simple average velocity for each profile line.
   - Visualizes individual face velocities along with the calculated averages.

8. Results Analysis:
   - Generates statistics for each profile line, including mean, max, and min velocities for both weighted and simple averages.

9. Data Export:
   - Saves processed data and results to CSV files for further analysis or reporting.

This notebook demonstrates advanced techniques for extracting and analyzing detailed face data from HEC-RAS 2D simulations using the RAS Commander library. It's particularly useful for understanding flow characteristics along specific cross-sections or profile lines within a 2D model domain.


==================================================

Folder: c:\GH\ras-commander\library_assistant\database
==================================================

File: c:\GH\ras-commander\library_assistant\Library_Assistant_REAME.md
==================================================
# Library Assistant

Library Assistant is an AI-powered tool for managing and querying library content, leveraging both Anthropic's Claude and OpenAI's GPT models. It provides a web interface for interacting with AI models while maintaining context awareness of your codebase or documentation.

## Features

- **Dual AI Provider Support**: Integration with both Anthropic (Claude) and OpenAI (GPT) models
- **Context-Aware Processing**: Two modes of operation:
  - Full Context: Uses complete codebase/documentation context
  - RAG (Retrieval-Augmented Generation): Dynamically retrieves relevant context
- **Web Interface**: Clean, intuitive web UI built with FastAPI and Bootstrap
- **Real-Time Cost Estimation**: Estimates API costs for each interaction
- **Conversation Management**: Save and export chat histories
- **Customizable Settings**: Configure API keys, models, and context handling
- **File Processing**:
  - Intelligent handling of Python and Markdown files
  - Configurable file/folder exclusions
  - Code stripping options for reduced token usage

## Installation

1. Clone the repository:
```bash
# Start of Selection
git clone https://github.com/billk-FM/ras-commander.git
# End of Selection
cd library-assistant
```

2. Install dependencies:
```bash
pip install fastapi uvicorn sqlalchemy jinja2 pandas anthropic openai tiktoken astor markdown python-multipart requests python-dotenv
```

3. Set up your environment:
   - Obtain API keys from [Anthropic](https://www.anthropic.com/) and/or [OpenAI](https://openai.com/)
   - Configure your settings through the web interface

## Usage

1. Start the application:
```bash
python assistant.py
```

2. Open your web browser to `http://127.0.0.1:8000`

3. Configure your settings:
   - Select your preferred AI model
   - Enter your API key(s)
   - Choose context handling mode
   - Adjust RAG parameters if using RAG mode

4. Start chatting with the AI assistant about your codebase or documentation

## Configuration

### Available Models

- **Anthropic**:
  - Claude 3.5 Sonnet

- **OpenAI**:
  - GPT-4o
  - GPT-4o-mini
  - o1-mini

### Context Modes

1. **Full Context**:
   - Provides complete codebase context to the AI
   - Best for smaller codebases
   - Higher token usage

2. **RAG Mode**:
   - Dynamically retrieves relevant context
   - More efficient for large codebases
   - Configurable chunk sizes

### File Processing Options

Configure exclusions in settings:
```python
omit_folders = [
    "__pycache__",
    ".git",
    "venv",
    # Add custom folders
]

omit_extensions = [
    ".jpg", ".png", ".pdf",
    # Add custom extensions
]

omit_files = [
    "specific_file.txt",
    # Add specific files
]
```

## Project Structure

```
library_assistant/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ anthropic.py    # Anthropic API integration
â”‚   â””â”€â”€ openai.py       # OpenAI API integration
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py       # Configuration management
â”œâ”€â”€ database/
â”‚   â””â”€â”€ models.py       # Database models
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ file_handling.py       # File processing utilities
â”‚   â”œâ”€â”€ cost_estimation.py     # API cost calculations
â”‚   â”œâ”€â”€ conversation.py        # Chat history management
â”‚   â””â”€â”€ context_processing.py  # Context handling
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ routes.py      # FastAPI routes
â”‚   â”œâ”€â”€ templates/     # HTML templates
â”‚   â””â”€â”€ static/        # Static assets
â””â”€â”€ assistant.py       # Main application entry point
```

## Error Handling

The application includes comprehensive error handling:
- API errors
- File processing errors
- Invalid settings
- Connection issues

Errors are logged and displayed in the web interface with appropriate messages.

## Development

### Adding New Features

1. Follow the existing project structure
2. Implement proper error handling
3. Update the web interface as needed
4. Document new features

### Code Style

- Follow PEP 8 guidelines
- Include docstrings for all functions
- Use type hints where appropriate
- Implement proper error handling
- Keep functions focused and modular

## Performance Considerations

- RAG mode is recommended for large codebases
- Adjust chunk sizes based on your needs
- Consider token limits of your chosen model
- Monitor API costs through the interface

## Limitations

- Maximum context window varies by model
- API rate limits apply
- Token costs vary by provider and model
- Some file types are excluded by default

## Support

For issues, questions, or contributions:
1. Check the existing documentation
2. Review the codebase for similar functionality
3. Open an issue for bugs or feature requests
4. Submit pull requests with improvements

## License

This project is licensed under the MIT License - see the LICENSE file for details.
==================================================

Folder: c:\GH\ras-commander\library_assistant\path
==================================================

File: c:\GH\ras-commander\library_assistant\requirements.txt
==================================================
# Web framework and ASGI server
fastapi==0.68.0
uvicorn==0.15.0

# Database ORM
sqlalchemy==1.4.23

# Template engine
jinja2==3.0.1

# Data manipulation
pandas==1.3.3

# AI/ML libraries
anthropic==0.2.7
openai==0.27.0
tiktoken==0.3.0

# Code parsing and manipulation
astor==0.8.1

# Markdown processing
markdown==3.3.4

# Form handling for FastAPI
python-multipart==0.0.5

# HTTP client for Python
requests==2.26.0

# Environment variable management
python-dotenv==0.19.0

# YAML parsing (if needed for configuration)
pyyaml==5.4.1

# Date and time manipulation
python-dateutil==2.8.2

# Progress bars (optional, for long-running tasks)
tqdm==4.62.3

# Testing framework
pytest==6.2.5

# Linting and code formatting
flake8==3.9.2
black==21.9b0

# Type checking
mypy==0.910

# Documentation generation
sphinx==4.2.0

==================================================

File: c:\GH\ras-commander\library_assistant\settings.db
==================================================
SQLite format 3   @                                                                     .v  n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N)eindexix_settings_idsettingsCREATE INDEX ix_settings_id ON settings (id)VtablesettingssettingsCREATE TABLE settings (
	id VARCHAR NOT NULL, 
	anthropic_api_key TEXT, 
	openai_api_key TEXT, 
	selected_model VARCHAR, 
	context_mode VARCHAR, 
	omit_folders TEXT, 
	omit_extensions TEXT, 
	omit_files TEXT, 
	chunk_level VARCHAR, 
	initial_chunk_size INTEGER, 
	followup_chunk_size INTEGER, 
	PRIMARY KEY (id)
)/C indexsqlite_autoindex_settings_1settings          w w                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             eUA%+Isingleton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jiowS6M1bPJl-sSEAp_1yCIrTFwcXO5ucclkKkiCuxmp73biN3T5k3hVJYuYwtgcAclaude-3-5-sonnet-20240620full_context["Bald Eagle Creek", "__pycache__", ".git", ".github", "tests", "build", "dist", "ras_commander.egg-info", "venv", "example_projects", "llm_summary", "misc", "future", "ai_tools", "docsExample_Projects_6_6"][".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff", ".webp", ".svg", ".ico", ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".zip", ".rar", ".7z", ".tar", ".gz", ".exe", ".dll", ".so", ".dylib", ".pyc", ".pyo", ".pyd", ".class", ".log", ".tmp", ".bak", ".swp", ".bat", ".sh"]["FunctionList.md", "DS_Store", "Thumbs.db", "llmsummarize", "example_projects.zip", "11_accessing_example_projects.ipynb", "Example_Projects_6_5.zip", "github_code_assistant.ipynb", "example_projects.ipynb", "11_Using_RasExamples.ipynb", "example_projects.csv", "rascommander_code_assistant.ipynb", "RasExamples.py"]function} >
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	singleton
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	singleton
==================================================

Folder: c:\GH\ras-commander\library_assistant\utils
==================================================

Folder: c:\GH\ras-commander\library_assistant\web
==================================================

File: c:\GH\ras-commander\library_assistant\api\anthropic.py
==================================================
"""
Anthropic API integration for the Library Assistant.
"""

from anthropic import AsyncAnthropic, Anthropic, APIError, AuthenticationError
from typing import AsyncGenerator, List, Optional, Union

async def anthropic_stream_response(
    client: Union[AsyncAnthropic, Anthropic], 
    prompt: str, 
    max_tokens: int = 8000,
    model: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """
    Streams a response from the Anthropic API using the Claude model.

    Args:
        client: An initialized Anthropic client (sync or async)
        prompt: The prompt to send to the API
        max_tokens: The maximum number of tokens to generate (default: 8000)
        model: The model to use (default: claude-3-5-sonnet-20240620)

    Yields:
        str: Chunks of the response text from the API

    Raises:
        APIError: If there's an error with the API call
        AuthenticationError: If authentication fails
    """
    try:
        model = model or "claude-3-5-sonnet-20240620"
        
        # Convert to async client if needed
        async_client = client if isinstance(client, AsyncAnthropic) else AsyncAnthropic(api_key=client.api_key)
        
        stream = await async_client.messages.create(
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}],
            model=model,
            stream=True
        )
        
        # Process the stream events
        current_line = []
        
        async for message in stream:
            try:
                if message.type == "content_block_delta" and message.delta and message.delta.text:
                    # Clean and normalize the chunk text
                    chunk = message.delta.text.replace('\r', '')
                    
                    # Accumulate text until we get a natural break
                    current_line.append(chunk)
                    
                    # Check for natural breaks (end of sentence or paragraph)
                    if any(chunk.endswith(end) for end in ['.', '!', '?', '\n']):
                        complete_line = ''.join(current_line)
                        if complete_line.strip():
                            yield complete_line
                        current_line = []
                        
            except Exception as e:
                print(f"Error processing message chunk: {str(e)}")
                continue
        
        # Yield any remaining text
        if current_line:
            remaining = ''.join(current_line)
            if remaining.strip():
                yield remaining
                
    except Exception as e:
        raise APIError(f"Unexpected error in Anthropic API call: {str(e)}")

def get_anthropic_client(api_key: str, async_client: bool = False) -> Union[Anthropic, AsyncAnthropic]:
    """
    Creates and returns an Anthropic client.

    Args:
        api_key: The Anthropic API key
        async_client: Whether to return an async client (default: False)

    Returns:
        An initialized Anthropic client (sync or async)

    Raises:
        ValueError: If the API key is not provided or invalid
    """
    if not api_key or not isinstance(api_key, str):
        raise ValueError("Valid Anthropic API key must be provided")
    return AsyncAnthropic(api_key=api_key) if async_client else Anthropic(api_key=api_key)

async def validate_anthropic_api_key(api_key: str) -> bool:
    """
    Validates the Anthropic API key by making a test API call.

    Args:
        api_key: The Anthropic API key to validate

    Returns:
        True if the API key is valid, False otherwise
    """
    try:
        client = get_anthropic_client(api_key, async_client=True)
        await client.messages.create(
            model="claude-3-5-sonnet-20240620",
            max_tokens=1,
            messages=[{"role": "user", "content": "Test"}],
            stream=False
        )
        return True
    except (anthropic.APIError, anthropic.AuthenticationError, ValueError):
        return False

def get_anthropic_models() -> List[str]:
    """
    Returns a list of available Anthropic models.

    Returns:
        List of strings representing available Anthropic model names
    """
    return ["claude-3-5-sonnet-20240620"]

async def stream_response(
    client: Union[AsyncAnthropic, Anthropic], 
    prompt: str, 
    max_tokens: int = 8000,
    model: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """
    Streams a response from the Anthropic API.

    This function is a wrapper around anthropic_stream_response to provide
    a consistent interface across different API providers.

    Args:
        client: An initialized Anthropic client (sync or async)
        prompt: The prompt to send to the API
        max_tokens: The maximum number of tokens to generate (default: 8000)
        model: The model to use (optional)

    Returns:
        An async generator yielding response chunks
    """
    async for chunk in anthropic_stream_response(client, prompt, max_tokens, model):
        yield chunk

==================================================

File: c:\GH\ras-commander\library_assistant\api\openai.py
==================================================
"""
OpenAI API integration for the Library Assistant.
"""

from openai import OpenAI, OpenAIError
from typing import AsyncGenerator, List, Optional, Dict, Any

async def openai_stream_response(
    client: OpenAI,
    model: str,
    messages: List[Dict[str, str]],
    max_tokens: int = 16000
) -> AsyncGenerator[str, None]:
    """
    Streams a response from the OpenAI API using the specified model.

    Args:
        client: An initialized OpenAI client
        model: The name of the OpenAI model to use
        messages: A list of message dictionaries to send to the API
        max_tokens: The maximum number of tokens to generate (default: 16000)

    Yields:
        str: Chunks of the response text from the API

    Raises:
        OpenAIError: If there's an error with the API call
    """
    try:
        stream = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                # Clean and normalize the chunk text
                text = chunk.choices[0].delta.content.replace('\r', '')
                if text.strip():  # Only yield non-empty chunks
                    yield text

    except OpenAIError as e:
        raise OpenAIError(f"OpenAI API error: {str(e)}")
    except Exception as e:
        raise OpenAIError(f"Unexpected error in OpenAI API call: {str(e)}")

def get_openai_client(api_key: str) -> OpenAI:
    """
    Creates and returns an OpenAI client.

    Args:
        api_key: The OpenAI API key

    Returns:
        OpenAI: An initialized OpenAI client

    Raises:
        ValueError: If the API key is not provided
    """
    if not api_key:
        raise ValueError("OpenAI API key not provided")
    return OpenAI(api_key=api_key)

async def validate_openai_api_key(api_key: str) -> bool:
    """
    Validates the OpenAI API key by making a test API call.

    Args:
        api_key: The OpenAI API key to validate

    Returns:
        bool: True if the API key is valid, False otherwise
    """
    try:
        client = get_openai_client(api_key)
        # Make a minimal API call to test the key
        client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=1,
            stream=False
        )
        return True
    except (OpenAIError, ValueError):
        return False

def get_openai_models() -> List[str]:
    """
    Returns a list of available OpenAI models.

    Returns:
        List[str]: A list of strings representing available OpenAI model names
    """
    return ["gpt-4o-2024-08-06", "gpt-4o-mini", "o1-mini"]

==================================================

Folder: c:\GH\ras-commander\library_assistant\api\__pycache__
==================================================

File: c:\GH\ras-commander\library_assistant\config\config.py
==================================================
"""
Configuration module for the Library Assistant.

This module provides functions for loading and updating settings,
as well as defining default settings for the application.

Functions:
- load_settings(): Loads the current settings from the database or initializes with defaults.
- update_settings(data): Updates the settings in the database with new values.

Constants:
- DEFAULT_SETTINGS: A dictionary containing the default settings for the application.
"""

import json
from sqlalchemy.orm import sessionmaker
from database.models import Settings, engine

# Create a SessionLocal class for database sessions
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Default settings for the application
DEFAULT_SETTINGS = {
    "anthropic_api_key": "",
    "openai_api_key": "",
    "selected_model": "",
    "context_mode": "",
    "omit_folders": [
        "Bald Eagle Creek", 
        "__pycache__", 
        ".git", 
        ".github", 
        "tests", 
        "build", 
        "dist", 
        "ras_commander.egg-info", 
        "venv", 
        "example_projects", 
        "llm_summary", 
        "misc", 
        "future", 
        "ai_tools",
        "docs"
        "Example_Projects_6_6"
    ],
    "omit_extensions": [
        '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.svg', '.ico',
        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
        '.zip', '.rar', '.7z', '.tar', '.gz',
        '.exe', '.dll', '.so', '.dylib',
        '.pyc', '.pyo', '.pyd',
        '.class',
        '.log', '.tmp', '.bak', '.swp',
        '.bat', '.sh',
    ],
    "omit_files": [
        'FunctionList.md',
        'DS_Store',
        'Thumbs.db',
        'llmsummarize',
        'example_projects.zip',
        '11_accessing_example_projects.ipynb',
        'Example_Projects_6_5.zip',
        'github_code_assistant.ipynb',
        'example_projects.ipynb',
        '11_Using_RasExamples.ipynb',
        'example_projects.csv',
        'rascommander_code_assistant.ipynb',
        'RasExamples.py'
    ],
    "chunk_level": "function",
    "initial_chunk_size": 32000,
    "followup_chunk_size": 16000
}

def load_settings():
    """
    Loads the current settings from the database or initializes with defaults.

    This function queries the database for existing settings. If no settings are found,
    it initializes the database with the default settings. The settings are stored
    as a singleton record in the database.

    Returns:
        Settings: An instance of the Settings model containing the current settings.
    """
    db = SessionLocal()
    settings = db.query(Settings).filter(Settings.id == "singleton").first()
    if not settings:
        # Initialize with default settings
        settings = Settings(
            id="singleton",
            **{key: json.dumps(value) if isinstance(value, list) else value 
               for key, value in DEFAULT_SETTINGS.items()}
        )
        db.add(settings)
        db.commit()
        db.refresh(settings)
    db.close()
    return settings

def update_settings(data):
    """
    Updates the settings in the database with new values.

    This function takes a dictionary of settings to update, queries the database
    for the existing settings, and updates the values accordingly. For list-type
    settings (omit_folders, omit_extensions, omit_files), the values are JSON-encoded
    before storage.

    Args:
        data (dict): A dictionary containing the settings to update.
                     Keys should match the attribute names in the Settings model.

    Note:
        This function does not return any value. It directly updates the database.
    """
    db = SessionLocal()
    settings = db.query(Settings).filter(Settings.id == "singleton").first()
    for key, value in data.items():
        if key in ["omit_folders", "omit_extensions", "omit_files"]:
            setattr(settings, key, json.dumps(value))
        else:
            setattr(settings, key, value)
    db.commit()
    db.close()

==================================================

Folder: c:\GH\ras-commander\library_assistant\config\__pycache__
==================================================

File: c:\GH\ras-commander\library_assistant\database\models.py
==================================================
"""
Database models for the Library Assistant.

This module defines the SQLAlchemy ORM models used for storing application settings.

Classes:
- Base: The declarative base class for SQLAlchemy models.
- Settings: The model representing application settings.

Constants:
- DATABASE_URL: The URL for the SQLite database.
- engine: The SQLAlchemy engine instance.
"""

from sqlalchemy import create_engine, Column, String, Text, Integer
from sqlalchemy.orm import declarative_base

# Define the database URL
DATABASE_URL = "sqlite:///./settings.db"

# Create the SQLAlchemy engine
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})

# Create the declarative base class
Base = declarative_base()

class Settings(Base):
    """
    SQLAlchemy ORM model for storing application settings.

    This class represents a single row in the settings table, which stores
    all configuration options for the Library Assistant application.

    Attributes:
        id (str): Primary key, set to "singleton" as there's only one settings record.
        anthropic_api_key (str): API key for Anthropic services.
        openai_api_key (str): API key for OpenAI services.
        selected_model (str): The currently selected AI model.
        context_mode (str): The context handling mode (e.g., 'full_context' or 'rag').
        omit_folders (str): JSON string of folders to omit from processing.
        omit_extensions (str): JSON string of file extensions to omit from processing.
        omit_files (str): JSON string of specific files to omit from processing.
        chunk_level (str): The level at which to chunk text (e.g., 'function').
        initial_chunk_size (int): The initial size of text chunks for processing.
        followup_chunk_size (int): The size of follow-up chunks for processing.
    """

    __tablename__ = "settings"

    id = Column(String, primary_key=True, index=True, default="singleton")
    anthropic_api_key = Column(Text, nullable=True)
    openai_api_key = Column(Text, nullable=True)
    selected_model = Column(String, nullable=True)
    context_mode = Column(String, nullable=True)
    omit_folders = Column(Text, nullable=True)
    omit_extensions = Column(Text, nullable=True)
    omit_files = Column(Text, nullable=True)
    chunk_level = Column(String, nullable=True)
    initial_chunk_size = Column(Integer, default=32000)
    followup_chunk_size = Column(Integer, default=16000)

# Create the database tables
Base.metadata.create_all(bind=engine)

==================================================

Folder: c:\GH\ras-commander\library_assistant\database\__pycache__
==================================================

Folder: c:\GH\ras-commander\library_assistant\path\to
==================================================

File: c:\GH\ras-commander\library_assistant\utils\context_processing.py
==================================================
"""
Utility functions for context processing in the Library Assistant.
"""

import tiktoken
from pathlib import Path
from config.config import load_settings
from utils.file_handling import combine_files, read_system_message, set_context_folder
import json

# Initialize global variables for context
preprocessed_context = ""
preprocessed_rag_context = ""
conversation_context = {}  # Store context for each conversation

def initialize_rag_context():
    """
    Initializes the RAG context for the application.
    """
    global preprocessed_context, preprocessed_rag_context
    
    try:
        # Load settings
        settings = load_settings()
        context_mode = settings.context_mode
        omit_folders = json.loads(settings.omit_folders)
        omit_extensions = json.loads(settings.omit_extensions)
        omit_files = json.loads(settings.omit_files)
        chunk_level = settings.chunk_level
        initial_chunk_size = settings.initial_chunk_size
        followup_chunk_size = settings.followup_chunk_size

        # Set up context
        context_folder = set_context_folder()
        
        # Combine files
        combined_text, total_token_count, _ = combine_files(
            summarize_subfolder=context_folder,
            omit_folders=omit_folders,
            omit_extensions=omit_extensions,
            omit_files=omit_files,
            strip_code=True,
            chunk_level=chunk_level
        )

        # Process context based on mode
        if context_mode == 'full_context':
            preprocessed_context = combined_text
        else:  # RAG mode
            preprocessed_rag_context = prepare_context(
                text=combined_text,
                mode='rag',
                initial_chunk_size=initial_chunk_size,
                followup_chunk_size=followup_chunk_size
            )

    except Exception as e:
        print(f"Error initializing context: {str(e)}")
        raise

def prepare_full_prompt(user_query: str, selected_files=None, conversation_id=None) -> str:
    """
    Prepares the full prompt for the AI model, including context and conversation history.
    
    Args:
        user_query (str): The user's query
        selected_files (list): List of files to include in context
        conversation_id (str): Unique identifier for the conversation
    
    Returns:
        str: The complete prompt including system message, context, and conversation history
    """
    settings = load_settings()
    context_mode = settings.context_mode
    system_message = read_system_message()
    
    try:
        # Get or initialize conversation context
        if conversation_id not in conversation_context:
            conversation_context[conversation_id] = {
                'selected_files': selected_files,
                'history': []
            }
        
        conv_data = conversation_context[conversation_id]
        
        # Update selected files if they've changed
        if selected_files != conv_data['selected_files']:
            conv_data['selected_files'] = selected_files
        
        if context_mode == 'full_context':
            if selected_files:
                # Get content of selected files
                context_folder = set_context_folder()
                combined_text, _, _ = combine_files(
                    summarize_subfolder=context_folder,
                    omit_folders=[],
                    omit_extensions=[],
                    omit_files=[],
                    strip_code=True,
                    chunk_level='file',
                    selected_files=selected_files
                )
                context = combined_text
            else:
                context = preprocessed_context
                
            # Format prompt with conversation history
            prompt = (f"{system_message}\n\n"
                     f"Files from RAS-Commander Repository for Context:\n{context}\n\n"
                     "Previous Conversation:\n")
            
            # Add conversation history
            for msg in conv_data['history']:
                prompt += f"{msg['role'].capitalize()}: {msg['content']}\n\n"
            
            # Add current query
            prompt += f"User Query: {user_query}"
            
            return prompt
            
        else:  # RAG mode
            prompt = (f"{system_message}\n\n"
                     "<context>\nFiles from RAS-Commander Repository for Context:\n"
                     f"{preprocessed_rag_context}\n</context>\n\n"
                     "Previous Conversation:\n")
            
            # Add conversation history
            for msg in conv_data['history']:
                prompt += f"{msg['role'].capitalize()}: {msg['content']}\n\n"
            
            prompt += f"Using the context above, please respond to this query:\n\n"
            prompt += f"User Query: {user_query}"
            
            return prompt
            
    except Exception as e:
        print(f"Error preparing prompt: {str(e)}")
        return f"{system_message}\n\nUser Query: {user_query}"

def update_conversation_history(conversation_id: str, role: str, content: str):
    """
    Updates the conversation history for a given conversation.
    
    Args:
        conversation_id (str): Unique identifier for the conversation
        role (str): Role of the message sender ('user' or 'assistant')
        content (str): Content of the message
    """
    if conversation_id not in conversation_context:
        conversation_context[conversation_id] = {
            'selected_files': None,
            'history': []
        }
    
    conversation_context[conversation_id]['history'].append({
        'role': role,
        'content': content
    })

def clear_conversation_history(conversation_id: str):
    """
    Clears the conversation history for a given conversation.
    
    Args:
        conversation_id (str): Unique identifier for the conversation
    """
    if conversation_id in conversation_context:
        conversation_context[conversation_id]['history'] = []

def prepare_context(text="", mode='full_context', selected_files=None, initial_chunk_size=32000, followup_chunk_size=16000):
    """
    Prepares context based on the specified mode.
    
    Args:
        text (str): The input text to process
        mode (str): Context preparation mode ('full_context' or 'rag')
        selected_files (list): List of files to include in context (for full_context mode)
        initial_chunk_size (int): Size for initial RAG chunks
        followup_chunk_size (int): Size for follow-up RAG chunks
    
    Returns:
        str: The prepared context
    """
    if mode == 'full_context':
        if selected_files:
            # Filter context to only include selected files
            filtered_text = ""
            current_file = None
            for line in text.split('\n'):
                if line.startswith("----- ") and " - " in line:
                    current_file = line.split(" - ")[0].replace("----- ", "")
                    if current_file in selected_files:
                        filtered_text += line + "\n"
                elif current_file in selected_files:
                    filtered_text += line + "\n"
            return filtered_text
        return text
    elif mode == 'rag':
        chunks = chunk_text(text, initial_chunk_size)
        ranked_chunks = rank_chunks(chunks)
        prepared_context = ""
        current_size = 0
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        for chunk in ranked_chunks:
            chunk_size = len(enc.encode(chunk))
            if current_size + chunk_size <= followup_chunk_size:
                prepared_context += chunk + "\n\n"
                current_size += chunk_size
            else:
                break
        return prepared_context
    else:
        raise ValueError("Invalid mode. Choose 'full_context' or 'rag'")

def rank_chunks(chunks):
    """
    Ranks chunks of text based on potential relevance.
    Currently returns chunks in original order, but could be enhanced with
    more sophisticated ranking algorithms.

    Args:
        chunks (list): A list of text chunks to be ranked.

    Returns:
        list: The ranked chunks.
    """
    return chunks

def chunk_text(text, chunk_size):
    """
    Splits text into chunks while maintaining context boundaries.

    Args:
        text (str): The text to be chunked.
        chunk_size (int): Target size for each chunk in tokens.

    Returns:
        list: List of text chunks.
    """
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
    chunks = []
    current_chunk = ""
    current_tokens = 0

    for line in text.split('\n'):
        line_tokens = len(enc.encode(line))
        
        if current_tokens + line_tokens > chunk_size:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = line
            current_tokens = line_tokens
        else:
            current_chunk += '\n' + line if current_chunk else line
            current_tokens += line_tokens

    if current_chunk:
        chunks.append(current_chunk)

    return chunks
==================================================

File: c:\GH\ras-commander\library_assistant\utils\conversation.py
==================================================
"""
Utility functions for conversation handling in the Library Assistant.

This module provides functions for managing conversation history,
including adding messages, retrieving the full conversation,
and saving the conversation to a file.

Functions:
- add_to_history(role, content): Adds a message to the conversation history.
- get_full_conversation(): Retrieves the full conversation history as a string.
- save_conversation(): Saves the current conversation history to a file.
"""

from datetime import datetime
import os

# Initialize conversation history
conversation_history = []

def add_to_history(role, content):
    """
    Adds a message to the conversation history.

    Args:
        role (str): The role of the message sender (e.g., 'user' or 'assistant').
        content (str): The content of the message.
    """
    conversation_history.append({"role": role, "content": content})

def get_full_conversation():
    """
    Retrieves the full conversation history as a formatted string.

    Returns:
        str: A string representation of the entire conversation history.
    """
    return "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in conversation_history])

def save_conversation():
    """
    Saves the current conversation history to a file.

    This function creates a text file with a timestamp in its name,
    containing the full conversation history.

    Returns:
        str: The file path of the saved conversation history.

    Raises:
        IOError: If there's an error writing to the file.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = f"conversation_history_{timestamp}.txt"
    file_path = os.path.join(os.getcwd(), file_name)
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for message in conversation_history:
                f.write(f"{message['role'].capitalize()}: {message['content']}\n\n")
        return file_path
    except IOError as e:
        raise IOError(f"Error saving conversation history: {str(e)}")

def clear_conversation_history():
    """
    Clears the current conversation history.

    This function removes all messages from the conversation history,
    effectively resetting it to an empty state.
    """
    global conversation_history
    conversation_history = []

def get_conversation_length():
    """
    Returns the number of messages in the current conversation history.

    Returns:
        int: The number of messages in the conversation history.
    """
    return len(conversation_history)

def get_last_message():
    """
    Retrieves the last message from the conversation history.

    Returns:
        dict: A dictionary containing the role and content of the last message,
              or None if the conversation history is empty.
    """
    if conversation_history:
        return conversation_history[-1]
    return None

==================================================

File: c:\GH\ras-commander\library_assistant\utils\cost_estimation.py
==================================================
"""
Utility functions for cost estimation in the Library Assistant.

This module provides functions for creating pricing dataframes and
estimating the cost of API calls based on token usage.

Functions:
- create_pricing_df(model): Creates a pricing dataframe for a given model.
- estimate_cost(input_tokens, output_tokens, pricing_df): Estimates the cost of an API call.
"""

import pandas as pd

def create_pricing_df(model):
    """
    Creates a pricing dataframe for a given model.

    This function returns a dictionary containing a pandas DataFrame with pricing information
    and the provider (OpenAI or Anthropic) for the specified model.

    Args:
        model (str): The name of the model (e.g., 'gpt-4', 'claude-3-5-sonnet-20240620').

    Returns:
        dict: A dictionary containing:
            - 'pricing_df': A pandas DataFrame with columns 'Model', 'Input ($/1M Tokens)',
                            'Output ($/1M Tokens)', 'Context Window (Tokens)', and 'Response Max Tokens'.
            - 'provider': A string indicating the provider ('openai' or 'anthropic').

    Raises:
        ValueError: If an unsupported model is specified.
    """
    if model.startswith("claude"):
        provider = "anthropic"
        pricing_data = {
            "Model": ["Claude 3.5 Sonnet"],
            "Input ($/1M Tokens)": [3],
            "Output ($/1M Tokens)": [15],
            "Context Window (Tokens)": [200000],
            "Response Max Tokens": [8192]
        }
    elif model.startswith("gpt") or model.startswith("o1"):
        provider = "openai"
        if model == "gpt-4o-2024-08-06":
            pricing_data = {
                "Model": ["gpt-4o-2024-08-06"],
                "Input ($/1M Tokens)": [2.50],
                "Output ($/1M Tokens)": [10.00],
                "Context Window (Tokens)": [128000],
                "Response Max Tokens": [16000]
            }
        elif model == "gpt-4o-mini":
            pricing_data = {
                "Model": ["GPT-4o-mini"],
                "Input ($/1M Tokens)": [0.150],
                "Output ($/1M Tokens)": [0.600],
                "Context Window (Tokens)": [128000],
                "Response Max Tokens": [16000]
            }
        elif model == "o1-mini":
            pricing_data = {
                "Model": ["o1-mini"],
                "Input ($/1M Tokens)": [3.00],
                "Output ($/1M Tokens)": [12.00],
                "Context Window (Tokens)": [128000],
                "Response Max Tokens": [16000]
            }
        else:
            raise ValueError(f"Unsupported OpenAI model selected: {model}")
    else:
        raise ValueError(f"Unsupported model: {model}")
    
    pricing_df = pd.DataFrame(pricing_data)
    return {"pricing_df": pricing_df, "provider": provider}

def estimate_cost(input_tokens, output_tokens, pricing_df):
    """
    Estimates the cost of an API call based on input and output tokens.

    Args:
        input_tokens (int): The number of input tokens used in the API call.
        output_tokens (int): The number of output tokens generated by the API call.
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        float: The estimated cost of the API call in dollars.
    """
    input_cost = (input_tokens / 1e6) * pricing_df['Input ($/1M Tokens)'].iloc[0]
    output_cost = (output_tokens / 1e6) * pricing_df['Output ($/1M Tokens)'].iloc[0]
    return input_cost + output_cost

def get_max_tokens(pricing_df):
    """
    Retrieves the maximum number of tokens allowed for a response.

    Args:
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        int: The maximum number of tokens allowed for a response.
    """
    return pricing_df['Response Max Tokens'].iloc[0]

def get_context_window(pricing_df):
    """
    Retrieves the context window size in tokens.

    Args:
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        int: The context window size in tokens.
    """
    return pricing_df['Context Window (Tokens)'].iloc[0]

==================================================

File: c:\GH\ras-commander\library_assistant\utils\file_handling.py
==================================================
"""
Utility functions for file handling in the Library Assistant.

This module provides functions for reading API keys, system messages,
and processing various file types for the Library Assistant application.

Functions:
- read_api_key(file_path): Reads an API key from a file.
- read_system_message(): Reads the system message from .cursorrules file.
- set_context_folder(): Sets the context folder for file processing.
- strip_code_from_functions(content): Strips code from function bodies.
- handle_python_file(content, filepath, strip_code, chunk_level='function'): Processes Python files.
- handle_markdown_file(content, filepath): Processes Markdown files.
- combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False, chunk_level='function', selected_files=None): Combines and processes multiple files.
"""

import os
import json
import re
import ast
import astor
from pathlib import Path
import tiktoken

def read_api_key(file_path):
    """
    Reads an API key from a file.

    Args:
        file_path (str): Path to the file containing the API key.

    Returns:
        str: The API key.

    Raises:
        FileNotFoundError: If the API key file is not found.
    """
    try:
        with open(file_path, 'r') as file:
            return file.read().strip()
    except FileNotFoundError:
        raise FileNotFoundError(f"API key file not found: {file_path}")

def read_system_message():
    """
    Reads the system message from .cursorrules file.

    Returns:
        str: The system message.

    Raises:
        FileNotFoundError: If the .cursorrules file is not found.
        ValueError: If no system message is found in the file.
    """
    current_dir = Path.cwd()
    cursor_rules_path = current_dir.parent / '.cursorrules'

    if not cursor_rules_path.exists():
        raise FileNotFoundError("This script expects to be in a directory within the ras_commander repo which has a .cursorrules file in its parent directory.")

    with open(cursor_rules_path, 'r') as f:
        system_message = f.read().strip()

    if not system_message:
        raise ValueError("No system message found in .cursorrules file.")

    return system_message

def set_context_folder():
    """
    Sets the context folder for file processing.

    Returns:
        Path: The path to the context folder.
    """
    return Path.cwd().parent

class FunctionStripper(ast.NodeTransformer):
    """AST NodeTransformer to strip code from function bodies."""
    def visit_FunctionDef(self, node):
        new_node = ast.FunctionDef(
            name=node.name,
            args=node.args,
            body=[ast.Pass()],
            decorator_list=node.decorator_list,
            returns=node.returns
        )
        if (node.body and isinstance(node.body[0], ast.Expr) and
            isinstance(node.body[0].value, ast.Str)):
            new_node.body = [node.body[0], ast.Pass()]
        return new_node

def strip_code_from_functions(content):
    """
    Strips code from function bodies, leaving only function signatures and docstrings.

    Args:
        content (str): The Python code content.

    Returns:
        str: The code with function bodies stripped.
    """
    try:
        tree = ast.parse(content)
        stripped_tree = FunctionStripper().visit(tree)
        return astor.to_source(stripped_tree)
    except SyntaxError:
        return content

def handle_python_file(content, filepath, strip_code, chunk_level='function'):
    """
    Processes Python files, optionally stripping code and chunking content.

    Args:
        content (str): The content of the Python file
        filepath (Path): The path to the Python file
        strip_code (bool): Whether to strip code from function bodies
        chunk_level (str): The level at which to chunk the content ('function' or 'file')

    Returns:
        str: The processed content of the Python file
    """
    # Extract header (imports and module docstring)
    header_end = content.find("class ") if "class " in content else content.find("def ") if "def " in content else len(content)
    header = content[:header_end].strip()
    
    if not header:
        return ""
        
    processed_content = [f"\n\n----- {filepath.name} - header -----\n\n{header}\n\n----- End of header -----\n\n"]
    
    if chunk_level == 'function':
        # Improved regex to better handle nested functions and class methods
        function_pattern = r"(?:^|\n)(?:async\s+)?def\s+[^()]+\([^)]*\)\s*(?:->[^:]+)?:\s*(?:[^\n]*\n\s+[^\n]+)*"
        function_chunks = re.finditer(function_pattern, content[header_end:], re.MULTILINE)
        
        for match in function_chunks:
            chunk = match.group(0)
            if strip_code:
                chunk = strip_code_from_functions(chunk)
            processed_content.append(
                f"\n\n----- {filepath.name} - chunk -----\n\n{chunk.strip()}\n\n----- End of chunk -----\n\n"
            )
    else:
        remaining_content = strip_code_from_functions(content[header_end:]) if strip_code else content[header_end:]
        if remaining_content.strip():
            processed_content.append(
                f"\n\n----- {filepath.name} - full_file -----\n\n{remaining_content.strip()}\n\n----- End of full_file -----\n\n"
            )
    
    return "".join(processed_content)

def handle_markdown_file(content, filepath):
    """
    Processes Markdown files, splitting them into sections.

    Args:
        content (str): The content of the Markdown file.
        filepath (Path): The path to the Markdown file.

    Returns:
        str: The processed content of the Markdown file.
    """
    if filepath.name in ["Comprehensive_Library_Guide.md", "STYLE_GUIDE.md"]:
        return f"\n\n----- {filepath.name} - full_file -----\n\n{content}\n\n----- End of {filepath.name} -----\n\n"
    
    sections = re.split(r'\n#+ ', content)
    processed_content = ""
    for section in sections:
        processed_content += f"\n\n----- {filepath.name} - section -----\n\n# {section}\n\n----- End of section -----\n\n"
    return processed_content

def combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False, chunk_level='function', selected_files=None):
    """
    Combines and processes multiple files, respecting omission rules and file selection.
    
    Args:
        summarize_subfolder (Path): The root folder to process
        omit_folders (list): List of folder names to omit
        omit_extensions (list): List of file extensions to omit
        omit_files (list): List of specific file names to omit
        strip_code (bool): Whether to strip code from function bodies
        chunk_level (str): The level at which to chunk content
        selected_files (list): Optional list of specific files to include
    
    Returns:
        tuple: (combined_text, total_token_count, file_token_counts)
    """
    combined_text = []
    file_token_counts = {}
    total_token_count = 0
    
    this_script = Path(__file__).name
    summarize_subfolder = Path(summarize_subfolder)
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")

    # Convert selected_files paths to relative paths for comparison
    selected_file_paths = None
    if selected_files:
        selected_file_paths = {Path(f).as_posix() for f in selected_files}

    for filepath in sorted(summarize_subfolder.rglob('*')):
        # Skip directories and filtered items
        if not filepath.is_file() or filepath.name == this_script:
            continue
            
        if (any(omit_folder in filepath.parts for omit_folder in omit_folders) or
            filepath.suffix.lower() in omit_extensions or
            any(omit_file in filepath.name for omit_file in omit_files)):
            continue

        # Check if file is in selected files list
        if selected_file_paths:
            relative_path = filepath.relative_to(summarize_subfolder).as_posix()
            if relative_path not in selected_file_paths:
                continue

        try:
            content = filepath.read_text(encoding='utf-8')
        except UnicodeDecodeError:
            content = filepath.read_bytes().decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"Error reading {filepath}: {e}")
            continue

        processed_content = ""
        if filepath.suffix.lower() == '.py':
            processed_content = handle_python_file(content, filepath, strip_code, chunk_level)
        elif filepath.suffix.lower() == '.md':
            processed_content = handle_markdown_file(content, filepath)
        else:
            processed_content = f"\n\n----- {filepath.name} - full_file -----\n\n{content}\n\n----- End of {filepath.name} -----\n\n"
        
        if processed_content:
            combined_text.append(processed_content)
            file_tokens = len(enc.encode(processed_content))
            file_token_counts[str(filepath)] = file_tokens
            total_token_count += file_tokens

    return "".join(combined_text), total_token_count, file_token_counts

==================================================

Folder: c:\GH\ras-commander\library_assistant\utils\__pycache__
==================================================

File: c:\GH\ras-commander\library_assistant\web\routes.py
==================================================
"""
Web routes for the Library Assistant.

This module defines the FastAPI routes for the web interface of the Library Assistant.
It handles user interactions, API calls, and serves the HTML template.

Routes:
- GET /: Serves the main page of the application.
- POST /chat: Handles chat interactions with the AI model.
- POST /submit: Handles form submissions for updating settings.
- POST /save_conversation: Saves the current conversation history to a file.
- GET /get_file_tree: Returns the file tree structure with token counts.
"""

from fastapi import APIRouter, Request, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from config.config import load_settings, update_settings
from utils.conversation import add_to_history, get_full_conversation, save_conversation
from utils.context_processing import prepare_full_prompt, update_conversation_history
from utils.cost_estimation import create_pricing_df, estimate_cost
from utils.file_handling import set_context_folder
from api.anthropic import anthropic_stream_response, get_anthropic_client
from api.openai import openai_stream_response, get_openai_client
import json
import tiktoken
import os
from pathlib import Path
import uuid

router = APIRouter()

# Set up Jinja2 templates
templates = Jinja2Templates(directory="web/templates")

# Set up static files
static_files = StaticFiles(directory="web/static")

@router.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """
    Serves the main page of the application.

    Args:
        request (Request): The incoming request object.

    Returns:
        TemplateResponse: The rendered HTML template with current settings.
    """
    settings = load_settings()
    return templates.TemplateResponse("index.html", {
        "request": request,
        "settings": settings
    })

@router.post("/chat")
async def chat(request: Request, message: dict):
    """
    Handles chat interactions with the AI model, supporting streaming responses.
    """
    try:
        # Validate input
        user_message = message.get("message")
        if not user_message:
            raise HTTPException(status_code=400, detail="No message provided.")
            
        # Get selected files for context if provided
        selected_files = message.get("selectedFiles", [])
        
        # Add user message to history
        add_to_history("user", user_message)
        
        # Load settings and prepare context
        settings = load_settings()
        selected_model = settings.selected_model
        
        # Get or generate conversation ID
        conversation_id = message.get("conversation_id", str(uuid.uuid4()))
        
        # Prepare prompt with conversation history
        full_prompt = prepare_full_prompt(
            user_message,
            selected_files if settings.context_mode == 'full_context' else None,
            conversation_id
        )
        
        # Update conversation history
        update_conversation_history(conversation_id, "user", user_message)
        
        # Estimate token usage and cost
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        input_tokens = len(enc.encode(full_prompt))
        output_tokens = 1000  # Assuming average response length
        
        # Get pricing info
        pricing_info = create_pricing_df(selected_model)
        pricing_df = pricing_info["pricing_df"]
        provider = pricing_info["provider"]
        
        estimated_cost = estimate_cost(input_tokens, output_tokens, pricing_df)

        async def stream_response():
            """Generator for streaming the AI response"""
            accumulated_response = []
            try:
                if provider == "anthropic":
                    client = get_anthropic_client(settings.anthropic_api_key)
                    async for chunk in anthropic_stream_response(client, full_prompt):
                        accumulated_response.append(chunk)
                        yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                        
                elif provider == "openai":
                    client = get_openai_client(settings.openai_api_key)
                    messages = [
                        {"role": "system", "content": "You are a helpful AI assistant."},
                        {"role": "user", "content": full_prompt}
                    ]
                    async for chunk in openai_stream_response(client, selected_model, messages):
                        accumulated_response.append(chunk)
                        yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                else:
                    raise HTTPException(status_code=400, detail="Unsupported provider selected.")
                
                # Add complete response to history
                complete_response = "".join(accumulated_response)
                add_to_history("assistant", complete_response)
                
                # Update history with assistant response
                update_conversation_history(conversation_id, "assistant", complete_response)
                
                # Send the cost estimate as a final message
                yield f"data: {json.dumps({'cost': estimated_cost, 'provider': provider})}\n\n"
                
            except Exception as e:
                error_msg = f"Error during streaming: {str(e)}"
                yield f"data: {json.dumps({'error': error_msg})}\n\n"

        return StreamingResponse(
            stream_response(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@router.post("/submit")
async def handle_submit(
    request: Request,
    anthropic_api_key: str = Form(None),
    openai_api_key: str = Form(None),
    selected_model: str = Form(None),
    context_mode: str = Form(None),
    initial_chunk_size: int = Form(None),
    followup_chunk_size: int = Form(None),
):
    """
    Handles form submissions for updating settings.

    This function updates the application settings based on form data submitted by the user.

    Args:
        request (Request): The incoming request object.
        anthropic_api_key (str, optional): The Anthropic API key.
        openai_api_key (str, optional): The OpenAI API key.
        selected_model (str, optional): The selected AI model.
        context_mode (str, optional): The context handling mode.
        initial_chunk_size (int, optional): The initial chunk size for RAG mode.
        followup_chunk_size (int, optional): The followup chunk size for RAG mode.

    Returns:
        JSONResponse: A JSON object indicating the success status of the update.
    """
    updated_data = {}
    if anthropic_api_key is not None:
        updated_data["anthropic_api_key"] = anthropic_api_key
    if openai_api_key is not None:
        updated_data["openai_api_key"] = openai_api_key
    if selected_model is not None:
        updated_data["selected_model"] = selected_model
    if context_mode is not None:
        updated_data["context_mode"] = context_mode
    if initial_chunk_size is not None:
        updated_data["initial_chunk_size"] = initial_chunk_size
    if followup_chunk_size is not None:
        updated_data["followup_chunk_size"] = followup_chunk_size
    
    update_settings(updated_data)
    return JSONResponse({"status": "success"})

@router.post("/save_conversation")
async def save_conversation_endpoint():
    """
    Saves the current conversation history to a file.

    This function triggers the saving of the conversation history and returns the file
    for download.

    Returns:
        FileResponse: The saved conversation history file for download.

    Raises:
        HTTPException: If there's an error saving the conversation.
    """
    try:
        file_path = save_conversation()
        return FileResponse(file_path, filename=os.path.basename(file_path))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save conversation: {str(e)}")

@router.get("/get_file_tree")
async def get_file_tree():
    """Get the file tree structure for the project."""
    try:
        root_dir = Path(__file__).parent.parent.parent
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        def count_tokens(content):
            return len(enc.encode(content))
        
        def build_tree(path):
            """Recursively build the file tree structure."""
            if path.name == '__pycache__':
                return None
                
            item = {
                "name": path.name,
                "path": str(path.relative_to(root_dir)),
                "type": "directory" if path.is_dir() else "file"
            }
            
            if path.is_dir():
                children = []
                total_tokens = 0
                
                cursorrules = path / '.cursorrules'
                if cursorrules.exists():
                    cursorrules_item = build_tree(cursorrules)
                    if cursorrules_item:
                        children.append(cursorrules_item)
                        total_tokens += cursorrules_item.get("tokens", 0)
                
                for child in path.iterdir():
                    if child.name != '.cursorrules':
                        child_item = build_tree(child)
                        if child_item:
                            children.append(child_item)
                            total_tokens += child_item.get("tokens", 0)
                
                item["children"] = sorted(children, key=lambda x: (x["type"] == "file", x["name"]))
                item["tokens"] = total_tokens
            else:
                try:
                    content = path.read_text(encoding='utf-8')
                    item["tokens"] = count_tokens(content)
                except (UnicodeDecodeError, OSError):
                    item["tokens"] = 0
            
            return item
            
        tree = build_tree(root_dir)
        return {"fileTree": tree}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error building file tree: {str(e)}")

@router.get("/get_file_content")
async def get_file_content(path: str):
    """Get the content of a specific file."""
    try:
        # Get the project root directory
        root_dir = Path(__file__).parent.parent.parent
        file_path = root_dir / path
        
        # Validate the path is within the project directory
        if not str(file_path.resolve()).startswith(str(root_dir.resolve())):
            raise HTTPException(status_code=403, detail="Access denied")
            
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="File not found")
            
        # Read and return the file content
        try:
            content = file_path.read_text(encoding='utf-8')
            return {"content": content}
        except UnicodeDecodeError:
            return {"content": "Binary file - cannot display content"}
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading file: {str(e)}")
==================================================

Folder: c:\GH\ras-commander\library_assistant\web\static
==================================================

Folder: c:\GH\ras-commander\library_assistant\web\templates
==================================================

Folder: c:\GH\ras-commander\library_assistant\web\__pycache__
==================================================

File: c:\GH\ras-commander\library_assistant\api\__pycache__\anthropic.cpython-311.pyc
==================================================

    o+g                     $   d Z ddlmZmZmZmZ ddlmZmZm	Z	m
Z
 	 	 dde
eef         deded	e	e         d
eedf         f
dZddeded
e
eef         fdZded
efdZd
ee         fdZ	 	 dde
eef         deded	e	e         d
eedf         f
dZdS )z6
Anthropic API integration for the Library Assistant.
    )AsyncAnthropic	AnthropicAPIErrorAuthenticationError)AsyncGeneratorListOptionalUnion@  Nclientprompt
max_tokensmodelreturnc                L  K   	 |pd}t          | t                    r| nt          | j                  }|j                            |d|dg|d           d{V }g }|2 3 d{V }	 |j        dk    r|j        r|j        j        r|j        j                            d	d
          |	                               t          fddD                       r0d
                    |          }|                                r|W V  g }# t          $ r)}	t          dt          |	                      Y d}	~	d}	~	ww xY w6 |r0d
                    |          }
|
                                r	|
W V  dS dS dS # t          $ r$}	t!          dt          |	                     d}	~	ww xY w)a  
    Streams a response from the Anthropic API using the Claude model.

    Args:
        client: An initialized Anthropic client (sync or async)
        prompt: The prompt to send to the API
        max_tokens: The maximum number of tokens to generate (default: 8000)
        model: The model to use (default: claude-3-5-sonnet-20240620)

    Yields:
        str: Chunks of the response text from the API

    Raises:
        APIError: If there's an error with the API call
        AuthenticationError: If authentication fails
    claude-3-5-sonnet-20240620api_keyuserrolecontentT)r   messagesr   streamNcontent_block_delta c              3   B   K   | ]}                     |          V  d S )N)endswith).0endchunks     6c:\GH\ras-commander\library_assistant\api\anthropic.py	<genexpr>z,anthropic_stream_response.<locals>.<genexpr>8   s/      PP35>>#..PPPPPP    ).!?
z Error processing message chunk: z(Unexpected error in Anthropic API call: )
isinstancer   r   r   createtypedeltatextreplaceappendanyjoinstrip	Exceptionprintstrr   )r   r   r   r   async_clientr   current_linemessagecomplete_linee	remainingr"   s              @r#   anthropic_stream_responser=      sV     ,+L55 ",FN!C!Covv`f`nIoIoIo#,33!%&99:	 4 
 
 
 
 
 
 
 
 # 	 	 	 	 	 	 	'<#888W]8w}Oa8#M.66tR@@E !''... PPPP:OPPPPP *(*(=(=(..00 0"/////')   AQAABBB! $*  	 --I    	  	      L L LJ#a&&JJKKKLsO   AE5 !D>'E5 )BDE5 
D;D61E5 6D;;4E5 5
F#?FF#Fr   r7   c                     | rt          | t                    st          d          |rt          |           nt	          |           S )a>  
    Creates and returns an Anthropic client.

    Args:
        api_key: The Anthropic API key
        async_client: Whether to return an async client (default: False)

    Returns:
        An initialized Anthropic client (sync or async)

    Raises:
        ValueError: If the API key is not provided or invalid
    z(Valid Anthropic API key must be providedr   )r*   r6   
ValueErrorr   r   )r   r7   s     r#   get_anthropic_clientr@   K   sU      E*Wc22 ECDDD.:Z>'****	RY@Z@Z@ZZr%   c                    K   	 t          | d          }|j                            dddddgd	           d
{V  dS # t          j        t          j        t          f$ r Y dS w xY w)z
    Validates the Anthropic API key by making a test API call.

    Args:
        api_key: The Anthropic API key to validate

    Returns:
        True if the API key is valid, False otherwise
    T)r7   r      r   Testr   F)r   r   r   r   N)r@   r   r+   	anthropicr   r   r?   )r   r   s     r#   validate_anthropic_api_keyrE   ]   s      
%gDAAAo$$.%&99:	 % 
 
 	
 	
 	
 	
 	
 	
 	
 t	 =zJ   uus   9? !A$#A$c                      dgS )z
    Returns a list of available Anthropic models.

    Returns:
        List of strings representing available Anthropic model names
    r    rG   r%   r#   get_anthropic_modelsrH   s   s     )))r%   c                J   K   t          | |||          2 3 d{V }|W V  6 dS )a  
    Streams a response from the Anthropic API.

    This function is a wrapper around anthropic_stream_response to provide
    a consistent interface across different API providers.

    Args:
        client: An initialized Anthropic client (sync or async)
        prompt: The prompt to send to the API
        max_tokens: The maximum number of tokens to generate (default: 8000)
        model: The model to use (optional)

    Returns:
        An async generator yielding response chunks
    N)r=   )r   r   r   r   r"   s        r#   stream_responserJ   |   sW      * 1USS       e TSSs   ")r   N)F)__doc__rD   r   r   r   r   typingr   r   r	   r
   r6   intr=   boolr@   rE   rH   rJ   rG   r%   r#   <module>rO      s    O N N N N N N N N N N N 8 8 8 8 8 8 8 8 8 8 8 8
 	AL AL.)+,ALAL AL C=	AL
 CIAL AL AL ALF[ [# [T [eIWeLeFf [ [ [ [$c d    ,*d3i * * * * 	 .)+,  C=	
 CI     r%   
==================================================

File: c:\GH\ras-commander\library_assistant\api\__pycache__\openai.cpython-311.pyc
==================================================

    un+g(                         d Z ddlmZmZ ddlmZmZmZmZm	Z	 	 ddede
deee
e
f                  ded	ee
d
f         f
dZde
d	efdZde
d	efdZd	ee
         fdZd
S )z3
OpenAI API integration for the Library Assistant.
    )OpenAIOpenAIError)AsyncGeneratorListOptionalDictAny>  clientmodelmessages
max_tokensreturnNc                  K   	 | j         j                            |||d          }|D ]]}|j        d         j        j        rD|j        d         j        j                            dd          }|                                r|W V  ^dS # t          $ r$}t          dt          |                     d}~wt          $ r$}t          dt          |                     d}~ww xY w)	a  
    Streams a response from the OpenAI API using the specified model.

    Args:
        client: An initialized OpenAI client
        model: The name of the OpenAI model to use
        messages: A list of message dictionaries to send to the API
        max_tokens: The maximum number of tokens to generate (default: 16000)

    Yields:
        str: Chunks of the response text from the API

    Raises:
        OpenAIError: If there's an error with the API call
    Tr   r   r   streamr    zOpenAI API error: Nz%Unexpected error in OpenAI API call: )chatcompletionscreatechoicesdeltacontentreplacestripr   str	Exception)r   r   r   r   r   chunktextes           3c:\GH\ras-commander\library_assistant\api\openai.pyopenai_stream_responser#      s     *L(//!	 0 
 
  	 	E}Q%- }Q'-5==dBGG::<< JJJJ	 	  9 9 97s1vv77888 L L LJ#a&&JJKKKLs$   BB	 	
C#B22C#?CC#api_keyc                 D    | st          d          t          |           S )z
    Creates and returns an OpenAI client.

    Args:
        api_key: The OpenAI API key

    Returns:
        OpenAI: An initialized OpenAI client

    Raises:
        ValueError: If the API key is not provided
    zOpenAI API key not providedr$   )
ValueErrorr   r&   s    r"   get_openai_clientr(   1   s,      86777'""""    c                    K   	 t          |           }|j        j                            ddddgdd           dS # t          t
          f$ r Y dS w xY w)	z
    Validates the OpenAI API key by making a test API call.

    Args:
        api_key: The OpenAI API key to validate

    Returns:
        bool: True if the API key is valid, False otherwise
    zgpt-3.5-turbouserTest)roler      Fr   T)r(   r   r   r   r   r'   )r$   r   s     r"   validate_openai_api_keyr/   B   s      "7++&&!%&99:	 	' 	
 	
 	
 t$   uus   6< AAc                  
    g dS )z
    Returns a list of available OpenAI models.

    Returns:
        List[str]: A list of strings representing available OpenAI model names
    )zgpt-4o-2024-08-06zgpt-4o-minizo1-mini r1   r)   r"   get_openai_modelsr2   Y   s     ;:::r)   )r
   )__doc__openair   r   typingr   r   r   r   r	   r   intr#   r(   boolr/   r2   r1   r)   r"   <module>r8      s-    ' & & & & & & & < < < < < < < < < < < < < < 	'L 'L'L'L 4S>"'L 	'L
 CI'L 'L 'L 'LR#s #v # # # #"3 4    .;49 ; ; ; ; ; ;r)   
==================================================

File: c:\GH\ras-commander\library_assistant\config\__pycache__\config.cpython-311.pyc
==================================================

    gL                     |    d Z ddlZddlmZ ddlmZmZ  edde          Zddddg dg d	g d
dddd
Zd Z	d Z
dS )a  
Configuration module for the Library Assistant.

This module provides functions for loading and updating settings,
as well as defining default settings for the application.

Functions:
- load_settings(): Loads the current settings from the database or initializes with defaults.
- update_settings(data): Updates the settings in the database with new values.

Constants:
- DEFAULT_SETTINGS: A dictionary containing the default settings for the application.
    N)sessionmaker)SettingsengineF)
autocommit	autoflushbind )zBald Eagle Creek__pycache__z.gitz.githubtestsbuilddistzras_commander.egg-infovenvexample_projectsllm_summarymiscfutureai_toolsdocsExample_Projects_6_6)#z.jpgz.jpegz.pngz.gifz.bmpz.tiffz.webpz.svgz.icoz.pdfz.docz.docxz.xlsz.xlsxz.pptz.pptxz.zipz.rarz.7zz.tarz.gzz.exez.dllz.soz.dylibz.pycz.pyoz.pydz.classz.logz.tmpz.bakz.swpz.batz.sh)zFunctionList.mdDS_Storez	Thumbs.dbllmsummarizezexample_projects.zipz#11_accessing_example_projects.ipynbzExample_Projects_6_5.zipzgithub_code_assistant.ipynbzexample_projects.ipynbz11_Using_RasExamples.ipynbzexample_projects.csvz!rascommander_code_assistant.ipynbzRasExamples.pyfunctioni }  i>  )
anthropic_api_keyopenai_api_keyselected_modelcontext_modeomit_foldersomit_extensions
omit_fileschunk_levelinitial_chunk_sizefollowup_chunk_sizec                     t                      } |                     t                                        t          j        dk                                              }|smt          dddid t                                          D             }|                     |           | 	                                 | 
                    |           |                                  |S )a  
    Loads the current settings from the database or initializes with defaults.

    This function queries the database for existing settings. If no settings are found,
    it initializes the database with the default settings. The settings are stored
    as a singleton record in the database.

    Returns:
        Settings: An instance of the Settings model containing the current settings.
    	singletonidc                 l    i | ]1\  }}|t          |t                    rt          j        |          n|2S  )
isinstancelistjsondumps).0keyvalues      6c:\GH\ras-commander\library_assistant\config\config.py
<dictcomp>z!load_settings.<locals>.<dictcomp>]   sL     ; ; ;3 E4)@)@KDJu%%%e ; ; ;    r&   )SessionLocalqueryr   filterr$   firstDEFAULT_SETTINGSitemsaddcommitrefreshclose)dbsettingss     r.   load_settingsr=   L   s     
Bxx!!(()CDDJJLLH 	 
 

; ;!1!7!7!9!9; ; ;
 

 	x
		


8HHJJJOr0   c                    t                      }|                    t                                        t          j        dk                                              }|                                 D ]>\  }}|dv r$t          ||t          j	        |                     -t          |||           ?|
                                 |                                 dS )aL  
    Updates the settings in the database with new values.

    This function takes a dictionary of settings to update, queries the database
    for the existing settings, and updates the values accordingly. For list-type
    settings (omit_folders, omit_extensions, omit_files), the values are JSON-encoded
    before storage.

    Args:
        data (dict): A dictionary containing the settings to update.
                     Keys should match the attribute names in the Settings model.

    Note:
        This function does not return any value. It directly updates the database.
    r#   )r   r   r   N)r1   r2   r   r3   r$   r4   r6   setattrr)   r*   r8   r:   )datar;   r<   r,   r-   s        r.   update_settingsrA   f   s      
Bxx!!(()CDDJJLLHjjll * *
UCCCHc4:e#4#45555Hc5))))IIKKKHHJJJJJr0   )__doc__r)   sqlalchemy.ormr   database.modelsr   r   r1   r5   r=   rA   r&   r0   r.   <module>rE      s      ' ' ' ' ' ' , , , , , , , , |uFKKK   $	 	 	    e3 3 j  4    r0   
==================================================

File: c:\GH\ras-commander\library_assistant\config\__pycache__\config.cpython-312.pyc
==================================================

    gL                     t    d Z ddlZddlmZ ddlmZmZ  edde      Zddddg dg d	g d
dddd
Zd Z	d Z
y)a  
Configuration module for the Library Assistant.

This module provides functions for loading and updating settings,
as well as defining default settings for the application.

Functions:
- load_settings(): Loads the current settings from the database or initializes with defaults.
- update_settings(data): Updates the settings in the database with new values.

Constants:
- DEFAULT_SETTINGS: A dictionary containing the default settings for the application.
    N)sessionmaker)SettingsengineF)
autocommit	autoflushbind )zBald Eagle Creek__pycache__z.gitz.githubtestsbuilddistzras_commander.egg-infovenvexample_projectsllm_summarymiscfutureai_toolsdocsExample_Projects_6_6)#z.jpgz.jpegz.pngz.gifz.bmpz.tiffz.webpz.svgz.icoz.pdfz.docz.docxz.xlsz.xlsxz.pptz.pptxz.zipz.rarz.7zz.tarz.gzz.exez.dllz.soz.dylibz.pycz.pyoz.pydz.classz.logz.tmpz.bakz.swpz.batz.sh)zFunctionList.mdDS_Storez	Thumbs.dbllmsummarizezexample_projects.zipz#11_accessing_example_projects.ipynbzExample_Projects_6_5.zipzgithub_code_assistant.ipynbzexample_projects.ipynbz11_Using_RasExamples.ipynbzexample_projects.csvz!rascommander_code_assistant.ipynbzRasExamples.pyfunctioni }  i>  )
anthropic_api_keyopenai_api_keyselected_modelcontext_modeomit_foldersomit_extensions
omit_fileschunk_levelinitial_chunk_sizefollowup_chunk_sizec                     t               } | j                  t              j                  t        j                  dk(        j                         }|st        dddit        j                         D ci c]-  \  }}|t        |t              rt        j                  |      n|/ c}}}| j                  |       | j                          | j                  |       | j                          |S c c}}w )a  
    Loads the current settings from the database or initializes with defaults.

    This function queries the database for existing settings. If no settings are found,
    it initializes the database with the default settings. The settings are stored
    as a singleton record in the database.

    Returns:
        Settings: An instance of the Settings model containing the current settings.
    	singletonid )SessionLocalqueryr   filterr$   firstDEFAULT_SETTINGSitems
isinstancelistjsondumpsaddcommitrefreshclose)dbsettingskeyvalues       6c:\GH\ras-commander\library_assistant\config\config.pyload_settingsr9   L   s     
Bxx!(()CDJJLH 

 "2!7!7!9;3 E4)@DJJu%eK ;

 	x
		


8HHJO;s   02C,c                 r   t               }|j                  t              j                  t        j                  dk(        j                         }| j                         D ]7  \  }}|dv r!t        ||t        j                  |             +t        |||       9 |j                          |j                          y)aL  
    Updates the settings in the database with new values.

    This function takes a dictionary of settings to update, queries the database
    for the existing settings, and updates the values accordingly. For list-type
    settings (omit_folders, omit_extensions, omit_files), the values are JSON-encoded
    before storage.

    Args:
        data (dict): A dictionary containing the settings to update.
                     Keys should match the attribute names in the Settings model.

    Note:
        This function does not return any value. It directly updates the database.
    r#   )r   r   r   N)r&   r'   r   r(   r$   r)   r+   setattrr.   r/   r1   r3   )datar4   r5   r6   r7   s        r8   update_settingsr=   f   s      
Bxx!(()CDJJLHjjl *
UCCHc4::e#45Hc5)	*
 IIKHHJ    )__doc__r.   sqlalchemy.ormr   database.modelsr   r   r&   r*   r9   r=   r%   r>   r8   <module>rB      sa     ' , uFK $	  e3 j4r>   
==================================================

File: c:\GH\ras-commander\library_assistant\database\__pycache__\models.cpython-311.pyc
==================================================

    ug
                         d Z ddlmZmZmZmZmZ ddlmZ dZ	 ee	ddi          Z
 e            Z G d d	e          Zej                            e

           dS )af  
Database models for the Library Assistant.

This module defines the SQLAlchemy ORM models used for storing application settings.

Classes:
- Base: The declarative base class for SQLAlchemy models.
- Settings: The model representing application settings.

Constants:
- DATABASE_URL: The URL for the SQLite database.
- engine: The SQLAlchemy engine instance.
    )create_engineColumnStringTextInteger)declarative_basezsqlite:///./settings.dbcheck_same_threadF)connect_argsc                   8   e Zd ZdZdZ eeddd          Z eed          Z	 eed          Z
 eed          Z eed          Z eed          Z eed          Z eed          Z eed          Z eed          Z eed	          Zd
S )Settingsa1  
    SQLAlchemy ORM model for storing application settings.

    This class represents a single row in the settings table, which stores
    all configuration options for the Library Assistant application.

    Attributes:
        id (str): Primary key, set to "singleton" as there's only one settings record.
        anthropic_api_key (str): API key for Anthropic services.
        openai_api_key (str): API key for OpenAI services.
        selected_model (str): The currently selected AI model.
        context_mode (str): The context handling mode (e.g., 'full_context' or 'rag').
        omit_folders (str): JSON string of folders to omit from processing.
        omit_extensions (str): JSON string of file extensions to omit from processing.
        omit_files (str): JSON string of specific files to omit from processing.
        chunk_level (str): The level at which to chunk text (e.g., 'function').
        initial_chunk_size (int): The initial size of text chunks for processing.
        followup_chunk_size (int): The size of follow-up chunks for processing.
    settingsT	singleton)primary_keyindexdefault)nullablei }  )r   i>  N)__name__
__module____qualname____doc____tablename__r   r   idr   anthropic_api_keyopenai_api_keyselected_modelcontext_modeomit_foldersomit_extensions
omit_fileschunk_levelr   initial_chunk_sizefollowup_chunk_size     8c:\GH\ras-commander\library_assistant\database\models.pyr   r      s        ( M	Dk	J	J	JBtd333VD4000NVFT222N6&4000L6$...LfTD111Ot,,,J&$///K777 &%888r$   r   )bindN)r   
sqlalchemyr   r   r   r   r   sqlalchemy.ormr   DATABASE_URLengineBaser   metadata
create_allr#   r$   r%   <module>r.      s     D C C C C C C C C C C C C C + + + + + + ) 
|3F2N	O	O	O !9 !9 !9 !9 !9t !9 !9 !9H   f  % % % % %r$   
==================================================

File: c:\GH\ras-commander\library_assistant\database\__pycache__\models.cpython-312.pyc
==================================================

    ug
                         d Z ddlmZmZmZmZmZ ddlmZ dZ	 ee	ddi      Z
 e       Z G d d	e      Zej                  j                  e

       y)af  
Database models for the Library Assistant.

This module defines the SQLAlchemy ORM models used for storing application settings.

Classes:
- Base: The declarative base class for SQLAlchemy models.
- Settings: The model representing application settings.

Constants:
- DATABASE_URL: The URL for the SQLite database.
- engine: The SQLAlchemy engine instance.
    )create_engineColumnStringTextInteger)declarative_basezsqlite:///./settings.dbcheck_same_threadF)connect_argsc                       e Zd ZdZdZ eeddd      Z eed      Z	 eed      Z
 eed      Z eed      Z eed      Z eed      Z eed      Z eed      Z eed      Z eed	      Zy
)Settingsa1  
    SQLAlchemy ORM model for storing application settings.

    This class represents a single row in the settings table, which stores
    all configuration options for the Library Assistant application.

    Attributes:
        id (str): Primary key, set to "singleton" as there's only one settings record.
        anthropic_api_key (str): API key for Anthropic services.
        openai_api_key (str): API key for OpenAI services.
        selected_model (str): The currently selected AI model.
        context_mode (str): The context handling mode (e.g., 'full_context' or 'rag').
        omit_folders (str): JSON string of folders to omit from processing.
        omit_extensions (str): JSON string of file extensions to omit from processing.
        omit_files (str): JSON string of specific files to omit from processing.
        chunk_level (str): The level at which to chunk text (e.g., 'function').
        initial_chunk_size (int): The initial size of text chunks for processing.
        followup_chunk_size (int): The size of follow-up chunks for processing.
    settingsT	singleton)primary_keyindexdefault)nullablei }  )r   i>  N)__name__
__module____qualname____doc____tablename__r   r   idr   anthropic_api_keyopenai_api_keyselected_modelcontext_modeomit_foldersomit_extensions
omit_fileschunk_levelr   initial_chunk_sizefollowup_chunk_size     8c:\GH\ras-commander\library_assistant\database\models.pyr   r      s    ( M	Dk	JBtd3D40NFT2N&40L$.LTD1Ot,J$/K7 %8r$   r   )bindN)r   
sqlalchemyr   r   r   r   r   sqlalchemy.ormr   DATABASE_URLengineBaser   metadata
create_allr#   r$   r%   <module>r.      s_    D C + ) 
|3F2N	O !9t !9H   f  %r$   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\context_processing.cpython-311.pyc
==================================================

    wn+g$                         d Z ddlZddlmZ ddlmZ ddlmZmZm	Z	 ddl
Z
dadai Zd Zdded	efd
ZdededefdZdefdZddZd Zd ZdS )zD
Utility functions for context processing in the Library Assistant.
    N)Path)load_settings)combine_filesread_system_messageset_context_folder c                     	 t                      } | j        }t          j        | j                  }t          j        | j                  }t          j        | j                  }| j        }| j        }| j	        }t                      }t          ||||d|          \  }	}
}|dk    r|	adS t          |	d||          adS # t          $ r%}t!          dt#          |                       d}~ww xY w)z:
    Initializes the RAG context for the application.
    T)summarize_subfolderomit_foldersomit_extensions
omit_files
strip_codechunk_levelfull_contextrag)textmodeinitial_chunk_sizefollowup_chunk_sizezError initializing context: N)r   context_modejsonloadsr   r   r   r   r   r   r   r   preprocessed_contextprepare_contextpreprocessed_rag_context	Exceptionprintstr)settingsr   r   r   r   r   r   r   context_foldercombined_texttotal_token_count_es                Ac:\GH\ras-commander\library_assistant\utils\context_processing.pyinitialize_rag_contextr&      s&   % ??,z("788*X%=>>Z 344
*%8&: ,-- /< .%+!#/
 /
 /
+(! >))#0   '6"#5$7	( ( ($$$    5SVV55666s   B$B= (B= =
C, C''C,
user_queryreturnc           	         t                      }|j        }t                      }	 |t          vr|g dt          |<   t          |         }||d         k    r||d<   |dk    rz|r+t	                      }t          |g g g dd|          \  }}	}	|}
nt          }
| d|
 d}|d	         D ]+}||d
                                          d|d          dz  },|d|  z  }|S | dt           d}|d	         D ]+}||d
                                          d|d          dz  },|dz  }|d|  z  }|S # t          $ r0}t          dt          |                      | d|  cY d}~S d}~ww xY w)a  
    Prepares the full prompt for the AI model, including context and conversation history.
    
    Args:
        user_query (str): The user's query
        selected_files (list): List of files to include in context
        conversation_id (str): Unique identifier for the conversation
    
    Returns:
        str: The complete prompt including system message, context, and conversation history
    selected_fileshistoryr+   r   Tfile)r
   r   r   r   r   r   r+   z3

Files from RAS-Commander Repository for Context:
z

Previous Conversation:
r,   rolez: content

zUser Query: z=

<context>
Files from RAS-Commander Repository for Context:
z$
</context>

Previous Conversation:
z8Using the context above, please respond to this query:

zError preparing prompt: z

User Query: N)r   r   r   conversation_contextr   r   r   
capitalizer   r   r   r   )r'   r+   conversation_idr   r   system_message	conv_datar    r!   r#   contextpromptmsgr$   s                 r%   prepare_full_promptr9   =   sC    H(L(**N>?"666"05 5 1
 )9	 Y'7888*8I&'>)) /!3!5!5&3(6!#$&!# &#1' ' '#q! (. ( 0 0JQ0 0 0F
 !+ N NS[3355MMYMMMM 1Z111FM ( 0 000 0 0F !+ N NS[3355MMYMMMMSSF1Z111FM ? ? ?1Q11222 >>*>>>>>>>>?s%   B3D) AD) )
E#3%EE#E#r3   r.   r/   c                     | t           vrdg dt           | <   t           |          d                             ||d           dS )a  
    Updates the conversation history for a given conversation.
    
    Args:
        conversation_id (str): Unique identifier for the conversation
        role (str): Role of the message sender ('user' or 'assistant')
        content (str): Content of the message
    Nr*   r,   )r.   r/   )r1   append)r3   r.   r/   s      r%   update_conversation_historyr<      sf     222"1
 1
_-
 ))4;;= =         c                 <    | t           v rg t           |          d<   dS dS )z
    Clears the conversation history for a given conversation.
    
    Args:
        conversation_id (str): Unique identifier for the conversation
    r,   N)r1   )r3   s    r%   clear_conversation_historyr?      s.     ...;=_-i888 /.r=   r    }  >  c                 ,   |dk    r|rd}d}|                      d          D ]c}|                    d          r@d|v r<|                     d          d                             dd          }||v r||dz   z  }W||v r||dz   z  }d|S | S |dk    rwt          | |          }t	          |          }	d}
d}t          j        d	          }|	D ];}t          |                    |                    }||z   |k    r|
|d
z   z  }
||z  }; |
S t          d          )a  
    Prepares context based on the specified mode.
    
    Args:
        text (str): The input text to process
        mode (str): Context preparation mode ('full_context' or 'rag')
        selected_files (list): List of files to include in context (for full_context mode)
        initial_chunk_size (int): Size for initial RAG chunks
        followup_chunk_size (int): Size for follow-up RAG chunks
    
    Returns:
        str: The prepared context
    r   r   N
z----- z - r   r   gpt-3.5-turbor0   z,Invalid mode. Choose 'full_context' or 'rag')
split
startswithreplace
chunk_textrank_chunkstiktokenencoding_for_modellenencode
ValueError)r   r   r+   r   r   filtered_textcurrent_filelinechunksranked_chunksprepared_contextcurrent_sizeencchunk
chunk_sizes                  r%   r   r      sj    ~ 	!ML

4(( 1 1??8,, 1$#'::e#4#4Q#7#?#?"#M#ML#~55%4!^33!TD[0M  	D"455#F++)/::" 	 	ESZZ..//Jj(,??? EFN2 
*GHHHr=   c                     | S )a'  
    Ranks chunks of text based on potential relevance.
    Currently returns chunks in original order, but could be enhanced with
    more sophisticated ranking algorithms.

    Args:
        chunks (list): A list of text chunks to be ranked.

    Returns:
        list: The ranked chunks.
     )rR   s    r%   rI   rI      s	     Mr=   c                 H   t          j        d          }g }d}d}|                     d          D ]Z}t          |                    |                    }||z   |k    r|r|                    |           |}|}I||rd|z   n|z  }||z  }[|r|                    |           |S )z
    Splits text into chunks while maintaining context boundaries.

    Args:
        text (str): The text to be chunked.
        chunk_size (int): Target size for each chunk in tokens.

    Returns:
        list: List of text chunks.
    rD   r   r   rC   )rJ   rK   rE   rL   rM   r;   )r   rX   rV   rR   current_chunkcurrent_tokensrQ   line_tokenss           r%   rH   rH      s     
%o
6
6CFMN

4   
* 
*#**T**++K'*44 -m,,, M(NNMCTD[[tCMk)NN %m$$$Mr=   )NN)r   r   Nr@   rA   )__doc__rJ   pathlibr   config.configr   utils.file_handlingr   r   r   r   r   r   r1   r&   r   r9   r<   r?   r   rI   rH   rZ   r=   r%   <module>rc      s>           ' ' ' ' ' ' V V V V V V V V V V     + + +ZN? N?C N?WZ N? N? N? N?` C #    (> > > > >,I ,I ,I ,I\      r=   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\context_processing.cpython-312.pyc
==================================================

    wg                     r    d Z ddlZddlmZ ddlmZ ddlmZmZm	Z	 ddl
Z
d Zd ZddZd	 Zd
 Zdadad Zy)a  
Utility functions for context processing in the Library Assistant.

This module provides functions for ranking and chunking text,
preparing context for AI processing, and initializing the RAG context.

Functions:
- rank_chunks(chunks): Ranks chunks of text (placeholder function).
- chunk_and_rank(combined_text, chunk_size): Chunks and ranks the combined text.
- prepare_context(combined_text, mode='full_context', initial_chunk_size=32000, followup_chunk_size=16000): Prepares context based on the specified mode.
- reconstruct_context(ranked_chunks, max_tokens): Reconstructs context from ranked chunks.
- initialize_rag_context(): Initializes the RAG context for the application.
- prepare_full_prompt(user_query): Prepares the full prompt for the AI model, including context and user query.
    N)Path)load_settings)combine_filesread_system_messageset_context_folderc                     | S )aP  
    Ranks chunks of text (placeholder function).

    This function is a placeholder for a more sophisticated ranking algorithm.
    Currently, it returns the chunks in their original order.

    Args:
        chunks (list): A list of text chunks to be ranked.

    Returns:
        list: The input chunks in their original order.
     )chunkss    Ac:\GH\ras-commander\library_assistant\utils\context_processing.pyrank_chunksr      s	     M    c                    t        j                  d      }g }d}d}| j                  d      D ]  }|j                  d      rH|j	                  d      r7|r|j                  ||f       d}|j                  d      j                         }\||dz   z  }t        |j                  |            |kD  s|j                  ||f       d} |r|j                  ||f       t        |      }|S )a  
    Chunks the combined text and ranks the resulting chunks.

    This function splits the combined text into chunks based on file and function
    boundaries, then ranks these chunks.

    Args:
        combined_text (str): The combined text to be chunked and ranked.
        chunk_size (int): The maximum size (in tokens) for each chunk.

    Returns:
        list: A list of tuples, where each tuple contains (file_name, chunk_content).
    gpt-3.5-turbo 
z----- z ------)
tiktokenencoding_for_modelsplit
startswithendswithappendstriplenencoder   )combined_text
chunk_sizeencr
   current_chunkcurrent_filelineranked_chunkss           r   chunk_and_rankr#   %   s     
%
%o
6CFML##D) 
#??8$x)@|];< "::c?002LTD[(M3::m,-
:|];< "
# |]34'Mr   c                 D    | dk(  rt         S | dk(  rt        S t        d      )a  
    Prepares context based on the specified mode.

    This function either returns the full context or uses RAG to prepare a
    more focused context.

    Args:
        context_mode (str): The context preparation mode ('full_context' or 'rag').
        initial_chunk_size (int): The initial chunk size for RAG mode.
        followup_chunk_size (int): The followup chunk size for RAG mode.

    Returns:
        str: The prepared context.

    Raises:
        ValueError: If an invalid mode is specified.
    full_contextragz-Invalid mode. Choose 'full_context' or 'rag'.)preprocessed_contextpreprocessed_rag_context
ValueError)context_modeinitial_chunk_sizefollowup_chunk_sizes      r   prepare_contextr-   J   s,    ( ~%##		''HIIr   c                     t        j                  d      }d}t        |j                  |            }| D ]7  \  }}t        |j                  |            }||z   |kD  r |S || dz  }||z  }9 |S )a  
    Reconstructs context from ranked chunks up to a maximum token limit.

    Args:
        ranked_chunks (list): A list of ranked text chunks.
        max_tokens (int): The maximum number of tokens to include in the context.

    Returns:
        str: The reconstructed context.
    r   zORetrieval system has provided these chunks which may be helpful to the query:

z

)r   r   r   r   )r"   
max_tokensr   contextcurrent_tokensfilechunkchunk_tokenss           r   reconstruct_contextr5   e   s     
%
%o
6CaGG,-N$ 'e3::e,-L(:5 N 	eWD>!,&' Nr   c                  8   t               } | j                  }| j                  }t        j                  | j
                        }t        j                  | j                        }t        j                  | j                        }| j                  }| j                  }| j                  }t        d       t               }	t        d       t        |	|||d|      \  }
}}t        d       t               }|dk(  rt        d       |
ayt        d       t!        |
d	||
      ay)a  
    Initializes the RAG context for the application.

    This function loads settings, combines files, and prepares the initial
    context for the RAG system.

    Note: This function modifies global variables `preprocessed_context`
    and `preprocessed_rag_context`.
    zSetting context folderzCombining filesT)summarize_subfolderomit_foldersomit_extensions
omit_files
strip_codechunk_levelzReading system messager%   zSetting full contextz*Preparing RAG Chunks (takes 10-20 seconds)r&   )r   moder+   r,   N)r   selected_modelr*   jsonloadsr8   r9   r:   r<   r+   r,   printr   r   r   r'   r-   r(   )settingsr>   r*   r8   r9   r:   r<   r+   r,   context_folderr   total_token_count_system_messages                 r   initialize_rag_contextrG   }   s    H,,N((L::h334Ljj!9!9:OH//0J&&K!44"66	
"#')N	
*7*!'+'M$a 

"#(*N~%$%,:;#2'1 3	$
 r   r   c                     t               }|j                  }t               }|dk(  rt        d      }| d| d|  }|S t        d      }| d| d|  }|S )z
    Prepares the full prompt for the AI model, including context and user query.

    Args:
        user_query (str): The user's query.

    Returns:
        str: The full prompt including system message, context, and user query.
    r%   )r*   z

Context:
z

User Query: r&   z

<context>Context Chunks:
zh

</context>
The context above is provided for your use in responding to the user's query:

User Query: )r   r*   r   r-   )
user_queryrB   r*   rF   r0   full_prompts         r   prepare_full_promptrK      s     H((L(*N~%!~>'(wi?OPZ|\  "u5<WI F%,( 	 r   )r%   i }  i>  )__doc__r   pathlibr   config.configr   utils.file_handlingr   r   r   r?   r   r#   r-   r5   rG   r'   r(   rK   r	   r   r   <module>rP      sM      ' V V #JJ60.
b   r   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\conversation.cpython-311.pyc
==================================================

    +g                     F    d Z ddlmZ ddlZg ad Zd Zd Zd Zd Zd	 Z	dS )
a  
Utility functions for conversation handling in the Library Assistant.

This module provides functions for managing conversation history,
including adding messages, retrieving the full conversation,
and saving the conversation to a file.

Functions:
- add_to_history(role, content): Adds a message to the conversation history.
- get_full_conversation(): Retrieves the full conversation history as a string.
- save_conversation(): Saves the current conversation history to a file.
    )datetimeNc                 @    t                               | |d           dS )z
    Adds a message to the conversation history.

    Args:
        role (str): The role of the message sender (e.g., 'user' or 'assistant').
        content (str): The content of the message.
    rolecontentN)conversation_historyappendr   s     ;c:\GH\ras-commander\library_assistant\utils\conversation.pyadd_to_historyr      s&     ' B BCCCCC    c                  J    d                     d t          D                       S )z
    Retrieves the full conversation history as a formatted string.

    Returns:
        str: A string representation of the entire conversation history.
    
c                 X    g | ]'}|d                                            d|d          (S )r   : r   )
capitalize).0msgs     r
   
<listcomp>z)get_full_conversation.<locals>.<listcomp>%   s:    fff#V//11EES^EEfffr   )joinr    r   r
   get_full_conversationr      s&     99ffQefffgggr   c                      t          j                                        d          } d|  d}t          j                            t          j                    |          }	 t          |dd          5 }t          D ];}|	                    |d         
                                 d|d	          d
           <	 ddd           n# 1 swxY w Y   |S # t          $ r$}t          dt          |                     d}~ww xY w)aA  
    Saves the current conversation history to a file.

    This function creates a text file with a timestamp in its name,
    containing the full conversation history.

    Returns:
        str: The file path of the saved conversation history.

    Raises:
        IOError: If there's an error writing to the file.
    z%Y%m%d_%H%M%Sconversation_history_z.txtwzutf-8)encodingr   r   r   z

Nz#Error saving conversation history: )r   nowstrftimeospathr   getcwdopenr   writer   IOErrorstr)	timestamp	file_name	file_pathfmessagees         r
   save_conversationr+   '   s_    ''88I7	777IRY[[)44IF)S7333 	Uq/ U U76?5577SS79;MSSSTTTTU	U 	U 	U 	U 	U 	U 	U 	U 	U 	U 	U 	U 	U 	U 	U  F F FDCFFDDEEEFs=   C 1AC6C CC 	C
C 
C=C88C=c                  
    g a dS )z
    Clears the current conversation history.

    This function removes all messages from the conversation history,
    effectively resetting it to an empty state.
    Nr   r   r   r
   clear_conversation_historyr.   @   s     r   c                  *    t          t                    S )z
    Returns the number of messages in the current conversation history.

    Returns:
        int: The number of messages in the conversation history.
    )lenr   r   r   r
   get_conversation_lengthr1   J   s     #$$$r   c                  .    t           rt           d         S dS )z
    Retrieves the last message from the conversation history.

    Returns:
        dict: A dictionary containing the role and content of the last message,
              or None if the conversation history is empty.
    Nr-   r   r   r
   get_last_messager4   S   s      (#B''4r   )
__doc__r   r   r   r   r   r+   r.   r1   r4   r   r   r
   <module>r6      s           				  D D Dh h hF F F2  % % %
 
 
 
 
r   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\conversation.cpython-312.pyc
==================================================

    ug                     D    d Z ddlmZ ddlZg ad Zd Zd Zd Zd Zd	 Z	y)
a  
Utility functions for conversation handling in the Library Assistant.

This module provides functions for managing conversation history,
including adding messages, retrieving the full conversation,
and saving the conversation to a file.

Functions:
- add_to_history(role, content): Adds a message to the conversation history.
- get_full_conversation(): Retrieves the full conversation history as a string.
- save_conversation(): Saves the current conversation history to a file.
    )datetimeNc                 4    t         j                  | |d       y)z
    Adds a message to the conversation history.

    Args:
        role (str): The role of the message sender (e.g., 'user' or 'assistant').
        content (str): The content of the message.
    rolecontentN)conversation_historyappendr   s     ;c:\GH\ras-commander\library_assistant\utils\conversation.pyadd_to_historyr      s     ' BC    c            	          dj                  t        D  cg c]  } | d   j                          d| d     c}       S c c} w )z
    Retrieves the full conversation history as a formatted string.

    Returns:
        str: A string representation of the entire conversation history.
    
r   : r   )joinr   
capitalize)msgs    r
   get_full_conversationr      s@     99Qef#V//12"S^4DEfggfs   "=c                     t        j                         j                  d      } d|  d}t        j                  j                  t        j                         |      }	 t        |dd      5 }t        D ]-  }|j                  |d   j                          d|d	    d
       / 	 ddd       |S # 1 sw Y   |S xY w# t        $ r}t        dt        |             d}~ww xY w)aA  
    Saves the current conversation history to a file.

    This function creates a text file with a timestamp in its name,
    containing the full conversation history.

    Returns:
        str: The file path of the saved conversation history.

    Raises:
        IOError: If there's an error writing to the file.
    z%Y%m%d_%H%M%Sconversation_history_z.txtwzutf-8)encodingr   r   r   z

Nz#Error saving conversation history: )r   nowstrftimeospathr   getcwdopenr   writer   IOErrorstr)	timestamp	file_name	file_pathfmessagees         r
   save_conversationr'   '   s     ''8I'	{$7IRYY[)4IF)S73 	Uq/ U76?557879;M:NdSTU	U 	U  F;CF8DEEFs6   B: +7B-#	B: -B72B: 7B: :	CCCc                      g a y)z
    Clears the current conversation history.

    This function removes all messages from the conversation history,
    effectively resetting it to an empty state.
    Nr    r   r
   clear_conversation_historyr+   @   s
     r   c                       t        t              S )z
    Returns the number of messages in the current conversation history.

    Returns:
        int: The number of messages in the conversation history.
    )lenr   r*   r   r
   get_conversation_lengthr.   J   s     #$$r   c                  "    t         r	t         d   S y)z
    Retrieves the last message from the conversation history.

    Returns:
        dict: A dictionary containing the role and content of the last message,
              or None if the conversation history is empty.
    Nr)   r*   r   r
   get_last_messager1   S   s     #B''r   )
__doc__r   r   r   r   r   r'   r+   r.   r1   r*   r   r
   <module>r3      s;     	  DhF2%
r   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\cost_estimation.cpython-311.pyc
==================================================

    v+g                     *    d Z ddlZd Zd Zd Zd ZdS )ap  
Utility functions for cost estimation in the Library Assistant.

This module provides functions for creating pricing dataframes and
estimating the cost of API calls based on token usage.

Functions:
- create_pricing_df(model): Creates a pricing dataframe for a given model.
- estimate_cost(input_tokens, output_tokens, pricing_df): Estimates the cost of an API call.
    Nc                    |                      d          rd}dgdgdgdgdgd}n|                      d	          s|                      d
          rPd}| dk    rdgdgdgdgdgd}nL| dk    rdgdgdgdgdgd}n8| dk    rdgdgdgdgdgd}n$t          d|            t          d|            t          j        |          }||dS )a  
    Creates a pricing dataframe for a given model.

    This function returns a dictionary containing a pandas DataFrame with pricing information
    and the provider (OpenAI or Anthropic) for the specified model.

    Args:
        model (str): The name of the model (e.g., 'gpt-4', 'claude-3-5-sonnet-20240620').

    Returns:
        dict: A dictionary containing:
            - 'pricing_df': A pandas DataFrame with columns 'Model', 'Input ($/1M Tokens)',
                            'Output ($/1M Tokens)', 'Context Window (Tokens)', and 'Response Max Tokens'.
            - 'provider': A string indicating the provider ('openai' or 'anthropic').

    Raises:
        ValueError: If an unsupported model is specified.
    claude	anthropiczClaude 3.5 Sonnet      i@ i    )ModelInput ($/1M Tokens)Output ($/1M Tokens)Context Window (Tokens)Response Max Tokensgpto1openaizgpt-4o-2024-08-06g      @g      $@i  i>  zgpt-4o-minizGPT-4o-minig333333?g333333?zo1-minig      @g      (@z#Unsupported OpenAI model selected: zUnsupported model: )
pricing_dfprovider)
startswith
ValueErrorpd	DataFrame)modelr   pricing_datar   s       >c:\GH\ras-commander\library_assistant\utils\cost_estimation.pycreate_pricing_dfr      sb   & !! &8)*$%3%'D(.x$(6
 
 
		%	 	  8E$4$4T$:$: 8'''-.(,v).,28(-w LL m##'(-w).,28(-w LL i#(,v).,28(-w LL J5JJKKK6u66777l++J$(;;;    c                 p    | dz  |d         j         d         z  }|dz  |d         j         d         z  }||z   S )a  
    Estimates the cost of an API call based on input and output tokens.

    Args:
        input_tokens (int): The number of input tokens used in the API call.
        output_tokens (int): The number of output tokens generated by the API call.
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        float: The estimated cost of the API call in dollars.
    g    .Ar	   r   r
   iloc)input_tokensoutput_tokensr   
input_costoutput_costs        r   estimate_costr"   L   sJ     $
3H(I(Nq(QQJ 3&*5K*L*QRS*TTK##r   c                 (    | d         j         d         S )z
    Retrieves the maximum number of tokens allowed for a response.

    Args:
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        int: The maximum number of tokens allowed for a response.
    r   r   r   r   s    r   get_max_tokensr%   \   s     +,1!44r   c                 (    | d         j         d         S )z
    Retrieves the context window size in tokens.

    Args:
        pricing_df (pd.DataFrame): A pandas DataFrame containing pricing information.

    Returns:
        int: The context window size in tokens.
    r   r   r   r$   s    r   get_context_windowr'   h   s     /05a88r   )__doc__pandasr   r   r"   r%   r'    r   r   <module>r+      sd   	 	    << << <<|$ $ $ 
5 
5 
5
9 
9 
9 
9 
9r   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\file_handling.cpython-311.pyc
==================================================

    tn+g~$                         d Z ddlZddlZddlZddlZddlZddlmZ ddlZd Z	d Z
d Z G d dej                  Zd	 ZddZd ZddZdS )aL  
Utility functions for file handling in the Library Assistant.

This module provides functions for reading API keys, system messages,
and processing various file types for the Library Assistant application.

Functions:
- read_api_key(file_path): Reads an API key from a file.
- read_system_message(): Reads the system message from .cursorrules file.
- set_context_folder(): Sets the context folder for file processing.
- strip_code_from_functions(content): Strips code from function bodies.
- handle_python_file(content, filepath, strip_code, chunk_level='function'): Processes Python files.
- handle_markdown_file(content, filepath): Processes Markdown files.
- combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False, chunk_level='function', selected_files=None): Combines and processes multiple files.
    N)Pathc                     	 t          | d          5 }|                                                                cddd           S # 1 swxY w Y   dS # t          $ r t          d|            w xY w)z
    Reads an API key from a file.

    Args:
        file_path (str): Path to the file containing the API key.

    Returns:
        str: The API key.

    Raises:
        FileNotFoundError: If the API key file is not found.
    rNzAPI key file not found: )openreadstripFileNotFoundError)	file_pathfiles     <c:\GH\ras-commander\library_assistant\utils\file_handling.pyread_api_keyr      s    H)S!! 	'T99;;$$&&	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' 	' H H H F9 F FGGGHs-   A &AA A		A A	A A/c                  D   t          j                    } | j        dz  }|                                st	          d          t          |d          5 }|                                                                }ddd           n# 1 swxY w Y   |st          d          |S )z
    Reads the system message from .cursorrules file.

    Returns:
        str: The system message.

    Raises:
        FileNotFoundError: If the .cursorrules file is not found.
        ValueError: If no system message is found in the file.
    z.cursorrulesz}This script expects to be in a directory within the ras_commander repo which has a .cursorrules file in its parent directory.r   Nz-No system message found in .cursorrules file.)	r   cwdparentexistsr	   r   r   r   
ValueError)current_dircursor_rules_pathfsystem_messages       r   read_system_messager   ,   s     (**K#*^;##%% a  !`  a  a  	a		%	% *))* * * * * * * * * * * * * * *  JHIIIs   'BBBc                  2    t          j                    j        S )zr
    Sets the context folder for file processing.

    Returns:
        Path: The path to the context folder.
    )r   r   r        r   set_context_folderr   E   s     8::r   c                       e Zd ZdZd ZdS )FunctionStripperz7AST NodeTransformer to strip code from function bodies.c                 z   t          j        |j        |j        t          j                    g|j        |j                  }|j        rtt          |j        d         t           j	                  rOt          |j        d         j
        t           j                  r%|j        d         t          j                    g|_        |S )N)nameargsbodydecorator_listreturnsr   )astFunctionDefr   r    Passr"   r#   r!   
isinstanceExprvalueStr)selfnodenew_nodes      r   visit_FunctionDefz"FunctionStripper.visit_FunctionDefP   s    ?(**.L
 
 
 I 	7*TYq\38<< 	7ty|)3733	7!Yq\38::6HMr   N)__name__
__module____qualname____doc__r.   r   r   r   r   r   N   s)        AA    r   r   c                     	 t          j        |           }t                                          |          }t	          j        |          S # t          $ r | cY S w xY w)z
    Strips code from function bodies, leaving only function signatures and docstrings.

    Args:
        content (str): The Python code content.

    Returns:
        str: The code with function bodies stripped.
    )r$   parser   visitastor	to_sourceSyntaxError)contenttreestripped_trees      r   strip_code_from_functionsr<   ]   sc    y!!(**0066}---   s   AA AAfunctionc                    d| v r|                      d          n'd| v r|                      d          nt          |           }| d|                                         }|sdS d|j         d| dg}|dk    rd	}t	          j        || |d         t          j                  }|D ][}	|	                    d
          }
|rt          |
          }
|	                    d|j         d|
                                 d           \nj|rt          | |d                   n	| |d         }|                                r3|	                    d|j         d|                                 d           d
                    |          S )a  
    Processes Python files, optionally stripping code and chunking content.

    Args:
        content (str): The content of the Python file
        filepath (Path): The path to the Python file
        strip_code (bool): Whether to strip code from function bodies
        chunk_level (str): The level at which to chunk the content ('function' or 'file')

    Returns:
        str: The processed content of the Python file
    zclass zdef N 

----- z - header -----

z

----- End of header -----

r=   zS(?:^|\n)(?:async\s+)?def\s+[^()]+\([^)]*\)\s*(?:->[^:]+)?:\s*(?:[^\n]*\n\s+[^\n]+)*r   z - chunk -----

z

----- End of chunk -----

 - full_file -----

z 

----- End of full_file -----

)findlenr   r   refinditer	MULTILINEgroupr<   appendjoin)r9   filepath
strip_codechunk_level
header_endheaderprocessed_contentfunction_patternfunction_chunksmatchchunkremaining_contents               r   handle_python_filerU   n   s    ,4w+>+>h'''\bfm\m\mGLLQWDXDXDXsvw~ssJ[j[!''))F rqhmqqqqqrj  q+&6
8Lbl[[$ 	 	EKKNNE 91%88$$mX]mmekkmmmmm   		 PZs5gjkk6JKKK_fgqgrgr_s""$$ 	$$ BX]  B  BBSBYBYB[B[  B  B  B   77$%%%r   c                     |j         dv rd|j          d|  d|j          dS t          j        d|           }d}|D ]}|d|j          d| d	z  }|S )
a  
    Processes Markdown files, splitting them into sections.

    Args:
        content (str): The content of the Markdown file.
        filepath (Path): The path to the Markdown file.

    Returns:
        str: The processed content of the Markdown file.
    )zComprehensive_Library_Guide.mdzSTYLE_GUIDE.mdr@   rA   

----- End of  -----

z\n#+ r?   z - section -----

# z

----- End of section -----

)r   rD   split)r9   rJ   sectionsrO   sections        r   handle_markdown_filer\      s     }LLLtHMtttt[c[httttx'**H { {z(-zzwzzzzr   Fc                    g }i }d}	t          t                    j        }
t          |           } t          j        d          }d}|rd |D             }t          |                     d                    D ]                                rj        |
k    r#t          fd|D                       s6j	        
                                |v st          fd|D                       ru|r,                    |                                           }||vr	                     d	          }na# t          $ r,                                                     dd
          }Y n,t"          $ r }t%          d d|            Y d}~d}~ww xY wd}j	        
                                dk    rt'          |||          }nDj	        
                                dk    rt)          |          }ndj         d| dj         d}|rN|                    |           t-          |                    |                    }||t1                    <   |	|z  }	d                    |          |	|fS )a  
    Combines and processes multiple files, respecting omission rules and file selection.
    
    Args:
        summarize_subfolder (Path): The root folder to process
        omit_folders (list): List of folder names to omit
        omit_extensions (list): List of file extensions to omit
        omit_files (list): List of specific file names to omit
        strip_code (bool): Whether to strip code from function bodies
        chunk_level (str): The level at which to chunk content
        selected_files (list): Optional list of specific files to include
    
    Returns:
        tuple: (combined_text, total_token_count, file_token_counts)
    r   zgpt-3.5-turboNc                 P    h | ]#}t          |                                          $S r   )r   as_posix).0r   s     r   	<setcomp>z combine_files.<locals>.<setcomp>   s*    JJJatAww//11JJJr   *c              3   *   K   | ]}|j         v V  d S N)parts)r`   omit_folderrJ   s     r   	<genexpr>z combine_files.<locals>.<genexpr>   s*      NN+x~-NNNNNNr   c              3   *   K   | ]}|j         v V  d S rd   )r   )r`   	omit_filerJ   s     r   rg   z combine_files.<locals>.<genexpr>   s*      GGy	X]*GGGGGGr   zutf-8)encodingignore)errorszError reading z: r?   z.pyz.mdr@   rA   rW   rX   )r   __file__r   tiktokenencoding_for_modelsortedrglobis_fileanysuffixlowerrelative_tor_   	read_textUnicodeDecodeError
read_bytesdecode	ExceptionprintrU   r\   rH   rC   encodestrrI   )summarize_subfolderomit_foldersomit_extensions
omit_filesrK   rL   selected_filescombined_textfile_token_countstotal_token_countthis_scriptencselected_file_pathsrelative_pathr9   erO   file_tokensrJ   s                     @r   combine_filesr      s     Mx..%K233

%o
6
6C  KJJ>JJJ.44S99:: $- $-!! 	X]k%A%ANNNNNNNNN 	O!!##66GGGGJGGGGG 7  	$001DEENNPPM$777	(('(::GG! 	M 	M 	M))++22782LLGGG 	 	 	2822q22333HHHH	 ?  ""e++ 27HjR] ^ ^_""$$-- 4Wh G G !FX]  !F  !FRY  !F  !Fltly  !F  !F  !F 	-  !2333cjj):;;<<K/:c(mm,,77=!!#46GGGs   D113F&	F/F

F)r=   )Fr=   N)r2   osjsonrD   r$   r6   pathlibr   rn   r   r   r   NodeTransformerr   r<   rU   r\   r   r   r   r   <module>r      s     
			  				 



        H H H&  2      s*     ")& )& )& )&V  (CH CH CH CH CH CHr   
==================================================

File: c:\GH\ras-commander\library_assistant\utils\__pycache__\file_handling.cpython-312.pyc
==================================================

    ug                          d Z ddlZddlZddlZddlZddlZddlmZ ddlZd Z	d Z
d Z G d dej                        Zd	 Zdd
Zd ZddZy)a7  
Utility functions for file handling in the Library Assistant.

This module provides functions for reading API keys, system messages,
and processing various file types for the Library Assistant application.

Functions:
- read_api_key(file_path): Reads an API key from a file.
- read_system_message(): Reads the system message from .cursorrules file.
- set_context_folder(): Sets the context folder for file processing.
- strip_code_from_functions(content): Strips code from function bodies.
- handle_python_file(content, filepath, strip_code, chunk_level='function'): Processes Python files.
- handle_markdown_file(content, filepath): Processes Markdown files.
- combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False, chunk_level='function'): Combines and processes multiple files.
    N)Pathc                     	 t        | d      5 }|j                         j                         cddd       S # 1 sw Y   yxY w# t        $ r t        d|        w xY w)z
    Reads an API key from a file.

    Args:
        file_path (str): Path to the file containing the API key.

    Returns:
        str: The API key.

    Raises:
        FileNotFoundError: If the API key file is not found.
    rNzAPI key file not found: )openreadstripFileNotFoundError)	file_pathfiles     <c:\GH\ras-commander\library_assistant\utils\file_handling.pyread_api_keyr      s^    H)S! 	'T99;$$&	' 	' 	' H":9+ FGGHs"   A 6	A ?A A Ac                     t        j                         } | j                  dz  }|j                         st	        d      t        |d      5 }|j                         j                         }ddd       st        d      |S # 1 sw Y   xY w)z
    Reads the system message from .cursorrules file.

    Returns:
        str: The system message.

    Raises:
        FileNotFoundError: If the .cursorrules file is not found.
        ValueError: If no system message is found in the file.
    z.cursorrulesz}This script expects to be in a directory within the ras_commander repo which has a .cursorrules file in its parent directory.r   Nz-No system message found in .cursorrules file.)	r   cwdparentexistsr	   r   r   r   
ValueError)current_dircursor_rules_pathfsystem_messages       r   read_system_messager   ,   s     ((*K#**^;##%  !`  a  	a		% *)* HII* *s   BB
c                  >    t        j                         j                  S )zr
    Sets the context folder for file processing.

    Returns:
        Path: The path to the context folder.
    )r   r   r        r   set_context_folderr   E   s     88:r   c                       e Zd ZdZd Zy)FunctionStripperz7AST NodeTransformer to strip code from function bodies.c                    t        j                  |j                  |j                  t        j                         g|j
                  |j                        }|j                  rt        |j                  d   t         j                        rYt        |j                  d   j                  t         j                        r(|j                  d   t        j                         g|_        |S )N)nameargsbodydecorator_listreturnsr   )astFunctionDefr   r    Passr"   r#   r!   
isinstanceExprvalueStr)selfnodenew_nodes      r   visit_FunctionDefz"FunctionStripper.visit_FunctionDefP   s    ??((*..LL
 II*TYYq\388<tyy|))3773!YYq\388:6HMr   N)__name__
__module____qualname____doc__r.   r   r   r   r   r   N   s
    Ar   r   c                     	 t        j                  |       }t               j                  |      }t	        j
                  |      S # t        $ r | cY S w xY w)z
    Strips code from function bodies, leaving only function signatures and docstrings.

    Args:
        content (str): The Python code content.

    Returns:
        str: The code with function bodies stripped.
    )r$   parser   visitastor	to_sourceSyntaxError)contenttreestripped_trees      r   strip_code_from_functionsr<   ]   sK    yy!(*006}-- s   AA AAc                 ~   d| v r| j                  d      n
t        |       }| d| }d|j                   d| d}|dk(  rTt        j                  d| |d t        j
                        }|D ]%  }|rt        |      }|d|j                   d| d	z  }' |S |rt        | |d       n| |d } |d|j                   d
|  dz  }|S )a  
    Processes Python files, optionally stripping code and chunking content.

    Args:
        content (str): The content of the Python file.
        filepath (Path): The path to the Python file.
        strip_code (bool): Whether to strip code from function bodies.
        chunk_level (str): The level at which to chunk the content ('function' or 'file').

    Returns:
        str: The processed content of the Python file.
    zclass N

----- z - header -----

z

----- End of header -----

functionz(def .*?(?=\ndef |\Z))z - chunk -----

z

----- End of chunk -----

 - full_file -----

z 

----- End of full_file -----

)findlenr   refindallDOTALLr<   )	r9   filepath
strip_codechunk_level
header_endheaderprocessed_contentfunction_chunkschunks	            r   handle_python_filerN   n   s     ,4w+>h'CLJ[j!F$X]]O3FvhNopj **%>
@TVXV_V_`$ 	wE1%8:hmm_<NugUu!vv	w  FP+GJK,@AU\]g]hUiz(--8NwiW{||r   c                     |j                   dv r d|j                    d|  d|j                    dS t        j                  d|       }d}|D ]  }|d|j                    d| d	z  } |S )
a  
    Processes Markdown files, splitting them into sections.

    Args:
        content (str): The content of the Markdown file.
        filepath (Path): The path to the Markdown file.

    Returns:
        str: The processed content of the Markdown file.
    )zComprehensive_Library_Guide.mdzSTYLE_GUIDE.mdr>   r@   

----- End of  -----

z\n#+  z - section -----

# z

----- End of section -----

)r   rC   split)r9   rF   sectionsrK   sections        r   handle_markdown_filerV      s     }}LLHMM?*@	IZ[c[h[hZiisttxx'*H {z(--8NwiWyzz{r   c                    d}i }d}t        t              j                  }	t        |       } t        j                  d      }
| j                  d      D ];  j                  |	k7  st        fd|D              r)j                  j                         |vsFt        fd|D              r[j                         sl	 t        dd	      5 }|j                         }d
d
d
       j                  j                         dk(  rt        ||      }nJj                  j                         dk(  rt              }n dj                   d dj                   d}||z  }t!        |
j#                  |            }||t%              <   ||z  }> |||fS # 1 sw Y   xY w# t        $ rG t        d      5 }|j                         j                  dd      }d
d
d
       n# 1 sw Y   nxY wY w xY w)a}  
    Combines and processes multiple files, respecting omission rules.

    Args:
        summarize_subfolder (Path): The root folder to process.
        omit_folders (list): List of folder names to omit.
        omit_extensions (list): List of file extensions to omit.
        omit_files (list): List of specific file names to omit.
        strip_code (bool): Whether to strip code from function bodies in Python files.
        chunk_level (str): The level at which to chunk content ('function' or 'file').

    Returns:
        tuple: A tuple containing the combined text, total token count, and a dictionary of file token counts.
    rR   r   zgpt-3.5-turbo*c              3   :   K   | ]  }|j                   v   y wN)parts).0omit_folderrF   s     r   	<genexpr>z combine_files.<locals>.<genexpr>   s     RkK8>>1R   c              3   :   K   | ]  }|j                   v   y wrZ   )r   )r\   	omit_filerF   s     r   r^   z combine_files.<locals>.<genexpr>   s     K9I.Kr_   r   zutf-8)encodingNrbignore)errorsz.pyz.mdr>   r@   rP   rQ   )r   __file__r   tiktokenencoding_for_modelrglobanysuffixloweris_filer   r   UnicodeDecodeErrordecoderN   rV   rB   encodestr)summarize_subfolderomit_foldersomit_extensions
omit_filesrG   rH   combined_textfile_token_countstotal_token_countthis_scriptencinfiler9   file_tokensrF   s                 @r   combine_filesr}      s    Mx.%%K23

%
%o
6C'--c2 1MM[(R\RROO!!#?:K
KK!Qhg> 0&"(++-0 ??((*e30(JP[\G__**,527HEG *8==/9OPWyXijrjwjwix  yC  DG(!#**W"563>!#h-0![0!114 +->>>%0 0) Qh- Q"(++-"6"6wx"6"PQ Q QQsB   ?F"FF"F	F""G27"G"	G2"G+'G21G2)r?   )Fr?   )r2   osjsonrC   r$   r6   pathlibr   rg   r   r   r   NodeTransformerr   r<   rN   rV   r}   r   r   r   <module>r      sW     
  	 
   H&2s** ":(1?r   
==================================================

File: c:\GH\ras-commander\library_assistant\web\static\fileTree.js
==================================================
// FileTreeViewer Component
class FileTreeViewer {
    constructor(container) {
        this.container = container;
        this.selectedFiles = new Set();
        this.expandedFolders = new Set(['library_assistant/']);
        this.searchQuery = '';
        this.sortOption = 'name-asc';
        this.hiddenTypes = new Set(['__pycache__']);
        this.fileContents = new Map();
        this.onSelectionChange = null;
    initialize(data) {
        this.fileData = null;
        this.container.innerHTML = '';
        this.fileData = data;
        if (data) {
        this.fileData = null;
            const rootElement = this.renderNode(data);
            this.container.appendChild(rootElement);
            
            // Auto-select default files after rendering
            this.autoSelectDefaultFiles();
        }
    }

    async autoSelectDefaultFiles() {
        // Expand all folders initially to ensure we can find the files
        const collectFolderPaths = (node, path = '') => {
            if (node.type === 'directory' && node.children) {
                const fullPath = path ? `${path}/${node.name}` : node.name;
                this.expandedFolders.add(fullPath);
                node.children.forEach(child => {
                    collectFolderPaths(child, fullPath);
                });
            }
        };
        
        if (this.fileData) {
            collectFolderPaths(this.fileData);
        }

        // Re-render with expanded folders
        this.render();

        // Find and select default files
        for (const filename of this.defaultFiles) {
            const filePath = this.findFilePath(this.fileData, filename);
            if (filePath) {
                await this.toggleFileSelection(filePath, true);
            }
        }

        // Keep folders expanded to show default files
        this.render();
    }

    findFilePath(node, targetFilename, currentPath = '') {
        if (node.type === 'file' && node.name === targetFilename) {
            return currentPath ? `${currentPath}/${node.name}` : node.name;
        }
        if (node.children) {
            for (const child of node.children) {
                const childPath = currentPath ? `${currentPath}/${node.name}` : node.name;
                const result = this.findFilePath(child, targetFilename, childPath);
                if (result) return result;
            }
        }
        return null;
    }

    getFolderStats(obj) {
        const stats = {
            totalTokens: 0,
            fileCount: 0,
            maxTokenFile: { name: '', tokens: 0 },
            minTokenFile: { name: '', tokens: Infinity },
            avgTokens: 0
        };

        const processNode = (node, path = '') => {
            Object.entries(node).forEach(([key, value]) => {
                const fullPath = path ? `${path}/${key}` : key;
                if (typeof value === 'number') {
                    stats.totalTokens += value;
                    stats.fileCount++;
                    if (value > stats.maxTokenFile.tokens) {
                        stats.maxTokenFile = { name: fullPath, tokens: value };
                    }
                    if (value < stats.minTokenFile.tokens) {
                        stats.minTokenFile = { name: fullPath, tokens: value };
                    }
                } else {
                    processNode(value, fullPath);
                }
            });
        };

        processNode(obj);
        stats.avgTokens = stats.fileCount > 0 ? Math.round(stats.totalTokens / stats.fileCount) : 0;
        return stats;
    }

    getSelectedStats() {
        const getNodeTokens = (obj, path) => {
            let total = 0;
            if (typeof obj === 'number') {
                return this.selectedFiles.has(path) ? obj : 0;
            }
            Object.entries(obj).forEach(([key, value]) => {
                const fullPath = path ? `${path}/${key}` : key;
                total += getNodeTokens(value, fullPath);
            });
            return total;
        };

        const selectedTokens = getNodeTokens(this.fileData, '');
        
        // Calculate costs for different models
        const costs = {
            claude: (selectedTokens / 1000000) * 3.00,  // $3.00 per million tokens
            gpt4: (selectedTokens / 1000000) * 10.00,   // $10.00 per million tokens
            gpt4mini: (selectedTokens / 1000000) * 0.60 // $0.60 per million tokens
        };

        return {
            totalTokens: selectedTokens,
            fileCount: this.selectedFiles.size,
            avgTokens: this.selectedFiles.size > 0 ? Math.round(selectedTokens / this.selectedFiles.size) : 0,
            costs: costs
        };
    }

    sortItems(items) {
        return Object.entries(items).sort(([keyA, valueA], [keyB, valueB]) => {
            const isFileA = typeof valueA === 'number';
            const isFileB = typeof valueB === 'number';
            
            if (isFileA !== isFileB) return isFileA ? 1 : -1;

            switch (this.sortOption) {
                case 'name-asc': return keyA.localeCompare(keyB);
                case 'name-desc': return keyB.localeCompare(keyA);
                case 'tokens-asc':
                    return (isFileA ? valueA : this.getFolderStats(valueA).totalTokens) 
                           - (isFileB ? valueB : this.getFolderStats(valueB).totalTokens);
                case 'tokens-desc':
                    return (isFileB ? valueB : this.getFolderStats(valueB).totalTokens)
                           - (isFileA ? valueA : this.getFolderStats(valueA).totalTokens);
                default: return 0;
            }
        });
    }

    renderToolbar() {
        return `
            <div class="mb-3">
                <div class="d-flex gap-2 mb-2">
                    <div class="flex-grow-1">
                        <input
                            type="text"
                            placeholder="Search files..."
                            value="${this.searchQuery}"
                            class="search-input form-control"
                        />
                    </div>
                    <select
                        class="sort-select form-select"
                        style="width: auto;"
                        value="${this.sortOption}"
                    >
                        <option value="name-asc">Name (A-Z)</option>
                        <option value="name-desc">Name (Z-A)</option>
                        <option value="tokens-asc">Tokens (Low to High)</option>
                        <option value="tokens-desc">Tokens (High to Low)</option>
                    </select>
                </div>
                <div class="d-flex gap-2">
                    <button class="expand-all btn btn-sm btn-outline-secondary">
                        Expand All
                    </button>
                    <button class="collapse-all btn btn-sm btn-outline-secondary">
                        Collapse All
                    </button>
                </div>
            </div>
        `;
    }

    renderTree(obj, path = '') {
        let html = '<div class="ml-4">';
        const sortedEntries = this.sortItems(obj);
        
        for (const [key, value] of sortedEntries) {
            const fullPath = path ? `${path}/${key}` : key;
            const isFolder = typeof value === 'object';
            
            if (this.hiddenTypes.has(key) && key !== '.cursorrules') continue;
            if (this.searchQuery && !fullPath.toLowerCase().includes(this.searchQuery.toLowerCase())) continue;
            
            if (isFolder) {
                const folderStats = this.getFolderStats(value);
                const isExpanded = this.expandedFolders.has(fullPath);
                
                html += `
                    <div class="folder" data-path="${fullPath}">
                        <div class="flex items-center py-1 hover:bg-gray-100">
                            <button class="toggle-folder mr-1">
                                ${isExpanded ? 'â–¼' : 'â–¶'}
                            </button>
                            <button class="select-folder mr-2">
                                â–¡
                            </button>
                            ðŸ“
                            <span class="font-medium ml-2">${key}</span>
                            <span class="ml-2 text-sm text-gray-500">(${folderStats.totalTokens.toLocaleString()} tokens)</span>
                        </div>
                        ${isExpanded ? this.renderTree(value, fullPath) : ''}
                    </div>
                `;
            } else {
                const isSelected = this.selectedFiles.has(fullPath);
                const isDefaultFile = this.defaultFiles.includes(key);
                
                html += `
                    <div class="flex items-center py-1 hover:bg-gray-100 ${isDefaultFile ? 'bg-gray-50' : ''}">
                        <div class="w-4 mr-1"></div>
                        <input
                            type="checkbox"
                            class="file-checkbox mr-2 ml-1"
                            data-path="${fullPath}"
                            ${isSelected ? 'checked' : ''}
                            ${isDefaultFile ? 'data-default="true"' : ''}
                        />
                        <span class="file-icon">ðŸ“„</span>
                        <span class="ml-2 ${isDefaultFile ? 'font-medium' : ''}">${key}</span>
                        <span class="ml-2 text-sm text-gray-500">(${value.toLocaleString()} tokens)</span>
                    </div>
                `;
            }
        }
        return html + '</div>';
    }

    renderStats() {
        const stats = this.getFolderStats(this.fileData);
        const selectedStats = this.getSelectedStats();

        return `
            <div class="p-3 bg-gray-100 rounded border" style="position: sticky; bottom: 0;">
                <div class="mb-2 border-bottom pb-2">
                    <div class="d-flex justify-content-between">
                        <span>Selected Files:</span>
                        <strong>${selectedStats.fileCount} of ${stats.fileCount} files</strong>
                    </div>
                    <div class="d-flex justify-content-between">
                        <span>Selected Tokens:</span>
                        <strong>${selectedStats.totalTokens.toLocaleString()} of ${stats.totalTokens.toLocaleString()}</strong>
                    </div>
                </div>
                <div>
                    <div class="fw-bold mb-1">Estimated Context Costs:</div>
                    <div class="d-flex justify-content-between">
                        <span>Claude 3.5:</span>
                        <strong>$${selectedStats.costs.claude.toFixed(4)}</strong>
                    </div>
                    <div class="d-flex justify-content-between">
                        <span>GPT-4:</span>
                        <strong>$${selectedStats.costs.gpt4.toFixed(4)}</strong>
                    </div>
                    <div class="d-flex justify-content-between">
                        <span>GPT-4 Mini:</span>
                        <strong>$${selectedStats.costs.gpt4mini.toFixed(4)}</strong>
                    </div>
                </div>
            </div>
        `;
    }

    render() {
        this.container.innerHTML = `
            <div class="border rounded-lg p-4 bg-white">
                <div class="flex justify-between items-center mb-4">
                    <h2 class="text-xl font-semibold">Project Files</h2>
                </div>
                ${this.renderToolbar()}
                <div class="border rounded p-4 bg-gray-50 mb-4" style="max-height: 60vh; overflow-y: auto;">
                    ${this.renderTree(this.fileData)}
                </div>
                ${this.renderStats()}
            </div>
        `;

        // Attach event listeners after rendering
        this.attachEventListeners();
    }

    attachEventListeners() {
        // File checkboxes
        this.container.querySelectorAll('.file-checkbox').forEach(checkbox => {
            checkbox.addEventListener('change', async (e) => {
                e.preventDefault();
                const path = e.target.dataset.path;
                await this.toggleFileSelection(path);
            });
        });

        // Folder expansion
        this.container.querySelectorAll('.toggle-folder').forEach(button => {
            button.addEventListener('click', (e) => {
                const folderDiv = e.target.closest('.folder');
                const path = folderDiv.dataset.path;
                if (this.expandedFolders.has(path)) {
                    this.expandedFolders.delete(path);
                } else {
                    this.expandedFolders.add(path);
                }
                this.render();
            });
        });

        // Search input
        this.container.querySelector('.search-input').addEventListener('input', (e) => {
            this.searchQuery = e.target.value;
            this.render();
        });

        // Sort select
        this.container.querySelector('.sort-select').addEventListener('change', (e) => {
            this.sortOption = e.target.value;
            this.render();
        });

        // Expand/Collapse all
        this.container.querySelector('.expand-all').addEventListener('click', () => {
            const collectFolderPaths = (obj, path = '') => {
                Object.entries(obj).forEach(([key, value]) => {
                    if (typeof value === 'object') {
                        const fullPath = path ? `${path}/${key}` : key;
                        this.expandedFolders.add(fullPath);
                        collectFolderPaths(value, fullPath);
                    }
                });
            };
            collectFolderPaths(this.fileData);
            this.render();
        });

        this.container.querySelector('.collapse-all').addEventListener('click', () => {
            this.expandedFolders.clear();
            this.render();
        });
    }

    async getFileContent(path) {
        try {
            if (!path) return null;
            
            // Check if we already have the content cached
            if (this.fileContents.has(path)) {
                return this.fileContents.get(path);
            }

            const response = await fetch(`/get_file_content?path=${encodeURIComponent(path)}`);
            if (!response.ok) {
                throw new Error(`Failed to fetch file content: ${response.statusText}`);
            }
            
            const content = await response.json();
            // Cache the content
            this.fileContents.set(path, content);
            return content;
        } catch (error) {
            console.error('Error fetching file content:', error);
            return null;
        }
    }

    async toggleFileSelection(path, forceSelect = false) {
        if (!forceSelect && this.selectedFiles.has(path)) {
            this.selectedFiles.delete(path);
            this.updateAllCostDisplays();
            this.render();
        } else {
            const checkbox = this.container.querySelector(`[data-path="${path}"]`);
            if (checkbox) checkbox.disabled = true;

            const fileContent = await this.getFileContent(path);
            
            if (checkbox) checkbox.disabled = false;

            if (fileContent) {
                this.selectedFiles.add(path);
                this.updateAllCostDisplays();
                this.render();
            } else {
                console.error('Failed to load file content:', path);
                alert(`Failed to load content for ${path}`);
                return;
            }
        }

        if (this.onSelectionChange) {
            this.onSelectionChange(Array.from(this.selectedFiles));
        }
    }

    setSelectionChangeCallback(callback) {
        this.onSelectionChange = callback;
    }

    async getSelectedContents() {
        const contents = new Map();
        for (const path of this.selectedFiles) {
            const content = await this.getFileContent(path);
            if (content) {
                contents.set(path, content);
            }
        }
        return contents;
    }

    updateAllCostDisplays() {
        const stats = this.getSelectedStats();
        
        // Update file tree stats by re-rendering
        this.render();
        
        // Update total cost estimation for the message input area
        if (window.updateTotalCostEstimation) {
            window.updateTotalCostEstimation();
        }
    }
}
==================================================

File: c:\GH\ras-commander\library_assistant\web\templates\index.html
==================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAS Commander Library Assistant</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/4.3.0/marked.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f8f9fa; }
        .container { max-width: 1800px; margin: auto; width: 98%; }
        .form-group { margin-bottom: 15px; }
        label { display: block; margin-bottom: 5px; }
        input, select, textarea { width: 100%; padding: 8px; box-sizing: border-box; }
        .chat-box { border: 1px solid #ced4da; padding: 10px; height: 400px; overflow-y: scroll; background-color: #ffffff; border-radius: 5px; }
        .message { margin: 10px 0; }
        .user { color: #0d6efd; }
        .assistant { color: #198754; }
        .error { color: red; }
        .btn { padding: 10px 20px; background-color: #0d6efd; color: white; border: none; cursor: pointer; border-radius: 5px; }
        .btn:hover { background-color: #0b5ed7; }
        .cost { margin-top: 10px; font-weight: bold; }
        .copy-btn {
            margin-left: 10px;
            padding: 5px 10px;
            background-color: #0d6efd;
            color: white;
            border: none;
            cursor: pointer;
            border-radius: 5px;
        }
        .copy-btn:hover {
            background-color: #0b5ed7;
        }
        .hidden { display: none; }
        /* Slider Styling */
        input[type=range] {
            -webkit-appearance: none;
            width: 100%;
            height: 10px;
            border-radius: 5px;
            background: #d3d3d3;
            outline: none;
            margin-top: 10px;
        }
        input[type=range]::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #0d6efd;
            cursor: pointer;
        }
        input[type=range]::-moz-range-thumb {
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #0d6efd;
            cursor: pointer;
        }
        /* Add to existing styles */
        #file-tree {
            max-height: 600px;
            overflow-y: auto;
        }
        
        .file-tree-item {
            padding: 4px 8px;
            cursor: pointer;
            display: flex;
            align-items: center;
        }
        
        .file-tree-item:hover {
            background-color: #f0f0f0;
        }
        
        .file-tree-toggle {
            margin-right: 5px;
            width: 16px;
            height: 16px;
            text-align: center;
            line-height: 16px;
        }
        
        .file-tree-indent {
            margin-left: 20px;
        }
        
        .file-tree-selected {
            background-color: #e3f2fd;
        }

        /* Enhanced file tree styling */
        .file-tree-container {
            width: 800px;
            flex-shrink: 0;
            border: 1px solid #dee2e6;
            border-radius: 0.25rem;
            background: white;
            display: flex;
            flex-direction: column;
        }

        .file-tree-header {
            padding: 10px;
            background: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .file-tree {
            padding: 10px;
            max-height: 600px;
            overflow-y: auto;
        }

        .file-item {
            display: flex;
            align-items: center;
            padding: 4px 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.2s;
            width: 100%;
        }

        .file-item:hover {
            background-color: #f0f0f0;
        }

        .file-item.selected {
            background-color: #e3f2fd;
        }

        .file-icon {
            margin-right: 8px;
            width: 16px;
            text-align: center;
        }

        .folder-toggle {
            cursor: pointer;
            transition: transform 0.2s;
        }

        .folder-toggle.open {
            transform: rotate(90deg);
        }

        .file-children {
            margin-left: 20px;
        }

        /* Main container layout */
        .main-container {
            display: flex;
            gap: 20px;
            margin-top: 20px;
            height: calc(100vh - 200px);
            min-height: 400px;
            width: 100%;
        }

        .chat-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            min-width: 400px;
        }

        .file-tree-header {
            padding: 10px;
            background: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-shrink: 0;
        }

        .file-tree {
            padding: 10px;
            overflow-y: auto;
            flex-grow: 1;
        }

        .chat-box {
            flex-grow: 1;
            overflow-y: auto;
            border: 1px solid #ced4da;
            border-radius: 5px;
            padding: 10px;
            background-color: #ffffff;
            margin-bottom: 10px;
        }

        /* Responsive adjustments */
        @media (max-width: 1400px) {
            .file-tree-container {
                width: 600px;
            }
        }

        @media (max-width: 768px) {
            .main-container {
                flex-direction: column;
                height: auto;
            }
            
            .file-tree-container {
                width: 100%;
                height: 300px;
                margin-bottom: 20px;
            }
            
            .chat-container {
                width: 100%;
            }
        }

        /* Add to existing styles */
        .file-checkbox {
            margin-right: 8px;
            cursor: pointer;
            width: 16px;
            height: 16px;
        }

        .file-item {
            display: flex;
            align-items: center;
            padding: 4px 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.2s;
        }

        .file-item:hover {
            background-color: #f0f0f0;
        }

        .file-item.selected {
            background-color: #e3f2fd;
        }

        .file-icon {
            margin-right: 8px;
            width: 16px;
            text-align: center;
        }

        .file-item span {
            cursor: pointer;
        }

        .file-checkbox:indeterminate {
            background-color: #86b7fe;
            border-color: #86b7fe;
        }

        /* Add to existing styles */
        .folder-toggle {
            display: inline-block;
            width: 16px;
            height: 16px;
            line-height: 14px;
            text-align: center;
            cursor: pointer;
            font-family: monospace;
            font-weight: bold;
            font-size: 16px;
            color: #666;
            user-select: none;
        }

        .folder-toggle:hover {
            color: #000;
        }

        .file-icon {
            margin-right: 8px;
            width: 16px;
            text-align: center;
            cursor: pointer;
        }

        .file-item {
            display: flex;
            align-items: center;
            padding: 4px 8px;
            cursor: pointer;
            border-radius: 4px;
            transition: background-color 0.2s;
        }

        .file-children {
            margin-left: 0; /* Remove left margin since we're using padding */
        }

        /* Add to existing styles */
        .token-count {
            margin-left: auto;
            color: #666;
            font-size: 0.85em;
            padding-left: 10px;
        }

        .total-tokens-display {
            padding: 10px;
            background-color: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
            font-weight: bold;
            color: #0d6efd;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="mt-4 mb-4">RAS Commander Library Assistant</h1>
        
        <div id="context-cost-display" class="alert alert-info mb-4" style="display: none;">
            <strong>Context Cost Estimates:</strong>
            <span id="context-costs"></span>
        </div>
        
        <div class="card mb-4">
            <div class="card-header">
                <h2 class="mb-0">
                    <button class="btn btn-link" type="button" data-bs-toggle="collapse" data-bs-target="#settingsCollapse">
                        Settings
                    </button>
                </h2>
            </div>
            <div id="settingsCollapse" class="collapse">
                <div class="card-body">
                    <form id="settings-form">
                        <div class="form-group">
                            <label for="selected_model">Select Model:</label>
                            <select id="selected_model" name="selected_model" class="form-select" required>
                                <option value="" disabled>Select a model</option>
                                <option value="claude-3-5-sonnet-20240620" {% if settings.selected_model == 'claude-3-5-sonnet-20240620' %}selected{% endif %}>Claude 3.5 Sonnet (Anthropic)</option>
                                <option value="gpt-4o-2024-08-06" {% if settings.selected_model == 'gpt-4o-2024-08-06' %}selected{% endif %}>GPT-4o (OpenAI)</option>
                                <option value="gpt-4o-mini" {% if settings.selected_model == 'gpt-4o-mini' %}selected{% endif %}>GPT-4o-mini (OpenAI)</option>
                                <option value="o1-mini" {% if settings.selected_model == 'o1-mini' %}selected{% endif %}>o1-mini (OpenAI)</option>
                            </select>
                        </div>
                        <div id="anthropic_api_key_group" class="form-group hidden">
                            <label for="anthropic_api_key">Anthropic API Key:</label>
                            <input type="password" id="anthropic_api_key" name="anthropic_api_key" class="form-control" value="{{ settings.anthropic_api_key }}">
                        </div>
                        <div id="openai_api_key_group" class="form-group hidden">
                            <label for="openai_api_key">OpenAI API Key:</label>
                            <input type="password" id="openai_api_key" name="openai_api_key" class="form-control" value="{{ settings.openai_api_key }}">
                        </div>
                        <div class="form-group">
                            <label for="context_mode">Context Handling Mode:</label>
                            <select id="context_mode" name="context_mode" class="form-select" required>
                                <option value="" disabled>Select a mode</option>
                                <option value="full_context" {% if settings.context_mode == 'full_context' %}selected{% endif %}>Full Context</option>
                                <option value="rag" {% if settings.context_mode == 'rag' %}selected{% endif %}>Retrieval-Augmented Generation (RAG)</option>
                            </select>
                        </div>
                        <div id="rag_sliders" class="hidden">
                            <div class="form-group">
                                <label for="initial_chunk_size">Initial RAG Context Size (tokens):</label>
                                <input type="range" id="initial_chunk_size" name="initial_chunk_size" min="16000" max="96000" step="1000" value="{{ settings.initial_chunk_size }}">
                                <span id="initial_chunk_size_value">{{ settings.initial_chunk_size }}</span>
                            </div>
                            <div class="form-group">
                                <label for="followup_chunk_size">Follow-up RAG Context Size (tokens):</label>
                                <input type="range" id="followup_chunk_size" name="followup_chunk_size" min="16000" max="64000" step="1000" value="{{ settings.followup_chunk_size }}">
                                <span id="followup_chunk_size_value">{{ settings.followup_chunk_size }}</span>
                            </div>
                        </div>
                    </form>
                </div>
            </div>
        </div>
        
        <div class="main-container">
            <!-- File Tree Container -->
            <div id="file-tree-container" class="file-tree-container" style="display: none;">
                <div class="file-tree-header">
                    <h5 class="mb-0">Project Files</h5>
                    <button class="btn btn-sm btn-outline-secondary" id="collapse-tree">
                        <i class="bi bi-arrows-collapse"></i>
                    </button>
                </div>
                <div class="file-tree" id="file-tree"></div>
            </div>
            
            <!-- Chat Container -->
            <div class="chat-container">
                <div class="chat-box" id="chat-box"></div>
                <form id="chat-form" class="mt-3">
                    <div class="form-group">
                        <label for="user-input">Your message:</label>
                        <textarea id="user-input" class="form-control" rows="3" required></textarea>
                    </div>
                    <div class="mt-2 d-flex justify-content-between align-items-center">
                        <div class="d-flex align-items-center gap-3">
                            <button type="submit" class="btn btn-primary">Send</button>
                            <div id="cost-display" class="text-muted"></div>
                        </div>
                        <button type="button" onclick="saveConversation()" class="btn btn-outline-secondary btn-sm">
                            Save Conversation
                        </button>
                    </div>
                </form>
            </div>
        </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        class FileTreeViewer {
            constructor(container) {
                this.container = container;
                this.selectedFiles = new Set();
                this.onSelectionChange = null;
                this.totalTokensDisplay = document.createElement('div');
                this.totalTokensDisplay.className = 'total-tokens-display';
                container.parentElement.insertBefore(this.totalTokensDisplay, container);
                this.expandedFolders = [
                    'ras-commander',           // Root project folder
                    'ras-commander/examples',  // Examples folder
                    'ras-commander/ras_commander'  // Main package folder
                ];
            }

            updateTotalTokens() {
                let totalTokens = 0;
                this.selectedFiles.forEach(path => {
                    const element = this.container.querySelector(`[data-path="${path}"]`);
                    if (element) {
                        totalTokens += parseInt(element.dataset.tokens || 0);
                    }
                });
                this.totalTokensDisplay.textContent = `Selected: ${totalTokens.toLocaleString()} tokens`;
            }

            renderNode(node, level = 0) {
                const item = document.createElement('div');
                item.className = 'file-item';
                item.style.paddingLeft = `${level * 20}px`;
                item.dataset.path = node.path;
                item.dataset.tokens = node.tokens;
                
                const checkbox = document.createElement('input');
                checkbox.type = 'checkbox';
                checkbox.className = 'file-checkbox me-2';
                
                if (node.type === 'directory') {
                    // Directory rendering
                    const toggleBtn = document.createElement('span');
                    toggleBtn.className = 'folder-toggle me-2';
                    
                    const icon = document.createElement('span');
                    icon.className = 'file-icon';
                    
                    const nameSpan = document.createElement('span');
                    nameSpan.textContent = node.name;
                    
                    const tokenCount = document.createElement('span');
                    tokenCount.className = 'token-count';
                    tokenCount.textContent = `${node.tokens.toLocaleString()} tokens`;
                    
                    item.appendChild(toggleBtn);
                    item.appendChild(checkbox);
                    item.appendChild(icon);
                    item.appendChild(nameSpan);
                    item.appendChild(tokenCount);
                    
                    const content = document.createElement('div');
                    content.className = 'file-children';
                    
                    // Check if this folder should be expanded by default
                    const shouldExpand = this.expandedFolders.some(path => {
                        const normalizedPath = path.replace(/\\/g, '/');
                        const nodePath = node.path.replace(/\\/g, '/');
                        return normalizedPath === nodePath || normalizedPath.startsWith(nodePath + '/');
                    });
                    
                    content.style.display = shouldExpand ? 'block' : 'none';
                    toggleBtn.textContent = shouldExpand ? 'âˆ’' : '+';
                    icon.textContent = shouldExpand ? 'ðŸ“‚' : 'ðŸ“';
                    
                    // Function to handle expand/collapse
                    const toggleFolder = (e) => {
                        e.stopPropagation();
                        const isExpanded = content.style.display !== 'none';
                        content.style.display = isExpanded ? 'none' : 'block';
                        toggleBtn.textContent = isExpanded ? '+' : 'âˆ’';
                        icon.textContent = isExpanded ? 'ðŸ“' : 'ðŸ“‚';
                    };
                    
                    // Add click handlers for toggle button, icon, and name
                    toggleBtn.onclick = toggleFolder;
                    icon.onclick = toggleFolder;
                    nameSpan.onclick = toggleFolder;
                    
                    // Handle directory checkbox click
                    checkbox.onclick = (e) => {
                        e.stopPropagation();
                        this.toggleDirectory(node, checkbox.checked, content);
                    };
                    
                    if (node.children) {
                        node.children.forEach(child => {
                            const childElement = this.renderNode(child, level + 1);
                            content.appendChild(childElement);
                        });
                    }
                    
                    const container = document.createElement('div');
                    container.appendChild(item);
                    container.appendChild(content);
                    return container;
                    
                } else {
                    // File rendering
                    const emptyToggle = document.createElement('span');
                    emptyToggle.className = 'folder-toggle me-2';
                    emptyToggle.style.visibility = 'hidden';
                    
                    const icon = document.createElement('span');
                    icon.className = 'file-icon';
                    icon.textContent = 'ðŸ“„';
                    
                    const nameSpan = document.createElement('span');
                    nameSpan.textContent = node.name;
                    
                    const tokenCount = document.createElement('span');
                    tokenCount.className = 'token-count';
                    tokenCount.textContent = `${node.tokens.toLocaleString()} tokens`;
                    
                    checkbox.checked = this.selectedFiles.has(node.path);
                    
                    item.appendChild(emptyToggle);
                    item.appendChild(checkbox);
                    item.appendChild(icon);
                    item.appendChild(nameSpan);
                    item.appendChild(tokenCount);
                    
                    // Update checkbox handler to include token counting
                    checkbox.onclick = (e) => {
                        e.stopPropagation();
                        if (checkbox.checked) {
                            this.selectedFiles.add(node.path);
                        } else {
                            this.selectedFiles.delete(node.path);
                        }
                        
                        if (this.onSelectionChange) {
                            this.onSelectionChange(Array.from(this.selectedFiles));
                        }
                        
                        this.updateParentCheckboxes(item);
                        this.updateTotalTokens();
                    };
                    
                    // Handle file name click (view content)
                    const viewContent = async (e) => {
                        e.stopPropagation();
                        
                        const selected = document.querySelector('.file-tree-selected');
                        if (selected) {
                            selected.classList.remove('file-tree-selected');
                        }
                        
                        item.classList.add('file-tree-selected');
                        
                        try {
                            const response = await fetch(`/get_file_content?path=${encodeURIComponent(node.path)}`);
                            if (!response.ok) {
                                throw new Error(`HTTP error! status: ${response.status}`);
                            }
                            const data = await response.json();
                            
                            const userInput = document.getElementById('user-input');
                            userInput.value = `Please help me with this file:\n\n${data.content}`;
                        } catch (error) {
                            console.error('Error loading file content:', error);
                            alert('Error loading file content: ' + error.message);
                        }
                    };
                    
                    // Add click handlers for both icon and name
                    icon.onclick = viewContent;
                    nameSpan.onclick = viewContent;
                    
                    return item;
                }
            }

            toggleDirectory(node, checked, contentElement) {
                const checkboxes = contentElement.querySelectorAll('.file-checkbox');
                checkboxes.forEach(checkbox => {
                    checkbox.checked = checked;
                    
                    const fileItem = checkbox.closest('.file-item');
                    const nameSpan = fileItem.querySelector('span:not(.file-icon):not(.token-count)');
                    const path = this.getNodePath(node, nameSpan.textContent);
                    
                    if (checked) {
                        this.selectedFiles.add(path);
                    } else {
                        this.selectedFiles.delete(path);
                    }
                });

                if (this.onSelectionChange) {
                    this.onSelectionChange(Array.from(this.selectedFiles));
                }
                
                this.updateTotalTokens();
            }

            getNodePath(node, fileName) {
                // Construct the full path for a file within a directory
                return node.path + '/' + fileName;
            }

            updateParentCheckboxes(element) {
                // Update parent directory checkboxes based on child selections
                let parent = element.parentElement;
                while (parent) {
                    const parentCheckbox = parent.querySelector(':scope > .file-item > .file-checkbox');
                    if (parentCheckbox) {
                        const childCheckboxes = parent.querySelectorAll('.file-children .file-checkbox');
                        const allChecked = Array.from(childCheckboxes).every(cb => cb.checked);
                        const someChecked = Array.from(childCheckboxes).some(cb => cb.checked);
                        
                        parentCheckbox.checked = allChecked;
                        parentCheckbox.indeterminate = someChecked && !allChecked;
                    }
                    parent = parent.parentElement;
                }
            }

            initialize(data) {
                this.container.innerHTML = '';
                if (data) {
                    const rootElement = this.renderNode(data);
                    this.container.appendChild(rootElement);
                }
            }

            getSelectedFiles() {
                return Array.from(this.selectedFiles);
            }
        }

        document.addEventListener('DOMContentLoaded', function() {
            const selectedModel = document.getElementById('selected_model');
            const anthropicGroup = document.getElementById('anthropic_api_key_group');
            const openaiGroup = document.getElementById('openai_api_key_group');
            const contextMode = document.getElementById('context_mode');
            const fileTreeContainer = document.getElementById('file-tree-container');
            const fileTree = document.getElementById('file-tree');
            const chatContainer = document.querySelector('.chat-container');
            const ragSliders = document.getElementById('rag_sliders');

            // Initialize file tree viewer
            const fileTreeViewer = new FileTreeViewer(document.getElementById('file-tree'));

            async function loadFileTree() {
                try {
                    const response = await fetch('/get_file_tree');
                    if (!response.ok) {
                        throw new Error(`HTTP error! status: ${response.status}`);
                    }
                    const data = await response.json();
                    fileTreeViewer.initialize(data.fileTree);
                } catch (error) {
                    console.error('Error loading file tree:', error);
                    alert('Error loading file tree: ' + error.message);
                }
            }

            function toggleFileTree() {
                if (contextMode.value === 'full_context') {
                    fileTreeContainer.style.display = 'flex';
                    loadFileTree();
                } else {
                    fileTreeContainer.style.display = 'none';
                }
            }

            function toggleApiKeys() {
                const model = selectedModel.value;
                if (model.startsWith('claude')) {
                    anthropicGroup.classList.remove('hidden');
                    openaiGroup.classList.add('hidden');
                } else if (model.startsWith('gpt') || model.startsWith('o1')) {
                    openaiGroup.classList.remove('hidden');
                    anthropicGroup.classList.add('hidden');
                } else {
                    anthropicGroup.classList.add('hidden');
                    openaiGroup.classList.add('hidden');
                }
            }

            function toggleRagSliders() {
                if (contextMode.value === 'rag') {
                    ragSliders.classList.remove('hidden');
                } else {
                    ragSliders.classList.add('hidden');
                }
            }

            // Add event listeners
            contextMode.addEventListener('change', () => {
                toggleFileTree();
                toggleRagSliders();
                autoSaveSettings();
            });

            selectedModel.addEventListener('change', () => {
                toggleApiKeys();
                autoSaveSettings();
            });

            // Initialize UI state
            toggleApiKeys();
            toggleRagSliders();

            // Force initial file tree display if in full_context mode
            if (contextMode.value === 'full_context') {
                fileTreeContainer.style.display = 'flex';
                loadFileTree();
            }

            // Handle chat form submission
            const chatForm = document.getElementById('chat-form');
            const chatBox = document.getElementById('chat-box');
            const userInput = document.getElementById('user-input');
            const costDisplay = document.getElementById('cost-display');

            chatForm.addEventListener('submit', async (e) => {
                e.preventDefault();
                const message = userInput.value.trim();
                if (message === '') return;

                // Get selected files if in full_context mode
                const selectedFiles = contextMode.value === 'full_context' ? 
                    fileTreeViewer.getSelectedFiles() : [];

                // Display user message with markdown rendering
                const userMessage = document.createElement('div');
                userMessage.className = 'message user';
                userMessage.innerHTML = 'User: ' + marked.parse(message);
                chatBox.appendChild(userMessage);

                // Clear input
                userInput.value = '';

                // Create assistant message container
                const assistantMessage = document.createElement('div');
                assistantMessage.className = 'message assistant';
                assistantMessage.innerHTML = 'Assistant: ';
                const responseText = document.createElement('span');
                assistantMessage.appendChild(responseText);
                chatBox.appendChild(assistantMessage);

                // Scroll to bottom
                chatBox.scrollTop = chatBox.scrollHeight;

                try {
                    const response = await fetch('/chat', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'Accept': 'text/event-stream',
                        },
                        body: JSON.stringify({ 
                            message: message,
                            selectedFiles: selectedFiles
                        })
                    });

                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let buffer = '';
                    let fullResponse = '';

                    while (true) {
                        const {value, done} = await reader.read();
                        if (done) break;
                        
                        buffer += decoder.decode(value, {stream: true});
                        const lines = buffer.split('\n');
                        
                        // Process all complete lines
                        buffer = lines.pop() || '';  // Keep any incomplete line in the buffer
                        
                        for (const line of lines) {
                            if (line.startsWith('data: ')) {
                                try {
                                    const data = JSON.parse(line.slice(6));
                                    
                                    if (data.chunk) {
                                        // Accumulate the complete response
                                        fullResponse += data.chunk;
                                        // Render the accumulated response
                                        responseText.innerHTML = marked.parse(fullResponse);
                                    } else if (data.cost) {
                                        costDisplay.textContent = `Estimated Cost: $${data.cost.toFixed(6)} (${data.provider})`;
                                        
                                        // Add copy button after streaming is complete
                                        const copyButton = document.createElement('button');
                                        copyButton.textContent = 'Copy Response';
                                        copyButton.className = 'copy-btn btn btn-sm';
                                        copyButton.onclick = () => copyToClipboard(fullResponse);
                                        assistantMessage.appendChild(copyButton);
                                    } else if (data.error) {
                                        const errorDiv = document.createElement('div');
                                        errorDiv.className = 'error';
                                        errorDiv.textContent = data.error;
                                        assistantMessage.appendChild(errorDiv);
                                    }
                                    
                                    // Scroll to bottom as content is added
                                    chatBox.scrollTop = chatBox.scrollHeight;
                                } catch (e) {
                                    console.error('Error parsing SSE data:', e);
                                }
                            }
                        }
                    }

                } catch (error) {
                    console.error('Error:', error);
                    const errorMessage = document.createElement('div');
                    errorMessage.className = 'message assistant error';
                    errorMessage.textContent = 'Error: ' + error.message;
                    chatBox.appendChild(errorMessage);
                    chatBox.scrollTop = chatBox.scrollHeight;
                }
            });

            function copyToClipboard(text) {
                navigator.clipboard.writeText(text).then(() => {
                    alert('Response copied to clipboard!');
                }, (err) => {
                    console.error('Could not copy text: ', err);
                });
            }

            function autoSaveSettings() {
                const formData = new FormData(document.getElementById('settings-form'));
                fetch('/submit', {
                    method: 'POST',
                    body: formData
                })
                .then(response => response.json())
                .then(data => {
                    console.log('Settings saved:', data);
                })
                .catch(error => {
                    console.error('Error saving settings:', error);
                });
            }
        });

        function saveConversation() {
            fetch('/save_conversation', {
                method: 'POST'
            })
            .then(response => {
                if (response.ok) {
                    return response.blob();
                } else {
                    throw new Error('Failed to save conversation history.');
                }
            })
            .then(blob => {
                const url = window.URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.style.display = 'none';
                a.href = url;
                a.download = 'conversation_history.txt';
                document.body.appendChild(a);
                a.click();
                window.URL.revokeObjectURL(url);
            })
            .catch(error => {
                console.error('Error:', error);
                alert(error.message);
            });
        }

        function updateContextCostDisplay(selectedStats) {
            const costDisplay = document.getElementById('context-cost-display');
            const costsSpan = document.getElementById('context-costs');
            
            if (selectedStats.totalTokens > 0) {
                costsSpan.innerHTML = `
                    <div class="mt-1">
                        <div>Tokens: ${selectedStats.totalTokens.toLocaleString()}</div>
                        <div>Claude 3.5: $${selectedStats.costs.claude.toFixed(4)}</div>
                        <div>GPT-4: $${selectedStats.costs.gpt4.toFixed(4)}</div>
                        <div>GPT-4 Mini: $${selectedStats.costs.gpt4mini.toFixed(4)}</div>
                    </div>
                `;
                costDisplay.style.display = 'block';
            } else {
                costDisplay.style.display = 'none';
            }
        }

        fileTreeViewer.setSelectionChangeCallback((selectedFiles) => {
            const selectedStats = fileTreeViewer.getSelectedStats();
            updateContextCostDisplay(selectedStats);
        });
    </script>
</body>
</html>
==================================================

File: c:\GH\ras-commander\library_assistant\web\__pycache__\routes.cpython-311.pyc
==================================================

    !o+gu.                     L   d Z ddlmZmZmZmZ ddlmZmZm	Z	m
Z
 ddlmZ ddlmZ ddlmZmZ ddlmZmZmZ ddlmZmZ dd	lmZmZ dd
lmZ ddlmZm Z  ddl!m"Z"m#Z# ddl$Z$ddl%Z%ddl&Z&ddl'm(Z( ddl)Z) e            Z* ed          Z+ ed          Z,e*-                    de          defd            Z.e*/                    d          dede0fd            Z1e*/                    d           ed           ed           ed           ed           ed           ed          fdede2de2de2de2de3de3fd             Z4e*/                    d!          d"             Z5e*-                    d#          d$             Z6e*-                    d%          d&e2fd'            Z7dS )(a  
Web routes for the Library Assistant.

This module defines the FastAPI routes for the web interface of the Library Assistant.
It handles user interactions, API calls, and serves the HTML template.

Routes:
- GET /: Serves the main page of the application.
- POST /chat: Handles chat interactions with the AI model.
- POST /submit: Handles form submissions for updating settings.
- POST /save_conversation: Saves the current conversation history to a file.
- GET /get_file_tree: Returns the file tree structure with token counts.
    )	APIRouterRequestFormHTTPException)HTMLResponseJSONResponseFileResponseStreamingResponse)Jinja2Templates)StaticFiles)load_settingsupdate_settings)add_to_historyget_full_conversationsave_conversation)prepare_full_promptupdate_conversation_history)create_pricing_dfestimate_cost)set_context_folder)anthropic_stream_responseget_anthropic_client)openai_stream_responseget_openai_clientN)Pathzweb/templates)	directoryz
web/static/)response_classrequestc                 ^   K   t                      }t                              d| |d          S )z
    Serves the main page of the application.

    Args:
        request (Request): The incoming request object.

    Returns:
        TemplateResponse: The rendered HTML template with current settings.
    z
index.htmlr   settings)r   	templatesTemplateResponser!   s     3c:\GH\ras-commander\library_assistant\web\routes.py	read_rootr&   (   s;       H%%l5 5       z/chatmessagec                 &  K   	 |                     d          }|st          dd          |                     dg           }t          d|           t                      j        |                     dt          t          j                                        t          |j	        dk    r|nd	          t          d|           t          j        d
          }t          |                                        }d}t                    }|d         }|d         t!          |||          fd}	t#           |	            ddddd          S # t$          $ r&}
t          ddt          |
                     d	}
~
ww xY w)zV
    Handles chat interactions with the AI model, supporting streaming responses.
    r(     zNo message provided.status_codedetailselectedFilesuserconversation_idfull_contextNgpt-3.5-turboi  
pricing_dfproviderc                   K   g } 	 
dk    r`t          j                  }t          |	          2 3 d{V }|                     |           dt	          j        d|i           dW V  :6 n
dk    rkt          j                  }ddd	d
	d	g}t          ||          2 3 d{V }|                     |           dt	          j        d|i           dW V  :6 nt          dd          d
                    |           }t          d|           t          d|           dt	          j        
d           dW V  dS # t          $ r:}dt          |           }dt	          j        d|i           dW V  Y d}~dS d}~ww xY w)z'Generator for streaming the AI response	anthropicNzdata: chunkz

openaisystemzYou are a helpful AI assistant.)rolecontentr/   r*   zUnsupported provider selected.r+    	assistant)costr4   zError during streaming: error)r   anthropic_api_keyr   appendjsondumpsr   openai_api_keyr   r   joinr   r   	Exceptionstr)accumulated_responseclientr7   messagescomplete_responsee	error_msgr0   estimated_costfull_promptr4   selected_modelr"   s          r%   stream_responsezchat.<locals>.stream_responseg   si     #% F{**1(2LMMF'@'U'U J J J J J J Je,33E:::Itz7E2B'C'CIIIIIIII (V'U )).x/FGGF!)6WXX!'K@@ H (>fnV^'_'_ J J J J J J Je,33E:::Itz7E2B'C'CIIIIIIII (`'_ (C@`aaaa %'GG,@$A$A!{,=>>> ,O[J[\\\ `tz>x*X*XYY_________ F F F?s1vv??	Etz7I*>??EEEEEEEEEEEEEFs/   *E A+A+E "C(BE 
F	/FF	ztext/event-streamzno-cachez
keep-aliveno)zCache-Control
ConnectionzX-Accel-Buffering)
media_typeheaders  zAn error occurred: )getr   r   r   rP   rG   uuiduuid4r   context_moder   tiktokenencoding_for_modellenencoder   r   r
   rF   )r   r(   user_messageselected_filesencinput_tokensoutput_tokenspricing_infor3   rQ   rL   r0   rN   rO   r4   rP   r"   s              @@@@@@r%   chatre   9   s      
XT{{9-- 	PC8NOOOO !_b99 	v|,,, !??!0 "++&7TZ\\9J9JKK *&3~EENN4
 
 	$OV\JJJ )/::3::k2233 )88!,/

+&|]JOO"	F "	F "	F "	F "	F "	F "	F "	F "	F "	FH !O*!+*%) 
 
 
 	
  T T T4R#a&&4R4RSSSSTs   EE   
F*!FFz/submitr@   rD   rP   rZ   initial_chunk_sizefollowup_chunk_sizec                    K   i }|||d<   |||d<   |||d<   |||d<   |||d<   |||d<   t          |           t          dd	i          S )
a  
    Handles form submissions for updating settings.

    This function updates the application settings based on form data submitted by the user.

    Args:
        request (Request): The incoming request object.
        anthropic_api_key (str, optional): The Anthropic API key.
        openai_api_key (str, optional): The OpenAI API key.
        selected_model (str, optional): The selected AI model.
        context_mode (str, optional): The context handling mode.
        initial_chunk_size (int, optional): The initial chunk size for RAG mode.
        followup_chunk_size (int, optional): The followup chunk size for RAG mode.

    Returns:
        JSONResponse: A JSON object indicating the success status of the update.
    Nr@   rD   rP   rZ   rf   rg   statussuccess)r   r   )r   r@   rD   rP   rZ   rf   rg   updated_datas           r%   handle_submitrl      s      6 L$,=()!)7%&!)7%&'3^$%-?)*&.A*+L!!!9-...r'   z/save_conversationc                     K   	 t                      } t          | t          j                            |                     S # t
          $ r&}t          ddt          |                     d}~ww xY w)aL  
    Saves the current conversation history to a file.

    This function triggers the saving of the conversation history and returns the file
    for download.

    Returns:
        FileResponse: The saved conversation history file for download.

    Raises:
        HTTPException: If there's an error saving the conversation.
    )filenamerV   zFailed to save conversation: r+   N)r   r	   ospathbasenamerF   r   rG   )	file_pathrL   s     r%   save_conversation_endpointrs      s      ^%''	I0@0@0K0KLLLL ^ ^ ^4\TWXYTZTZ4\4\]]]]^s   ;A   
A0
!A++A0z/get_file_treec                    K   	 t          t                    j        j        j        t          j        d          fdfd           } d| iS # t
          $ r&}t          ddt          |                     d}~ww xY w)	z,Get the file tree structure for the project.r2   c                 H    t                              |                     S )N)r]   r^   )r;   ra   s    r%   count_tokensz#get_file_tree.<locals>.count_tokens   s    szz'**+++r'   c                    | j         dk    rdS | j         t          |                                         |                                 rdndd}|                                 rg }d}| dz  }|                                r; 	|          }|r.|                    |           ||                    dd          z  }|                                 D ]H}|j         dk    r; 	|          }|r.|                    |           ||                    dd          z  }It          |d	 
          |d<   ||d<   nB	 | 	                    d          } 
|          |d<   n# t          t          f$ r d|d<   Y nw xY w|S )z*Recursively build the file tree structure.__pycache__Nr   file)namerp   typer   z.cursorrulestokensc                 *    | d         dk    | d         fS )Nr{   ry   rz    )xs    r%   <lambda>z3get_file_tree.<locals>.build_tree.<locals>.<lambda>   s    1V9PVCVXYZ`XaBb r'   )keychildrenutf-8encoding)rz   rG   relative_tois_direxistsrA   rW   iterdirsorted	read_textUnicodeDecodeErrorOSError)rp   itemr   total_tokenscursorrulescursorrules_itemchild
child_itemr;   
build_treerv   root_dirs            r%   r   z!get_file_tree.<locals>.build_tree   s   yM))t 	D,,X6677'+{{}}@& D {{}} ' "^3%%'' J'1z+'>'>$' J (8999$(8(<(<Xq(I(II!\\^^ H HEz^33%/Z%6%6
% H$OOJ777(JNN8Q,G,GGL#)(8b8b#c#c#cZ !-X'"nngn>>G%1\'%:%:DNN*G4 ' ' '%&DNNN' Ks   5$E E32E3fileTreerV   zError building file tree: r+   N)r   __file__parentr[   r\   rF   r   rG   )treerL   r   rv   ra   r   s     @@@@r%   get_file_treer      s      3[>>(/6)/::	, 	, 	, 	, 	,&	 &	 &	 &	 &	 &	 &	P z(##D!! [ [ [4YQTUVQWQW4Y4YZZZZ[s   AA 
B
$!BB
z/get_file_contentrp   c                 B  K   	 t          t                    j        j        j        }|| z  }t          |                                                              t          |                                                    st          dd          |                                st          dd          	 |                    d          }d|iS # t          $ r dd	icY S w xY w# t          $ r  t          $ r&}t          d
dt          |                     d}~ww xY w)z#Get the content of a specific file.i  zAccess deniedr+   i  zFile not foundr   r   r;   z$Binary file - cannot display contentrV   zError reading file: N)r   r   r   rG   resolve
startswithr   r   r   r   rF   )rp   r   rr   r;   rL   s        r%   get_file_contentr     sZ     U>>(/6tO	 9$$&&''223x7G7G7I7I3J3JKK 	ICHHHH!! 	JC8HIIII	G))7);;Gw''! 	G 	G 	GEFFFF	G     U U U4S3q664S4STTTTUs6   B1C$ 6C C!C$  C!!C$ $D8!DD)8__doc__fastapir   r   r   r   fastapi.responsesr   r   r	   r
   fastapi.templatingr   fastapi.staticfilesr   config.configr   r   utils.conversationr   r   r   utils.context_processingr   r   utils.cost_estimationr   r   utils.file_handlingr   api.anthropicr   r   
api.openair   r   rB   r[   ro   pathlibr   rX   routerr#   static_filesrW   r&   postdictre   rG   intrl   rs   r   r   r~   r'   r%   <module>r      sd    < ; ; ; ; ; ; ; ; ; ; ; Y Y Y Y Y Y Y Y Y Y Y Y . . . . . . + + + + + + 8 8 8 8 8 8 8 8 W W W W W W W W W W U U U U U U U U B B B B B B B B 2 2 2 2 2 2 I I I I I I I I @ @ @ @ @ @ @ @   				       	 Oo666	 {\222C--W    .-  W\T \T$ \T \T \T \T| Y "T$ZZ$t**$t**T

"d4jj#tDzz)/ )/)/)/ )/ 	)/
 )/ )/ )/ )/ )/ )/V !""^ ^ #"^& 5[ 5[ 5[n   U U U U ! U U Ur'   
==================================================

File: c:\GH\ras-commander\library_assistant\web\__pycache__\routes.cpython-312.pyc
==================================================

    yg                        d Z ddlmZmZmZmZ ddlmZmZm	Z	m
Z
 ddlmZ ddlmZmZ ddlmZmZmZ ddlmZ ddlmZmZ dd	lmZmZ dd
lmZmZ ddlZddl Z ddl!Z! e       Z" ed      Z#e"jI                  de      defd       Z%e"jM                  d      dede'fd       Z(e"jM                  d       ed       ed       ed       ed       ed       ed      fdede)de)de)de)de*de*fd       Z+e"jM                  d      d        Z,y)a  
Web routes for the Library Assistant.

This module defines the FastAPI routes for the web interface of the Library Assistant.
It handles user interactions, API calls, and serves the HTML template.

Routes:
- GET /: Serves the main page of the application.
- POST /chat: Handles chat interactions with the AI model.
- POST /submit: Handles form submissions for updating settings.
- POST /save_conversation: Saves the current conversation history to a file.
    )	APIRouterRequestFormHTTPException)HTMLResponseJSONResponseFileResponseStreamingResponse)Jinja2Templates)load_settingsupdate_settings)add_to_historyget_full_conversationsave_conversation)prepare_full_prompt)create_pricing_dfestimate_cost)anthropic_stream_responseget_anthropic_client)openai_stream_responseget_openai_clientNzweb/templates)	directory/)response_classrequestc                 P   K   t               }t        j                  d| |d      S w)z
    Serves the main page of the application.

    Args:
        request (Request): The incoming request object.

    Returns:
        TemplateResponse: The rendered HTML template with current settings.
    z
index.htmlr   settings)r   	templatesTemplateResponser   s     3c:\GH\ras-commander\library_assistant\web\routes.py	read_rootr"       s/      H%%l5  s   $&z/chatmessagec                   K   |j                  d      }|st        dd      	 t        d|       t               j                  t        |      t        j                  d      }t        |j                              }d}t              }|d   }|d	   t        |||      }fd
}	t         |	       d      S # t        $ r}
t        ddt        |
             d}
~
ww xY ww)a  
    Handles chat interactions with the AI model.

    This function processes user messages, prepares the context, sends the query to the
    appropriate AI model, and returns the response along with cost estimates.

    Args:
        request (Request): The incoming request object.
        message (dict): A dictionary containing the user's message.

    Returns:
        JSONResponse: A JSON object containing the AI's response and cost estimate.

    Raises:
        HTTPException: If there's an error processing the request.
    r#     zNo message provided.status_codedetailuserzgpt-3.5-turboi  
pricing_dfproviderc                   K   dk(  rKj                   } t        |       }t        |      2 3 d {   }dt        j                  d|i       d (dk(  rVj
                  }t        |      }dddd	dg}t        ||      2 3 d {   }dt        j                  d|i       d (t        d
d      7 6 y 7 46 y w)N	anthropiczdata: chunkz

openaisystemzYou are a helpful AI assistant.)rolecontentr)   r%   zUnsupported provider selected.r&   )	anthropic_api_keyr   r   jsondumpsopenai_api_keyr   r   r   )	r3   clientr.   r6   messagesfull_promptr+   selected_modelr   s	        r!   response_generatorz chat.<locals>.response_generator_   s     ;&$,$>$>!-.?@#<V[#Q F F%"4::w.>#?"@EEX%!)!8!8*>:%2ST#< $:&.RZ#[ F F%"4::w.>#?"@EE#<\]]F#QF#[sJ   )CB>B<B>ACCC C-C<B>>C CCztext/event-stream)
media_type  zAn error occurred: N)getr   r   r   r:   r   tiktokenencoding_for_modellenencoder   r   r
   	Exceptionstr)r   r#   user_messageencinput_tokensoutput_tokenspricing_infor*   estimated_costr;   er9   r+   r:   r   s              @@@@r!   chatrL   1   s     $ ;;y)L4JKK,Tv|, !?!00 *,7 ))/:3::k23 )8!,/

+&|]JO	^$ !!3!5BUVV T6I#a&4RSSTs)   !C,BC C,	C)C$$C))C,z/submitr3   r6   r:   context_modeinitial_chunk_sizefollowup_chunk_sizec                    K   i }|||d<   |||d<   |||d<   |||d<   |||d<   |||d<   t        |       t        ddi      S w)	a  
    Handles form submissions for updating settings.

    This function updates the application settings based on form data submitted by the user.

    Args:
        request (Request): The incoming request object.
        anthropic_api_key (str, optional): The Anthropic API key.
        openai_api_key (str, optional): The OpenAI API key.
        selected_model (str, optional): The selected AI model.
        context_mode (str, optional): The context handling mode.
        initial_chunk_size (int, optional): The initial chunk size for RAG mode.
        followup_chunk_size (int, optional): The followup chunk size for RAG mode.

    Returns:
        JSONResponse: A JSON object indicating the success status of the update.
    r3   r6   r:   rM   rN   rO   statussuccess)r   r   )r   r3   r6   r:   rM   rN   rO   updated_datas           r!   handle_submitrT   u   s     6 L$,=()!)7%&!)7%&'3^$%-?)*&.A*+L!9-..s   AAz/save_conversationc                     K   	 t               } t        | t        j                  j	                  |             S # t
        $ r}t        ddt        |             d}~ww xY ww)aL  
    Saves the current conversation history to a file.

    This function triggers the saving of the conversation history and returns the file
    for download.

    Returns:
        FileResponse: The saved conversation history file for download.

    Raises:
        HTTPException: If there's an error saving the conversation.
    )filenamer=   zFailed to save conversation: r&   N)r   r	   ospathbasenamerC   r   rD   )	file_pathrK   s     r!   save_conversation_endpointr[      s^     ^%'	I0@0@0KLL ^6STWXYTZS[4\]]^s%   A"38 A"	AAAA")-__doc__fastapir   r   r   r   fastapi.responsesr   r   r	   r
   fastapi.templatingr   config.configr   r   utils.conversationr   r   r   utils.context_processingr   utils.cost_estimationr   r   api.anthropicr   r   
api.openair   r   r4   r?   rW   routerr   r>   r"   postdictrL   rD   intrT   r[        r!   <module>rl      sR   < ; Y Y . 8 W W 8 B I @   		 o6	C-W  .  WAT AT$ AT ATF Y "$Zt*t*T
"4j#Dz)/)/)/ )/ 	)/
 )/ )/ )/ )/V !"^ #^rk   
==================================================

