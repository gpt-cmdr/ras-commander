Project Structure (files included):
├── AGENTS.md
├── Decorators.py
├── HdfBase.py
├── HdfBndry.py
├── HdfFluvialPluvial.py
├── HdfHydraulicTables.py
├── HdfInfiltration.py
├── HdfMesh.py
├── HdfPipe.py
├── HdfPlan.py
├── HdfPlot.py
├── HdfPump.py
├── HdfResultsBreach.py
├── HdfResultsMesh.py
├── HdfResultsPlan.py
├── HdfResultsPlot.py
├── HdfResultsXsec.py
├── HdfStruc.py
├── HdfUtils.py
├── HdfXsec.py
├── LoggingConfig.py
├── RasBreach.py
├── RasCmdr.py
├── RasControl.py
├── RasDss.py
├── RasExamples.py
├── RasGeo.py
├── RasGeometry.py
├── RasGeometryUtils.py
├── RasGuiAutomation.py
├── RasMap.py
├── RasPlan.py
├── RasPrj.py
├── RasUnsteady.py
├── RasUtils.py
├── __init__.py
└── _hec_monolith.py

File: c:\GH\ras-commander\ras_commander\AGENTS.md
==================================================
**Scope**
- Guidance for agents working inside `ras_commander/` (the core library). Inherits root policies; adds coding and API usage specifics.

**Module Layout (key classes)**
- `RasPrj` (`RasPrj.py`): Initialize and manage a RAS project. Exposes dataframes: `plan_df`, `geom_df`, `flow_df`, `unsteady_df`, `boundaries_df`. Helpers: `init_ras_project()`, `get_ras_exe()`.
- `RasPlan` (`RasPlan.py`): Clone/retarget plans, update intervals, cores, run flags, titles/descriptions, geometry/unsteady bindings.
- `RasCmdr` (`RasCmdr.py`): Execute plans (single/sequential/parallel). `compute_plan()`, `compute_parallel()`, `compute_test_mode()`.
- `RasControl` (`RasControl.py`): Legacy HEC-RAS support (3.x-4.x) via COM interface. ras-commander style API with plan numbers. `run_plan()`, `get_steady_results()`, `get_unsteady_results()`, `get_output_times()`, `set_current_plan()`.
- `RasMap` (`RasMap.py`): Parse `.rasmap`, post-process stored maps.
- `Hdf*` modules: Geometry and results accessors.
  - `HdfBase`, `HdfUtils`: shared helpers.
  - `HdfMesh`, `HdfBndry`, `HdfXsec`, `HdfStruc`, `HdfPlan`.
  - `HdfResultsMesh`, `HdfResultsPlan`, `HdfResultsXsec`, `HdfResultsPlot`.
  - `HdfPipe`, `HdfPump`, `HdfInfiltration`, `HdfFluvialPluvial`.
  - `HdfPlot`: convenience plotting helpers.

**Conventions**
- Static namespaces: Many classes expose only `@staticmethod`s. Do not instantiate unless design requires state.
- Plan numbers: Use two digits (e.g., "01"). Helpers accept both strings and paths.
- Logging: `from ras_commander import get_logger, log_call`; then `logger = get_logger(__name__)`; decorate public methods with `@log_call`.
- Imports: stdlib → third‑party → local; keep ≤79 chars where practical.

**Input Normalization (standardize_input)**
- Most HDF-facing functions are decorated with `@standardize_input(file_type=...)` and accept:
  - An `h5py.File`, a `Path`, a string path, or a plan/geom number (e.g., "01", "p01").
  - A `ras_object` kwarg to disambiguate when multiple projects are active.
- `file_type='plan_hdf'` or `'geom_hdf'` resolves plan/geometry HDF paths via the active `RasPrj`.
- Functions may also accept `hdf_file` as the first arg to work directly with an open `h5py.File`.

**Common Recipes**
- Initialize and compute:
  - `from ras_commander import init_ras_project, RasCmdr`
  - `init_ras_project(<project_folder>, <path_to_Ras.exe>)`
  - `RasCmdr.compute_plan("01", dest_folder="working/run01", overwrite_dest=True)`
- 2D mesh basics:
  - `from ras_commander import HdfMesh, HdfResultsMesh`
  - `cells = HdfMesh.get_mesh_cell_polygons("06")`
  - `faces = HdfMesh.get_mesh_cell_faces("06")`
  - `ts = HdfResultsMesh.get_mesh_timeseries("06", variables=["Water Surface"] )`
- 1D cross sections:
  - `from ras_commander import HdfXsec, HdfResultsXsec`
  - `xsecs = HdfXsec.get_cross_sections("01")`
  - `xs_ts = HdfResultsXsec.get_xsec_timeseries("01", river="...")`
- Pipes and pumps (HEC-RAS 6.6+):
  - `from ras_commander import HdfPipe, HdfPump`
  - `pipes = HdfPipe.get_pipe_conduits("02")`
  - `pumps = HdfPump.get_pump_stations("02")`
- Legacy version extraction (HEC-RAS 3.x-4.x):
  - `from ras_commander import init_ras_project, RasControl`
  - `init_ras_project(path, "4.1")  # Specify version`
  - `RasControl.run_plan("02")  # Use plan numbers`
  - `df_steady = RasControl.get_steady_results("02")`
  - `df_unsteady = RasControl.get_unsteady_results("01", max_times=10)`
  - Note: Uses plan numbers like HDF methods; automatically closes HEC-RAS to prevent conflicts

**Face/Cell Utilities (from examples, not library)**
- The examples include a notebook-only helper `find_nearest_cell_face(point, cell_faces_df)` that computes nearest faces to a point and plots selections. It is not part of the library API; port into scripts as needed.

**Performance & Execution**
- `RasCmdr.compute_plan(..., num_cores=N)`: choose modest N (2–8) unless models benefit from higher parallelism.
- `RasCmdr.compute_parallel(...)`: multiply `max_workers * num_cores` conservatively relative to physical cores/RAM.
- Use `dest_folder` or temporary copies to keep originals immutable.

**Testing & Examples**
- Prefer `ras_commander.RasExamples` to extract official models into a writable location. Avoid committing extracted content.
- See `examples/AGENTS.md` for a task-oriented index of notebooks and unique snippets.

**Out of Scope for Agents**
- `ai_tools/` knowledge base generation. Do not read or run those scripts; they are maintainer-only.


==================================================

File: c:\GH\ras-commander\ras_commander\Decorators.py
==================================================
from functools import wraps
from pathlib import Path
from typing import Union
import logging
import h5py
import inspect
import pandas as pd
from numbers import Number


def log_call(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        logger.debug(f"Calling {func.__name__}")
        result = func(*args, **kwargs)
        logger.debug(f"Finished {func.__name__}")
        return result
    return wrapper

def standardize_input(file_type: str = 'plan_hdf'):
    """
    Decorator to standardize input for HDF file operations.

    This decorator processes various input types and converts them to a Path object
    pointing to the correct HDF file. It handles the following input types:
    - h5py.File objects
    - pathlib.Path objects
    - Strings (file paths or plan/geom numbers)
    - Integers (interpreted as plan/geom numbers)

    The decorator also manages RAS object references and logging.

    Args:
        file_type (str): Specifies whether to look for 'plan_hdf' or 'geom_hdf' files.

    Returns:
        A decorator that wraps the function to standardize its input to a Path object.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(func.__module__)
            
            # Check if the function expects an hdf_path parameter
            sig = inspect.signature(func)
            param_names = list(sig.parameters.keys())
            
            # If first parameter is 'hdf_file', pass an h5py object
            if param_names and param_names[0] == 'hdf_file':
                if isinstance(args[0], h5py.File):
                    return func(*args, **kwargs)
                elif isinstance(args[0], (str, Path)):
                    with h5py.File(args[0], 'r') as hdf:
                        return func(hdf, *args[1:], **kwargs)
                else:
                    raise ValueError(f"Expected h5py.File or path, got {type(args[0])}")
                
            # Handle both static method calls and regular function calls
            if args and isinstance(args[0], type):
                # Static method call, remove the class argument
                args = args[1:]
            
            # Get hdf_input from kwargs if provided with hdf_path key, or take first positional arg
            hdf_input = kwargs.pop('hdf_path', None) if 'hdf_path' in kwargs else (args[0] if args else None)
            
            # Import ras here to ensure we get the most current instance
            from .RasPrj import ras as ras
            # ras_object is always keyword-only, never in args
            ras_object = kwargs.pop('ras_object', None)
            ras_obj = ras_object or ras

            # If no hdf_input provided, return the function unmodified
            if hdf_input is None:
                return func(*args, **kwargs)

            hdf_path = None

            # Clean and normalize string inputs
            if isinstance(hdf_input, str):
                # Clean the string (remove extra whitespace, normalize path separators)
                hdf_input = hdf_input.strip()
                
                # Check if it's a raw file path that exists
                try:
                    test_path = Path(hdf_input)
                    if test_path.is_file():
                        hdf_path = test_path
                        logger.info(f"Using HDF file from direct string path: {hdf_path}")
                except Exception as e:
                    logger.debug(f"Error converting string to path: {str(e)}")

            # If a valid path wasn't created from string processing, continue with normal flow
            if hdf_path is None:
                # If hdf_input is already a Path and exists, use it directly
                if isinstance(hdf_input, Path) and hdf_input.is_file():
                    hdf_path = hdf_input
                    logger.info(f"Using existing Path object HDF file: {hdf_path}")
                # If hdf_input is an h5py.File object, use its filename
                elif isinstance(hdf_input, h5py.File):
                    hdf_path = Path(hdf_input.filename)
                    logger.info(f"Using HDF file from h5py.File object: {hdf_path}")
                # Handle Path objects that might not be verified yet
                elif isinstance(hdf_input, Path):
                    if hdf_input.is_file():
                        hdf_path = hdf_input
                        logger.info(f"Using verified Path object HDF file: {hdf_path}")
                # Handle string inputs that are plan/geom numbers
                elif isinstance(hdf_input, str) and (hdf_input.isdigit() or (len(hdf_input) > 1 and hdf_input[0] == 'p' and hdf_input[1:].isdigit())):
                    try:
                        ras_obj.check_initialized()
                    except Exception as e:
                        raise ValueError(f"RAS object is not initialized: {str(e)}")

                    # Extract the number part and strip leading zeros
                    number_str = hdf_input if hdf_input.isdigit() else hdf_input[1:]
                    stripped_number = number_str.lstrip('0')
                    if stripped_number == '':  # Handle case where input was '0' or '00'
                        stripped_number = '0'

                    # Convert to integer and validate range
                    try:
                        number_int = int(stripped_number)
                        if not (1 <= number_int <= 99):
                            raise ValueError(f"Plan/geometry number must be between 1 and 99, got {number_int}")
                    except (ValueError, TypeError) as e:
                        raise ValueError(f"Cannot convert plan/geometry number '{hdf_input}' to integer") from e
                    
                    if file_type == 'plan_hdf':
                        try:
                            # Convert plan_number column to integers for comparison after stripping zeros
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].str.lstrip('0').astype(int) == number_int]
                            if not plan_info.empty:
                                # Make sure HDF_Results_Path is a string and not None
                                hdf_path_str = plan_info.iloc[0]['HDF_Results_Path']
                                if pd.notna(hdf_path_str):
                                    hdf_path = Path(str(hdf_path_str))
                        except Exception as e:
                            logger.warning(f"Error retrieving plan HDF path: {str(e)}")

                    elif file_type == 'plan':
                        try:
                            # Get plan file path (.p##)
                            from .RasUtils import RasUtils
                            plan_number_str = RasUtils.normalize_ras_number(number_int)
                            hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number_str}"
                            if not hdf_path.exists():
                                raise FileNotFoundError(f"Plan file not found: {hdf_path}")
                        except Exception as e:
                            logger.warning(f"Error retrieving plan file path: {str(e)}")

                    elif file_type == 'geom_hdf':
                        try:
                            # First try to get the geometry number from the plan
                            from ras_commander import RasPlan
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].astype(int) == number_int]
                            if not plan_info.empty:
                                # Extract the geometry number from the plan
                                geom_number = plan_info.iloc[0]['geometry_number']
                                if pd.notna(geom_number) and geom_number is not None:
                                    # Handle different types of geom_number (string or int)
                                    try:
                                        # Get the geometry path using RasPlan
                                        geom_path = RasPlan.get_geom_path(str(geom_number), ras_obj)

                                        if geom_path is not None:
                                            # Create the HDF path by adding .hdf to the geometry path
                                            hdf_path = Path(str(geom_path) + ".hdf")
                                            if hdf_path.exists():
                                                logger.info(f"Found geometry HDF file for plan {number_int}: {hdf_path}")
                                            else:
                                                # Try to find it in the geom_df if direct path doesn't exist
                                                geom_info = ras_obj.geom_df[ras_obj.geom_df['full_path'] == str(geom_path)]
                                                if not geom_info.empty and 'hdf_path' in geom_info.columns:
                                                    hdf_path_str = geom_info.iloc[0]['hdf_path']
                                                    if pd.notna(hdf_path_str):
                                                        hdf_path = Path(str(hdf_path_str))
                                                        logger.info(f"Found geometry HDF file from geom_df for plan {number_int}: {hdf_path}")
                                    except (TypeError, ValueError) as e:
                                        logger.warning(f"Error processing geometry number {geom_number}: {str(e)}")
                                else:
                                    logger.warning(f"No valid geometry number found for plan {number_int}")
                        except Exception as e:
                            logger.warning(f"Error retrieving geometry HDF path: {str(e)}")
                    else:
                        raise ValueError(f"Invalid file type: {file_type}")
                    



                # Handle numeric inputs (int, float, numpy types, etc. - assuming they're plan or geom numbers)
                elif isinstance(hdf_input, Number):
                    try:
                        ras_obj.check_initialized()
                    except Exception as e:
                        raise ValueError(f"RAS object is not initialized: {str(e)}")

                    # Convert to integer and validate range
                    try:
                        number_int = int(hdf_input)
                        if not (1 <= number_int <= 99):
                            raise ValueError(f"Plan/geometry number must be between 1 and 99, got {number_int}")
                    except (ValueError, TypeError) as e:
                        raise ValueError(f"Cannot convert plan/geometry number to integer: {hdf_input}") from e

                    if file_type == 'plan_hdf':
                        try:
                            # Convert plan_number column to integers for comparison after stripping zeros
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].str.lstrip('0').astype(int) == number_int]
                            if not plan_info.empty:
                                # Make sure HDF_Results_Path is a string and not None
                                hdf_path_str = plan_info.iloc[0]['HDF_Results_Path']
                                if pd.notna(hdf_path_str):
                                    hdf_path = Path(str(hdf_path_str))
                        except Exception as e:
                            logger.warning(f"Error retrieving plan HDF path: {str(e)}")

                    elif file_type == 'plan':
                        try:
                            # Get plan file path (.p##)
                            from .RasUtils import RasUtils
                            plan_number_str = RasUtils.normalize_ras_number(number_int)
                            hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number_str}"
                            if not hdf_path.exists():
                                raise FileNotFoundError(f"Plan file not found: {hdf_path}")
                        except Exception as e:
                            logger.warning(f"Error retrieving plan file path: {str(e)}")

                    elif file_type == 'geom_hdf':
                        try:
                            # First try finding plan info to get geometry number
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].astype(int) == number_int]
                            if not plan_info.empty:
                                # Extract the geometry number from the plan
                                geom_number = plan_info.iloc[0]['geometry_number']
                                if pd.notna(geom_number) and geom_number is not None:
                                    # Handle different types of geom_number (string or int)
                                    try:
                                        # Get the geometry path using RasPlan
                                        from ras_commander import RasPlan
                                        geom_path = RasPlan.get_geom_path(str(geom_number), ras_obj)

                                        if geom_path is not None:
                                            # Create the HDF path by adding .hdf to the geometry path
                                            hdf_path = Path(str(geom_path) + ".hdf")
                                            if hdf_path.exists():
                                                logger.info(f"Found geometry HDF file for plan {number_int}: {hdf_path}")
                                            else:
                                                # Try to find it in the geom_df if direct path doesn't exist
                                                geom_info = ras_obj.geom_df[ras_obj.geom_df['full_path'] == str(geom_path)]
                                                if not geom_info.empty and 'hdf_path' in geom_info.columns:
                                                    hdf_path_str = geom_info.iloc[0]['hdf_path']
                                                    if pd.notna(hdf_path_str):
                                                        hdf_path = Path(str(hdf_path_str))
                                                        logger.info(f"Found geometry HDF file from geom_df for plan {number_int}: {hdf_path}")
                                    except (TypeError, ValueError) as e:
                                        logger.warning(f"Error processing geometry number {geom_number}: {str(e)}")
                                else:
                                    logger.warning(f"No valid geometry number found for plan {number_int}")
                        except Exception as e:
                            logger.warning(f"Error retrieving geometry HDF path: {str(e)}")
                    else:
                        raise ValueError(f"Invalid file type: {file_type}")

            # Final verification that the path exists
            if hdf_path is None or not hdf_path.exists():
                file_type_name = "HDF file" if 'hdf' in file_type else "file"
                error_msg = f"{file_type_name} not found: {hdf_input}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)

            logger.info(f"Final validated file path: {hdf_path}")

            # Validate HDF file structure (only for HDF types)
            if 'hdf' in file_type:
                try:
                    with h5py.File(hdf_path, 'r') as test_file:
                        # Just open to verify it's a valid HDF5 file
                        logger.debug(f"Successfully opened HDF file for validation: {hdf_path}")
                except Exception as e:
                    logger.warning(f"Warning: Could not validate HDF file: {str(e)}")
                    # Continue anyway, let the function handle detailed validation
            
            # Pass all original arguments and keywords, replacing hdf_input with standardized hdf_path
            # If the original input was positional, replace the first argument
            if args and 'hdf_path' not in kwargs:
                new_args = (hdf_path,) + args[1:]
            else:
                new_args = args
                kwargs['hdf_path'] = hdf_path
                
            return func(*new_args, **kwargs)

        return wrapper
    return decorator
==================================================

File: c:\GH\ras-commander\ras_commander\HdfBase.py
==================================================
"""
HdfBase: Core HDF File Operations for HEC-RAS

This module provides fundamental methods for interacting with HEC-RAS HDF files.
It serves as a foundation for more specialized HDF classes.

Attribution:
    Derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    Copyright (c) 2024 fema-ffrd - MIT License

Features:
    - Time parsing and conversion utilities
    - HDF attribute and dataset access
    - Geometric data extraction
    - 2D flow area information retrieval

Classes:
    HdfBase: Base class containing static methods for HDF operations

Key Methods:
    Time Operations:
        - get_simulation_start_time(): Get simulation start datetime
        - get_unsteady_timestamps(): Get unsteady output timestamps
        - parse_ras_datetime(): Parse RAS datetime strings
    
    Data Access:
        - get_2d_flow_area_names_and_counts(): Get 2D flow area info
        - get_projection(): Get spatial projection
        - get_attrs(): Access HDF attributes
        - get_dataset_info(): Explore HDF structure
        - get_polylines_from_parts(): Extract geometric polylines

Example:
    ```python
    from ras_commander import HdfBase
    
    with h5py.File('model.hdf', 'r') as hdf:
        start_time = HdfBase.get_simulation_start_time(hdf)
        timestamps = HdfBase.get_unsteady_timestamps(hdf)
    ```
"""
import re
from datetime import datetime, timedelta
import h5py
import numpy as np
import pandas as pd
import xarray as xr
from typing import List, Tuple, Union, Optional, Dict, Any
from pathlib import Path
import logging
from shapely.geometry import LineString, MultiLineString

from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfBase:
    """
    Base class for HEC-RAS HDF file operations.

    This class provides static methods for fundamental HDF file operations,
    including time parsing, attribute access, and geometric data extraction.
    All methods are designed to work with h5py.File objects or pathlib.Path
    inputs.

    Note:
        This class is not meant to be instantiated. All methods are static
        and should be called directly from the class.
    """

    @staticmethod
    def get_simulation_start_time(hdf_file: h5py.File) -> datetime:
        """
        Extract the simulation start time from the HDF file.

        Args:
            hdf_file: Open HDF file object containing RAS simulation data.

        Returns:
            datetime: Simulation start time as a datetime object.

        Raises:
            ValueError: If Plan Information is not found or start time cannot be parsed.
        
        Note:
            Expects 'Plan Data/Plan Information' group with 'Simulation Start Time' attribute.
        """
        plan_info = hdf_file.get("Plan Data/Plan Information")
        if plan_info is None:
            raise ValueError("Plan Information not found in HDF file")
        time_str = plan_info.attrs.get('Simulation Start Time')
        return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))

    @staticmethod
    def get_unsteady_timestamps(hdf_file: h5py.File) -> List[datetime]:
        """
        Extract the list of unsteady timestamps from the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.

        Returns:
            List[datetime]: A list of datetime objects representing the unsteady timestamps.
        """
        group_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
        raw_datetimes = hdf_file[group_path][:]
        return [HdfUtils.parse_ras_datetime_ms(x.decode("utf-8")) for x in raw_datetimes]

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_2d_flow_area_names_and_counts(hdf_path: Path) -> List[Tuple[str, int]]:
        """
        Get the names and cell counts of 2D flow areas from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            List[Tuple[str, int]]: A list of tuples containing the name and cell count of each 2D flow area.

        Raises:
            ValueError: If there's an error reading the HDF file or accessing the required data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                flow_area_2d_path = "Geometry/2D Flow Areas"
                if flow_area_2d_path not in hdf_file:
                    return []
                
                attributes = hdf_file[f"{flow_area_2d_path}/Attributes"][()]
                names = [HdfUtils.convert_ras_string(name) for name in attributes["Name"]]
                
                cell_info = hdf_file[f"{flow_area_2d_path}/Cell Info"][()]
                cell_counts = [info[1] for info in cell_info]
                
                return list(zip(names, cell_counts))
        except Exception as e:
            logger.error(f"Error reading 2D flow area names and counts from {hdf_path}: {str(e)}")
            raise ValueError(f"Failed to get 2D flow area names and counts: {str(e)}")


    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_projection(hdf_path: Path) -> Optional[str]:
        """
        Get projection information from HDF file or RASMapper project file.
        Converts WKT projection to EPSG code for GeoDataFrame compatibility.
        
        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            Optional[str]: The projection as EPSG code (e.g. "EPSG:6556"), or None if not found.
        """
        from pyproj import CRS

        project_folder = hdf_path.parent
        wkt = None
        proj_file = None  # Initialize proj_file variable
        
        # Try HDF file
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                proj_wkt = hdf_file.attrs.get("Projection")
                if proj_wkt is not None:
                    if isinstance(proj_wkt, (bytes, np.bytes_)):
                        wkt = proj_wkt.decode("utf-8")
                        logger.info(f"Found projection in HDF file: {hdf_path}")
                        return wkt
        except Exception as e:
            logger.error(f"Error reading projection from HDF file {hdf_path}: {str(e)}")

        # Try RASMapper file if no HDF projection
        if not wkt:
            try:
                rasmap_files = list(project_folder.glob("*.rasmap"))
                if rasmap_files:
                    with open(rasmap_files[0], 'r') as f:
                        content = f.read()
                        
                    proj_match = re.search(r'<RASProjectionFilename Filename="(.*?)"', content)
                    if proj_match:
                        proj_file = project_folder / proj_match.group(1).replace('.\\', '')
                        if proj_file.exists():
                            with open(proj_file, 'r') as f:
                                wkt = f.read().strip()
                                logger.info(f"Found projection in RASMapper file: {proj_file}")
                                return wkt
            except Exception as e:
                logger.error(f"Error reading RASMapper projection file: {str(e)}")
        
        # Customize error message based on whether proj_file was found
        if proj_file:
            error_msg = (
                "No valid projection found. Checked:\n"
                f"1. HDF file projection attribute: {hdf_path}\n"
                f"2. RASMapper projection file {proj_file} found in RASMapper file, but was invalid"
            )
        else:
            error_msg = (
                "No valid projection found. Checked:\n"
                f"1. HDF file projection attribute: {hdf_path}\n was checked and no projection attribute found"
                "2. No RASMapper projection file found"
            )

        error_msg += (
            "\nTo fix this:\n"
            "1. Open RASMapper\n"
            "2. Click Map > Set Projection\n" 
            "3. Select an appropriate projection file or coordinate system\n"
            "4. Save the RASMapper project"
        )
        
        logger.critical(error_msg)
        return None

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_attrs(hdf_file: h5py.File, attr_path: str) -> Dict[str, Any]:
        """
        Get attributes from an HDF file at a specified path.

        Args:
            hdf_file (h5py.File): The opened HDF file.
            attr_path (str): Path to the attributes in the HDF file.

        Returns:
            Dict[str, Any]: Dictionary of attributes.
        """
        try:
            if attr_path not in hdf_file:
                logger.warning(f"Path {attr_path} not found in HDF file")
                return {}
            
            return HdfUtils.convert_hdf5_attrs_to_dict(hdf_file[attr_path].attrs)
        except Exception as e:
            logger.error(f"Error getting attributes from {attr_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_dataset_info(file_path: Path, group_path: str = '/') -> None:
        """
        Recursively explore and print the structure of an HDF5 file.

        Displays detailed information about groups, datasets, and their attributes
        in a hierarchical format.

        Args:
            file_path: Path to the HDF5 file.
            group_path: Starting group path to explore (default: root '/').

        Prints:
            - Group and dataset names with hierarchical indentation
            - Dataset shapes and data types
            - All attributes for groups and datasets
        """
        def recurse(name, obj, indent=0):
            spacer = "    " * indent
            if isinstance(obj, h5py.Group):
                print(f"{spacer}Group: {name}")
                HdfBase.print_attrs(name, obj)
                for key in obj:
                    recurse(f"{name}/{key}", obj[key], indent+1)
            elif isinstance(obj, h5py.Dataset):
                print(f"{spacer}Dataset: {name}")
                print(f"{spacer}    Shape: {obj.shape}")
                print(f"{spacer}    Dtype: {obj.dtype}")
                HdfBase.print_attrs(name, obj)
            else:
                print(f"{spacer}Unknown object: {name}")

        try:
            with h5py.File(file_path, 'r') as hdf_file:
                if group_path in hdf_file:
                    print("")
                    print(f"Exploring group: {group_path}\n")
                    group = hdf_file[group_path]
                    for key in group:
                        print("")
                        recurse(f"{group_path}/{key}", group[key], indent=1)
                else:
                    print(f"Group path '{group_path}' not found in the HDF5 file.")
        except Exception as e:
            print(f"Error exploring HDF5 file: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_polylines_from_parts(hdf_path: Path, path: str, info_name: str = "Polyline Info", 
                              parts_name: str = "Polyline Parts", 
                              points_name: str = "Polyline Points") -> List[LineString]:
        """
        Extract polylines from HDF file parts data.

        Args:
            hdf_path: Path to the HDF file.
            path: Internal HDF path to polyline data.
            info_name: Name of polyline info dataset.
            parts_name: Name of polyline parts dataset.
            points_name: Name of polyline points dataset.

        Returns:
            List of Shapely LineString/MultiLineString geometries.

        Note:
            Expects HDF datasets containing:
            - Polyline information (start points and counts)
            - Parts information for multi-part lines
            - Point coordinates
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                polyline_info_path = f"{path}/{info_name}"
                polyline_parts_path = f"{path}/{parts_name}"
                polyline_points_path = f"{path}/{points_name}"

                polyline_info = hdf_file[polyline_info_path][()]
                polyline_parts = hdf_file[polyline_parts_path][()]
                polyline_points = hdf_file[polyline_points_path][()]

                geoms = []
                for pnt_start, pnt_cnt, part_start, part_cnt in polyline_info:
                    points = polyline_points[pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(LineString(points))
                    else:
                        parts = polyline_parts[part_start : part_start + part_cnt]
                        geoms.append(
                            MultiLineString(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return geoms
        except Exception as e:
            logger.error(f"Error getting polylines: {str(e)}")
            return []

    @staticmethod
    def print_attrs(name: str, obj: Union[h5py.Dataset, h5py.Group]) -> None:
        """
        Print the attributes of an HDF5 object (Dataset or Group).

        Args:
            name (str): Name of the object
            obj (Union[h5py.Dataset, h5py.Group]): HDF5 object whose attributes are to be printed
        """
        if len(obj.attrs) > 0:
            print(f"    Attributes for {name}:")
            for key, value in obj.attrs.items():
                print(f"        {key}: {value}")




==================================================

File: c:\GH\ras-commander\ras_commander\HdfBndry.py
==================================================
"""
Class: HdfBndry

A utility class for extracting and processing boundary-related features from HEC-RAS HDF files,
including boundary conditions, breaklines, refinement regions, and reference features.

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfBndry:
- get_bc_lines()           # Returns boundary condition lines as a GeoDataFrame.
- get_breaklines()         # Returns 2D mesh area breaklines as a GeoDataFrame.
- get_refinement_regions() # Returns refinement regions as a GeoDataFrame.
- get_reference_lines()    # Returns reference lines as a GeoDataFrame.
- get_reference_points()   # Returns reference points as a GeoDataFrame.



"""
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .HdfMesh import HdfMesh
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfBndry:
    """
    A class for handling boundary-related data from HEC-RAS HDF files.

    This class provides methods to extract and process various boundary elements
    such as boundary condition lines, breaklines, refinement regions, and reference
    lines/points from HEC-RAS geometry HDF files.

    Methods in this class return data primarily as GeoDataFrames, making it easy
    to work with spatial data in a geospatial context.

    Note:
        This class relies on the HdfBase and HdfUtils classes for some of its
        functionality. Ensure these classes are available in the same package.
    """
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_bc_lines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area boundary condition lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the boundary condition lines and their attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                bc_lines_path = "Geometry/Boundary Condition Lines"
                if bc_lines_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                # Get geometries
                bc_line_data = hdf_file[bc_lines_path]
                geoms = HdfBase.get_polylines_from_parts(hdf_path, bc_lines_path)
                
                # Get attributes
                attributes = pd.DataFrame(bc_line_data["Attributes"][()])
                
                # Convert string columns
                str_columns = ['Name', 'SA-2D', 'Type']
                for col in str_columns:
                    if col in attributes.columns:
                        attributes[col] = attributes[col].apply(HdfUtils.convert_ras_string)
                
                # Create GeoDataFrame with all attributes
                gdf = gpd.GeoDataFrame(
                    attributes,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_file)
                )
                
                # Add ID column if not present
                if 'bc_line_id' not in gdf.columns:
                    gdf['bc_line_id'] = range(len(gdf))
                    
                return gdf

        except Exception as e:
            logger.error(f"Error reading boundary condition lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_breaklines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area breaklines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the breaklines.

        Notes
        -----
        - Zero-length breaklines are logged and skipped. 
        - Single-point breaklines are logged and skipped.
        - These invalid breaklines should be removed in RASMapper to prevent potential issues.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                breaklines_path = "Geometry/2D Flow Area Break Lines"
                if breaklines_path not in hdf_file:
                    logger.warning(f"Breaklines path '{breaklines_path}' not found in HDF file.")
                    return gpd.GeoDataFrame()

                bl_line_data = hdf_file[breaklines_path]
                attributes = bl_line_data["Attributes"][()]
                
                # Initialize lists to store valid breakline data
                valid_ids = []
                valid_names = []
                valid_geoms = []

                # Track invalid breaklines for summary
                zero_length_count = 0
                single_point_count = 0
                other_error_count = 0

                # Process each breakline
                for idx, (pnt_start, pnt_cnt, part_start, part_cnt) in enumerate(bl_line_data["Polyline Info"][()]):
                    name = HdfUtils.convert_ras_string(attributes["Name"][idx])

                    # Check for zero-length breaklines
                    if pnt_cnt == 0:
                        zero_length_count += 1
                        logger.debug(f"Zero-length breakline found (FID: {idx}, Name: {name})")
                        continue

                    # Check for single-point breaklines
                    if pnt_cnt == 1:
                        single_point_count += 1
                        logger.debug(f"Single-point breakline found (FID: {idx}, Name: {name})")
                        continue

                    try:
                        points = bl_line_data["Polyline Points"][()][pnt_start:pnt_start + pnt_cnt]
                        
                        # Additional validation of points array
                        if len(points) < 2:
                            single_point_count += 1
                            logger.debug(f"Invalid point count in breakline (FID: {idx}, Name: {name})")
                            continue

                        if part_cnt == 1:
                            geom = LineString(points)
                        else:
                            parts = bl_line_data["Polyline Parts"][()][part_start:part_start + part_cnt]
                            geom = MultiLineString([
                                points[part_pnt_start:part_pnt_start + part_pnt_cnt]
                                for part_pnt_start, part_pnt_cnt in parts
                                if part_pnt_cnt > 1  # Skip single-point parts
                            ])
                            # Skip if no valid parts remain
                            if len(geom.geoms) == 0:
                                other_error_count += 1
                                logger.debug(f"No valid parts in multipart breakline (FID: {idx}, Name: {name})")
                                continue

                        valid_ids.append(idx)
                        valid_names.append(name)
                        valid_geoms.append(geom)

                    except Exception as e:
                        other_error_count += 1
                        logger.debug(f"Error processing breakline {idx}: {str(e)}")
                        continue

                # Log summary of invalid breaklines
                total_invalid = zero_length_count + single_point_count + other_error_count
                if total_invalid > 0:
                    logger.info(
                        f"Breakline processing summary:\n"
                        f"- Zero-length breaklines: {zero_length_count}\n"
                        f"- Single-point breaklines: {single_point_count}\n"
                        f"- Other invalid breaklines: {other_error_count}\n"
                        f"Consider removing these invalid breaklines using RASMapper."
                    )

                # Create GeoDataFrame with valid breaklines
                if not valid_ids:
                    logger.warning("No valid breaklines found in the HDF file.")
                    return gpd.GeoDataFrame()

                return gpd.GeoDataFrame(
                    {
                        "bl_id": valid_ids,
                        "Name": valid_names,
                        "geometry": valid_geoms
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading breaklines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_refinement_regions(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area refinement regions.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the refinement regions.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                refinement_regions_path = "/Geometry/2D Flow Area Refinement Regions"
                if refinement_regions_path not in hdf_file:
                    return gpd.GeoDataFrame()
                rr_data = hdf_file[refinement_regions_path]
                rr_ids = range(rr_data["Attributes"][()].shape[0])
                names = np.vectorize(HdfUtils.convert_ras_string)(rr_data["Attributes"][()]["Name"])
                geoms = list()
                for pnt_start, pnt_cnt, part_start, part_cnt in rr_data["Polygon Info"][()]:
                    points = rr_data["Polygon Points"][()][pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(Polygon(points))
                    else:
                        parts = rr_data["Polygon Parts"][()][part_start : part_start + part_cnt]
                        geoms.append(
                            MultiPolygon(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return gpd.GeoDataFrame(
                    {"rr_id": rr_ids, "Name": names, "geometry": geoms},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading refinement regions: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_lines(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference lines geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference lines. If mesh_name is provided,
            returns only lines for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_lines_path = "Geometry/Reference Lines"
                attributes_path = f"{reference_lines_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                attributes = hdf_file[attributes_path][()]
                refline_ids = range(attributes.shape[0])
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA-2D"])
                
                try:
                    types = v_conv_str(attributes["Type"])
                except ValueError:
                    types = np.array([""] * attributes.shape[0])
                
                geoms = HdfBase.get_polylines_from_parts(hdf_path, reference_lines_path)
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refln_id": refline_ids,
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Type": types,
                        "geometry": geoms,
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_points(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference points geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference points. If mesh_name is provided,
            returns only points for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_points_path = "Geometry/Reference Points"
                attributes_path = f"{reference_points_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                ref_points_group = hdf_file[reference_points_path]
                attributes = ref_points_group["Attributes"][:]
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA/2D"])
                cell_id = attributes["Cell Index"]
                points = ref_points_group["Points"][()]
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refpt_id": range(attributes.shape[0]),
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Cell Index": cell_id,
                        "geometry": list(map(Point, points)),
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference points: {str(e)}")
            return gpd.GeoDataFrame()

    

==================================================

File: c:\GH\ras-commander\ras_commander\HdfFluvialPluvial.py
==================================================
"""
Class: HdfFluvialPluvial

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfFluvialPluvial:
- calculate_fluvial_pluvial_boundary(): Returns LineStrings representing the boundary.
- generate_fluvial_pluvial_polygons(): Returns dissolved Polygons for fluvial, pluvial, and ambiguous zones.
- _process_cell_adjacencies()
- _get_boundary_cell_pairs()
- _identify_boundary_edges()

"""

from typing import Dict, List, Tuple, Set, Optional
import pandas as pd
import geopandas as gpd
from collections import defaultdict
from shapely.geometry import LineString, MultiLineString
from tqdm import tqdm
from .HdfMesh import HdfMesh
from .HdfUtils import HdfUtils
from .Decorators import standardize_input
from .HdfResultsMesh import HdfResultsMesh
from .LoggingConfig import get_logger
from pathlib import Path

logger = get_logger(__name__)

class HdfFluvialPluvial:
    """
    A class for analyzing and visualizing fluvial-pluvial boundaries in HEC-RAS 2D model results.

    This class provides methods to process and visualize HEC-RAS 2D model outputs,
    specifically focusing on the delineation of fluvial and pluvial flood areas.
    It includes functionality for calculating fluvial-pluvial boundaries based on
    the timing of maximum water surface elevations.

    Key Concepts:
    - Fluvial flooding: Flooding from rivers/streams
    - Pluvial flooding: Flooding from rainfall/surface water
    - delta_t: Time threshold (in hours) used to distinguish between fluvial and pluvial cells.
               Cells with max WSE time differences greater than delta_t are considered boundaries.

    Data Requirements:
    - HEC-RAS plan HDF file containing:
        - 2D mesh cell geometry (accessed via HdfMesh)
        - Maximum water surface elevation times (accessed via HdfResultsMesh)

    Usage Example:
        >>> from ras_commander import HdfFluvialPluvial
        >>> hdf_path = Path("path/to/plan.hdf")
        
        # To get just the boundary lines
        >>> boundary_lines_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(
        ...     hdf_path, 
        ...     delta_t=12
        ... )
        
        # To get classified flood polygons
        >>> flood_polygons_gdf = HdfFluvialPluvial.generate_fluvial_pluvial_polygons(
        ...     hdf_path,
        ...     delta_t=12,
        ...     temporal_tolerance_hours=1.0
        ... )
    """
    def __init__(self):
        self.logger = get_logger(__name__)  # Initialize logger with module name
    
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def calculate_fluvial_pluvial_boundary(
        hdf_path: Path, 
        delta_t: float = 12,
        min_line_length: Optional[float] = None
    ) -> gpd.GeoDataFrame:
        """
        Calculate the fluvial-pluvial boundary lines based on cell polygons and maximum water surface elevation times.

        This function is useful for visualizing the line of transition between flooding mechanisms.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            delta_t (float): Threshold time difference in hours. Cells with time differences
                             greater than this value are considered boundaries. Default is 12 hours.
            min_line_length (float, optional): Minimum length (in CRS units) for boundary lines to be included.
                                               Lines shorter than this will be dropped. Default is None (no filtering).

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing the fluvial-pluvial boundary lines.
        """
        try:
            logger.info("Getting cell polygons from HDF file...")
            cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(hdf_path)
            if cell_polygons_gdf.empty:
                raise ValueError("No cell polygons found in HDF file")

            logger.info("Getting maximum water surface data from HDF file...")
            max_ws_df = HdfResultsMesh.get_mesh_max_ws(hdf_path)
            if max_ws_df.empty:
                raise ValueError("No maximum water surface data found in HDF file")

            logger.info("Converting maximum water surface timestamps...")
            max_ws_df['maximum_water_surface_time'] = max_ws_df['maximum_water_surface_time'].apply(
                lambda x: HdfUtils.parse_ras_datetime(x) if isinstance(x, str) else x
            )

            logger.info("Processing cell adjacencies...")
            cell_adjacency, common_edges = HdfFluvialPluvial._process_cell_adjacencies(cell_polygons_gdf)
            
            logger.info("Extracting cell times from maximum water surface data...")
            cell_times = max_ws_df.set_index('cell_id')['maximum_water_surface_time'].to_dict()
            
            logger.info("Identifying boundary edges...")
            boundary_edges = HdfFluvialPluvial._identify_boundary_edges(
                cell_adjacency, common_edges, cell_times, delta_t, min_line_length=min_line_length
            )

            logger.info("Creating final GeoDataFrame for boundaries...")
            boundary_gdf = gpd.GeoDataFrame(
                geometry=boundary_edges, 
                crs=cell_polygons_gdf.crs
            )

            logger.info("Boundary line calculation completed successfully.")
            return boundary_gdf

        except Exception as e:
            logger.error(f"Error calculating fluvial-pluvial boundary lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def generate_fluvial_pluvial_polygons(
        hdf_path: Path, 
        delta_t: float = 12, 
        temporal_tolerance_hours: float = 1.0,
        min_polygon_area_acres: Optional[float] = None
    ) -> gpd.GeoDataFrame:
        """
        Generates dissolved polygons representing fluvial, pluvial, and ambiguous flood zones.

        This function classifies each wetted cell and merges them into three distinct regions
        based on the timing of maximum water surface elevation.

        Optionally, for polygons classified as fluvial or pluvial, if their area is less than
        min_polygon_area_acres, they are reclassified to the opposite type and merged with
        adjacent polygons of that type. Ambiguous polygons are exempt from this logic.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            delta_t (float): The time difference (in hours) between adjacent cells that defines
                             the initial boundary between fluvial and pluvial zones. Default is 12.
            temporal_tolerance_hours (float): The maximum time difference (in hours) for a cell
                                              to be considered part of an expanding region. 
                                              Default is 1.0.
            min_polygon_area_acres (float, optional): Minimum polygon area (in acres). For fluvial or pluvial
                                                      polygons smaller than this, reclassify to the opposite
                                                      type and merge with adjacent polygons of that type.
                                                      Ambiguous polygons are not affected.

        Returns:
            gpd.GeoDataFrame: A GeoDataFrame with dissolved polygons for 'fluvial', 'pluvial',
                              and 'ambiguous' zones.
        """
        try:
            # --- 1. Data Loading and Preparation ---
            logger.info("Loading mesh and results data...")
            cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(hdf_path)
            max_ws_df = HdfResultsMesh.get_mesh_max_ws(hdf_path)
            max_ws_df['maximum_water_surface_time'] = max_ws_df['maximum_water_surface_time'].apply(
                lambda x: HdfUtils.parse_ras_datetime(x) if isinstance(x, str) else x
            )
            cell_times = max_ws_df.set_index('cell_id')['maximum_water_surface_time'].to_dict()
            
            logger.info("Processing cell adjacencies...")
            cell_adjacency, _ = HdfFluvialPluvial._process_cell_adjacencies(cell_polygons_gdf)

            # --- 2. Seeding the Classifications ---
            logger.info(f"Identifying initial boundary seeds with delta_t = {delta_t} hours...")
            boundary_pairs = HdfFluvialPluvial._get_boundary_cell_pairs(cell_adjacency, cell_times, delta_t)

            classifications = pd.Series('unclassified', index=cell_polygons_gdf['cell_id'], name='classification')
            
            for cell1, cell2 in boundary_pairs:
                if cell_times.get(cell1) > cell_times.get(cell2):
                    classifications.loc[cell1] = 'fluvial'
                    classifications.loc[cell2] = 'pluvial'
                else:
                    classifications.loc[cell1] = 'pluvial'
                    classifications.loc[cell2] = 'fluvial'
            
            # --- 3. Iterative Region Growth ---
            logger.info(f"Starting iterative region growth with tolerance = {temporal_tolerance_hours} hours...")
            fluvial_frontier = set(classifications[classifications == 'fluvial'].index)
            pluvial_frontier = set(classifications[classifications == 'pluvial'].index)
            
            iteration = 0
            with tqdm(desc="Region Growing", unit="iter") as pbar:
                while fluvial_frontier or pluvial_frontier:
                    iteration += 1
                    
                    next_fluvial_candidates = set()
                    for cell_id in fluvial_frontier:
                        for neighbor_id in cell_adjacency.get(cell_id, []):
                            if classifications.loc[neighbor_id] == 'unclassified' and pd.notna(cell_times.get(neighbor_id)):
                                time_diff_seconds = abs((cell_times[cell_id] - cell_times[neighbor_id]).total_seconds())
                                if time_diff_seconds <= temporal_tolerance_hours * 3600:
                                    next_fluvial_candidates.add(neighbor_id)
                    
                    next_pluvial_candidates = set()
                    for cell_id in pluvial_frontier:
                        for neighbor_id in cell_adjacency.get(cell_id, []):
                            if classifications.loc[neighbor_id] == 'unclassified' and pd.notna(cell_times.get(neighbor_id)):
                                time_diff_seconds = abs((cell_times[cell_id] - cell_times[neighbor_id]).total_seconds())
                                if time_diff_seconds <= temporal_tolerance_hours * 3600:
                                    next_pluvial_candidates.add(neighbor_id)
                    
                    # Resolve conflicts
                    ambiguous_cells = next_fluvial_candidates.intersection(next_pluvial_candidates)
                    if ambiguous_cells:
                        classifications.loc[list(ambiguous_cells)] = 'ambiguous'
                        
                    # Classify non-conflicted cells
                    newly_fluvial = next_fluvial_candidates - ambiguous_cells
                    if newly_fluvial:
                        classifications.loc[list(newly_fluvial)] = 'fluvial'

                    newly_pluvial = next_pluvial_candidates - ambiguous_cells
                    if newly_pluvial:
                        classifications.loc[list(newly_pluvial)] = 'pluvial'
                    
                    # Update frontiers for the next iteration
                    fluvial_frontier = newly_fluvial
                    pluvial_frontier = newly_pluvial
                                        
                    pbar.update(1)
                    pbar.set_postfix({
                        "Fluvial": len(fluvial_frontier), 
                        "Pluvial": len(pluvial_frontier),
                        "Ambiguous": len(ambiguous_cells)
                    })
            
            logger.info(f"Region growing completed in {iteration} iterations.")
            
            # --- 4. Finalization and Dissolving ---
            # Classify any remaining unclassified (likely isolated) cells as ambiguous
            classifications[classifications == 'unclassified'] = 'ambiguous'

            logger.info("Merging classifications with cell polygons...")
            classified_gdf = cell_polygons_gdf.merge(classifications.to_frame(), left_on='cell_id', right_index=True)
            
            logger.info("Dissolving polygons by classification...")
            final_regions_gdf = classified_gdf.dissolve(by='classification', aggfunc='first').reset_index()

            # --- 5. Minimum Polygon Area Filtering and Merging (if requested) ---
            if min_polygon_area_acres is not None:
                logger.info(f"Applying minimum polygon area filter: {min_polygon_area_acres} acres")
                # Calculate area in acres (1 acre = 4046.8564224 m^2)
                # If CRS is not projected, warn and skip area filtering
                if not final_regions_gdf.crs or not final_regions_gdf.crs.is_projected:
                    logger.warning("CRS is not projected. Area-based filtering skipped.")
                else:
                    # Explode to individual polygons for area filtering
                    exploded = final_regions_gdf.explode(index_parts=False, ignore_index=True)
                    exploded['area_acres'] = exploded.geometry.area / 4046.8564224

                    # Only consider fluvial and pluvial polygons for area filtering
                    mask_fluvial = (exploded['classification'] == 'fluvial') & (exploded['area_acres'] < min_polygon_area_acres)
                    mask_pluvial = (exploded['classification'] == 'pluvial') & (exploded['area_acres'] < min_polygon_area_acres)

                    n_fluvial = mask_fluvial.sum()
                    n_pluvial = mask_pluvial.sum()
                    logger.info(f"Found {n_fluvial} small fluvial and {n_pluvial} small pluvial polygons to reclassify.")

                    # Reclassify small fluvial polygons as pluvial, and small pluvial polygons as fluvial
                    exploded.loc[mask_fluvial, 'classification'] = 'pluvial'
                    exploded.loc[mask_pluvial, 'classification'] = 'fluvial'
                    # Ambiguous polygons are not changed

                    # Redissolve by classification to merge with adjacent polygons of the same type
                    final_regions_gdf = exploded.dissolve(by='classification', aggfunc='first').reset_index()
                    logger.info("Redissolved polygons after reclassification of small areas.")

            logger.info("Polygon generation completed successfully.")
            return final_regions_gdf
            
        except Exception as e:
            logger.error(f"Error generating fluvial-pluvial polygons: {str(e)}", exc_info=True)
            return gpd.GeoDataFrame()
        
        
    @staticmethod
    def _process_cell_adjacencies(cell_polygons_gdf: gpd.GeoDataFrame) -> Tuple[Dict[int, List[int]], Dict[int, Dict[int, LineString]]]:
        """
        Optimized method to process cell adjacencies by extracting shared edges directly.
        """
        cell_adjacency = defaultdict(list)
        common_edges = defaultdict(dict)
        edge_to_cells = defaultdict(set)

        def edge_key(coords1, coords2, precision=8):
            coords1 = tuple(round(coord, precision) for coord in coords1)
            coords2 = tuple(round(coord, precision) for coord in coords2)
            return tuple(sorted([coords1, coords2]))

        for _, row in cell_polygons_gdf.iterrows():
            cell_id = row['cell_id']
            geom = row['geometry']
            if geom.is_empty or not geom.is_valid:
                continue
            coords = list(geom.exterior.coords)
            for i in range(len(coords) - 1):
                key = edge_key(coords[i], coords[i + 1])
                edge_to_cells[key].add(cell_id)

        for edge, cells in edge_to_cells.items():
            cell_list = list(cells)
            if len(cell_list) >= 2:
                for i in range(len(cell_list)):
                    for j in range(i + 1, len(cell_list)):
                        cell1, cell2 = cell_list[i], cell_list[j]
                        cell_adjacency[cell1].append(cell2)
                        cell_adjacency[cell2].append(cell1)
                        common_edge = LineString([edge[0], edge[1]])
                        common_edges[cell1][cell2] = common_edge
                        common_edges[cell2][cell1] = common_edge

        return cell_adjacency, common_edges
    
    @staticmethod
    def _get_boundary_cell_pairs(
        cell_adjacency: Dict[int, List[int]], 
        cell_times: Dict[int, pd.Timestamp], 
        delta_t: float
    ) -> List[Tuple[int, int]]:
        """
        Identifies pairs of adjacent cell IDs that form a boundary.

        A boundary is defined where the difference in max water surface time
        between two adjacent cells is greater than delta_t.
        
        Args:
            cell_adjacency (Dict[int, List[int]]): Dictionary of cell adjacencies.
            cell_times (Dict[int, pd.Timestamp]): Dictionary mapping cell IDs to their max WSE times.
            delta_t (float): Time threshold in hours.

        Returns:
            List[Tuple[int, int]]: A list of tuples, where each tuple contains a pair of
                                   cell IDs forming a boundary.
        """
        boundary_cell_pairs = []
        processed_pairs = set()
        delta_t_seconds = delta_t * 3600

        for cell_id, neighbors in cell_adjacency.items():
            time1 = cell_times.get(cell_id)
            if not pd.notna(time1):
                continue

            for neighbor_id in neighbors:
                pair = tuple(sorted((cell_id, neighbor_id)))
                if pair in processed_pairs:
                    continue

                time2 = cell_times.get(neighbor_id)
                if not pd.notna(time2):
                    continue
                
                time_diff = abs((time1 - time2).total_seconds())

                if time_diff >= delta_t_seconds:
                    boundary_cell_pairs.append(pair)
                
                processed_pairs.add(pair)
        
        return boundary_cell_pairs

    @staticmethod
    def _identify_boundary_edges(
        cell_adjacency: Dict[int, List[int]], 
        common_edges: Dict[int, Dict[int, LineString]], 
        cell_times: Dict[int, pd.Timestamp], 
        delta_t: float,
        min_line_length: Optional[float] = None
    ) -> List[LineString]:
        """
        Identify boundary edges between cells with significant time differences.
        
        This function now uses the helper `_get_boundary_cell_pairs`.

        Args:
            cell_adjacency (Dict[int, List[int]]): Dictionary of cell adjacencies.
            common_edges (Dict[int, Dict[int, LineString]]): Dictionary of shared edges between cells.
            cell_times (Dict[int, pd.Timestamp]): Dictionary mapping cell IDs to their max WSE times.
            delta_t (float): Time threshold in hours.
            min_line_length (float, optional): Minimum length (in CRS units) for boundary lines to be included.
                                               Lines shorter than this will be dropped. Default is None (no filtering).

        Returns:
            List[LineString]: List of LineString geometries representing boundaries.
        """
        boundary_pairs = HdfFluvialPluvial._get_boundary_cell_pairs(cell_adjacency, cell_times, delta_t)
        
        boundary_edges = [common_edges[c1][c2] for c1, c2 in boundary_pairs]
        
        logger.info(f"Identified {len(boundary_edges)} boundary edges using delta_t of {delta_t} hours.")

        if min_line_length is not None:
            filtered_edges = [edge for edge in boundary_edges if edge.length >= min_line_length]
            num_dropped = len(boundary_edges) - len(filtered_edges)
            if num_dropped > 0:
                logger.info(f"{num_dropped} boundary line(s) shorter than {min_line_length} units were dropped after filtering.")
            boundary_edges = filtered_edges

        return boundary_edges

==================================================

File: c:\GH\ras-commander\ras_commander\HdfHydraulicTables.py
==================================================
"""
HdfHydraulicTables - Extract hydraulic property tables (HTAB) from HEC-RAS geometry HDF files

All methods are static and designed to be used without instantiation.

Hydraulic property tables contain preprocessed hydraulic properties computed during
geometry preprocessing. These tables enable hydraulic analysis without re-running HEC-RAS,
including:
- Area vs elevation curves
- Conveyance vs elevation
- Wetted perimeter vs elevation
- Top width vs elevation
- Rating curves and stage-discharge relationships

Available Functions:
- get_xs_htab() - Extract property table for a single cross section
- get_all_xs_htabs() - Extract property tables for all cross sections

Technical Notes:
    Property tables are stored in geometry HDF files (.g##.hdf), NOT plan HDF files.
    Path: /Geometry/Cross Sections/Property Tables/

    Data Structure:
        - XSEC Info: Index array mapping XS to property table rows [start_index, count, ds_cell]
        - XSEC Value: Property table data (N rows × 23 columns)
        - Variables attribute: Column names and units

    23 Hydraulic Properties Available:
        1. Elevation
        2-4. Area (LOB, Channel, ROB)
        5-7. Area Ineffective (LOB, Channel, ROB)
        8-10. Conveyance (LOB, Channel, ROB)
        11-13. Wetted Perimeter (LOB, Channel, ROB)
        14-16. Manning's n (LOB, Channel, ROB)
        17-20. Top Width (Total, LOB, Channel, ROB)
        21. Alpha (velocity distribution coefficient)
        22. Storage Area
        23. Beta (momentum coefficient)

Example Usage:
    >>> from ras_commander import HdfHydraulicTables
    >>> from pathlib import Path
    >>>
    >>> # Get property table for specific cross section
    >>> hdf_file = Path("BaldEagle.g01.hdf")
    >>> htab = HdfHydraulicTables.get_xs_htab(hdf_file, "Bald Eagle", "Loc Hav", "1")
    >>>
    >>> # Plot area-elevation curve
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(htab['Elevation'], htab['Area_Total'])
    >>> plt.xlabel('Elevation (ft)')
    >>> plt.ylabel('Area (sq ft)')
    >>> plt.title('Cross Section Area-Elevation Curve')
    >>> plt.show()
    >>>
    >>> # Calculate flow at specific stage
    >>> target_elev = 665.0
    >>> idx = (htab['Elevation'] - target_elev).abs().idxmin()
    >>> flow_area = htab.loc[idx, 'Area_Total']
    >>> conveyance = htab.loc[idx, 'Conveyance_Total']
    >>> print(f"At elevation {target_elev} ft:")
    >>> print(f"  Flow area: {flow_area:.1f} sq ft")
    >>> print(f"  Conveyance: {conveyance:.1f} cfs")

References:
    - See HdfXsec for cross section geometry extraction
    - See RasGeometry for plain text geometry operations
    - Property tables computed during geometry preprocessing in HEC-RAS
"""

from pathlib import Path
from typing import Union, Optional, Dict, Tuple
import h5py
import pandas as pd
import numpy as np

from .LoggingConfig import get_logger
from .Decorators import log_call, standardize_input

logger = get_logger(__name__)


class HdfHydraulicTables:
    """
    Extract hydraulic property tables (HTAB) from HEC-RAS geometry HDF files.

    All methods are static and designed to be used without instantiation.

    Property tables provide preprocessed hydraulic properties (area, conveyance,
    wetted perimeter, etc.) as functions of elevation for cross sections and structures.
    """

    @staticmethod
    def _get_xs_index(hdf_file: h5py.File, river: str, reach: str, rs: str) -> Optional[int]:
        """
        Find cross section index from river/reach/RS identifiers.

        Parameters:
            hdf_file (h5py.File): Open HDF file handle
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            Optional[int]: Cross section index, or None if not found

        Notes:
            - Uses /Geometry/Cross Sections/Attributes to map names to indices
            - Case-sensitive matching
        """
        try:
            attrs_path = '/Geometry/Cross Sections/Attributes'
            if attrs_path not in hdf_file:
                logger.error(f"Attributes path not found: {attrs_path}")
                return None

            attrs = hdf_file[attrs_path][:]

            # Check if required fields exist
            required_fields = ['River', 'Reach', 'RS']
            for field in required_fields:
                if field not in attrs.dtype.names:
                    logger.error(f"Required field '{field}' not found in Attributes")
                    return None

            # Search for matching cross section
            for i, attr in enumerate(attrs):
                attr_river = attr['River'].decode('utf-8').strip()
                attr_reach = attr['Reach'].decode('utf-8').strip()
                attr_rs = attr['RS'].decode('utf-8').strip()

                if attr_river == river and attr_reach == reach and attr_rs == rs:
                    logger.debug(f"Found XS at index {i}: {river}/{reach}/RS {rs}")
                    return i

            logger.warning(f"Cross section not found: {river}/{reach}/RS {rs}")
            return None

        except Exception as e:
            logger.error(f"Error finding XS index: {str(e)}")
            return None

    @staticmethod
    def _extract_property_table(hdf_file: h5py.File, xs_index: int) -> Optional[pd.DataFrame]:
        """
        Extract property table for a cross section index.

        Parameters:
            hdf_file (h5py.File): Open HDF file handle
            xs_index (int): Cross section index

        Returns:
            Optional[pd.DataFrame]: Property table with all 23 hydraulic properties

        Notes:
            - Reads from /Geometry/Cross Sections/Property Tables/
            - Returns DataFrame with elevation + 22 other properties
        """
        try:
            prop_path = '/Geometry/Cross Sections/Property Tables'
            if prop_path not in hdf_file:
                logger.error(f"Property Tables path not found: {prop_path}")
                return None

            prop_tables = hdf_file[prop_path]

            # Read index info
            if 'XSEC Info' not in prop_tables:
                logger.error("XSEC Info not found in Property Tables")
                return None

            xsec_info = prop_tables['XSEC Info'][:]

            if xs_index >= len(xsec_info):
                logger.error(f"XS index {xs_index} out of range (max: {len(xsec_info)-1})")
                return None

            # Get start index and count for this XS
            start_idx = xsec_info[xs_index][0]
            count = xsec_info[xs_index][1]

            logger.debug(f"XS {xs_index}: start={start_idx}, count={count}")

            # Read property table values
            if 'XSEC Value' not in prop_tables:
                logger.error("XSEC Value not found in Property Tables")
                return None

            xsec_value = prop_tables['XSEC Value']

            # Extract data for this XS
            data = xsec_value[start_idx:start_idx + count, :]

            # Get column names from Variables attribute
            if 'Variables' in xsec_value.attrs:
                variables = xsec_value.attrs['Variables']
                # Variables is Nx2 array: [name, units]
                col_names = [var[0].decode('utf-8').strip() for var in variables]

                # Create friendly column names
                friendly_names = []
                for name in col_names:
                    # Convert names like "Area LOB" to "Area_LOB"
                    friendly = name.replace(' ', '_')
                    # Special handling for total values
                    if friendly == 'Area_Chan' and 'Area_LOB' in friendly_names:
                        # Calculate total area
                        pass  # Will compute after DataFrame creation
                    friendly_names.append(friendly)

            else:
                # Fallback column names
                col_names = [f'Property_{i}' for i in range(data.shape[1])]
                friendly_names = col_names

            # Create DataFrame
            df = pd.DataFrame(data, columns=friendly_names)

            # Calculate total values from LOB + Chan + ROB
            if 'Area_LOB' in df.columns and 'Area_Chan' in df.columns and 'Area_ROB' in df.columns:
                df['Area_Total'] = df['Area_LOB'] + df['Area_Chan'] + df['Area_ROB']

            if 'Conv_LOB' in df.columns and 'Conv_Chan' in df.columns and 'Conv_ROB' in df.columns:
                df['Conveyance_Total'] = df['Conv_LOB'] + df['Conv_Chan'] + df['Conv_ROB']

            if 'WP_LOB' in df.columns and 'WP_Chan' in df.columns and 'WP_ROB' in df.columns:
                df['Wetted_Perimeter_Total'] = df['WP_LOB'] + df['WP_Chan'] + df['WP_ROB']

            logger.info(f"Extracted property table: {len(df)} elevations × {len(df.columns)} properties")

            return df

        except Exception as e:
            logger.error(f"Error extracting property table: {str(e)}")
            return None

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_xs_htab(hdf_path: Union[str, Path],
                    river: str,
                    reach: str,
                    rs: str) -> pd.DataFrame:
        """
        Extract hydraulic property table (HTAB) for a cross section.

        Reads preprocessed hydraulic properties from geometry HDF file, including
        area, conveyance, wetted perimeter, top width, and other properties as
        functions of elevation.

        Parameters:
            hdf_path (Union[str, Path]): Path to geometry HDF file (.g##.hdf)
            river (str): River name (case-sensitive)
            reach (str): Reach name (case-sensitive)
            rs (str): River station (as string, e.g., "1")

        Returns:
            pd.DataFrame: Property table with columns:
                - Elevation: Water surface elevation (ft or m)
                - Area_LOB: Left overbank area (sq ft or sq m)
                - Area_Chan: Channel area
                - Area_ROB: Right overbank area
                - Area_Total: Total flow area (computed)
                - Area_Ineff_LOB: Ineffective area left overbank
                - Area_Ineff_Chan: Ineffective area channel
                - Area_Ineff_ROB: Ineffective area right overbank
                - Conv_LOB: Conveyance left overbank (cfs or cms)
                - Conv_Chan: Conveyance channel
                - Conv_ROB: Conveyance right overbank
                - Conveyance_Total: Total conveyance (computed)
                - WP_LOB: Wetted perimeter left overbank (ft or m)
                - WP_Chan: Wetted perimeter channel
                - WP_ROB: Wetted perimeter right overbank
                - Wetted_Perimeter_Total: Total wetted perimeter (computed)
                - Mann_N_LOB: Manning's n left overbank
                - Mann_N_Chan: Manning's n channel
                - Mann_N_ROB: Manning's n right overbank
                - Top_Width: Total top width (ft or m)
                - Top_Width_LOB: Top width left overbank
                - Top_Width_Chan: Top width channel
                - Top_Width_ROB: Top width right overbank
                - Alpha: Velocity distribution coefficient
                - Storage_Area: Storage area (sq ft or sq m)
                - Beta: Momentum coefficient

        Raises:
            FileNotFoundError: If HDF file doesn't exist
            ValueError: If cross section not found
            IOError: If HDF read fails

        Example:
            >>> from ras_commander import HdfHydraulicTables
            >>> from pathlib import Path
            >>> import matplotlib.pyplot as plt
            >>>
            >>> # Extract property table
            >>> hdf_file = Path("BaldEagle.g01.hdf")
            >>> htab = HdfHydraulicTables.get_xs_htab(hdf_file, "Bald Eagle", "Loc Hav", "1")
            >>>
            >>> print(f"Property table: {len(htab)} elevations")
            >>> print(f"Elevation range: {htab['Elevation'].min():.2f} to {htab['Elevation'].max():.2f}")
            >>>
            >>> # Plot area-elevation curve
            >>> plt.figure(figsize=(10, 6))
            >>> plt.plot(htab['Area_Total'], htab['Elevation'], 'b-', linewidth=2)
            >>> plt.xlabel('Flow Area (sq ft)')
            >>> plt.ylabel('Elevation (ft)')
            >>> plt.title('Cross Section Area-Elevation Curve')
            >>> plt.grid(True, alpha=0.3)
            >>> plt.show()
            >>>
            >>> # Calculate hydraulic radius
            >>> htab['Hydraulic_Radius'] = htab['Area_Total'] / htab['Wetted_Perimeter_Total']
            >>> print(f"Max hydraulic radius: {htab['Hydraulic_Radius'].max():.2f} ft")

        Notes:
            - Property tables are in GEOMETRY HDF (.g##.hdf), not plan HDF
            - Tables computed during geometry preprocessing in HEC-RAS
            - Use for rating curves, stage-discharge, hydraulic analysis
            - Total values (area, conveyance, WP) computed from LOB + Chan + ROB
            - See HdfXsec.get_cross_sections() for XS geometry
        """
        hdf_path = Path(hdf_path)

        if not hdf_path.exists():
            raise FileNotFoundError(f"Geometry HDF file not found: {hdf_path}")

        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Find cross section index
                xs_index = HdfHydraulicTables._get_xs_index(hdf, river, reach, rs)

                if xs_index is None:
                    raise ValueError(
                        f"Cross section not found in HDF: {river}/{reach}/RS {rs}\n"
                        f"Check that river, reach, and RS names match exactly (case-sensitive)"
                    )

                # Extract property table
                df = HdfHydraulicTables._extract_property_table(hdf, xs_index)

                if df is None:
                    raise IOError(f"Failed to extract property table for {river}/{reach}/RS {rs}")

                logger.info(
                    f"Extracted HTAB for {river}/{reach}/RS {rs}: "
                    f"{len(df)} elevations, {len(df.columns)} properties"
                )

                return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading property table from HDF: {str(e)}")
            raise IOError(f"Failed to read property table: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_all_xs_htabs(hdf_path: Union[str, Path]) -> Dict[Tuple[str, str, str], pd.DataFrame]:
        """
        Extract hydraulic property tables for ALL cross sections in geometry.

        Batch extraction of property tables for all cross sections, returned as
        a dictionary keyed by (river, reach, rs) tuples.

        Parameters:
            hdf_path (Union[str, Path]): Path to geometry HDF file (.g##.hdf)

        Returns:
            Dict[Tuple[str, str, str], pd.DataFrame]: Dictionary mapping
                (river, reach, rs) tuples to property table DataFrames

        Raises:
            FileNotFoundError: If HDF file doesn't exist
            IOError: If HDF read fails

        Example:
            >>> from ras_commander import HdfHydraulicTables
            >>> from pathlib import Path
            >>>
            >>> # Extract all property tables
            >>> hdf_file = Path("BaldEagle.g01.hdf")
            >>> all_htabs = HdfHydraulicTables.get_all_xs_htabs(hdf_file)
            >>>
            >>> print(f"Extracted {len(all_htabs)} property tables")
            >>>
            >>> # Access specific cross section
            >>> htab = all_htabs[("Bald Eagle", "Loc Hav", "1")]
            >>>
            >>> # Calculate statistics across all cross sections
            >>> max_areas = {}
            >>> for (river, reach, rs), htab in all_htabs.items():
            ...     max_area = htab['Area_Total'].max()
            ...     max_areas[rs] = max_area
            >>>
            >>> # Find cross section with largest area
            >>> largest_rs = max(max_areas, key=max_areas.get)
            >>> print(f"Largest XS: RS {largest_rs} with area {max_areas[largest_rs]:.1f} sq ft")

        Notes:
            - More efficient than calling get_xs_htab() repeatedly
            - Returns all cross sections in single HDF file read
            - Dictionary keys are (river, reach, rs) tuples for easy lookup
        """
        hdf_path = Path(hdf_path)

        if not hdf_path.exists():
            raise FileNotFoundError(f"Geometry HDF file not found: {hdf_path}")

        try:
            all_htabs = {}

            with h5py.File(hdf_path, 'r') as hdf:
                # Read attributes to get all river/reach/RS combinations
                attrs_path = '/Geometry/Cross Sections/Attributes'
                if attrs_path not in hdf:
                    logger.error(f"Attributes path not found: {attrs_path}")
                    return all_htabs

                attrs = hdf[attrs_path][:]

                # Extract property table for each cross section
                for i, attr in enumerate(attrs):
                    river = attr['River'].decode('utf-8').strip()
                    reach = attr['Reach'].decode('utf-8').strip()
                    rs = attr['RS'].decode('utf-8').strip()

                    # Extract property table
                    df = HdfHydraulicTables._extract_property_table(hdf, i)

                    if df is not None:
                        all_htabs[(river, reach, rs)] = df
                    else:
                        logger.warning(f"Failed to extract HTAB for {river}/{reach}/RS {rs}")

            logger.info(f"Extracted {len(all_htabs)} property tables from {hdf_path.name}")

            return all_htabs

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading property tables from HDF: {str(e)}")
            raise IOError(f"Failed to read property tables: {str(e)}")

==================================================

File: c:\GH\ras-commander\ras_commander\HdfInfiltration.py
==================================================
"""
Class: HdfInfiltration

A comprehensive class for handling infiltration-related operations in HEC-RAS HDF geometry files.
This class provides methods for managing infiltration parameters, soil statistics, and raster data processing.

Key Features:
- Infiltration parameter management (scaling, setting, retrieving)
- Soil statistics calculation and analysis
- Raster data processing and mapping
- Weighted parameter calculations
- Data export and file management

Methods:
1. Geometry File Base Override Management:
   - scale_infiltration_data(): Updates infiltration parameters with scaling factors in geometry file
   - get_infiltration_data(): Retrieves current infiltration parameters from geometry file
   - set_infiltration_table(): Sets infiltration parameters directly in geometry file

2. Raster and Mapping Operations (uses rasmap_df HDF files):
   - get_infiltration_map(): Reads infiltration raster map from rasmap_df HDF file
   - calculate_soil_statistics(): Processes zonal statistics for soil analysis

3. Soil Analysis (uses rasmap_df HDF files):
   - get_significant_mukeys(): Identifies mukeys above percentage threshold
   - calculate_total_significant_percentage(): Computes total coverage of significant mukeys
   - get_infiltration_parameters(): Retrieves parameters for specific mukey
   - calculate_weighted_parameters(): Computes weighted average parameters

4. Data Management (uses rasmap_df HDF files):
   - save_statistics(): Exports soil statistics to CSV

Constants:
- SQM_TO_ACRE: Conversion factor from square meters to acres (0.000247105)
- SQM_TO_SQMILE: Conversion factor from square meters to square miles (3.861e-7)

Dependencies:
- pathlib: Path handling
- pandas: Data manipulation
- geopandas: Geospatial data processing
- h5py: HDF file operations
- rasterstats: Zonal statistics calculation (optional)

Note:
- Methods in section 1 work with base overrides in geometry files
- Methods in sections 2-4 work with HDF files from rasmap_df by default
- All methods are static and decorated with @standardize_input and @log_call
- The class is designed to work with both HEC-RAS geometry files and rasmap_df HDF files
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any, List, Tuple
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)
        
from pathlib import Path
import pandas as pd
import geopandas as gpd
import h5py

from .Decorators import log_call, standardize_input

class HdfInfiltration:
        
    """
    A class for handling infiltration-related operations on HEC-RAS HDF geometry files.

    This class provides methods to extract and modify infiltration data from HEC-RAS HDF geometry files,
    including base overrides of infiltration parameters.
    """

    # Constants for unit conversion
    SQM_TO_ACRE = 0.000247105
    SQM_TO_SQMILE = 3.861e-7
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)


    @staticmethod
    @log_call 
    def get_infiltration_baseoverrides(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Retrieve current infiltration parameters from a HEC-RAS geometry HDF file.
        Dynamically reads whatever columns are present in the table.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        Optional[pd.DataFrame]
            DataFrame containing infiltration parameters if successful, None if operation fails
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                table_path = '/Geometry/Infiltration/Base Overrides'
                if table_path not in hdf_file:
                    logger.warning(f"No infiltration data found in {hdf_path}")
                    return None

                # Get column info
                col_names, _, _ = HdfInfiltration._get_table_info(hdf_file, table_path)
                if not col_names:
                    logger.error(f"No columns found in infiltration table")
                    return None
                    
                # Read data
                data = hdf_file[table_path][()]
                
                # Convert to DataFrame
                df_dict = {}
                for col in col_names:
                    values = data[col]
                    # Convert byte strings to regular strings if needed
                    if values.dtype.kind == 'S':
                        values = [v.decode('utf-8').strip() for v in values]
                    df_dict[col] = values
                
                return pd.DataFrame(df_dict)

        except Exception as e:
            logger.error(f"Error reading infiltration data from {hdf_path}: {str(e)}")
            return None
        


    # set_infiltration_baseoverrides goes here, once finalized tested and fixed. 



    # Since the infiltration base overrides are in the geometry file, the above functions work on the geometry files
    # The below functions work on the infiltration layer HDF files.  Changes only take effect if no base overrides are present. 
           
    @staticmethod
    @log_call 
    def get_infiltration_layer_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Retrieve current infiltration parameters from a HEC-RAS infiltration layer HDF file.
        Extracts the Variables dataset which contains the layer data.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS infiltration layer HDF file

        Returns
        -------
        Optional[pd.DataFrame]
            DataFrame containing infiltration parameters if successful, None if operation fails
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                variables_path = '//Variables'
                if variables_path not in hdf_file:
                    logger.warning(f"No Variables dataset found in {hdf_path}")
                    return None
                
                # Read data from Variables dataset
                data = hdf_file[variables_path][()]
                
                # Convert to DataFrame
                df_dict = {}
                for field_name in data.dtype.names:
                    values = data[field_name]
                    # Convert byte strings to regular strings if needed
                    if values.dtype.kind == 'S':
                        values = [v.decode('utf-8').strip() for v in values]
                    df_dict[field_name] = values
                
                return pd.DataFrame(df_dict)

        except Exception as e:
            logger.error(f"Error reading infiltration layer data from {hdf_path}: {str(e)}")
            return None
        

    @staticmethod
    @log_call
    def set_infiltration_layer_data(
        hdf_path: Path,
        infiltration_df: pd.DataFrame
    ) -> Optional[pd.DataFrame]:
        """
        Set infiltration layer data in the infiltration layer HDF file directly from the provided DataFrame.
        # NOTE: This will not work if there are base overrides present in the Geometry HDF file. 
        Updates the Variables dataset with the provided data.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS infiltration layer HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters with columns:
            - Name (string)
            - Curve Number (float)
            - Abstraction Ratio (float)
            - Minimum Infiltration Rate (float)

        Returns
        -------
        Optional[pd.DataFrame]
            The infiltration DataFrame if successful, None if operation fails
        """
        try:
            variables_path = '//Variables'
            
            # Validate required columns
            required_columns = ['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate']
            missing_columns = [col for col in required_columns if col not in infiltration_df.columns]
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            with h5py.File(hdf_path, 'a') as hdf_file:
                # Delete existing dataset if it exists
                if variables_path in hdf_file:
                    del hdf_file[variables_path]

                # Create dtype for structured array
                dt = np.dtype([
                    ('Name', f'S{infiltration_df["Name"].str.len().max()}'),
                    ('Curve Number', 'f4'),
                    ('Abstraction Ratio', 'f4'),
                    ('Minimum Infiltration Rate', 'f4')
                ])

                # Create structured array
                structured_array = np.zeros(infiltration_df.shape[0], dtype=dt)
                
                # Fill structured array
                structured_array['Name'] = infiltration_df['Name'].values.astype(f'|S{dt["Name"].itemsize}')
                structured_array['Curve Number'] = infiltration_df['Curve Number'].values
                structured_array['Abstraction Ratio'] = infiltration_df['Abstraction Ratio'].values
                structured_array['Minimum Infiltration Rate'] = infiltration_df['Minimum Infiltration Rate'].values

                # Create new dataset
                hdf_file.create_dataset(
                    variables_path,
                    data=structured_array,
                    dtype=dt,
                    compression='gzip',
                    compression_opts=1,
                    chunks=(100,),
                    maxshape=(None,)
                )

            return infiltration_df

        except Exception as e:
            logger.error(f"Error setting infiltration layer data in {hdf_path}: {str(e)}")
            return None
        



    @staticmethod
    @standardize_input(file_type='geom_hdf')
    @log_call
    def scale_infiltration_data(
        hdf_path: Path,
        infiltration_df: pd.DataFrame,
        scale_factors: Dict[str, float]
    ) -> Optional[pd.DataFrame]:
        """
        Update infiltration parameters in the HDF file with scaling factors.
        Supports any numeric columns present in the DataFrame.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters
        scale_factors : Dict[str, float]
            Dictionary mapping column names to their scaling factors

        Returns
        -------
        Optional[pd.DataFrame]
            The updated infiltration DataFrame if successful, None if operation fails
        """
        try:
            # Make a copy to avoid modifying the input DataFrame
            infiltration_df = infiltration_df.copy()
            
            # Apply scaling factors to specified columns
            for col, factor in scale_factors.items():
                if col in infiltration_df.columns and pd.api.types.is_numeric_dtype(infiltration_df[col]):
                    infiltration_df[col] *= factor
                else:
                    logger.warning(f"Column {col} not found or not numeric - skipping scaling")

            # Use set_infiltration_table to write the scaled data
            return HdfInfiltration.set_infiltration_table(hdf_path, infiltration_df)

        except Exception as e:
            logger.error(f"Error scaling infiltration data in {hdf_path}: {str(e)}")
            return None



    # Need to reorganize these soil staatistics functions so they are more straightforward.  


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_soils_raster_stats(
        geom_hdf_path: Path,
        soil_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate soil group statistics for each 2D flow area using the area's perimeter.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        soil_hdf_path : Path, optional
            Path to the soil HDF file. If None, uses soil_layer_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with soil statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - mukey: Soil mukey identifier
            - percentage: Percentage of 2D flow area covered by this soil type
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get the soil HDF path
        if soil_hdf_path is None:
            if ras_object is None:
                from .RasPrj import ras
                ras_object = ras
            
            # Try to get soil_layer_path from rasmap_df
            try:
                soil_hdf_path = Path(ras_object.rasmap_df.loc[0, 'soil_layer_path'][0])
                if not soil_hdf_path.exists():
                    logger.warning(f"Soil HDF path from rasmap_df does not exist: {soil_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving soil_layer_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get infiltration map - pass as hdf_path to ensure standardize_input works correctly
        try:
            raster_map = HdfInfiltration.get_infiltration_map(hdf_path=soil_hdf_path, ras_object=ras_object)
            if not raster_map:
                logger.error(f"No infiltration map found in {soil_hdf_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error getting infiltration map: {str(e)}")
            return pd.DataFrame()
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Extract the raster data for analysis
        tif_path = soil_hdf_path.with_suffix('.tif')
        if not tif_path.exists():
            logger.error(f"No raster file found at {tif_path}")
            return pd.DataFrame()
            
        # Read the raster data and info
        import rasterio
        with rasterio.open(tif_path) as src:
            grid_data = src.read(1)
            
            # Get transform directly from rasterio
            transform = src.transform
            no_data = src.nodata if src.nodata is not None else -9999
            
            # List to store all results
            all_results = []
            
            # Calculate zonal statistics for each 2D flow area
            for _, mesh_row in mesh_areas.iterrows():
                mesh_name = mesh_row['mesh_name']
                mesh_geom = mesh_row['geometry']
                
                # Get zonal statistics directly using numpy array
                try:
                    stats = zonal_stats(
                        mesh_geom,
                        grid_data,
                        affine=transform,
                        categorical=True,
                        nodata=no_data
                    )[0]
                    
                    # Skip if no stats
                    if not stats:
                        logger.warning(f"No soil data found for 2D flow area: {mesh_name}")
                        continue
                    
                    # Calculate total area and percentages
                    total_area_sqm = sum(stats.values())
                    
                    # Process each mukey
                    for raster_val, area_sqm in stats.items():
                        # Skip NoData values
                        if raster_val is None or raster_val == no_data:
                            continue
                            
                        try:
                            mukey = raster_map.get(int(raster_val), f"Unknown-{raster_val}")
                        except (ValueError, TypeError):
                            mukey = f"Unknown-{raster_val}"
                            
                        percentage = (area_sqm / total_area_sqm) * 100 if total_area_sqm > 0 else 0
                        
                        all_results.append({
                            'mesh_name': mesh_name,
                            'mukey': mukey,
                            'percentage': percentage,
                            'area_sqm': area_sqm,
                            'area_acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                            'area_sqmiles': area_sqm * HdfInfiltration.SQM_TO_SQMILE
                        })
                except Exception as e:
                    logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                    continue
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name and percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df






    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_soil_raster_stats(
        geom_hdf_path: Path,
        landcover_hdf_path: Path = None,
        soil_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate combined land cover and soil infiltration statistics for each 2D flow area.
        
        This function processes both land cover and soil data to calculate statistics
        for each combination (Land Cover : Soil Type) within each 2D flow area.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        landcover_hdf_path : Path, optional
            Path to the land cover HDF file. If None, uses landcover_hdf_path from rasmap_df
        soil_hdf_path : Path, optional
            Path to the soil HDF file. If None, uses soil_layer_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with combined statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - combined_type: Combined land cover and soil type (e.g. "Mixed Forest : B")
            - percentage: Percentage of 2D flow area covered by this combination
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
            - curve_number: Curve number for this combination
            - abstraction_ratio: Abstraction ratio for this combination
            - min_infiltration_rate: Minimum infiltration rate for this combination
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
            import rasterio
            from rasterio.merge import merge
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas rasterio'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get RAS object
        if ras_object is None:
            from .RasPrj import ras
            ras_object = ras
        
        # Get the landcover HDF path
        if landcover_hdf_path is None:
            try:
                landcover_hdf_path = Path(ras_object.rasmap_df.loc[0, 'landcover_hdf_path'][0])
                if not landcover_hdf_path.exists():
                    logger.warning(f"Land cover HDF path from rasmap_df does not exist: {landcover_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving landcover_hdf_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get the soil HDF path
        if soil_hdf_path is None:
            try:
                soil_hdf_path = Path(ras_object.rasmap_df.loc[0, 'soil_layer_path'][0])
                if not soil_hdf_path.exists():
                    logger.warning(f"Soil HDF path from rasmap_df does not exist: {soil_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving soil_layer_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get land cover map (raster to ID mapping)
        try:
            with h5py.File(landcover_hdf_path, 'r') as hdf:
                if '//Raster Map' not in hdf:
                    logger.error(f"No Raster Map found in {landcover_hdf_path}")
                    return pd.DataFrame()
                
                landcover_map_data = hdf['//Raster Map'][()]
                landcover_map = {int(item[0]): item[1].decode('utf-8').strip() for item in landcover_map_data}
        except Exception as e:
            logger.error(f"Error reading land cover data from HDF: {str(e)}")
            return pd.DataFrame()
        
        # Get soil map (raster to ID mapping)
        try:
            soil_map = HdfInfiltration.get_infiltration_map(hdf_path=soil_hdf_path, ras_object=ras_object)
            if not soil_map:
                logger.error(f"No soil map found in {soil_hdf_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error getting soil map: {str(e)}")
            return pd.DataFrame()
        
        # Get infiltration parameters
        try:
            infiltration_params = HdfInfiltration.get_infiltration_layer_data(soil_hdf_path)
            if infiltration_params is None or infiltration_params.empty:
                logger.warning(f"No infiltration parameters found in {soil_hdf_path}")
                infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        except Exception as e:
            logger.error(f"Error getting infiltration parameters: {str(e)}")
            infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Check for the TIF files with same name as HDF
        landcover_tif_path = landcover_hdf_path.with_suffix('.tif')
        soil_tif_path = soil_hdf_path.with_suffix('.tif')
        
        if not landcover_tif_path.exists():
            logger.error(f"No land cover raster file found at {landcover_tif_path}")
            return pd.DataFrame()
        
        if not soil_tif_path.exists():
            logger.error(f"No soil raster file found at {soil_tif_path}")
            return pd.DataFrame()
        
        # List to store all results
        all_results = []
        
        # Read the raster data
        try:
            with rasterio.open(landcover_tif_path) as landcover_src, rasterio.open(soil_tif_path) as soil_src:
                landcover_nodata = landcover_src.nodata if landcover_src.nodata is not None else -9999
                soil_nodata = soil_src.nodata if soil_src.nodata is not None else -9999
                
                # Calculate zonal statistics for each 2D flow area
                for _, mesh_row in mesh_areas.iterrows():
                    mesh_name = mesh_row['mesh_name']
                    mesh_geom = mesh_row['geometry']
                    
                    # Get zonal statistics for land cover
                    try:
                        landcover_stats = zonal_stats(
                            mesh_geom,
                            landcover_tif_path,
                            categorical=True,
                            nodata=landcover_nodata
                        )[0]
                        
                        # Get zonal statistics for soil
                        soil_stats = zonal_stats(
                            mesh_geom,
                            soil_tif_path,
                            categorical=True,
                            nodata=soil_nodata
                        )[0]
                        
                        # Skip if no stats
                        if not landcover_stats or not soil_stats:
                            logger.warning(f"No land cover or soil data found for 2D flow area: {mesh_name}")
                            continue
                        
                        # Calculate total area
                        landcover_total = sum(landcover_stats.values())
                        soil_total = sum(soil_stats.values())
                        
                        # Create a cross-tabulation of land cover and soil types
                        # This is an approximation since we don't have the exact pixel-by-pixel overlap
                        mesh_area_sqm = mesh_row['geometry'].area
                        
                        # Calculate percentage of each land cover type
                        landcover_pct = {k: v/landcover_total for k, v in landcover_stats.items() if k is not None and k != landcover_nodata}
                        
                        # Calculate percentage of each soil type
                        soil_pct = {k: v/soil_total for k, v in soil_stats.items() if k is not None and k != soil_nodata}
                        
                        # Generate combinations
                        for lc_id, lc_pct in landcover_pct.items():
                            lc_name = landcover_map.get(int(lc_id), f"Unknown-{lc_id}")
                            
                            for soil_id, soil_pct in soil_pct.items():
                                try:
                                    soil_name = soil_map.get(int(soil_id), f"Unknown-{soil_id}")
                                except (ValueError, TypeError):
                                    soil_name = f"Unknown-{soil_id}"
                                
                                # Calculate combined percentage (approximate)
                                # This is a simplification; actual overlap would require pixel-by-pixel analysis
                                combined_pct = lc_pct * soil_pct * 100
                                combined_area_sqm = mesh_area_sqm * (combined_pct / 100)
                                
                                # Create combined name
                                combined_name = f"{lc_name} : {soil_name}"
                                
                                # Look up infiltration parameters
                                param_row = infiltration_params[infiltration_params['Name'] == combined_name]
                                if param_row.empty:
                                    # Try with NoData for soil type
                                    param_row = infiltration_params[infiltration_params['Name'] == f"{lc_name} : NoData"]
                                
                                if not param_row.empty:
                                    curve_number = param_row.iloc[0]['Curve Number']
                                    abstraction_ratio = param_row.iloc[0]['Abstraction Ratio']
                                    min_infiltration_rate = param_row.iloc[0]['Minimum Infiltration Rate']
                                else:
                                    curve_number = None
                                    abstraction_ratio = None
                                    min_infiltration_rate = None
                                
                                all_results.append({
                                    'mesh_name': mesh_name,
                                    'combined_type': combined_name,
                                    'percentage': combined_pct,
                                    'area_sqm': combined_area_sqm,
                                    'area_acres': combined_area_sqm * HdfInfiltration.SQM_TO_ACRE,
                                    'area_sqmiles': combined_area_sqm * HdfInfiltration.SQM_TO_SQMILE,
                                    'curve_number': curve_number,
                                    'abstraction_ratio': abstraction_ratio,
                                    'min_infiltration_rate': min_infiltration_rate
                                })
                    except Exception as e:
                        logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error opening raster files: {str(e)}")
            return pd.DataFrame()
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name, percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df






    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_infiltration_stats(
        geom_hdf_path: Path,
        landcover_hdf_path: Path = None,
        soil_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate combined land cover and soil infiltration statistics for each 2D flow area.
        
        This function processes both land cover and soil data to calculate statistics
        for each combination (Land Cover : Soil Type) within each 2D flow area.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        landcover_hdf_path : Path, optional
            Path to the land cover HDF file. If None, uses landcover_hdf_path from rasmap_df
        soil_hdf_path : Path, optional
            Path to the soil HDF file. If None, uses soil_layer_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with combined statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - combined_type: Combined land cover and soil type (e.g. "Mixed Forest : B")
            - percentage: Percentage of 2D flow area covered by this combination
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
            - curve_number: Curve number for this combination
            - abstraction_ratio: Abstraction ratio for this combination
            - min_infiltration_rate: Minimum infiltration rate for this combination
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
            import rasterio
            from rasterio.merge import merge
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas rasterio'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get RAS object
        if ras_object is None:
            from .RasPrj import ras
            ras_object = ras
        
        # Get the landcover HDF path
        if landcover_hdf_path is None:
            try:
                landcover_hdf_path = Path(ras_object.rasmap_df.loc[0, 'landcover_hdf_path'][0])
                if not landcover_hdf_path.exists():
                    logger.warning(f"Land cover HDF path from rasmap_df does not exist: {landcover_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving landcover_hdf_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get the soil HDF path
        if soil_hdf_path is None:
            try:
                soil_hdf_path = Path(ras_object.rasmap_df.loc[0, 'soil_layer_path'][0])
                if not soil_hdf_path.exists():
                    logger.warning(f"Soil HDF path from rasmap_df does not exist: {soil_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving soil_layer_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get land cover map (raster to ID mapping)
        try:
            with h5py.File(landcover_hdf_path, 'r') as hdf:
                if '//Raster Map' not in hdf:
                    logger.error(f"No Raster Map found in {landcover_hdf_path}")
                    return pd.DataFrame()
                
                landcover_map_data = hdf['//Raster Map'][()]
                landcover_map = {int(item[0]): item[1].decode('utf-8').strip() for item in landcover_map_data}
        except Exception as e:
            logger.error(f"Error reading land cover data from HDF: {str(e)}")
            return pd.DataFrame()
        
        # Get soil map (raster to ID mapping)
        try:
            soil_map = HdfInfiltration.get_infiltration_map(hdf_path=soil_hdf_path, ras_object=ras_object)
            if not soil_map:
                logger.error(f"No soil map found in {soil_hdf_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error getting soil map: {str(e)}")
            return pd.DataFrame()
        
        # Get infiltration parameters
        try:
            infiltration_params = HdfInfiltration.get_infiltration_layer_data(soil_hdf_path)
            if infiltration_params is None or infiltration_params.empty:
                logger.warning(f"No infiltration parameters found in {soil_hdf_path}")
                infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        except Exception as e:
            logger.error(f"Error getting infiltration parameters: {str(e)}")
            infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Check for the TIF files with same name as HDF
        landcover_tif_path = landcover_hdf_path.with_suffix('.tif')
        soil_tif_path = soil_hdf_path.with_suffix('.tif')
        
        if not landcover_tif_path.exists():
            logger.error(f"No land cover raster file found at {landcover_tif_path}")
            return pd.DataFrame()
        
        if not soil_tif_path.exists():
            logger.error(f"No soil raster file found at {soil_tif_path}")
            return pd.DataFrame()
        
        # List to store all results
        all_results = []
        
        # Read the raster data
        try:
            with rasterio.open(landcover_tif_path) as landcover_src, rasterio.open(soil_tif_path) as soil_src:
                landcover_nodata = landcover_src.nodata if landcover_src.nodata is not None else -9999
                soil_nodata = soil_src.nodata if soil_src.nodata is not None else -9999
                
                # Calculate zonal statistics for each 2D flow area
                for _, mesh_row in mesh_areas.iterrows():
                    mesh_name = mesh_row['mesh_name']
                    mesh_geom = mesh_row['geometry']
                    
                    # Get zonal statistics for land cover
                    try:
                        landcover_stats = zonal_stats(
                            mesh_geom,
                            landcover_tif_path,
                            categorical=True,
                            nodata=landcover_nodata
                        )[0]
                        
                        # Get zonal statistics for soil
                        soil_stats = zonal_stats(
                            mesh_geom,
                            soil_tif_path,
                            categorical=True,
                            nodata=soil_nodata
                        )[0]
                        
                        # Skip if no stats
                        if not landcover_stats or not soil_stats:
                            logger.warning(f"No land cover or soil data found for 2D flow area: {mesh_name}")
                            continue
                        
                        # Calculate total area
                        landcover_total = sum(landcover_stats.values())
                        soil_total = sum(soil_stats.values())
                        
                        # Create a cross-tabulation of land cover and soil types
                        # This is an approximation since we don't have the exact pixel-by-pixel overlap
                        mesh_area_sqm = mesh_row['geometry'].area
                        
                        # Calculate percentage of each land cover type
                        landcover_pct = {k: v/landcover_total for k, v in landcover_stats.items() if k is not None and k != landcover_nodata}
                        
                        # Calculate percentage of each soil type
                        soil_pct = {k: v/soil_total for k, v in soil_stats.items() if k is not None and k != soil_nodata}
                        
                        # Generate combinations
                        for lc_id, lc_pct in landcover_pct.items():
                            lc_name = landcover_map.get(int(lc_id), f"Unknown-{lc_id}")
                            
                            for soil_id, soil_pct in soil_pct.items():
                                try:
                                    soil_name = soil_map.get(int(soil_id), f"Unknown-{soil_id}")
                                except (ValueError, TypeError):
                                    soil_name = f"Unknown-{soil_id}"
                                
                                # Calculate combined percentage (approximate)
                                # This is a simplification; actual overlap would require pixel-by-pixel analysis
                                combined_pct = lc_pct * soil_pct * 100
                                combined_area_sqm = mesh_area_sqm * (combined_pct / 100)
                                
                                # Create combined name
                                combined_name = f"{lc_name} : {soil_name}"
                                
                                # Look up infiltration parameters
                                param_row = infiltration_params[infiltration_params['Name'] == combined_name]
                                if param_row.empty:
                                    # Try with NoData for soil type
                                    param_row = infiltration_params[infiltration_params['Name'] == f"{lc_name} : NoData"]
                                
                                if not param_row.empty:
                                    curve_number = param_row.iloc[0]['Curve Number']
                                    abstraction_ratio = param_row.iloc[0]['Abstraction Ratio']
                                    min_infiltration_rate = param_row.iloc[0]['Minimum Infiltration Rate']
                                else:
                                    curve_number = None
                                    abstraction_ratio = None
                                    min_infiltration_rate = None
                                
                                all_results.append({
                                    'mesh_name': mesh_name,
                                    'combined_type': combined_name,
                                    'percentage': combined_pct,
                                    'area_sqm': combined_area_sqm,
                                    'area_acres': combined_area_sqm * HdfInfiltration.SQM_TO_ACRE,
                                    'area_sqmiles': combined_area_sqm * HdfInfiltration.SQM_TO_SQMILE,
                                    'curve_number': curve_number,
                                    'abstraction_ratio': abstraction_ratio,
                                    'min_infiltration_rate': min_infiltration_rate
                                })
                    except Exception as e:
                        logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error opening raster files: {str(e)}")
            return pd.DataFrame()
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name, percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df



















    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_infiltration_map(hdf_path: Path = None, ras_object: Any = None) -> dict:
        """Read the infiltration raster map from HDF file
        
        Args:
            hdf_path: Optional path to the HDF file. If not provided, uses first infiltration_hdf_path from rasmap_df
            ras_object: Optional RAS object. If not provided, uses global ras instance
            
        Returns:
            Dictionary mapping raster values to mukeys
        """
        if hdf_path is None:
            if ras_object is None:
                from .RasPrj import ras
                ras_object = ras
            hdf_path = Path(ras_object.rasmap_df.iloc[0]['infiltration_hdf_path'][0])
            
        with h5py.File(hdf_path, 'r') as hdf:
            raster_map_data = hdf['Raster Map'][:]
            return {int(item[0]): item[1].decode('utf-8') for item in raster_map_data}

    @staticmethod
    @log_call
    def calculate_soil_statistics(zonal_stats: list, raster_map: dict) -> pd.DataFrame:
        """Calculate soil statistics from zonal statistics
        
        Args:
            zonal_stats: List of zonal statistics
            raster_map: Dictionary mapping raster values to mukeys
            
        Returns:
            DataFrame with soil statistics including percentages and areas
        """
        
        try:
            from rasterstats import zonal_stats
        except ImportError as e:
            logger.error("Failed to import rasterstats. Please run 'pip install rasterstats' and try again.")
            raise e
        # Initialize areas dictionary
        mukey_areas = {mukey: 0 for mukey in raster_map.values()}
        
        # Calculate total area and mukey areas
        total_area_sqm = 0
        for stat in zonal_stats:
            for raster_val, area in stat.items():
                mukey = raster_map.get(raster_val)
                if mukey:
                    mukey_areas[mukey] += area
                total_area_sqm += area

        # Create DataFrame rows
        rows = []
        for mukey, area_sqm in mukey_areas.items():
            if area_sqm > 0:
                rows.append({
                    'mukey': mukey,
                    'Percentage': (area_sqm / total_area_sqm) * 100,
                    'Area in Acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                    'Area in Square Miles': area_sqm * HdfInfiltration.SQM_TO_SQMILE
                })
        
        return pd.DataFrame(rows)

    @staticmethod
    @log_call
    def get_significant_mukeys(soil_stats: pd.DataFrame, 
                             threshold: float = 1.0) -> pd.DataFrame:
        """Get mukeys with percentage greater than threshold
        
        Args:
            soil_stats: DataFrame with soil statistics
            threshold: Minimum percentage threshold (default 1.0)
            
        Returns:
            DataFrame with significant mukeys and their statistics
        """
        significant = soil_stats[soil_stats['Percentage'] > threshold].copy()
        significant.sort_values('Percentage', ascending=False, inplace=True)
        return significant

    @staticmethod
    @log_call
    def calculate_total_significant_percentage(significant_mukeys: pd.DataFrame) -> float:
        """Calculate total percentage covered by significant mukeys
        
        Args:
            significant_mukeys: DataFrame of significant mukeys
            
        Returns:
            Total percentage covered by significant mukeys
        """
        return significant_mukeys['Percentage'].sum()

    @staticmethod
    @log_call
    def save_statistics(soil_stats: pd.DataFrame, output_path: Path, 
                       include_timestamp: bool = True):
        """Save soil statistics to CSV
        
        Args:
            soil_stats: DataFrame with soil statistics
            output_path: Path to save CSV file
            include_timestamp: Whether to include timestamp in filename
        """
        if include_timestamp:
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            output_path = output_path.with_name(
                f"{output_path.stem}_{timestamp}{output_path.suffix}")
        
        soil_stats.to_csv(output_path, index=False)

    @staticmethod
    @log_call
    @standardize_input
    def get_infiltration_parameters(hdf_path: Path = None, mukey: str = None, ras_object: Any = None) -> dict:
        """Get infiltration parameters for a specific mukey from HDF file
        
        Args:
            hdf_path: Optional path to the HDF file. If not provided, uses first infiltration_hdf_path from rasmap_df
            mukey: Mukey identifier
            ras_object: Optional RAS object. If not provided, uses global ras instance
            
        Returns:
            Dictionary of infiltration parameters
        """
        if hdf_path is None:
            if ras_object is None:
                from .RasPrj import ras
                ras_object = ras
            hdf_path = Path(ras_object.rasmap_df.iloc[0]['infiltration_hdf_path'][0])
            
        with h5py.File(hdf_path, 'r') as hdf:
            if 'Infiltration Parameters' not in hdf:
                raise KeyError("No infiltration parameters found in HDF file")
                
            params = hdf['Infiltration Parameters'][:]
            for row in params:
                if row[0].decode('utf-8') == mukey:
                    return {
                        'Initial Loss (in)': float(row[1]),
                        'Constant Loss Rate (in/hr)': float(row[2]),
                        'Impervious Area (%)': float(row[3])
                    }
        return None

    @staticmethod
    @log_call
    def calculate_weighted_parameters(soil_stats: pd.DataFrame, 
                                   infiltration_params: dict) -> dict:
        """Calculate weighted infiltration parameters based on soil statistics
        
        Args:
            soil_stats: DataFrame with soil statistics
            infiltration_params: Dictionary of infiltration parameters by mukey
            
        Returns:
            Dictionary of weighted average infiltration parameters
        """
        total_weight = soil_stats['Percentage'].sum()
        
        weighted_params = {
            'Initial Loss (in)': 0.0,
            'Constant Loss Rate (in/hr)': 0.0,
            'Impervious Area (%)': 0.0
        }
        
        for _, row in soil_stats.iterrows():
            mukey = row['mukey']
            weight = row['Percentage'] / total_weight
            
            if mukey in infiltration_params:
                for param in weighted_params:
                    weighted_params[param] += (
                        infiltration_params[mukey][param] * weight
                    )
        
        return weighted_params
    

    @staticmethod
    def _get_table_info(hdf_file: h5py.File, table_path: str) -> Tuple[List[str], List[str], List[str]]:
        """Get column names and types from HDF table
        
        Args:
            hdf_file: Open HDF file object
            table_path: Path to table in HDF file
            
        Returns:
            Tuple of (column names, numpy dtypes, column descriptions)
        """
        if table_path not in hdf_file:
            return [], [], []
            
        dataset = hdf_file[table_path]
        dtype = dataset.dtype
        
        # Extract column names and types
        col_names = []
        col_types = []
        col_descs = []
        
        for name in dtype.names:
            col_names.append(name)
            col_types.append(dtype[name].str)
            col_descs.append(name)  # Could be enhanced to get actual descriptions
            
        return col_names, col_types, col_descs


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_landcover_raster_stats(
        geom_hdf_path: Path,
        landcover_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate land cover statistics for each 2D flow area using the area's perimeter.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        landcover_hdf_path : Path, optional
            Path to the land cover HDF file. If None, uses landcover_hdf_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with land cover statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - land_cover: Land cover classification name
            - percentage: Percentage of 2D flow area covered by this land cover type
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
            - mannings_n: Manning's n value for this land cover type
            - percent_impervious: Percent impervious for this land cover type
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
            import rasterio
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas rasterio'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get the landcover HDF path
        if landcover_hdf_path is None:
            if ras_object is None:
                from .RasPrj import ras
                ras_object = ras
            
            # Try to get landcover_hdf_path from rasmap_df
            try:
                landcover_hdf_path = Path(ras_object.rasmap_df.loc[0, 'landcover_hdf_path'][0])
                if not landcover_hdf_path.exists():
                    logger.warning(f"Land cover HDF path from rasmap_df does not exist: {landcover_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving landcover_hdf_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get land cover map (raster to ID mapping)
        try:
            with h5py.File(landcover_hdf_path, 'r') as hdf:
                if '//Raster Map' not in hdf:
                    logger.error(f"No Raster Map found in {landcover_hdf_path}")
                    return pd.DataFrame()
                
                raster_map_data = hdf['//Raster Map'][()]
                raster_map = {int(item[0]): item[1].decode('utf-8').strip() for item in raster_map_data}
                
                # Get land cover variables (mannings_n and percent_impervious)
                variables = {}
                if '//Variables' in hdf:
                    var_data = hdf['//Variables'][()]
                    for row in var_data:
                        name = row[0].decode('utf-8').strip()
                        mannings_n = float(row[1])
                        percent_impervious = float(row[2])
                        variables[name] = {
                            'mannings_n': mannings_n,
                            'percent_impervious': percent_impervious
                        }
        except Exception as e:
            logger.error(f"Error reading land cover data from HDF: {str(e)}")
            return pd.DataFrame()
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Check for the TIF file with same name as HDF
        tif_path = landcover_hdf_path.with_suffix('.tif')
        if not tif_path.exists():
            logger.error(f"No raster file found at {tif_path}")
            return pd.DataFrame()
        
        # List to store all results
        all_results = []
        
        # Read the raster data and info
        try:
            with rasterio.open(tif_path) as src:
                # Get transform directly from rasterio
                transform = src.transform
                no_data = src.nodata if src.nodata is not None else -9999
                
                # Calculate zonal statistics for each 2D flow area
                for _, mesh_row in mesh_areas.iterrows():
                    mesh_name = mesh_row['mesh_name']
                    mesh_geom = mesh_row['geometry']
                    
                    # Get zonal statistics directly using rasterio grid
                    try:
                        stats = zonal_stats(
                            mesh_geom,
                            tif_path,
                            categorical=True,
                            nodata=no_data
                        )[0]
                        
                        # Skip if no stats
                        if not stats:
                            logger.warning(f"No land cover data found for 2D flow area: {mesh_name}")
                            continue
                        
                        # Calculate total area and percentages
                        total_area_sqm = sum(stats.values())
                        
                        # Process each land cover type
                        for raster_val, area_sqm in stats.items():
                            # Skip NoData values
                            if raster_val is None or raster_val == no_data:
                                continue
                                
                            try:
                                # Get land cover name from raster map
                                land_cover = raster_map.get(int(raster_val), f"Unknown-{raster_val}")
                                
                                # Get Manning's n and percent impervious
                                mannings_n = variables.get(land_cover, {}).get('mannings_n', None)
                                percent_impervious = variables.get(land_cover, {}).get('percent_impervious', None)
                                
                                percentage = (area_sqm / total_area_sqm) * 100 if total_area_sqm > 0 else 0
                                
                                all_results.append({
                                    'mesh_name': mesh_name,
                                    'land_cover': land_cover,
                                    'percentage': percentage,
                                    'area_sqm': area_sqm,
                                    'area_acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                                    'area_sqmiles': area_sqm * HdfInfiltration.SQM_TO_SQMILE,
                                    'mannings_n': mannings_n,
                                    'percent_impervious': percent_impervious
                                })
                            except Exception as e:
                                logger.warning(f"Error processing raster value {raster_val}: {e}")
                                continue
                    except Exception as e:
                        logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error opening raster file {tif_path}: {str(e)}")
            return pd.DataFrame()
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name, percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df



'''

THIS FUNCTION IS VERY CLOSE BUT DOES NOT WORK BECAUSE IT DOES NOT PRESERVE THE EXACT STRUCTURE OF THE HDF FILE.
WHEN RAS LOADS THE HDF, IT IGNORES THE DATA IN THE TABLE AND REPLACES IT WITH NULLS.


    @staticmethod
    @log_call
    def set_infiltration_baseoverrides(
        hdf_path: Path,
        infiltration_df: pd.DataFrame
    ) -> Optional[pd.DataFrame]:
        """
        Set base overrides for infiltration parameters in the HDF file while preserving
        the exact structure of the existing dataset.
        
        This function ensures that the HDF structure is maintained exactly as in the
        original file, including field names, data types, and string lengths. It updates
        the values while preserving all dataset attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters with columns matching HDF structure.
            The first column should be 'Name' or 'Land Cover Name'.

        Returns
        -------
        Optional[pd.DataFrame]
            The infiltration DataFrame if successful, None if operation fails
        """
        try:
            # Make a copy to avoid modifying the input DataFrame
            infiltration_df = infiltration_df.copy()
            
            # Check for and rename the first column if needed
            if "Land Cover Name" in infiltration_df.columns:
                name_col = "Land Cover Name"
            else:
                name_col = "Name"
                # Rename 'Name' to 'Land Cover Name' for HDF dataset
                infiltration_df = infiltration_df.rename(columns={"Name": "Land Cover Name"})
                
            table_path = '/Geometry/Infiltration/Base Overrides'
            
            with h5py.File(hdf_path, 'r') as hdf_file_read:
                # Check if dataset exists
                if table_path not in hdf_file_read:
                    logger.warning(f"No infiltration data found in {hdf_path}. Creating new dataset.")
                    # If dataset doesn't exist, use the standard set_infiltration_baseoverrides method
                    return HdfInfiltration.set_infiltration_baseoverrides(hdf_path, infiltration_df)
                
                # Get the exact dtype of the existing dataset
                existing_dtype = hdf_file_read[table_path].dtype
                
                # Extract column names from the existing dataset
                existing_columns = existing_dtype.names
                
                # Check if all columns in the DataFrame exist in the HDF dataset
                for col in infiltration_df.columns:
                    hdf_col = col
                    if col == "Name" and "Land Cover Name" in existing_columns:
                        hdf_col = "Land Cover Name"
                    
                    if hdf_col not in existing_columns:
                        logger.warning(f"Column {col} not found in existing dataset - it will be ignored")
                
                # Get current dataset to preserve structure for non-updated fields
                existing_data = hdf_file_read[table_path][()]
            
            # Create a structured array with the exact same dtype as the existing dataset
            structured_array = np.zeros(len(infiltration_df), dtype=existing_dtype)
            
            # Copy data from DataFrame to structured array, preserving existing structure
            for col in existing_columns:
                df_col = col
                # Map 'Land Cover Name' to 'Name' if needed
                if col == "Land Cover Name" and name_col == "Name":
                    df_col = "Name"
                    
                if df_col in infiltration_df.columns:
                    # Handle string fields - need to maintain exact string length
                    if existing_dtype[col].kind == 'S':
                        # Get the exact string length from dtype
                        max_str_len = existing_dtype[col].itemsize
                        # Convert to bytes with correct length
                        structured_array[col] = infiltration_df[df_col].astype(str).values.astype(f'|S{max_str_len}')
                    else:
                        # Handle numeric fields - ensure correct numeric type
                        if existing_dtype[col].kind in ('f', 'i'):
                            structured_array[col] = infiltration_df[df_col].values.astype(existing_dtype[col])
                        else:
                            # For any other type, just copy as is
                            structured_array[col] = infiltration_df[df_col].values
                else:
                    logger.warning(f"Column {col} not in DataFrame - using default values")
                    # Use zeros for numeric fields or empty strings for string fields
                    if existing_dtype[col].kind == 'S':
                        structured_array[col] = np.array([''] * len(infiltration_df), dtype=f'|S{existing_dtype[col].itemsize}')
            
            # Write back to HDF file
            with h5py.File(hdf_path, 'a') as hdf_file_write:
                # Delete existing dataset
                if table_path in hdf_file_write:
                    del hdf_file_write[table_path]
                
                # Create new dataset with exact same properties as original
                dataset = hdf_file_write.create_dataset(
                    table_path,
                    data=structured_array,
                    dtype=existing_dtype,
                    compression='gzip',
                    compression_opts=1,
                    chunks=(100,),
                    maxshape=(None,)
                )
            
            # Return the DataFrame with columns matching what was actually written
            result_df = pd.DataFrame()
            for col in existing_columns:
                if existing_dtype[col].kind == 'S':
                    # Convert bytes back to string
                    result_df[col] = [val.decode('utf-8').strip() for val in structured_array[col]]
                else:
                    result_df[col] = structured_array[col]
                    
            return result_df

        except Exception as e:
            logger.error(f"Error setting infiltration data in {hdf_path}: {str(e)}")
            return None






'''
==================================================

File: c:\GH\ras-commander\ras_commander\HdfMesh.py
==================================================
"""
A static class for handling mesh-related operations on HEC-RAS HDF files.

This class provides static methods to extract and analyze mesh data from HEC-RAS HDF files,
including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
2D flow area attributes. No instantiation is required to use these methods.

All methods are designed to work with the mesh geometry data stored in
HEC-RAS HDF files, providing functionality to retrieve and process various aspects
of the 2D flow areas and their associated mesh structures.


List of Functions:
-----------------
get_mesh_area_names()
    Returns list of 2D mesh area names
get_mesh_areas()
    Returns 2D flow area perimeter polygons
get_mesh_cell_polygons()
    Returns 2D flow mesh cell polygons
get_mesh_cell_points()
    Returns 2D flow mesh cell center points
get_mesh_cell_faces()
    Returns 2D flow mesh cell faces
get_mesh_area_attributes()
    Returns geometry 2D flow area attributes
get_mesh_face_property_tables()
    Returns Face Property Tables for each Face in all 2D Flow Areas
get_mesh_cell_property_tables()
    Returns Cell Property Tables for each Cell in all 2D Flow Areas

Each function is decorated with @standardize_input and @log_call for consistent
input handling and logging functionality.
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import Polygon, Point, LineString, MultiLineString, MultiPolygon
from shapely.ops import polygonize  # Importing polygonize to resolve the undefined name error
from typing import List, Tuple, Optional, Dict, Any
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfMesh:
    """
    A class for handling mesh-related operations on HEC-RAS HDF files.

    This class provides methods to extract and analyze mesh data from HEC-RAS HDF files,
    including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
    2D flow area attributes.

    Methods in this class are designed to work with the mesh geometry data stored in
    HEC-RAS HDF files, providing functionality to retrieve and process various aspects
    of the 2D flow areas and their associated mesh structures.

    Note: This class relies on HdfBase and HdfUtils for some underlying operations.
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_area_names(hdf_path: Path) -> List[str]:
        """
        Return a list of the 2D mesh area names from the RAS geometry.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        List[str]
            A list of the 2D mesh area names within the RAS geometry.
            Returns an empty list if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/2D Flow Areas" not in hdf_file:
                    return list()
                return list(
                    [
                        HdfUtils.convert_ras_string(n.decode('utf-8'))
                        for n in hdf_file["Geometry/2D Flow Areas/Attributes"][()]["Name"]
                    ]
                )
        except Exception as e:
            logger.error(f"Error reading mesh area names from {hdf_path}: {str(e)}")
            return list()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_areas(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow area perimeter polygons.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow area perimeter polygons if 2D areas exist.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()
                mesh_area_polygons = [
                    Polygon(hdf_file["Geometry/2D Flow Areas/{}/Perimeter".format(n)][()])
                    for n in mesh_area_names
                ]
                return GeoDataFrame(
                    {"mesh_name": mesh_area_names, "geometry": mesh_area_polygons},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading mesh areas from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_cell_polygons(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow mesh cell polygons.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell polygons with columns:
            - mesh_name: name of the mesh area
            - cell_id: unique identifier for each cell
            - geometry: polygon geometry of the cell
            Returns an empty GeoDataFrame if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Get face geometries once
                face_gdf = HdfMesh.get_mesh_cell_faces(hdf_path)
                
                # Pre-allocate lists for better memory efficiency
                all_mesh_names = []
                all_cell_ids = []
                all_geometries = []

                for mesh_name in mesh_area_names:
                    # Get cell face info in one read
                    cell_face_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Face and Orientation Info"][()]
                    cell_face_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Face and Orientation Values"][()][:, 0]
                    
                    # Create face lookup dictionary for this mesh
                    mesh_faces_dict = dict(face_gdf[face_gdf.mesh_name == mesh_name][["face_id", "geometry"]].values)

                    # Process each cell
                    for cell_id, (start, length) in enumerate(cell_face_info[:, :2]):
                        face_ids = cell_face_values[start:start + length]
                        face_geoms = [mesh_faces_dict[face_id] for face_id in face_ids]
                        
                        # Create polygon
                        polygons = list(polygonize(face_geoms))
                        if polygons:
                            all_mesh_names.append(mesh_name)
                            all_cell_ids.append(cell_id)
                            all_geometries.append(Polygon(polygons[0]))

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "cell_id": all_cell_ids,
                        "geometry": all_geometries
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell polygons from {hdf_path}: {str(e)}")
            return GeoDataFrame()
        
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cell_points(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow mesh cell center points.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell center points.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()
                
                # Pre-allocate lists
                all_mesh_names = []
                all_cell_ids = []
                all_points = []

                for mesh_name in mesh_area_names:
                    # Get all cell centers in one read
                    cell_centers = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"][()]
                    cell_count = len(cell_centers)
                    
                    # Extend lists efficiently
                    all_mesh_names.extend([mesh_name] * cell_count)
                    all_cell_ids.extend(range(cell_count))
                    all_points.extend(Point(coords) for coords in cell_centers)

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "cell_id": all_cell_ids,
                        "geometry": all_points
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell points from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cell_faces(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow mesh cell faces.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell faces.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Pre-allocate lists
                all_mesh_names = []
                all_face_ids = []
                all_geometries = []

                for mesh_name in mesh_area_names:
                    # Read all data at once
                    facepoints_index = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Perimeter Values"][()]

                    # Process each face
                    for face_id, ((pnt_a_idx, pnt_b_idx), (start_row, count)) in enumerate(zip(facepoints_index, faces_perim_info)):
                        coords = [facepoints_coords[pnt_a_idx]]
                        
                        if count > 0:
                            coords.extend(faces_perim_values[start_row:start_row + count])
                            
                        coords.append(facepoints_coords[pnt_b_idx])
                        
                        all_mesh_names.append(mesh_name)
                        all_face_ids.append(face_id)
                        all_geometries.append(LineString(coords))

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "face_id": all_face_ids,
                        "geometry": all_geometries
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell faces from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_area_attributes(hdf_path: Path) -> pd.DataFrame:
        """
        Return geometry 2D flow area attributes from a HEC-RAS HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing the 2D flow area attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                d2_flow_area = hdf_file.get("Geometry/2D Flow Areas/Attributes")
                if d2_flow_area is not None and isinstance(d2_flow_area, h5py.Dataset):
                    result = {}
                    for name in d2_flow_area.dtype.names:
                        try:
                            value = d2_flow_area[name][()]
                            if isinstance(value, bytes):
                                value = value.decode('utf-8')  # Decode as UTF-8
                            result[name] = value if not isinstance(value, bytes) else value.decode('utf-8')
                        except Exception as e:
                            logger.warning(f"Error converting attribute '{name}': {str(e)}")
                    return pd.DataFrame.from_dict(result, orient='index', columns=['Value'])
                else:
                    logger.info("No 2D Flow Area attributes found or invalid dataset.")
                    return pd.DataFrame()  # Return an empty DataFrame
        except Exception as e:
            logger.error(f"Error reading 2D flow area attributes from {hdf_path}: {str(e)}")
            return pd.DataFrame()  # Return an empty DataFrame

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_face_property_tables(hdf_path: Path) -> Dict[str, pd.DataFrame]:
        """
        Extract Face Property Tables for each Face in all 2D Flow Areas.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        Dict[str, pd.DataFrame]
            A dictionary where:
            - keys: mesh area names (str)
            - values: DataFrames with columns:
                - Face ID: unique identifier for each face
                - Z: elevation
                - Area: face area
                - Wetted Perimeter: wetted perimeter length
                - Manning's n: Manning's roughness coefficient
            Returns an empty dictionary if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return {}

                result = {}
                for mesh_name in mesh_area_names:
                    area_elevation_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Area Elevation Info"][()]
                    area_elevation_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Area Elevation Values"][()]
                    
                    face_data = []
                    for face_id, (start_index, count) in enumerate(area_elevation_info):
                        face_values = area_elevation_values[start_index:start_index+count]
                        for z, area, wetted_perimeter, mannings_n in face_values:
                            face_data.append({
                                'Face ID': face_id,
                                'Z': str(z),
                                'Area': str(area), 
                                'Wetted Perimeter': str(wetted_perimeter),
                                "Manning's n": str(mannings_n)
                            })
                    
                    result[mesh_name] = pd.DataFrame(face_data)
                
                return result

        except Exception as e:
            logger.error(f"Error extracting face property tables from {hdf_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_cell_property_tables(hdf_path: Path) -> Dict[str, pd.DataFrame]:
        """
        Extract Cell Property Tables for each Cell in all 2D Flow Areas.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        Dict[str, pd.DataFrame]
            A dictionary where:
            - keys: mesh area names (str)
            - values: DataFrames with columns:
                - Cell ID: unique identifier for each cell
                - Z: elevation
                - Volume: cell volume
                - Surface Area: cell surface area
            Returns an empty dictionary if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return {}

                result = {}
                for mesh_name in mesh_area_names:
                    cell_elevation_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Elevation Volume Info"][()]
                    cell_elevation_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Elevation Volume Values"][()]
                    
                    cell_data = []
                    for cell_id, (start_index, count) in enumerate(cell_elevation_info):
                        cell_values = cell_elevation_values[start_index:start_index+count]
                        for z, volume, surface_area in cell_values:
                            cell_data.append({
                                'Cell ID': cell_id,
                                'Z': str(z),
                                'Volume': str(volume),
                                'Surface Area': str(surface_area)
                            })
                    
                    result[mesh_name] = pd.DataFrame(cell_data)
                
                return result

        except Exception as e:
            logger.error(f"Error extracting cell property tables from {hdf_path}: {str(e)}")
            return {}

==================================================

File: c:\GH\ras-commander\ras_commander\HdfPipe.py
==================================================
"""
Class: HdfPipe

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPipe:
Geometry Retrieval Functions:
- get_pipe_conduits() - Get pipe conduit geometries and attributes
- get_pipe_nodes() - Get pipe node geometries and attributes
- get_pipe_network() - Get complete pipe network data
- get_pipe_profile() - Get elevation profile for a specific conduit
- extract_pipe_network_data() - Extract both nodes and conduits data

Results Retrieval Functions:
- get_pipe_network_timeseries() - Get timeseries data for pipe network variables
- get_pipe_network_summary() - Get summary statistics for pipe networks
- get_pipe_node_timeseries() - Get timeseries data for a specific node
- get_pipe_conduit_timeseries() - Get timeseries data for a specific conduit

Note: All functions use the @standardize_input decorator to validate input paths
and the @log_call decorator for logging function calls.
"""
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import LineString, Point, MultiLineString, Polygon, MultiPolygon
from typing import List, Dict, Any, Optional, Union, Tuple
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger
from .HdfResultsMesh import HdfResultsMesh
import logging  

logger = get_logger(__name__)

class HdfPipe:
    """
    Static methods for handling pipe network data from HEC-RAS HDF files.

    Contains methods for:
    - Geometry retrieval (nodes, conduits, networks, profiles)
    - Results retrieval (timeseries and summary data)

    All methods use @standardize_input for path validation and @log_call
    """

    # Geometry Retrieval Functions
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_conduits(hdf_path: Path, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Extracts pipe conduit geometries and attributes from HDF5 file.

        Parameters:
            hdf_path: Path to the HDF5 file
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame with columns:
            - Attributes from HDF5
            - Polyline: LineString geometries
            - Terrain_Profiles: List of (station, elevation) tuples
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Conduits/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Polyline Data ---
            polyline_info = group['Polyline Info'][:]  # Shape (132,4) - point_start_idx, point_count, part_start_idx, part_count
            polyline_points = group['Polyline Points'][:]  # Shape (396,2) - x,y coordinates
            
            polyline_geometries = []
            for info in polyline_info:
                point_start_idx = info[0]
                point_count = info[1]
                
                # Extract coordinates for this polyline directly using start index and count
                coords = polyline_points[point_start_idx:point_start_idx + point_count]
                
                if len(coords) < 2:
                    polyline_geometries.append(None)
                else:
                    polyline_geometries.append(LineString(coords))
            
            # --- Read Terrain Profiles Data ---
            terrain_info = group['Terrain Profiles Info'][:]
            terrain_values = group['Terrain Profiles Values'][:]
            
            # Create a list of (Station, Elevation) tuples for Terrain Profiles
            terrain_coords = list(zip(terrain_values[:, 0], terrain_values[:, 1]))
            
            terrain_profiles_list: List[List[Tuple[float, float]]] = []
            
            for i in range(len(terrain_info)):
                info = terrain_info[i]
                start_idx = info[0]
                count = info[1]
                
                # Extract (Station, Elevation) pairs
                segment = terrain_coords[start_idx : start_idx + count]
                
                terrain_profiles_list.append(segment)  # Store the list of (Station, Elevation) tuples
            
            # --- Combine Data into GeoDataFrame ---
            attr_df['Polyline'] = polyline_geometries
            attr_df['Terrain_Profiles'] = terrain_profiles_list
            
            # Initialize GeoDataFrame with Polyline geometries
            gdf = gpd.GeoDataFrame(attr_df, geometry='Polyline', crs=crs)
            
            return gdf


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_nodes(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for Pipe Node points and their attributes from an HDF5 file.
        
        Parameters:
        - hdf_path: Path to the HDF5 file.
        
        Returns:
        - A GeoDataFrame containing pipe node attributes and their geometries.
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Nodes/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns  # Changed 'S' to object
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Points Data ---
            points = group['Points'][:]
            # Create Shapely Point geometries
            geometries = [Point(xy) for xy in points]
            
            # --- Combine Attributes and Geometries into GeoDataFrame ---
            gdf = gpd.GeoDataFrame(attr_df, geometry=geometries)
            
            return gdf
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network(hdf_path: Path, pipe_network_name: Optional[str] = None, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for a pipe network's geometry.

        Parameters:
            hdf_path: Path to the HDF5 file
            pipe_network_name: Name of network (uses first if None)
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame containing:
            - Cell polygons (primary geometry)
            - Face polylines
            - Node points
            - Associated attributes
        """
        with h5py.File(hdf_path, 'r') as f:
            pipe_networks_group = f['/Geometry/Pipe Networks/']
            
            # --- Determine Pipe Network to Use ---
            attributes = pipe_networks_group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode 'Name' from byte strings to UTF-8
            attr_df['Name'] = attr_df['Name'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            if pipe_network_name:
                if pipe_network_name not in attr_df['Name'].values:
                    raise ValueError(f"Pipe network '{pipe_network_name}' not found in the HDF5 file.")
                network_idx = attr_df.index[attr_df['Name'] == pipe_network_name][0]
            else:
                network_idx = 0  # Default to first network
            
            # Get the name of the selected pipe network
            selected_network_name = attr_df.at[network_idx, 'Name']
            logging.info(f"Selected Pipe Network: {selected_network_name}")
            
            # Access the selected pipe network group
            network_group_path = f"/Geometry/Pipe Networks/{selected_network_name}/"
            network_group = f[network_group_path]
            
            # --- Helper Functions ---
            def decode_bytes(df: pd.DataFrame) -> pd.DataFrame:
                """Decode byte string columns to UTF-8."""
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                return df
            
            def build_polygons(info, parts, points) -> List[Optional[Polygon or MultiPolygon]]:
                """Build Shapely Polygon or MultiPolygon geometries from HDF5 datasets."""
                poly_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    cell_info = info[i]
                    point_start_idx = cell_info[0]
                    point_count = cell_info[1]
                    part_start_idx = cell_info[2]
                    part_count = cell_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = poly_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 3:
                            continue  # Not a valid polygon part
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        try:
                            geometries.append(Polygon(parts_list[0]))
                        except ValueError:
                            geometries.append(None)
                    else:
                        try:
                            geometries.append(MultiPolygon([Polygon(p) for p in parts_list if len(p) >= 3]))
                        except ValueError:
                            geometries.append(None)
                return geometries
            
            def build_multilinestring(info, parts, points) -> List[Optional[LineString or MultiLineString]]:
                """Build Shapely LineString or MultiLineString geometries from HDF5 datasets."""
                line_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    face_info = info[i]
                    point_start_idx = face_info[0]
                    point_count = face_info[1]
                    part_start_idx = face_info[2]
                    part_count = face_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = line_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 2:
                            continue  # Cannot form LineString with fewer than 2 points
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        geometries.append(LineString(parts_list[0]))
                    else:
                        geometries.append(MultiLineString(parts_list))
                return geometries
            
            # --- Read and Process Cell Polygons ---
            cell_polygons_info = network_group['Cell Polygons Info'][:]
            cell_polygons_parts = network_group['Cell Polygons Parts'][:]
            cell_polygons_points = network_group['Cell Polygons Points'][:]
            
            cell_polygons_geometries = build_polygons(cell_polygons_info, cell_polygons_parts, cell_polygons_points)
            
            # --- Read and Process Face Polylines ---
            face_polylines_info = network_group['Face Polylines Info'][:]
            face_polylines_parts = network_group['Face Polylines Parts'][:]
            face_polylines_points = network_group['Face Polylines Points'][:]
            
            face_polylines_geometries = build_multilinestring(face_polylines_info, face_polylines_parts, face_polylines_points)
            
            # --- Read and Process Node Points ---
            node_surface_connectivity_group = network_group.get('Node Surface Connectivity', None)
            if node_surface_connectivity_group is not None:
                node_surface_connectivity = node_surface_connectivity_group[:]
            else:
                node_surface_connectivity = None
            
            # Assuming Node Connectivity Info and Values contain node coordinates
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            node_indices = network_group['Node Indices'][:]
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            
            # For simplicity, assuming that node connectivity includes X and Y coordinates
            # This may need to be adjusted based on actual data structure
            # Here, we'll create dummy points as placeholder
            # Replace with actual coordinate extraction logic as per data structure
            # For demonstration, we'll create random points
            # You should replace this with actual data extraction
            # Example:
            # node_points = network_group['Node Coordinates'][:]
            # node_geometries = [Point(x, y) for x, y in node_points]
            
            # Placeholder for node geometries
            # Assuming node_indices contains Node IDs and coordinates
            # Adjust based on actual dataset structure
            # Here, we assume that node_indices has columns: [Node ID, X, Y]
            # But based on the log, Node Surface Connectivity has ['Node ID', 'Layer', 'Layer ID', 'Sublayer ID']
            # No coordinates are provided, so we cannot create Point geometries unless coordinates are available elsewhere
            # Therefore, this part may need to be adapted based on actual data
            # For now, we'll skip node points geometries
            node_geometries = [None] * len(node_indices)  # Placeholder
            
            # --- Read and Process Cell Property Table ---
            cell_property_table = network_group['Cell Property Table'][:]
            cell_property_df = pd.DataFrame(cell_property_table)
            
            # Decode byte strings if any
            cell_property_df = decode_bytes(cell_property_df)
            
            # --- Read and Process Cells DS Face Indices ---
            cells_ds_face_info = network_group['Cells DS Face Indices Info'][:]
            cells_ds_face_values = network_group['Cells DS Face Indices Values'][:]
            
            # Create lists of DS Face Indices per cell
            cells_ds_face_indices = []
            for i in range(len(cells_ds_face_info)):
                info = cells_ds_face_info[i]
                start_idx, count = info
                indices = cells_ds_face_values[start_idx : start_idx + count]
                cells_ds_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Face Indices ---
            cells_face_info = network_group['Cells Face Indices Info'][:]
            cells_face_values = network_group['Cells Face Indices Values'][:]
            
            # Create lists of Face Indices per cell
            cells_face_indices = []
            for i in range(len(cells_face_info)):
                info = cells_face_info[i]
                start_idx, count = info
                indices = cells_face_values[start_idx : start_idx + count]
                cells_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Minimum Elevations ---
            cells_min_elevations = network_group['Cells Minimum Elevations'][:]
            cells_min_elevations_df = pd.DataFrame(cells_min_elevations, columns=['Minimum_Elevation'])
            
            # --- Read and Process Cells Node and Conduit IDs ---
            cells_node_conduit_ids = network_group['Cells Node and Conduit IDs'][:]
            cells_node_conduit_df = pd.DataFrame(cells_node_conduit_ids, columns=['Node_ID', 'Conduit_ID'])
            
            # --- Read and Process Cells US Face Indices ---
            cells_us_face_info = network_group['Cells US Face Indices Info'][:]
            cells_us_face_values = network_group['Cells US Face Indices Values'][:]
            
            # Create lists of US Face Indices per cell
            cells_us_face_indices = []
            for i in range(len(cells_us_face_info)):
                info = cells_us_face_info[i]
                start_idx, count = info
                indices = cells_us_face_values[start_idx : start_idx + count]
                cells_us_face_indices.append(indices.tolist())
            
            # --- Read and Process Conduit Indices ---
            conduit_indices = network_group['Conduit Indices'][:]
            conduit_indices_df = pd.DataFrame(conduit_indices, columns=['Conduit_ID'])
            
            # --- Read and Process Face Property Table ---
            face_property_table = network_group['Face Property Table'][:]
            face_property_df = pd.DataFrame(face_property_table)
            
            # Decode byte strings if any
            face_property_df = decode_bytes(face_property_df)
            
            # --- Read and Process Face Conduit ID and Stations ---
            faces_conduit_id_stations = network_group['Faces Conduit ID and Stations'][:]
            faces_conduit_df = pd.DataFrame(faces_conduit_id_stations, columns=['ConduitID', 'ConduitStation', 'CellUS', 'CellDS', 'Elevation'])
            
            # --- Read and Process Node Connectivity Info and Values ---
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            
            # Create lists of connected nodes per node
            node_connectivity = []
            for i in range(len(node_connectivity_info)):
                info = node_connectivity_info[i]
                start_idx, count = info
                connections = node_connectivity_values[start_idx : start_idx + count]
                node_connectivity.append(connections.tolist())
            
            # --- Read and Process Node Indices ---
            node_indices = network_group['Node Indices'][:]
            node_indices_df = pd.DataFrame(node_indices, columns=['Node_ID'])
            
            # --- Read and Process Node Surface Connectivity ---
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            node_surface_connectivity_df = pd.DataFrame(node_surface_connectivity, columns=['Node_ID', 'Layer', 'Layer_ID', 'Sublayer_ID'])
            
            # --- Combine All Cell-Related Data ---
            cells_df = pd.DataFrame({
                'Cell_ID': range(len(cell_polygons_geometries)),
                'Conduit_ID': cells_node_conduit_df['Conduit_ID'],
                'Node_ID': cells_node_conduit_df['Node_ID'],
                'Minimum_Elevation': cells_min_elevations_df['Minimum_Elevation'],
                'DS_Face_Indices': cells_ds_face_indices,
                'Face_Indices': cells_face_indices,
                'US_Face_Indices': cells_us_face_indices,
                'Cell_Property_Info_Index': cell_property_df['Info Index'],
                # Add other cell properties as needed
            })
            
            # Merge with cell property table
            cells_df = cells_df.merge(cell_property_df, left_on='Cell_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Face-Related Data ---
            faces_df = pd.DataFrame({
                'Face_ID': range(len(face_polylines_geometries)),
                'Conduit_ID': faces_conduit_df['ConduitID'],
                'Conduit_Station': faces_conduit_df['ConduitStation'],
                'Cell_US': faces_conduit_df['CellUS'],
                'Cell_DS': faces_conduit_df['CellDS'],
                'Elevation': faces_conduit_df['Elevation'],
                'Face_Property_Info_Index': face_property_df['Info Index'],
                # Add other face properties as needed
            })
            
            # Merge with face property table
            faces_df = faces_df.merge(face_property_df, left_on='Face_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Node-Related Data ---
            nodes_df = pd.DataFrame({
                'Node_ID': node_indices_df['Node_ID'],
                'Connected_Nodes': node_connectivity,
                # Add other node properties as needed
            })
            
            # Merge with node surface connectivity
            nodes_df = nodes_df.merge(node_surface_connectivity_df, on='Node_ID', how='left')
            
            # --- Create GeoDataFrame ---
            # Main DataFrame will be cells with their polygons
            cells_df['Cell_Polygon'] = cell_polygons_geometries
            
            # Add face polylines as a separate column (list of geometries)
            cells_df['Face_Polylines'] = cells_df['Face_Indices'].apply(lambda indices: [face_polylines_geometries[i] for i in indices if i < len(face_polylines_geometries)])
            
            # Add node points if geometries are available
            # Currently, node_geometries are placeholders (None). Replace with actual geometries if available.
            cells_df['Node_Point'] = cells_df['Node_ID'].apply(lambda nid: node_geometries[nid] if nid < len(node_geometries) else None)
            
            # Initialize GeoDataFrame with Cell Polygons
            gdf = gpd.GeoDataFrame(cells_df, geometry='Cell_Polygon', crs=crs)
            
            # Optionally, add Face Polylines and Node Points as separate columns
            # Note: GeoPandas primarily supports one geometry column, so these are stored as object columns
            gdf['Face_Polylines'] = cells_df['Face_Polylines']
            gdf['Node_Point'] = cells_df['Node_Point']
            
            # You can further expand this GeoDataFrame by merging with faces_df and nodes_df if needed
            
            return gdf
        
        
        
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_profile(hdf_path: Path, conduit_id: int) -> pd.DataFrame:
        """
        Extract the profile data for a specific pipe conduit.

        Args:
            hdf_path (Path): Path to the HDF file.
            conduit_id (int): ID of the conduit to extract profile for.

        Returns:
            pd.DataFrame: DataFrame containing the pipe profile data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
            IndexError: If the specified conduit_id is out of range.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Get conduit info
                terrain_profiles_info = hdf['/Geometry/Pipe Conduits/Terrain Profiles Info'][()]
                
                if conduit_id >= len(terrain_profiles_info):
                    raise IndexError(f"conduit_id {conduit_id} is out of range")

                start, count = terrain_profiles_info[conduit_id]

                # Extract profile data
                profile_values = hdf['/Geometry/Pipe Conduits/Terrain Profiles Values'][start:start+count]

                # Create DataFrame
                df = pd.DataFrame(profile_values, columns=['Station', 'Elevation'])

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except IndexError as e:
            logger.error(f"Invalid conduit_id: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe profile data: {e}")
            raise
        
        
   









# RESULTS FUNCTIONS: 

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract results summary data for pipe networks from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pipe network summary data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pipe Network"
                if summary_path not in hdf:
                    logger.warning("Pipe Network summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame
                df = pd.DataFrame(summary_data)

                # Convert column names
                df.columns = [col.decode('utf-8') for col in df.columns]

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network summary data: {e}")
            raise




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_node(plan_hdf_path: Path, node_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific node.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        node_id : int
            ID of the node to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Depth
            - Drop Inlet Flow
            - Water Surface
        """
        try:
            node_variables = ["Nodes/Depth", "Nodes/Drop Inlet Flow", "Nodes/Water Surface"]
            node_data = {}

            for variable in node_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                node_data[variable] = data.sel(location=node_id)
            
            return node_data
        except Exception as e:
            logger.error(f"Error extracting time series data for node {node_id}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_conduit(plan_hdf_path: Path, conduit_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific conduit.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        conduit_id : int
            ID of the conduit to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Pipe Flow (US/DS)
            - Velocity (US/DS)
        """
        try:
            conduit_variables = ["Pipes/Pipe Flow DS", "Pipes/Pipe Flow US", 
                                "Pipes/Vel DS", "Pipes/Vel US"]
            conduit_data = {}

            for variable in conduit_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                conduit_data[variable] = data.sel(location=conduit_id)
            
            return conduit_data
        except Exception as e:
            logger.error(f"Error extracting time series data for conduit {conduit_id}: {str(e)}")
            raise


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_timeseries(hdf_path: Path, variable: str) -> xr.DataArray:
        """
        Extracts timeseries data for a pipe network variable.

        Parameters:
            hdf_path: Path to the HDF5 file
            variable: Variable name to extract. Valid options:
                - Cell: Courant, Water Surface
                - Face: Flow, Velocity, Water Surface
                - Pipes: Pipe Flow (DS/US), Vel (DS/US)
                - Nodes: Depth, Drop Inlet Flow, Water Surface

        Returns:
            xarray.DataArray with dimensions (time, location)
        """
        valid_variables = [
            "Cell Courant", "Cell Water Surface", "Face Flow", "Face Velocity",
            "Face Water Surface", "Pipes/Pipe Flow DS", "Pipes/Pipe Flow US",
            "Pipes/Vel DS", "Pipes/Vel US", "Nodes/Depth", "Nodes/Drop Inlet Flow",
            "Nodes/Water Surface"
        ]

        if variable not in valid_variables:
            raise ValueError(f"Invalid variable. Must be one of: {', '.join(valid_variables)}")

        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract timeseries data
                data_path = f"/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pipe Networks/Davis/{variable}"
                data = hdf[data_path][()]

                # Extract time information using the correct method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'location'],
                    coords={'time': time, 'location': range(data.shape[1])},
                    name=variable
                )

                # Add attributes
                da.attrs['units'] = hdf[data_path].attrs.get('Units', b'').decode('utf-8')
                da.attrs['variable'] = variable

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network timeseries data: {e}")
            raise





==================================================

File: c:\GH\ras-commander\ras_commander\HdfPlan.py
==================================================
"""
Class: HdfPlan

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.


- get_plan_start_time()
- get_plan_end_time()
- get_plan_timestamps_list()     
- get_plan_information()
- get_plan_parameters()
- get_plan_met_precip()
- get_geometry_information()






"""

import h5py
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import re
import numpy as np

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfPlan:
    """
    A class for handling HEC-RAS plan HDF files.

    Provides static methods for extracting data from HEC-RAS plan HDF files including 
    simulation times, plan information, and geometry attributes. All methods use 
    @standardize_input for handling different input types and @log_call for logging.

    Note: This code is partially derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    under MIT license.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_start_time(hdf_path: Path) -> datetime:
        """
        Get the plan start time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan start time in UTC format.

        Raises:
            ValueError: If there's an error reading the plan start time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_simulation_start_time(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan start time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_end_time(hdf_path: Path) -> datetime:
        """
        Get the plan end time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan end time.

        Raises:
            ValueError: If there's an error reading the plan end time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info = hdf_file.get('Plan Data/Plan Information')
                if plan_info is None:
                    raise ValueError("Plan Information not found in HDF file")
                time_str = plan_info.attrs.get('Simulation End Time')
                return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))
        except Exception as e:
            raise ValueError(f"Failed to get plan end time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_timestamps_list(hdf_path: Path) -> List[datetime]:
        """
        Get the list of output timestamps from the plan simulation.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            List[datetime]: Chronological list of simulation output timestamps in UTC.

        Raises:
            ValueError: If there's an error retrieving the plan timestamps.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_unsteady_timestamps(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan timestamps: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_information(hdf_path: Path) -> Dict:
        """
        Get plan information from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Plan information including simulation times, flow regime, 
                computation settings, etc.

        Raises:
            ValueError: If there's an error retrieving the plan information.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info_path = "Plan Data/Plan Information"
                if plan_info_path not in hdf_file:
                    raise ValueError(f"Plan Information not found in {hdf_path}")
                
                attrs = {}
                for key in hdf_file[plan_info_path].attrs.keys():
                    value = hdf_file[plan_info_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            raise ValueError(f"Failed to get plan information attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_parameters(hdf_path: Path) -> pd.DataFrame:
        """
        Get plan parameter attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: A DataFrame containing the plan parameters with columns:
                - Parameter: Name of the parameter
                - Value: Value of the parameter (decoded if byte string)
                - Plan: Plan number (01-99) extracted from the filename (ProjectName.pXX.hdf)

        Raises:
            ValueError: If there's an error retrieving the plan parameter attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_params_path = "Plan Data/Plan Parameters"
                if plan_params_path not in hdf_file:
                    raise ValueError(f"Plan Parameters not found in {hdf_path}")
                
                # Extract parameters
                params_dict = {}
                for key in hdf_file[plan_params_path].attrs.keys():
                    value = hdf_file[plan_params_path].attrs[key]
                    
                    # Handle different types of values
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    elif isinstance(value, np.ndarray):
                        # Handle array values
                        if value.dtype.kind in {'S', 'a'}:  # Array of byte strings
                            value = [v.decode('utf-8') if isinstance(v, bytes) else v for v in value]
                        else:
                            value = value.tolist()  # Convert numpy array to list
                        
                        # If it's a single-item list, extract the value
                        if len(value) == 1:
                            value = value[0]
                    
                    params_dict[key] = value
                
                # Create DataFrame from parameters
                df = pd.DataFrame.from_dict(params_dict, orient='index', columns=['Value'])
                df.index.name = 'Parameter'
                df = df.reset_index()
                
                # Extract plan number from filename
                filename = Path(hdf_path).name
                plan_match = re.search(r'\.p(\d{2})\.', filename)
                if plan_match:
                    plan_num = plan_match.group(1)
                else:
                    plan_num = "00"  # Default if no match found
                    logger.warning(f"Could not extract plan number from filename: {filename}")
                
                df['Plan'] = plan_num
                
                # Reorder columns to put Plan first
                df = df[['Plan', 'Parameter', 'Value']]
                
                return df

        except Exception as e:
            raise ValueError(f"Failed to get plan parameter attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_met_precip(hdf_path: Path) -> Dict:
        """
        Get precipitation attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Precipitation attributes including method, time series data,
                and spatial distribution if available. Returns empty dict if
                no precipitation data exists.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                precip_path = "Event Conditions/Meteorology/Precipitation"
                if precip_path not in hdf_file:
                    logger.error(f"Precipitation data not found in {hdf_path}")
                    return {}
                
                attrs = {}
                for key in hdf_file[precip_path].attrs.keys():
                    value = hdf_file[precip_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            logger.error(f"Failed to get precipitation attributes: {str(e)}")
            return {}
        
    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_geometry_information(hdf_path: Path) -> pd.DataFrame:
        """
        Get root level geometry attributes from the HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: DataFrame with geometry attributes including Creation Date/Time,
                        Version, Units, and Projection information.

        Raises:
            ValueError: If Geometry group is missing or there's an error reading attributes.
        """
        logger.info(f"Getting geometry attributes from {hdf_path}")
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                geom_attrs_path = "Geometry"
                logger.info(f"Checking for Geometry group in {hdf_path}")
                if geom_attrs_path not in hdf_file:
                    logger.error(f"Geometry group not found in {hdf_path}")
                    raise ValueError(f"Geometry group not found in {hdf_path}")

                attrs = {}
                geom_group = hdf_file[geom_attrs_path]
                logger.info("Getting root level geometry attributes")
                # Get root level geometry attributes only
                for key, value in geom_group.attrs.items():
                    if isinstance(value, bytes):
                        try:
                            value = HdfUtils.convert_ras_string(value)
                        except UnicodeDecodeError:
                            logger.warning(f"Failed to decode byte string for root attribute {key}")
                            continue
                    attrs[key] = value
                    logger.debug(f"Geometry attribute: {key} = {value}")

                logger.info(f"Successfully extracted {len(attrs)} root level geometry attributes")
                return pd.DataFrame.from_dict(attrs, orient='index', columns=['Value'])

        except (OSError, RuntimeError) as e:
            logger.error(f"Failed to read HDF file {hdf_path}: {str(e)}")
            raise ValueError(f"Failed to read HDF file {hdf_path}: {str(e)}")
        except Exception as e:
            logger.error(f"Failed to get geometry attributes: {str(e)}")
            raise ValueError(f"Failed to get geometry attributes: {str(e)}")



==================================================

File: c:\GH\ras-commander\ras_commander\HdfPlot.py
==================================================
"""
Class: HdfPlot

A collection of static methods for plotting general HDF data from HEC-RAS models.
"""

import matplotlib.pyplot as plt
import pandas as pd
import geopandas as gpd
from typing import Optional, Union, Tuple
from .Decorators import log_call, standardize_input
from .HdfUtils import HdfUtils

class HdfPlot:
    """
    A class containing static methods for plotting general HDF data from HEC-RAS models.
    
    This class provides plotting functionality for HDF data, focusing on
    geometric elements like cell polygons and time series data.
    """

    @staticmethod
    @log_call
    def plot_mesh_cells(
        cell_polygons_df: pd.DataFrame, ## THIS IS A GEODATAFRAME - NEED TO EDIT BOTH ARGUMENT AND USAGE
        projection: str,
        title: str = '2D Flow Area Mesh Cells',
        figsize: Tuple[int, int] = (12, 8)
    ) -> Optional[gpd.GeoDataFrame]:
        """
        Plots the mesh cells from the provided DataFrame and returns the GeoDataFrame.

        Args:
            cell_polygons_df (pd.DataFrame): DataFrame containing cell polygons.
            projection (str): The coordinate reference system to assign to the GeoDataFrame.
            title (str, optional): Plot title. Defaults to '2D Flow Area Mesh Cells'.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 8).

        Returns:
            Optional[gpd.GeoDataFrame]: GeoDataFrame containing the mesh cells, or None if no cells found.
        """
        if cell_polygons_df.empty:
            print("No Cell Polygons found.")
            return None

        # Convert any datetime columns to strings using HdfUtils
        cell_polygons_df = HdfUtils.convert_df_datetimes_to_str(cell_polygons_df)
        
        cell_polygons_gdf = gpd.GeoDataFrame(cell_polygons_df, crs=projection)

        print("Cell Polygons CRS:", cell_polygons_gdf.crs)
        display(cell_polygons_gdf.head())

        fig, ax = plt.subplots(figsize=figsize)
        cell_polygons_gdf.plot(ax=ax, edgecolor='blue', facecolor='none')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()

        return cell_polygons_gdf

    @staticmethod
    @log_call
    def plot_time_series(
        df: pd.DataFrame,
        x_col: str,
        y_col: str,
        title: str = None,
        figsize: Tuple[int, int] = (12, 6)
    ) -> None:
        """
        Plots time series data from HDF results.

        Args:
            df (pd.DataFrame): DataFrame containing the time series data
            x_col (str): Name of the column containing x-axis data (usually time)
            y_col (str): Name of the column containing y-axis data
            title (str, optional): Plot title. Defaults to None.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 6).
        """
        # Convert any datetime columns to strings
        df = HdfUtils.convert_df_datetimes_to_str(df)
        
        fig, ax = plt.subplots(figsize=figsize)
        df.plot(x=x_col, y=y_col, ax=ax)
        
        if title:
            ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()
    
    
    
    
    
    
    
    
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\HdfPump.py
==================================================
"""
Class: HdfPump

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPump:
- get_pump_stations()
- get_pump_groups()
- get_pump_station_timeseries()
- get_pump_station_summary()
- get_pump_operation_timeseries()


"""


import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import Point
from typing import List, Dict, Any, Optional, Union
from .HdfUtils import HdfUtils
from .HdfBase import HdfBase
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfPump:
    """
    A class for handling pump station related data from HEC-RAS HDF files.

    This class provides static methods to extract and process pump station data, including:
    - Pump station locations and attributes
    - Pump group configurations and efficiency curves
    - Time series results for pump operations
    - Summary statistics for pump stations

    All methods are static and designed to work with HEC-RAS HDF files containing pump data.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_stations(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Extract pump station data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing pump station data with columns:
                - geometry: Point geometry of pump station location
                - station_id: Unique identifier for each pump station
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump station datasets are not found in the HDF file.
            Exception: If there are errors processing the pump station data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump station data
                attributes = hdf['/Geometry/Pump Stations/Attributes'][()]
                points = hdf['/Geometry/Pump Stations/Points'][()]

                # Create geometries
                geometries = [Point(x, y) for x, y in points]

                # Create GeoDataFrame
                gdf = gpd.GeoDataFrame(geometry=geometries)
                gdf['station_id'] = range(len(gdf))

                # Add attributes and decode byte strings
                attr_df = pd.DataFrame(attributes)
                string_columns = attr_df.select_dtypes([object]).columns
                for col in string_columns:
                    attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                
                for col in attr_df.columns:
                    gdf[col] = attr_df[col]

                # Set CRS if available
                crs = HdfBase.get_projection(hdf_path)
                if crs:
                    gdf.set_crs(crs, inplace=True)

                return gdf

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_groups(hdf_path: Path) -> pd.DataFrame:
        """
        Extract pump group data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump group data with columns:
                - efficiency_curve_start: Starting index of efficiency curve data
                - efficiency_curve_count: Number of points in efficiency curve
                - efficiency_curve: List of efficiency curve values
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump group datasets are not found in the HDF file.
            Exception: If there are errors processing the pump group data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump group data
                attributes = hdf['/Geometry/Pump Stations/Pump Groups/Attributes'][()]
                efficiency_curves_info = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Info'][()]
                efficiency_curves_values = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Values'][()]

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(attributes)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                # Add efficiency curve data
                df['efficiency_curve_start'] = efficiency_curves_info[:, 0]
                df['efficiency_curve_count'] = efficiency_curves_info[:, 1]

                # Process efficiency curves
                def get_efficiency_curve(start, count):
                    return efficiency_curves_values[start:start+count].tolist()

                df['efficiency_curve'] = df.apply(lambda row: get_efficiency_curve(row['efficiency_curve_start'], row['efficiency_curve_count']), axis=1)

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump group data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_station_timeseries(hdf_path: Path, pump_station: str) -> xr.DataArray:
        """
        Extract timeseries results data for a specific pump station.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.
            pump_station (str): Name or identifier of the pump station.

        Returns:
            xr.DataArray: DataArray containing the timeseries data with dimensions:
                - time: Timestamps of simulation
                - variable: Variables including ['Flow', 'Stage HW', 'Stage TW', 
                           'Pump Station', 'Pumps on']
            Attributes include units and pump station name.

        Raises:
            KeyError: If required datasets are not found in the HDF file.
            ValueError: If the specified pump station name is not found.
            Exception: If there are errors processing the timeseries data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Check if the pump station exists
                pumping_stations_path = "/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pumping Stations"
                if pump_station not in hdf[pumping_stations_path]:
                    raise ValueError(f"Pump station '{pump_station}' not found in HDF file")

                # Extract timeseries data
                data_path = f"{pumping_stations_path}/{pump_station}/Structure Variables"
                data = hdf[data_path][()]

                # Extract time information - Updated to use new method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'variable'],
                    coords={'time': time, 'variable': ['Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on']},
                    name=pump_station
                )

                # Add attributes and decode byte strings
                units = hdf[data_path].attrs.get('Variable_Unit', b'')
                da.attrs['units'] = units.decode('utf-8') if isinstance(units, bytes) else units
                da.attrs['pump_station'] = pump_station

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except ValueError as e:
            logger.error(str(e))
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station timeseries data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_station_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract summary statistics and performance data for all pump stations.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump station summary data including
                operational statistics and performance metrics. Returns empty DataFrame
                if no summary data is found.

        Raises:
            KeyError: If the summary dataset is not found in the HDF file.
            Exception: If there are errors processing the summary data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pump Station"
                if summary_path not in hdf:
                    logger.warning("Pump Station summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame and decode byte strings
                df = pd.DataFrame(summary_data)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station summary data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_operation_timeseries(hdf_path: Path, pump_station: str) -> pd.DataFrame:
        """
        Extract detailed pump operation results data for a specific pump station.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.
            pump_station (str): Name or identifier of the pump station.

        Returns:
            pd.DataFrame: DataFrame containing pump operation data with columns:
                - Time: Simulation timestamps
                - Flow: Pump flow rate
                - Stage HW: Headwater stage
                - Stage TW: Tailwater stage
                - Pump Station: Station identifier
                - Pumps on: Number of active pumps

        Raises:
            KeyError: If required datasets are not found in the HDF file.
            ValueError: If the specified pump station name is not found.
            Exception: If there are errors processing the operation data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Check if the pump station exists
                pump_stations_path = "/Results/Unsteady/Output/Output Blocks/DSS Profile Output/Unsteady Time Series/Pumping Stations"
                if pump_station not in hdf[pump_stations_path]:
                    raise ValueError(f"Pump station '{pump_station}' not found in HDF file")

                # Extract pump operation data
                data_path = f"{pump_stations_path}/{pump_station}/Structure Variables"
                data = hdf[data_path][()]

                # Extract time information - Updated to use new method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(data, columns=['Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on'])
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                    
                df['Time'] = time

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except ValueError as e:
            logger.error(str(e))
            raise
        except Exception as e:
            logger.error(f"Error extracting pump operation data: {e}")
            raise
==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsBreach.py
==================================================
"""
HdfResultsBreach: Dam breach results extraction from HEC-RAS HDF files.

This module provides methods for extracting breach results from HDF output files,
including time series data, summary statistics, and breach geometry evolution.

Architectural Note:
    - HdfResultsBreach: Breach RESULTS from HDF files (.p##.hdf)
    - RasBreach: Breach PARAMETERS from plan files (.p##)
    - HdfStruc: Structure data from HDF files (non-breach specific)

This class focuses exclusively on HDF results extraction. For reading/writing breach
parameters in plan files, use RasBreach class.

Classes:
    HdfResultsBreach: Static methods for breach results extraction

Key Methods:
    - get_structure_variables(): Extract structure flow variables (Total Flow, HW, TW)
    - get_breaching_variables(): Extract breach geometry progression
    - get_breach_timeseries(): Combined breach + structure time series (primary method)
    - get_breach_summary(): Summary statistics (peaks, timing, final geometry)

Examples:
    >>> from ras_commander import HdfResultsBreach
    >>>
    >>> # Extract complete breach time series
    >>> df = HdfResultsBreach.get_breach_timeseries("02", "Laxton_Dam")
    >>>
    >>> # Get summary statistics
    >>> summary = HdfResultsBreach.get_breach_summary("02")
    >>>
    >>> # Plot breach evolution
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(df['datetime'], df['bottom_width'])
    >>> plt.ylabel('Breach Width (ft)')

Author: ras-commander development team
Date: 2025
"""

from typing import Union, Optional
from pathlib import Path
import h5py
import pandas as pd
import numpy as np

from .Decorators import standardize_input, log_call
from .HdfBase import HdfBase
from .LoggingConfig import get_logger
from .RasPrj import ras

logger = get_logger(__name__)


class HdfResultsBreach:
    """
    Handles dam breach results extraction from HEC-RAS HDF files.

    This class provides comprehensive breach results extraction including:
    - Time series data (flow, water levels, breach geometry)
    - Summary statistics (peak values, timing)
    - Structure-level flow variables

    All methods are static and designed for plan-number-based access
    via the @standardize_input decorator.

    Architectural Note:
        - Use HdfResultsBreach for extracting breach RESULTS from HDF files
        - Use RasBreach for reading/writing breach PARAMETERS in plan files
        - Use HdfStruc for structure listings and metadata
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_structure_variables(hdf_path: Path, structure_name: str = None, *,
                                ras_object=None) -> pd.DataFrame:
        """
        Extract structure-level flow variables (Total Flow, Weir Flow, HW, TW).

        This is the primary time series for overall structure performance.
        Available for all SA/2D connections (with or without breach).

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure name. If None, returns all structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Time series data with columns:
            - datetime: Timestamp
            - structure: Structure name (if multiple structures)
            - total_flow: Total flow through structure (cfs or m³/s)
            - weir_flow: Flow over weir (cfs or m³/s)
            - hw: Headwater elevation at representative station (ft or m)
            - tw: Tailwater elevation at representative station (ft or m)

        Examples
        --------
        >>> # Get all structures
        >>> df = HdfResultsBreach.get_structure_variables("02")

        >>> # Get specific structure
        >>> df = HdfResultsBreach.get_structure_variables("02", "Laxton_Dam")

        >>> # Plot flow hydrograph
        >>> import matplotlib.pyplot as plt
        >>> plt.plot(df['datetime'], df['total_flow'])
        >>> plt.ylabel('Flow (cfs)')

        Notes
        -----
        - HW and TW are at representative stations defined in structure attributes
        - For breach structures, use get_breaching_variables() for breach-specific data
        - Units depend on project unit system (US Customary or SI)
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                sa_conn_path = f"{base_path}/SA 2D Area Conn"

                if sa_conn_path not in hdf_file:
                    logger.warning(f"No SA 2D Area Conn data in {hdf_path.name}")
                    return pd.DataFrame()

                # Get timestamps
                time_stamps = HdfBase.get_unsteady_timestamps(hdf_file)

                # Get structure names
                if structure_name:
                    structures = [structure_name]
                else:
                    from .HdfStruc import HdfStruc
                    structures = HdfStruc.list_sa2d_connections(hdf_path, ras_object=ras_object)

                # Extract data for each structure
                data_list = []
                for struct in structures:
                    struct_path = f"{sa_conn_path}/{struct}"
                    var_path = f"{struct_path}/Structure Variables"

                    if var_path not in hdf_file:
                        logger.warning(f"Structure Variables not found for {struct}")
                        continue

                    # Extract dataset
                    dataset = hdf_file[var_path][:]  # shape: (n_timesteps, 4)

                    # Get variable names and units from attributes
                    if 'Variable_Unit' in hdf_file[var_path].attrs:
                        var_unit = hdf_file[var_path].attrs['Variable_Unit']
                        # var_unit is array of [name, unit] pairs

                    # Create DataFrame for this structure
                    struct_data = pd.DataFrame({
                        'datetime': time_stamps,
                        'total_flow': dataset[:, 0],
                        'weir_flow': dataset[:, 1],
                        'hw': dataset[:, 2],
                        'tw': dataset[:, 3]
                    })

                    if len(structures) > 1:
                        struct_data.insert(1, 'structure', struct)

                    data_list.append(struct_data)

                # Combine all structures
                if data_list:
                    result_df = pd.concat(data_list, ignore_index=True)
                    logger.info(f"Extracted {len(time_stamps)} timesteps for {len(structures)} structure(s)")
                    return result_df
                else:
                    return pd.DataFrame()

        except Exception as e:
            logger.error(f"Error extracting structure variables: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_breaching_variables(hdf_path: Path, structure_name: str = None, *,
                               ras_object=None) -> pd.DataFrame:
        """
        Extract breach-specific geometry progression and flow data.

        Only available for structures with breach capability. This dataset shows
        how the breach evolves over time (width, depth, flow, etc.).

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure name. If None, returns all breach structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Breach progression data with columns:
            - datetime: Timestamp
            - structure: Structure name (if multiple structures)
            - hw: Headwater stage at breach (ft or m)
            - tw: Tailwater stage at breach (ft or m)
            - bottom_width: Current breach bottom width (ft or m)
            - bottom_elevation: Current breach bottom elevation (ft or m)
            - left_slope: Left side slope (feet/feet or m/m)
            - right_slope: Right side slope (feet/feet or m/m)
            - breach_flow: Flow through breach opening (cfs or m³/s)
            - breach_velocity: Average velocity through breach (ft/s or m/s)
            - breach_flow_area: Flow area of breach (ft² or m²)

        Examples
        --------
        >>> # Get breach progression for specific dam
        >>> df = HdfResultsBreach.get_breaching_variables("02", "Laxton_Dam")

        >>> # Plot breach width evolution
        >>> import matplotlib.pyplot as plt
        >>> plt.plot(df['datetime'], df['bottom_width'])
        >>> plt.ylabel('Breach Width (ft)')

        >>> # Get all breach structures
        >>> df = HdfResultsBreach.get_breaching_variables("02")

        Notes
        -----
        - Returns empty DataFrame if structure has no breach capability
        - NaN values indicate breach not yet formed at that timestep
        - Units depend on project unit system
        - For total structure flow, use get_structure_variables()

        Raises
        ------
        ValueError
            If specified structure_name doesn't exist in HDF
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                sa_conn_path = f"{base_path}/SA 2D Area Conn"

                if sa_conn_path not in hdf_file:
                    logger.warning(f"No SA 2D Area Conn data in {hdf_path.name}")
                    return pd.DataFrame()

                # Get timestamps
                time_stamps = HdfBase.get_unsteady_timestamps(hdf_file)

                # Get structure names with breach capability
                from .HdfStruc import HdfStruc
                breach_info = HdfStruc.get_sa2d_breach_info(hdf_path, ras_object=ras_object)
                available_breach_structures = breach_info[breach_info['has_breach']]['structure'].tolist()

                if not available_breach_structures:
                    logger.warning("No breach structures found in HDF file")
                    return pd.DataFrame()

                # Determine structures to extract
                if structure_name:
                    if structure_name not in available_breach_structures:
                        raise ValueError(f"Structure '{structure_name}' does not have breach capability. "
                                       f"Available breach structures: {available_breach_structures}")
                    structures = [structure_name]
                else:
                    structures = available_breach_structures

                # Extract data for each structure
                data_list = []
                for struct in structures:
                    breach_var_path = f"{sa_conn_path}/{struct}/Breaching Variables"

                    # Extract dataset
                    dataset = hdf_file[breach_var_path][:]  # shape: (n_timesteps, 9)

                    # Get variable names and units from attributes
                    var_unit = hdf_file[breach_var_path].attrs['Variable_Unit']
                    # var_unit[0] = [b'Stage HW', b'ft'], etc.

                    # Create DataFrame for this structure
                    struct_data = pd.DataFrame({
                        'datetime': time_stamps,
                        'hw': dataset[:, 0],
                        'tw': dataset[:, 1],
                        'bottom_width': dataset[:, 2],
                        'bottom_elevation': dataset[:, 3],
                        'left_slope': dataset[:, 4],
                        'right_slope': dataset[:, 5],
                        'breach_flow': dataset[:, 6],
                        'breach_velocity': dataset[:, 7],
                        'breach_flow_area': dataset[:, 8]
                    })

                    if len(structures) > 1:
                        struct_data.insert(1, 'structure', struct)

                    data_list.append(struct_data)

                # Combine all structures
                if data_list:
                    result_df = pd.concat(data_list, ignore_index=True)
                    logger.info(f"Extracted breach variables for {len(structures)} structure(s), "
                              f"{len(time_stamps)} timesteps")
                    return result_df
                else:
                    return pd.DataFrame()

        except Exception as e:
            logger.error(f"Error extracting breaching variables: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_breach_timeseries(hdf_path: Path, structure_name: str = None, *,
                             ras_object=None) -> pd.DataFrame:
        """
        Extract combined breach and structure time series (primary user function).

        This is a convenience function that combines data from both:
        - Structure Variables (total flow, weir flow)
        - Breaching Variables (breach geometry and breach-specific flow)

        Provides a complete picture of dam breach behavior over time.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure name. If None, returns all breach structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Combined time series with columns:
            - datetime: Timestamp
            - structure: Structure name (if multiple structures)
            - total_flow: Total flow through structure (cfs)
            - weir_flow: Flow over remaining weir (cfs)
            - breach_flow: Flow through breach opening (cfs)
            - hw: Headwater elevation (ft)
            - tw: Tailwater elevation (ft)
            - bottom_width: Breach width (ft)
            - bottom_elevation: Breach bottom elevation (ft)
            - left_slope: Left side slope
            - right_slope: Right side slope
            - breach_velocity: Breach velocity (ft/s)
            - breach_flow_area: Breach flow area (ft²)

        Examples
        --------
        >>> # Extract all breach data for plan 02
        >>> df = HdfResultsBreach.get_breach_timeseries("02")

        >>> # Get specific dam
        >>> df = HdfResultsBreach.get_breach_timeseries("02", "Laxton_Dam")

        >>> # Visualize
        >>> import matplotlib.pyplot as plt
        >>> fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        >>>
        >>> # Flow hydrograph
        >>> ax1.plot(df['datetime'], df['total_flow'], label='Total Flow')
        >>> ax1.plot(df['datetime'], df['breach_flow'], label='Breach Flow')
        >>> ax1.set_ylabel('Flow (cfs)')
        >>> ax1.legend()
        >>>
        >>> # Breach width evolution
        >>> ax2.plot(df['datetime'], df['bottom_width'])
        >>> ax2.set_ylabel('Breach Width (ft)')
        >>> ax2.set_xlabel('Time')
        >>> plt.tight_layout()

        Notes
        -----
        - Only returns structures with breach capability
        - For non-breach SA/2D connections, use get_structure_variables()
        - NaN values in breach columns indicate breach not yet formed

        See Also
        --------
        get_structure_variables : Structure-level data only
        get_breaching_variables : Breach-specific data only
        """
        try:
            # Get structure variables (total flow, weir flow, hw, tw)
            struct_df = HdfResultsBreach.get_structure_variables(hdf_path, structure_name, ras_object=ras_object)

            # Get breaching variables (breach geometry and breach flow)
            breach_df = HdfResultsBreach.get_breaching_variables(hdf_path, structure_name, ras_object=ras_object)

            if struct_df.empty:
                logger.warning("No structure data available")
                return pd.DataFrame()

            if breach_df.empty:
                logger.warning("No breach data available, returning structure data only")
                return struct_df

            # Determine merge columns
            merge_cols = ['datetime']
            if 'structure' in struct_df.columns and 'structure' in breach_df.columns:
                merge_cols.append('structure')

            # Merge the two dataframes
            combined_df = pd.merge(
                struct_df,
                breach_df[['datetime', 'structure', 'bottom_width', 'bottom_elevation',
                          'left_slope', 'right_slope', 'breach_flow', 'breach_velocity',
                          'breach_flow_area']] if 'structure' in breach_df.columns
                        else breach_df[['datetime', 'bottom_width', 'bottom_elevation',
                                       'left_slope', 'right_slope', 'breach_flow',
                                       'breach_velocity', 'breach_flow_area']],
                on=merge_cols,
                how='left'  # Keep all structure timesteps, even if no breach data
            )

            # Reorder columns for better user experience
            col_order = ['datetime']
            if 'structure' in combined_df.columns:
                col_order.append('structure')
            col_order.extend(['total_flow', 'weir_flow', 'breach_flow', 'hw', 'tw',
                            'bottom_width', 'bottom_elevation', 'left_slope', 'right_slope',
                            'breach_velocity', 'breach_flow_area'])

            combined_df = combined_df[col_order]

            logger.info(f"Created combined breach timeseries with {len(combined_df)} rows")
            return combined_df

        except Exception as e:
            logger.error(f"Error creating combined breach timeseries: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_breach_summary(hdf_path: Path, structure_name: str = None, *,
                          ras_object=None) -> pd.DataFrame:
        """
        Extract breach summary statistics (peak values, timing, final geometry).

        Provides quick overview of breach performance without full time series.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure. If None, returns all breach structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Summary statistics with columns:
            - structure: Structure name
            - breach_initiated: Boolean, True if breach formed
            - breach_at_time: Time of breach initiation (days)
            - breach_at_date: Date/time of breach
            - max_total_flow: Maximum total flow (cfs)
            - max_total_flow_time: Time of max total flow
            - max_breach_flow: Maximum breach flow (cfs)
            - max_breach_flow_time: Time of max breach flow
            - final_breach_width: Final breach width (ft)
            - final_breach_depth: Final breach depth (ft)
            - max_hw: Maximum headwater elevation (ft)
            - max_tw: Maximum tailwater elevation (ft)

        Examples
        --------
        >>> summary = HdfResultsBreach.get_breach_summary("02")
        >>> print(summary[['structure', 'max_total_flow', 'final_breach_width']])

        Notes
        -----
        - Returns summary even if breach didn't fully form (NaN for incomplete data)
        - Times are pandas datetime objects
        - If only 1 timestep available, "max" values are that single value
        """
        try:
            # Get full timeseries
            ts_df = HdfResultsBreach.get_breach_timeseries(hdf_path, structure_name, ras_object=ras_object)

            if ts_df.empty:
                return pd.DataFrame()

            # Get breach info
            from .HdfStruc import HdfStruc
            info_df = HdfStruc.get_sa2d_breach_info(hdf_path, ras_object=ras_object)

            # Determine grouping
            if 'structure' in ts_df.columns:
                structures = ts_df['structure'].unique()
            else:
                # Single structure, create pseudo-structure column
                structures = [structure_name] if structure_name else ['Unknown']
                ts_df['structure'] = structures[0]

            summary_list = []
            for struct in structures:
                struct_ts = ts_df[ts_df['structure'] == struct].copy()
                struct_info = info_df[info_df['structure'] == struct].iloc[0] if len(info_df) > 0 else {}

                # Calculate summary stats
                summary = {
                    'structure': struct,
                    'breach_initiated': struct_info.get('has_breach', False),
                    'breach_at_time': struct_info.get('breach_at_time', None),
                    'breach_at_date': struct_info.get('breach_at_date', None),
                }

                # Max flows and timing
                if 'total_flow' in struct_ts.columns:
                    max_total_idx = struct_ts['total_flow'].idxmax()
                    summary['max_total_flow'] = struct_ts.loc[max_total_idx, 'total_flow']
                    summary['max_total_flow_time'] = struct_ts.loc[max_total_idx, 'datetime']

                if 'breach_flow' in struct_ts.columns:
                    # Filter out NaN values
                    valid_breach = struct_ts[struct_ts['breach_flow'].notna()]
                    if len(valid_breach) > 0:
                        max_breach_idx = valid_breach['breach_flow'].idxmax()
                        summary['max_breach_flow'] = valid_breach.loc[max_breach_idx, 'breach_flow']
                        summary['max_breach_flow_time'] = valid_breach.loc[max_breach_idx, 'datetime']
                    else:
                        summary['max_breach_flow'] = np.nan
                        summary['max_breach_flow_time'] = None

                # Final breach geometry (last non-NaN value)
                if 'bottom_width' in struct_ts.columns:
                    valid_width = struct_ts[struct_ts['bottom_width'].notna()]
                    summary['final_breach_width'] = valid_width['bottom_width'].iloc[-1] if len(valid_width) > 0 else np.nan

                if 'bottom_elevation' in struct_ts.columns:
                    valid_elev = struct_ts[struct_ts['bottom_elevation'].notna()]
                    if len(valid_elev) > 0:
                        final_bottom = valid_elev['bottom_elevation'].iloc[-1]
                        # Calculate depth if we have HW
                        if 'hw' in struct_ts.columns:
                            final_hw = struct_ts['hw'].iloc[-1]
                            summary['final_breach_depth'] = final_hw - final_bottom
                        else:
                            summary['final_breach_depth'] = np.nan
                    else:
                        summary['final_breach_depth'] = np.nan

                # Max water levels
                if 'hw' in struct_ts.columns:
                    summary['max_hw'] = struct_ts['hw'].max()
                if 'tw' in struct_ts.columns:
                    summary['max_tw'] = struct_ts['tw'].max()

                summary_list.append(summary)

            result_df = pd.DataFrame(summary_list)
            logger.info(f"Generated breach summary for {len(structures)} structure(s)")
            return result_df

        except Exception as e:
            logger.error(f"Error generating breach summary: {e}")
            raise

==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsMesh.py
==================================================
"""
Class: HdfResultsMesh

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All methods in this class are static and designed to be used without instantiation.

Public Functions:
- get_mesh_summary(): Get summary output data for a variable 
- get_mesh_timeseries(): Get timeseries output for a mesh and variable  
- get_mesh_faces_timeseries(): Get timeseries for all face-based variables
- get_mesh_cells_timeseries(): Get timeseries for mesh cells
- get_mesh_last_iter(): Get last iteration count for cells
- get_mesh_max_ws(): Get maximum water surface elevation at each cell   
- get_mesh_min_ws(): Get minimum water surface elevation at each cell
- get_mesh_max_face_v(): Get maximum face velocity at each face
- get_mesh_min_face_v(): Get minimum face velocity at each face
- get_mesh_max_ws_err(): Get maximum water surface error at each cell
- get_mesh_max_iter(): Get maximum iteration count at each cell

Private Functions:
- _get_mesh_timeseries_output_path(): Get HDF path for timeseries output  #REDUNDANT??
- _get_mesh_cells_timeseries_output(): Internal handler for cell timeseries   #REDUNDANT??
- _get_mesh_timeseries_output(): Internal handler for mesh timeseries       # FACES?? 
- _get_mesh_timeseries_output_values_units(): Get values and units for timeseries
- _get_available_meshes(): Get list of available meshes in HDF            #USE HDFBASE OR HDFUTIL
- get_mesh_summary_output(): Internal handler for summary output        
- get_mesh_summary_output_group(): Get HDF group for summary output         #REDUNDANT??  Include in Above

The class works with HEC-RAS version 6.0+ plan HDF files and uses HdfBase and 
HdfUtils for common operations. Methods use @log_call decorator for logging and 
@standardize_input decorator to handle different input types.









"""

import numpy as np
import pandas as pd
import xarray as xr
from pathlib import Path
import h5py
from typing import Union, List, Optional, Dict, Any, Tuple
from .HdfMesh import HdfMesh
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import log_call, standardize_input
from .LoggingConfig import setup_logging, get_logger
import geopandas as gpd

logger = get_logger(__name__)

class HdfResultsMesh:
    """
    Handles mesh-related results from HEC-RAS HDF files.

    Provides methods to extract and analyze:
    - Mesh summary outputs
    - Timeseries data
    - Water surface elevations
    - Velocities
    - Error metrics

    Works with HEC-RAS 6.0+ plan HDF files.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary(hdf_path: Path, var: str, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, var, round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_summary: {str(e)}")
            logger.error(f"Variable: {var}")
            raise ValueError(f"Failed to get summary output: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_timeseries(hdf_path: Path, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        with h5py.File(hdf_path, 'r') as hdf_path:
            return HdfResultsMesh._get_mesh_timeseries_output(hdf_path, mesh_name, var, truncate)

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_faces_timeseries(hdf_path: Path, mesh_name: str) -> xr.Dataset:
        """
        Get timeseries output for all face-based variables of a specific mesh.

        Args:
            hdf_path (Path): Path to the HDF file.
            mesh_name (str): Name of the mesh.

        Returns:
            xr.Dataset: Dataset containing the timeseries output for all face-based variables.
        """
        face_vars = ["Face Velocity", "Face Flow"]
        datasets = []
        
        for var in face_vars:
            try:
                da = HdfResultsMesh.get_mesh_timeseries(hdf_path, mesh_name, var)
                # Assign the variable name as the DataArray name
                da.name = var.lower().replace(' ', '_')
                datasets.append(da)
            except Exception as e:
                logger.warning(f"Failed to process {var} for mesh {mesh_name}: {str(e)}")
        
        if not datasets:
            logger.error(f"No valid data found for mesh {mesh_name}")
            return xr.Dataset()
        
        try:
            return xr.merge(datasets)
        except Exception as e:
            logger.error(f"Failed to merge datasets: {str(e)}")
            return xr.Dataset()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cells_timeseries(hdf_path: Path, mesh_names: Optional[Union[str, List[str]]] = None, var: Optional[str] = None, truncate: bool = False, ras_object: Optional[Any] = None) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output.

        Args:
            hdf_path (Path): Path to HDF file
            mesh_names (str|List[str], optional): Mesh name(s). If None, processes all meshes
            var (str, optional): Variable name. If None, retrieves all variables
            truncate (bool): Remove trailing zeros if True
            ras_object (Any, optional): RAS object if available

        Returns:
            Dict[str, xr.Dataset]: Dictionary mapping mesh names to datasets containing:
                - Time-indexed variables
                - Cell/face IDs
                - Variable metadata
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_path:
                return HdfResultsMesh._get_mesh_cells_timeseries_output(hdf_path, mesh_names, var, truncate)
        except Exception as e:
            logger.error(f"Error in get_mesh_cells_timeseries: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_last_iter(hdf_file: Path) -> pd.DataFrame:
        """
        Get last iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing last iteration counts.
        """
        return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Last Iteration")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get maximum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing maximum water surface elevations with geometry.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface: {str(e)}")
        




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get minimum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing minimum water surface elevations with geometry.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_ws: {str(e)}")
            raise ValueError(f"Failed to get minimum water surface: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum face velocity for each mesh face.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum face velocities.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_face_v: {str(e)}")
            raise ValueError(f"Failed to get maximum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get minimum face velocity for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing minimum face velocities.

        Raises:
            ValueError: If there's an error processing the minimum face velocity data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_face_v: {str(e)}")
            raise ValueError(f"Failed to get minimum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws_err(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum water surface error for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum water surface errors.

        Raises:
            ValueError: If there's an error processing the maximum water surface error data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Maximum Water Surface Error", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws_err: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface error: {str(e)}")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_iter(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get maximum iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing maximum iteration counts with geometry.
                Includes columns:
                - mesh_name: Name of the mesh
                - cell_id: ID of the cell
                - cell_last_iteration: Maximum number of iterations
                - cell_last_iteration_time: Time when max iterations occurred
                - geometry: Point geometry representing cell center
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Last Iteration", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_iter: {str(e)}")
            raise ValueError(f"Failed to get maximum iteration count: {str(e)}")
        
        


    @staticmethod
    def _get_mesh_timeseries_output_path(mesh_name: str, var_name: str) -> str:
        """
        Get the HDF path for mesh timeseries output.

        Args:
            mesh_name (str): Name of the mesh.
            var_name (str): Name of the variable.

        Returns:
            str: The HDF path for the specified mesh and variable.
        """
        return f"Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas/{mesh_name}/{var_name}"


    @staticmethod
    def _get_mesh_cells_timeseries_output(hdf_path: h5py.File, 
                                         mesh_names: Optional[Union[str, List[str]]] = None,
                                         var: Optional[str] = None, 
                                         truncate: bool = False) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output for specified meshes and variables.
        
        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_names (Optional[Union[str, List[str]]]): Name(s) of the mesh(es). If None, processes all available meshes.
            var (Optional[str]): Name of the variable to retrieve. If None, retrieves all variables.
            truncate (bool): If True, truncates the output to remove trailing zeros.

        Returns:
            Dict[str, xr.Dataset]: A dictionary of xarray Datasets, one for each mesh, containing the mesh cells timeseries output.

        Raises:
            ValueError: If there's an error processing the timeseries output data.
        """
        TIME_SERIES_OUTPUT_VARS = {
            "cell": [
                "Water Surface", "Depth", "Velocity", "Velocity X", "Velocity Y",
                "Froude Number", "Courant Number", "Shear Stress", "Bed Elevation",
                "Precipitation Rate", "Infiltration Rate", "Evaporation Rate",
                "Percolation Rate", "Groundwater Elevation", "Groundwater Depth",
                "Groundwater Flow", "Groundwater Velocity", "Groundwater Velocity X",
                "Groundwater Velocity Y"
            ],
            "face": [
                "Face Velocity", "Face Flow", "Face Water Surface", "Face Courant",
                "Face Cumulative Volume", "Face Eddy Viscosity", "Face Flow Period Average",
                "Face Friction Term", "Face Pressure Gradient Term", "Face Shear Stress",
                "Face Tangential Velocity"
            ]
        }

        try:
            start_time = HdfBase.get_simulation_start_time(hdf_path)
            time_stamps = HdfBase.get_unsteady_timestamps(hdf_path)

            if mesh_names is None:
                mesh_names = HdfResultsMesh._get_available_meshes(hdf_path)
            elif isinstance(mesh_names, str):
                mesh_names = [mesh_names]

            if var:
                variables = [var]
            else:
                variables = TIME_SERIES_OUTPUT_VARS["cell"] + TIME_SERIES_OUTPUT_VARS["face"]

            datasets = {}
            for mesh_name in mesh_names:
                data_vars = {}
                for variable in variables:
                    try:
                        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, variable)
                        dataset = hdf_path[path]
                        values = dataset[:]
                        units = dataset.attrs.get("Units", "").decode("utf-8")

                        if truncate:
                            last_nonzero = np.max(np.nonzero(values)[1]) + 1 if values.size > 0 else 0
                            values = values[:, :last_nonzero]
                            truncated_time_stamps = time_stamps[:last_nonzero]
                        else:
                            truncated_time_stamps = time_stamps

                        if values.shape[0] != len(truncated_time_stamps):
                            logger.warning(f"Mismatch between time steps ({len(truncated_time_stamps)}) and data shape ({values.shape}) for variable {variable}")
                            continue

                        # Determine if this is a face-based or cell-based variable
                        id_dim = "face_id" if any(face_var in variable for face_var in TIME_SERIES_OUTPUT_VARS["face"]) else "cell_id"

                        data_vars[variable] = xr.DataArray(
                            data=values,
                            dims=['time', id_dim],
                            coords={'time': truncated_time_stamps, id_dim: np.arange(values.shape[1])},
                            attrs={'units': units}
                        )
                    except KeyError:
                        logger.warning(f"Variable '{variable}' not found in the HDF file for mesh '{mesh_name}'. Skipping.")
                    except Exception as e:
                        logger.error(f"Error processing variable '{variable}' for mesh '{mesh_name}': {str(e)}")

                if data_vars:
                    datasets[mesh_name] = xr.Dataset(
                        data_vars=data_vars,
                        attrs={'mesh_name': mesh_name, 'start_time': start_time}
                    )
                else:
                    logger.warning(f"No valid data variables found for mesh '{mesh_name}'")

            return datasets
        except Exception as e:
            logger.error(f"Error in _mesh_cells_timeseries_output: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")



    @staticmethod
    def _get_mesh_timeseries_output(hdf_path: h5py.File, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.
            truncate (bool): Whether to truncate the output to remove trailing zeros (default True).

        Returns:
            xr.DataArray: DataArray containing the timeseries output.

        Raises:
            ValueError: If the specified path is not found in the HDF file or if there's an error processing the data.
        """
        try:
            path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
            
            if path not in hdf_path:
                raise ValueError(f"Path {path} not found in HDF file")

            dataset = hdf_path[path]
            values = dataset[:]
            units = dataset.attrs.get("Units", "").decode("utf-8")
            
            # Get start time and timesteps
            start_time = HdfBase.get_simulation_start_time(hdf_path)
            # Updated to use the new function name from HdfUtils
            timesteps = HdfUtils.convert_timesteps_to_datetimes(
                np.array(hdf_path["Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time"][:]),
                start_time
            )

            if truncate:
                non_zero = np.nonzero(values)[0]
                if len(non_zero) > 0:
                    start, end = non_zero[0], non_zero[-1] + 1
                    values = values[start:end]
                    timesteps = timesteps[start:end]

            # Determine if this is a face-based or cell-based variable
            id_dim = "face_id" if "Face" in var else "cell_id"
            dims = ["time", id_dim] if values.ndim == 2 else ["time"]
            coords = {"time": timesteps}
            if values.ndim == 2:
                coords[id_dim] = np.arange(values.shape[1])

            return xr.DataArray(
                values,
                coords=coords,
                dims=dims,
                attrs={"units": units, "mesh_name": mesh_name, "variable": var},
            )
        except Exception as e:
            logger.error(f"Error in get_mesh_timeseries_output: {str(e)}")
            raise ValueError(f"Failed to get timeseries output: {str(e)}")


    @staticmethod
    def _get_mesh_timeseries_output_values_units(hdf_path: h5py.File, mesh_name: str, var: str) -> Tuple[np.ndarray, str]:
        """
        Get the mesh timeseries output values and units for a specific variable from the HDF file.

        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.

        Returns:
            Tuple[np.ndarray, str]: A tuple containing the output values and units.
        """
        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
        group = hdf_path[path]
        values = group[:]
        units = group.attrs.get("Units")
        if units is not None:
            units = units.decode("utf-8")
        return values, units


    @staticmethod
    def _get_available_meshes(hdf_path: h5py.File) -> List[str]:
        """
        Get the names of all available meshes in the HDF file.

        Args:
            hdf_path (h5py.File): Open HDF file object.

        Returns:
            List[str]: A list of mesh names.
        """
        return HdfMesh.get_mesh_area_names(hdf_path)
    
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary_output(hdf_file: h5py.File, var: str, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get the summary output data for a given variable from the HDF file.

        Parameters
        ----------
        hdf_path : h5py.File
            Open HDF file object.
        var : str
            The summary output variable to retrieve.
        round_to : str, optional
            The time unit to round the datetimes to. Default is "100ms".

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the summary output data with decoded attributes as metadata.
            Returns empty GeoDataFrame if variable is not found.
        """
        try:
            dfs = []
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            
            logger.info(f"Processing summary output for variable: {var}")
            d2_flow_areas = hdf_file.get("Geometry/2D Flow Areas/Attributes")
            if d2_flow_areas is None:
                logger.info("No 2D Flow Areas found in HDF file")
                return gpd.GeoDataFrame()

            for d2_flow_area in d2_flow_areas[:]:
                mesh_name = HdfUtils.convert_ras_string(d2_flow_area[0])
                cell_count = d2_flow_area[-1]
                logger.debug(f"Processing mesh: {mesh_name} with {cell_count} cells")
                
                try:
                    group = HdfResultsMesh.get_mesh_summary_output_group(hdf_file, mesh_name, var)
                except ValueError:
                    logger.info(f"Variable '{var}' not present in output file for mesh '{mesh_name}', skipping")
                    continue
                
                data = group[:]
                logger.debug(f"Data shape for {var} in {mesh_name}: {data.shape}")
                logger.debug(f"Data type: {data.dtype}")
                logger.debug(f"Attributes: {dict(group.attrs)}")
                
                if data.ndim == 2 and data.shape[0] == 2:
                    # Handle 2D datasets (e.g. Maximum Water Surface)
                    row_variables = group.attrs.get('Row Variables', [b'Value', b'Time'])
                    row_variables = [v.decode('utf-8').strip() if isinstance(v, bytes) else v for v in row_variables]
                    
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * data.shape[1],
                        "cell_id" if "Face" not in var else "face_id": range(data.shape[1]),
                        f"{var.lower().replace(' ', '_')}": data[0, :],
                        f"{var.lower().replace(' ', '_')}_time": HdfUtils.convert_timesteps_to_datetimes(
                            data[1, :], start_time, time_unit="days", round_to=round_to
                        )
                    })
                    
                elif data.ndim == 1:
                    # Handle 1D datasets (e.g. Cell Last Iteration)
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * len(data),
                        "cell_id" if "Face" not in var else "face_id": range(len(data)),
                        var.lower().replace(' ', '_'): data
                    })
                    
                else:
                    raise ValueError(f"Unexpected data shape for {var} in {mesh_name}. "
                                  f"Got shape {data.shape}")
                
                # Add geometry based on variable type
                if "Face" in var:
                    face_df = HdfMesh.get_mesh_cell_faces(hdf_file)
                    if not face_df.empty:
                        df = df.merge(face_df[['mesh_name', 'face_id', 'geometry']], 
                                    on=['mesh_name', 'face_id'], 
                                    how='left')
                else:
                    cell_df = HdfMesh.get_mesh_cell_points(hdf_file)
                    if not cell_df.empty:
                        df = df.merge(cell_df[['mesh_name', 'cell_id', 'geometry']], 
                                    on=['mesh_name', 'cell_id'], 
                                    how='left')
                
                # Add group attributes as metadata with proper decoding
                df.attrs['mesh_name'] = mesh_name
                for attr_name, attr_value in group.attrs.items():
                    if isinstance(attr_value, bytes):
                        # Decode single byte string
                        decoded_value = attr_value.decode('utf-8')
                    elif isinstance(attr_value, np.ndarray):
                        if attr_value.dtype.kind in {'S', 'a'}:  # Array of byte strings
                            # Decode array of byte strings
                            decoded_value = [v.decode('utf-8') if isinstance(v, bytes) else v for v in attr_value]
                        else:
                            # Convert other numpy arrays to list
                            decoded_value = attr_value.tolist()
                    else:
                        decoded_value = attr_value
                    df.attrs[attr_name] = decoded_value
                
                dfs.append(df)
            
            if not dfs:
                return gpd.GeoDataFrame()
                
            result = pd.concat(dfs, ignore_index=True)
            
            # Convert to GeoDataFrame
            gdf = gpd.GeoDataFrame(result, geometry='geometry')
            
            # Get CRS from HdfUtils
            crs = HdfBase.get_projection(hdf_file)
            if crs:
                gdf.set_crs(crs, inplace=True)
            
            # Combine attributes from all meshes with decoded values
            combined_attrs = {}
            for df in dfs:
                for key, value in df.attrs.items():
                    if key not in combined_attrs:
                        combined_attrs[key] = value
                    elif combined_attrs[key] != value:
                        combined_attrs[key] = f"Multiple values: {combined_attrs[key]}, {value}"
            
            gdf.attrs.update(combined_attrs)
            
            logger.info(f"Processed {len(gdf)} rows of summary output data")
            return gdf
        
        except Exception as e:
            logger.error(f"Error processing summary output data: {e}")
            raise ValueError(f"Error processing summary output data: {e}")

    @staticmethod
    def get_mesh_summary_output_group(hdf_file: h5py.File, mesh_name: str, var: str) -> Union[h5py.Group, h5py.Dataset]:
        """
        Return the HDF group for a given mesh and summary output variable.

        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Name of the summary output variable.

        Returns:
            Union[h5py.Group, h5py.Dataset]: The HDF group or dataset for the specified mesh and variable.

        Raises:
            ValueError: If the specified group or dataset is not found in the HDF file.
        """
        output_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/{var}"
        output_item = hdf_file.get(output_path)
        if output_item is None:
            raise ValueError(f"Dataset not found at path '{output_path}'")
        return output_item

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_boundary_conditions_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Get timeseries output for all boundary conditions as a single combined xarray Dataset.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            xr.Dataset: Dataset containing all boundary condition data with:
                - Dimensions: time, bc_name (boundary condition name), face_id
                - Variables: stage, flow, flow_per_face, stage_per_face
                - Coordinates and attributes preserving original metadata

        Example:
            >>> bc_data = HdfResultsMesh.get_boundary_conditions_timeseries_combined(hdf_path)
            >>> print(bc_data)
            >>> # Plot flow for all boundary conditions
            >>> bc_data.flow.plot(x='time', hue='bc_name')
            >>> # Extract data for a specific boundary condition
            >>> upstream_data = bc_data.sel(bc_name='Upstream Inflow')
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Get the base path and check if boundary conditions exist
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                bc_base_path = f"{base_path}/Boundary Conditions"
                
                if bc_base_path not in hdf_file:
                    logger.warning(f"No boundary conditions found in HDF file")
                    return xr.Dataset()
                
                # Get timestamps
                start_time = HdfBase.get_simulation_start_time(hdf_file)
                time_data = hdf_file[f"{base_path}/Time"][:]
                timestamps = HdfUtils.convert_timesteps_to_datetimes(time_data, start_time)
                
                # Get all boundary condition names (excluding those with " - Flow per Face" or " - Stage per Face" suffix)
                bc_names = [name for name in hdf_file[bc_base_path].keys() 
                        if " - Flow per Face" not in name and " - Stage per Face" not in name]
                
                if not bc_names:
                    logger.warning(f"No boundary conditions found in HDF file")
                    return xr.Dataset()
                
                # Initialize arrays for main stage and flow data
                num_timesteps = len(timestamps)
                num_bcs = len(bc_names)
                
                stage_data = np.full((num_timesteps, num_bcs), np.nan)
                flow_data = np.full((num_timesteps, num_bcs), np.nan)
                
                # Dictionary to store face-specific data
                face_data = {
                    'flow_per_face': {},
                    'stage_per_face': {}
                }
                
                # Extract metadata from all boundary conditions
                bc_metadata = {}
                
                # Process each boundary condition
                for bc_idx, bc_name in enumerate(bc_names):
                    bc_path = f"{bc_base_path}/{bc_name}"
                    
                    try:
                        # Extract main boundary data
                        bc_data = hdf_file[bc_path][:]
                        bc_attrs = dict(hdf_file[bc_path].attrs)
                        
                        # Store metadata
                        bc_metadata[bc_name] = {
                            k: v.decode('utf-8') if isinstance(v, bytes) else v 
                            for k, v in bc_attrs.items()
                        }
                        
                        # Get column indices for Stage and Flow
                        if 'Columns' in bc_attrs:
                            columns = [col.decode('utf-8') if isinstance(col, bytes) else col 
                                    for col in bc_attrs['Columns']]
                            
                            stage_idx = columns.index('Stage') if 'Stage' in columns else None
                            flow_idx = columns.index('Flow') if 'Flow' in columns else None
                            
                            if stage_idx is not None:
                                stage_data[:, bc_idx] = bc_data[:, stage_idx]
                            if flow_idx is not None:
                                flow_data[:, bc_idx] = bc_data[:, flow_idx]
                        
                        # Extract Flow per Face data
                        flow_face_path = f"{bc_path} - Flow per Face"
                        if flow_face_path in hdf_file:
                            flow_face_data = hdf_file[flow_face_path][:]
                            flow_face_attrs = dict(hdf_file[flow_face_path].attrs)
                            
                            # Get face IDs
                            face_ids = flow_face_attrs.get('Faces', [])
                            if isinstance(face_ids, np.ndarray):
                                face_ids = face_ids.tolist()
                            else:
                                face_ids = list(range(flow_face_data.shape[1]))
                            
                            face_data['flow_per_face'][bc_name] = {
                                'data': flow_face_data,
                                'faces': face_ids,
                                'attrs': {
                                    k: v.decode('utf-8') if isinstance(v, bytes) else v 
                                    for k, v in flow_face_attrs.items()
                                }
                            }
                        
                        # Extract Stage per Face data
                        stage_face_path = f"{bc_path} - Stage per Face"
                        if stage_face_path in hdf_file:
                            stage_face_data = hdf_file[stage_face_path][:]
                            stage_face_attrs = dict(hdf_file[stage_face_path].attrs)
                            
                            # Get face IDs
                            face_ids = stage_face_attrs.get('Faces', [])
                            if isinstance(face_ids, np.ndarray):
                                face_ids = face_ids.tolist()
                            else:
                                face_ids = list(range(stage_face_data.shape[1]))
                            
                            face_data['stage_per_face'][bc_name] = {
                                'data': stage_face_data,
                                'faces': face_ids,
                                'attrs': {
                                    k: v.decode('utf-8') if isinstance(v, bytes) else v 
                                    for k, v in stage_face_attrs.items()
                                }
                            }
                    
                    except Exception as e:
                        logger.warning(f"Error processing boundary condition '{bc_name}': {str(e)}")
                        continue
                
                # Create base dataset with stage and flow data
                ds = xr.Dataset(
                    data_vars={
                        'stage': xr.DataArray(
                            stage_data,
                            dims=['time', 'bc_name'],
                            coords={
                                'time': timestamps,
                                'bc_name': bc_names
                            },
                            attrs={'description': 'Water surface elevation at boundary condition'}
                        ),
                        'flow': xr.DataArray(
                            flow_data,
                            dims=['time', 'bc_name'],
                            coords={
                                'time': timestamps,
                                'bc_name': bc_names
                            },
                            attrs={'description': 'Flow at boundary condition'}
                        )
                    },
                    attrs={
                        'source': 'HEC-RAS HDF Boundary Conditions',
                        'start_time': start_time
                    }
                )
                
                # Add metadata as coordinates
                for key in bc_metadata[bc_names[0]]:
                    if key != 'Columns':  # Skip Columns attribute as it's used for Stage/Flow
                        try:
                            values = [bc_metadata[bc].get(key, '') for bc in bc_names]
                            ds = ds.assign_coords({f'{key.lower()}': ('bc_name', values)})
                        except Exception as e:
                            logger.debug(f"Could not add metadata coordinate '{key}': {str(e)}")
                
                # Add face-specific data variables if available
                if face_data['flow_per_face']:
                    # First determine the maximum number of faces across all BCs
                    all_flow_faces = set()
                    for bc_name in face_data['flow_per_face']:
                        all_flow_faces.update(face_data['flow_per_face'][bc_name]['faces'])
                    
                    # Create a merged array with NaN values for missing faces
                    all_flow_faces = sorted(list(all_flow_faces))
                    flow_face_data = np.full((num_timesteps, num_bcs, len(all_flow_faces)), np.nan)
                    
                    # Fill in the data where available
                    for bc_idx, bc_name in enumerate(bc_names):
                        if bc_name in face_data['flow_per_face']:
                            bc_faces = face_data['flow_per_face'][bc_name]['faces']
                            bc_data = face_data['flow_per_face'][bc_name]['data']
                            
                            for face_idx, face_id in enumerate(bc_faces):
                                if face_id in all_flow_faces:
                                    target_idx = all_flow_faces.index(face_id)
                                    flow_face_data[:, bc_idx, target_idx] = bc_data[:, face_idx]
                    
                    # Add to the dataset
                    ds['flow_per_face'] = xr.DataArray(
                        flow_face_data,
                        dims=['time', 'bc_name', 'face_id'],
                        coords={
                            'time': timestamps,
                            'bc_name': bc_names,
                            'face_id': all_flow_faces
                        },
                        attrs={'description': 'Flow per face at boundary condition'}
                    )
                
                # Similar approach for stage per face
                if face_data['stage_per_face']:
                    all_stage_faces = set()
                    for bc_name in face_data['stage_per_face']:
                        all_stage_faces.update(face_data['stage_per_face'][bc_name]['faces'])
                    
                    all_stage_faces = sorted(list(all_stage_faces))
                    stage_face_data = np.full((num_timesteps, num_bcs, len(all_stage_faces)), np.nan)
                    
                    for bc_idx, bc_name in enumerate(bc_names):
                        if bc_name in face_data['stage_per_face']:
                            bc_faces = face_data['stage_per_face'][bc_name]['faces']
                            bc_data = face_data['stage_per_face'][bc_name]['data']
                            
                            for face_idx, face_id in enumerate(bc_faces):
                                if face_id in all_stage_faces:
                                    target_idx = all_stage_faces.index(face_id)
                                    stage_face_data[:, bc_idx, target_idx] = bc_data[:, face_idx]
                    
                    ds['stage_per_face'] = xr.DataArray(
                        stage_face_data,
                        dims=['time', 'bc_name', 'face_id'],
                        coords={
                            'time': timestamps,
                            'bc_name': bc_names,
                            'face_id': all_stage_faces
                        },
                        attrs={'description': 'Water surface elevation per face at boundary condition'}
                    )
                
                return ds
                
        except Exception as e:
            logger.error(f"Error getting all boundary conditions timeseries: {str(e)}")
            return xr.Dataset()
==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsPlan.py
==================================================
"""
HdfResultsPlan: A module for extracting and analyzing HEC-RAS plan HDF file results.

Attribution:
    Substantial code sourced/derived from https://github.com/fema-ffrd/rashdf
    Copyright (c) 2024 fema-ffrd, MIT license

Description:
    Provides static methods for extracting both unsteady and steady flow results,
    volume accounting, and reference data from HEC-RAS plan HDF files.

Available Functions:
    Unsteady Flow:
        - get_unsteady_info: Extract unsteady attributes
        - get_unsteady_summary: Extract unsteady summary data
        - get_volume_accounting: Extract volume accounting data
        - get_runtime_data: Extract runtime and compute time data
        - get_reference_timeseries: Extract reference line/point timeseries
        - get_reference_summary: Extract reference line/point summary

    Steady Flow:
        - is_steady_plan: Check if HDF contains steady state results
        - get_steady_profile_names: Extract steady state profile names
        - get_steady_wse: Extract WSE data for steady state profiles
        - get_steady_info: Extract steady flow attributes and metadata

    Computation Messages:
        - get_compute_messages: Extract computation messages from HDF (with .txt fallback)

Note:
    All methods are static and designed to be used without class instantiation.
"""

from typing import Dict, List, Union, Optional
from pathlib import Path
import h5py
import pandas as pd
import xarray as xr
from .Decorators import standardize_input, log_call
from .HdfUtils import HdfUtils
from .HdfResultsXsec import HdfResultsXsec
from .LoggingConfig import get_logger
import numpy as np
from datetime import datetime
from .RasPrj import ras

logger = get_logger(__name__)


class HdfResultsPlan:
    """
    Handles extraction of results data from HEC-RAS plan HDF files.

    This class provides static methods for accessing and analyzing:
        - Unsteady flow results
        - Volume accounting data
        - Runtime statistics
        - Reference line/point time series outputs

    All methods use:
        - @standardize_input decorator for consistent file path handling
        - @log_call decorator for operation logging
        - HdfUtils class for common HDF operations

    Note:
        No instantiation required - all methods are static.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_info(hdf_path: Path) -> pd.DataFrame:
        """
        Get unsteady attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: A DataFrame containing the decoded unsteady attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady" group is not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady" not in hdf_file:
                    raise KeyError("Results/Unsteady group not found in the HDF file.")
                
                # Create dictionary from attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Results/Unsteady"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Get results unsteady summary attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: A DataFrame containing the decoded results unsteady summary attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady/Summary" group is not found in the HDF file.
        """
        try:           
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary" not in hdf_file:
                    raise KeyError("Results/Unsteady/Summary group not found in the HDF file.")
                
                # Create dictionary from attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Results/Unsteady/Summary"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady summary attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_volume_accounting(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Get volume accounting attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[pd.DataFrame]: DataFrame containing the decoded volume accounting attributes,
                                  or None if the group is not found.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary/Volume Accounting" not in hdf_file:
                    return None
                
                # Get attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Results/Unsteady/Summary/Volume Accounting"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading volume accounting attributes: {str(e)}")

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_runtime_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Extract detailed runtime and computational performance metrics from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[pd.DataFrame]: DataFrame containing runtime statistics or None if data cannot be extracted

        Notes:
            - Times are reported in multiple units (ms, s, hours)
            - Compute speeds are calculated as simulation-time/compute-time ratios
            - Process times include: geometry, preprocessing, event conditions, 
              and unsteady flow computations
        """
        try:
            if hdf_path is None:
                logger.error(f"Could not find HDF file for input")
                return None

            with h5py.File(hdf_path, 'r') as hdf_file:
                logger.info(f"Extracting Plan Information from: {Path(hdf_file.filename).name}")
                plan_info = hdf_file.get('/Plan Data/Plan Information')
                if plan_info is None:
                    logger.warning("Group '/Plan Data/Plan Information' not found.")
                    return None

                # Extract plan information
                plan_name = HdfUtils.convert_ras_string(plan_info.attrs.get('Plan Name', 'Unknown'))
                start_time_str = HdfUtils.convert_ras_string(plan_info.attrs.get('Simulation Start Time', 'Unknown'))
                end_time_str = HdfUtils.convert_ras_string(plan_info.attrs.get('Simulation End Time', 'Unknown'))

                try:
                    # Check if times are already datetime objects
                    if isinstance(start_time_str, datetime):
                        start_time = start_time_str
                    else:
                        start_time = datetime.strptime(start_time_str, "%d%b%Y %H:%M:%S")
                        
                    if isinstance(end_time_str, datetime):
                        end_time = end_time_str
                    else:
                        end_time = datetime.strptime(end_time_str, "%d%b%Y %H:%M:%S")
                        
                    simulation_duration = end_time - start_time
                    simulation_hours = simulation_duration.total_seconds() / 3600
                except ValueError as e:
                    logger.error(f"Error parsing simulation times: {e}")
                    return None

                logger.info(f"Plan Name: {plan_name}")
                logger.info(f"Simulation Duration (hours): {simulation_hours}")

                # Extract compute processes data
                compute_processes = hdf_file.get('/Results/Summary/Compute Processes')
                if compute_processes is None:
                    logger.warning("Dataset '/Results/Summary/Compute Processes' not found.")
                    return None

                # Process compute times
                process_names = [HdfUtils.convert_ras_string(name) for name in compute_processes['Process'][:]]
                filenames = [HdfUtils.convert_ras_string(filename) for filename in compute_processes['Filename'][:]]
                completion_times = compute_processes['Compute Time (ms)'][:]

                compute_processes_df = pd.DataFrame({
                    'Process': process_names,
                    'Filename': filenames,
                    'Compute Time (ms)': completion_times,
                    'Compute Time (s)': completion_times / 1000,
                    'Compute Time (hours)': completion_times / (1000 * 3600)
                })

                # Create summary DataFrame
                compute_processes_summary = {
                    'Plan Name': [plan_name],
                    'File Name': [Path(hdf_file.filename).name],
                    'Simulation Start Time': [start_time_str],
                    'Simulation End Time': [end_time_str],
                    'Simulation Duration (s)': [simulation_duration.total_seconds()],
                    'Simulation Time (hr)': [simulation_hours]
                }

                # Add process-specific times
                process_types = {
                    'Completing Geometry': 'Completing Geometry (hr)',
                    'Preprocessing Geometry': 'Preprocessing Geometry (hr)',
                    'Completing Event Conditions': 'Completing Event Conditions (hr)',
                    'Unsteady Flow Computations': 'Unsteady Flow Computations (hr)'
                }

                for process, column in process_types.items():
                    time_value = compute_processes_df[
                        compute_processes_df['Process'] == process
                    ]['Compute Time (hours)'].values[0] if process in process_names else 'N/A'
                    compute_processes_summary[column] = [time_value]

                # Add total process time
                total_time = compute_processes_df['Compute Time (hours)'].sum()
                compute_processes_summary['Complete Process (hr)'] = [total_time]

                # Calculate speeds
                if compute_processes_summary['Unsteady Flow Computations (hr)'][0] != 'N/A':
                    compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = [
                        simulation_hours / compute_processes_summary['Unsteady Flow Computations (hr)'][0]
                    ]
                else:
                    compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = ['N/A']

                compute_processes_summary['Complete Process Speed (hr/hr)'] = [
                    simulation_hours / total_time
                ]

                return pd.DataFrame(compute_processes_summary)

        except Exception as e:
            logger.error(f"Error in get_runtime_data: {str(e)}")
            return None

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_reference_timeseries(hdf_path: Path, reftype: str) -> pd.DataFrame:
        """
        Get reference line or point timeseries output from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            reftype (str): Type of reference data ('lines' or 'points')
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing reference timeseries data
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                ref_path = f"{base_path}/Reference {reftype.capitalize()}"
                
                if ref_path not in hdf_file:
                    logger.warning(f"Reference {reftype} data not found in HDF file")
                    return pd.DataFrame()

                ref_group = hdf_file[ref_path]
                time_data = hdf_file[f"{base_path}/Time"][:]
                
                dfs = []
                for ref_name in ref_group.keys():
                    ref_data = ref_group[ref_name][:]
                    df = pd.DataFrame(ref_data, columns=[ref_name])
                    df['Time'] = time_data
                    dfs.append(df)

                if not dfs:
                    return pd.DataFrame()

                return pd.concat(dfs, axis=1)

        except Exception as e:
            logger.error(f"Error reading reference {reftype} timeseries: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_reference_summary(hdf_path: Path, reftype: str) -> pd.DataFrame:
        """
        Get reference line or point summary output from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            reftype (str): Type of reference data ('lines' or 'points')
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing reference summary data
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Summary Output"
                ref_path = f"{base_path}/Reference {reftype.capitalize()}"
                
                if ref_path not in hdf_file:
                    logger.warning(f"Reference {reftype} summary data not found in HDF file")
                    return pd.DataFrame()

                ref_group = hdf_file[ref_path]
                dfs = []
                
                for ref_name in ref_group.keys():
                    ref_data = ref_group[ref_name][:]
                    if ref_data.ndim == 2:
                        df = pd.DataFrame(ref_data.T, columns=['Value', 'Time'])
                    else:
                        df = pd.DataFrame({'Value': ref_data})
                    df['Reference'] = ref_name
                    dfs.append(df)

                if not dfs:
                    return pd.DataFrame()

                return pd.concat(dfs, ignore_index=True)

        except Exception as e:
            logger.error(f"Error reading reference {reftype} summary: {str(e)}")
            return pd.DataFrame()

    # ==================== STEADY STATE METHODS ====================

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def is_steady_plan(hdf_path: Path) -> bool:
        """
        Check if HDF file contains steady state results.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            bool: True if the HDF contains steady state results, False otherwise

        Notes:
            - Checks for existence of Results/Steady group
            - Does not guarantee results are complete or valid
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return "Results/Steady" in hdf_file
        except Exception as e:
            logger.error(f"Error checking if plan is steady: {str(e)}")
            return False

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_steady_profile_names(hdf_path: Path) -> List[str]:
        """
        Extract profile names from steady state results.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            List[str]: List of profile names (e.g., ['50Pct', '10Pct', '1Pct'])

        Raises:
            FileNotFoundError: If the specified HDF file is not found
            KeyError: If steady state results or profile names are not found
            ValueError: If the plan is not a steady state plan

        Example:
            >>> from ras_commander import HdfResultsPlan, init_ras_project
            >>> init_ras_project(Path('/path/to/project'), '6.6')
            >>> profiles = HdfResultsPlan.get_steady_profile_names('01')
            >>> print(profiles)
            ['50Pct', '20Pct', '10Pct', '4Pct', '2Pct', '1Pct', '0.2Pct']
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Check if this is a steady state plan
                if "Results/Steady" not in hdf_file:
                    raise ValueError(f"HDF file does not contain steady state results: {hdf_path.name}")

                # Path to profile names
                profile_names_path = "Results/Steady/Output/Output Blocks/Base Output/Steady Profiles/Profile Names"

                if profile_names_path not in hdf_file:
                    raise KeyError(f"Profile names not found at: {profile_names_path}")

                # Read profile names dataset
                profile_names_ds = hdf_file[profile_names_path]
                profile_names_raw = profile_names_ds[()]

                # Decode byte strings to regular strings
                profile_names = []
                for name in profile_names_raw:
                    if isinstance(name, bytes):
                        profile_names.append(name.decode('utf-8').strip())
                    else:
                        profile_names.append(str(name).strip())

                logger.info(f"Found {len(profile_names)} steady state profiles: {profile_names}")
                return profile_names

        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except KeyError as e:
            raise KeyError(f"Error accessing steady state profile names: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading steady state profile names: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_steady_wse(
        hdf_path: Path,
        profile_index: Optional[int] = None,
        profile_name: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Extract water surface elevation (WSE) data for steady state profiles.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            profile_index (int, optional): Index of profile to extract (0-based). If None, extracts all profiles.
            profile_name (str, optional): Name of profile to extract (e.g., '1Pct'). If specified, overrides profile_index.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing WSE data with columns:
                - River: River name
                - Reach: Reach name
                - Station: Cross section river station
                - Profile: Profile name (if multiple profiles)
                - WSE: Water surface elevation (ft)

        Raises:
            FileNotFoundError: If the specified HDF file is not found
            KeyError: If steady state results or WSE data are not found
            ValueError: If profile_index or profile_name is invalid

        Example:
            >>> # Extract single profile by index
            >>> wse_df = HdfResultsPlan.get_steady_wse('01', profile_index=5)  # 100-year profile

            >>> # Extract single profile by name
            >>> wse_df = HdfResultsPlan.get_steady_wse('01', profile_name='1Pct')

            >>> # Extract all profiles
            >>> wse_df = HdfResultsPlan.get_steady_wse('01')
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Check if this is a steady state plan
                if "Results/Steady" not in hdf_file:
                    raise ValueError(f"HDF file does not contain steady state results: {hdf_path.name}")

                # Paths to data
                wse_path = "Results/Steady/Output/Output Blocks/Base Output/Steady Profiles/Cross Sections/Water Surface"
                xs_attrs_path = "Results/Steady/Output/Geometry Info/Cross Section Attributes"
                profile_names_path = "Results/Steady/Output/Output Blocks/Base Output/Steady Profiles/Profile Names"

                # Check required paths exist
                if wse_path not in hdf_file:
                    raise KeyError(f"WSE data not found at: {wse_path}")
                if xs_attrs_path not in hdf_file:
                    raise KeyError(f"Cross section attributes not found at: {xs_attrs_path}")

                # Get WSE dataset (shape: num_profiles × num_cross_sections)
                wse_ds = hdf_file[wse_path]
                wse_data = wse_ds[()]
                num_profiles, num_xs = wse_data.shape

                # Get profile names
                if profile_names_path in hdf_file:
                    profile_names_raw = hdf_file[profile_names_path][()]
                    profile_names = [
                        name.decode('utf-8').strip() if isinstance(name, bytes) else str(name).strip()
                        for name in profile_names_raw
                    ]
                else:
                    # Fallback to numbered profiles
                    profile_names = [f"Profile_{i+1}" for i in range(num_profiles)]

                # Get cross section attributes
                xs_attrs = hdf_file[xs_attrs_path][()]

                # Determine which profiles to extract
                if profile_name is not None:
                    # Find profile by name
                    try:
                        profile_idx = profile_names.index(profile_name)
                    except ValueError:
                        raise ValueError(
                            f"Profile name '{profile_name}' not found. "
                            f"Available profiles: {profile_names}"
                        )
                    profiles_to_extract = [(profile_idx, profile_name)]

                elif profile_index is not None:
                    # Validate profile index
                    if profile_index < 0 or profile_index >= num_profiles:
                        raise ValueError(
                            f"Profile index {profile_index} out of range. "
                            f"Valid range: 0 to {num_profiles-1}"
                        )
                    profiles_to_extract = [(profile_index, profile_names[profile_index])]

                else:
                    # Extract all profiles
                    profiles_to_extract = list(enumerate(profile_names))

                # Build DataFrame
                rows = []
                for prof_idx, prof_name in profiles_to_extract:
                    wse_values = wse_data[prof_idx, :]

                    for xs_idx in range(num_xs):
                        river = xs_attrs[xs_idx]['River']
                        reach = xs_attrs[xs_idx]['Reach']
                        station = xs_attrs[xs_idx]['Station']

                        # Decode byte strings
                        river = river.decode('utf-8') if isinstance(river, bytes) else str(river)
                        reach = reach.decode('utf-8') if isinstance(reach, bytes) else str(reach)
                        station = station.decode('utf-8') if isinstance(station, bytes) else str(station)

                        row = {
                            'River': river.strip(),
                            'Reach': reach.strip(),
                            'Station': station.strip(),
                            'WSE': float(wse_values[xs_idx])
                        }

                        # Only add Profile column if extracting multiple profiles
                        if len(profiles_to_extract) > 1:
                            row['Profile'] = prof_name

                        rows.append(row)

                df = pd.DataFrame(rows)

                # Reorder columns
                if 'Profile' in df.columns:
                    df = df[['River', 'Reach', 'Station', 'Profile', 'WSE']]
                else:
                    df = df[['River', 'Reach', 'Station', 'WSE']]

                logger.info(
                    f"Extracted WSE data for {len(profiles_to_extract)} profile(s), "
                    f"{num_xs} cross sections"
                )

                return df

        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except KeyError as e:
            raise KeyError(f"Error accessing steady state WSE data: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading steady state WSE data: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_steady_info(hdf_path: Path) -> pd.DataFrame:
        """
        Get steady flow attributes and metadata from HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing steady flow attributes including:
                - Program Name
                - Program Version
                - Type of Run
                - Run Time Window
                - Solution status
                - And other metadata attributes

        Raises:
            FileNotFoundError: If the specified HDF file is not found
            KeyError: If steady state results are not found
            ValueError: If the plan is not a steady state plan

        Example:
            >>> info_df = HdfResultsPlan.get_steady_info('01')
            >>> print(info_df['Solution'].values[0])
            'Steady Finished Successfully'
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Check if this is a steady state plan
                if "Results/Steady" not in hdf_file:
                    raise ValueError(f"HDF file does not contain steady state results: {hdf_path.name}")

                attrs_dict = {}

                # Get attributes from Results/Steady/Output
                output_path = "Results/Steady/Output"
                if output_path in hdf_file:
                    output_group = hdf_file[output_path]
                    for key, value in output_group.attrs.items():
                        if isinstance(value, bytes):
                            attrs_dict[key] = value.decode('utf-8')
                        else:
                            attrs_dict[key] = value

                # Get attributes from Results/Steady/Summary
                summary_path = "Results/Steady/Summary"
                if summary_path in hdf_file:
                    summary_group = hdf_file[summary_path]
                    for key, value in summary_group.attrs.items():
                        if isinstance(value, bytes):
                            attrs_dict[key] = value.decode('utf-8')
                        else:
                            attrs_dict[key] = value

                # Add flow file information from Plan Data
                plan_info_path = "Plan Data/Plan Information"
                if plan_info_path in hdf_file:
                    plan_info = hdf_file[plan_info_path]
                    for key in ['Flow Filename', 'Flow Title']:
                        if key in plan_info.attrs:
                            value = plan_info.attrs[key]
                            if isinstance(value, bytes):
                                attrs_dict[key] = value.decode('utf-8')
                            else:
                                attrs_dict[key] = value

                if not attrs_dict:
                    logger.warning("No steady state attributes found in HDF file")
                    return pd.DataFrame()

                logger.info(f"Extracted {len(attrs_dict)} steady state attributes")
                return pd.DataFrame(attrs_dict, index=[0])

        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except KeyError as e:
            raise KeyError(f"Error accessing steady state info: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading steady state info: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_compute_messages(hdf_path: Path) -> str:
        """
        Read computation messages from HDF file with fallback to .txt file.

        Extracts computation messages from the HDF Results/Summary structure.
        This includes detailed information about the computation process,
        warnings, errors, convergence information, and performance metrics.

        If HDF path not found, falls back to .txt file extraction using RasControl.

        Args:
            hdf_path: Path to plan HDF file (or plan number string if using
                     standardize_input decorator, which resolves to HDF path)

        Returns:
            String containing computation messages, or empty string if unavailable

        Example:
            >>> from ras_commander import init_ras_project, HdfResultsPlan
            >>> init_ras_project(r"/path/to/project", "6.5")
            >>> msgs = HdfResultsPlan.get_compute_messages("01")
            >>> print(msgs)

        Note:
            Modern HEC-RAS versions (6.x+) store computation messages in HDF:
            /Results/Summary/Compute Messages (text)

            Older versions (pre-6.x) use .txt files which are accessed via
            fallback to RasControl.get_comp_msgs()

            Function naming follows HDF structure conventions (get_compute_messages)
            vs RasControl legacy naming (get_comp_msgs) to reflect technological lineage.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Define HDF path for compute messages
                compute_msgs_path = "Results/Summary/Compute Messages (text)"

                # Check if path exists in HDF
                if compute_msgs_path not in hdf_file:
                    logger.warning(
                        f"Compute Messages not found in HDF at '{compute_msgs_path}', "
                        f"falling back to .txt file extraction"
                    )

                    # Fallback to .txt file using RasControl
                    try:
                        # Late import to avoid circular dependency
                        from .RasControl import RasControl

                        # Extract plan info from HDF path
                        # e.g., "C:/path/BaldEagle.p10.hdf" -> use path for RasControl
                        txt_contents = RasControl.get_comp_msgs(hdf_path)
                        if txt_contents:
                            logger.info(f"Successfully retrieved {len(txt_contents)} characters from .txt file")
                            return txt_contents
                    except Exception as e:
                        logger.debug(f".txt file fallback failed: {e}")

                    # Both methods failed
                    logger.debug(
                        f"No computation messages found in HDF or .txt sources for {hdf_path.name}"
                    )
                    return ""

                # Read dataset from HDF
                logger.info(f"Reading computation messages from HDF: {hdf_path.name}")
                dataset = hdf_file[compute_msgs_path]
                data = dataset[()]

                # Decode byte string to UTF-8
                if isinstance(data, bytes):
                    contents = data.decode('utf-8', errors='ignore')
                elif isinstance(data, np.ndarray) and len(data) > 0:
                    # Handle array of byte strings
                    if isinstance(data[0], bytes):
                        contents = data[0].decode('utf-8', errors='ignore')
                    else:
                        contents = str(data[0])
                else:
                    contents = str(data)

                logger.info(f"Successfully extracted {len(contents)} characters from HDF")
                return contents

        except FileNotFoundError:
            logger.debug(f"HDF file not found: {hdf_path}")

            # Try .txt fallback
            try:
                from .RasControl import RasControl
                txt_contents = RasControl.get_comp_msgs(hdf_path)
                if txt_contents:
                    logger.warning(
                        f"HDF file not found, successfully retrieved computation messages from .txt file"
                    )
                    return txt_contents
            except Exception as e:
                logger.debug(f".txt file fallback failed: {e}")

            logger.debug(f"No computation messages found for {hdf_path.name}")
            return ""

        except Exception as e:
            logger.debug(f"Error reading computation messages from HDF: {str(e)}")

            # Try .txt fallback on any HDF error
            try:
                from .RasControl import RasControl
                txt_contents = RasControl.get_comp_msgs(hdf_path)
                if txt_contents:
                    logger.warning(
                        f"HDF extraction failed, successfully retrieved computation messages from .txt file"
                    )
                    return txt_contents
            except Exception as fallback_error:
                logger.debug(f".txt file fallback failed: {fallback_error}")

            logger.debug(f"No computation messages found for {hdf_path.name}")
            return ""
==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsPlot.py
==================================================
"""
Class: HdfResultsPlot

A collection of static methods for visualizing HEC-RAS results data from HDF files using matplotlib.

Public Functions:
    plot_results_mesh_variable(variable_df, variable_name, colormap='viridis', point_size=10):
        Generic plotting function for any mesh variable with customizable styling.
        
    plot_results_max_wsel(max_ws_df):
        Visualizes the maximum water surface elevation distribution across mesh cells.
        
    plot_results_max_wsel_time(max_ws_df):
        Displays the timing of maximum water surface elevation for each cell,
        including statistics about the temporal distribution.

Requirements:
    - matplotlib
    - pandas
    - geopandas (for geometry handling)

Input DataFrames must contain:
    - 'geometry' column with Point objects containing x,y coordinates
    - Variable data columns as specified in individual function docstrings
"""

import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict
from .Decorators import log_call
from .HdfMesh import HdfMesh

class HdfResultsPlot:
    """
    A class containing static methods for plotting HEC-RAS results data.
    
    This class provides visualization methods for various types of HEC-RAS results,
    including maximum water surface elevations and timing information.
    """

    @staticmethod
    @log_call
    def plot_results_max_wsel(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the maximum water surface elevation per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface elevations.
        """
        # Extract x and y coordinates from the geometry column
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            print("Error: 'x' or 'y' columns not found in the merged dataframe.")
            print("Available columns:", max_ws_df.columns.tolist())
            return

        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=max_ws_df['maximum_water_surface'], 
                           cmap='viridis', s=10)

        ax.set_title('Max Water Surface per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label='Max Water Surface (ft)')

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

    @staticmethod
    @log_call
    def plot_results_max_wsel_time(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the time of the maximum water surface elevation (WSEL) per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface timing information.
        """
        # Convert datetime strings using the renamed utility function
        max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])
        
        # Extract coordinates
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            raise ValueError("x and y coordinates are missing from the DataFrame. Make sure the 'geometry' column exists and contains valid coordinate data.")

        fig, ax = plt.subplots(figsize=(12, 8))

        min_time = max_ws_df['max_wsel_time'].min()
        color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600

        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=color_values, cmap='viridis', s=10)

        ax.set_title('Time of Maximum Water Surface Elevation per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')

        cbar = plt.colorbar(scatter)
        cbar.set_label('Hours since simulation start')
        cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))
        cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

        # Print timing information
        print(f"\nSimulation Start Time: {min_time}")
        print(f"Time Range: {color_values.max():.1f} hours")
        print("\nTiming Statistics (hours since start):")
        print(color_values.describe()) 

    @staticmethod
    @log_call
    def plot_results_mesh_variable(variable_df: pd.DataFrame, variable_name: str, colormap: str = 'viridis', point_size: int = 10) -> None:
        """
        Plot any mesh variable with consistent styling.
        
        Args:
            variable_df (pd.DataFrame): DataFrame containing the variable data
            variable_name (str): Name of the variable (for labels)
            colormap (str): Matplotlib colormap to use. Default: 'viridis'
            point_size (int): Size of the scatter points. Default: 10

        Returns:
            None

        Raises:
            ImportError: If matplotlib is not installed
            ValueError: If required columns are missing from variable_df
        """
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            logger.error("matplotlib is required for plotting. Please install it with 'pip install matplotlib'")
            raise ImportError("matplotlib is required for plotting")

        # Get cell coordinates if not in variable_df
        if 'geometry' not in variable_df.columns:
            cell_coords = HdfMesh.mesh_cell_points(plan_hdf_path)
            merged_df = pd.merge(variable_df, cell_coords, on=['mesh_name', 'cell_id'])
        else:
            merged_df = variable_df
            
        # Extract coordinates, handling None values
        merged_df = merged_df.dropna(subset=['geometry'])
        merged_df['x'] = merged_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        merged_df['y'] = merged_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)
        
        # Drop any rows with None coordinates
        merged_df = merged_df.dropna(subset=['x', 'y'])
        
        if len(merged_df) == 0:
            logger.error("No valid coordinates found for plotting")
            raise ValueError("No valid coordinates found for plotting")
            
        # Create plot
        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(merged_df['x'], merged_df['y'], 
                           c=merged_df[variable_name], 
                           cmap=colormap, 
                           s=point_size)
        
        # Customize plot
        ax.set_title(f'{variable_name} per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label=variable_name)
        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsXsec.py
==================================================
"""
Class: HdfResultsXsec

Contains methods for extracting 1D results data from HDF files. 
This includes cross section timeseries, structures and reference line/point timeseries as these are all 1D elements.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfResultsXsec:
- get_xsec_timeseries(): Extract cross-section timeseries data including water surface, velocity, and flow
- get_ref_lines_timeseries(): Get timeseries output for reference lines
- get_ref_points_timeseries(): Get timeseries output for reference points

TO BE IMPLEMENTED: 
DSS Hydrograph Extraction for 1D and 2D Structures. 

Planned functions:
- get_bridge_timeseries(): Extract timeseries data for bridge structures
- get_inline_structures_timeseries(): Extract timeseries data for inline structures

Notes:
- All functions use the get_ prefix to indicate they return data
- Results data functions use results_ prefix to indicate they handle results data
- All functions include proper error handling and logging
- Functions return xarray Datasets for efficient handling of multi-dimensional data
"""

from pathlib import Path
from typing import Union, Optional, List, Dict, Tuple

import h5py
import numpy as np
import pandas as pd
import xarray as xr

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfResultsXsec:
    """
    A static class for extracting and processing 1D results data from HEC-RAS HDF files.

    This class provides methods to extract and process unsteady flow simulation results
    for cross-sections, reference lines, and reference points. All methods are static
    and designed to be used without class instantiation.

    The class handles:
    - Cross-section timeseries (water surface, velocity, flow)
    - Reference line timeseries
    - Reference point timeseries

    Dependencies:
        - HdfBase: Core HDF file operations
        - HdfUtils: Utility functions for HDF processing
    """


# Tested functions from AWS webinar where the code was developed
# Need to add examples


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_xsec_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract Water Surface, Velocity Total, Velocity Channel, Flow Lateral, and Flow data from HEC-RAS HDF file.
        Includes Cross Section Only and Cross Section Attributes as coordinates in the xarray.Dataset.
        Also calculates maximum values for key parameters.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Xarray Dataset containing the extracted cross-section results with appropriate coordinates and attributes.
            Includes maximum values for Water Surface, Flow, Channel Velocity, Total Velocity, and Lateral Flow.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Define base paths
                base_output_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections/"
                time_stamp_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
                
                # Extract Cross Section Attributes
                attrs_dataset = hdf_file[f"{base_output_path}Cross Section Attributes"][:]
                rivers = [attr['River'].decode('utf-8').strip() for attr in attrs_dataset]
                reaches = [attr['Reach'].decode('utf-8').strip() for attr in attrs_dataset]
                stations = [attr['Station'].decode('utf-8').strip() for attr in attrs_dataset]
                names = [attr['Name'].decode('utf-8').strip() for attr in attrs_dataset]
                
                # Extract Cross Section Only (Unique Names)
                cross_section_only_dataset = hdf_file[f"{base_output_path}Cross Section Only"][:]
                cross_section_names = [cs.decode('utf-8').strip() for cs in cross_section_only_dataset]
                
                # Extract Time Stamps and convert to datetime
                time_stamps = hdf_file[time_stamp_path][:]
                if any(isinstance(ts, bytes) for ts in time_stamps):
                    time_stamps = [ts.decode('utf-8') for ts in time_stamps]
                # Convert RAS format timestamps to datetime
                times = pd.to_datetime(time_stamps, format='%d%b%Y %H:%M:%S:%f')
                
                # Extract Required Datasets
                water_surface = hdf_file[f"{base_output_path}Water Surface"][:]
                velocity_total = hdf_file[f"{base_output_path}Velocity Total"][:]
                velocity_channel = hdf_file[f"{base_output_path}Velocity Channel"][:]
                flow_lateral = hdf_file[f"{base_output_path}Flow Lateral"][:]
                flow = hdf_file[f"{base_output_path}Flow"][:]
                
                # Calculate maximum values along time axis
                max_water_surface = np.max(water_surface, axis=0)
                max_flow = np.max(flow, axis=0)
                max_velocity_channel = np.max(velocity_channel, axis=0)
                max_velocity_total = np.max(velocity_total, axis=0)
                max_flow_lateral = np.max(flow_lateral, axis=0)
                
                # Create Xarray Dataset
                ds = xr.Dataset(
                    {
                        'Water_Surface': (['time', 'cross_section'], water_surface),
                        'Velocity_Total': (['time', 'cross_section'], velocity_total),
                        'Velocity_Channel': (['time', 'cross_section'], velocity_channel),
                        'Flow_Lateral': (['time', 'cross_section'], flow_lateral),
                        'Flow': (['time', 'cross_section'], flow),
                    },
                    coords={
                        'time': times,
                        'cross_section': cross_section_names,
                        'River': ('cross_section', rivers),
                        'Reach': ('cross_section', reaches),
                        'Station': ('cross_section', stations),
                        'Name': ('cross_section', names),
                        'Maximum_Water_Surface': ('cross_section', max_water_surface),
                        'Maximum_Flow': ('cross_section', max_flow),
                        'Maximum_Channel_Velocity': ('cross_section', max_velocity_channel),
                        'Maximum_Velocity_Total': ('cross_section', max_velocity_total),
                        'Maximum_Flow_Lateral': ('cross_section', max_flow_lateral)
                    },
                    attrs={
                        'description': 'Cross-section results extracted from HEC-RAS HDF file',
                        'source_file': str(hdf_path)
                    }
                )
                
                return ds

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting cross section results: {e}")
            raise



    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_lines_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference lines from HEC-RAS HDF file.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing flow, velocity, and water surface data for reference lines.
            Returns empty dataset if reference line data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="lines")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_points_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference points from HEC-RAS HDF file.

        This method extracts flow, velocity, and water surface elevation data for all
        reference points defined in the model. Reference points are user-defined locations
        where detailed output is desired.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing the following variables for each reference point:
            - Flow [cfs or m³/s]
            - Velocity [ft/s or m/s]
            - Water Surface [ft or m]
            
            The dataset includes coordinates:
            - time: Simulation timesteps
            - refpt_id: Unique identifier for each reference point
            - refpt_name: Name of each reference point
            - mesh_name: Associated 2D mesh area name
            
            Returns empty dataset if reference point data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file

        Examples:
        --------
        >>> ds = HdfResultsXsec.get_ref_points_timeseries("path/to/plan.hdf")
        >>> # Get water surface timeseries for first reference point
        >>> ws = ds['Water Surface'].isel(refpt_id=0)
        >>> # Get all data for a specific reference point by name
        >>> point_data = ds.sel(refpt_name='Point1')
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="points")
    

    @staticmethod
    def _reference_timeseries_output(hdf_file: h5py.File, reftype: str = "lines") -> xr.Dataset:
        """
        Internal method to return timeseries output data for reference lines or points from a HEC-RAS HDF plan file.

        Parameters
        ----------
        hdf_file : h5py.File
            Open HDF file object.
        reftype : str, optional
            The type of reference data to retrieve. Must be either "lines" or "points".
            (default: "lines")

        Returns
        -------
        xr.Dataset
            An xarray Dataset with reference line or point timeseries data.
            Returns an empty Dataset if the reference output data is not found.

        Raises
        ------
        ValueError
            If reftype is not "lines" or "points".
        """
        if reftype == "lines":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Lines"
            abbrev = "refln"
        elif reftype == "points":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Points"
            abbrev = "refpt"
        else:
            raise ValueError('reftype must be either "lines" or "points".')

        try:
            reference_group = hdf_file[output_path]
        except KeyError:
            logger.error(f"Could not find HDF group at path '{output_path}'. "
                         f"The Plan HDF file may not contain reference {reftype[:-1]} output data.")
            return xr.Dataset()

        reference_names = reference_group["Name"][:]
        names = []
        mesh_areas = []
        for s in reference_names:
            name, mesh_area = s.decode("utf-8").split("|")
            names.append(name)
            mesh_areas.append(mesh_area)

        times = HdfBase.get_unsteady_timestamps(hdf_file)

        das = {}
        for var in ["Flow", "Velocity", "Water Surface"]:
            group = reference_group.get(var)
            if group is None:
                continue
            values = group[:]
            units = group.attrs["Units"].decode("utf-8")
            da = xr.DataArray(
                values,
                name=var,
                dims=["time", f"{abbrev}_id"],
                coords={
                    "time": times,
                    f"{abbrev}_id": range(values.shape[1]),
                    f"{abbrev}_name": (f"{abbrev}_id", names),
                    "mesh_name": (f"{abbrev}_id", mesh_areas),
                },
                attrs={"units": units, "hdf_path": f"{output_path}/{var}"},
            )
            das[var] = da
        return xr.Dataset(das)

==================================================

File: c:\GH\ras-commander\ras_commander\HdfStruc.py
==================================================
"""
Class: HdfStruc

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfStruc:
- get_structures()
- get_geom_structures_attrs()
"""
from typing import Dict, Any, List, Union
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point, GeometryCollection
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfBase import HdfBase
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfStruc:
    """
    Handles 2D structure geometry data extraction from HEC-RAS HDF files.

    This class provides static methods for extracting and analyzing structure geometries
    and their attributes from HEC-RAS geometry HDF files. All methods are designed to work
    without class instantiation.

    Notes
    -----
    - 1D Structure data should be accessed via the HdfResultsXsec class
    - All methods use @standardize_input for consistent file handling
    - All methods use @log_call for operation logging
    - Returns GeoDataFrames with both geometric and attribute data
    """
    
    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_structures(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts structure data from a HEC-RAS geometry HDF5 file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF5 file
        datetime_to_str : bool, optional
            If True, converts datetime objects to ISO format strings, by default False

        Returns
        -------
        GeoDataFrame
            Structure data with columns:
            - Structure ID: unique identifier
            - Geometry: LineString of structure centerline
            - Various attribute columns from the HDF file
            - Profile_Data: list of station/elevation dictionaries
            - Bridge coefficient attributes (if present)
            - Table info attributes (if present)

        Notes
        -----
        - Group-level attributes are stored in GeoDataFrame.attrs['group_attributes']
        - Invalid geometries are dropped with warning
        - All byte strings are decoded to UTF-8
        - CRS is preserved from the source file
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                if "Geometry/Structures" not in hdf:
                    logger.error(f"No Structures Found in the HDF, Empty Geodataframe Returned: {hdf_path}")
                    return GeoDataFrame()
                
                # Check if required datasets exist
                required_datasets = [
                    "Geometry/Structures/Centerline Info",
                    "Geometry/Structures/Centerline Points"
                ]
                
                for dataset in required_datasets:
                    if dataset not in hdf:
                        logger.error(f"No Structures Found in the HDF, Empty Geodataframe Returned: {hdf_path}")
                        return GeoDataFrame()

                def get_dataset_df(path: str) -> pd.DataFrame:
                    """
                    Converts an HDF5 dataset to a pandas DataFrame.

                    Parameters
                    ----------
                    path : str
                        Dataset path within the HDF5 file

                    Returns
                    -------
                    pd.DataFrame
                        DataFrame containing the dataset values.
                        - For compound datasets, column names match field names
                        - For simple datasets, generic column names (Value_0, Value_1, etc.)
                        - Empty DataFrame if dataset not found

                    Notes
                    -----
                    Automatically decodes byte strings to UTF-8 with error handling.
                    """
                    if path not in hdf:
                        logger.warning(f"Dataset not found: {path}")
                        return pd.DataFrame()
                    
                    data = hdf[path][()]
                    
                    if data.dtype.names:
                        df = pd.DataFrame(data)
                        # Decode byte strings to UTF-8
                        for col in df.columns:
                            if df[col].dtype.kind in {'S', 'a'}:  # Byte strings
                                df[col] = df[col].str.decode('utf-8', errors='ignore')
                        return df
                    else:
                        # If no named fields, assign generic column names
                        return pd.DataFrame(data, columns=[f'Value_{i}' for i in range(data.shape[1])])

                # Extract relevant datasets
                group_attrs = HdfBase.get_attrs(hdf, "Geometry/Structures")
                struct_attrs = get_dataset_df("Geometry/Structures/Attributes")
                bridge_coef = get_dataset_df("Geometry/Structures/Bridge Coefficient Attributes")
                table_info = get_dataset_df("Geometry/Structures/Table Info")
                profile_data = get_dataset_df("Geometry/Structures/Profile Data")

                # Assign 'Structure ID' based on index (starting from 1)
                struct_attrs.reset_index(drop=True, inplace=True)
                struct_attrs['Structure ID'] = range(1, len(struct_attrs) + 1)
                logger.debug(f"Assigned Structure IDs: {struct_attrs['Structure ID'].tolist()}")

                # Check if 'Structure ID' was successfully assigned
                if 'Structure ID' not in struct_attrs.columns:
                    logger.error("'Structure ID' column could not be assigned to Structures/Attributes.")
                    return GeoDataFrame()

                # Get centerline geometry
                centerline_info = hdf["Geometry/Structures/Centerline Info"][()]
                centerline_points = hdf["Geometry/Structures/Centerline Points"][()]
                
                # Create LineString geometries for each structure
                geoms = []
                for i in range(len(centerline_info)):
                    start_idx = centerline_info[i][0]  # Point Starting Index
                    point_count = centerline_info[i][1]  # Point Count
                    points = centerline_points[start_idx:start_idx + point_count]
                    if len(points) >= 2:
                        geoms.append(LineString(points))
                    else:
                        logger.warning(f"Insufficient points for LineString in structure index {i}.")
                        geoms.append(None)

                # Create base GeoDataFrame with Structures Attributes and geometries
                struct_gdf = GeoDataFrame(
                    struct_attrs,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Drop entries with invalid geometries
                initial_count = len(struct_gdf)
                struct_gdf = struct_gdf.dropna(subset=['geometry']).reset_index(drop=True)
                final_count = len(struct_gdf)
                if final_count < initial_count:
                    logger.warning(f"Dropped {initial_count - final_count} structures due to invalid geometries.")

                # Merge Bridge Coefficient Attributes on 'Structure ID'
                if not bridge_coef.empty and 'Structure ID' in bridge_coef.columns:
                    struct_gdf = struct_gdf.merge(
                        bridge_coef,
                        on='Structure ID',
                        how='left',
                        suffixes=('', '_bridge_coef')
                    )
                    logger.debug("Merged Bridge Coefficient Attributes successfully.")
                else:
                    logger.warning("Bridge Coefficient Attributes missing or 'Structure ID' not present.")

                # Merge Table Info based on the DataFrame index (one-to-one correspondence)
                if not table_info.empty:
                    if len(table_info) != len(struct_gdf):
                        logger.warning("Table Info count does not match Structures count. Skipping merge.")
                    else:
                        struct_gdf = pd.concat([struct_gdf, table_info.reset_index(drop=True)], axis=1)
                        logger.debug("Merged Table Info successfully.")
                else:
                    logger.warning("Table Info dataset is empty or missing.")

                # Process Profile Data based on Table Info
                if not profile_data.empty and not table_info.empty:
                    # Assuming 'Centerline Profile (Index)' and 'Centerline Profile (Count)' are in 'Table Info'
                    if ('Centerline Profile (Index)' in table_info.columns and
                        'Centerline Profile (Count)' in table_info.columns):
                        struct_gdf['Profile_Data'] = struct_gdf.apply(
                            lambda row: [
                                {'Station': float(profile_data.iloc[i, 0]),
                                 'Elevation': float(profile_data.iloc[i, 1])}
                                for i in range(
                                    int(row['Centerline Profile (Index)']),
                                    int(row['Centerline Profile (Index)']) + int(row['Centerline Profile (Count)'])
                                )
                            ],
                            axis=1
                        )
                        logger.debug("Processed Profile Data successfully.")
                    else:
                        logger.warning("Required columns for Profile Data not found in Table Info.")
                else:
                    logger.warning("Profile Data dataset is empty or Table Info is missing.")

                # Convert datetime columns to string if requested
                if datetime_to_str:
                    datetime_cols = struct_gdf.select_dtypes(include=['datetime64']).columns
                    for col in datetime_cols:
                        struct_gdf[col] = struct_gdf[col].dt.isoformat()
                        logger.debug(f"Converted datetime column '{col}' to string.")

                # Ensure all byte strings are decoded (if any remain)
                for col in struct_gdf.columns:
                    if struct_gdf[col].dtype == object:
                        struct_gdf[col] = struct_gdf[col].apply(
                            lambda x: x.decode('utf-8', errors='ignore') if isinstance(x, bytes) else x
                        )

                # Final GeoDataFrame
                logger.info("Successfully extracted structures GeoDataFrame.")
                
                # Add group attributes to the GeoDataFrame's attrs['group_attributes']
                struct_gdf.attrs['group_attributes'] = group_attrs
                
                logger.info("Successfully extracted structures GeoDataFrame with attributes.")
                
                return struct_gdf

        except Exception as e:
            logger.error(f"Error reading structures from {hdf_path}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_geom_structures_attrs(hdf_path: Path) -> pd.DataFrame:
        """
        Extracts structure attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        pd.DataFrame
            DataFrame containing structure attributes from the Geometry/Structures group.
            Returns empty DataFrame if no structures are found.

        Notes
        -----
        Attributes are extracted from the HDF5 group 'Geometry/Structures'.
        All byte strings in attributes are automatically decoded to UTF-8.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/Structures" not in hdf_file:
                    logger.info(f"No structures found in the geometry file: {hdf_path}")
                    return pd.DataFrame()
                
                # Get attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Geometry/Structures"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])

        except Exception as e:
            logger.error(f"Error reading geometry structures attributes: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def list_sa2d_connections(hdf_path: Path, *, ras_object=None) -> List[str]:
        """
        List all SA/2D Area Connection structures in HDF results file.

        This includes both breach structures and regular SA/2D connections with
        time series results.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        List[str]
            Names of all SA/2D Area Connection structures with time series results.
            Returns empty list if no SA/2D connections found.

        Examples
        --------
        >>> structures = HdfStruc.list_sa2d_connections("02")
        >>> print(structures)
        ['Laxton_Dam', 'PineCreek#1_Dam', 'US_2DArea_Res2']

        Notes
        -----
        - Not all structures returned have breach capability
        - Use get_sa2d_breach_info() to determine which have "Breaching Variables"
        - Empty list returned if no SA/2D connections in results
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/SA 2D Area Conn"

                if base_path not in hdf_file:
                    logger.warning(f"No SA 2D Area Conn data found in {hdf_path.name}")
                    return []

                # List all groups (structure names) under SA 2D Area Conn
                structures = list(hdf_file[base_path].keys())
                logger.info(f"Found {len(structures)} SA/2D connection structures: {structures}")
                return structures

        except Exception as e:
            logger.error(f"Error listing SA/2D connection structures: {e}")
            return []

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_sa2d_breach_info(hdf_path: Path, *, ras_object=None) -> pd.DataFrame:
        """
        Get information about which SA/2D connection structures have breach capability.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            DataFrame with columns:
            - structure: Structure name
            - has_breach: Boolean, True if "Breaching Variables" dataset exists
            - breach_at_time: Time of breach initiation (if available)
            - breach_at_date: Date/time of breach (if available)
            - centerline_breach: Centerline station for breach (if available)

        Examples
        --------
        >>> info = HdfStruc.get_sa2d_breach_info("02")
        >>> breach_dams = info[info['has_breach']]['structure'].tolist()
        >>> print(f"Breach structures: {breach_dams}")

        Notes
        -----
        - Returns empty DataFrame if no SA/2D connections found
        - Only structures with "Breaching Variables" have has_breach=True
        - Use in conjunction with RasBreach for reading/modifying breach parameters
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                structures = HdfStruc.list_sa2d_connections(hdf_path, ras_object=ras_object)

                if not structures:
                    return pd.DataFrame()

                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/SA 2D Area Conn"

                info_list = []
                for struct_name in structures:
                    struct_path = f"{base_path}/{struct_name}"
                    breach_var_path = f"{struct_path}/Breaching Variables"

                    info = {'structure': struct_name}

                    # Check if breach variables exist
                    if breach_var_path in hdf_file:
                        info['has_breach'] = True

                        # Extract breach metadata from attributes
                        breach_dataset = hdf_file[breach_var_path]
                        if 'Breach at' in breach_dataset.attrs:
                            breach_at = breach_dataset.attrs['Breach at']
                            info['breach_at_date'] = breach_at.decode('utf-8') if isinstance(breach_at, bytes) else breach_at
                        else:
                            info['breach_at_date'] = None

                        if 'Breach at Time (Days)' in breach_dataset.attrs:
                            info['breach_at_time'] = float(breach_dataset.attrs['Breach at Time (Days)'])
                        else:
                            info['breach_at_time'] = None

                        if 'Centerline Breach' in breach_dataset.attrs:
                            info['centerline_breach'] = float(breach_dataset.attrs['Centerline Breach'])
                        else:
                            info['centerline_breach'] = None
                    else:
                        info['has_breach'] = False
                        info['breach_at_date'] = None
                        info['breach_at_time'] = None
                        info['centerline_breach'] = None

                    info_list.append(info)

                return pd.DataFrame(info_list)

        except Exception as e:
            logger.error(f"Error getting SA/2D breach info: {e}")
            raise

==================================================

File: c:\GH\ras-commander\ras_commander\HdfUtils.py
==================================================
"""
HdfUtils Class
-------------

A utility class providing static methods for working with HEC-RAS HDF files.

Attribution:
    A substantial amount of code in this file is sourced or derived from the 
    https://github.com/fema-ffrd/rashdf library, released under MIT license 
    and Copyright (c) 2024 fema-ffrd. The file has been forked and modified 
    for use in RAS Commander.

Key Features:
- HDF file data conversion and parsing
- DateTime handling for RAS-specific formats
- Spatial operations using KDTree
- HDF attribute management

Main Method Categories:

1. Data Conversion
    - convert_ras_string: Convert RAS HDF strings to Python objects
    - convert_ras_hdf_value: Convert general HDF values to Python objects
    - convert_df_datetimes_to_str: Convert DataFrame datetime columns to strings
    - convert_hdf5_attrs_to_dict: Convert HDF5 attributes to dictionary
    - convert_timesteps_to_datetimes: Convert timesteps to datetime objects

2. Spatial Operations
    - perform_kdtree_query: KDTree search between datasets
    - find_nearest_neighbors: Find nearest neighbors within dataset

3. DateTime Parsing
    - parse_ras_datetime: Parse standard RAS datetime format (ddMMMYYYY HH:MM:SS)
    - parse_ras_window_datetime: Parse simulation window datetime (ddMMMYYYY HHMM)
    - parse_duration: Parse duration strings (HH:MM:SS)
    - parse_ras_datetime_ms: Parse datetime with milliseconds
    - parse_run_time_window: Parse time window strings

Usage Notes:
- All methods are static and can be called without class instantiation
- Methods handle both raw HDF data and converted Python objects
- Includes comprehensive error handling for RAS-specific data formats
- Supports various RAS datetime formats and conversions
"""
import logging
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Union, Optional, Dict, List, Tuple, Any
from scipy.spatial import KDTree
import re
from shapely.geometry import LineString  # Import LineString to avoid NameError

from .Decorators import standardize_input, log_call 
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfUtils:
    """
    Utility class for working with HEC-RAS HDF files.

    This class provides general utility functions for HDF file operations,
    including attribute extraction, data conversion, and common HDF queries.
    It also includes spatial operations and helper methods for working with
    HEC-RAS specific data structures.

    Note:
    - Use this class for general HDF utility functions that are not specific to plan or geometry files.
    - All methods in this class are static and can be called without instantiating the class.
    """




# RENAME TO convert_ras_string and make public

    @staticmethod
    def convert_ras_string(value: Union[str, bytes]) -> Union[bool, datetime, List[datetime], timedelta, str]:
        """
        Convert a string value from an HEC-RAS HDF file into a Python object.

        Args:
            value (Union[str, bytes]): The value to convert.

        Returns:
            Union[bool, datetime, List[datetime], timedelta, str]: The converted value.
        """
        if isinstance(value, bytes):
            s = value.decode("utf-8")
        else:
            s = value

        if s == "True":
            return True
        elif s == "False":
            return False
        
        ras_datetime_format1_re = r"\d{2}\w{3}\d{4} \d{2}:\d{2}:\d{2}"
        ras_datetime_format2_re = r"\d{2}\w{3}\d{4} \d{2}\d{2}"
        ras_duration_format_re = r"\d{2}:\d{2}:\d{2}"

        if re.match(rf"^{ras_datetime_format1_re}", s):
            if re.match(rf"^{ras_datetime_format1_re} to {ras_datetime_format1_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_datetime(split[0]),
                    HdfUtils.parse_ras_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_datetime(s)
        elif re.match(rf"^{ras_datetime_format2_re}", s):
            if re.match(rf"^{ras_datetime_format2_re} to {ras_datetime_format2_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_window_datetime(split[0]),
                    HdfUtils.parse_ras_window_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_window_datetime(s)
        elif re.match(rf"^{ras_duration_format_re}$", s):
            return HdfUtils.parse_ras_duration(s)
        return s





    @staticmethod
    def convert_ras_hdf_value(value: Any) -> Union[None, bool, str, List[str], int, float, List[int], List[float]]:
        """
        Convert a value from a HEC-RAS HDF file into a Python object.

        Args:
            value (Any): The value to convert.

        Returns:
            Union[None, bool, str, List[str], int, float, List[int], List[float]]: The converted value.
        """
        if isinstance(value, np.floating) and np.isnan(value):
            return None
        elif isinstance(value, (bytes, np.bytes_)):
            return value.decode('utf-8')
        elif isinstance(value, np.integer):
            return int(value)
        elif isinstance(value, np.floating):
            return float(value)
        elif isinstance(value, (int, float)):
            return value
        elif isinstance(value, (list, tuple, np.ndarray)):
            if len(value) > 1:
                return [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                return HdfUtils.convert_ras_hdf_value(value[0])
        else:
            return str(value)










# RENAME TO convert_df_datetimes_to_str 

    @staticmethod
    def convert_df_datetimes_to_str(df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert any datetime64 columns in a DataFrame to strings.

        Args:
            df (pd.DataFrame): The DataFrame to convert.

        Returns:
            pd.DataFrame: The DataFrame with datetime columns converted to strings.
        """
        for col in df.select_dtypes(include=['datetime64']).columns:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
        return df


# KDTree Methods: 


    @staticmethod
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = HdfUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = HdfUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped




# Datetime Parsing Methods: 

    @staticmethod
    @log_call
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Public method to parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
# Rename to convert_timesteps_to_datetimes and make public
    @staticmethod
    def convert_timesteps_to_datetimes(timesteps: np.ndarray, start_time: datetime, time_unit: str = "days", round_to: str = "100ms") -> pd.DatetimeIndex:
        """
        Convert RAS timesteps to datetime objects.

        Args:
            timesteps (np.ndarray): Array of timesteps.
            start_time (datetime): Start time of the simulation.
            time_unit (str): Unit of the timesteps. Default is "days".
            round_to (str): Frequency string to round the times to. Default is "100ms" (100 milliseconds).

        Returns:
            pd.DatetimeIndex: DatetimeIndex of converted and rounded datetimes.
        """
        if time_unit == "days":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='D')
        elif time_unit == "hours":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='H')
        else:
            raise ValueError(f"Unsupported time unit: {time_unit}")

        return pd.DatetimeIndex(datetimes).round(round_to)
    
# rename to convert_hdf5_attrs_to_dict and make public

    @staticmethod
    def convert_hdf5_attrs_to_dict(attrs: Union[h5py.AttributeManager, Dict], prefix: Optional[str] = None) -> Dict:
        """
        Convert HDF5 attributes to a Python dictionary.

        Args:
            attrs (Union[h5py.AttributeManager, Dict]): The attributes to convert.
            prefix (Optional[str]): A prefix to add to the attribute keys.

        Returns:
            Dict: A dictionary of converted attributes.
        """
        result = {}
        for key, value in attrs.items():
            if prefix:
                key = f"{prefix}/{key}"
            if isinstance(value, (np.ndarray, list)):
                result[key] = [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                result[key] = HdfUtils.convert_ras_hdf_value(value)
        return result
    
    

    @staticmethod
    def parse_run_time_window(window: str) -> Tuple[datetime, datetime]:
        """
        Parse a run time window string into a tuple of datetime objects.

        Args:
            window (str): The run time window string to be parsed.

        Returns:
            Tuple[datetime, datetime]: A tuple containing two datetime objects representing the start and end of the run
            time window.
        """
        split = window.split(" to ")
        begin = HdfUtils._parse_ras_datetime(split[0])
        end = HdfUtils._parse_ras_datetime(split[1])
        return begin, end

    


                
                
                
                
                
                
                
                
                
                
                
                
## MOVED FROM HdfBase to HdfUtils:
# _parse_ras_datetime   
# _parse_ras_simulation_window_datetime
# _parse_duration
# _parse_ras_datetime_ms
# _convert_ras_hdf_string

# Which were renamed and made public as: 
# parse_ras_datetime
# parse_ras_window_datetime
# parse_ras_datetime_ms
# parse_ras_duration
# parse_ras_time_window


# Rename to parse_ras_datetime and make public

    @staticmethod
    def parse_ras_datetime(datetime_str: str) -> datetime:
        """
        Parse a RAS datetime string into a datetime object.

        Args:
            datetime_str (str): The datetime string in format "ddMMMYYYY HH:MM:SS"

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H:%M:%S")

# Rename to parse_ras_window_datetime and make public

    @staticmethod
    def parse_ras_window_datetime(datetime_str: str) -> datetime:
        """
        Parse a datetime string from a RAS simulation window into a datetime object.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H%M")


# Rename to parse_duration and make public


    @staticmethod
    def parse_duration(duration_str: str) -> timedelta:
        """
        Parse a duration string into a timedelta object.

        Args:
            duration_str (str): The duration string to parse.

        Returns:
            timedelta: The parsed duration as a timedelta object.
        """
        hours, minutes, seconds = map(int, duration_str.split(':'))
        return timedelta(hours=hours, minutes=minutes, seconds=seconds)
    
    
# Rename to parse_ras_datetime_ms and make public
    
    @staticmethod
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\HdfXsec.py
==================================================
"""
Class: HdfXsec

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

This source code has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

Available Functions:
- get_cross_sections(): Extract cross sections from HDF geometry file
- get_river_centerlines(): Extract river centerlines from HDF geometry file
- get_river_stationing(): Calculate river stationing along centerlines
- get_river_reaches(): Return the model 1D river reach lines
- get_river_edge_lines(): Return the model river edge lines
- get_river_bank_lines(): Extract river bank lines from HDF geometry file
- _interpolate_station(): Private helper method for station interpolation

All functions follow the get_ prefix convention for methods that return data.
Private helper methods use the underscore prefix convention.

Each function returns a GeoDataFrame containing geometries and associated attributes
specific to the requested feature type. All functions include proper error handling
and logging.
"""

from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString
from typing import List  # Import List to avoid NameError
from .Decorators import standardize_input, log_call
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .LoggingConfig import get_logger
import logging



logger = get_logger(__name__)

class HdfXsec:
    """
    Handles cross-section and river geometry data extraction from HEC-RAS HDF files.

    This class provides static methods to extract and process:
    - Cross-section geometries and attributes
    - River centerlines and reaches
    - River edge and bank lines
    - Station-elevation profiles

    All methods are designed to return GeoDataFrames with standardized geometries 
    and attributes following the HEC-RAS data structure.

    Note:
        Requires HEC-RAS geometry HDF files with standard structure and naming conventions.
        All methods use proper error handling and logging.
    """
    @staticmethod
    @log_call
    def get_cross_sections(hdf_path: str, datetime_to_str: bool = True, ras_object=None) -> gpd.GeoDataFrame:
        """
        Extracts cross-section geometries and attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : str
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to True
        ras_object : RasPrj, optional
            RAS project object for additional context, defaults to None

        Returns
        -------
        gpd.GeoDataFrame
            Cross-section data with columns:
            - geometry: LineString of cross-section path
            - station_elevation: Station-elevation profile points
            - mannings_n: Dictionary of Manning's n values and stations
            - ineffective_blocks: List of ineffective flow area blocks
            - River, Reach, RS: River system identifiers
            - Name, Description: Cross-section labels
            - Len Left/Channel/Right: Flow path lengths
            - Left/Right Bank: Bank station locations
            - Additional hydraulic parameters and attributes

        Notes
        -----
        The returned GeoDataFrame includes the coordinate system from the HDF file
        when available. All byte strings are converted to regular strings.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract required datasets
                poly_info = hdf['/Geometry/Cross Sections/Polyline Info'][:]
                poly_parts = hdf['/Geometry/Cross Sections/Polyline Parts'][:]
                poly_points = hdf['/Geometry/Cross Sections/Polyline Points'][:]
                
                station_info = hdf['/Geometry/Cross Sections/Station Elevation Info'][:]
                station_values = hdf['/Geometry/Cross Sections/Station Elevation Values'][:]
                
                # Get attributes for cross sections
                xs_attrs = hdf['/Geometry/Cross Sections/Attributes'][:]
                
                # Get Manning's n data
                mann_info = hdf["/Geometry/Cross Sections/Manning's n Info"][:]
                mann_values = hdf["/Geometry/Cross Sections/Manning's n Values"][:]
                
                # Get ineffective blocks data if they exist
                if '/Geometry/Cross Sections/Ineffective Blocks' in hdf:
                    ineff_blocks = hdf['/Geometry/Cross Sections/Ineffective Blocks'][:]
                    ineff_info = hdf['/Geometry/Cross Sections/Ineffective Info'][:]
                else:
                    ineff_blocks = None
                    ineff_info = None
                
                # Initialize lists to store data
                geometries = []
                station_elevations = []
                mannings_n = []
                ineffective_blocks = []
                
                # Process each cross section
                for i in range(len(poly_info)):
                    # Extract polyline info
                    point_start_idx = poly_info[i][0]
                    point_count = poly_info[i][1]
                    part_start_idx = poly_info[i][2]
                    part_count = poly_info[i][3]
                    
                    # Extract parts for current polyline
                    parts = poly_parts[part_start_idx:part_start_idx + part_count]
                    
                    # Collect all points for this cross section
                    xs_points = []
                    for part in parts:
                        part_point_start = point_start_idx + part[0]
                        part_point_count = part[1]
                        points = poly_points[part_point_start:part_point_start + part_point_count]
                        xs_points.extend(points)
                    
                    # Create LineString geometry
                    if len(xs_points) >= 2:
                        geometry = LineString(xs_points)
                        geometries.append(geometry)
                        
                        # Extract station-elevation data
                        start_idx = station_info[i][0]
                        count = station_info[i][1]
                        station_elev = station_values[start_idx:start_idx + count]
                        station_elevations.append(station_elev)
                        
                        # Extract Manning's n data
                        mann_start_idx = mann_info[i][0]
                        mann_count = mann_info[i][1]
                        mann_n_section = mann_values[mann_start_idx:mann_start_idx + mann_count]
                        mann_n_dict = {
                            'Station': mann_n_section[:, 0].tolist(),
                            'Mann n': mann_n_section[:, 1].tolist()
                        }
                        mannings_n.append(mann_n_dict)
                        
                        # Extract ineffective blocks data
                        if ineff_info is not None and ineff_blocks is not None:
                            ineff_start_idx = ineff_info[i][0]
                            ineff_count = ineff_info[i][1]
                            if ineff_count > 0:
                                blocks = ineff_blocks[ineff_start_idx:ineff_start_idx + ineff_count]
                                blocks_list = []
                                for block in blocks:
                                    block_dict = {
                                        'Left Sta': float(block['Left Sta']),
                                        'Right Sta': float(block['Right Sta']), 
                                        'Elevation': float(block['Elevation']),
                                        'Permanent': bool(block['Permanent'])
                                    }
                                    blocks_list.append(block_dict)
                                ineffective_blocks.append(blocks_list)
                            else:
                                ineffective_blocks.append([])
                        else:
                            ineffective_blocks.append([])
                
                # Create base dictionary with required fields
                data = {
                    'geometry': geometries,
                    'station_elevation': station_elevations,
                    'mannings_n': mannings_n,
                    'ineffective_blocks': ineffective_blocks,
                }
                
                # Define field mappings with default values
                field_mappings = {
                    'River': ('River', ''),
                    'Reach': ('Reach', ''),
                    'RS': ('RS', ''),
                    'Name': ('Name', ''),
                    'Description': ('Description', ''),
                    'Len Left': ('Len Left', 0.0),
                    'Len Channel': ('Len Channel', 0.0),
                    'Len Right': ('Len Right', 0.0),
                    'Left Bank': ('Left Bank', 0.0),
                    'Right Bank': ('Right Bank', 0.0),
                    'Friction Mode': ('Friction Mode', ''),
                    'Contr': ('Contr', 0.0),
                    'Expan': ('Expan', 0.0),
                    'Left Levee Sta': ('Left Levee Sta', None),
                    'Left Levee Elev': ('Left Levee Elev', None),
                    'Right Levee Sta': ('Right Levee Sta', None),
                    'Right Levee Elev': ('Right Levee Elev', None),
                    'HP Count': ('HP Count', 0),
                    'HP Start Elev': ('HP Start Elev', 0.0),
                    'HP Vert Incr': ('HP Vert Incr', 0.0),
                    'HP LOB Slices': ('HP LOB Slices', 0),
                    'HP Chan Slices': ('HP Chan Slices', 0),
                    'HP ROB Slices': ('HP ROB Slices', 0),
                    'Ineff Block Mode': ('Ineff Block Mode', 0),
                    'Obstr Block Mode': ('Obstr Block Mode', 0),
                    'Default Centerline': ('Default Centerline', 0),
                    'Last Edited': ('Last Edited', '')
                }
                
                # Add fields that exist in xs_attrs
                for field_name, (attr_name, default_value) in field_mappings.items():
                    if attr_name in xs_attrs.dtype.names:
                        if xs_attrs[attr_name].dtype.kind == 'S':
                            # Handle string fields
                            data[field_name] = [x[attr_name].decode('utf-8').strip() 
                                              for x in xs_attrs]
                        else:
                            # Handle numeric fields
                            data[field_name] = xs_attrs[attr_name]
                    else:
                        # Use default value if field doesn't exist
                        data[field_name] = [default_value] * len(geometries)
                        logger.debug(f"Field {attr_name} not found in attributes, using default value")
                
                if geometries:
                    gdf = gpd.GeoDataFrame(data)
                    
                    # Set CRS if available
                    if 'Projection' in hdf['/Geometry'].attrs:
                        proj = hdf['/Geometry'].attrs['Projection']
                        if isinstance(proj, bytes):
                            proj = proj.decode('utf-8')
                        gdf.set_crs(proj, allow_override=True)
                    
                    return gdf
                
                return gpd.GeoDataFrame()
                
        except Exception as e:
            logger.error(f"Error processing cross-section data: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_centerlines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts river centerline geometries and attributes from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to False

        Returns
        -------
        GeoDataFrame
            River centerline data with columns:
            - geometry: LineString of river centerline
            - River Name, Reach Name: River system identifiers
            - US/DS Type, Name: Upstream/downstream connection info
            - length: Centerline length in project units
            Additional attributes from the HDF file are included

        Notes
        -----
        Returns an empty GeoDataFrame if no centerlines are found.
        All string attributes are stripped of whitespace.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    logger.warning("No river centerlines found in geometry file")
                    return GeoDataFrame()

                centerline_data = hdf_file["Geometry/River Centerlines"]
                
                # Get attributes directly from HDF dataset
                attrs = centerline_data["Attributes"][()]
                
                # Create initial dictionary for DataFrame
                centerline_dict = {}
                
                # Process each attribute field
                for name in attrs.dtype.names:
                    values = attrs[name]
                    if values.dtype.kind == 'S':
                        # Convert byte strings to regular strings
                        centerline_dict[name] = [val.decode('utf-8').strip() for val in values]
                    else:
                        centerline_dict[name] = values.tolist()  # Convert numpy array to list

                # Get polylines using utility function
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Centerlines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                centerline_gdf = GeoDataFrame(
                    centerline_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Clean up string columns
                str_columns = ['River Name', 'Reach Name', 'US Type', 
                            'US Name', 'DS Type', 'DS Name']
                for col in str_columns:
                    if col in centerline_gdf.columns:
                        centerline_gdf[col] = centerline_gdf[col].str.strip()

                # Add length calculation in project units
                if not centerline_gdf.empty:
                    centerline_gdf['length'] = centerline_gdf.geometry.length
                    
                    # Convert datetime columns if requested
                    if datetime_to_str:
                        datetime_cols = centerline_gdf.select_dtypes(
                            include=['datetime64']).columns
                        for col in datetime_cols:
                            centerline_gdf[col] = centerline_gdf[col].dt.strftime(
                                '%Y-%m-%d %H:%M:%S')

                logger.info(f"Extracted {len(centerline_gdf)} river centerlines")
                return centerline_gdf

        except Exception as e:
            logger.error(f"Error reading river centerlines: {str(e)}")
            return GeoDataFrame()



    @staticmethod
    @log_call
    def get_river_stationing(centerlines_gdf: GeoDataFrame) -> GeoDataFrame:
        """
        Calculates stationing along river centerlines with interpolated points.

        Parameters
        ----------
        centerlines_gdf : GeoDataFrame
            River centerline geometries from get_river_centerlines()

        Returns
        -------
        GeoDataFrame
            Original centerlines with additional columns:
            - station_start: Starting station value (0 or length)
            - station_end: Ending station value (length or 0)
            - stations: Array of station values along centerline
            - points: Array of interpolated point geometries

        Notes
        -----
        Station direction (increasing/decreasing) is determined by
        upstream/downstream junction connections. Stations are calculated
        at 100 evenly spaced points along each centerline.
        """
        if centerlines_gdf.empty:
            logger.warning("Empty centerlines GeoDataFrame provided")
            return centerlines_gdf

        try:
            # Create copy to avoid modifying original
            result_gdf = centerlines_gdf.copy()
            
            # Initialize new columns
            result_gdf['station_start'] = 0.0
            result_gdf['station_end'] = 0.0
            result_gdf['stations'] = None
            result_gdf['points'] = None
            
            # Process each centerline
            for idx, row in result_gdf.iterrows():
                # Get line geometry
                line = row.geometry
                
                # Calculate length
                total_length = line.length
                
                # Generate points along the line
                distances = np.linspace(0, total_length, num=100)  # Adjust num for desired density
                points = [line.interpolate(distance) for distance in distances]
                
                # Store results
                result_gdf.at[idx, 'station_start'] = 0.0
                result_gdf.at[idx, 'station_end'] = total_length
                result_gdf.at[idx, 'stations'] = distances
                result_gdf.at[idx, 'points'] = points
                
                # Add stationing direction based on upstream/downstream info
                if row['US Type'] == 'Junction' and row['DS Type'] != 'Junction':
                    # Reverse stationing if upstream is junction
                    result_gdf.at[idx, 'station_start'] = total_length
                    result_gdf.at[idx, 'station_end'] = 0.0
                    result_gdf.at[idx, 'stations'] = total_length - distances
            
            return result_gdf

        except Exception as e:
            logger.error(f"Error calculating river stationing: {str(e)}")
            return centerlines_gdf

    @staticmethod
    def _interpolate_station(line, distance):
        """
        Interpolates a point along a line at a given distance.

        Parameters
        ----------
        line : LineString
            Shapely LineString geometry
        distance : float
            Distance along the line to interpolate

        Returns
        -------
        tuple
            (x, y) coordinates of interpolated point
        """
        if distance <= 0:
            return line.coords[0]
        elif distance >= line.length:
            return line.coords[-1]
        return line.interpolate(distance).coords[0]



    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_reaches(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Return the model 1D river reach lines.

        This method extracts river reach data from the HEC-RAS geometry HDF file,
        including attributes and geometry information.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        datetime_to_str : bool, optional
            If True, convert datetime objects to strings. Default is False.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the river reaches with their attributes and geometries.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    return GeoDataFrame()

                river_data = hdf_file["Geometry/River Centerlines"]
                v_conv_val = np.vectorize(HdfUtils.convert_ras_string)
                river_attrs = river_data["Attributes"][()]
                river_dict = {"river_id": range(river_attrs.shape[0])}
                river_dict.update(
                    {name: v_conv_val(river_attrs[name]) for name in river_attrs.dtype.names}
                )
                
                # Get polylines for river reaches
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, "Geometry/River Centerlines"
                )

                river_gdf = GeoDataFrame(
                    river_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path),
                )
                if datetime_to_str:
                    river_gdf["Last Edited"] = river_gdf["Last Edited"].apply(
                        lambda x: pd.Timestamp.isoformat(x)
                    )
                return river_gdf
        except Exception as e:
            logger.error(f"Error reading river reaches: {str(e)}")
            return GeoDataFrame()


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_edge_lines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Return the model river edge lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        datetime_to_str : bool, optional
            If True, convert datetime objects to strings. Default is False.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing river edge lines with their attributes and geometries.
            Each row represents a river bank (left or right) with associated attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Edge Lines" not in hdf_file:
                    logger.warning("No river edge lines found in geometry file")
                    return GeoDataFrame()

                edge_data = hdf_file["Geometry/River Edge Lines"]
                
                # Get attributes if they exist
                if "Attributes" in edge_data:
                    attrs = edge_data["Attributes"][()]
                    v_conv_val = np.vectorize(HdfUtils.convert_ras_string)
                    
                    # Create dictionary of attributes
                    edge_dict = {"edge_id": range(attrs.shape[0])}
                    edge_dict.update(
                        {name: v_conv_val(attrs[name]) for name in attrs.dtype.names}
                    )
                    
                    # Add bank side indicator
                    if edge_dict["edge_id"].size % 2 == 0:  # Ensure even number of edges
                        edge_dict["bank_side"] = ["Left", "Right"] * (edge_dict["edge_id"].size // 2)
                else:
                    edge_dict = {"edge_id": [], "bank_side": []}

                # Get polyline geometries
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Edge Lines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                edge_gdf = GeoDataFrame(
                    edge_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Convert datetime objects to strings if requested
                if datetime_to_str and 'Last Edited' in edge_gdf.columns:
                    edge_gdf["Last Edited"] = edge_gdf["Last Edited"].apply(
                        lambda x: pd.Timestamp.isoformat(x) if pd.notnull(x) else None
                    )

                # Add length calculation in project units
                if not edge_gdf.empty:
                    edge_gdf['length'] = edge_gdf.geometry.length

                return edge_gdf

        except Exception as e:
            logger.error(f"Error reading river edge lines: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_bank_lines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extract river bank lines from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, by default False

        Returns
        -------
        GeoDataFrame
            GeoDataFrame containing river bank line geometries with attributes:
            - bank_id: Unique identifier for each bank line
            - bank_side: Left or Right bank indicator
            - geometry: LineString geometry of the bank
            - length: Length of the bank line in project units
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Bank Lines" not in hdf_file:
                    logger.warning("No river bank lines found in geometry file")
                    return GeoDataFrame()

                # Get polyline geometries using existing helper method
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Bank Lines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create basic attributes
                bank_dict = {
                    "bank_id": range(len(geoms)),
                    "bank_side": ["Left", "Right"] * (len(geoms) // 2)  # Assuming pairs of left/right banks
                }

                # Create GeoDataFrame
                bank_gdf = GeoDataFrame(
                    bank_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Add length calculation in project units
                if not bank_gdf.empty:
                    bank_gdf['length'] = bank_gdf.geometry.length

                return bank_gdf

        except Exception as e:
            logger.error(f"Error reading river bank lines: {str(e)}")
            return GeoDataFrame()


==================================================

File: c:\GH\ras-commander\ras_commander\LoggingConfig.py
==================================================
# logging_config.py

import logging
import logging.handlers
from pathlib import Path
import functools

# Define log levels
DEBUG = logging.DEBUG
INFO = logging.INFO
WARNING = logging.WARNING
ERROR = logging.ERROR
CRITICAL = logging.CRITICAL


_logging_setup_done = False

def setup_logging(log_file=None, log_level=logging.INFO):
    """Set up logging configuration for the ras-commander library."""
    global _logging_setup_done
    if _logging_setup_done:
        return
    
    # Define log format
    log_format = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # Configure console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_format)

    # Set up root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(console_handler)

    # Configure file handler if log_file is provided
    if log_file:
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        log_file_path = log_dir / log_file

        file_handler = logging.handlers.RotatingFileHandler(
            log_file_path, maxBytes=10*1024*1024, backupCount=5
        )
        file_handler.setFormatter(log_format)
        root_logger.addHandler(file_handler)
    
    _logging_setup_done = True

def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the specified name.
    
    Args:
        name: The name for the logger, typically __name__ or module path
        
    Returns:
        logging.Logger: Configured logger instance
    """
    logger = logging.getLogger(name)
    if not logger.handlers:  # Only add handler if none exists
        setup_logging()  # Ensure logging is configured
    return logger

def log_call(logger=None):
    """Decorator to log function calls."""
    def get_logger():
        # Check if logger is None or doesn't have a debug method
        if logger is None or not hasattr(logger, 'debug'):
            return logging.getLogger(__name__)
        return logger

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            log = get_logger()
            log.debug(f"Calling {func.__name__}")
            result = func(*args, **kwargs)
            log.debug(f"Finished {func.__name__}")
            return result
        return wrapper
    
    # Check if we're being called as @log_call or @log_call()
    if callable(logger):
        return decorator(logger)
    return decorator

# Set up logging when this module is imported
setup_logging()
==================================================

File: c:\GH\ras-commander\ras_commander\RasBreach.py
==================================================
"""
RasBreach: Dam breach parameter modification for HEC-RAS plan files.

This module provides methods for reading and writing breach parameters in plain text
plan files (.p##). For extracting breach RESULTS from HDF files, use HdfResultsBreach class.

Architectural Separation:
    - RasBreach: Breach PARAMETERS in plain text plan files (.p##)
    - HdfResultsBreach: Breach RESULTS from HDF files (.p##.hdf)
    - HdfStruc: Structure listings and metadata from HDF

The class follows ras-commander conventions with static methods and support for
plan numbers, integers, or file paths.

Classes:
    RasBreach: Static methods for breach parameter operations
    BreachLocation: Dataclass for breach location data
    BreachBlock: Dataclass for breach parameter blocks

Key Plan File Methods:
    - list_breach_structures_plan(): List breach structures in plan file
    - read_breach_block(): Parse breach parameters from plan
    - update_breach_block(): Modify breach parameters in plan

For HDF Results Extraction, see HdfResultsBreach:
    - HdfResultsBreach.get_breach_timeseries(): Extract time series
    - HdfResultsBreach.get_breach_summary(): Extract summary statistics
    - HdfResultsBreach.get_breaching_variables(): Breach geometry evolution
    - HdfResultsBreach.get_structure_variables(): Structure flow variables

Author: ras-commander development team
Date: 2025
"""

from typing import Dict, List, Union, Optional, Tuple
from pathlib import Path
import pandas as pd
from datetime import datetime
import re
from dataclasses import dataclass

from .Decorators import log_call
from .LoggingConfig import get_logger
from .RasPrj import ras

logger = get_logger(__name__)


class RasBreach:
    """
    Handles dam breach parameter reading and modification in plan files.

    This class provides methods for manipulating breach parameters in plain text
    plan files (.p##). For extracting breach RESULTS from HDF files, use HdfResultsBreach.

    Key Functionality:
    - List breach structures defined in plan files
    - Read breach parameters (method, geometry, timing, etc.)
    - Modify breach parameters (activation, progression, geometry)
    - Create backups before modification
    - Validate CRLF line endings for HEC-RAS compatibility

    All methods accept plan numbers, integers, or file paths.

    Examples:
        >>> from ras_commander import RasBreach, HdfResultsBreach
        >>>
        >>> # List breach structures in plan file
        >>> structures = RasBreach.list_breach_structures_plan("02")
        >>>
        >>> # Read breach parameters
        >>> params = RasBreach.read_breach_block("02", "Dam")
        >>> print(f"Active: {params['is_active']}")
        >>>
        >>> # Modify parameters
        >>> RasBreach.update_breach_block("02", "Dam", method=1)
        >>>
        >>> # For HDF results extraction, use HdfResultsBreach:
        >>> timeseries = HdfResultsBreach.get_breach_timeseries("02", "Dam")
        >>> summary = HdfResultsBreach.get_breach_summary("02")
    """

    # ==========================================================================
    # PLAN FILE PARAMETER METHODS
    # NOTE: For HDF results extraction, use HdfResultsBreach class:
    #   - HdfResultsBreach.get_breach_timeseries()
    #   - HdfResultsBreach.get_breach_summary()
    #   - HdfResultsBreach.get_breaching_variables()
    #   - HdfResultsBreach.get_structure_variables()
    # ==========================================================================

    # ==========================================================================
    # PLAN FILE PARAMETER METHODS
    # ==========================================================================

    @dataclass
    class BreachLocation:
        """Represents the structured data encoded in the `Breach Loc` line."""
        river: str
        reach: str
        station: str
        is_active: bool
        structure: str

        @classmethod
        def from_value(cls, value: str) -> "RasBreach.BreachLocation":
            parts = value.split(",")
            if len(parts) < 5:
                raise ValueError(f"Unexpected Breach Loc format: '{value}'")
            river = parts[0].strip()
            reach = parts[1].strip()
            station = parts[2].strip()
            flag = parts[3].strip()
            structure = ",".join(parts[4:]).strip()
            return cls(
                river=river,
                reach=reach,
                station=station,
                is_active=flag.strip().lower() in {"true", "1", "yes"},
                structure=structure,
            )

    @dataclass
    class BreachBlock:
        """Structured representation of a breach block within a plan file."""
        start_index: int
        end_index: int
        order: List[Tuple[str, str]]
        values: Dict[str, str]
        table_rows: Dict[str, List[List[float]]]
        table_row_lengths: Dict[str, List[int]]

        # Numeric table keys
        NUMERIC_TABLE_KEYS = {
            "Breach Progression",
            "Simplified Physical Breach Downcutting",
            "Simplified Physical Breach Widening",
        }
        DEFAULT_VALUES_PER_ROW = 10
        FIXED_WIDTH = 8

        @property
        def location(self) -> "RasBreach.BreachLocation":
            return RasBreach.BreachLocation.from_value(self.values["Breach Loc"])

        @property
        def structure_name(self) -> str:
            return self.location.structure.strip()

        @property
        def is_active(self) -> bool:
            return self.location.is_active

        def to_dict(self) -> Dict:
            """Convert breach block to dictionary for easy inspection."""
            return {
                'structure_name': self.structure_name,
                'is_active': self.is_active,
                'river': self.location.river,
                'reach': self.location.reach,
                'station': self.location.station,
                'values': self.values.copy(),
                'table_rows': self.table_rows.copy(),
            }

        def to_lines(self) -> List[str]:
            """Serialize breach block back to plan file format."""
            lines: List[str] = []
            for kind, key in self.order:
                if kind == "line":
                    lines.append(f"{key}={self.values[key]}")
                elif kind == "table":
                    rows = self.table_rows.get(key, [])
                    if rows:
                        lines.extend(RasBreach._format_numeric_rows(rows, width=self.FIXED_WIDTH))
                elif kind == "blank":
                    lines.append("")
                elif kind == "literal":
                    lines.append(key)
            return lines

    @staticmethod
    @log_call
    def list_breach_structures_plan(plan_input: Union[str, int, Path], *, ras_object=None) -> List[Dict]:
        """
        List all breach structures defined in plan file.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        List[Dict]
            List of dictionaries containing breach location information:
            - structure: Structure name
            - river: River name
            - reach: Reach name
            - station: River station
            - is_active: Boolean, True if breach is active

        Examples
        --------
        >>> # Using plan number
        >>> structures = RasBreach.list_breach_structures_plan("02")
        >>> for struct in structures:
        ...     print(f"{struct['structure']}: Active={struct['is_active']}")
        >>>
        >>> # Using plan file path
        >>> plan_path = Path("MyProject.p02")
        >>> structures = RasBreach.list_breach_structures_plan(plan_path)

        Notes
        -----
        - Returns breach structures regardless of activation status
        - Use is_active field to filter for active breaches only
        - Accepts plan number (string/int) or full plan file path
        """
        from .RasUtils import RasUtils
        
        ras_obj = ras_object or ras
        
        try:
            # Handle plan number or path input
            if isinstance(plan_input, Path):
                plan_path = plan_input
            elif isinstance(plan_input, str):
                # Check if it's a file path
                test_path = Path(plan_input)
                if test_path.exists():
                    plan_path = test_path
                else:
                    # It's a plan number
                    ras_obj.check_initialized()
                    plan_number = RasUtils.normalize_ras_number(plan_input)
                    plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            elif isinstance(plan_input, int):
                # It's a plan number
                ras_obj.check_initialized()
                plan_number = RasUtils.normalize_ras_number(plan_input)
                plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            else:
                raise ValueError(f"Invalid plan_input type: {type(plan_input)}")
            
            if not plan_path.exists():
                raise FileNotFoundError(f"Plan file not found: {plan_path}")
            
            blocks = RasBreach._read_breach_blocks_internal(plan_path)
            locations = []
            for block in blocks:
                loc = block.location
                locations.append({
                    'structure': loc.structure,
                    'river': loc.river,
                    'reach': loc.reach,
                    'station': loc.station,
                    'is_active': loc.is_active
                })
            logger.info(f"Found {len(locations)} breach structures in {plan_path.name}")
            return locations
        except Exception as e:
            logger.error(f"Error listing breach structures: {e}")
            raise

    @staticmethod
    @log_call
    def read_breach_block(plan_input: Union[str, int, Path], structure_name: str, *, ras_object=None) -> Dict:
        """
        Read breach parameters for specified structure from plan file.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        structure_name : str
            Name of breach structure to read
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        Dict
            Dictionary containing all breach parameters:
            - structure_name: Structure name
            - is_active: Boolean, breach activation status
            - river, reach, station: Location information
            - values: Dict of all breach parameter values
            - table_rows: Dict of numeric tables (progression, downcutting, etc.)

        Examples
        --------
        >>> # Using plan number
        >>> breach_data = RasBreach.read_breach_block("02", "Laxton_Dam")
        >>> print(f"Active: {breach_data['is_active']}")
        >>>
        >>> # Using plan file path
        >>> plan_path = Path("MyProject.p02")
        >>> breach_data = RasBreach.read_breach_block(plan_path, "Laxton_Dam")

        Raises
        ------
        ValueError
            If specified structure not found in plan file

        Notes
        -----
        - Accepts plan number (string/int) or full plan file path
        - Uses RasUtils.normalize_ras_number() for plan number handling
        - All values returned as strings; parse as needed
        """
        from .RasUtils import RasUtils

        ras_obj = ras_object or ras

        try:
            # Handle plan number or path input
            if isinstance(plan_input, Path):
                plan_path = plan_input
            elif isinstance(plan_input, str):
                # Check if it's a file path
                test_path = Path(plan_input)
                if test_path.exists():
                    plan_path = test_path
                else:
                    # It's a plan number
                    ras_obj.check_initialized()
                    plan_number = RasUtils.normalize_ras_number(plan_input)
                    plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            elif isinstance(plan_input, int):
                # It's a plan number
                ras_obj.check_initialized()
                plan_number = RasUtils.normalize_ras_number(plan_input)
                plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            else:
                raise ValueError(f"Invalid plan_input type: {type(plan_input)}")

            if not plan_path.exists():
                raise FileNotFoundError(f"Plan file not found: {plan_path}")

            blocks = RasBreach._read_breach_blocks_internal(plan_path)
            block = RasBreach._find_block_by_structure(blocks, structure_name)

            if block is None:
                raise ValueError(f"Structure '{structure_name}' not found in {plan_path.name}")

            logger.info(f"Read breach block for {structure_name} from {plan_path.name}")
            return block.to_dict()

        except Exception as e:
            logger.error(f"Error reading breach block: {e}")
            raise

    @staticmethod
    @log_call
    def update_breach_block(
        plan_input: Union[str, int, Path],
        structure_name: str,
        *,
        is_active: bool = None,
        method: int = None,
        geom_values: List = None,
        start_values: List = None,
        progression_mode: int = None,
        progression_pairs: List[Tuple[float, float]] = None,
        downcutting_pairs: List[Tuple[float, float]] = None,
        widening_pairs: List[Tuple[float, float]] = None,
        calculator_data: List = None,
        create_backup: bool = True,
        ras_object=None
    ) -> Dict:
        """
        Update breach parameters for specified structure in plan file.

        **CRITICAL**: Creates backup before modification. Uses CRLF line endings for HEC-RAS compatibility.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        structure_name : str
            Name of breach structure to update
        is_active : bool, optional
            Set breach activation status (True/False)
        method : int, optional
            Breach calculation method (0-7)
        geom_values : List, optional
            Breach geometry values: [center_station, final_width, final_elev,
            left_slope, right_slope, weir_coef, formation_time]
        start_values : List, optional
            Breach starting conditions
        progression_mode : int, optional
            Progression mode (0=Linear, 1=Non-linear)
        progression_pairs : List[Tuple[float, float]], optional
            Time/breach fraction pairs for non-linear progression
        downcutting_pairs : List[Tuple[float, float]], optional
            Time/elevation pairs for physical breach downcutting
        widening_pairs : List[Tuple[float, float]], optional
            Time/width pairs for physical breach widening
        calculator_data : List, optional
            Breach calculator heuristic inputs
        create_backup : bool, default True
            Create backup file before modification
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        Dict
            Updated breach block as dictionary

        Examples
        --------
        >>> # Activate breach
        >>> RasBreach.update_breach_block("02", "Laxton_Dam", is_active=True)

        >>> # Set breach geometry
        >>> geom = [150, 100, 1400, 1, 1, 2.6, 0.5]  # center, width, elev, slopes, coef, time
        >>> RasBreach.update_breach_block("02", "Laxton_Dam", geom_values=geom)

        >>> # Set non-linear progression
        >>> progression = [(0, 0), (0.5, 0.3), (1.0, 1.0)]  # time, fraction pairs
        >>> RasBreach.update_breach_block("02", "Laxton_Dam",
        ...                               progression_mode=1,
        ...                               progression_pairs=progression)

        Raises
        ------
        ValueError
            If structure not found in plan file
        RuntimeError
            If CRLF line endings not preserved (HEC-RAS incompatibility)

        Warnings
        --------
        - Modifies plan file in-place
        - Backup created in same directory with timestamp
        - HEC-RAS must be closed before modification
        - Validates CRLF line endings after write

        Notes
        -----
        Based on TNTech Dam Breach Dashboard breach_io.py implementation.
        Adapted to ras-commander conventions with plan-number support.
        """
        from .RasUtils import RasUtils

        ras_obj = ras_object or ras

        try:
            # Handle plan number or path input
            if isinstance(plan_input, Path):
                plan_path = plan_input
            elif isinstance(plan_input, str):
                # Check if it's a file path
                test_path = Path(plan_input)
                if test_path.exists():
                    plan_path = test_path
                else:
                    # It's a plan number
                    ras_obj.check_initialized()
                    plan_number = RasUtils.normalize_ras_number(plan_input)
                    plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            elif isinstance(plan_input, int):
                # It's a plan number
                ras_obj.check_initialized()
                plan_number = RasUtils.normalize_ras_number(plan_input)
                plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            else:
                raise ValueError(f"Invalid plan_input type: {type(plan_input)}")

            if not plan_path.exists():
                raise FileNotFoundError(f"Plan file not found: {plan_path}")

            # Read all breach blocks
            lines = plan_path.read_text().splitlines()
            blocks = RasBreach._parse_breach_blocks(lines)
            block = RasBreach._find_block_by_structure(blocks, structure_name)

            if block is None:
                raise ValueError(f"Structure '{structure_name}' not found in {plan_path.name}")

            # Apply updates
            if is_active is not None:
                RasBreach._set_activation(block, is_active)
            if method is not None:
                block.values["Breach Method"] = f" {int(method)}"
            if geom_values is not None:
                block.values["Breach Geom"] = RasBreach._format_csv(geom_values)
            if start_values is not None:
                block.values["Breach Start"] = RasBreach._format_csv(start_values)
            if progression_mode is not None or progression_pairs is not None:
                mode = progression_mode if progression_mode is not None else int(block.values["Breach Progression"].strip())
                RasBreach._set_progression(block, mode, progression_pairs)
            if downcutting_pairs is not None:
                RasBreach._set_table_pairs(block, "Simplified Physical Breach Downcutting", downcutting_pairs)
            if widening_pairs is not None:
                RasBreach._set_table_pairs(block, "Simplified Physical Breach Widening", widening_pairs)
            if calculator_data is not None:
                block.values["Breach Calculator Data"] = RasBreach._format_csv(calculator_data)

            # Replace block lines in file
            new_block_lines = block.to_lines()
            lines[block.start_index:block.end_index] = new_block_lines
            block.end_index = block.start_index + len(new_block_lines)

            # Create backup
            if create_backup:
                RasBreach._create_backup(plan_path)

            # Write with CRLF line endings (CRITICAL for HEC-RAS)
            if lines and not lines[-1].endswith("\n"):
                output = "\r\n".join(lines) + "\r\n"
            else:
                output = "\r\n".join(lines)

            # Use open() with newline='' to preserve CRLF
            with open(plan_path, 'w', encoding='utf-8', newline='') as f:
                f.write(output)

            # Validate CRLF preservation
            if not RasBreach._validate_crlf(plan_path):
                raise RuntimeError(
                    f"CRITICAL: Failed to preserve CRLF line endings in {plan_path}. "
                    "HEC-RAS will not be able to open this project."
                )

            logger.info(f"Updated breach block for {structure_name} in {plan_path.name}")
            return block.to_dict()

        except Exception as e:
            logger.error(f"Error updating breach block: {e}")
            raise

    @staticmethod
    @log_call
    def set_breach_geom(
        plan_input: Union[str, int, Path],
        structure_name: str,
        *,
        centerline: Optional[float] = None,
        initial_width: Optional[float] = None,
        final_bottom_elev: Optional[float] = None,
        left_slope: Optional[float] = None,
        right_slope: Optional[float] = None,
        active: Optional[bool] = None,
        weir_coef: Optional[float] = None,
        top_elev: Optional[float] = None,
        formation_method: Optional[int] = None,
        formation_time: Optional[float] = None,
        ras_object=None
    ) -> Dict:
        """
        Update individual breach geometry parameters.

        Convenience function for modifying specific breach geometry fields without
        reconstructing the entire Breach Geom CSV. Reads current values and updates
        only the specified parameters.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        structure_name : str
            Name of breach structure to update
        centerline : float, optional
            Centerline/station location (ft or m)
        initial_width : float, optional
            Initial breach bottom width (ft or m)
        final_bottom_elev : float, optional
            Final breach bottom elevation (ft or m) - **Common modification**
        left_slope : float, optional
            Left side slope (H:V ratio, e.g., 0.5 = 0.5H:1V)
        right_slope : float, optional
            Right side slope (H:V ratio)
        active : bool, optional
            Breach activation flag (True/False)
        weir_coef : float, optional
            Weir discharge coefficient (dimensionless)
        top_elev : float, optional
            Top elevation (ft or m)
        formation_method : int, optional
            Formation method (1=Time-based, 2=Trigger-based)
        formation_time : float, optional
            Formation time (hrs) or trigger threshold (ft)
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        Dict
            Updated breach block dictionary

        Examples
        --------
        >>> # Update just Final Bottom Elevation
        >>> RasBreach.set_breach_geom("19", "Dam", final_bottom_elev=605)
        >>>
        >>> # Update multiple parameters
        >>> RasBreach.set_breach_geom("19", "Dam",
        ...                           initial_width=250,
        ...                           final_bottom_elev=600,
        ...                           formation_time=3.0)
        >>>
        >>> # Change breach to time-based formation
        >>> RasBreach.set_breach_geom("template_plan", "Dam",
        ...                           formation_method=1,
        ...                           formation_time=2.5)

        Notes
        -----
        - Only specified parameters are modified; others retain current values
        - Automatically reads current Breach Geom and updates selectively
        - Creates backup before modification
        - Validates CRLF line endings

        Breach Geom Field Reference:
            [0] centerline - Breach centerline/station location
            [1] initial_width - Starting breach width
            [2] final_bottom_elev - Final breach bottom elevation
            [3] left_slope - Left side slope (H:V)
            [4] right_slope - Right side slope (H:V)
            [5] active - Activation flag
            [6] weir_coef - Weir discharge coefficient
            [7] top_elev - Top elevation
            [8] formation_method - 1=Time, 2=Trigger
            [9] formation_time - Time (hrs) or threshold (ft)
        """
        try:
            # Read current breach block
            current_block = RasBreach.read_breach_block(plan_input, structure_name, ras_object=ras_object)

            # Parse current Breach Geom values
            geom_str = current_block['values'].get('Breach Geom', '')
            if not geom_str:
                raise ValueError(f"No Breach Geom found for structure '{structure_name}'")

            current_geom = [x.strip() for x in geom_str.split(',')]

            if len(current_geom) < 10:
                raise ValueError(f"Breach Geom has {len(current_geom)} fields, expected 10")

            # Update specified parameters (preserve current values for None parameters)
            new_geom = current_geom.copy()

            if centerline is not None:
                new_geom[0] = centerline
            if initial_width is not None:
                new_geom[1] = initial_width
            if final_bottom_elev is not None:
                new_geom[2] = final_bottom_elev
            if left_slope is not None:
                new_geom[3] = left_slope
            if right_slope is not None:
                new_geom[4] = right_slope
            if active is not None:
                new_geom[5] = active
            if weir_coef is not None:
                new_geom[6] = weir_coef
            if top_elev is not None:
                new_geom[7] = top_elev
            if formation_method is not None:
                new_geom[8] = formation_method
            if formation_time is not None:
                new_geom[9] = formation_time

            # Log what changed
            changes = []
            field_names = ['centerline', 'initial_width', 'final_bottom_elev', 'left_slope', 'right_slope',
                          'active', 'weir_coef', 'top_elev', 'formation_method', 'formation_time']
            for idx, (old, new, name) in enumerate(zip(current_geom, new_geom, field_names)):
                if str(old) != str(new):
                    changes.append(f"{name}: {old} → {new}")

            if changes:
                logger.info(f"Modifying breach geometry for {structure_name}: {', '.join(changes)}")
            else:
                logger.warning(f"No changes specified for {structure_name}")

            # Update using existing update_breach_block method
            return RasBreach.update_breach_block(
                plan_input,
                structure_name,
                geom_values=new_geom,
                ras_object=ras_object
            )

        except Exception as e:
            logger.error(f"Error setting breach geometry: {e}")
            raise

    # ==========================================================================
    # INTERNAL HELPER METHODS
    # ==========================================================================

    @staticmethod
    def _read_breach_blocks_internal(plan_path: Path) -> List["RasBreach.BreachBlock"]:
        """Internal method to read and parse breach blocks from plan file."""
        lines = plan_path.read_text().splitlines()
        return RasBreach._parse_breach_blocks(lines)

    @staticmethod
    def _parse_breach_blocks(lines: List[str]) -> List["RasBreach.BreachBlock"]:
        """Parse all breach blocks from plan file lines."""
        blocks: List[RasBreach.BreachBlock] = []
        idx = 0
        while idx < len(lines):
            line = lines[idx]
            if line.startswith("Breach Loc="):
                start_idx = idx
                block_lines = [line]
                idx += 1
                while idx < len(lines):
                    candidate = lines[idx]
                    if candidate.startswith("Breach Loc=") and block_lines:
                        break
                    block_lines.append(candidate)
                    idx += 1
                end_idx = start_idx + len(block_lines)
                block = RasBreach._parse_block(block_lines, start_idx, end_idx)
                blocks.append(block)
            else:
                idx += 1
        return blocks

    @staticmethod
    def _parse_block(block_lines: List[str], start_index: int, end_index: int) -> "RasBreach.BreachBlock":
        """Parse single breach block from lines."""
        values: Dict[str, str] = {}
        table_rows: Dict[str, List[List[float]]] = {}
        order: List[Tuple[str, str]] = []
        current_table_key: Optional[str] = None

        for line in block_lines:
            if "=" in line:
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.rstrip()
                values[key] = value
                order.append(("line", key))
                if key in RasBreach.BreachBlock.NUMERIC_TABLE_KEYS:
                    order.append(("table", key))
                    current_table_key = key
                    table_rows.setdefault(key, [])
                else:
                    current_table_key = None
            else:
                if current_table_key:
                    stripped = line.strip()
                    if stripped:
                        numeric_row = [float(part) for part in stripped.split()]
                        table_rows.setdefault(current_table_key, []).append(numeric_row)
                else:
                    if line.strip() == "":
                        order.append(("blank", ""))
                    else:
                        order.append(("literal", line))

        table_row_lengths = {key: [len(row) for row in rows] for key, rows in table_rows.items()}
        return RasBreach.BreachBlock(
            start_index=start_index,
            end_index=end_index,
            order=order,
            values=values,
            table_rows=table_rows,
            table_row_lengths=table_row_lengths,
        )

    @staticmethod
    def _find_block_by_structure(blocks: List["RasBreach.BreachBlock"], structure_name: str) -> Optional["RasBreach.BreachBlock"]:
        """Find breach block by structure name (case-insensitive)."""
        target = structure_name.strip().lower()
        for block in blocks:
            if block.structure_name.lower() == target:
                return block
        return None

    @staticmethod
    def _set_activation(block: "RasBreach.BreachBlock", is_active: bool) -> None:
        """Set breach activation status."""
        loc = block.location
        loc.is_active = bool(is_active)
        river = (loc.river or "").rjust(16)
        reach = (loc.reach or "").rjust(16)
        station = (loc.station or "").rjust(8)
        flag = "True" if loc.is_active else "False"
        structure = (loc.structure or "").ljust(16)
        block.values["Breach Loc"] = f"{river},{reach},{station},{flag},{structure}"

    @staticmethod
    def _set_progression(block: "RasBreach.BreachBlock", mode: int, pairs: Optional[List[Tuple[float, float]]]) -> None:
        """Set breach progression mode and pairs."""
        block.values["Breach Progression"] = f" {int(mode)}"
        if pairs is not None:
            flat_values: List[float] = []
            for pair in pairs:
                if len(pair) != 2:
                    raise ValueError("Progression pairs must contain exactly two values")
                flat_values.extend([float(pair[0]), float(pair[1])])
            RasBreach._set_table_values(block, "Breach Progression", flat_values)

    @staticmethod
    def _set_table_pairs(block: "RasBreach.BreachBlock", key: str, pairs: List[Tuple[float, float]]) -> None:
        """Set table values from time/value pairs."""
        flat_values: List[float] = []
        for pair in pairs:
            if len(pair) != 2:
                raise ValueError(f"{key} pairs must contain exactly two values")
            flat_values.extend([float(pair[0]), float(pair[1])])
        RasBreach._set_table_values(block, key, flat_values)

    @staticmethod
    def _set_table_values(block: "RasBreach.BreachBlock", key: str, values: List[float]) -> None:
        """Set numeric table values for breach block."""
        lengths = block.table_row_lengths.get(key)
        if lengths and sum(lengths) == len(values):
            rows: List[List[float]] = []
            index = 0
            for length in lengths:
                rows.append(list(values[index:index + length]))
                index += length
        else:
            rows = []
            chunk = RasBreach.BreachBlock.DEFAULT_VALUES_PER_ROW
            for index in range(0, len(values), chunk):
                rows.append(list(values[index:index + chunk]))

        block.table_rows[key] = rows
        block.table_row_lengths[key] = [len(row) for row in rows]

    @staticmethod
    def _format_numeric_rows(rows: List[List[float]], width: int) -> List[str]:
        """Format numeric table rows for plan file."""
        formatted: List[str] = []
        for row in rows:
            formatted.append("".join(RasBreach._format_numeric_value(value, width=width) for value in row))
        return formatted

    @staticmethod
    def _format_numeric_value(value: float, width: int) -> str:
        """Format single numeric value with fixed width."""
        numeric = float(value)
        if numeric == 0:
            text = "0"
        elif abs(numeric) >= 10000 or (0 < abs(numeric) < 1e-4):
            text = f"{numeric:.3e}"
        else:
            text = f"{numeric:.6g}"
        if len(text) > width:
            text = f"{numeric:.6e}"
        if len(text) > width:
            text = text[:width]
        return text.rjust(width)

    @staticmethod
    def _format_csv(values: List) -> str:
        """Format values as comma-separated string."""
        formatted: List[str] = []
        for item in values:
            if item is None:
                formatted.append("")
            elif isinstance(item, bool):
                formatted.append("True" if item else "False")
            elif isinstance(item, (int, float)):
                formatted.append(str(item))
            else:
                formatted.append(str(item))
        return ",".join(formatted)

    @staticmethod
    def _create_backup(plan_path: Path) -> None:
        """Create timestamped backup of plan file."""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = plan_path.parent / f"{plan_path.stem}_backup_{timestamp}{plan_path.suffix}"
        backup_path.write_text(plan_path.read_text())
        logger.info(f"Created backup: {backup_path.name}")

    @staticmethod
    def _validate_crlf(plan_path: Path) -> bool:
        """Validate that file has CRLF line endings."""
        content = plan_path.read_bytes()
        # Check if file contains \r\n (CRLF)
        return b'\r\n' in content

==================================================

File: c:\GH\ras-commander\ras_commander\RasCmdr.py
==================================================
"""
RasCmdr - Execution operations for running HEC-RAS simulations

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function():
        
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasCmdr:
- compute_plan()
- compute_parallel()
- compute_test_mode()
        
        
        
"""
import os
import subprocess
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from .RasPrj import ras, RasPrj, init_ras_project, get_ras_exe
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasUtils import RasUtils
import logging
import time
import queue
from threading import Thread, Lock
from typing import Union, List, Optional, Dict
from pathlib import Path
import shutil
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Thread
from itertools import cycle
from ras_commander.RasPrj import RasPrj  # Ensure RasPrj is imported
from threading import Lock, Thread, current_thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import cycle
from typing import Union, List, Optional, Dict
from numbers import Number
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

# Module code starts here

# TODO: Future Enhancements
# 1. Alternate Run Mode for compute_plan and compute_parallel:
#    - Use Powershell to execute HEC-RAS command
#    - Hide RAS window and all child windows
#    - Note: This mode may prevent execution if the plan has a popup
#    - Intended for background runs or popup-free scenarios
#    - Limit to non-commercial use
#
# 2. Implement compute_plan_remote:
#    - Execute compute_plan on a remote machine via psexec
#    - Use keyring package for secure credential storage
#    - Implement psexec command for remote HEC-RAS execution
#    - Create remote_worker objects to store machine details:
#      (machine name, username, password, ras_exe_path, local folder path, etc.)
#    - Develop RasRemote class for remote_worker management and abstractions
#    - Implement compute_plan_remote in RasCmdr as a thin wrapper around RasRemote
#      (similar to existing compute_plan functions but for remote execution)


class RasCmdr:
    
    @staticmethod
    @log_call
    def compute_plan(
        plan_number: Union[str, Number, Path],
        dest_folder=None,
        ras_object=None,
        clear_geompre=False,
        num_cores=None,
        overwrite_dest=False
    ):
        """
        Execute a single HEC-RAS plan in a specified location.

        This function runs a HEC-RAS plan by launching the HEC-RAS executable through command line,
        allowing for destination folder specification, core count control, and geometry preprocessor management.

        Args:
            plan_number (Union[str, Number, Path]): The plan number to execute (e.g., "01", 1, 1.0) or the full path to the plan file.
                Recommended to use two-digit strings for plan numbers for consistency (e.g., "01" instead of 1).
            dest_folder (str, Path, optional): Name of the folder or full path for computation.
                If a string is provided, it will be created in the same parent directory as the project folder.
                If a full path is provided, it will be used as is.
                If None, computation occurs in the original project folder, modifying the original project.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
                Useful when working with multiple projects simultaneously.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.
                Set to True when geometry has been modified to force recomputation of preprocessor files.
            num_cores (int, optional): Number of cores to use for the plan execution. 
                If None, the current setting in the plan file is not changed.
                Generally, 2-4 cores provides good performance for most models.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. Defaults to False.
                Set to True to replace an existing destination folder with the same name.

        Returns:
            bool: True if the execution was successful, False otherwise.

        Raises:
            ValueError: If the specified dest_folder already exists and is not empty, and overwrite_dest is False.
            FileNotFoundError: If the plan file or project file cannot be found.
            PermissionError: If there are issues accessing or writing to the destination folder.
            subprocess.CalledProcessError: If the HEC-RAS execution fails.

        Examples:
            # Run a plan in the original project folder
            RasCmdr.compute_plan("01")
            
            # Run a plan in a separate folder
            RasCmdr.compute_plan("01", dest_folder="computation_folder")
            
            # Run a plan with a specific number of cores
            RasCmdr.compute_plan("01", num_cores=4)
            
            # Run a plan in a specific folder, overwriting if it exists
            RasCmdr.compute_plan("01", dest_folder="computation_folder", overwrite_dest=True)
            
            # Run a plan in a specific folder with multiple options
            RasCmdr.compute_plan(
                "01", 
                dest_folder="computation_folder",
                num_cores=2,
                clear_geompre=True,
                overwrite_dest=True
            )
            
        Notes:
            - For executing multiple plans, consider using compute_parallel() or compute_test_mode().
            - Setting num_cores appropriately is important for performance:
              * 1-2 cores: Highest efficiency per core, good for small models
              * 3-8 cores: Good balance for most models
              * >8 cores: May have diminishing returns due to overhead
            - This function updates the RAS object's dataframes (plan_df, geom_df, etc.) after execution.
        """
        try:
            ras_obj = ras_object if ras_object is not None else ras
            logger.info(f"Using ras_object with project folder: {ras_obj.project_folder}")
            ras_obj.check_initialized()
            
            if dest_folder is not None:
                dest_folder = Path(ras_obj.project_folder).parent / dest_folder if isinstance(dest_folder, str) else Path(dest_folder)
                
                if dest_folder.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder)
                        logger.info(f"Destination folder '{dest_folder}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder.iterdir()):
                        error_msg = f"Destination folder '{dest_folder}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                
                dest_folder.mkdir(parents=True, exist_ok=True)
                shutil.copytree(ras_obj.project_folder, dest_folder, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder}")
                
                compute_ras = RasPrj()
                compute_ras.initialize(dest_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
            else:
                compute_ras = ras_obj
                compute_prj_path = ras_obj.prj_file

            # Determine the plan path
            compute_plan_path = Path(plan_number) if isinstance(plan_number, (str, Path)) and Path(plan_number).is_file() else RasPlan.get_plan_path(plan_number, compute_ras)

            if not compute_prj_path or not compute_plan_path:
                logger.error(f"Could not find project file or plan file for plan {plan_number}")
                return False

            # Clear geometry preprocessor files if requested
            if clear_geompre:
                try:
                    RasGeo.clear_geompre_files(compute_plan_path, ras_object=compute_ras)
                    logger.info(f"Cleared geometry preprocessor files for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error clearing geometry preprocessor files for plan {plan_number}: {str(e)}")

            # Set the number of cores if specified
            if num_cores is not None:
                try:
                    RasPlan.set_num_cores(compute_plan_path, num_cores=num_cores, ras_object=compute_ras)
                    logger.info(f"Set number of cores to {num_cores} for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error setting number of cores for plan {plan_number}: {str(e)}")

            # Prepare the command for HEC-RAS execution
            cmd = f'"{compute_ras.ras_exe_path}" -c "{compute_prj_path}" "{compute_plan_path}"'
            logger.info("Running HEC-RAS from the Command Line:")
            logger.info(f"Running command: {cmd}")

            # Execute the HEC-RAS command
            start_time = time.time()
            try:
                subprocess.run(cmd, check=True, shell=True, capture_output=True, text=True)
                end_time = time.time()
                run_time = end_time - start_time
                logger.info(f"HEC-RAS execution completed for plan: {plan_number}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return True
            except subprocess.CalledProcessError as e:
                end_time = time.time()
                run_time = end_time - start_time
                logger.error(f"Error running plan: {plan_number}")
                logger.error(f"Error message: {e.output}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return False
        except Exception as e:
            logger.critical(f"Error in compute_plan: {str(e)}")
            return False
        finally:
            # Update the RAS object's dataframes
            if ras_obj:
                ras_obj.plan_df = ras_obj.get_plan_entries()
                ras_obj.geom_df = ras_obj.get_geom_entries()
                ras_obj.flow_df = ras_obj.get_flow_entries()
                ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
    


    @staticmethod
    @log_call
    def compute_parallel(
        plan_number: Union[str, Number, List[Union[str, Number]], None] = None,
        max_workers: int = 2,
        num_cores: int = 2,
        clear_geompre: bool = False,
        ras_object: Optional['RasPrj'] = None,
        dest_folder: Union[str, Path, None] = None,
        overwrite_dest: bool = False
    ) -> Dict[str, bool]:
        """
        Execute multiple HEC-RAS plans in parallel using multiple worker instances.

        This method creates separate worker folders for each parallel process, runs plans
        in those folders, and then consolidates results to a final destination folder.
        It's ideal for running independent plans simultaneously to make better use of system resources.

        Args:
            plan_number (Union[str, List[str], None]): Plan number(s) to compute. 
                If None, all plans in the project are computed.
                If string, only that plan will be computed.
                If list, all specified plans will be computed.
                Recommended to use two-digit strings for plan numbers for consistency (e.g., "01" instead of 1).
            max_workers (int): Maximum number of parallel workers (separate HEC-RAS instances).
                Each worker gets a separate folder with a copy of the project.
                Optimal value depends on CPU cores and memory available.
                A good starting point is: max_workers = floor(physical_cores / num_cores).
            num_cores (int): Number of cores to use per plan computation.
                Controls computational resources allocated to each individual HEC-RAS instance.
                For parallel execution, 2-4 cores per worker often provides the best balance.
            clear_geompre (bool): Whether to clear geometry preprocessor files before computation.
                Set to True when geometry has been modified to force recomputation.
            ras_object (Optional[RasPrj]): RAS project object. If None, uses global 'ras' instance.
                Useful when working with multiple projects simultaneously.
            dest_folder (Union[str, Path, None]): Destination folder for computed results.
                If None, creates a "[Computed]" folder adjacent to the project folder.
                If string, creates folder in the project's parent directory.
                If Path, uses the exact path provided.
            overwrite_dest (bool): Whether to overwrite existing destination folder.
                Set to True to replace an existing destination folder with the same name.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.
                Keys are plan numbers and values are boolean success indicators.

        Raises:
            ValueError: If the destination folder already exists, is not empty, and overwrite_dest is False.
            FileNotFoundError: If project files cannot be found.
            PermissionError: If there are issues accessing or writing to folders.
            RuntimeError: If worker initialization fails.

        Examples:
            # Run all plans in parallel with default settings
            RasCmdr.compute_parallel()
            
            # Run all plans with 4 workers, 2 cores per worker
            RasCmdr.compute_parallel(max_workers=4, num_cores=2)
            
            # Run specific plans in parallel
            RasCmdr.compute_parallel(plan_number=["01", "03"], max_workers=2)
            
            # Run all plans with dynamic worker allocation based on system resources
            import psutil
            physical_cores = psutil.cpu_count(logical=False)
            cores_per_worker = 2
            max_workers = max(1, physical_cores // cores_per_worker)
            RasCmdr.compute_parallel(max_workers=max_workers, num_cores=cores_per_worker)
            
            # Run all plans in a specific destination folder
            RasCmdr.compute_parallel(dest_folder="parallel_results", overwrite_dest=True)

        Notes:
            - Worker Assignment: Plans are assigned to workers in a round-robin fashion.
              For example, with 3 workers and 5 plans, assignment would be:
              Worker 1: Plans 1 & 4, Worker 2: Plans 2 & 5, Worker 3: Plan 3.
            
            - Resource Management: Each HEC-RAS instance (worker) typically requires:
              * 2-4 GB of RAM
              * 2-4 cores for optimal performance
            
            - When to use parallel vs. sequential:
              * Parallel: For independent plans, faster overall completion
              * Sequential: For dependent plans, consistent resource usage, easier debugging
            
            - The function creates worker folders during execution and consolidates results
              to the destination folder upon completion.
              
            - This function updates the RAS object's dataframes (plan_df, geom_df, etc.) after execution.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()

            project_folder = Path(ras_obj.project_folder)

            if dest_folder is not None:
                dest_folder_path = Path(dest_folder)
                if dest_folder_path.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder_path)
                        logger.info(f"Destination folder '{dest_folder_path}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder_path.iterdir()):
                        error_msg = f"Destination folder '{dest_folder_path}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                dest_folder_path.mkdir(parents=True, exist_ok=True)
                shutil.copytree(project_folder, dest_folder_path, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder_path}")
                project_folder = dest_folder_path

            # Store filtered plan numbers separately to ensure only these are executed
            filtered_plan_numbers = []

            if plan_number:
                if isinstance(plan_number, (str, Number)):
                    plan_number = [plan_number]
                ras_obj.plan_df = ras_obj.plan_df[ras_obj.plan_df['plan_number'].isin(plan_number)]
                filtered_plan_numbers = list(ras_obj.plan_df['plan_number'])
                logger.info(f"Filtered plans to execute: {filtered_plan_numbers}")
            else:
                filtered_plan_numbers = list(ras_obj.plan_df['plan_number'])

            num_plans = len(ras_obj.plan_df)
            max_workers = min(max_workers, num_plans) if num_plans > 0 else 1
            logger.info(f"Adjusted max_workers to {max_workers} based on the number of plans: {num_plans}")

            worker_ras_objects = {}
            for worker_id in range(1, max_workers + 1):
                worker_folder = project_folder.parent / f"{project_folder.name} [Worker {worker_id}]"
                if worker_folder.exists():
                    shutil.rmtree(worker_folder)
                    logger.info(f"Removed existing worker folder: {worker_folder}")
                shutil.copytree(project_folder, worker_folder)
                logger.info(f"Created worker folder: {worker_folder}")

                try:
                    worker_ras = RasPrj()
                    worker_ras_object = init_ras_project(
                        ras_project_folder=worker_folder,
                        ras_version=ras_obj.ras_exe_path,
                        ras_object=worker_ras
                    )
                    worker_ras_objects[worker_id] = worker_ras_object
                except Exception as e:
                    logger.critical(f"Failed to initialize RAS project for worker {worker_id}: {str(e)}")
                    worker_ras_objects[worker_id] = None

            # Explicitly use the filtered plan numbers for assignments
            worker_cycle = cycle(range(1, max_workers + 1))
            plan_assignments = [(next(worker_cycle), plan_num) for plan_num in filtered_plan_numbers]

            execution_results: Dict[str, bool] = {}

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(
                        RasCmdr.compute_plan,
                        plan_num, 
                        ras_object=worker_ras_objects[worker_id], 
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    for worker_id, plan_num in plan_assignments
                ]

                for future, (worker_id, plan_num) in zip(as_completed(futures), plan_assignments):
                    try:
                        success = future.result()
                        execution_results[plan_num] = success
                        logger.info(f"Plan {plan_num} executed in worker {worker_id}: {'Successful' if success else 'Failed'}")
                    except Exception as e:
                        execution_results[plan_num] = False
                        logger.error(f"Plan {plan_num} failed in worker {worker_id}: {str(e)}")

            final_dest_folder = dest_folder_path if dest_folder is not None else project_folder.parent / f"{project_folder.name} [Computed]"
            final_dest_folder.mkdir(parents=True, exist_ok=True)
            logger.info(f"Final destination for computed results: {final_dest_folder}")

            for worker_ras in worker_ras_objects.values():
                if worker_ras is None:
                    continue
                worker_folder = Path(worker_ras.project_folder)
                try:
                    # First, close any open resources in the worker RAS object
                    worker_ras.close() if hasattr(worker_ras, 'close') else None
                    
                    # Add a small delay to ensure file handles are released
                    time.sleep(1)
                    
                    # Move files with retry mechanism
                    max_retries = 3
                    for retry in range(max_retries):
                        try:
                            for item in worker_folder.iterdir():
                                dest_path = final_dest_folder / item.name
                                if dest_path.exists():
                                    if dest_path.is_dir():
                                        shutil.rmtree(dest_path)
                                    else:
                                        dest_path.unlink()
                                # Use copy instead of move for more reliability
                                if item.is_dir():
                                    shutil.copytree(item, dest_path)
                                else:
                                    shutil.copy2(item, dest_path)
                            
                            # Add another small delay before removal
                            time.sleep(1)
                            
                            # Try to remove the worker folder
                            if worker_folder.exists():
                                shutil.rmtree(worker_folder)
                            break  # If successful, break the retry loop
                            
                        except PermissionError as pe:
                            if retry == max_retries - 1:  # If this was the last retry
                                logger.error(f"Failed to move/remove files after {max_retries} attempts: {str(pe)}")
                                raise
                            time.sleep(2 ** retry)  # Exponential backoff
                            continue
                            
                except Exception as e:
                    logger.error(f"Error moving results from {worker_folder} to {final_dest_folder}: {str(e)}")

            try:
                final_dest_folder_ras = RasPrj()
                final_dest_folder_ras_obj = init_ras_project(
                    ras_project_folder=final_dest_folder, 
                    ras_version=ras_obj.ras_exe_path,
                    ras_object=final_dest_folder_ras
                )
                final_dest_folder_ras_obj.check_initialized()
            except Exception as e:
                logger.critical(f"Failed to initialize RasPrj for final destination: {str(e)}")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj = ras_object or ras
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_parallel: {str(e)}")
            return {}

    @staticmethod
    @log_call
    def compute_test_mode(
        plan_number: Union[str, Number, List[Union[str, Number]], None] = None,
        dest_folder_suffix="[Test]",
        clear_geompre=False,
        num_cores=None,
        ras_object=None,
        overwrite_dest=False
    ):
        """
        Execute HEC-RAS plans sequentially in a separate test folder.

        This function creates a separate test folder, copies the project there, and executes
        the specified plans in sequential order. It's useful for batch processing plans that 
        need to be run in a specific order or when you want to ensure consistent resource usage.

        Args:
            plan_number (Union[str, Number, List[Union[str, Number]], None], optional): Plan number or list of plan numbers to execute (e.g., "01", 1, 1.0, or ["01", 2]). 
                If None, all plans will be executed. Default is None.
                Recommended to use two-digit strings for plan numbers for consistency (e.g., "01" instead of 1).
            dest_folder_suffix (str, optional): Suffix to append to the test folder name. 
                Defaults to "[Test]".
                The test folder is always created in the project folder's parent directory.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files.
                Defaults to False.
                Set to True when geometry has been modified to force recomputation.
            num_cores (int, optional): Number of cores to use for each plan.
                If None, the current setting in the plan file is not changed. Default is None.
                For sequential execution, 4-8 cores often provides good performance.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
                Useful when working with multiple projects simultaneously.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. 
                Defaults to False.
                Set to True to replace an existing test folder with the same name.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.
                Keys are plan numbers and values are boolean success indicators.

        Raises:
            ValueError: If the destination folder already exists, is not empty, and overwrite_dest is False.
            FileNotFoundError: If project files cannot be found.
            PermissionError: If there are issues accessing or writing to folders.

        Examples:
            # Run all plans sequentially
            RasCmdr.compute_test_mode()
            
            # Run a specific plan
            RasCmdr.compute_test_mode(plan_number="01")
            
            # Run multiple specific plans
            RasCmdr.compute_test_mode(plan_number=["01", "03", "05"])
            
            # Run plans with a custom folder suffix
            RasCmdr.compute_test_mode(dest_folder_suffix="[SequentialRun]")
            
            # Run plans with a specific number of cores
            RasCmdr.compute_test_mode(num_cores=4)
            
            # Run specific plans with multiple options
            RasCmdr.compute_test_mode(
                plan_number=["01", "02"],
                dest_folder_suffix="[SpecificSequential]",
                clear_geompre=True,
                num_cores=6,
                overwrite_dest=True
            )

        Notes:
            - This function was created to replicate the original HEC-RAS command line -test flag,
              which does not work in recent versions of HEC-RAS.
            
            - Key differences from other compute functions:
              * compute_plan: Runs a single plan, with option for destination folder
              * compute_parallel: Runs multiple plans simultaneously in worker folders
              * compute_test_mode: Runs multiple plans sequentially in a single test folder
            
            - Use cases:
              * Running plans in a specific order
              * Ensuring consistent resource usage
              * Easier debugging (one plan at a time)
              * Isolated test environment
            
            - Performance considerations:
              * Sequential execution is generally slower overall than parallel execution
              * Each plan gets consistent resource usage
              * Execution time scales linearly with the number of plans
            
            - This function updates the RAS object's dataframes (plan_df, geom_df, etc.) after execution.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()
            
            logger.info("Starting the compute_test_mode...")
               
            project_folder = Path(ras_obj.project_folder)

            if not project_folder.exists():
                logger.error(f"Project folder '{project_folder}' does not exist.")
                return {}

            compute_folder = project_folder.parent / f"{project_folder.name} {dest_folder_suffix}"
            logger.info(f"Creating the test folder: {compute_folder}...")

            if compute_folder.exists():
                if overwrite_dest:
                    shutil.rmtree(compute_folder)
                    logger.info(f"Compute folder '{compute_folder}' exists. Overwriting as per overwrite_dest=True.")
                elif any(compute_folder.iterdir()):
                    error_msg = (
                        f"Compute folder '{compute_folder}' exists and is not empty. "
                        "Use overwrite_dest=True to overwrite."
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg)

            try:
                shutil.copytree(project_folder, compute_folder)
                logger.info(f"Copied project folder to compute folder: {compute_folder}")
            except Exception as e:
                logger.critical(f"Error occurred while copying project folder: {str(e)}")
                return {}

            try:
                compute_ras = RasPrj()
                compute_ras.initialize(compute_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
                logger.info(f"Initialized RAS project in compute folder: {compute_prj_path}")
            except Exception as e:
                logger.critical(f"Error initializing RAS project in compute folder: {str(e)}")
                return {}

            if not compute_prj_path:
                logger.error("Project file not found.")
                return {}

            logger.info("Getting plan entries...")
            try:
                ras_compute_plan_entries = compute_ras.plan_df
                logger.info("Retrieved plan entries successfully.")
            except Exception as e:
                logger.critical(f"Error retrieving plan entries: {str(e)}")
                return {}

            if plan_number:
                if isinstance(plan_number, (str, Number)):
                    plan_number = [plan_number]
                ras_compute_plan_entries = ras_compute_plan_entries[
                    ras_compute_plan_entries['plan_number'].isin(plan_number)
                ]
                logger.info(f"Filtered plans to execute: {plan_number}")

            execution_results = {}
            logger.info("Running selected plans sequentially...")
            for _, plan in ras_compute_plan_entries.iterrows():
                plan_number = plan["plan_number"]
                start_time = time.time()
                try:
                    success = RasCmdr.compute_plan(
                        plan_number,
                        ras_object=compute_ras,
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    execution_results[plan_number] = success
                    if success:
                        logger.info(f"Successfully computed plan {plan_number}")
                    else:
                        logger.error(f"Failed to compute plan {plan_number}")
                except Exception as e:
                    execution_results[plan_number] = False
                    logger.error(f"Error computing plan {plan_number}: {str(e)}")
                finally:
                    end_time = time.time()
                    run_time = end_time - start_time
                    logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")

            logger.info("All selected plans have been executed.")
            logger.info("compute_test_mode completed.")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_test_mode: {str(e)}")
            return {}
==================================================

File: c:\GH\ras-commander\ras_commander\RasControl.py
==================================================
"""
RasControl - HECRASController API Wrapper (ras-commander style)

Provides ras-commander style API for legacy HEC-RAS versions (3.x-4.x)
that use HECRASController COM interface instead of HDF files.

Includes robust process management with session tracking, orphan detection,
and optional watchdog protection for Jupyter kernel restarts.

Public functions (HEC-RAS Operations):
- RasControl.run_plan(plan, ras_object=None, force_recompute=False, use_watchdog=True, max_runtime=3600) -> Tuple[bool, List[str]]
- RasControl.get_steady_results(plan, ras_object=None) -> pandas.DataFrame
- RasControl.get_unsteady_results(plan, max_times=None, ras_object=None) -> pandas.DataFrame
- RasControl.get_output_times(plan, ras_object=None) -> List[str]
- RasControl.get_plans(plan, ras_object=None) -> List[dict]
- RasControl.set_current_plan(plan, ras_object=None) -> bool
- RasControl.get_comp_msgs(plan, ras_object=None) -> str

Public functions (Process Management):
- RasControl.list_processes(show_all=False) -> pandas.DataFrame
- RasControl.scan_orphans() -> List[SessionLock]
- RasControl.cleanup_orphans(interactive=True, dry_run=False) -> int
- RasControl.force_cleanup_all() -> int

Private functions:
- _terminate_ras_process() -> None
- _is_ras_running() -> bool
- RasControl._normalize_version(version: str) -> str
- RasControl._get_project_info(plan, ras_object=None) -> Tuple[Path, str, Optional[str], Optional[str]]
- RasControl._com_open_close(project_path: Path, version: str, operation_func: Callable[[Any], Any]) -> Any

Session tracking infrastructure:
- SessionLock dataclass - Tracks active COM sessions with lock files
- Module-level _active_sessions dict - Tracks all active sessions
- atexit handler - Emergency cleanup on Python exit
- Watchdog support - Optional independent process for kernel restart protection

"""

import win32com.client
import psutil
import pandas as pd
from pathlib import Path
from typing import Optional, List, Tuple, Callable, Any, Union, Dict
import logging
import time
import json
import socket
import tempfile
import uuid
import atexit
import sys
import subprocess
import os
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

# Import ras-commander components
from .RasPrj import ras


# ============================================================================
# SESSION TRACKING INFRASTRUCTURE
# ============================================================================

@dataclass
class SessionLock:
    """
    Represents a tracked RasControl session for process cleanup.

    Stored as JSON in temp directory to track active COM sessions and enable
    orphan detection after crashes/kernel restarts.
    """
    python_pid: int              # Python process PID
    ras_pid: Optional[int]       # ras.exe PID (None if couldn't detect)
    project_path: str            # Absolute path to .prj file
    ras_version: str             # HEC-RAS version (e.g., "6.5")
    session_id: str              # Unique session UUID
    start_time: float            # time.time() when session started
    python_exe: str              # sys.executable
    hostname: str                # socket.gethostname()
    detection_confidence: int    # 0-100 score from PID detection

    def to_json(self) -> str:
        """Serialize to JSON string."""
        return json.dumps(asdict(self), indent=2)

    @classmethod
    def from_json(cls, data: str) -> 'SessionLock':
        """Deserialize from JSON string."""
        return cls(**json.loads(data))

    @classmethod
    def from_file(cls, path: Path) -> 'SessionLock':
        """Load from lock file."""
        return cls.from_json(path.read_text(encoding='utf-8'))


# Module-level session tracking
_active_sessions: Dict[str, SessionLock] = {}  # {session_id: SessionLock}

# Lock file directory
LOCK_DIR = Path(tempfile.gettempdir()) / "rascontrol_sessions"
LOCK_DIR.mkdir(exist_ok=True)


def _get_lock_file_path(session_id: str) -> Path:
    """Generate lock file path for a session."""
    filename = f"rasctl_{os.getpid()}_{session_id}.lock"
    return LOCK_DIR / filename


def _find_our_ras_process(project_path: Path, before_snapshot: Dict[int, Any]) -> Tuple[Optional[int], int]:
    """
    Multi-strategy detection to find the ras.exe process we just launched.

    Args:
        project_path: Path to .prj file being opened
        before_snapshot: Dict of {pid: proc_info} before COM launch

    Returns:
        Tuple of (pid, confidence_score). PID is None if detection failed.
        Confidence score is 0-100.
    """
    time.sleep(0.3)  # Give process time to appear

    candidates = {}  # {pid: confidence_score}

    try:
        after = {
            p.pid: p.info
            for p in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time'])
            if p.info['name'] and p.info['name'].lower() == 'ras.exe'
        }
    except Exception as e:
        logger.warning(f"Error scanning for ras.exe processes: {e}")
        return None, 0

    new_pids = set(after.keys()) - set(before_snapshot.keys())

    for pid in after.keys():
        proc_info = after[pid]
        score = 0

        # Criteria 1: Newly appeared (50 points)
        if pid in new_pids:
            score += 50

        # Criteria 2: Project path in cmdline (40 points)
        try:
            cmdline = ' '.join(proc_info['cmdline'] or [])
            if str(project_path) in cmdline or project_path.name in cmdline:
                score += 40
        except (TypeError, AttributeError):
            pass

        # Criteria 3: Very recent creation time (30 points)
        try:
            age = time.time() - proc_info['create_time']
            if age < 2.0:  # Created within 2 seconds
                score += 30
        except (TypeError, KeyError):
            pass

        # Criteria 4: Only one new process (20 points)
        if len(new_pids) == 1 and pid in new_pids:
            score += 20

        if score > 0:
            candidates[pid] = score

    if not candidates:
        logger.warning(f"Could not reliably identify ras.exe PID for {project_path.name}")
        return None, 0

    # Return highest confidence PID
    best_pid = max(candidates, key=candidates.get)
    confidence = candidates[best_pid]

    if confidence < 50:
        logger.warning(f"Low confidence ({confidence}/100) for PID {best_pid}")
    else:
        logger.info(f"Detected ras.exe PID {best_pid} (confidence: {confidence}/100)")

    return best_pid, confidence


def _classify_lock_file(lock: SessionLock) -> str:
    """
    Classify lock file state.

    Returns:
        'active' - Python still running, session active
        'stale_orphan' - Python dead, ras.exe still running
        'stale_clean' - Both dead, safe to delete
        'foreign_machine' - From different machine, don't touch
    """
    # Check 1: Different machine?
    if lock.hostname != socket.gethostname():
        return 'foreign_machine'

    # Check 2: Is Python process still running?
    python_alive = False
    try:
        python_proc = psutil.Process(lock.python_pid)
        if python_proc.is_running():
            # Verify it's actually Python (not PID reuse)
            if 'python' in python_proc.name().lower():
                python_alive = True
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        pass

    if python_alive:
        return 'active'

    # Check 3: Is ras.exe still running?
    if lock.ras_pid is not None:
        try:
            ras_proc = psutil.Process(lock.ras_pid)
            if ras_proc.is_running() and ras_proc.name().lower() == 'ras.exe':
                # Verify it's working on our project (if cmdline available)
                try:
                    cmdline = ' '.join(ras_proc.cmdline() or [])
                    if lock.project_path in cmdline or Path(lock.project_path).name in cmdline:
                        return 'stale_orphan'  # Orphaned process!
                except (psutil.AccessDenied, psutil.NoSuchProcess):
                    pass
                # Couldn't verify project, but ras.exe exists - assume orphan if Python dead
                return 'stale_orphan'
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass

    return 'stale_clean'


def _create_session_lock(session_id: str, lock_data: SessionLock) -> Path:
    """Create a lock file for the session."""
    lock_path = _get_lock_file_path(session_id)
    try:
        lock_path.write_text(lock_data.to_json(), encoding='utf-8')
        logger.debug(f"Created session lock: {lock_path.name}")
        return lock_path
    except Exception as e:
        logger.warning(f"Failed to create session lock file: {e}")
        return lock_path


def _remove_session_lock(session_id: str) -> None:
    """Remove a session lock file."""
    lock_path = _get_lock_file_path(session_id)
    try:
        lock_path.unlink(missing_ok=True)
        logger.debug(f"Removed session lock: {lock_path.name}")
    except Exception as e:
        logger.warning(f"Failed to remove session lock file: {e}")


def _cleanup_session(session_id: str) -> None:
    """Clean up a specific session."""
    if session_id in _active_sessions:
        lock = _active_sessions[session_id]

        # Try to terminate the ras.exe process gracefully
        if lock.ras_pid:
            try:
                proc = psutil.Process(lock.ras_pid)
                if proc.is_running() and proc.name().lower() == 'ras.exe':
                    logger.info(f"Terminating tracked ras.exe PID {lock.ras_pid}")
                    proc.terminate()
                    proc.wait(timeout=5)
            except (psutil.NoSuchProcess, psutil.TimeoutExpired, psutil.AccessDenied) as e:
                logger.debug(f"Could not terminate PID {lock.ras_pid}: {e}")

        # Remove from tracking
        del _active_sessions[session_id]

        # Remove lock file
        _remove_session_lock(session_id)


def _emergency_cleanup_all() -> None:
    """
    Emergency cleanup of all tracked sessions.
    Called by atexit handler.
    """
    if not _active_sessions:
        return

    logger.info(f"Emergency cleanup: {len(_active_sessions)} active session(s)")

    for session_id in list(_active_sessions.keys()):
        _cleanup_session(session_id)


def _spawn_watchdog(parent_pid: int, ras_pid: int, max_runtime: int,
                    lock_file_path: Path) -> int:
    """
    Spawn independent watchdog process for long-running operations.

    The watchdog monitors for:
    1. Parent Python process death (orphan detection)
    2. Runtime timeout
    3. Manual cancellation via lock file deletion

    Returns:
        Watchdog process PID
    """
    watchdog_script = f"""
import psutil
import time
import sys
from pathlib import Path

PARENT_PID = {parent_pid}
RAS_PID = {ras_pid}
MAX_RUNTIME = {max_runtime}
LOCK_FILE = Path({str(lock_file_path)!r})
CHECK_INTERVAL = 5  # seconds

start_time = time.time()

while True:
    time.sleep(CHECK_INTERVAL)

    # Check 1: Parent Python still alive?
    try:
        parent = psutil.Process(PARENT_PID)
        if not parent.is_running():
            # Parent died, orphan detected
            print(f"[Watchdog] Parent {{PARENT_PID}} died, terminating ras.exe {{RAS_PID}}", flush=True)
            try:
                ras = psutil.Process(RAS_PID)
                ras.terminate()
                ras.wait(timeout=10)
            except:
                pass
            LOCK_FILE.unlink(missing_ok=True)
            sys.exit(0)
    except psutil.NoSuchProcess:
        # Parent already gone
        try:
            ras = psutil.Process(RAS_PID)
            ras.terminate()
            ras.wait(timeout=10)
        except:
            pass
        LOCK_FILE.unlink(missing_ok=True)
        sys.exit(0)

    # Check 2: Timeout exceeded?
    if time.time() - start_time > MAX_RUNTIME:
        print(f"[Watchdog] Timeout exceeded, terminating ras.exe {{RAS_PID}}", flush=True)
        try:
            ras = psutil.Process(RAS_PID)
            ras.terminate()
            ras.wait(timeout=10)
        except:
            pass
        LOCK_FILE.unlink(missing_ok=True)
        sys.exit(0)

    # Check 3: Lock file deleted? (manual cancel signal)
    if not LOCK_FILE.exists():
        print(f"[Watchdog] Lock file deleted, assuming manual cleanup", flush=True)
        sys.exit(0)
"""

    try:
        # Launch watchdog as completely independent process
        creationflags = subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
        proc = subprocess.Popen(
            [sys.executable, '-c', watchdog_script],
            creationflags=creationflags,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )

        logger.info(f"Spawned watchdog process PID {proc.pid} (monitoring PID {ras_pid})")
        return proc.pid
    except Exception as e:
        logger.error(f"Failed to spawn watchdog process: {e}")
        return 0


def _terminate_watchdog(watchdog_pid: int) -> None:
    """Terminate a watchdog process."""
    if watchdog_pid == 0:
        return

    try:
        proc = psutil.Process(watchdog_pid)
        proc.terminate()
        proc.wait(timeout=3)
        logger.debug(f"Terminated watchdog process PID {watchdog_pid}")
    except (psutil.NoSuchProcess, psutil.TimeoutExpired, psutil.AccessDenied):
        pass


# Register atexit cleanup handler
atexit.register(_emergency_cleanup_all)


# ============================================================================
# LEGACY PROCESS TERMINATION FUNCTIONS (kept for compatibility)
# ============================================================================

def _terminate_ras_process() -> None:
    """Force terminate any running ras.exe processes."""
    for proc in psutil.process_iter(['name']):
        try:
            if proc.info['name'] and proc.info['name'].lower() == 'ras.exe':
                proc.terminate()
                proc.wait(timeout=3)
                logger.info("Terminated ras.exe process")
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
            pass


def _is_ras_running() -> bool:
    """Check if HEC-RAS is currently running"""
    for proc in psutil.process_iter(['name']):
        try:
            if proc.info['name'] and proc.info['name'].lower() == 'ras.exe':
                return True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    return False


class RasControl:
    """
    HECRASController API wrapper with ras-commander style interface.

    Works with legacy HEC-RAS versions (3.x-4.x) that use COM interface
    instead of HDF files. Integrates with ras-commander project management.

    Usage (ras-commander style):
        >>> from ras_commander import init_ras_project, RasControl
        >>>
        >>> # Initialize with version (with or without periods)
        >>> init_ras_project(path, "4.1")  # or "41"
        >>>
        >>> # Use plan numbers like HDF methods
        >>> RasControl.run_plan("02")
        >>> df = RasControl.get_steady_results("02")

    Supported Versions:
        All installed versions: 3.x, 4.x, 5.0.x, 6.0-6.7+
        Accepts formats: "4.1", "41", "5.0.6", "506", "6.6", "66", etc.
    """

    # Version mapping based on ACTUAL COM interfaces registered on system
    # Only these COM interfaces exist: RAS41, RAS503, RAS505, RAS506, RAS507,
    # RAS60, RAS631, RAS641, RAS65, RAS66, RAS67
    # Other versions use nearest available fallback
    VERSION_MAP = {
        # HEC-RAS 3.x → Use 4.1 (3.x COM not registered)
        '3.0': 'RAS41.HECRASController',
        '30': 'RAS41.HECRASController',
        '3.1': 'RAS41.HECRASController',
        '31': 'RAS41.HECRASController',
        '3.1.1': 'RAS41.HECRASController',
        '311': 'RAS41.HECRASController',
        '3.1.2': 'RAS41.HECRASController',
        '312': 'RAS41.HECRASController',
        '3.1.3': 'RAS41.HECRASController',
        '313': 'RAS41.HECRASController',

        # HEC-RAS 4.x
        '4.0': 'RAS41.HECRASController',    # Use 4.1 (4.0 COM not registered)
        '40': 'RAS41.HECRASController',
        '4.1': 'RAS41.HECRASController',    # ✓ EXISTS
        '41': 'RAS41.HECRASController',
        '4.1.0': 'RAS41.HECRASController',
        '410': 'RAS41.HECRASController',

        # HEC-RAS 5.0.x
        '5.0': 'RAS503.HECRASController',   # Use 5.0.3 (RAS50 COM not registered)
        '50': 'RAS503.HECRASController',
        '5.0.1': 'RAS501.HECRASController', # ✓ EXISTS
        '501': 'RAS501.HECRASController',
        '5.0.3': 'RAS503.HECRASController', # ✓ EXISTS
        '503': 'RAS503.HECRASController',
        '5.0.4': 'RAS504.HECRASController', # ✓ EXISTS (newly installed)
        '504': 'RAS504.HECRASController',
        '5.0.5': 'RAS505.HECRASController', # ✓ EXISTS
        '505': 'RAS505.HECRASController',
        '5.0.6': 'RAS506.HECRASController', # ✓ EXISTS
        '506': 'RAS506.HECRASController',
        '5.0.7': 'RAS507.HECRASController', # ✓ EXISTS
        '507': 'RAS507.HECRASController',

        # HEC-RAS 6.x
        '6.0': 'RAS60.HECRASController',    # ✓ EXISTS
        '60': 'RAS60.HECRASController',
        '6.1': 'RAS60.HECRASController',    # Use 6.0 (6.1 COM not registered)
        '61': 'RAS60.HECRASController',
        '6.2': 'RAS60.HECRASController',    # Use 6.0 (6.2 COM not registered)
        '62': 'RAS60.HECRASController',
        '6.3': 'RAS631.HECRASController',   # Use 6.3.1 (6.3 COM not registered)
        '63': 'RAS631.HECRASController',
        '6.3.1': 'RAS631.HECRASController', # ✓ EXISTS
        '631': 'RAS631.HECRASController',
        '6.4': 'RAS641.HECRASController',   # Use 6.4.1 (6.4 COM not registered)
        '64': 'RAS641.HECRASController',
        '6.4.1': 'RAS641.HECRASController', # ✓ EXISTS
        '641': 'RAS641.HECRASController',
        '6.5': 'RAS65.HECRASController',    # ✓ EXISTS
        '65': 'RAS65.HECRASController',
        '6.6': 'RAS66.HECRASController',    # ✓ EXISTS
        '66': 'RAS66.HECRASController',
        '6.7': 'RAS67.HECRASController',    # ✓ EXISTS
        '67': 'RAS67.HECRASController',
        '6.7 Beta 4': 'RAS67.HECRASController',
    }

    # Legacy reference (kept for backwards compatibility)
    SUPPORTED_VERSIONS = VERSION_MAP

    # Output variable codes
    WSEL = 2
    ENERGY = 3
    MAX_CHL_DPTH = 4
    MIN_CH_EL = 5
    ENERGY_SLOPE = 6
    FLOW_TOTAL = 24
    VEL_TOTAL = 25
    STA_WS_LFT = 36
    STA_WS_RGT = 37
    FROUDE_CHL = 48
    FROUDE_XS = 49
    Q_WEIR = 94
    Q_CULVERT_TOT = 242

    # ========== PRIVATE METHODS (HECRASController COM API) ==========

    @staticmethod
    def _normalize_version(version: str) -> str:
        """
        Normalize version string to match VERSION_MAP keys.

        Handles formats like:
            "6.6", "66" → "6.6"
            "4.1", "41" → "4.1"
            "5.0.6", "506" → "5.0.6"
            "6.7 Beta 4" → "6.7 Beta 4"

        Returns:
            Normalized version string that exists in VERSION_MAP

        Raises:
            ValueError: If version cannot be normalized or is not supported
        """
        version_str = str(version).strip()

        # Direct match
        if version_str in RasControl.VERSION_MAP:
            return version_str

        # Try common normalizations
        normalized_candidates = [
            version_str,
            version_str.replace('.', ''),  # "6.6" → "66"
        ]

        # Try adding periods for compact formats
        if len(version_str) == 2:  # "66" → "6.6"
            normalized_candidates.append(f"{version_str[0]}.{version_str[1]}")
        elif len(version_str) == 3 and version_str.startswith('5'):  # "506" → "5.0.6"
            normalized_candidates.append(f"5.0.{version_str[2]}")
        elif len(version_str) == 3:  # "631" → "6.3.1"
            normalized_candidates.append(f"{version_str[0]}.{version_str[1]}.{version_str[2]}")

        # Check all candidates
        for candidate in normalized_candidates:
            if candidate in RasControl.VERSION_MAP:
                logger.debug(f"Normalized version '{version}' → '{candidate}'")
                return candidate

        # Not found
        raise ValueError(
            f"Version '{version}' not supported. Supported versions:\n"
            f"  3.x: 3.0, 3.1 (3.1.1, 3.1.2, 3.1.3)\n"
            f"  4.x: 4.0, 4.1\n"
            f"  5.0.x: 5.0, 5.0.1, 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7\n"
            f"  6.x: 6.0, 6.1, 6.2, 6.3, 6.3.1, 6.4, 6.4.1, 6.5, 6.6, 6.7\n"
            f"  Formats: Can use '6.6' or '66', '5.0.6' or '506', etc."
        )

    @staticmethod
    def _get_project_info(plan: Union[str, Path], ras_object=None):
        """
        Resolve plan number/path to project path, version, and plan details.

        Returns:
            Tuple[Path, str, str, str]: (project_path, version, plan_number, plan_name)
            plan_number and plan_name are None if using direct .prj path
        """
        if ras_object is None:
            ras_object = ras

        # If it's a path to .prj file
        plan_path = Path(plan) if isinstance(plan, str) else plan
        if plan_path.exists() and plan_path.suffix == '.prj':
            # Direct path - need version from ras_object
            if not hasattr(ras_object, 'ras_version') or not ras_object.ras_version:
                raise ValueError(
                    "When using direct .prj paths, project must be initialized with version.\n"
                    "Use: init_ras_project(path, '4.1') or similar"
                )
            return plan_path, ras_object.ras_version, None, None

        # Otherwise treat as plan number
        plan_num = str(plan).zfill(2)

        # Get project path from ras_object
        if not hasattr(ras_object, 'prj_file') or not ras_object.prj_file:
            raise ValueError(
                "No project initialized. Use init_ras_project() first.\n"
                "Example: init_ras_project(path, '4.1')"
            )

        project_path = Path(ras_object.prj_file)

        # Get version
        if not hasattr(ras_object, 'ras_version') or not ras_object.ras_version:
            raise ValueError(
                "Project initialized without version. Re-initialize with:\n"
                "init_ras_project(path, '4.1')  # or '41', '501', etc."
            )

        version = ras_object.ras_version

        # Get plan name from plan_df
        plan_row = ras_object.plan_df[ras_object.plan_df['plan_number'] == plan_num]
        if plan_row.empty:
            raise ValueError(f"Plan '{plan_num}' not found in project")

        plan_name = plan_row['Plan Title'].iloc[0]

        return project_path, version, plan_num, plan_name

    @staticmethod
    def _com_open_close(project_path: Path, version: str, operation_func: Callable[[Any], Any]) -> Any:
        """
        PRIVATE: Open HEC-RAS via COM, run operation, close HEC-RAS.

        This is the core COM interface handler. All public methods use this.
        Includes session tracking for robust cleanup on crashes/kernel restarts.
        """
        # Normalize version (handles "6.6" → "6.6", "66" → "6.6", etc.)
        normalized_version = RasControl._normalize_version(version)

        if not project_path.exists():
            raise FileNotFoundError(f"Project file not found: {project_path}")

        com_rc = None
        result = None
        session_id = str(uuid.uuid4())
        lock_path = None

        # Take snapshot of ras.exe processes before COM launch
        before_snapshot = {}
        try:
            before_snapshot = {
                p.pid: p.info
                for p in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time'])
                if p.info['name'] and p.info['name'].lower() == 'ras.exe'
            }
        except Exception as e:
            logger.debug(f"Could not snapshot processes: {e}")

        try:
            # Open HEC-RAS COM interface
            com_string = RasControl.VERSION_MAP[normalized_version]
            logger.info(f"Opening HEC-RAS: {com_string} (version: {version})")
            com_rc = win32com.client.Dispatch(com_string)

            # Open project
            logger.info(f"Opening project: {project_path}")
            com_rc.Project_Open(str(project_path))

            # Detect ras.exe PID after COM launch
            ras_pid, confidence = _find_our_ras_process(project_path, before_snapshot)

            # Create session lock
            lock_data = SessionLock(
                python_pid=os.getpid(),
                ras_pid=ras_pid,
                project_path=str(project_path),
                ras_version=version,
                session_id=session_id,
                start_time=time.time(),
                python_exe=sys.executable,
                hostname=socket.gethostname(),
                detection_confidence=confidence
            )

            # Track session globally
            _active_sessions[session_id] = lock_data

            # Create lock file
            lock_path = _create_session_lock(session_id, lock_data)

            # Perform operation
            logger.info("Executing operation...")
            result = operation_func(com_rc)
            logger.info("Operation completed successfully")

            return result

        except Exception as e:
            logger.error(f"Operation failed: {e}")
            raise

        finally:
            # ALWAYS close
            logger.info("Closing HEC-RAS...")

            if com_rc is not None:
                try:
                    com_rc.QuitRas()
                    logger.info("HEC-RAS closed via QuitRas()")
                except Exception as e:
                    logger.warning(f"QuitRas() failed: {e}")

            # Clean up session tracking (terminates only our tracked PID)
            _cleanup_session(session_id)

            # Check if our specific process is still running
            if session_id in _active_sessions:
                logger.warning("Session cleanup may have failed - session still tracked")
            else:
                logger.debug("Session cleanup completed successfully")

    # ========== PUBLIC API (ras-commander style) ==========

    @staticmethod
    def run_plan(plan: Union[str, Path], ras_object=None, force_recompute: bool = False,
                 use_watchdog: bool = True, max_runtime: int = 86400) -> Tuple[bool, List[str]]:
        """
        Run a plan (steady or unsteady) and wait for completion.

        This method checks if results are current before running. If results
        are up-to-date, it skips computation (unless force_recompute=True).
        When computation is needed, it starts the computation and polls
        Compute_Complete() until the run finishes. It will block until completion.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance (uses global ras if None)
            force_recompute: If False (default), checks if results are current
                before running. If results are up-to-date, skips computation.
                If True, always runs the plan regardless of current status.
                Defaults to False.
            use_watchdog: If True, spawns independent watchdog process that will
                terminate ras.exe if Python crashes/kernel restarts. Provides
                protection against orphaned processes in Jupyter notebooks.
                Defaults to True (recommended). Set to False to disable.
            max_runtime: Maximum runtime in seconds before watchdog terminates the
                process. Only used if use_watchdog=True. Defaults to 3600 (1 hour).

        Returns:
            Tuple of (success: bool, messages: List[str])

        Example:
            >>> from ras_commander import init_ras_project, RasControl
            >>> init_ras_project(path, "4.1")
            >>> # Default: with watchdog protection (recommended)
            >>> success, msgs = RasControl.run_plan("02")
            >>> # Force recomputation even if results are current
            >>> success, msgs = RasControl.run_plan("02", force_recompute=True)
            >>> # Disable watchdog (not recommended in Jupyter)
            >>> success, msgs = RasControl.run_plan("01", use_watchdog=False)
            >>> # Long-running with extended timeout
            >>> success, msgs = RasControl.run_plan("01", max_runtime=7200)

        Note:
            Can take several minutes for large models or unsteady runs.
            Progress is logged every 30 seconds.
            If PlanOutput_IsCurrent() check fails (e.g., older HEC-RAS versions),
            the plan will be run as a safe fallback.

            Watchdog protection (use_watchdog=True):
            - Spawns independent Python process monitoring parent death
            - Survives kernel restarts and crashes
            - Automatically terminates orphaned ras.exe processes
            - Enforces max_runtime timeout
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _run_operation(com_rc):
            watchdog_pid = 0

            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            # Check if results are current (unless force_recompute=True)
            if not force_recompute:
                try:
                    is_current = com_rc.PlanOutput_IsCurrent()
                    if is_current:
                        logger.info(f"Plan {plan_num} results are current. Skipping computation.")
                        logger.info("Use force_recompute=True to recompute anyway.")
                        return True, ["Results are current - computation skipped"]
                except Exception as e:
                    logger.warning(f"Could not check PlanOutput_IsCurrent(): {e}")
                    logger.warning("Proceeding with computation...")

            # Version-specific behavior (normalize for checking)
            norm_version = RasControl._normalize_version(version)

            # Start computation (returns immediately - ASYNCHRONOUS!)
            logger.info("Starting computation...")
            if norm_version.startswith('4') or norm_version.startswith('3'):
                status, _, messages = com_rc.Compute_CurrentPlan(None, None)
            else:
                status, _, messages, _ = com_rc.Compute_CurrentPlan(None, None)

            # Spawn watchdog if requested
            if use_watchdog:
                # Find our session to get ras_pid and lock file
                current_session = None
                for session in _active_sessions.values():
                    if session.project_path == str(project_path):
                        current_session = session
                        break

                if current_session and current_session.ras_pid:
                    lock_file = _get_lock_file_path(current_session.session_id)
                    watchdog_pid = _spawn_watchdog(
                        parent_pid=os.getpid(),
                        ras_pid=current_session.ras_pid,
                        max_runtime=max_runtime,
                        lock_file_path=lock_file
                    )
                else:
                    logger.warning("Could not spawn watchdog - ras.exe PID not detected")

            try:
                # CRITICAL: Wait for computation to complete
                # Compute_CurrentPlan is ASYNCHRONOUS - it returns before computation finishes
                logger.info("Waiting for computation to complete...")
                poll_count = 0
                while True:
                    try:
                        # Check if computation is complete
                        is_complete = com_rc.Compute_Complete()

                        if is_complete:
                            logger.info(f"Computation completed (polled {poll_count} times)")
                            break

                        # Still computing - wait and poll again
                        time.sleep(1)  # Poll every second
                        poll_count += 1

                        # Log progress every 30 seconds
                        if poll_count % 30 == 0:
                            logger.info(f"Still computing... ({poll_count} seconds elapsed.  Simulation will timeout after {max_runtime} seconds.  Set max_runtime to override.)")

                    except Exception as e:
                        logger.error(f"Error checking completion status: {e}")
                        # If we can't check status, break and hope for the best
                        break

                return status, list(messages) if messages else []

            finally:
                # Always terminate watchdog on completion (even if error)
                if watchdog_pid:
                    _terminate_watchdog(watchdog_pid)

        return RasControl._com_open_close(project_path, version, _run_operation)

    @staticmethod
    def _parse_ras_datetime(time_string: str) -> pd.Timestamp:
        """
        Parse HEC-RAS COM datetime string to pandas Timestamp.

        Args:
            time_string: RAS format (e.g., "18FEB1999 0000" or "01JAN2000 0000")

        Returns:
            pandas Timestamp, or pd.NaT if string is "Max WS" or parsing fails

        Note:
            This is a private helper method for converting RAS datetime strings
            from the COM interface into proper datetime64[ns] objects. The "Max WS"
            special value is converted to pd.NaT to allow clean filtering.

            Special handling for "2400" hours: HEC-RAS uses 2400 to represent
            midnight at the end of a day (equivalent to 0000 of the next day).
        """
        time_str = time_string.strip()

        # Special case: Max WS row contains computational maximums, not a timestamp
        if time_str == 'Max WS':
            return pd.NaT

        # Special case: 2400 hours (midnight at end of day)
        # HEC-RAS uses 2400 to mean 24:00 (midnight at end of day)
        # Convert to 0000 of next day
        if ' 2400' in time_str:
            # Replace 2400 with 0000 and parse, then add 1 day
            temp_str = time_str.replace(' 2400', ' 0000')
            try:
                dt = pd.to_datetime(temp_str, format='%d%b%Y %H%M')
                # Add 1 day to get correct midnight
                return dt + pd.Timedelta(days=1)
            except (ValueError, TypeError):
                logger.warning(f"Could not parse RAS datetime with 2400: '{time_str}'")
                return pd.NaT

        try:
            # Primary format: "01JAN2000 0000" (%d%b%Y %H%M)
            return pd.to_datetime(time_str, format='%d%b%Y %H%M')
        except (ValueError, TypeError):
            try:
                # Alternate format with seconds: "01JAN2000 0000:00"
                return pd.to_datetime(time_str, format='%d%b%Y %H%M:%S')
            except (ValueError, TypeError):
                logger.warning(f"Could not parse RAS datetime: '{time_str}'")
                return pd.NaT

    @staticmethod
    def get_steady_results(plan: Union[str, Path], ras_object=None) -> pd.DataFrame:
        """
        Extract steady state profile results from HEC-RAS via COM interface.

        Opens HEC-RAS, loads the specified plan, extracts water surface elevations
        and hydraulic parameters for all profiles at all cross sections, then closes
        HEC-RAS.

        Parameters
        ----------
        plan : str or Path
            Plan number (e.g., "01", "02") or full path to .prj file
        ras_object : RasPrj, optional
            RAS project object. If None, uses global `ras` object.

        Returns
        -------
        pd.DataFrame
            Steady state results with one row per cross-section per profile

            **Schema:**

            +----------------+----------+---------------------------------------+
            | Column         | Type     | Description                           |
            +================+==========+=======================================+
            | river          | str      | River name                            |
            +----------------+----------+---------------------------------------+
            | reach          | str      | Reach name                            |
            +----------------+----------+---------------------------------------+
            | node_id        | str      | Cross section river station           |
            +----------------+----------+---------------------------------------+
            | profile        | str      | Profile name (e.g., "PF 1", "50Pct")  |
            +----------------+----------+---------------------------------------+
            | wsel           | float    | Water surface elevation (ft or m)     |
            +----------------+----------+---------------------------------------+
            | velocity       | float    | Total velocity (ft/s or m/s)          |
            +----------------+----------+---------------------------------------+
            | flow           | float    | Total flow (cfs or cms)               |
            +----------------+----------+---------------------------------------+
            | froude         | float    | Channel Froude number (dimensionless) |
            +----------------+----------+---------------------------------------+
            | energy         | float    | Energy grade elevation (ft or m)      |
            +----------------+----------+---------------------------------------+
            | max_depth      | float    | Maximum channel depth (ft or m)       |
            +----------------+----------+---------------------------------------+
            | min_ch_el      | float    | Minimum channel elevation (ft or m)   |
            +----------------+----------+---------------------------------------+

            **Note on data types:**

            - String columns (`river`, `reach`, `node_id`, `profile`) are decoded
              from COM byte strings and stripped of whitespace
            - Numeric columns are float64
            - Units depend on project settings (US customary or SI)

        Raises
        ------
        ValueError
            - If project not initialized with version
            - If plan number not found in project
        RuntimeError
            - If no steady state results found
            - If model run was not successful

        Notes
        -----
        **Comparison with HDF Methods:**

        This COM-based method returns MORE data than the HDF-based
        `HdfResultsPlan.get_steady_wse()`, which only returns WSE.
        RasControl includes velocity, flow, Froude, energy, and depths.

        **Performance Notes:**

        - HEC-RAS is opened and closed for each call (not persistent)
        - For HEC-RAS 6.0+, HDF methods may offer better performance
        - COM interface is single-threaded

        Examples
        --------
        Extract steady results for Plan 02:

        >>> from ras_commander import init_ras_project, RasControl
        >>> init_ras_project(path, "4.1")
        >>> df = RasControl.get_steady_results("02")
        >>> df.to_csv('steady_results.csv', index=False)

        Plot water surface profile:

        >>> import matplotlib.pyplot as plt
        >>> profile_data = df[df['profile'] == 'PF 1']
        >>> plt.plot(profile_data['node_id'].astype(float),
        ...          profile_data['wsel'])
        >>> plt.xlabel('Station')
        >>> plt.ylabel('Water Surface Elevation (ft)')
        >>> plt.show()

        See Also
        --------
        get_unsteady_results : Extract unsteady time series
        run_plan : Run a plan before extracting results
        HdfResultsPlan.get_steady_wse : Modern HDF-based steady extraction

        References
        ----------
        For comparison with HDF-based methods, see:
        ``feature_dev_notes/rascontrol_vs_hdf_comparison.md``
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _extract_operation(com_rc):
            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            results = []
            error_logged = False  # Track if we've already logged comp_msgs

            # Get profiles
            _, profile_names = com_rc.Output_GetProfiles(2, None)

            if profile_names is None:
                raise RuntimeError(
                    "No steady state results found. Please ensure:\n"
                    "  1. The model has been run (use RasControl.run_plan() first)\n"
                    "  2. The current plan is a steady state plan\n"
                    "  3. Results were successfully computed"
                )

            profiles = [{'name': name, 'code': i+1} for i, name in enumerate(profile_names)]
            logger.info(f"Found {len(profiles)} profiles")

            # Get rivers
            _, river_names = com_rc.Output_GetRivers(0, None)

            if river_names is None:
                raise RuntimeError("No river geometry found in model.")

            logger.info(f"Found {len(river_names)} rivers")

            # Extract data
            for riv_code, riv_name in enumerate(river_names, start=1):
                _, _, reach_names = com_rc.Geometry_GetReaches(riv_code, None, None)

                for rch_code, rch_name in enumerate(reach_names, start=1):
                    _, _, _, node_ids, node_types = com_rc.Geometry_GetNodes(
                        riv_code, rch_code, None, None, None
                    )

                    for node_code, (node_id, node_type) in enumerate(
                        zip(node_ids, node_types), start=1
                    ):
                        if node_type == '':  # Cross sections only
                            for profile in profiles:
                                try:
                                    row = {
                                        'river': riv_name.strip(),
                                        'reach': rch_name.strip(),
                                        'node_id': node_id.strip(),
                                        'profile': profile['name'].strip(),
                                    }

                                    # Extract output variables
                                    row['wsel'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.WSEL
                                    )[0]

                                    row['min_ch_el'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.MIN_CH_EL
                                    )[0]

                                    row['velocity'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.VEL_TOTAL
                                    )[0]

                                    row['flow'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.FLOW_TOTAL
                                    )[0]

                                    row['froude'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.FROUDE_CHL
                                    )[0]

                                    row['energy'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.ENERGY
                                    )[0]

                                    row['max_depth'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.MAX_CHL_DPTH
                                    )[0]

                                    results.append(row)

                                except Exception as e:
                                    if not error_logged:
                                        # First error - read and log comp_msgs to diagnose issue
                                        logger.error(
                                            f"Failed to extract results at {riv_name}/{rch_name}/{node_id} "
                                            f"profile {profile['name']}: {e}"
                                        )
                                        logger.error(
                                            "This usually indicates the model run was not successful or "
                                            "results are invalid. Reading computation messages..."
                                        )

                                        # Read comp_msgs file
                                        try:
                                            project_base = project_path.stem
                                            plan_file = project_path.parent / f"{project_base}.p{plan_num}"
                                            comp_msgs_file = Path(str(plan_file) + ".comp_msgs.txt")

                                            if comp_msgs_file.exists():
                                                with open(comp_msgs_file, 'r', encoding='utf-8', errors='ignore') as f:
                                                    comp_msgs = f.read()
                                                logger.error(f"\n{'='*80}\nCOMPUTATION MESSAGES:\n{'='*80}\n{comp_msgs}\n{'='*80}")
                                            else:
                                                logger.error(f"Computation messages file not found: {comp_msgs_file}")
                                        except Exception as msg_error:
                                            logger.error(f"Could not read computation messages: {msg_error}")

                                        error_logged = True
                                        logger.info("Suppressing further extraction warnings...")

            if error_logged and len(results) == 0:
                raise RuntimeError(
                    "Failed to extract any results. The model run likely failed or produced invalid results. "
                    "Check the computation messages above for details."
                )

            logger.info(f"Extracted {len(results)} result rows")
            return pd.DataFrame(results)

        return RasControl._com_open_close(project_path, version, _extract_operation)

    @staticmethod
    def get_unsteady_results(plan: Union[str, Path], max_times: Optional[int] = None,
                            ras_object=None) -> pd.DataFrame:
        """
        Extract unsteady flow time series results from HEC-RAS via COM interface.

        Opens HEC-RAS, loads the specified plan, extracts all computed time series
        data including the critical "Max WS" row, then closes HEC-RAS.

        Parameters
        ----------
        plan : str or Path
            Plan number (e.g., "01", "02") or full path to .prj file.
        max_times : int, optional
            Maximum number of timesteps to extract. If None, extracts all timesteps.
            Note: "Max WS" row is always included and doesn't count toward this limit.
        ras_object : RasPrj, optional
            RAS project object. If None, uses global `ras` object.

        Returns
        -------
        pd.DataFrame
            Unsteady flow time series with one row per cross-section per timestep,
            plus one "Max WS" row per cross-section containing computational maximums.

            **Schema:**

            +-----------------+----------------+-------------------------------------------+
            | Column          | Type           | Description                               |
            +=================+================+===========================================+
            | river           | str            | River name                                |
            +-----------------+----------------+-------------------------------------------+
            | reach           | str            | Reach name                                |
            +-----------------+----------------+-------------------------------------------+
            | node_id         | str            | Cross section river station               |
            +-----------------+----------------+-------------------------------------------+
            | time_index      | int            | 1-based timestep index                    |
            |                 |                | 1 = "Max WS", 2+ = actual timesteps       |
            +-----------------+----------------+-------------------------------------------+
            | time_string     | str            | RAS datetime format "01JAN2000 0000"      |
            |                 |                | or "Max WS" for maximum value row         |
            +-----------------+----------------+-------------------------------------------+
            | datetime        | datetime64[ns] | Parsed timestamp                          |
            |                 |                | pd.NaT for "Max WS" rows                  |
            +-----------------+----------------+-------------------------------------------+
            | wsel            | float          | Water surface elevation (ft or m)         |
            +-----------------+----------------+-------------------------------------------+
            | velocity        | float          | Total velocity (ft/s or m/s)              |
            +-----------------+----------------+-------------------------------------------+
            | flow            | float          | Total flow (cfs or cms)                   |
            +-----------------+----------------+-------------------------------------------+
            | froude          | float          | Channel Froude number (dimensionless)     |
            +-----------------+----------------+-------------------------------------------+
            | energy          | float          | Energy grade elevation (ft or m)          |
            +-----------------+----------------+-------------------------------------------+
            | max_depth       | float          | Maximum channel depth (ft or m)           |
            +-----------------+----------------+-------------------------------------------+
            | min_ch_el       | float          | Minimum channel elevation (ft or m)       |
            +-----------------+----------------+-------------------------------------------+

            **Units depend on project settings (US Customary or SI).**

        Raises
        ------
        ValueError
            - If project not initialized with version
            - If plan not found in project
        RuntimeError
            - If no unsteady results found
            - If HEC-RAS computation was not successful

        Notes
        -----
        **Understanding "Max WS" Rows:**

        The "Max WS" row (time_index=1, time_string="Max WS") contains the maximum
        value at ANY computational timestep, not just the output intervals. This is
        critical for design applications because:

        - HEC-RAS computes at finer intervals than it outputs
        - Peak values often occur between output timesteps
        - "Max WS" captures the true computational maximum

        To separate "Max WS" from time series data:

        >>> df_max = df[df['time_string'] == 'Max WS']
        >>> df_timeseries = df[df['datetime'].notna()]  # Excludes Max WS (has NaT)

        **New in v0.81.0:**

        The `datetime` column is now included automatically as datetime64[ns] objects.
        Users no longer need to manually parse `time_string`. For backward compatibility,
        `time_string` is still included.

        **Performance Notes:**

        - HEC-RAS is opened and closed for each call (not persistent)
        - For large time series, consider using HDF-based methods for better performance
        - COM interface is single-threaded

        Examples
        --------
        Extract and plot time series at a cross section:

        >>> from ras_commander import init_ras_project, RasControl
        >>> import matplotlib.pyplot as plt
        >>>
        >>> init_ras_project(path, "4.1")
        >>> df = RasControl.get_unsteady_results("01")
        >>>
        >>> # Separate max WS from time series
        >>> df_max = df[df['time_string'] == 'Max WS']
        >>> df_ts = df[df['datetime'].notna()]
        >>>
        >>> # Plot time series for specific cross section
        >>> xs_data = df_ts[df_ts['node_id'] == '10000'].sort_values('datetime')
        >>> plt.plot(xs_data['datetime'], xs_data['wsel'])
        >>> plt.axhline(df_max[df_max['node_id'] == '10000']['wsel'].iloc[0],
        ...             color='r', linestyle='--', label='Max WS')
        >>> plt.xlabel('Date/Time')
        >>> plt.ylabel('WSE (ft)')
        >>> plt.legend()
        >>> plt.show()

        Filter to specific time range using datetime column:

        >>> import pandas as pd
        >>> start = pd.Timestamp('1999-02-18')
        >>> end = pd.Timestamp('1999-02-20')
        >>> filtered = df_ts[(df_ts['datetime'] >= start) & (df_ts['datetime'] <= end)]

        See Also
        --------
        get_steady_results : Extract steady state profile results
        get_output_times : List available timesteps before extracting
        run_plan : Run a plan before extracting results
        HdfResultsXsec.get_xsec_timeseries : Modern HDF-based extraction (returns xarray)

        References
        ----------
        For comparison with HDF-based methods, see:
        ``feature_dev_notes/rascontrol_vs_hdf_comparison.md``
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _extract_operation(com_rc):
            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            results = []
            error_logged = False  # Track if we've already logged comp_msgs

            # Get output times
            _, time_strings = com_rc.Output_GetProfiles(0, None)

            if time_strings is None:
                raise RuntimeError(
                    "No unsteady results found. Please ensure:\n"
                    "  1. The model has been run (use RasControl.run_plan() first)\n"
                    "  2. The current plan is an unsteady flow plan\n"
                    "  3. Results were successfully computed"
                )

            times = list(time_strings)
            if max_times:
                times = times[:max_times]

            logger.info(f"Extracting {len(times)} time steps")

            # Get rivers
            _, river_names = com_rc.Output_GetRivers(0, None)

            if river_names is None:
                raise RuntimeError("No river geometry found in model.")

            logger.info(f"Found {len(river_names)} rivers")

            # Extract data
            for riv_code, riv_name in enumerate(river_names, start=1):
                _, _, reach_names = com_rc.Geometry_GetReaches(riv_code, None, None)

                for rch_code, rch_name in enumerate(reach_names, start=1):
                    _, _, _, node_ids, node_types = com_rc.Geometry_GetNodes(
                        riv_code, rch_code, None, None, None
                    )

                    for node_code, (node_id, node_type) in enumerate(
                        zip(node_ids, node_types), start=1
                    ):
                        if node_type == '':  # Cross sections only
                            for time_idx, time_str in enumerate(times, start=1):
                                try:
                                    row = {
                                        'river': riv_name.strip(),
                                        'reach': rch_name.strip(),
                                        'node_id': node_id.strip(),
                                        'time_index': time_idx,
                                        'time_string': time_str.strip(),
                                        'datetime': RasControl._parse_ras_datetime(time_str),
                                    }

                                    # Extract output variables (time_idx is profile code for unsteady)
                                    row['wsel'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.WSEL
                                    )[0]

                                    row['min_ch_el'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.MIN_CH_EL
                                    )[0]

                                    row['velocity'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.VEL_TOTAL
                                    )[0]

                                    row['flow'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.FLOW_TOTAL
                                    )[0]

                                    row['froude'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.FROUDE_CHL
                                    )[0]

                                    row['energy'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.ENERGY
                                    )[0]

                                    row['max_depth'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.MAX_CHL_DPTH
                                    )[0]

                                    results.append(row)

                                except Exception as e:
                                    if not error_logged:
                                        # First error - read and log comp_msgs to diagnose issue
                                        logger.error(
                                            f"Failed to extract results at {riv_name}/{rch_name}/{node_id} "
                                            f"time {time_str}: {e}"
                                        )
                                        logger.error(
                                            "This usually indicates the model run was not successful or "
                                            "results are invalid. Reading computation messages..."
                                        )

                                        # Read comp_msgs file
                                        try:
                                            project_base = project_path.stem
                                            plan_file = project_path.parent / f"{project_base}.p{plan_num}"
                                            comp_msgs_file = Path(str(plan_file) + ".comp_msgs.txt")

                                            if comp_msgs_file.exists():
                                                with open(comp_msgs_file, 'r', encoding='utf-8', errors='ignore') as f:
                                                    comp_msgs = f.read()
                                                logger.error(f"\n{'='*80}\nCOMPUTATION MESSAGES:\n{'='*80}\n{comp_msgs}\n{'='*80}")
                                            else:
                                                logger.error(f"Computation messages file not found: {comp_msgs_file}")
                                        except Exception as msg_error:
                                            logger.error(f"Could not read computation messages: {msg_error}")

                                        error_logged = True
                                        logger.info("Suppressing further extraction warnings...")

            if error_logged and len(results) == 0:
                raise RuntimeError(
                    "Failed to extract any results. The model run likely failed or produced invalid results. "
                    "Check the computation messages above for details."
                )

            logger.info(f"Extracted {len(results)} result rows")
            return pd.DataFrame(results)

        return RasControl._com_open_close(project_path, version, _extract_operation)

    @staticmethod
    def get_output_times(plan: Union[str, Path], ras_object=None) -> List[str]:
        """
        Get list of output times for unsteady run.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance (uses global ras if None)

        Returns:
            List of time strings (e.g., ["01JAN2000 0000", ...])

        Example:
            >>> times = RasControl.get_output_times("01")
            >>> print(f"Found {len(times)} output times")
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _get_times(com_rc):
            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            _, time_strings = com_rc.Output_GetProfiles(0, None)

            if time_strings is None:
                raise RuntimeError(
                    "No unsteady output times found. Ensure plan has been run."
                )

            times = list(time_strings)
            logger.info(f"Found {len(times)} output times")
            return times

        return RasControl._com_open_close(project_path, version, _get_times)

    @staticmethod
    def get_plans(plan: Union[str, Path], ras_object=None) -> List[dict]:
        """
        Get list of plans in project.

        Args:
            plan: Plan number or path to .prj file
            ras_object: Optional RasPrj instance

        Returns:
            List of dicts with 'name' and 'filename' keys
        """
        project_path, version, _, _ = RasControl._get_project_info(plan, ras_object)

        def _get_plans(com_rc):
            # Don't set current plan - just getting list
            _, plan_names, _ = com_rc.Plan_Names(None, None, None)

            plans = []
            for name in plan_names:
                filename, _ = com_rc.Plan_GetFilename(name)
                plans.append({'name': name, 'filename': filename})

            logger.info(f"Found {len(plans)} plans")
            return plans

        return RasControl._com_open_close(project_path, version, _get_plans)

    @staticmethod
    def set_current_plan(plan: Union[str, Path], ras_object=None) -> bool:
        """
        Set the current/active plan by plan number.

        Note: This is rarely needed - run_plan() and get_*_results()
        automatically set the correct plan. This is provided for
        advanced use cases.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance

        Returns:
            True if successful

        Example:
            >>> RasControl.set_current_plan("02")  # Set to Plan 02
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        if not plan_name:
            raise ValueError("Cannot set current plan - plan name could not be determined")

        def _set_plan(com_rc):
            com_rc.Plan_SetCurrent(plan_name)
            logger.info(f"Set current plan to Plan {plan_num}: {plan_name}")
            return True

        return RasControl._com_open_close(project_path, version, _set_plan)

    @staticmethod
    def get_comp_msgs(plan: Union[str, Path], ras_object=None) -> str:
        """
        Read computation messages from .txt file with fallback to HDF.

        The comp_msgs file is created by HEC-RAS during plan computation
        and contains detailed messages about the computation process,
        including warnings, errors, and convergence information.

        This method checks for two .txt naming patterns (version-dependent):
        - .comp_msgs.txt (HEC-RAS 3.x-5.x)
        - .computeMsgs.txt (HEC-RAS 6.x+)

        If neither .txt file exists, falls back to HDF extraction.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance (uses global ras if None)

        Returns:
            String containing computation messages, or empty string if unavailable

        Example:
            >>> from ras_commander import init_ras_project, RasControl
            >>> init_ras_project(path, "4.1")
            >>> msgs = RasControl.get_comp_msgs("08")
            >>> print(msgs)

        Note:
            File naming conventions vary by HEC-RAS version:
            - Older: {plan_file}.comp_msgs.txt
            - Newer: {plan_file}.computeMsgs.txt
            Falls back to HDF: /Results/Summary/Compute Messages (text)
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        # Construct plan file path
        # e.g., "A100_00_00.prj" -> "A100_00_00"
        project_base = project_path.stem
        plan_file = project_path.parent / f"{project_base}.p{plan_num}"

        # Try both .txt file naming patterns (version-dependent)
        comp_msgs_file_old = Path(str(plan_file) + ".comp_msgs.txt")
        comp_msgs_file_new = Path(str(plan_file) + ".computeMsgs.txt")

        comp_msgs_file = None
        if comp_msgs_file_old.exists():
            comp_msgs_file = comp_msgs_file_old
        elif comp_msgs_file_new.exists():
            comp_msgs_file = comp_msgs_file_new

        # If .txt file found, read and return
        if comp_msgs_file is not None:
            logger.info(f"Reading computation messages from: {comp_msgs_file}")

            try:
                with open(comp_msgs_file, 'r', encoding='utf-8', errors='ignore') as f:
                    contents = f.read()

                logger.info(f"Read {len(contents)} characters from comp_msgs file")
                return contents
            except Exception as e:
                logger.error(f"Error reading .txt file: {e}, attempting HDF fallback")

        # If no .txt file found, try HDF fallback
        logger.warning(
            f"Computation messages .txt file not found (tried .comp_msgs.txt and .computeMsgs.txt), "
            f"falling back to HDF extraction"
        )

        try:
            # Late import to avoid circular dependency
            from .HdfResultsPlan import HdfResultsPlan

            # Construct HDF path
            hdf_file = Path(str(plan_file) + ".hdf")
            if hdf_file.exists():
                hdf_contents = HdfResultsPlan.get_compute_messages(hdf_file)
                if hdf_contents:
                    logger.info(f"Successfully retrieved {len(hdf_contents)} characters from HDF")
                    return hdf_contents
        except Exception as e:
            logger.debug(f"HDF fallback failed: {e}")

        # Both methods failed
        logger.debug(
            f"No computation messages found in .txt or HDF sources for plan {plan_num}"
        )
        return ""

    # ========== PROCESS MANAGEMENT API ==========

    @staticmethod
    def list_processes(show_all: bool = False) -> pd.DataFrame:
        """
        List ras.exe processes with tracking status.

        Args:
            show_all: If True, show all ras.exe processes. If False (default),
                     only show processes tracked by this Python session.

        Returns:
            DataFrame with columns: pid, tracked, project, age_sec, status

        Example:
            >>> # Show only tracked processes
            >>> df = RasControl.list_processes()
            >>> print(df)

            >>> # Show all ras.exe on system
            >>> df_all = RasControl.list_processes(show_all=True)
            >>> print(df_all)
        """
        tracked_pids = {lock.ras_pid for lock in _active_sessions.values() if lock.ras_pid}

        rows = []
        for proc in psutil.process_iter(['pid', 'name', 'create_time', 'cmdline']):
            try:
                if proc.info['name'] and proc.info['name'].lower() != 'ras.exe':
                    continue

                is_tracked = proc.info['pid'] in tracked_pids

                if not show_all and not is_tracked:
                    continue

                age = time.time() - proc.info['create_time']

                # Try to extract project from cmdline
                project = "Unknown"
                try:
                    cmdline = ' '.join(proc.info['cmdline'] or [])
                    for token in cmdline.split():
                        if token.endswith('.prj'):
                            project = Path(token).name
                            break
                except (TypeError, AttributeError):
                    pass

                rows.append({
                    'pid': proc.info['pid'],
                    'tracked': is_tracked,
                    'project': project,
                    'age_sec': round(age, 1),
                    'status': 'TRACKED' if is_tracked else 'UNTRACKED'
                })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        if not rows:
            logger.info("No ras.exe processes found")
            return pd.DataFrame(columns=['pid', 'tracked', 'project', 'age_sec', 'status'])

        return pd.DataFrame(rows)

    @staticmethod
    def scan_orphans() -> List[SessionLock]:
        """
        Scan lock files for orphaned sessions from crashed Python processes.

        Returns:
            List of SessionLock objects for orphaned processes (Python dead,
            ras.exe still running).

        Example:
            >>> orphans = RasControl.scan_orphans()
            >>> if orphans:
            >>>     print(f"Found {len(orphans)} orphaned processes")
            >>>     for orphan in orphans:
            >>>         print(f"  PID {orphan.ras_pid}: {Path(orphan.project_path).name}")
        """
        orphans = []

        if not LOCK_DIR.exists():
            return orphans

        for lock_file in LOCK_DIR.glob("rasctl_*.lock"):
            try:
                lock = SessionLock.from_file(lock_file)
                status = _classify_lock_file(lock)

                if status == 'stale_orphan':
                    orphans.append(lock)
                elif status == 'stale_clean':
                    # Clean up stale lock files
                    try:
                        lock_file.unlink()
                        logger.debug(f"Cleaned stale lock file: {lock_file.name}")
                    except Exception as e:
                        logger.debug(f"Could not clean stale lock: {e}")
            except Exception as e:
                logger.warning(f"Error reading lock file {lock_file.name}: {e}")

        return orphans

    @staticmethod
    def cleanup_orphans(interactive: bool = True, dry_run: bool = False) -> int:
        """
        Clean up orphaned ras.exe processes from crashed Python sessions.

        This method ONLY terminates processes that:
        1. Were started by RasControl (have session lock files)
        2. Have a dead parent Python process
        3. Are still running

        Args:
            interactive: If True, prompts user for confirmation before cleanup
            dry_run: If True, only reports what would be cleaned (no action)

        Returns:
            Number of processes cleaned up

        Example:
            >>> # Interactive cleanup (prompts for confirmation)
            >>> RasControl.cleanup_orphans()

            >>> # Automatic cleanup (no prompts)
            >>> count = RasControl.cleanup_orphans(interactive=False)
            >>> print(f"Cleaned {count} orphans")

            >>> # Dry run (see what would be cleaned)
            >>> RasControl.cleanup_orphans(dry_run=True)
        """
        orphans = RasControl.scan_orphans()

        if not orphans:
            print("✅ No orphaned processes found")
            logger.info("No orphaned processes found")
            return 0

        print(f"Found {len(orphans)} orphaned RAS process(es):")
        for orphan in orphans:
            age_min = (time.time() - orphan.start_time) / 60
            print(f"  • PID {orphan.ras_pid}: {Path(orphan.project_path).name} "
                  f"(running {age_min:.1f} min, Python {orphan.python_pid} crashed)")

        if dry_run:
            print("\n[Dry run - no action taken]")
            logger.info("Dry run - no orphans terminated")
            return 0

        if interactive:
            response = input("\nClean up these processes? (y/n): ")
            if response.lower() != 'y':
                print("Cancelled")
                logger.info("Cleanup cancelled by user")
                return 0

        cleaned = 0
        for orphan in orphans:
            try:
                proc = psutil.Process(orphan.ras_pid)
                proc.terminate()
                proc.wait(timeout=10)
                print(f"✅ Terminated PID {orphan.ras_pid}")
                logger.info(f"Terminated orphaned PID {orphan.ras_pid}")
                cleaned += 1

                # Remove lock file
                lock_file = _get_lock_file_path(orphan.session_id)
                lock_file.unlink(missing_ok=True)
            except psutil.TimeoutExpired:
                # Force kill if graceful termination fails
                try:
                    proc.kill()
                    print(f"⚠️  Force killed PID {orphan.ras_pid}")
                    logger.warning(f"Force killed orphaned PID {orphan.ras_pid}")
                    cleaned += 1
                except Exception as e:
                    print(f"❌ Failed to kill PID {orphan.ras_pid}: {e}")
                    logger.error(f"Failed to kill orphaned PID {orphan.ras_pid}: {e}")
            except Exception as e:
                print(f"❌ Failed to terminate PID {orphan.ras_pid}: {e}")
                logger.error(f"Failed to terminate orphaned PID {orphan.ras_pid}: {e}")

        print(f"\n✅ Cleaned up {cleaned}/{len(orphans)} processes")
        logger.info(f"Cleaned up {cleaned}/{len(orphans)} orphaned processes")
        return cleaned

    @staticmethod
    def force_cleanup_all() -> int:
        """
        NUCLEAR OPTION: Terminate ALL ras.exe processes on the system.

        ⚠️  WARNING: This will kill:
        - Your tracked processes
        - Other users' processes
        - Manual HEC-RAS GUI sessions
        - Other Python scripts' processes

        Requires explicit "YES" confirmation to prevent accidental use.

        Returns:
            Number of processes terminated

        Example:
            >>> # Prompts for "YES" confirmation
            >>> RasControl.force_cleanup_all()
        """
        all_ras = [p for p in psutil.process_iter(['pid', 'name'])
                   if p.info['name'] and p.info['name'].lower() == 'ras.exe']

        if not all_ras:
            print("No ras.exe processes found")
            logger.info("No ras.exe processes to clean up")
            return 0

        print(f"⚠️  WARNING: This will terminate ALL {len(all_ras)} ras.exe process(es)")
        print("This includes:")
        print("  • Your tracked processes")
        print("  • Other users' processes")
        print("  • Manual HEC-RAS GUI sessions")
        print("  • Other Python scripts' processes")

        response = input("\n⚠️  Type 'YES' in all caps to confirm: ")
        if response != 'YES':
            print("Cancelled")
            logger.info("Force cleanup cancelled by user")
            return 0

        terminated = 0
        for proc in all_ras:
            try:
                proc.terminate()
                proc.wait(timeout=5)
                print(f"✅ Terminated PID {proc.pid}")
                logger.info(f"Force terminated PID {proc.pid}")
                terminated += 1
            except psutil.TimeoutExpired:
                try:
                    proc.kill()
                    print(f"⚠️  Force killed PID {proc.pid}")
                    logger.warning(f"Force killed PID {proc.pid}")
                    terminated += 1
                except Exception as e:
                    print(f"❌ Failed to kill PID {proc.pid}: {e}")
                    logger.error(f"Failed to kill PID {proc.pid}: {e}")
            except Exception as e:
                print(f"❌ Failed to terminate PID {proc.pid}: {e}")
                logger.error(f"Failed to terminate PID {proc.pid}: {e}")

        print(f"\n✅ Terminated {terminated}/{len(all_ras)} processes")
        logger.info(f"Force cleanup terminated {terminated}/{len(all_ras)} processes")

        # Clean up all lock files
        if LOCK_DIR.exists():
            for lock_file in LOCK_DIR.glob("rasctl_*.lock"):
                try:
                    lock_file.unlink()
                except Exception:
                    pass

        return terminated


if __name__ == '__main__':
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("RasControl (ras-commander API) loaded successfully")
    print(f"Supported versions: {list(RasControl.SUPPORTED_VERSIONS.keys())}")
    print("\nUsage example:")
    print("  from ras_commander import init_ras_project, RasControl")
    print("  init_ras_project(path, '4.1')")
    print("  df = RasControl.get_steady_results('02')")

==================================================

File: c:\GH\ras-commander\ras_commander\RasDss.py
==================================================
"""
RasDss - DSS File Operations for ras-commander

Read HEC-DSS files (V6 and V7) using HEC Monolith libraries via pyjnius.
Lazy loading - no imports until methods are called.

Based on dssrip2 approach: https://github.com/mkoohafkan/dssrip2
Java bridge: pyjnius (https://pyjnius.readthedocs.io/)
"""

import sys
import os
from pathlib import Path
from typing import List, Dict, Optional, Union
import pandas as pd
import numpy as np
import logging

from .Decorators import log_call

logger = logging.getLogger(__name__)


class RasDss:
    """
    Static class for DSS file operations.

    Uses HEC Monolith libraries (auto-downloaded on first use).
    Supports both DSS V6 and V7 formats.

    Usage:
        from RasDss import RasDss

        # Read time series
        df = RasDss.read_timeseries("file.dss", "/BASIN/LOC/FLOW//1HOUR/OBS/")

        # Get catalog
        paths = RasDss.get_catalog("file.dss")
    """

    _jvm_configured = False
    _monolith = None

    @staticmethod
    def _ensure_monolith():
        """Ensure HEC Monolith is downloaded and available."""
        if RasDss._monolith is not None:
            return RasDss._monolith

        # Lazy import
        from ._hec_monolith import HecMonolithDownloader

        RasDss._monolith = HecMonolithDownloader()

        if not RasDss._monolith.is_installed():
            print("\n" + "="*80)
            print("HEC Monolith libraries not found")
            print("Installing automatically (one-time download, ~20 MB)...")
            print("="*80)
            RasDss._monolith.install()

        return RasDss._monolith

    @staticmethod
    def _configure_jvm():
        """Configure JVM classpath for pyjnius (must be done before first import)."""
        if RasDss._jvm_configured:
            return

        # Ensure monolith is installed
        monolith = RasDss._ensure_monolith()

        # Lazy import pyjnius
        try:
            import jnius_config
        except ImportError:
            raise ImportError(
                "pyjnius is required for DSS file operations.\n"
                "Install with: pip install pyjnius"
            )

        # Check if JVM already started
        try:
            from jnius import autoclass
            # If this succeeds, JVM already started
            RasDss._jvm_configured = True
            return
        except:
            pass

        # Get classpath and library path
        classpath = monolith.get_classpath()
        library_path = monolith.get_library_path()

        print("Configuring Java VM for DSS operations...")

        # Set JAVA_HOME if not already set
        if 'JAVA_HOME' not in os.environ:
            # Try to find Java
            java_candidates = [
                Path("C:/Program Files/Java/jre1.8.0_471"),
                Path("C:/Program Files/Java/jdk-11"),
                Path("C:/Program Files/Java/jdk-17"),
                Path("C:/Program Files (x86)/Java/jre1.8.0_471"),
            ]
            for java_home in java_candidates:
                if java_home.exists():
                    os.environ['JAVA_HOME'] = str(java_home)
                    print(f"  Found Java: {java_home}")
                    break
            else:
                raise RuntimeError(
                    "Java not found. Please set JAVA_HOME environment variable or install Java JDK/JRE.\n"
                    "Download from: https://www.oracle.com/java/technologies/downloads/"
                )

        # Set classpath (must be done before first import from jnius)
        jnius_config.add_classpath(*classpath)

        # Set library path for native libraries
        if 'LD_LIBRARY_PATH' in os.environ:
            os.environ['LD_LIBRARY_PATH'] = library_path + ':' + os.environ['LD_LIBRARY_PATH']
        else:
            os.environ['LD_LIBRARY_PATH'] = library_path

        # Windows: Add to PATH for native DLLs
        if os.name == 'nt':
            os.environ['PATH'] = library_path + os.pathsep + os.environ.get('PATH', '')

        RasDss._jvm_configured = True
        print("[OK] Java VM configured")

    @staticmethod
    @log_call
    def get_catalog(dss_file: Union[str, Path]) -> List[str]:
        """
        Get list of all data paths in DSS file.

        Args:
            dss_file: Path to DSS file

        Returns:
            List of DSS path strings

        Example:
            paths = RasDss.get_catalog("sample.dss")
            for path in paths:
                print(path)
        """
        # Configure JVM (must be before first jnius import)
        RasDss._configure_jvm()

        # Import Java classes via pyjnius
        from jnius import autoclass

        HecDss = autoclass('hec.heclib.dss.HecDss')

        dss_file = str(Path(dss_file).resolve())

        # Open DSS file
        dss = HecDss.open(dss_file)

        try:
            # Get catalog (returns Java Vector of pathname strings)
            catalog_vector = dss.getCatalogedPathnames()

            # Convert Java Vector to Python list
            paths = []
            for i in range(catalog_vector.size()):
                paths.append(str(catalog_vector.get(i)))

            return paths

        finally:
            dss.done()

    @staticmethod
    @log_call
    def read_timeseries(
        dss_file: Union[str, Path],
        pathname: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Read time series from DSS file.

        Args:
            dss_file: Path to DSS file
            pathname: DSS pathname (e.g., "/BASIN/LOC/FLOW//1HOUR/OBS/")
            start_date: Optional start date filter
            end_date: Optional end date filter

        Returns:
            pandas DataFrame with DatetimeIndex and 'value' column

        Example:
            df = RasDss.read_timeseries("file.dss", "/BASIN/LOC/FLOW//1HOUR/OBS/")
            print(df.head())
        """
        # Configure JVM (must be before first jnius import)
        RasDss._configure_jvm()

        # Import Java classes via pyjnius
        from jnius import autoclass, cast

        HecDss = autoclass('hec.heclib.dss.HecDss')
        TimeSeriesContainer = autoclass('hec.io.TimeSeriesContainer')

        dss_file = str(Path(dss_file).resolve())

        # Open DSS file
        dss = HecDss.open(dss_file)

        try:
            # Read time series
            # True = ignore D-part (date) for wildcards
            container = dss.get(pathname, True)

            if container is None:
                raise ValueError(f"No data found for pathname: {pathname}")

            # Cast to TimeSeriesContainer to access fields
            tsc = cast('hec.io.TimeSeriesContainer', container)

            # Extract values and times from Java container
            # pyjnius automatically converts Java arrays to Python lists
            values = np.array(tsc.values)  # Java double[] → Python list → numpy array
            times = np.array(tsc.times)    # Java int[] → Python list → numpy array (minutes since 1899-12-31)

            # Convert HEC time to numpy datetime64
            # HEC epoch: December 31, 1899 00:00
            HEC_EPOCH = np.datetime64('1899-12-31T00:00:00')
            datetimes = HEC_EPOCH + times.astype('timedelta64[m]')

            # Create DataFrame
            df = pd.DataFrame({
                'value': values
            }, index=pd.DatetimeIndex(datetimes, name='datetime'))

            # Add metadata as attributes
            df.attrs['pathname'] = pathname
            df.attrs['units'] = str(tsc.units) if tsc.units else ""
            df.attrs['type'] = str(tsc.type) if tsc.type else ""
            df.attrs['interval'] = int(tsc.interval) if hasattr(tsc, 'interval') else None
            df.attrs['dss_file'] = dss_file

            return df

        finally:
            dss.done()

    @staticmethod
    @log_call
    def read_multiple_timeseries(
        dss_file: Union[str, Path],
        pathnames: List[str]
    ) -> Dict[str, pd.DataFrame]:
        """
        Read multiple time series from DSS file.

        Args:
            dss_file: Path to DSS file
            pathnames: List of DSS pathnames

        Returns:
            Dictionary mapping pathnames to DataFrames

        Example:
            paths = ["/BASIN/LOC1/FLOW//1HOUR/OBS/", "/BASIN/LOC2/FLOW//1HOUR/OBS/"]
            data = RasDss.read_multiple_timeseries("file.dss", paths)
            for path, df in data.items():
                print(f"{path}: {len(df)} points")
        """
        results = {}
        for pathname in pathnames:
            try:
                results[pathname] = RasDss.read_timeseries(dss_file, pathname)
            except Exception as e:
                print(f"Warning: Could not read {pathname}: {e}")
                results[pathname] = None

        return results

    @staticmethod
    @log_call
    def get_info(dss_file: Union[str, Path]) -> Dict:
        """
        Get summary information about DSS file.

        Args:
            dss_file: Path to DSS file

        Returns:
            Dictionary with file information

        Example:
            info = RasDss.get_info("sample.dss")
            print(f"Total paths: {info['total_paths']}")
            print(f"File size: {info['file_size_mb']:.2f} MB")
        """
        dss_path = Path(dss_file)

        catalog = RasDss.get_catalog(dss_file)

        return {
            'filepath': str(dss_path.resolve()),
            'filename': dss_path.name,
            'file_size_mb': dss_path.stat().st_size / (1024 * 1024),
            'total_paths': len(catalog),
            'first_5_paths': catalog[:5] if len(catalog) > 5 else catalog,
        }

    @staticmethod
    @log_call
    def extract_boundary_timeseries(
        boundaries_df: pd.DataFrame,
        project_dir: Optional[Union[str, Path]] = None,
        ras_object=None
    ) -> pd.DataFrame:
        """
        Extract DSS time series data for all DSS-defined boundaries.

        Reads boundaries_df and extracts time series for any boundary condition
        defined by a DSS file. Adds the extracted data to the dataframe.

        Args:
            boundaries_df: DataFrame from ras.boundaries_df
            project_dir: Project directory (for resolving relative DSS paths)
            ras_object: RasPrj object (alternative to project_dir)

        Returns:
            Enhanced DataFrame with 'dss_timeseries' column containing extracted data

        Example:
            from ras_commander import init_ras_project
            from ras_commander.RasDss import RasDss

            ras = init_ras_project("project_path", "6.6")

            # Extract all DSS boundary data
            enhanced_boundaries = RasDss.extract_boundary_timeseries(ras.boundaries_df, ras_object=ras)

            # Now enhanced_boundaries has a 'dss_timeseries' column with DataFrames
            for idx, row in enhanced_boundaries.iterrows():
                if row['Use DSS']:
                    print(f"{row['bc_type']}: {len(row['dss_timeseries'])} points")
        """
        # Get project directory
        if ras_object is not None:
            project_dir = ras_object.project_folder
        elif project_dir is None:
            raise ValueError("Must provide either project_dir or ras_object")

        project_dir = Path(project_dir)

        # Create a copy to avoid modifying original
        result_df = boundaries_df.copy()

        # Add column for time series data
        result_df['dss_timeseries'] = None

        # Find DSS-defined boundaries
        # Note: 'Use DSS' column may be string 'True'/'False' or boolean True/False
        dss_boundaries = result_df[
            (result_df['Use DSS'] == True) | (result_df['Use DSS'] == 'True')
        ]

        if len(dss_boundaries) == 0:
            logger.info("No DSS-defined boundaries found")
            return result_df

        logger.info(f"Found {len(dss_boundaries)} DSS-defined boundaries")

        # Extract time series for each DSS boundary
        success_count = 0
        fail_count = 0

        for idx, row in dss_boundaries.iterrows():
            dss_file = row['DSS File']
            dss_path = row['DSS Path']

            if pd.isna(dss_file) or pd.isna(dss_path):
                logger.warning(f"Row {idx}: Missing DSS File or DSS Path")
                continue

            # Resolve DSS file path (may be relative to project directory)
            dss_file_path = Path(dss_file)
            if not dss_file_path.is_absolute():
                dss_file_path = project_dir / dss_file

            if not dss_file_path.exists():
                logger.warning(f"Row {idx}: DSS file not found: {dss_file_path}")
                fail_count += 1
                continue

            try:
                # Read time series
                df_ts = RasDss.read_timeseries(dss_file_path, dss_path)

                # Store in result
                result_df.at[idx, 'dss_timeseries'] = df_ts

                success_count += 1
                logger.info(f"Row {idx}: Extracted {len(df_ts)} points from {dss_file_path.name}")

            except Exception as e:
                logger.warning(f"Row {idx}: Failed to read DSS data: {e}")
                fail_count += 1

        logger.info(f"Extraction complete: {success_count} success, {fail_count} failed")

        return result_df

    @staticmethod
    def shutdown_jvm():
        """
        Shutdown Java Virtual Machine.

        Note: With pyjnius, JVM shutdown is typically not needed.
        This is a placeholder for API compatibility.
        """
        logger.info("pyjnius handles JVM lifecycle automatically")
        pass


if __name__ == "__main__":
    """Test RasDss class"""
    import sys

    print("="*80)
    print("RasDss Test")
    print("="*80)

    # Test file (from TestData)
    test_data_dir = Path(__file__).parent.parent / "TestData"

    # Find a DSS file to test with
    dss_files = list(test_data_dir.glob("*.dss"))

    if not dss_files:
        print("No DSS files found in TestData/")
        sys.exit(1)

    # Use BaldEagleDamBrk.dss (V7 file that we know works)
    test_file = test_data_dir / "BaldEagleDamBrk.dss"

    if not test_file.exists():
        # Use first available file
        test_file = dss_files[0]

    print(f"\nTest file: {test_file.name}")
    print(f"Size: {test_file.stat().st_size / 1024:.2f} KB")

    # Get file info
    print("\n" + "-"*80)
    print("Getting file info...")
    print("-"*80)
    info = RasDss.get_info(test_file)
    for key, value in info.items():
        if key == 'first_5_paths':
            print(f"{key}:")
            for path in value:
                print(f"  - {path}")
        else:
            print(f"{key}: {value}")

    # Get full catalog
    print("\n" + "-"*80)
    print("Getting catalog...")
    print("-"*80)
    catalog = RasDss.get_catalog(test_file)
    print(f"Total paths: {len(catalog)}")

    if len(catalog) > 0:
        # Read first time series
        print("\n" + "-"*80)
        print(f"Reading time series: {catalog[0]}")
        print("-"*80)
        df = RasDss.read_timeseries(test_file, catalog[0])

        print(f"\nDataFrame shape: {df.shape}")
        print(f"Date range: {df.index.min()} to {df.index.max()}")
        print(f"Value range: {df['value'].min():.2f} to {df['value'].max():.2f}")
        print(f"Units: {df.attrs.get('units', 'N/A')}")

        print("\nFirst 10 rows:")
        print(df.head(10))

        print("\nLast 10 rows:")
        print(df.tail(10))

    print("\n" + "="*80)
    print("Test complete!")
    print("="*80)

==================================================

File: c:\GH\ras-commander\ras_commander\RasExamples.py
==================================================
"""
RasExamples - Manage and load HEC-RAS example projects for testing and development

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasExamples:   
- get_example_projects()
- list_categories()
- list_projects()
- extract_project()
- is_project_extracted()
- clean_projects_directory()
        
"""
import os
import requests
import zipfile
import pandas as pd
from pathlib import Path
import shutil
from typing import Union, List
import csv
from datetime import datetime
import logging
import re
from tqdm import tqdm
from ras_commander import get_logger
from ras_commander.LoggingConfig import log_call

logger = get_logger(__name__)

class RasExamples:
    """
    A class for quickly loading HEC-RAS example projects for testing and development of ras-commander.
    All methods are class methods, so no initialization is required.
    """
    base_url = 'https://github.com/HydrologicEngineeringCenter/hec-downloads/releases/download/'
    valid_versions = [
            "6.6", "6.5", "6.4.1", "6.3.1", "6.3", "6.2", "6.1", "6.0",
            "5.0.7", "5.0.6", "5.0.5", "5.0.4", "5.0.3", "5.0.1", "5.0",
            "4.1", "4.0", "3.1.3", "3.1.2", "3.1.1", "3.0", "2.2"
        ]
    base_dir = Path.cwd()
    examples_dir = base_dir
    projects_dir = examples_dir / 'example_projects'
    csv_file_path = examples_dir / 'example_projects.csv'
    
    # Special projects that are not in the main zip file
    SPECIAL_PROJECTS = {
        'NewOrleansMetro': 'https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299502039/299502111/1/1747692522764/NewOrleansMetroPipesExample.zip',
        'BeaverLake': 'https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299501780/299502090/1/1747692179014/BeaverLake-SWMM-Import-Solution.zip'
    }

    _folder_df = None
    _zip_file_path = None

    def __init__(self):
        """Initialize RasExamples and ensure data is loaded"""
        self._ensure_initialized()

    @property
    def folder_df(self):
        """Access the folder DataFrame"""
        self._ensure_initialized()
        return self._folder_df

    def _ensure_initialized(self):
        """Ensure the class is initialized with required data"""
        self.projects_dir.mkdir(parents=True, exist_ok=True)
        if self._folder_df is None:
            self._load_project_data()

    def _load_project_data(self):
        """Load project data from CSV if up-to-date, otherwise extract from zip."""
        logger.debug("Loading project data")
        self._find_zip_file()
        
        if not self._zip_file_path:
            logger.info("No example projects zip file found. Downloading...")
            self.get_example_projects()
        
        try:
            zip_modified_time = os.path.getmtime(self._zip_file_path)
        except FileNotFoundError:
            logger.error(f"Zip file not found at {self._zip_file_path}.")
            return
        
        if self.csv_file_path.exists():
            csv_modified_time = os.path.getmtime(self.csv_file_path)
            
            if csv_modified_time >= zip_modified_time:
                logger.info("Loading project data from CSV...")
                try:
                    self._folder_df = pd.read_csv(self.csv_file_path)
                    logger.info(f"Loaded {len(self._folder_df)} projects from CSV.")
                    return
                except Exception as e:
                    logger.error(f"Failed to read CSV file: {e}")
                    self._folder_df = None

        logger.info("Extracting folder structure from zip file...")
        self._extract_folder_structure()
        self._save_to_csv()

    @classmethod
    def extract_project(cls, project_names: Union[str, List[str]], output_path: Union[str, Path] = None) -> Union[Path, List[Path]]:
        """Extract one or more specific HEC-RAS projects from the zip file.
        
        Args:
            project_names: Single project name as string or list of project names
            output_path: Optional path where the project folder will be placed.
                        Can be a relative path (creates subfolder in current directory)
                        or an absolute path. If None, uses default 'example_projects' folder.
            
        Returns:
            Path: Single Path object if one project extracted
            List[Path]: List of Path objects if multiple projects extracted
        """
        logger.debug(f"Extracting projects: {project_names}")
        
        # Initialize if needed
        if cls._folder_df is None:
            cls._find_zip_file()
            if not cls._zip_file_path:
                logger.info("No example projects zip file found. Downloading...")
                cls.get_example_projects()
            cls._load_project_data()
        
        if isinstance(project_names, str):
            project_names = [project_names]

        # Determine the output directory
        if output_path is None:
            # Use default 'example_projects' folder
            base_output_path = cls.projects_dir
        else:
            # Convert to Path object
            base_output_path = Path(output_path)
            # If relative path, make it relative to current working directory
            if not base_output_path.is_absolute():
                base_output_path = Path.cwd() / base_output_path
            # Create the directory if it doesn't exist
            base_output_path.mkdir(parents=True, exist_ok=True)

        extracted_paths = []

        for project_name in project_names:
            # Check if this is a special project
            if project_name in cls.SPECIAL_PROJECTS:
                try:
                    special_path = cls._extract_special_project(project_name, base_output_path)
                    extracted_paths.append(special_path)
                    continue
                except Exception as e:
                    logger.error(f"Failed to extract special project '{project_name}': {e}")
                    continue
            
            # Regular project extraction logic
            logger.info("----- RasExamples Extracting Project -----")
            logger.info(f"Extracting project '{project_name}'")
            project_path = base_output_path

            if (project_path / project_name).exists():
                logger.info(f"Project '{project_name}' already exists. Deleting existing folder...")
                try:
                    shutil.rmtree(project_path / project_name)
                    logger.info(f"Existing folder for project '{project_name}' has been deleted.")
                except Exception as e:
                    logger.error(f"Failed to delete existing project folder '{project_name}': {e}")
                    continue

            project_info = cls._folder_df[cls._folder_df['Project'] == project_name]
            if project_info.empty:
                error_msg = f"Project '{project_name}' not found in the zip file."
                logger.error(error_msg)
                raise ValueError(error_msg)

            try:
                with zipfile.ZipFile(cls._zip_file_path, 'r') as zip_ref:
                    for file in zip_ref.namelist():
                        parts = Path(file).parts
                        if len(parts) > 1 and parts[1] == project_name:
                            relative_path = Path(*parts[1:])
                            extract_path = project_path / relative_path
                            if file.endswith('/'):
                                extract_path.mkdir(parents=True, exist_ok=True)
                            else:
                                extract_path.parent.mkdir(parents=True, exist_ok=True)
                                with zip_ref.open(file) as source, open(extract_path, "wb") as target:
                                    shutil.copyfileobj(source, target)

                logger.info(f"Successfully extracted project '{project_name}' to {project_path / project_name}")
                extracted_paths.append(project_path / project_name)
            except Exception as e:
                logger.error(f"An error occurred while extracting project '{project_name}': {str(e)}")

        # Return single path if only one project was extracted, otherwise return list
        return extracted_paths[0] if len(project_names) == 1 else extracted_paths

    @classmethod
    def _find_zip_file(cls):
        """Locate the example projects zip file in the examples directory."""
        for version in cls.valid_versions:
            potential_zip = cls.examples_dir / f"Example_Projects_{version.replace('.', '_')}.zip"
            if potential_zip.exists():
                cls._zip_file_path = potential_zip
                logger.info(f"Found zip file: {cls._zip_file_path}")
                break
        else:
            logger.warning("No existing example projects zip file found.")

    @classmethod
    def get_example_projects(cls, version_number='6.6'):
        """
        Download and extract HEC-RAS example projects for a specified version.
        """
        logger.info(f"Getting example projects for version {version_number}")
        if version_number not in cls.valid_versions:
            error_msg = f"Invalid version number. Valid versions are: {', '.join(cls.valid_versions)}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        zip_url = f"{cls.base_url}1.0.33/Example_Projects_{version_number.replace('.', '_')}.zip"
        
        cls.examples_dir.mkdir(parents=True, exist_ok=True)
        
        cls._zip_file_path = cls.examples_dir / f"Example_Projects_{version_number.replace('.', '_')}.zip"

        if not cls._zip_file_path.exists():
            logger.info(f"Downloading HEC-RAS Example Projects from {zip_url}. \nThe file is over 400 MB, so it may take a few minutes to download....")
            try:
                response = requests.get(zip_url, stream=True)
                response.raise_for_status()
                with open(cls._zip_file_path, 'wb') as file:
                    shutil.copyfileobj(response.raw, file)
                logger.info(f"Downloaded to {cls._zip_file_path}")
            except requests.exceptions.RequestException as e:
                logger.error(f"Failed to download the zip file: {e}")
                raise
        else:
            logger.info("HEC-RAS Example Projects zip file already exists. Skipping download.")

        cls._load_project_data()
        return cls.projects_dir

    @classmethod
    def _load_project_data(cls):
        """Load project data from CSV if up-to-date, otherwise extract from zip."""
        logger.debug("Loading project data")
        
        try:
            zip_modified_time = os.path.getmtime(cls._zip_file_path)
        except FileNotFoundError:
            logger.error(f"Zip file not found at {cls._zip_file_path}.")
            return
        
        if cls.csv_file_path.exists():
            csv_modified_time = os.path.getmtime(cls.csv_file_path)
            
            if csv_modified_time >= zip_modified_time:
                logger.info("Loading project data from CSV...")
                try:
                    cls._folder_df = pd.read_csv(cls.csv_file_path)
                    logger.info(f"Loaded {len(cls._folder_df)} projects from CSV.")
                    return
                except Exception as e:
                    logger.error(f"Failed to read CSV file: {e}")
                    cls._folder_df = None

        logger.info("Extracting folder structure from zip file...")
        cls._extract_folder_structure()
        cls._save_to_csv()

    @classmethod
    def _extract_folder_structure(cls):
        """
        Extract folder structure from the zip file.

        Populates folder_df with category and project information.
        """
        folder_data = []
        try:
            with zipfile.ZipFile(cls._zip_file_path, 'r') as zip_ref:
                for file in zip_ref.namelist():
                    parts = Path(file).parts
                    if len(parts) > 1:
                        folder_data.append({
                            'Category': parts[0],
                            'Project': parts[1]
                        })
        
            cls._folder_df = pd.DataFrame(folder_data).drop_duplicates()
            logger.info(f"Extracted {len(cls._folder_df)} projects.")
            logger.debug(f"folder_df:\n{cls._folder_df}")
        except zipfile.BadZipFile:
            logger.error(f"The file {cls._zip_file_path} is not a valid zip file.")
            cls._folder_df = pd.DataFrame(columns=['Category', 'Project'])
        except Exception as e:
            logger.error(f"An error occurred while extracting the folder structure: {str(e)}")
            cls._folder_df = pd.DataFrame(columns=['Category', 'Project'])

    @classmethod
    def _save_to_csv(cls):
        """Save the extracted folder structure to CSV file."""
        if cls._folder_df is not None and not cls._folder_df.empty:
            try:
                cls._folder_df.to_csv(cls.csv_file_path, index=False)
                logger.info(f"Saved project data to {cls.csv_file_path}")
            except Exception as e:
                logger.error(f"Failed to save project data to CSV: {e}")
        else:
            logger.warning("No folder data to save to CSV.")

    @classmethod
    def list_categories(cls):
        """
        List all categories of example projects.
        """
        if cls._folder_df is None or 'Category' not in cls._folder_df.columns:
            logger.warning("No categories available. Make sure the zip file is properly loaded.")
            return []
        categories = cls._folder_df['Category'].unique()
        logger.info(f"Available categories: {', '.join(categories)}")
        return categories.tolist()

    @classmethod
    def list_projects(cls, category=None):
        """
        List all projects or projects in a specific category.
        
        Note: Special projects (NewOrleansMetro, BeaverLake) are also available but not listed
        in categories as they are downloaded separately.
        """
        if cls._folder_df is None:
            logger.warning("No projects available. Make sure the zip file is properly loaded.")
            return []
        if category:
            projects = cls._folder_df[cls._folder_df['Category'] == category]['Project'].unique()
            logger.info(f"Projects in category '{category}': {', '.join(projects)}")
        else:
            projects = cls._folder_df['Project'].unique()
            # Add special projects to the list
            all_projects = list(projects) + list(cls.SPECIAL_PROJECTS.keys())
            logger.info(f"All available projects: {', '.join(all_projects)}")
            return all_projects
        return projects.tolist()

    @classmethod
    def is_project_extracted(cls, project_name):
        """
        Check if a specific project is already extracted.
        """
        project_path = cls.projects_dir / project_name
        is_extracted = project_path.exists()
        logger.info(f"Project '{project_name}' extracted: {is_extracted}")
        return is_extracted

    @classmethod
    def clean_projects_directory(cls):
        """Remove all extracted projects from the example_projects directory."""
        logger.info(f"Cleaning projects directory: {cls.projects_dir}")
        if cls.projects_dir.exists():
            try:
                shutil.rmtree(cls.projects_dir)
                logger.info("All projects have been removed.")
            except Exception as e:
                logger.error(f"Failed to remove projects directory: {e}")
        else:
            logger.warning("Projects directory does not exist.")
        cls.projects_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Projects directory cleaned and recreated.")

    @classmethod
    def download_fema_ble_model(cls, huc8, output_dir=None):
        """
        Download a FEMA Base Level Engineering (BLE) model for a given HUC8.

        Args:
            huc8 (str): The 8-digit Hydrologic Unit Code (HUC) for the desired watershed.
            output_dir (str, optional): The directory to save the downloaded files. If None, uses the current working directory.

        Returns:
            str: The path to the downloaded and extracted model directory.

        Note:
            This method downloads the BLE model from the FEMA website and extracts it to the specified directory.
        """
        # Method implementation...

    @classmethod
    def _make_safe_folder_name(cls, name: str) -> str:
        """
        Convert a string to a safe folder name by replacing unsafe characters with underscores.
        """
        safe_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', name)
        logger.debug(f"Converted '{name}' to safe folder name '{safe_name}'")
        return safe_name

    @classmethod
    def _download_file_with_progress(cls, url: str, dest_folder: Path, file_size: int) -> Path:
        """
        Download a file from a URL to a specified destination folder with progress bar.
        """
        local_filename = dest_folder / url.split('/')[-1]
        try:
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                with open(local_filename, 'wb') as f, tqdm(
                    desc=local_filename.name,
                    total=file_size,
                    unit='iB',
                    unit_scale=True,
                    unit_divisor=1024,
                ) as progress_bar:
                    for chunk in r.iter_content(chunk_size=8192):
                        size = f.write(chunk)
                        progress_bar.update(size)
            logger.info(f"Successfully downloaded {url} to {local_filename}")
            return local_filename
        except requests.exceptions.RequestException as e:
            logger.error(f"Request failed for {url}: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to write file {local_filename}: {e}")
            raise

    @classmethod
    def _convert_size_to_bytes(cls, size_str: str) -> int:
        """
        Convert a human-readable file size to bytes.
        """
        units = {'B': 1, 'KB': 1024, 'MB': 1024**2, 'GB': 1024**3, 'TB': 1024**4}
        size_str = size_str.upper().replace(' ', '')
        if not re.match(r'^\d+(\.\d+)?[BKMGT]B?$', size_str):
            raise ValueError(f"Invalid size string: {size_str}")
        
        number, unit = float(re.findall(r'[\d\.]+', size_str)[0]), re.findall(r'[BKMGT]B?', size_str)[0]
        return int(number * units[unit])

    @classmethod
    def _extract_special_project(cls, project_name: str, output_path: Path = None) -> Path:
        """
        Download and extract special projects that are not in the main zip file.
        
        Args:
            project_name: Name of the special project ('NewOrleansMetro' or 'BeaverLake')
            output_path: Base output directory path. If None, uses cls.projects_dir
            
        Returns:
            Path: Path to the extracted project directory
            
        Raises:
            ValueError: If the project is not a recognized special project
        """
        if project_name not in cls.SPECIAL_PROJECTS:
            raise ValueError(f"'{project_name}' is not a recognized special project")
        
        logger.info(f"----- RasExamples Extracting Special Project -----")
        logger.info(f"Extracting special project '{project_name}'")
        
        # Use provided output_path or default
        base_path = output_path if output_path else cls.projects_dir
        
        # Create the project directory
        project_path = base_path / project_name
        
        # Check if already exists
        if project_path.exists():
            logger.info(f"Special project '{project_name}' already exists. Deleting existing folder...")
            try:
                shutil.rmtree(project_path)
                logger.info(f"Existing folder for project '{project_name}' has been deleted.")
            except Exception as e:
                logger.error(f"Failed to delete existing project folder '{project_name}': {e}")
                raise
        
        # Create the project directory
        project_path.mkdir(parents=True, exist_ok=True)
        
        # Download the zip file
        url = cls.SPECIAL_PROJECTS[project_name]
        zip_file_path = base_path / f"{project_name}_temp.zip"
        
        logger.info(f"Downloading special project from: {url}")
        logger.info("This may take a few moments...")
        
        try:
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()
            
            # Get total file size if available
            total_size = int(response.headers.get('content-length', 0))
            
            # Download with progress bar
            with open(zip_file_path, 'wb') as file:
                if total_size > 0:
                    with tqdm(
                        desc=f"Downloading {project_name}",
                        total=total_size,
                        unit='iB',
                        unit_scale=True,
                        unit_divisor=1024,
                    ) as progress_bar:
                        for chunk in response.iter_content(chunk_size=8192):
                            size = file.write(chunk)
                            progress_bar.update(size)
                else:
                    # No content length, download without progress bar
                    for chunk in response.iter_content(chunk_size=8192):
                        file.write(chunk)
            
            logger.info(f"Downloaded special project zip file to {zip_file_path}")
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to download special project '{project_name}': {e}")
            if zip_file_path.exists():
                zip_file_path.unlink()
            raise
        
        # Extract the zip file directly to the project directory
        try:
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                # Extract directly to the project directory (no internal folder structure)
                zip_ref.extractall(project_path)
            logger.info(f"Successfully extracted special project '{project_name}' to {project_path}")
            
        except Exception as e:
            logger.error(f"Failed to extract special project '{project_name}': {e}")
            if project_path.exists():
                shutil.rmtree(project_path)
            raise
        finally:
            # Clean up the temporary zip file
            if zip_file_path.exists():
                zip_file_path.unlink()
                logger.debug(f"Removed temporary zip file: {zip_file_path}")
        
        return project_path
==================================================

File: c:\GH\ras-commander\ras_commander\RasGeo.py
==================================================
"""
RasGeo - Operations for handling geometry files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasGeo:
- clear_geompre_files(): Clears geometry preprocessor files for specified plan files
- get_mannings_baseoverrides(): Reads base Manning's n table from a geometry file
- get_mannings_regionoverrides(): Reads Manning's n region overrides from a geometry file
- set_mannings_baseoverrides(): Writes base Manning's n values to a geometry file
- set_mannings_regionoverrides(): Writes regional Manning's n overrides to a geometry file
"""
import os
from pathlib import Path
from typing import List, Union
import pandas as pd  # Added pandas import
from .RasPlan import RasPlan
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasGeo:
    """
    A class for operations on HEC-RAS geometry files.
    """
    
    @staticmethod
    @log_call
    def clear_geompre_files(
        plan_files: Union[str, Path, List[Union[str, Path]]] = None,
        ras_object = None
    ) -> None:
        """
        Clear HEC-RAS geometry preprocessor files for specified plan files.

        Geometry preprocessor files (.c* extension) contain computed hydraulic properties derived
        from the geometry. These should be cleared when the geometry changes to ensure that
        HEC-RAS recomputes all hydraulic tables with updated geometry information.

        Limitations/Future Work:
        - This function only deletes the geometry preprocessor file.
        - It does not clear the IB tables.
        - It also does not clear geometry preprocessor tables from the geometry HDF.
        - All of these features will need to be added to reliably remove geometry preprocessor 
          files for 1D and 2D projects.
        
        Parameters:
            plan_files (Union[str, Path, List[Union[str, Path]]], optional): 
                Full path(s) to the HEC-RAS plan file(s) (.p*).
                If None, clears all plan files in the project directory.
            ras_object: An optional RAS object instance.
        
        Returns:
            None: The function deletes files and updates the ras object's geometry dataframe

        Example:
            # Clone a plan and geometry
            new_plan_number = RasPlan.clone_plan("01")
            new_geom_number = RasPlan.clone_geom("01")
            
            # Set the new geometry for the cloned plan
            RasPlan.set_geom(new_plan_number, new_geom_number)
            plan_path = RasPlan.get_plan_path(new_plan_number)
            
            # Clear geometry preprocessor files to ensure clean results
            RasGeo.clear_geompre_files(plan_path)
            print(f"Cleared geometry preprocessor files for plan {new_plan_number}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        def clear_single_file(plan_file: Union[str, Path], ras_obj) -> None:
            plan_path = Path(plan_file)
            geom_preprocessor_suffix = '.c' + ''.join(plan_path.suffixes[1:]) if plan_path.suffixes else '.c'
            geom_preprocessor_file = plan_path.with_suffix(geom_preprocessor_suffix)
            if geom_preprocessor_file.exists():
                try:
                    geom_preprocessor_file.unlink()
                    logger.info(f"Deleted geometry preprocessor file: {geom_preprocessor_file}")
                except PermissionError:
                    logger.error(f"Permission denied: Unable to delete geometry preprocessor file: {geom_preprocessor_file}")
                    raise PermissionError(f"Unable to delete geometry preprocessor file: {geom_preprocessor_file}. Permission denied.")
                except OSError as e:
                    logger.error(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
                    raise OSError(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
            else:
                logger.warning(f"No geometry preprocessor file found for: {plan_file}")
        
        if plan_files is None:
            logger.info("Clearing all geometry preprocessor files in the project directory.")
            plan_files_to_clear = list(ras_obj.project_folder.glob(r'*.p*'))
        elif isinstance(plan_files, (str, Path)):
            plan_files_to_clear = [plan_files]
            logger.info(f"Clearing geometry preprocessor file for single plan: {plan_files}")
        elif isinstance(plan_files, list):
            plan_files_to_clear = plan_files
            logger.info(f"Clearing geometry preprocessor files for multiple plans: {plan_files}")
        else:
            logger.error("Invalid input type for plan_files.")
            raise ValueError("Invalid input. Please provide a string, Path, list of paths, or None.")
        
        for plan_file in plan_files_to_clear:
            clear_single_file(plan_file, ras_obj)
        
        try:
            ras_obj.geom_df = ras_obj.get_geom_entries()
            logger.info("Geometry dataframe updated successfully.")
        except Exception as e:
            logger.error(f"Failed to update geometry dataframe: {str(e)}")
            raise

    @staticmethod
    @log_call
    def get_mannings_baseoverrides(geom_file_path):
        """
        Reads the base Manning's n table from a HEC-RAS geometry file.
        
        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)
        
        Returns:
        --------
        pandas.DataFrame
            DataFrame with Table Number, Land Cover Name, and Base Manning's n Value
            
        Example:
        --------
        >>> geom_path = RasPlan.get_geom_path("01")
        >>> mannings_df = RasGeo.get_mannings_baseoverrides(geom_path)
        >>> print(mannings_df)
        """
        import pandas as pd
        from pathlib import Path
        
        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)
        
        base_table_rows = []
        table_number = None
        
        # Read the geometry file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()
        
        # Parse the file
        reading_base_table = False
        for line in lines:
            line = line.strip()
            
            # Find the table number
            if line.startswith('LCMann Table='):
                table_number = line.split('=')[1]
                reading_base_table = True
                continue
            
            # Stop reading when we hit a line without a comma or starting with LCMann
            if reading_base_table and (not ',' in line or line.startswith('LCMann')):
                reading_base_table = False
                continue
                
            # Parse data rows in base table
            if reading_base_table and ',' in line:
                # Check if there are multiple commas in the line
                parts = line.split(',')
                if len(parts) > 2:
                    # Handle case where land cover name contains commas
                    name = ','.join(parts[:-1])
                    value = parts[-1]
                else:
                    name, value = parts
                
                try:
                    base_table_rows.append([table_number, name, float(value)])
                except ValueError:
                    # Log the error and continue
                    print(f"Error parsing line: {line}")
                    continue
        
        # Create DataFrame
        if base_table_rows:
            df = pd.DataFrame(base_table_rows, columns=['Table Number', 'Land Cover Name', 'Base Mannings n Value'])
            return df
        else:
            return pd.DataFrame(columns=['Table Number', 'Land Cover Name', 'Base Mannings n Value'])


    @staticmethod
    @log_call
    def get_mannings_regionoverrides(geom_file_path):
        """
        Reads the Manning's n region overrides from a HEC-RAS geometry file.
        
        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)
        
        Returns:
        --------
        pandas.DataFrame
            DataFrame with Table Number, Land Cover Name, MainChannel value, and Region Name
            
        Example:
        --------
        >>> geom_path = RasPlan.get_geom_path("01")
        >>> region_overrides_df = RasGeo.get_mannings_regionoverrides(geom_path)
        >>> print(region_overrides_df)
        """
        import pandas as pd
        from pathlib import Path
        
        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)
        
        region_rows = []
        current_region = None
        current_table = None
        
        # Read the geometry file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()
        
        # Parse the file
        reading_region_table = False
        for line in lines:
            line = line.strip()
            
            # Find region name
            if line.startswith('LCMann Region Name='):
                current_region = line.split('=')[1]
                continue
                
            # Find region table number
            if line.startswith('LCMann Region Table='):
                current_table = line.split('=')[1]
                reading_region_table = True
                continue
            
            # Stop reading when we hit a line without a comma or starting with LCMann
            if reading_region_table and (not ',' in line or line.startswith('LCMann')):
                reading_region_table = False
                continue
                
            # Parse data rows in region table
            if reading_region_table and ',' in line and current_region is not None:
                # Check if there are multiple commas in the line
                parts = line.split(',')
                if len(parts) > 2:
                    # Handle case where land cover name contains commas
                    name = ','.join(parts[:-1])
                    value = parts[-1]
                else:
                    name, value = parts
                
                try:
                    region_rows.append([current_table, name, float(value), current_region])
                except ValueError:
                    # Log the error and continue
                    print(f"Error parsing line: {line}")
                    continue
        
        # Create DataFrame
        if region_rows:
            return pd.DataFrame(region_rows, columns=['Table Number', 'Land Cover Name', 'MainChannel', 'Region Name'])
        else:
            return pd.DataFrame(columns=['Table Number', 'Land Cover Name', 'MainChannel', 'Region Name'])
        


    @staticmethod
    @log_call
    def set_mannings_baseoverrides(geom_file_path, mannings_data):
        """
        Writes base Manning's n values to a HEC-RAS geometry file.
        
        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)
        mannings_data : DataFrame
            DataFrame with columns 'Table Number', 'Land Cover Name', and 'Base Manning\'s n Value'
        
        Returns:
        --------
        bool
            True if successful
        """
        from pathlib import Path
        import shutil
        import pandas as pd
        import datetime
        
        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)
        
        # Create backup
        backup_path = geom_file_path.with_suffix(geom_file_path.suffix + '.bak')
        shutil.copy2(geom_file_path, backup_path)
        
        # Read the entire file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()
        
        # Find the Manning's table section
        table_number = str(mannings_data['Table Number'].iloc[0])
        start_idx = None
        end_idx = None
        
        for i, line in enumerate(lines):
            if line.strip() == f"LCMann Table={table_number}":
                start_idx = i
                # Find the end of this table (next LCMann directive or end of file)
                for j in range(i+1, len(lines)):
                    if lines[j].strip().startswith('LCMann'):
                        end_idx = j
                        break
                if end_idx is None:  # If we reached the end of the file
                    end_idx = len(lines)
                break
        
        if start_idx is None:
            raise ValueError(f"Manning's table {table_number} not found in the geometry file")
        
        # Extract existing land cover names from the file
        existing_landcover = []
        for i in range(start_idx+1, end_idx):
            line = lines[i].strip()
            if ',' in line:
                parts = line.split(',')
                if len(parts) > 2:
                    # Handle case where land cover name contains commas
                    name = ','.join(parts[:-1])
                else:
                    name = parts[0]
                existing_landcover.append(name)
        
        # Check if all land cover names in the dataframe match the file
        df_landcover = mannings_data['Land Cover Name'].tolist()
        if set(df_landcover) != set(existing_landcover):
            missing = set(existing_landcover) - set(df_landcover)
            extra = set(df_landcover) - set(existing_landcover)
            error_msg = "Land cover names don't match between file and dataframe.\n"
            if missing:
                error_msg += f"Missing in dataframe: {missing}\n"
            if extra:
                error_msg += f"Extra in dataframe: {extra}"
            raise ValueError(error_msg)
        
        # Create new content for the table
        new_content = [f"LCMann Table={table_number}\n"]
        
        # Add base table entries
        for _, row in mannings_data.iterrows():
            new_content.append(f"{row['Land Cover Name']},{row['Base Manning''s n Value']}\n")
        
        # Replace the section in the original file
        updated_lines = lines[:start_idx] + new_content + lines[end_idx:]
        
        # Update the time stamp
        current_time = datetime.datetime.now().strftime("%b/%d/%Y %H:%M:%S")
        for i, line in enumerate(updated_lines):
            if line.strip().startswith("LCMann Time="):
                updated_lines[i] = f"LCMann Time={current_time}\n"
                break
        
        # Write the updated file
        with open(geom_file_path, 'w') as f:
            f.writelines(updated_lines)
        
        return True







    @staticmethod
    @log_call
    def set_mannings_regionoverrides(geom_file_path, mannings_data):
        """
        Writes regional Manning's n overrides to a HEC-RAS geometry file.
        
        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)
        mannings_data : DataFrame
            DataFrame with columns 'Table Number', 'Land Cover Name', 'MainChannel', and 'Region Name'
        
        Returns:
        --------
        bool
            True if successful
        """
        from pathlib import Path
        import shutil
        import pandas as pd
        import datetime
        
        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)
        
        # Create backup
        backup_path = geom_file_path.with_suffix(geom_file_path.suffix + '.bak')
        shutil.copy2(geom_file_path, backup_path)
        
        # Read the entire file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()
        
        # Group data by region
        regions = mannings_data.groupby('Region Name')
        
        # Find the Manning's region sections
        for region_name, region_data in regions:
            table_number = str(region_data['Table Number'].iloc[0])
            
            # Find the region section
            region_start_idx = None
            region_table_idx = None
            region_end_idx = None
            region_polygon_line = None
            
            for i, line in enumerate(lines):
                if line.strip() == f"LCMann Region Name={region_name}":
                    region_start_idx = i
                
                if region_start_idx is not None and line.strip() == f"LCMann Region Table={table_number}":
                    region_table_idx = i
                    
                    # Find the end of this region (next LCMann Region or end of file)
                    for j in range(i+1, len(lines)):
                        if lines[j].strip().startswith('LCMann Region Name=') or lines[j].strip().startswith('LCMann Region Polygon='):
                            if lines[j].strip().startswith('LCMann Region Polygon='):
                                region_polygon_line = lines[j]
                            region_end_idx = j
                            break
                    if region_end_idx is None:  # If we reached the end of the file
                        region_end_idx = len(lines)
                    break
            
            if region_start_idx is None or region_table_idx is None:
                raise ValueError(f"Region {region_name} with table {table_number} not found in the geometry file")
            
            # Extract existing land cover names from the file
            existing_landcover = []
            for i in range(region_table_idx+1, region_end_idx):
                line = lines[i].strip()
                if ',' in line and not line.startswith('LCMann'):
                    parts = line.split(',')
                    if len(parts) > 2:
                        # Handle case where land cover name contains commas
                        name = ','.join(parts[:-1])
                    else:
                        name = parts[0]
                    existing_landcover.append(name)
            
            # Check if all land cover names in the dataframe match the file
            df_landcover = region_data['Land Cover Name'].tolist()
            if set(df_landcover) != set(existing_landcover):
                missing = set(existing_landcover) - set(df_landcover)
                extra = set(df_landcover) - set(existing_landcover)
                error_msg = f"Land cover names for region {region_name} don't match between file and dataframe.\n"
                if missing:
                    error_msg += f"Missing in dataframe: {missing}\n"
                if extra:
                    error_msg += f"Extra in dataframe: {extra}"
                raise ValueError(error_msg)
            
            # Create new content for the region
            new_content = [
                f"LCMann Region Name={region_name}\n",
                f"LCMann Region Table={table_number}\n"
            ]
            
            # Add region table entries
            for _, row in region_data.iterrows():
                new_content.append(f"{row['Land Cover Name']},{row['MainChannel']}\n")
            
            # Add the region polygon line if it exists
            if region_polygon_line:
                new_content.append(region_polygon_line)
            
            # Replace the section in the original file
            if region_polygon_line:
                # If we have a polygon line, include it in the replacement
                updated_lines = lines[:region_start_idx] + new_content + lines[region_end_idx+1:]
            else:
                # If no polygon line, just replace up to the end index
                updated_lines = lines[:region_start_idx] + new_content + lines[region_end_idx:]
            
            # Update the lines for the next region
            lines = updated_lines
        
        # Update the time stamp
        current_time = datetime.datetime.now().strftime("%b/%d/%Y %H:%M:%S")
        for i, line in enumerate(lines):
            if line.strip().startswith("LCMann Region Time="):
                lines[i] = f"LCMann Region Time={current_time}\n"
                break
        
        # Write the updated file
        with open(geom_file_path, 'w') as f:
            f.writelines(lines)
        
        return True
==================================================

File: c:\GH\ras-commander\ras_commander\RasGeometry.py
==================================================
"""
RasGeometry - Operations for parsing and modifying HEC-RAS geometry files

This module provides comprehensive functionality for reading and modifying
HEC-RAS plain text geometry files (.g##). It handles 1D cross sections,
2D flow areas, storage areas, connections, and all related geometry data.

All methods are static and designed to be used without instantiation.

List of Functions:

Cross Section Operations:
- get_cross_sections() - Extract all cross section metadata
- get_station_elevation() - Read station/elevation pairs for a cross section
- set_station_elevation() - Write station/elevation with automatic bank interpolation
- get_bank_stations() - Read left and right bank station locations
- get_expansion_contraction() - Read expansion and contraction coefficients
- get_mannings_n() - Read Manning's roughness values with LOB/Channel/ROB classification

Storage Area Operations:
- get_storage_areas() - List all storage area names (excluding 2D flow areas)
- get_storage_elevation_volume() - Read elevation-volume curve for a storage area

Lateral Structure Operations:
- get_lateral_structures() - List all lateral weir structures with metadata
- get_lateral_weir_profile() - Read station-elevation profile for lateral weir

SA/2D Connection Operations:
- get_connections() - List all SA/2D area connections
- get_connection_weir_profile() - Read dam/weir crest station-elevation profile
- get_connection_gates() - Read gate definitions (CSV format, 23+ parameters)

Example Usage:
    >>> from ras_commander import RasGeometry
    >>> from pathlib import Path
    >>>
    >>> # List all cross sections
    >>> geom_file = Path("BaldEagle.g01")
    >>> xs_df = RasGeometry.get_cross_sections(geom_file)
    >>> print(f"Found {len(xs_df)} cross sections")
    >>>
    >>> # Get station/elevation for specific XS
    >>> sta_elev = RasGeometry.get_station_elevation(
    ...     geom_file, "Bald Eagle Creek", "Reach 1", "138154.4"
    ... )
    >>> print(sta_elev.head())
    >>>
    >>> # Modify and write back
    >>> sta_elev['Elevation'] += 1.0  # Raise XS by 1 foot
    >>> RasGeometry.set_station_elevation(
    ...     geom_file, "Bald Eagle Creek", "Reach 1", "138154.4", sta_elev
    ... )

Technical Notes:
    - Uses FORTRAN-era fixed-width format (8-char columns for numeric data)
    - Count interpretation: "#Sta/Elev= 40" means 40 PAIRS (80 total values)
    - Always creates .bak backup before modification


References:
    - See research/geometry file parsing/geometry_docs/1D_geometry_structure.md
    - See research/geometry file parsing/geometry_docs/_PARSING_PATTERNS_REFERENCE.md
"""

from pathlib import Path
from typing import Union, Optional, List, Tuple, Dict, Any
import pandas as pd
import numpy as np

from .LoggingConfig import get_logger
from .Decorators import log_call
from .RasGeometryUtils import RasGeometryUtils

logger = get_logger(__name__)


class RasGeometry:
    """
    Operations for parsing and modifying HEC-RAS geometry files.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8      # Character width for numeric data in geometry files
    VALUES_PER_LINE = 10        # Number of values per line in fixed-width format
    MAX_XS_POINTS = 450         # HEC-RAS hard limit on cross section points

    # Parsing constants
    DEFAULT_SEARCH_RANGE = 50   # Default number of lines to search for keywords after XS header
    MAX_PARSE_LINES = 100       # Safety limit on lines to parse for data blocks

    # ========== PRIVATE HELPER METHODS ==========

    @staticmethod
    def _find_cross_section(lines: List[str], river: str, reach: str, rs: str) -> Optional[int]:
        """
        Find cross section in geometry file and return starting line index.

        This helper eliminates ~320 lines of duplication across 8 public methods.

        Args:
            lines: File lines (from readlines())
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string, e.g., "138154.4")

        Returns:
            Line index where "Type RM Length L Ch R =" for matching XS starts,
            or None if not found

        Example:
            >>> with open(geom_file, 'r') as f:
            ...     lines = f.readlines()
            >>> idx = RasGeometry._find_cross_section(lines, "Bald Eagle", "Loc Hav", "138154.4")
            >>> if idx:
            ...     # Process XS block starting at lines[idx]
        """
        current_river = None
        current_reach = None

        for i, line in enumerate(lines):
            # Track current river/reach
            if line.startswith("River Reach="):
                values = RasGeometryUtils.extract_comma_list(line, "River Reach")
                if len(values) >= 2:
                    current_river = values[0]
                    current_reach = values[1]

            # Find matching cross section
            elif line.startswith("Type RM Length L Ch R ="):
                value_str = RasGeometryUtils.extract_keyword_value(line, "Type RM Length L Ch R")
                values = [v.strip() for v in value_str.split(',')]

                if len(values) > 1:
                    # Format: Type, RS, Length_L, Length_Ch, Length_R
                    xs_rs = values[1]  # RS is second value

                    if (current_river == river and
                        current_reach == reach and
                        xs_rs == rs):
                        logger.debug(f"Found XS at line {i}: {river}/{reach}/RS {rs}")
                        return i

        logger.debug(f"XS not found: {river}/{reach}/RS {rs}")
        return None

    @staticmethod
    def _read_bank_stations(lines: List[str], start_idx: int,
                           search_range: Optional[int] = None) -> Optional[Tuple[float, float]]:
        """
        Read bank stations from XS block starting at start_idx.

        This helper eliminates ~40 lines of duplication across 4 public methods.

        Args:
            lines: File lines (from readlines())
            start_idx: Index to start searching (typically from _find_cross_section)
            search_range: Number of lines to search ahead (default: DEFAULT_SEARCH_RANGE)

        Returns:
            (left_bank, right_bank) tuple or None if no banks defined

        Example:
            >>> xs_idx = RasGeometry._find_cross_section(lines, river, reach, rs)
            >>> banks = RasGeometry._read_bank_stations(lines, xs_idx)
            >>> if banks:
            ...     left, right = banks
        """
        if search_range is None:
            search_range = RasGeometry.DEFAULT_SEARCH_RANGE

        for k in range(start_idx, min(start_idx + search_range, len(lines))):
            if lines[k].startswith("Bank Sta="):
                bank_str = RasGeometryUtils.extract_keyword_value(lines[k], "Bank Sta")
                bank_values = [v.strip() for v in bank_str.split(',')]
                if len(bank_values) >= 2:
                    left_bank = float(bank_values[0])
                    right_bank = float(bank_values[1])
                    logger.debug(f"Read bank stations: {left_bank}, {right_bank}")
                    return (left_bank, right_bank)

        return None

    @staticmethod
    def _parse_data_block(lines: List[str], start_idx: int, expected_count: int,
                         column_width: Optional[int] = None,
                         max_lines: Optional[int] = None) -> List[float]:
        """
        Parse fixed-width numeric data block following a count keyword.

        This helper eliminates ~120 lines of duplication across 8 public methods.

        Args:
            lines: File lines (from readlines())
            start_idx: Index to start parsing (typically count_line + 1)
            expected_count: Number of values to read
            column_width: Character width of each column (default: FIXED_WIDTH_COLUMN)
            max_lines: Safety limit on lines to read (default: MAX_PARSE_LINES)

        Returns:
            List of parsed float values

        Example:
            >>> # After finding "#Sta/Elev= 40"
            >>> values = RasGeometry._parse_data_block(lines, count_line_idx + 1, 80)
            >>> # Returns 80 values (40 pairs)
        """
        if column_width is None:
            column_width = RasGeometry.FIXED_WIDTH_COLUMN
        if max_lines is None:
            max_lines = RasGeometry.MAX_PARSE_LINES

        values = []
        line_idx = start_idx

        while len(values) < expected_count and line_idx < len(lines):
            # Stop if hit next keyword
            if lines[line_idx].strip() and lines[line_idx].strip()[0].isupper():
                if '=' in lines[line_idx]:
                    break

            parsed = RasGeometryUtils.parse_fixed_width(lines[line_idx], column_width=column_width)
            values.extend(parsed)
            line_idx += 1

            # Safety check
            if line_idx > start_idx + max_lines:
                logger.warning(f"Exceeded max lines ({max_lines}) while parsing data block")
                break

        return values

    @staticmethod
    def _parse_paired_data(lines: List[str], start_idx: int, count: int,
                          col1_name: str = 'Station',
                          col2_name: str = 'Elevation') -> pd.DataFrame:
        """
        Parse paired data (station/elevation, elevation/volume, etc.) into DataFrame.

        This helper eliminates duplication in 5 public methods.

        Args:
            lines: File lines (from readlines())
            start_idx: Index to start parsing (typically count_line + 1)
            count: Number of PAIRS (not total values)
            col1_name: Name for first column (default: 'Station')
            col2_name: Name for second column (default: 'Elevation')

        Returns:
            DataFrame with two columns

        Example:
            >>> # After finding "#Sta/Elev= 40" (means 40 pairs)
            >>> df = RasGeometry._parse_paired_data(lines, count_line_idx + 1, 40,
            ...                                     'Station', 'Elevation')
            >>> # Returns DataFrame with 40 rows, 2 columns
        """
        total_values = count * 2
        values = RasGeometry._parse_data_block(lines, start_idx, total_values)

        if len(values) != total_values:
            logger.warning(f"Expected {total_values} values, got {len(values)}")

        # Split into pairs
        col1_data = values[0::2]  # Every other value starting at 0
        col2_data = values[1::2]  # Every other value starting at 1

        return pd.DataFrame({col1_name: col1_data, col2_name: col2_data})

    # ========== PUBLIC API METHODS ==========

    @staticmethod
    @log_call
    def get_cross_sections(geom_file: Union[str, Path],
                          river: Optional[str] = None,
                          reach: Optional[str] = None) -> pd.DataFrame:
        """
        Extract cross section metadata from geometry file.

        Parses all cross sections and returns their metadata including
        river, reach, river station, type, and reach lengths.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (Optional[str]): Filter by specific river name. If None, returns all rivers.
            reach (Optional[str]): Filter by specific reach name. If None, returns all reaches.
                                  Note: If reach is specified, river must also be specified.

        Returns:
            pd.DataFrame: DataFrame with columns:
                - River (str): River name
                - Reach (str): Reach name
                - RS (str): River station
                - Type (int): Cross section type (1=natural, etc.)
                - Length_Left (float): Left overbank length to next XS
                - Length_Channel (float): Channel length to next XS
                - Length_Right (float): Right overbank length to next XS
                - NodeName (str): Node name (if specified)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If reach specified without river

        Example:
            >>> # Get all cross sections
            >>> xs_df = RasGeometry.get_cross_sections("BaldEagle.g01")
            >>> print(f"Total XS: {len(xs_df)}")
            >>>
            >>> # Filter by river
            >>> xs_df = RasGeometry.get_cross_sections("BaldEagle.g01", river="Bald Eagle Creek")
            >>>
            >>> # Filter by river and reach
            >>> xs_df = RasGeometry.get_cross_sections("BaldEagle.g01",
            ...                                        river="Bald Eagle Creek",
            ...                                        reach="Reach 1")

        Notes:
            - Cross sections are listed in downstream order within each reach
            - Type codes: 1=natural, others vary by HEC-RAS version
            - Lengths are to the next downstream cross section
            - See 1D_geometry_structure.md Section 4 for format details
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        if reach is not None and river is None:
            raise ValueError("If reach is specified, river must also be specified")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            cross_sections = []
            current_river = None
            current_reach = None

            i = 0
            while i < len(lines):
                line = lines[i].strip()

                # Track current river/reach
                if line.startswith("River Reach="):
                    values = RasGeometryUtils.extract_comma_list(lines[i], "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]
                        logger.debug(f"Parsing {current_river} / {current_reach}")

                # Parse cross section metadata
                elif line.startswith("Type RM Length L Ch R ="):
                    if current_river is None or current_reach is None:
                        logger.warning(f"Found XS without river/reach at line {i}")
                        i += 1
                        continue

                    # Parse the metadata line
                    # Format: "Type RM Length L Ch R = TYPE, RS, Length_L, Length_Ch, Length_R"
                    value_str = RasGeometryUtils.extract_keyword_value(lines[i], "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]

                    if len(values) >= 4:
                        xs_type_code = int(values[0]) if values[0] else 1
                        rs = values[1]  # RS is second value, not first
                        try:
                            node_name = ""

                            # Look ahead for Node Name
                            j = i + 1
                            while j < len(lines) and j < i + 10:  # Look ahead max 10 lines
                                next_line = lines[j].strip()
                                if next_line.startswith("Node Name="):
                                    node_name = RasGeometryUtils.extract_keyword_value(lines[j], "Node Name")
                                if next_line.startswith("Type RM Length") or next_line.startswith("River Reach="):
                                    break
                                j += 1

                            # Use the type code we already extracted
                            xs_type = xs_type_code

                            # Lengths are values[2], values[3], values[4]
                            length_left = float(values[2]) if len(values) > 2 and values[2] else 0.0
                            length_channel = float(values[3]) if len(values) > 3 and values[3] else 0.0
                            length_right = float(values[4]) if len(values) > 4 and values[4] else 0.0

                            # Apply filters
                            if river is not None and current_river != river:
                                i += 1
                                continue
                            if reach is not None and current_reach != reach:
                                i += 1
                                continue

                            cross_sections.append({
                                'River': current_river,
                                'Reach': current_reach,
                                'RS': rs,
                                'Type': xs_type,
                                'Length_Left': length_left,
                                'Length_Channel': length_channel,
                                'Length_Right': length_right,
                                'NodeName': node_name
                            })

                        except (ValueError, IndexError) as e:
                            logger.warning(f"Error parsing XS at line {i}: {e}")

                i += 1

            df = pd.DataFrame(cross_sections)
            logger.info(f"Extracted {len(df)} cross sections from {geom_file.name}")

            if river is not None:
                logger.debug(f"Filtered to river '{river}': {len(df)} cross sections")
            if reach is not None:
                logger.debug(f"Filtered to reach '{reach}': {len(df)} cross sections")

            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error extracting cross sections: {str(e)}")
            raise IOError(f"Failed to extract cross sections: {str(e)}")

    @staticmethod
    @log_call
    def get_station_elevation(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str) -> pd.DataFrame:
        """
        Extract station/elevation pairs for a cross section.

        Reads the cross section geometry data from the plain text geometry file.
        Uses fixed-width parsing (8-character columns) following FORTRAN conventions.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name (case-sensitive)
            reach (str): Reach name (case-sensitive)
            rs (str): River station (as string, e.g., "138154.4")

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station along cross section (ft or m)
                - Elevation (float): Ground elevation at station (ft or m)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If cross section not found

        Example:
            >>> sta_elev = RasGeometry.get_station_elevation(
            ...     "BaldEagle.g01", "Bald Eagle Creek", "Reach 1", "138154.4"
            ... )
            >>> print(f"XS has {len(sta_elev)} points")
            >>> print(f"Station range: {sta_elev['Station'].min():.1f} to {sta_elev['Station'].max():.1f}")
            >>> print(f"Elevation range: {sta_elev['Elevation'].min():.2f} to {sta_elev['Elevation'].max():.2f}")
            >>>
            >>> # Plot cross section
            >>> import matplotlib.pyplot as plt
            >>> plt.plot(sta_elev['Station'], sta_elev['Elevation'])
            >>> plt.xlabel('Station (ft)')
            >>> plt.ylabel('Elevation (ft)')
            >>> plt.title(f'{river} - {reach} - RS {rs}')

        Notes:
            - CRITICAL: Uses 8-character fixed-width parsing (NOT whitespace splitting)
            - Count interpretation: "#Sta/Elev= 40" means 40 PAIRS (80 total values)
            - Values alternate: station1, elev1, station2, elev2, ...
            - Typically 10 values per line (80 characters total)
            - See 1D_geometry_structure.md Section 4.6 for format details
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = RasGeometry._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(
                    f"Cross section not found: {river}/{reach}/RS {rs} in {geom_file.name}"
                )

            # Find #Sta/Elev= line within search range
            for j in range(xs_idx, min(xs_idx + RasGeometry.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Sta/Elev="):
                    # Extract count
                    count_str = RasGeometryUtils.extract_keyword_value(lines[j], "#Sta/Elev")
                    count = int(count_str.strip())

                    logger.debug(f"#Sta/Elev= {count} (means {count} pairs)")

                    # Calculate total values using interpret_count (handles count interpretation)
                    total_values = RasGeometryUtils.interpret_count("#Sta/Elev", count)
                    logger.debug(f"Reading {total_values} total values ({count} pairs)")

                    # Parse paired data using helper
                    df = RasGeometry._parse_paired_data(
                        lines, j + 1, count, 'Station', 'Elevation'
                    )

                    logger.info(
                        f"Extracted {len(df)} station/elevation pairs for "
                        f"{river}/{reach}/RS {rs}"
                    )

                    return df

            # If we get here, #Sta/Elev not found for this XS
            raise ValueError(
                f"#Sta/Elev data not found for {river}/{reach}/RS {rs}"
            )

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading station/elevation: {str(e)}")
            raise IOError(f"Failed to read station/elevation: {str(e)}")

    @staticmethod
    def _interpolate_at_banks(sta_elev_df: pd.DataFrame,
                             bank_left: Optional[float] = None,
                             bank_right: Optional[float] = None) -> pd.DataFrame:
        """
        Interpolate elevation at bank stations and insert into station/elevation data.

        HEC-RAS REQUIRES that bank station values appear as exact points in the
        station/elevation data. This method ensures banks are interpolated and inserted.

        Parameters:
            sta_elev_df (pd.DataFrame): Station/elevation data
            bank_left (Optional[float]): Left bank station
            bank_right (Optional[float]): Right bank station

        Returns:
            pd.DataFrame: Modified DataFrame with banks interpolated and inserted

        Notes:
            - Uses linear interpolation between adjacent points
            - Inserts banks into sorted station list
            - Required for HEC-RAS compatibility
        """
        result_df = sta_elev_df.copy()

        # Interpolate and insert left bank if needed
        if bank_left is not None:
            stations = result_df['Station'].values
            elevations = result_df['Elevation'].values

            if bank_left not in stations:
                # Interpolate elevation at left bank
                bank_left_elev = np.interp(bank_left, stations, elevations)

                # Insert into DataFrame
                new_row = pd.DataFrame({'Station': [bank_left], 'Elevation': [bank_left_elev]})
                result_df = pd.concat([result_df, new_row], ignore_index=True)
                result_df = result_df.sort_values('Station').reset_index(drop=True)

                logger.debug(f"Interpolated left bank at station {bank_left:.2f}, elevation {bank_left_elev:.2f}")

        # Interpolate and insert right bank if needed
        if bank_right is not None:
            stations = result_df['Station'].values
            elevations = result_df['Elevation'].values

            if bank_right not in stations:
                # Interpolate elevation at right bank
                bank_right_elev = np.interp(bank_right, stations, elevations)

                # Insert into DataFrame
                new_row = pd.DataFrame({'Station': [bank_right], 'Elevation': [bank_right_elev]})
                result_df = pd.concat([result_df, new_row], ignore_index=True)
                result_df = result_df.sort_values('Station').reset_index(drop=True)

                logger.debug(f"Interpolated right bank at station {bank_right:.2f}, elevation {bank_right_elev:.2f}")

        return result_df

    @staticmethod
    @log_call
    def set_station_elevation(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str,
                             sta_elev_df: pd.DataFrame,
                             bank_left: Optional[float] = None,
                             bank_right: Optional[float] = None):
        """
        Write station/elevation pairs to a cross section with automatic bank interpolation.

        Modifies the geometry file in-place, replacing the station/elevation data and
        optionally updating bank stations. Creates a .bak backup automatically.

        CRITICAL REQUIREMENTS (HEC-RAS compatibility):
        - Bank stations MUST appear as exact points in station/elevation data
        - This method automatically interpolates elevations at bank locations
        - Maximum 450 points per cross section (HEC-RAS hard limit)

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station
            sta_elev_df (pd.DataFrame): DataFrame with 'Station' and 'Elevation' columns
            bank_left (Optional[float]): Left bank station. If provided, updates bank in file.
                                         If None, reads existing banks and interpolates them.
            bank_right (Optional[float]): Right bank station. If provided, updates bank in file.

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If cross section not found, DataFrame invalid, or >450 points
            IOError: If file write fails

        Example:
            >>> # Simple elevation modification (banks auto-interpolated)
            >>> sta_elev = RasGeometry.get_station_elevation(geom_file, river, reach, rs)
            >>> sta_elev['Elevation'] += 1.0
            >>> RasGeometry.set_station_elevation(geom_file, river, reach, rs, sta_elev)
            >>>
            >>> # Modify geometry AND change bank stations
            >>> sta_elev = RasGeometry.get_station_elevation(geom_file, river, reach, rs)
            >>> RasGeometry.set_station_elevation(geom_file, river, reach, rs, sta_elev,
            ...                                   bank_left=200.0, bank_right=400.0)

        Notes:
            - ALWAYS interpolates elevation at bank stations (HEC-RAS requirement)
            - If banks not provided, reads existing banks from file
            - Validates max 450 points AFTER interpolation
            - Creates .bak backup before modification
            - Formats in 8-char fixed-width, 10 values per line
            - Updates "Bank Sta=" line if new banks provided
            - Stations must be in ascending order
            - Geometry preprocessor must be re-run after modification
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        # Validate DataFrame
        if not isinstance(sta_elev_df, pd.DataFrame):
            raise ValueError("sta_elev_df must be a pandas DataFrame")

        if 'Station' not in sta_elev_df.columns or 'Elevation' not in sta_elev_df.columns:
            raise ValueError("DataFrame must have 'Station' and 'Elevation' columns")

        if len(sta_elev_df) == 0:
            raise ValueError("DataFrame cannot be empty")

        # Validate banks if provided
        if bank_left is not None and bank_right is not None:
            if bank_left >= bank_right:
                raise ValueError(f"Left bank ({bank_left}) must be < right bank ({bank_right})")

        # Validate initial point count (before interpolation)
        if len(sta_elev_df) > RasGeometry.MAX_XS_POINTS:
            raise ValueError(
                f"Cross section has {len(sta_elev_df)} points, exceeds HEC-RAS limit of {RasGeometry.MAX_XS_POINTS} points.\n"
                f"Reduce point count by decimating or simplifying the cross section geometry."
            )

        try:
            # Create backup
            backup_path = RasGeometryUtils.create_backup(geom_file)
            logger.info(f"Created backup: {backup_path}")

            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            i = RasGeometry._find_cross_section(lines, river, reach, rs)

            if i is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            modified_lines = lines.copy()

            # Read existing bank stations if not provided (using helper)
            existing_banks = None
            if bank_left is None or bank_right is None:
                existing_banks = RasGeometry._read_bank_stations(lines, i)

            # Use provided banks or existing banks
            if existing_banks:
                existing_bank_left, existing_bank_right = existing_banks
            else:
                existing_bank_left = existing_bank_right = None

            final_bank_left = bank_left if bank_left is not None else existing_bank_left
            final_bank_right = bank_right if bank_right is not None else existing_bank_right

            # Interpolate at bank stations (HEC-RAS requirement)
            sta_elev_with_banks = RasGeometry._interpolate_at_banks(
                sta_elev_df, final_bank_left, final_bank_right
            )

            # Validate point count AFTER interpolation (HEC-RAS limit)
            if len(sta_elev_with_banks) > RasGeometry.MAX_XS_POINTS:
                raise ValueError(
                    f"Cross section would have {len(sta_elev_with_banks)} points after bank interpolation, "
                    f"exceeds HEC-RAS limit of {RasGeometry.MAX_XS_POINTS} points.\n"
                    f"Original points: {len(sta_elev_df)}, added by interpolation: "
                    f"{len(sta_elev_with_banks) - len(sta_elev_df)}.\n"
                    f"Reduce point count before writing."
                )

            # Validate stations are in ascending order
            if not sta_elev_with_banks['Station'].is_monotonic_increasing:
                raise ValueError("Stations must be in ascending order")

            logger.info(
                f"Prepared geometry: {len(sta_elev_with_banks)} points "
                f"(original: {len(sta_elev_df)}, interpolated: "
                f"{len(sta_elev_with_banks) - len(sta_elev_df)})"
            )

            # Find #Sta/Elev= line
            for j in range(i, min(i + RasGeometry.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Sta/Elev="):
                    # Extract old count
                    old_count_str = RasGeometryUtils.extract_keyword_value(lines[j], "#Sta/Elev")
                    old_count = int(old_count_str.strip())
                    old_total_values = RasGeometryUtils.interpret_count("#Sta/Elev", old_count)

                    # Calculate old data line count
                    old_data_lines = (old_total_values + RasGeometry.VALUES_PER_LINE - 1) // RasGeometry.VALUES_PER_LINE

                    # Prepare new data (using bank-interpolated DataFrame)
                    new_count = len(sta_elev_with_banks)

                    # Interleave station and elevation
                    new_values = []
                    for _, row in sta_elev_with_banks.iterrows():
                        new_values.append(row['Station'])
                        new_values.append(row['Elevation'])

                    # Format new data lines using constants
                    new_data_lines = RasGeometryUtils.format_fixed_width(
                        new_values,
                        column_width=RasGeometry.FIXED_WIDTH_COLUMN,
                        values_per_line=RasGeometry.VALUES_PER_LINE,
                        precision=2
                    )

                    # Update count line
                    modified_lines[j] = f"#Sta/Elev= {new_count}\n"

                    # Replace data lines
                    # Remove old data lines
                    for k in range(old_data_lines):
                        if j + 1 + k < len(modified_lines):
                            modified_lines[j + 1 + k] = None  # Mark for deletion

                    # Insert new data lines
                    for k, data_line in enumerate(new_data_lines):
                        if j + 1 + k < len(modified_lines):
                            modified_lines[j + 1 + k] = data_line
                        else:
                            # Append if needed
                            modified_lines.append(data_line)

                    # Clean up None entries
                    modified_lines = [line for line in modified_lines if line is not None]

                    # Update Bank Sta= line if new banks provided
                    if bank_left is not None and bank_right is not None:
                        # Find Bank Sta= line in the modified lines
                        bank_sta_updated = False
                        for k in range(i, min(i + RasGeometry.DEFAULT_SEARCH_RANGE, len(modified_lines))):
                            if modified_lines[k].startswith("Bank Sta="):
                                # Update with new bank stations (format: no spaces after comma)
                                modified_lines[k] = f"Bank Sta={bank_left:g},{bank_right:g}\n"
                                bank_sta_updated = True
                                logger.debug(f"Updated Bank Sta= line: {bank_left:g},{bank_right:g}")
                                break

                        if not bank_sta_updated:
                            logger.warning(f"Bank Sta= line not found for XS {rs}, banks not updated in file")

                    # Write modified file
                    with open(geom_file, 'w') as f:
                        f.writelines(modified_lines)

                    logger.info(
                        f"Updated station/elevation for {river}/{reach}/RS {rs}: "
                        f"{new_count} pairs written"
                    )

                    if bank_left is not None and bank_right is not None:
                        logger.info(f"Updated bank stations: {bank_left:g}, {bank_right:g}")

                    return

            raise ValueError(
                f"#Sta/Elev data not found for {river}/{reach}/RS {rs}"
            )

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error writing station/elevation: {str(e)}")
            # Attempt to restore from backup if write failed
            if backup_path and backup_path.exists():
                logger.info(f"Restoring from backup: {backup_path}")
                import shutil
                shutil.copy2(backup_path, geom_file)
            raise IOError(f"Failed to write station/elevation: {str(e)}")

    @staticmethod
    @log_call
    def get_storage_areas(geom_file: Union[str, Path],
                         exclude_2d: bool = True) -> List[str]:
        """
        Extract list of storage area names from geometry file.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            exclude_2d (bool, optional): If True, exclude 2D flow areas (Storage Area Is2D=-1).
                                        If False, include all storage areas. Defaults to True.

        Returns:
            List[str]: List of storage area names

        Example:
            >>> storage_areas = RasGeometry.get_storage_areas("BaldEagleDamBrk.g01")
            >>> print(f"Found {len(storage_areas)} storage areas")
            >>> print(storage_areas)
            ['Res Pool 1', 'Res Pool 2']

        Notes:
            - Storage areas can be traditional elevation-volume storage (Type 1)
            - Or 2D flow areas with "Storage Area Is2D=-1" flag
            - Use exclude_2d=False to include 2D flow areas in the list
            - See dam_break_structure.md for format details
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            storage_areas = []
            current_storage_name = None
            is_2d = None

            for line in lines:
                # Find storage area definition
                if line.startswith("Storage Area="):
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Storage Area")
                    # Storage Area format: Name,X,Y - extract just the name
                    values = [v.strip() for v in value_str.split(',')]
                    current_storage_name = values[0] if values else value_str
                    is_2d = None  # Reset for new storage area

                # Check if it's a 2D flow area
                elif line.startswith("Storage Area Is2D=") and current_storage_name:
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Storage Area Is2D")
                    is_2d_value = int(value_str.strip())

                    # Add to list based on filter
                    if exclude_2d:
                        if is_2d_value != -1:  # Not a 2D flow area
                            storage_areas.append(current_storage_name)
                    else:
                        storage_areas.append(current_storage_name)

                    current_storage_name = None  # Reset

            logger.info(f"Found {len(storage_areas)} storage areas in {geom_file.name}")

            return storage_areas

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error extracting storage areas: {str(e)}")
            raise IOError(f"Failed to extract storage areas: {str(e)}")

    @staticmethod
    @log_call
    def get_storage_elevation_volume(geom_file: Union[str, Path],
                                     area_name: str) -> pd.DataFrame:
        """
        Extract storage area elevation-volume curve.

        Reads the elevation-volume relationship for a storage area, which defines
        how much volume is stored at each elevation.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            area_name (str): Storage area name (case-sensitive)

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Elevation (float): Water surface elevation (ft or m)
                - Volume (float): Storage volume (cu ft or cu m)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If storage area not found or is a 2D flow area

        Example:
            >>> elev_vol = RasGeometry.get_storage_elevation_volume(
            ...     "BaldEagleDamBrk.g01", "Res Pool 1"
            ... )
            >>> print(f"Storage curve has {len(elev_vol)} points")
            >>> print(f"Volume range: {elev_vol['Volume'].min():.0f} to {elev_vol['Volume'].max():.0f} cu ft")
            >>>
            >>> # Plot storage curve
            >>> import matplotlib.pyplot as plt
            >>> plt.plot(elev_vol['Volume'], elev_vol['Elevation'])
            >>> plt.xlabel('Volume (cu ft)')
            >>> plt.ylabel('Elevation (ft)')
            >>> plt.title(f'Storage Area: {area_name}')

        Notes:
            - Uses 8-char fixed-width parsing for elevation/volume pairs
            - Count interpretation: "Storage Area Elev Volume= 53" means 53 PAIRS (106 values)
            - Only works for Type 1 storage areas (not 2D flow areas)
            - See dam_break_structure.md Section 2 for format details
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the storage area
            found_storage = False
            in_target_storage = False

            for i, line in enumerate(lines):
                # Find storage area by name
                if line.startswith("Storage Area="):
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Storage Area")
                    # Storage Area format: Name,X,Y - extract just the name
                    values = [v.strip() for v in value_str.split(',')]
                    current_name = values[0] if values else value_str
                    in_target_storage = (current_name == area_name)

                    if in_target_storage:
                        found_storage = True
                        logger.debug(f"Found storage area '{area_name}' at line {i}")

                # Check if it's a 2D flow area (can't extract elev-volume for 2D)
                elif line.startswith("Storage Area Is2D=") and in_target_storage:
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Storage Area Is2D")
                    if int(value_str.strip()) == -1:
                        raise ValueError(
                            f"Storage area '{area_name}' is a 2D flow area. "
                            f"Use get_2d_perimeter() instead."
                        )

                # Find elevation-volume data (keyword is "Vol Elev" not "Elev Volume")
                elif line.startswith("Storage Area Vol Elev=") and in_target_storage:
                    # Extract count
                    count_str = RasGeometryUtils.extract_keyword_value(line, "Storage Area Vol Elev")
                    count = int(count_str.strip())

                    logger.debug(f"Storage Area Vol Elev= {count} (means {count} pairs)")

                    # Parse elevation/volume data using helper
                    df = RasGeometry._parse_paired_data(
                        lines, i + 1, count, 'Elevation', 'Volume'
                    )

                    logger.info(f"Extracted {len(df)} elevation/volume pairs for '{area_name}'")

                    return df

            if not found_storage:
                raise ValueError(f"Storage area not found: {area_name}")

            # If we found the storage area but no elev-volume data
            raise ValueError(f"No elevation-volume data found for storage area: {area_name}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading storage elevation-volume: {str(e)}")
            raise IOError(f"Failed to read storage elevation-volume: {str(e)}")

    @staticmethod
    @log_call
    def get_lateral_structures(geom_file: Union[str, Path],
                               river: Optional[str] = None,
                               reach: Optional[str] = None) -> pd.DataFrame:
        """
        Extract lateral structure definitions from geometry file.

        Lateral structures are weirs, culverts, or other hydraulic structures along
        the side of a channel that connect to adjacent areas (detention basins,
        floodplains, irrigation ditches, etc.).

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (Optional[str]): Filter by specific river. If None, returns all rivers.
            reach (Optional[str]): Filter by specific reach. If None, returns all reaches.

        Returns:
            pd.DataFrame: DataFrame with columns:
                - River (str): River name
                - Reach (str): Reach name
                - RS (str): River station where lateral structure is located
                - Position (int): Lateral position (0, 1, 2...)
                - Distance (float): Distance from XS (ft or m)
                - Width (float): Weir width (ft or m)
                - Coefficient (float): Weir discharge coefficient
                - Type (int): Lateral weir type code
                - SE_Count (int): Number of station-elevation pairs in profile
                - HW_RS (str): Headwater river station
                - HW_Distance (float): Headwater distance
                - Description (str): XS description (often describes the lateral)

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> lat_strucs = RasGeometry.get_lateral_structures("A100_00_00.g08")
            >>> print(f"Found {len(lat_strucs)} lateral structures")
            >>> print(lat_strucs[['River', 'Reach', 'RS', 'Distance', 'Width']])

        Notes:
            - Lateral structures appear as keywords after cross section definitions
            - Station-elevation profile follows "Lateral Weir SE=" keyword
            - Uses 8-char fixed-width parsing for profiles
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            lateral_structures = []
            current_river = None
            current_reach = None
            current_rs = None
            current_description = ""
            in_lateral = False
            lateral_data = {}

            i = 0
            while i < len(lines):
                line = lines[i].strip()

                # Track current river/reach
                if line.startswith("River Reach="):
                    values = RasGeometryUtils.extract_comma_list(lines[i], "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                # Track current cross section
                elif line.startswith("Type RM Length L Ch R ="):
                    value_str = RasGeometryUtils.extract_keyword_value(lines[i], "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]
                    # Format: Type, RS, Length_L, Length_Ch, Length_R
                    # values[0] = Type, values[1] = RS
                    if len(values) > 1:
                        current_rs = values[1]
                    elif len(values) > 0:
                        current_rs = values[0]

                # Track description
                elif line.startswith("BEGIN DESCRIPTION:"):
                    current_description = ""
                    j = i + 1
                    while j < len(lines) and not lines[j].strip().startswith("END DESCRIPTION"):
                        current_description += lines[j].strip() + " "
                        j += 1
                    current_description = current_description.strip()

                # Detect lateral weir start
                elif line.startswith("Lateral Weir Pos="):
                    # If already in lateral, save the previous one first
                    if in_lateral and lateral_data:
                        if river is None or lateral_data['River'] == river:
                            if reach is None or lateral_data['Reach'] == reach:
                                lateral_structures.append(lateral_data)

                    # Start new lateral
                    in_lateral = True
                    lateral_data = {
                        'River': current_river,
                        'Reach': current_reach,
                        'RS': current_rs,
                        'Description': current_description,
                        'Position': int(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir Pos"))
                    }
                    logger.debug(f"Started new lateral: {current_river}/{current_reach}/RS {current_rs}, line {i}")

                # Extract lateral weir parameters
                elif in_lateral:
                    # Check if starting a new lateral while already in one
                    if line.startswith("Lateral Weir Pos="):
                        print(f"DEBUG: Line {i} - New Lateral while in_lateral, saving previous")  # TEMP DEBUG
                        # Save previous lateral
                        if lateral_data:
                            print(f"DEBUG: lateral_data exists, checking filters")  # TEMP DEBUG
                            if river is None or lateral_data['River'] == river:
                                if reach is None or lateral_data['Reach'] == reach:
                                    lateral_structures.append(lateral_data)
                                    print(f"DEBUG: APPENDED! Total now: {len(lateral_structures)}")  # TEMP DEBUG

                        # Start new lateral
                        lateral_data = {
                            'River': current_river,
                            'Reach': current_reach,
                            'RS': current_rs,
                            'Description': current_description,
                            'Position': int(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir Pos"))
                        }

                    elif line.startswith("Lateral Weir Distance="):
                        lateral_data['Distance'] = float(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir Distance"))

                    elif line.startswith("Lateral Weir WD="):
                        lateral_data['Width'] = float(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir WD"))

                    elif line.startswith("Lateral Weir Coef="):
                        lateral_data['Coefficient'] = float(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir Coef"))

                    elif line.startswith("Lateral Weir Type="):
                        lateral_data['Type'] = int(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir Type"))

                    elif line.startswith("Lateral Weir SE="):
                        lateral_data['SE_Count'] = int(RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir SE"))

                    elif line.startswith("Lateral Weir HW RS Station="):
                        value_str = RasGeometryUtils.extract_keyword_value(lines[i], "Lateral Weir HW RS Station")
                        values = [v.strip() for v in value_str.split(',')]
                        lateral_data['HW_RS'] = values[0] if len(values) > 0 else ""
                        lateral_data['HW_Distance'] = float(values[1]) if len(values) > 1 and values[1] else 0.0

                    # End of lateral weir section
                    elif line.startswith("Type RM Length") or line.startswith("River Reach="):
                        # Save this lateral structure
                        if lateral_data:
                            # Apply filters
                            if river is None or lateral_data['River'] == river:
                                if reach is None or lateral_data['Reach'] == reach:
                                    lateral_structures.append(lateral_data)

                        in_lateral = False
                        lateral_data = {}
                        # Don't increment i, re-process this line
                        continue

                i += 1

            # Handle last lateral if file ends while in lateral section
            if in_lateral and lateral_data:
                if river is None or lateral_data['River'] == river:
                    if reach is None or lateral_data['Reach'] == reach:
                        lateral_structures.append(lateral_data)

            df = pd.DataFrame(lateral_structures)
            logger.info(f"Extracted {len(df)} lateral structures from {geom_file.name}")

            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error extracting lateral structures: {str(e)}")
            raise IOError(f"Failed to extract lateral structures: {str(e)}")

    @staticmethod
    @log_call
    def get_lateral_weir_profile(geom_file: Union[str, Path],
                                  river: str,
                                  reach: str,
                                  rs: str,
                                  position: int = 0) -> pd.DataFrame:
        """
        Extract lateral weir station-elevation profile.

        Reads the weir crest profile which defines the spillway geometry.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station of cross section with lateral weir
            position (int, optional): Lateral weir position if multiple at same XS. Defaults to 0.

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station along weir (ft or m)
                - Elevation (float): Weir crest elevation at station (ft or m)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If lateral weir not found

        Example:
            >>> profile = RasGeometry.get_lateral_weir_profile(
            ...     "A100_00_00.g08", "A100", "A100", "16473", position=0
            ... )
            >>> print(f"Weir profile has {len(profile)} points")
            >>>
            >>> # Plot weir profile
            >>> import matplotlib.pyplot as plt
            >>> plt.plot(profile['Station'], profile['Elevation'])
            >>> plt.xlabel('Station (ft)')
            >>> plt.ylabel('Elevation (ft)')
            >>> plt.title('Lateral Weir Crest Profile')

        Notes:
            - Uses 8-char fixed-width parsing
            - Count from "Lateral Weir SE=" indicates number of pairs
            - Profile data appears after SE count line
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the lateral weir
            current_river = None
            current_reach = None
            current_rs = None
            in_target_lateral = False
            found_lateral = False

            for i, line in enumerate(lines):
                # Track current river/reach
                if line.startswith("River Reach="):
                    values = RasGeometryUtils.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                # Track current cross section
                elif line.startswith("Type RM Length L Ch R ="):
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]
                    # Format: Type, RS, Length_L, Length_Ch, Length_R
                    # values[0] = Type, values[1] = RS
                    if len(values) > 1:
                        current_rs = values[1]
                    elif len(values) > 0:
                        current_rs = values[0]

                # Check for matching lateral weir
                elif line.startswith("Lateral Weir Pos="):
                    pos = int(RasGeometryUtils.extract_keyword_value(line, "Lateral Weir Pos"))

                    if (current_river == river and
                        current_reach == reach and
                        current_rs == rs and
                        pos == position):
                        in_target_lateral = True
                        found_lateral = True
                        logger.debug(f"Found lateral weir at {river}/{reach}/RS {rs}, pos {position}")

                # Extract station-elevation data
                elif line.startswith("Lateral Weir SE=") and in_target_lateral:
                    # Extract count
                    count_str = RasGeometryUtils.extract_keyword_value(line, "Lateral Weir SE")
                    count = int(count_str.strip())

                    logger.debug(f"Lateral Weir SE= {count} (means {count} pairs)")

                    # Parse station/elevation data using helper (note: max_lines=20 for lateral weirs)
                    df = RasGeometry._parse_paired_data(
                        lines, i + 1, count, 'Station', 'Elevation'
                    )

                    logger.info(
                        f"Extracted {len(df)} station/elevation pairs for "
                        f"lateral weir at {river}/{reach}/RS {rs}, pos {position}"
                    )

                    return df

                # End of lateral weir section
                elif line.startswith("Type RM Length") and in_target_lateral:
                    in_target_lateral = False

            if not found_lateral:
                raise ValueError(
                    f"Lateral weir not found: {river}/{reach}/RS {rs}, position {position}"
                )

            # If found but no SE data
            raise ValueError(
                f"No station-elevation profile found for lateral weir at "
                f"{river}/{reach}/RS {rs}, position {position}"
            )

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading lateral weir profile: {str(e)}")
            raise IOError(f"Failed to read lateral weir profile: {str(e)}")

    @staticmethod
    @log_call
    def get_connections(geom_file: Union[str, Path]) -> pd.DataFrame:
        """
        Extract all SA/2D area connection definitions.

        Connections link storage areas to 2D flow areas (or 2D to 2D) for
        dam breach modeling, levee overtopping, or floodplain connectivity.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Connection_Name (str): Connection name
                - Upstream_Area (str): Upstream storage/2D area name
                - Downstream_Area (str): Downstream storage/2D area name
                - Weir_Width (float): Weir width (ft or m)
                - Weir_Coefficient (float): Weir discharge coefficient
                - SE_Count (int): Number of station-elevation pairs in weir profile
                - Num_Gates (int): Number of gates in connection
                - Routing_Type (int): Connection routing type

        Example:
            >>> connections = RasGeometry.get_connections("BaldEagleDamBrk.g01")
            >>> print(f"Found {len(connections)} connections")
            >>> print(connections[['Connection_Name', 'Upstream_Area', 'Downstream_Area']])

        Notes:
            - Connections defined with "Connection=" keyword
            - Format: Connection=Name,X,Y
            - See dam_break_structure.md for complete format details
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            connections = []
            in_connection = False
            conn_data = {}

            for i, line in enumerate(lines):
                # Detect connection start
                if line.startswith("Connection="):
                    # Save previous connection if exists
                    if in_connection and conn_data:
                        connections.append(conn_data)

                    in_connection = True
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Connection")
                    # Format: Name,X,Y
                    values = [v.strip() for v in value_str.split(',')]
                    conn_name = values[0] if values else value_str

                    conn_data = {'Connection_Name': conn_name}

                # Extract connection parameters
                elif in_connection:
                    if line.startswith("Connection Up SA="):
                        conn_data['Upstream_Area'] = RasGeometryUtils.extract_keyword_value(line, "Connection Up SA").strip()

                    elif line.startswith("Connection Dn SA="):
                        conn_data['Downstream_Area'] = RasGeometryUtils.extract_keyword_value(line, "Connection Dn SA").strip()

                    elif line.startswith("Conn Routing Type="):
                        conn_data['Routing_Type'] = int(RasGeometryUtils.extract_keyword_value(line, "Conn Routing Type"))

                    elif line.startswith("Conn Weir WD="):
                        conn_data['Weir_Width'] = float(RasGeometryUtils.extract_keyword_value(line, "Conn Weir WD"))

                    elif line.startswith("Conn Weir Coef="):
                        conn_data['Weir_Coefficient'] = float(RasGeometryUtils.extract_keyword_value(line, "Conn Weir Coef"))

                    elif line.startswith("Conn Weir SE="):
                        conn_data['SE_Count'] = int(RasGeometryUtils.extract_keyword_value(line, "Conn Weir SE"))

                    # Count gates
                    elif line.startswith("Conn Gate Name"):
                        # Count gate lines (lines starting with "Gate #")
                        num_gates = 0
                        j = i + 1
                        while j < len(lines) and lines[j].startswith("Gate #"):
                            num_gates += 1
                            j += 1
                        conn_data['Num_Gates'] = num_gates

                    # End of connection when hitting storage area
                    elif line.startswith("Storage Area="):
                        if conn_data:
                            connections.append(conn_data)
                        in_connection = False
                        conn_data = {}
                        continue  # Don't increment i

            # Handle last connection if file ends
            if in_connection and conn_data:
                connections.append(conn_data)

            df = pd.DataFrame(connections)

            # Ensure Num_Gates column exists
            if 'Num_Gates' not in df.columns:
                df['Num_Gates'] = 0

            logger.info(f"Extracted {len(df)} connections from {geom_file.name}")

            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error extracting connections: {str(e)}")
            raise IOError(f"Failed to extract connections: {str(e)}")

    @staticmethod
    @log_call
    def get_connection_weir_profile(geom_file: Union[str, Path],
                                    connection_name: str) -> pd.DataFrame:
        """
        Extract weir/dam crest station-elevation profile for a connection.

        Reads the weir crest geometry which defines the spillway or dam crest elevation
        as a function of station along the connection.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            connection_name (str): Connection name (case-sensitive)

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station along weir/dam crest (ft or m)
                - Elevation (float): Crest elevation at station (ft or m)

        Example:
            >>> profile = RasGeometry.get_connection_weir_profile(
            ...     "BaldEagleDamBrk.g01", "Dam"
            ... )
            >>> print(f"Dam crest has {len(profile)} station/elevation points")
            >>> plt.plot(profile['Station'], profile['Elevation'])

        Notes:
            - Uses 8-char fixed-width parsing
            - Count from "Conn Weir SE=" indicates number of pairs
            - Profile data follows SE count line
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the connection
            in_target_conn = False
            found_conn = False

            for i, line in enumerate(lines):
                # Find connection by name
                if line.startswith("Connection="):
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Connection")
                    values = [v.strip() for v in value_str.split(',')]
                    current_name = values[0] if values else value_str

                    in_target_conn = (current_name == connection_name)
                    if in_target_conn:
                        found_conn = True
                        logger.debug(f"Found connection '{connection_name}' at line {i}")

                # Extract weir profile
                elif line.startswith("Conn Weir SE=") and in_target_conn:
                    count_str = RasGeometryUtils.extract_keyword_value(line, "Conn Weir SE")
                    count = int(count_str.strip())

                    logger.debug(f"Conn Weir SE= {count} (means {count} pairs)")

                    # Parse station/elevation data using helper
                    df = RasGeometry._parse_paired_data(
                        lines, i + 1, count, 'Station', 'Elevation'
                    )

                    logger.info(
                        f"Extracted {len(df)} station/elevation pairs for "
                        f"connection '{connection_name}'"
                    )

                    return df

                # End of connection
                elif line.startswith("Connection=") and in_target_conn:
                    in_target_conn = False

            if not found_conn:
                raise ValueError(f"Connection not found: {connection_name}")

            # If found but no SE data
            raise ValueError(f"No weir profile found for connection: {connection_name}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading connection weir profile: {str(e)}")
            raise IOError(f"Failed to read connection weir profile: {str(e)}")

    @staticmethod
    @log_call
    def get_connection_gates(geom_file: Union[str, Path],
                            connection_name: str) -> pd.DataFrame:
        """
        Extract gate definitions for a connection.

        Reads gate parameters in CSV format (23+ fields per gate).

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            connection_name (str): Connection name

        Returns:
            pd.DataFrame: DataFrame with gate parameters (columns vary by gate data)
                Typical columns include:
                - Gate_Name, Width, Height, Invert, Gate_Coefficient,
                - Expansion_T, Expansion_O, Expansion_H, Type, Weir_Coefficient,
                - Is_Ogee, Spill_Height, Design_Head, Num_Openings, etc.

        Example:
            >>> gates = RasGeometry.get_connection_gates("BaldEagleDamBrk.g01", "Dam")
            >>> if not gates.empty:
            ...     print(gates[['Gate_Name', 'Width', 'Height', 'Invert']])

        Notes:
            - Gate data is CSV format with 23+ fields
            - Header line: "Conn Gate Name Wd,H,Inv,GCoef,..."
            - Data lines: "Gate #1     ,7,15,590,..."
            - Returns empty DataFrame if no gates
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the connection
            in_target_conn = False
            gates = []

            for i, line in enumerate(lines):
                # Find connection by name
                if line.startswith("Connection="):
                    value_str = RasGeometryUtils.extract_keyword_value(line, "Connection")
                    values = [v.strip() for v in value_str.split(',')]
                    current_name = values[0] if values else value_str

                    in_target_conn = (current_name == connection_name)

                # Find gate definitions
                elif line.startswith("Conn Gate Name") and in_target_conn:
                    # Next lines contain gate data
                    j = i + 1
                    while j < len(lines) and lines[j].startswith("Gate #"):
                        gate_line = lines[j].strip()
                        # Parse CSV
                        gate_values = [v.strip() for v in gate_line.split(',')]

                        # Build gate dict (using simplified field names)
                        gate_dict = {
                            'Gate_Name': gate_values[0] if len(gate_values) > 0 else "",
                            'Width': float(gate_values[1]) if len(gate_values) > 1 and gate_values[1] else 0.0,
                            'Height': float(gate_values[2]) if len(gate_values) > 2 and gate_values[2] else 0.0,
                            'Invert': float(gate_values[3]) if len(gate_values) > 3 and gate_values[3] else 0.0,
                            'Gate_Coefficient': float(gate_values[4]) if len(gate_values) > 4 and gate_values[4] else 0.0,
                        }

                        # Add remaining fields as generic Gate_Param_N
                        for idx in range(5, len(gate_values)):
                            if gate_values[idx]:
                                gate_dict[f'Gate_Param_{idx}'] = gate_values[idx]

                        gates.append(gate_dict)
                        j += 1

                    break  # Found gates, stop searching

                # End of connection
                elif line.startswith("Connection=") and in_target_conn:
                    break

            df = pd.DataFrame(gates)

            if len(df) > 0:
                logger.info(f"Extracted {len(df)} gates for connection '{connection_name}'")
            else:
                logger.info(f"No gates found for connection '{connection_name}'")

            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading connection gates: {str(e)}")
            raise IOError(f"Failed to read connection gates: {str(e)}")

    @staticmethod
    @log_call
    def get_bank_stations(geom_file: Union[str, Path],
                         river: str,
                         reach: str,
                         rs: str) -> Optional[Tuple[float, float]]:
        """
        Extract left and right bank station locations for a cross section.

        Bank stations define the boundary between overbank areas and the main channel,
        used for subsection conveyance calculations.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            Optional[Tuple[float, float]]: (left_bank, right_bank) or None if no banks defined

        Example:
            >>> banks = RasGeometry.get_bank_stations("BaldEagle.g01", "Bald Eagle", "Loc Hav", "138154.4")
            >>> if banks:
            ...     left, right = banks
            ...     print(f"Bank stations: Left={left}, Right={right}")
            ...     print(f"Main channel width: {right - left} ft")

        Notes:
            - Format: Bank Sta=<left>,<right> (CSV, no spaces)
            - Returns None for cross sections without banks (bridges, inline structures)
            - ~10% of cross sections don't have bank stations
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = RasGeometry._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            # Read bank stations using helper
            banks = RasGeometry._read_bank_stations(lines, xs_idx)

            if banks:
                left_bank, right_bank = banks
                logger.info(f"Extracted bank stations for {river}/{reach}/RS {rs}: {left_bank}, {right_bank}")
                return banks
            else:
                logger.info(f"No bank stations found for {river}/{reach}/RS {rs}")
                return None

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bank stations: {str(e)}")
            raise IOError(f"Failed to read bank stations: {str(e)}")

    @staticmethod
    @log_call
    def get_expansion_contraction(geom_file: Union[str, Path],
                                  river: str,
                                  reach: str,
                                  rs: str) -> Tuple[float, float]:
        """
        Extract expansion and contraction coefficients for a cross section.

        These coefficients account for energy losses due to flow expansion
        (downstream) and contraction (upstream) at cross sections.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            Tuple[float, float]: (expansion, contraction) coefficients

        Example:
            >>> exp, cntr = RasGeometry.get_expansion_contraction(
            ...     "BaldEagle.g01", "Bald Eagle", "Loc Hav", "138154.4"
            ... )
            >>> print(f"Expansion: {exp}, Contraction: {cntr}")
            >>> # Typical values: expansion=0.3, contraction=0.1

        Notes:
            - Format: Exp/Cntr=<expansion>,<contraction> (CSV, no spaces)
            - Default values if not specified: 0.3 (expansion), 0.1 (contraction)
            - Used in energy loss calculations between cross sections
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = RasGeometry._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            # Find Exp/Cntr= line within search range
            for j in range(xs_idx, min(xs_idx + RasGeometry.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("Exp/Cntr="):
                    exp_cntr_str = RasGeometryUtils.extract_keyword_value(lines[j], "Exp/Cntr")
                    values = [v.strip() for v in exp_cntr_str.split(',')]

                    if len(values) >= 2:
                        expansion = float(values[0])
                        contraction = float(values[1])

                        logger.info(
                            f"Extracted expansion/contraction for {river}/{reach}/RS {rs}: "
                            f"{expansion}, {contraction}"
                        )
                        return (expansion, contraction)

            # XS found but no Exp/Cntr= (use defaults)
            logger.info(f"No Exp/Cntr found for {river}/{reach}/RS {rs}, using defaults")
            return (0.3, 0.1)  # HEC-RAS defaults

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading expansion/contraction: {str(e)}")
            raise IOError(f"Failed to read expansion/contraction: {str(e)}")

    @staticmethod
    @log_call
    def get_mannings_n(geom_file: Union[str, Path],
                      river: str,
                      reach: str,
                      rs: str) -> pd.DataFrame:
        """
        Extract Manning's n roughness values for a cross section.

        Manning's n values define channel roughness and are organized by subsections
        (Left Overbank, Main Channel, Right Overbank) based on bank station locations.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station where this Manning's n value starts
                - n_value (float): Manning's roughness coefficient
                - Subsection (str): 'LOB' (Left Overbank), 'Channel', or 'ROB' (Right Overbank)

        Example:
            >>> mann = RasGeometry.get_mannings_n("BaldEagle.g01", "Bald Eagle", "Loc Hav", "138154.4")
            >>> print(mann)
               Station  n_value Subsection
            0      0.0     0.06        LOB
            1    190.0     0.04    Channel
            2    375.0     0.10        ROB
            >>>
            >>> # Calculate average channel Manning's n
            >>> channel_n = mann[mann['Subsection'] == 'Channel']['n_value'].mean()

        Notes:
            - Format: #Mann= <count> , 0 , 0 (standard 3-segment L-MC-R)
            - Data: Triplets (station, n_value, 0) in 8-char fixed-width
            - Subsection classification uses bank stations
            - Supports standard format (3 segments) and variable segments
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = RasGeometry._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            # Get bank stations using helper (for subsection classification)
            banks = RasGeometry._read_bank_stations(lines, xs_idx)
            bank_left = bank_right = None
            if banks:
                bank_left, bank_right = banks

            # Find #Mann= line
            for j in range(xs_idx, min(xs_idx + RasGeometry.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Mann="):
                    # Extract count
                    mann_str = RasGeometryUtils.extract_keyword_value(lines[j], "#Mann")
                    count_values = [v.strip() for v in mann_str.split(',')]

                    num_segments = int(count_values[0]) if count_values[0] else 0
                    format_flag = int(count_values[1]) if len(count_values) > 1 and count_values[1] else 0

                    logger.debug(f"Manning's n: {num_segments} segments, format={format_flag}")

                    # Calculate total values to read (triplets)
                    total_values = num_segments * 3

                    # Parse Manning's n data using helper (note: max_lines=20 for Manning's n)
                    values = RasGeometry._parse_data_block(
                        lines, j + 1, total_values,
                        column_width=RasGeometry.FIXED_WIDTH_COLUMN,
                        max_lines=20
                    )

                    # Convert triplets to DataFrame
                    segments = []
                    for seg_idx in range(0, len(values), 3):
                        if seg_idx + 2 < len(values):
                            station = values[seg_idx]
                            n_value = values[seg_idx + 1]
                            # values[seg_idx + 2] is always 0, ignore

                            # Classify subsection based on bank stations
                            if bank_left is not None and bank_right is not None:
                                if station < bank_left:
                                    subsection = 'LOB'
                                elif station < bank_right:
                                    subsection = 'Channel'
                                else:
                                    subsection = 'ROB'
                            else:
                                subsection = 'Unknown'

                            segments.append({
                                'Station': station,
                                'n_value': n_value,
                                'Subsection': subsection
                            })

                    df = pd.DataFrame(segments)

                    logger.info(
                        f"Extracted {len(df)} Manning's n segments for {river}/{reach}/RS {rs}"
                    )

                    return df

            # XS found but no Manning's n
            raise ValueError(f"No Manning's n data found for {river}/{reach}/RS {rs}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading Manning's n: {str(e)}")
            raise IOError(f"Failed to read Manning's n: {str(e)}")

==================================================

File: c:\GH\ras-commander\ras_commander\RasGeometryUtils.py
==================================================
"""
RasGeometryUtils - Utility functions for parsing HEC-RAS geometry files

This module provides reusable utility functions for parsing and manipulating
HEC-RAS geometry files. These utilities handle FORTRAN-era fixed-width formats,
count interpretation, section identification, and file manipulation.

All methods are static and designed to be used without instantiation.

List of Functions:
- parse_fixed_width() - Parse fixed-width numeric data (8 or 16 char columns)
- format_fixed_width() - Format values into fixed-width lines
- interpret_count() - Interpret count declarations based on context
- identify_section() - Find section boundaries by keyword marker
- extract_keyword_value() - Extract value following keyword
- extract_comma_list() - Extract comma-separated list
- create_backup() - Create .bak backup before modification
- validate_river_reach_rs() - Validate river/reach/RS exists

Example Usage:
    >>> from ras_commander import RasGeometryUtils
    >>> # Parse fixed-width line (8-char columns)
    >>> line = "       0  963.04    27.2  963.04"
    >>> values = RasGeometryUtils.parse_fixed_width(line, column_width=8)
    >>> print(values)
    [0.0, 963.04, 27.2, 963.04]

    >>> # Interpret count declaration
    >>> total_values = RasGeometryUtils.interpret_count("#Sta/Elev", 40)
    >>> print(f"40 pairs = {total_values} total values")
    40 pairs = 80 total values
"""

import re
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
from datetime import datetime

from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)


class RasGeometryUtils:
    """
    Utility functions for parsing HEC-RAS geometry files.

    All methods are static and designed to be used without instantiation.
    """

    @staticmethod
    def parse_fixed_width(line: str, column_width: int = 8) -> List[float]:
        """
        Parse fixed-width numeric data from a line.

        HEC-RAS uses FORTRAN-era fixed-width columns for numeric data:
        - 8-character columns: Station/elevation, Manning's n, elevation-volume
        - 16-character columns: 2D coordinates (X, Y pairs)

        Values are right-aligned and left-padded with spaces within each column.
        This function MUST parse by column position, NOT by whitespace splitting.

        Parameters:
            line (str): Line containing fixed-width values
            column_width (int): Width of each column in characters. Defaults to 8.
                               Use 16 for 2D coordinate data.

        Returns:
            List[float]: Parsed numeric values

        Raises:
            ValueError: If a column contains non-numeric data that can't be parsed

        Example:
            >>> # 8-character columns (station/elevation)
            >>> line = "       0  963.04    27.2  963.04   32.64  963.02"
            >>> values = RasGeometryUtils.parse_fixed_width(line, 8)
            >>> print(values)
            [0.0, 963.04, 27.2, 963.04, 32.64, 963.02]

            >>> # 16-character columns (2D coordinates)
            >>> line = "   648224.43125   4551425.84375   648229.43125   4551425.84375"
            >>> coords = RasGeometryUtils.parse_fixed_width(line, 16)
            >>> print(coords)
            [648224.43125, 4551425.84375, 648229.43125, 4551425.84375]

        Notes:
            - Based on successful RasUnsteady.parse_fixed_width_table() pattern
            - Handles merged values (adjacent numbers without spaces) using regex
            - Skips empty columns
            - Strips line before parsing to remove trailing newlines
        """
        values = []
        line_stripped = line.rstrip('\n\r')

        # Parse by column position (CRITICAL: do NOT use .split())
        for i in range(0, len(line_stripped), column_width):
            column = line_stripped[i:i+column_width].strip()

            if not column:
                continue  # Skip empty columns

            try:
                # Try direct conversion first
                values.append(float(column))
            except ValueError:
                # Handle merged values (e.g., "123.45678.90" without space)
                # Use regex to split merged numeric values
                merged_values = re.findall(r'-?\d+\.?\d*', column)
                if merged_values:
                    for val_str in merged_values:
                        try:
                            values.append(float(val_str))
                        except ValueError:
                            logger.warning(f"Could not parse value '{val_str}' from merged column '{column}'")
                else:
                    logger.warning(f"Could not parse column '{column}' as numeric")

        return values

    @staticmethod
    def format_fixed_width(values: List[float],
                          column_width: int = 8,
                          values_per_line: int = 10,
                          precision: int = 2) -> List[str]:
        """
        Format values into fixed-width lines for writing to geometry files.

        Creates properly formatted lines with right-aligned values, left-padded
        with spaces to fill the column width. Follows HEC-RAS conventions:
        - 8-char columns: Typically 10 values per line (80 chars total)
        - 16-char columns: Typically 4 values per line (64 chars total)

        Parameters:
            values (List[float]): List of numeric values to format
            column_width (int): Width of each column in characters. Defaults to 8.
            values_per_line (int): Number of values per line. Defaults to 10.
            precision (int): Decimal places for formatting. Defaults to 2.

        Returns:
            List[str]: Lines with fixed-width formatted values (with newlines)

        Example:
            >>> values = [0.0, 963.04, 27.2, 963.04]
            >>> lines = RasGeometryUtils.format_fixed_width(values, 8, 10, 2)
            >>> print(lines[0])
            '    0.00  963.04   27.20  963.04\\n'

            >>> # 16-char columns for coordinates
            >>> coords = [648224.43125, 4551425.84375]
            >>> lines = RasGeometryUtils.format_fixed_width(coords, 16, 4, 5)
            >>> print(lines[0])
            '  648224.43125  4551425.84375\\n'

        Notes:
            - Based on RasUnsteady.write_table_to_file() pattern
            - Values are formatted as f'{value:{column_width}.{precision}f}'
            - Right-aligned within column, left-padded with spaces
            - Last line may have fewer than values_per_line values
        """
        lines = []

        for i in range(0, len(values), values_per_line):
            row_values = values[i:i+values_per_line]
            # Format each value with specified width and precision
            formatted_row = ''.join(f'{value:{column_width}.{precision}f}' for value in row_values)
            lines.append(formatted_row + '\n')

        return lines

    @staticmethod
    @log_call
    def interpret_count(keyword: str,
                       count_value: int,
                       additional_values: Optional[List[int]] = None) -> int:
        """
        Interpret count declarations based on keyword context.

        CRITICAL: Different keywords use counts differently. This is a common
        source of parsing bugs if not handled correctly.

        Count Interpretation Rules:
        - "#Sta/Elev= 40" → 40 PAIRS → 80 total values (station + elevation)
        - "#Mann= 3 , 0 , 0" → 3 SEGMENTS → 9 total values (3 left + 3 channel + 3 right)
        - "Reach XY= 591" → 591 PAIRS → 1182 total values (591 X + 591 Y)
        - "Storage Area Elev Volume= 53" → 53 PAIRS → 106 total values
        - "Levee= 12 , 0" → 12 + 0 = 12 values (left side only)

        Parameters:
            keyword (str): Section keyword (e.g., "#Sta/Elev", "#Mann", "Reach XY")
            count_value (int): First count value after keyword
            additional_values (Optional[List[int]]): Additional count values if comma-separated

        Returns:
            int: Total number of values to read from the file

        Example:
            >>> # Station/elevation: 40 pairs = 80 values
            >>> RasGeometryUtils.interpret_count("#Sta/Elev", 40)
            80

            >>> # Manning's n: 3 segments × 3 positions = 9 values
            >>> RasGeometryUtils.interpret_count("#Mann", 3, [0, 0])
            9

            >>> # Reach coordinates: 591 pairs = 1182 values
            >>> RasGeometryUtils.interpret_count("Reach XY", 591)
            1182

            >>> # Levees: 12 left + 0 right = 12 values
            >>> RasGeometryUtils.interpret_count("Levee", 12, [0])
            12

        Notes:
            - See _PARSING_PATTERNS_REFERENCE.md for complete count interpretation guide
            - This is based on extensive validation against HDF files
        """
        keyword_lower = keyword.lower()

        # Station/Elevation pairs (most common)
        if 'sta' in keyword_lower and 'elev' in keyword_lower:
            return count_value * 2  # Pairs: station + elevation

        # Manning's n segments (triplets: left, channel, right)
        if 'mann' in keyword_lower:
            # #Mann= 3 , 0 , 0 means 3 segments with left/channel/right values each
            return count_value * 3

        # Coordinate pairs (X, Y)
        if 'xy' in keyword_lower or ('x' in keyword_lower and 'y' in keyword_lower):
            return count_value * 2  # Pairs: X + Y

        # Elevation-Volume pairs (storage areas)
        if 'elev' in keyword_lower and 'volume' in keyword_lower:
            return count_value * 2  # Pairs: elevation + volume

        # Levees (can have left and right counts)
        if 'levee' in keyword_lower:
            if additional_values:
                return count_value + sum(additional_values)
            return count_value

        # Default: count is total values (not pairs)
        logger.debug(f"Using default count interpretation for keyword '{keyword}': {count_value} values")
        return count_value

    @staticmethod
    @log_call
    def identify_section(lines: List[str],
                        keyword: str,
                        start_index: int = 0) -> Optional[Tuple[int, int]]:
        """
        Find section boundaries based on keyword marker.

        Searches for a line starting with the specified keyword and determines
        where the section ends (either at the next keyword or end of file).

        Parameters:
            lines (List[str]): All lines from geometry file
            keyword (str): Section marker keyword to search for
            start_index (int): Line index to start searching from. Defaults to 0.

        Returns:
            Optional[Tuple[int, int]]: (start_line, end_line) or None if not found
                                       start_line: Index of line with keyword
                                       end_line: Index of last line in section (exclusive)

        Example:
            >>> with open("geometry.g01") as f:
            ...     lines = f.readlines()
            >>> section = RasGeometryUtils.identify_section(lines, "River Reach=")
            >>> if section:
            ...     start, end = section
            ...     print(f"River Reach section: lines {start} to {end}")

        Notes:
            - Keyword matching is case-insensitive
            - Returns None if keyword not found
            - Section ends at next keyword starting with capital letter or "=" sign
        """
        start_line = None

        # Find the start of the section
        for i in range(start_index, len(lines)):
            if lines[i].strip().lower().startswith(keyword.lower()):
                start_line = i
                break

        if start_line is None:
            logger.debug(f"Keyword '{keyword}' not found starting from line {start_index}")
            return None

        # Find the end of the section (next keyword or end of file)
        end_line = len(lines)
        for i in range(start_line + 1, len(lines)):
            line_stripped = lines[i].strip()
            # Section ends at next keyword (starts with capital or contains "=")
            if line_stripped and (line_stripped[0].isupper() or '=' in line_stripped):
                # Check if it looks like a keyword (not just data with "=")
                if '=' in line_stripped:
                    end_line = i
                    break

        logger.debug(f"Section '{keyword}' found: lines {start_line} to {end_line}")
        return (start_line, end_line)

    @staticmethod
    def extract_keyword_value(line: str, keyword: str) -> str:
        """
        Extract value following keyword marker.

        Finds keyword followed by "=" and returns everything after the "=".

        Parameters:
            line (str): Line containing keyword
            keyword (str): Keyword to search for

        Returns:
            str: Value after "=" (stripped of leading/trailing whitespace)

        Example:
            >>> line = "Geom Title=White Lick Creek Geometry"
            >>> title = RasGeometryUtils.extract_keyword_value(line, "Geom Title")
            >>> print(title)
            'White Lick Creek Geometry'

            >>> line = "Program Version=6.30"
            >>> version = RasGeometryUtils.extract_keyword_value(line, "Program Version")
            >>> print(version)
            '6.30'

        Notes:
            - Keyword matching is case-insensitive
            - Returns empty string if keyword not found or no value after "="
        """
        # Pattern: keyword (case-insensitive) followed by = and value
        pattern = rf'{re.escape(keyword)}\s*=\s*(.+)'
        match = re.search(pattern, line, re.IGNORECASE)

        if match:
            return match.group(1).strip()
        return ""

    @staticmethod
    def extract_comma_list(line: str, keyword: str) -> List[str]:
        """
        Extract comma-separated list following keyword.

        Handles embedded commas in quoted strings properly.

        Parameters:
            line (str): Line containing keyword and comma-separated values
            keyword (str): Keyword before the list

        Returns:
            List[str]: List of values (stripped of whitespace)

        Example:
            >>> line = "River Reach=White Lick,Reach 1"
            >>> values = RasGeometryUtils.extract_comma_list(line, "River Reach")
            >>> print(values)
            ['White Lick', 'Reach 1']

            >>> line = "Storage Area=Res Pool 1"
            >>> values = RasGeometryUtils.extract_comma_list(line, "Storage Area")
            >>> print(values)
            ['Res Pool 1']

        Notes:
            - Handles cases with or without commas
            - Handles quoted strings with embedded commas
        """
        value_str = RasGeometryUtils.extract_keyword_value(line, keyword)

        if not value_str:
            return []

        # Split by comma, handling quoted strings
        # Simple approach: split by comma and strip
        values = [v.strip().strip('"\'') for v in value_str.split(',')]

        return values

    @staticmethod
    @log_call
    def create_backup(file_path: Path) -> Path:
        """
        Create .bak backup of file before modification.

        Creates a backup copy with .bak extension. If .bak already exists,
        creates .bak1, .bak2, etc.

        Parameters:
            file_path (Path): Path to file to backup

        Returns:
            Path: Path to backup file

        Raises:
            FileNotFoundError: If original file doesn't exist
            IOError: If backup creation fails

        Example:
            >>> from pathlib import Path
            >>> geom_file = Path("MyProject.g01")
            >>> backup = RasGeometryUtils.create_backup(geom_file)
            >>> print(f"Backup created: {backup}")
            Backup created: MyProject.g01.bak

        Notes:
            - Based on RasGeo.set_mannings_baseoverrides() pattern
            - Always creates backup before file modification
            - Finds next available .bakN filename if .bak exists
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"Cannot create backup: file not found: {file_path}")

        # Find next available backup filename
        backup_path = file_path.with_suffix(file_path.suffix + '.bak')
        counter = 1

        while backup_path.exists():
            backup_path = file_path.with_suffix(f'{file_path.suffix}.bak{counter}')
            counter += 1

        try:
            # Copy file to backup
            import shutil
            shutil.copy2(file_path, backup_path)
            logger.info(f"Created backup: {backup_path}")
            return backup_path

        except Exception as e:
            logger.error(f"Failed to create backup of {file_path}: {str(e)}")
            raise IOError(f"Backup creation failed: {str(e)}")

    @staticmethod
    def update_timestamp(lines: List[str], keyword: str) -> List[str]:
        """
        Update timestamp for a modified section.

        Finds lines with timestamp keywords and updates them to current time.

        Parameters:
            lines (List[str]): File lines to modify
            keyword (str): Timestamp keyword to search for

        Returns:
            List[str]: Modified lines with updated timestamp

        Example:
            >>> lines = ["LCMann Time=01Jan2023 14:30:45\\n"]
            >>> updated = RasGeometryUtils.update_timestamp(lines, "LCMann Time")
            >>> print(updated[0])
            'LCMann Time=11Nov2025 10:45:30\\n'

        Notes:
            - Timestamp format: DDMmmYYYY HH:MM:SS
            - Only updates lines matching the specified keyword
            - Preserves all other lines unchanged
        """
        current_time = datetime.now()
        timestamp_str = current_time.strftime("%d%b%Y %H:%M:%S")

        updated_lines = []
        for line in lines:
            if keyword in line and '=' in line:
                # Replace the timestamp after the "="
                parts = line.split('=')
                updated_line = f"{parts[0]}={timestamp_str}\n"
                updated_lines.append(updated_line)
            else:
                updated_lines.append(line)

        return updated_lines

    @staticmethod
    @log_call
    def validate_river_reach_rs(geom_file: Path,
                               river: str,
                               reach: str,
                               rs: str) -> bool:
        """
        Validate that river/reach/RS combination exists in geometry file.

        Parameters:
            geom_file (Path): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            bool: True if combination exists

        Raises:
            ValueError: If river/reach/RS not found in geometry file

        Example:
            >>> from pathlib import Path
            >>> geom_file = Path("BaldEagle.g01")
            >>> valid = RasGeometryUtils.validate_river_reach_rs(
            ...     geom_file, "Bald Eagle Creek", "Reach 1", "138154.4"
            ... )
            >>> print(valid)
            True

        Notes:
            - Used before modification operations to ensure valid target
            - Searches for "Type RM Length L Ch R =" line with matching RS
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find River Reach line
            current_river = None
            current_reach = None

            for i, line in enumerate(lines):
                # Check for River Reach definition
                if line.startswith("River Reach="):
                    values = RasGeometryUtils.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                # Check for cross section with matching RS
                if line.startswith("Type RM Length L Ch R ="):
                    # Next line should have river station
                    if i + 1 < len(lines):
                        parts = lines[i].split('=')
                        if len(parts) > 1:
                            values = parts[1].strip().split(',')
                            if len(values) > 0:
                                xs_rs = values[0].strip()
                                if (current_river == river and
                                    current_reach == reach and
                                    xs_rs == rs):
                                    logger.debug(f"Found XS: {river}/{reach}/RS {rs}")
                                    return True

            raise ValueError(f"Cross section not found: {river}, {reach}, RS {rs}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error validating river/reach/RS: {str(e)}")
            raise ValueError(f"Validation failed: {str(e)}")

==================================================

File: c:\GH\ras-commander\ras_commander\RasGuiAutomation.py
==================================================
"""
RasGuiAutomation - GUI automation for HEC-RAS using win32com

This module provides functionality to automate HEC-RAS GUI operations using Windows
COM automation and win32gui. It enables programmatic control of menu items, dialogs,
and buttons for workflows that don't have API support.

Public functions:
    get_windows_by_pid(pid)                    - Return all windows for a given process ID as (hwnd, title) tuples.
    find_main_hecras_window(windows)           - Identify the main HEC-RAS window from a window list.
    enumerate_all_menus(hwnd)                  - Return all top-level menus and items for the given window handle.
    click_menu_item(hwnd, menu_id)             - Trigger a menu item by sending WM_COMMAND to the main window.
    find_dialog_by_title(pattern, exact)       - Locate a visible dialog window by title substring or exact match.
    find_button_by_text(hwnd, text)            - Find a button control in a dialog window by its text.
    click_button(button_hwnd)                  - Simulate a click on a button control.
    find_combobox_by_neighbor(hwnd, text)      - Find a combo box control near a label with specific text.
    select_combobox_item_by_text(combo, text)  - Select an item in a combo box by its text.
    set_current_plan(hwnd, plan_number, ...)   - Set the current plan in HEC-RAS by selecting from the plan dropdown.
    wait_for_window(find_window_func, ...)     - Wait for a window using a polling function and timeout.
    open_and_compute(...)                      - Open HEC-RAS, set plan, navigate via menu, optionally click Compute.
    close_window(hwnd)                         - Close the given window handle via WM_CLOSE.
    run_multiple_plans(...)                    - Automate GUI workflow for "Run Multiple Plans" in HEC-RAS.

Private functions (scoped within above):
    Various local callback functions for window and child window enumeration.

This module is part of the ras-commander library and uses a centralized logging configuration.
All public functions are static methods on RasGuiAutomation and are decorated with @log_call.
"""

import time
import ctypes
from ctypes import wintypes
import subprocess
import sys
from pathlib import Path
from typing import Optional, List, Tuple, Callable, Any

# Win32 imports
try:
    import win32gui
    import win32con
    import win32api
    import win32com.client
    import win32process
except ImportError:
    raise ImportError(
        "win32 libraries are required for GUI automation. "
        "Install with: pip install pywin32"
    )

from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

# Windows constants
WM_COMMAND = 0x0111
MF_BYPOSITION = 0x00000400


class RasGuiAutomation:
    """
    Static class for automating HEC-RAS GUI operations using win32com.

    This class provides methods to programmatically control HEC-RAS GUI elements
    including menus, dialogs, and buttons. It's designed for workflows that don't
    have programmatic API support (e.g., floodplain mapping).

    All methods are static and use the @log_call decorator for automatic logging.
    """

    @staticmethod
    @log_call
    def get_windows_by_pid(pid: int) -> List[Tuple[int, str]]:
        """
        Find all windows belonging to a specific process ID.

        Args:
            pid (int): Process ID to search for.

        Returns:
            List[Tuple[int, str]]: List of (window_handle, window_title) tuples.

        Examples:
            >>> windows = RasGuiAutomation.get_windows_by_pid(12345)
            >>> for hwnd, title in windows:
            ...     print(f"Window: {title}")
        """
        def callback(hwnd, hwnds):
            if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):
                # Get the process ID for this window
                _, window_pid = win32process.GetWindowThreadProcessId(hwnd)
                if window_pid == pid:
                    window_title = win32gui.GetWindowText(hwnd)
                    if window_title:  # Only include windows with titles
                        hwnds.append((hwnd, window_title))
            return True

        hwnds = []
        win32gui.EnumWindows(callback, hwnds)
        return hwnds

    @staticmethod
    @log_call
    def find_main_hecras_window(windows: List[Tuple[int, str]]) -> Tuple[Optional[int], Optional[str]]:
        """
        Find the main HEC-RAS window from a list of windows.

        The main window is identified by having "HEC-RAS" in the title and a menu bar.

        Args:
            windows (List[Tuple[int, str]]): List of (window_handle, window_title) tuples.

        Returns:
            Tuple[Optional[int], Optional[str]]: (window_handle, window_title) or (None, None).

        Examples:
            >>> windows = RasGuiAutomation.get_windows_by_pid(12345)
            >>> hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
        """
        for hwnd, title in windows:
            # Main window usually has "HEC-RAS" in title and has a menu bar
            if "HEC-RAS" in title and win32gui.GetMenu(hwnd):
                logger.debug(f"Found main HEC-RAS window: {title}")
                return hwnd, title
        return None, None

    @staticmethod
    @log_call
    def get_menu_string(menu_handle: int, pos: int) -> str:
        """
        Get menu item string at a specific position.

        Args:
            menu_handle (int): Handle to the menu.
            pos (int): Position index of the menu item.

        Returns:
            str: Menu item text, or empty string if not found.
        """
        # Create buffer for menu string
        buf_size = 256
        buf = ctypes.create_unicode_buffer(buf_size)

        # Get menu item info
        user32 = ctypes.windll.user32
        result = user32.GetMenuStringW(
            menu_handle,
            pos,
            buf,
            buf_size,
            MF_BYPOSITION
        )

        if result:
            return buf.value
        return ""

    @staticmethod
    @log_call
    def enumerate_all_menus(hwnd: int) -> dict:
        """
        Enumerate all menus and their items in a window.

        Args:
            hwnd (int): Handle to the window.

        Returns:
            dict: Dictionary mapping menu text to list of (item_text, menu_id) tuples.

        Examples:
            >>> hwnd = 12345
            >>> menus = RasGuiAutomation.enumerate_all_menus(hwnd)
            >>> print(menus['&Run'])
            [('&Unsteady Flow Analysis ...', 47), ...]
        """
        menu_bar = win32gui.GetMenu(hwnd)
        if not menu_bar:
            logger.warning("No menu bar found")
            return {}

        menu_count = win32gui.GetMenuItemCount(menu_bar)
        logger.debug(f"Found {menu_count} top-level menus")

        all_menus = {}

        for i in range(menu_count):
            # Get menu text
            menu_text = RasGuiAutomation.get_menu_string(menu_bar, i)

            # Get submenu handle
            submenu = win32gui.GetSubMenu(menu_bar, i)
            if submenu:
                item_count = win32gui.GetMenuItemCount(submenu)
                menu_items = []

                for j in range(item_count):
                    item_text = RasGuiAutomation.get_menu_string(submenu, j)
                    menu_id = win32gui.GetMenuItemID(submenu, j)
                    menu_items.append((item_text, menu_id))

                all_menus[menu_text] = menu_items

        return all_menus

    @staticmethod
    @log_call
    def click_menu_item(hwnd: int, menu_id: int) -> bool:
        """
        Click a menu item by sending a WM_COMMAND message.

        Args:
            hwnd (int): Handle to the main window.
            menu_id (int): Menu item ID to activate.

        Returns:
            bool: True if message was posted successfully.

        Examples:
            >>> # Click "Run > Unsteady Flow Analysis" (menu ID 47)
            >>> RasGuiAutomation.click_menu_item(hwnd, 47)
        """
        try:
            win32api.PostMessage(hwnd, WM_COMMAND, menu_id, 0)
            logger.info(f"Clicked menu item ID: {menu_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to click menu item {menu_id}: {e}")
            return False

    @staticmethod
    @log_call
    def find_dialog_by_title(title_pattern: str, exact_match: bool = False) -> Optional[int]:
        """
        Find a dialog window by title pattern.

        Args:
            title_pattern (str): Text to search for in window title.
            exact_match (bool): If True, require exact match. Default is substring match.

        Returns:
            Optional[int]: Window handle if found, None otherwise.

        Examples:
            >>> # Find "Unsteady Flow Analysis" dialog
            >>> dialog_hwnd = RasGuiAutomation.find_dialog_by_title("Unsteady Flow Analysis")
        """
        def callback(hwnd, dialogs):
            if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):
                window_title = win32gui.GetWindowText(hwnd)
                if exact_match:
                    if window_title == title_pattern:
                        dialogs.append(hwnd)
                else:
                    if title_pattern.lower() in window_title.lower():
                        dialogs.append(hwnd)
            return True

        dialogs = []
        win32gui.EnumWindows(callback, dialogs)

        if dialogs:
            logger.debug(f"Found dialog matching '{title_pattern}': {len(dialogs)} window(s)")
            return dialogs[0]

        logger.debug(f"No dialog found matching '{title_pattern}'")
        return None

    @staticmethod
    @log_call
    def find_button_by_text(dialog_hwnd: int, button_text: str) -> Optional[int]:
        """
        Find a button in a dialog by its text.

        Args:
            dialog_hwnd (int): Handle to the dialog window.
            button_text (str): Text on the button (case-insensitive).

        Returns:
            Optional[int]: Button handle if found, None otherwise.

        Examples:
            >>> button_hwnd = RasGuiAutomation.find_button_by_text(dialog_hwnd, "Compute")
        """
        def callback(child_hwnd, buttons):
            try:
                text = win32gui.GetWindowText(child_hwnd)
                class_name = win32gui.GetClassName(child_hwnd)
                if button_text.lower() in text.lower() and class_name == "Button":
                    buttons.append(child_hwnd)
            except:
                pass
            return True

        buttons = []
        win32gui.EnumChildWindows(dialog_hwnd, callback, buttons)

        if buttons:
            logger.debug(f"Found button with text '{button_text}'")
            return buttons[0]

        logger.debug(f"No button found with text '{button_text}'")
        return None

    @staticmethod
    @log_call
    def click_button(button_hwnd: int) -> bool:
        """
        Click a button by sending BN_CLICKED message.

        Args:
            button_hwnd (int): Handle to the button.

        Returns:
            bool: True if successful.
        """
        try:
            win32api.SendMessage(button_hwnd, win32con.BM_CLICK, 0, 0)
            logger.info(f"Clicked button: {win32gui.GetWindowText(button_hwnd)}")
            return True
        except Exception as e:
            logger.error(f"Failed to click button: {e}")
            return False

    @staticmethod
    @log_call
    def find_combobox_by_neighbor(hwnd: int, neighbor_text: str) -> Optional[int]:
        """
        Find a combo box control near a label with specific text.

        Args:
            hwnd (int): Handle to the parent window.
            neighbor_text (str): Text of a nearby label (case-insensitive).

        Returns:
            Optional[int]: Combo box handle if found, None otherwise.

        Examples:
            >>> combo = RasGuiAutomation.find_combobox_by_neighbor(hwnd, "Plan:")
        """
        def callback(child_hwnd, combos):
            try:
                class_name = win32gui.GetClassName(child_hwnd)
                if "ComboBox" in class_name:
                    combos.append(child_hwnd)
            except:
                pass
            return True

        combos = []
        win32gui.EnumChildWindows(hwnd, callback, combos)

        if combos:
            logger.debug(f"Found {len(combos)} combo box(es)")
            # For now, return the first combo box found
            # In a more sophisticated implementation, we could check proximity to the label
            return combos[0]

        logger.debug(f"No combo box found near '{neighbor_text}'")
        return None

    @staticmethod
    @log_call
    def select_combobox_item_by_text(combo_hwnd: int, item_text: str) -> bool:
        """
        Select an item in a combo box by its text.

        Args:
            combo_hwnd (int): Handle to the combo box.
            item_text (str): Text of the item to select (partial match, case-insensitive).

        Returns:
            bool: True if item was found and selected.

        Examples:
            >>> RasGuiAutomation.select_combobox_item_by_text(combo_hwnd, "p01")
        """
        try:
            # CB_GETCOUNT = 0x0146
            CB_GETCOUNT = 0x0146
            # CB_GETLBTEXTLEN = 0x0149
            CB_GETLBTEXTLEN = 0x0149
            # CB_GETLBTEXT = 0x0148
            CB_GETLBTEXT = 0x0148
            # CB_SETCURSEL = 0x014E
            CB_SETCURSEL = 0x014E

            # Get number of items in combo box
            count = win32api.SendMessage(combo_hwnd, CB_GETCOUNT, 0, 0)
            logger.debug(f"Combo box has {count} items")

            # Search for matching item
            for i in range(count):
                # Get length of text for this item
                text_len = win32api.SendMessage(combo_hwnd, CB_GETLBTEXTLEN, i, 0)
                if text_len > 0:
                    # Get the text
                    buffer = ctypes.create_unicode_buffer(text_len + 1)
                    win32api.SendMessage(combo_hwnd, CB_GETLBTEXT, i, buffer)
                    item = buffer.value

                    logger.debug(f"Combo box item {i}: '{item}'")

                    # Check for match (case-insensitive, partial match)
                    if item_text.lower() in item.lower():
                        # Select this item
                        win32api.SendMessage(combo_hwnd, CB_SETCURSEL, i, 0)
                        logger.info(f"Selected combo box item {i}: '{item}'")
                        return True

            logger.warning(f"Could not find item containing '{item_text}' in combo box")
            return False

        except Exception as e:
            logger.error(f"Failed to select combo box item: {e}")
            return False

## CHANGE THIS (START)

    @staticmethod
    @log_call
    def set_current_plan(hwnd: int, plan_number: str, ras_object=None) -> bool:
        """
        Set the current plan in HEC-RAS by finding and selecting from the plan dropdown.

        Args:
            hwnd (int): Handle to the main HEC-RAS window.
            plan_number (str): Plan number to select (e.g., "01", "02").
            ras_object: Optional RAS object instance.

        Returns:
            bool: True if plan was successfully selected.

        Examples:
            >>> RasGuiAutomation.set_current_plan(hwnd, "01")
        """
        ras_obj = ras_object or ras
        
        # Try to find the plan combo box
        # In HEC-RAS, the plan selector is typically a combo box near a "Plan:" label
        plan_combo = RasGuiAutomation.find_combobox_by_neighbor(hwnd, "Plan:")
        
        if not plan_combo:
            logger.warning("Could not find plan combo box")
            return False

        # Get plan details to construct the full plan text
        # Plans are typically shown as "p01 - Plan Title" or similar
        try:
            from .RasPlan import RasPlan
            plan_title = RasPlan.get_plan_title(plan_number, ras_object=ras_obj)
            plan_shortid = RasPlan.get_shortid(plan_number, ras_object=ras_obj)
            
            # Try different formats that HEC-RAS might use
            search_terms = [
                f"p{plan_number}",  # Just the plan number
                f"{plan_shortid}",  # Short ID
                f"p{plan_number} - {plan_title}",  # Full format with title
                f"p{plan_number} - {plan_shortid}",  # Format with short ID
            ]
            
            for term in search_terms:
                if RasGuiAutomation.select_combobox_item_by_text(plan_combo, term):
                    logger.info(f"Successfully set current plan to p{plan_number}")
                    return True
            
            # If none of the specific formats worked, just try the plan number
            if RasGuiAutomation.select_combobox_item_by_text(plan_combo, plan_number):
                logger.info(f"Successfully set current plan to p{plan_number}")
                return True
                
        except Exception as e:
            logger.warning(f"Could not get plan details, trying simple search: {e}")
            # Fallback to simple plan number search
            if RasGuiAutomation.select_combobox_item_by_text(plan_combo, f"p{plan_number}"):
                logger.info(f"Successfully set current plan to p{plan_number}")
                return True

        logger.error(f"Failed to set current plan to p{plan_number}")
        return False

## CHANGE THIS (START)


    @staticmethod
    @log_call
    def wait_for_window(
        find_window_func: Callable,
        timeout: int = 60,
        check_interval: int = 2
    ) -> Any:
        """
        Wait for a window to appear using a custom search function.

        Args:
            find_window_func (Callable): Function that returns window handle or None.
            timeout (int): Maximum time to wait in seconds. Default is 60.
            check_interval (int): Time between checks in seconds. Default is 2.

        Returns:
            Any: Result from find_window_func if found within timeout, None otherwise.

        Examples:
            >>> # Wait for main HEC-RAS window
            >>> def find_ras():
            ...     windows = RasGuiAutomation.get_windows_by_pid(pid)
            ...     hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
            ...     return hwnd
            >>> hwnd = RasGuiAutomation.wait_for_window(find_ras, timeout=30)
        """
        start_time = time.time()
        while time.time() - start_time < timeout:
            result = find_window_func()
            if result:
                logger.debug("Window found")
                return result
            logger.debug(f"Window not found, waiting {check_interval} seconds...")
            time.sleep(check_interval)

        logger.warning(f"Window not found after {timeout} seconds")
        return None

    @staticmethod
    @log_call
    def open_and_compute(
        plan_number: str,
        ras_object=None,
        auto_click_compute: bool = True,
        wait_for_user: bool = True
    ) -> bool:
        """
        Open HEC-RAS, set the current plan, navigate to Unsteady Flow Analysis, and optionally click Compute.

        This function automates the workflow:
        1. Open HEC-RAS with the project
        2. Wait for main window to appear
        3. Set the current plan to the specified plan_number
        4. Click "Run > Unsteady Flow Analysis" menu (ID 47)
        5. Optionally click "Compute" button in dialog
        6. Wait for user to close HEC-RAS (or return immediately)

        Args:
            plan_number (str): Plan number to run (e.g., "01", "02").
            ras_object: Optional RAS object instance.
            auto_click_compute (bool): If True, automatically click Compute button. Default True.
            wait_for_user (bool): If True, wait for user to close HEC-RAS. Default True.

        Returns:
            bool: True if successful, False otherwise.

        Examples:
            >>> # Full automation - runs plan "01"
            >>> RasGuiAutomation.open_and_compute("01", auto_click_compute=True)

            >>> # Just open dialog for plan "02", let user click Compute
            >>> RasGuiAutomation.open_and_compute("02", auto_click_compute=False)

        Notes:
            - This is designed for floodplain mapping workflows that require GUI execution
            - The function will attempt to set the current plan before running
            - Menu ID 47 is "Run > Unsteady Flow Analysis" in HEC-RAS 6.x
            - If plan selection or auto_click_compute fails, user can manually complete the workflow
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Step 1: Set current plan in .prj file BEFORE opening HEC-RAS
        # This ensures HEC-RAS opens with the correct plan active
        logger.info(f"Setting current plan to {plan_number} in project file...")
        try:
            ras_obj.set_current_plan(plan_number)
            logger.info(f"Current plan set to {plan_number} in {ras_obj.prj_file}")
        except Exception as e:
            logger.error(f"Failed to set current plan: {e}")
            return False

        # Step 2: Open HEC-RAS
        logger.info("Opening HEC-RAS...")
        ras_exe = ras_obj.ras_exe_path
        prj_path = f'"{str(ras_obj.prj_file)}"'
        command = f"{ras_exe} {prj_path}"

        try:
            if sys.platform == "win32":
                hecras_process = subprocess.Popen(command)
            else:
                hecras_process = subprocess.Popen([str(ras_exe), str(ras_obj.prj_file)])

            logger.info(f"HEC-RAS opened with Process ID: {hecras_process.pid}")
        except Exception as e:
            logger.error(f"Failed to open HEC-RAS: {e}")
            return False

        # Step 3: Wait for main window
        logger.info("Waiting for HEC-RAS main window...")
        time.sleep(3)  # Initial wait for process to start

        def find_ras_window():
            windows = RasGuiAutomation.get_windows_by_pid(hecras_process.pid)
            hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
            return hwnd

        hec_ras_hwnd = RasGuiAutomation.wait_for_window(find_ras_window, timeout=30)

        if not hec_ras_hwnd:
            logger.error("Could not find main HEC-RAS window")
            return False

        logger.info(f"Found HEC-RAS main window: {win32gui.GetWindowText(hec_ras_hwnd)}")

        # Note: Current plan was already set in .prj file before opening HEC-RAS (Step 1)
        # HEC-RAS should now have the correct plan active
        time.sleep(1)  # Let window fully load

        # Step 4: Click "Run > Unsteady Flow Analysis" (menu ID 47)
        logger.info("Clicking 'Run > Unsteady Flow Analysis' menu...")
        time.sleep(0.5)

        if not RasGuiAutomation.click_menu_item(hec_ras_hwnd, 47):
            logger.warning("Failed to click menu item, but continuing...")

        time.sleep(2)  # Wait for dialog to open

        # Step 5: Find and click Compute button (if auto_click_compute)
        if auto_click_compute:
            logger.info("Looking for Unsteady Flow Analysis dialog...")

            def find_unsteady_dialog():
                return RasGuiAutomation.find_dialog_by_title("Unsteady Flow Analysis")

            dialog_hwnd = RasGuiAutomation.wait_for_window(find_unsteady_dialog, timeout=15)

            if dialog_hwnd:
                logger.info("Found Unsteady Flow Analysis dialog")
                logger.info("Looking for Compute button...")

                # Try to find and click Compute button
                compute_button = RasGuiAutomation.find_button_by_text(dialog_hwnd, "Compute")

                if compute_button:
                    logger.info("Clicking Compute button...")
                    RasGuiAutomation.click_button(compute_button)
                else:
                    logger.warning("Could not find Compute button - user must click manually")
                    logger.info("Trying keyboard shortcut as fallback...")
                    try:
                        shell = win32com.client.Dispatch("WScript.Shell")
                        time.sleep(0.5)
                        shell.SendKeys("{ENTER}")
                        logger.info("Sent Enter key to dialog")
                    except Exception as e:
                        logger.warning(f"Keyboard fallback failed: {e}")
            else:
                logger.warning("Could not find Unsteady Flow Analysis dialog")
                logger.info("User must manually click 'Run > Unsteady Flow Analysis' and Compute")

        # Step 6: Wait for user to close HEC-RAS (or return immediately)
        if wait_for_user:
            logger.info("Waiting for user to close HEC-RAS...")
            logger.info(f"Please monitor plan {plan_number} execution and close HEC-RAS when complete")

            try:
                hecras_process.wait()
                logger.info("HEC-RAS has been closed")
            except Exception as e:
                logger.error(f"Error waiting for HEC-RAS to close: {e}")
                return False
        else:
            logger.info("Returning without waiting for HEC-RAS to close")
            logger.info(f"HEC-RAS process ID: {hecras_process.pid}")

        return True

    @staticmethod
    @log_call
    def close_window(hwnd: int) -> bool:
        """
        Close a window by sending WM_CLOSE message.

        Args:
            hwnd (int): Handle to the window to close.

        Returns:
            bool: True if successful.
        """
        try:
            win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)
            logger.info(f"Closed window: {win32gui.GetWindowText(hwnd)}")
            return True
        except Exception as e:
            logger.error(f"Failed to close window: {e}")
            return False

    @staticmethod
    @log_call
    def run_multiple_plans(
        plan_numbers: Optional[List[str]] = None,
        ras_object=None,
        check_all: bool = True,
        wait_for_user: bool = True
    ) -> bool:
        """
        Open HEC-RAS and automate "Run > Run Multiple Plans" workflow.

        This function automates the workflow:
        1. Open HEC-RAS with the project
        2. Wait for main window to appear
        3. Click "Run > Run Multiple Plans" menu (ID 52)
        4. Optionally check all plans or select specific plans
        5. Click "Compute" or "Run All Checked Plans" button
        6. Wait for user to close HEC-RAS (or return immediately)

        Args:
            plan_numbers (Optional[List[str]]): List of plan numbers to run. If None and
                check_all=True, all plans will be checked. Currently informational only -
                the function checks all plans regardless.
            ras_object: Optional RAS object instance.
            check_all (bool): If True, attempts to check all plans. Default True.
            wait_for_user (bool): If True, wait for user to close HEC-RAS. Default True.

        Returns:
            bool: True if successful, False otherwise.

        Examples:
            >>> # Run all plans
            >>> RasGuiAutomation.run_multiple_plans(check_all=True)

            >>> # Run specific plans (currently checks all, but logs which plans were requested)
            >>> RasGuiAutomation.run_multiple_plans(plan_numbers=["01", "02"])

        Notes:
            - Menu ID 52 is "Run > Run Multiple Plans" in HEC-RAS 6.x
            - This is useful for batch processing multiple plans or stored maps
            - Currently checks all plans; specific plan selection would require
              analyzing the dialog checkbox structure
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        if plan_numbers:
            logger.info(f"Requested plans: {', '.join(plan_numbers)}")
            logger.info("Note: Currently checking all plans. Specific plan selection not yet implemented.")

        # Step 1: Open HEC-RAS
        logger.info("Opening HEC-RAS...")
        ras_exe = ras_obj.ras_exe_path
        prj_path = f'"{str(ras_obj.prj_file)}"'
        command = f"{ras_exe} {prj_path}"

        try:
            if sys.platform == "win32":
                hecras_process = subprocess.Popen(command)
            else:
                hecras_process = subprocess.Popen([str(ras_exe), str(ras_obj.prj_file)])

            logger.info(f"HEC-RAS opened with Process ID: {hecras_process.pid}")
        except Exception as e:
            logger.error(f"Failed to open HEC-RAS: {e}")
            return False

        # Step 2: Wait for main window
        logger.info("Waiting for HEC-RAS main window...")
        time.sleep(3)  # Initial wait for process to start

        def find_ras_window():
            windows = RasGuiAutomation.get_windows_by_pid(hecras_process.pid)
            hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
            return hwnd

        hec_ras_hwnd = RasGuiAutomation.wait_for_window(find_ras_window, timeout=30)

        if not hec_ras_hwnd:
            logger.error("Could not find main HEC-RAS window")
            return False

        logger.info(f"Found HEC-RAS main window: {win32gui.GetWindowText(hec_ras_hwnd)}")

        # Step 3: Click "Run > Run Multiple Plans" (menu ID 52)
        logger.info("Clicking 'Run > Run Multiple Plans' menu...")
        time.sleep(1)  # Let window fully load

        if not RasGuiAutomation.click_menu_item(hec_ras_hwnd, 52):
            logger.warning("Failed to click menu item, but continuing...")

        time.sleep(2)  # Wait for dialog to open

        # Step 4: Find the Run Multiple Plans dialog
        logger.info("Looking for Run Multiple Plans dialog...")

        def find_multiple_plans_dialog():
            # Try multiple possible dialog titles
            for title_pattern in ["Run Multiple Plans", "Multiple Plans", "Compute Multiple"]:
                hwnd = RasGuiAutomation.find_dialog_by_title(title_pattern)
                if hwnd:
                    return hwnd
            return None

        dialog_hwnd = RasGuiAutomation.wait_for_window(find_multiple_plans_dialog, timeout=15)

        if dialog_hwnd:
            logger.info(f"Found dialog: {win32gui.GetWindowText(dialog_hwnd)}")

            # Step 5: Try to check all plans (if check_all)
            if check_all:
                logger.info("Attempting to check all plans...")

                # Try to find "Check All" or "Select All" button
                check_all_button = None
                for button_text in ["Check All", "Select All", "All"]:
                    check_all_button = RasGuiAutomation.find_button_by_text(dialog_hwnd, button_text)
                    if check_all_button:
                        logger.info(f"Found '{button_text}' button")
                        RasGuiAutomation.click_button(check_all_button)
                        time.sleep(0.5)
                        break

                if not check_all_button:
                    logger.warning("Could not find 'Check All' button - plans may need manual selection")

            # Step 6: Click "Compute" or "Run All Checked Plans" button
            logger.info("Looking for Compute button...")
            time.sleep(1)

            compute_button = None
            for button_text in ["Compute", "Run", "Run All Checked Plans", "Start"]:
                compute_button = RasGuiAutomation.find_button_by_text(dialog_hwnd, button_text)
                if compute_button:
                    logger.info(f"Found '{button_text}' button")
                    RasGuiAutomation.click_button(compute_button)
                    break

            if not compute_button:
                logger.warning("Could not find Compute button - trying keyboard fallback...")
                try:
                    shell = win32com.client.Dispatch("WScript.Shell")
                    time.sleep(0.5)
                    shell.SendKeys("{ENTER}")
                    logger.info("Sent Enter key to dialog")
                except Exception as e:
                    logger.warning(f"Keyboard fallback failed: {e}")
                    logger.info("User must manually click Compute button")

        else:
            logger.warning("Could not find Run Multiple Plans dialog")
            logger.info("User must manually navigate to 'Run > Run Multiple Plans' and click Compute")

        # Step 7: Wait for user to close HEC-RAS (or return immediately)
        if wait_for_user:
            logger.info("Waiting for user to close HEC-RAS...")
            if plan_numbers:
                logger.info(f"Please monitor execution of plans: {', '.join(plan_numbers)}")
            else:
                logger.info("Please monitor execution and close HEC-RAS when complete")

            try:
                hecras_process.wait()
                logger.info("HEC-RAS has been closed")
            except Exception as e:
                logger.error(f"Error waiting for HEC-RAS to close: {e}")
                return False
        else:
            logger.info("Returning without waiting for HEC-RAS to close")
            logger.info(f"HEC-RAS process ID: {hecras_process.pid}")

        return True

==================================================

File: c:\GH\ras-commander\ras_commander\RasMap.py
==================================================
"""
RasMap - Parses HEC-RAS mapper configuration files (.rasmap)

This module provides functionality to extract and organize information from 
HEC-RAS mapper configuration files, including paths to terrain, soil, and land cover data.
It also includes functions to automate the post-processing of stored maps.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL

Classes:
    RasMap: Class for parsing and accessing HEC-RAS mapper configuration.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasMap:
- parse_rasmap(): Parse a .rasmap file and extract relevant information
- get_rasmap_path(): Get the path to the .rasmap file based on the current project
- initialize_rasmap_df(): Initialize the rasmap_df as part of project initialization
- get_terrain_names(): Extracts terrain layer names from a given .rasmap file
- postprocess_stored_maps(): Automates the generation of stored floodplain map outputs (e.g., .tif files)
- get_results_folder(): Get the folder path containing raster results for a specified plan
- get_results_raster(): Get the .vrt file path for a specified plan and variable name
"""

import os
import re
import xml.etree.ElementTree as ET
from pathlib import Path
import pandas as pd
import shutil
from typing import Union, Optional, Dict, List, Any

from .RasPrj import ras
from .RasPlan import RasPlan
from .RasCmdr import RasCmdr
from .RasUtils import RasUtils
from .RasGuiAutomation import RasGuiAutomation
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasMap:
    """
    Class for parsing and accessing information from HEC-RAS mapper configuration files (.rasmap).
    
    This class provides methods to extract paths to terrain, soil, land cover data,
    and various project settings from the .rasmap file associated with a HEC-RAS project.
    It also includes functionality to automate the post-processing of stored maps.
    """
    
    @staticmethod
    @log_call
    def parse_rasmap(rasmap_path: Union[str, Path], ras_object=None) -> pd.DataFrame:
        """
        Parse a .rasmap file and extract relevant information.
        
        Args:
            rasmap_path (Union[str, Path]): Path to the .rasmap file.
            ras_object: Optional RAS object instance.
            
        Returns:
            pd.DataFrame: DataFrame containing extracted information from the .rasmap file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        rasmap_path = Path(rasmap_path)
        if not rasmap_path.exists():
            logger.error(f"RASMapper file not found: {rasmap_path}")
            # Create a single row DataFrame with all empty values
            return pd.DataFrame({
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            })
        
        try:
            # Initialize data for the DataFrame - just one row with lists
            data = {
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            }
            
            # Read the file content
            with open(rasmap_path, 'r', encoding='utf-8') as f:
                xml_content = f.read()
            
            # Check if it's a valid XML file
            if not xml_content.strip().startswith('<'):
                logger.error(f"File does not appear to be valid XML: {rasmap_path}")
                return pd.DataFrame(data)
            
            # Parse the XML file
            try:
                tree = ET.parse(rasmap_path)
                root = tree.getroot()
            except ET.ParseError as e:
                logger.error(f"Error parsing XML in {rasmap_path}: {e}")
                return pd.DataFrame(data)
            
            # Helper function to convert relative paths to absolute paths
            def to_absolute_path(relative_path: str) -> str:
                if not relative_path:
                    return None
                # Remove any leading .\ or ./
                relative_path = relative_path.lstrip('.\\').lstrip('./')
                # Convert to absolute path relative to project folder
                return str(ras_obj.project_folder / relative_path)
            
            # Extract projection path
            try:
                projection_elem = root.find(".//RASProjectionFilename")
                if projection_elem is not None and 'Filename' in projection_elem.attrib:
                    data['projection_path'][0] = to_absolute_path(projection_elem.attrib['Filename'])
            except Exception as e:
                logger.warning(f"Error extracting projection path: {e}")
            
            # Extract profile lines path
            try:
                profile_lines_elem = root.find(".//Features/Layer[@Name='Profile Lines']")
                if profile_lines_elem is not None and 'Filename' in profile_lines_elem.attrib:
                    data['profile_lines_path'][0].append(to_absolute_path(profile_lines_elem.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting profile lines path: {e}")
            
            # Extract soil layer paths
            try:
                soil_layers = root.findall(".//Layer[@Name='Hydrologic Soil Groups']")
                for layer in soil_layers:
                    if 'Filename' in layer.attrib:
                        data['soil_layer_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting soil layer paths: {e}")
            
            # Extract infiltration HDF paths
            try:
                infiltration_layers = root.findall(".//Layer[@Name='Infiltration']")
                for layer in infiltration_layers:
                    if 'Filename' in layer.attrib:
                        data['infiltration_hdf_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting infiltration HDF paths: {e}")
            
            # Extract landcover HDF paths
            try:
                landcover_layers = root.findall(".//Layer[@Name='LandCover']")
                for layer in landcover_layers:
                    if 'Filename' in layer.attrib:
                        data['landcover_hdf_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting landcover HDF paths: {e}")
            
            # Extract terrain HDF paths
            try:
                terrain_layers = root.findall(".//Terrains/Layer")
                for layer in terrain_layers:
                    if 'Filename' in layer.attrib:
                        data['terrain_hdf_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting terrain HDF paths: {e}")
            
            # Extract current settings
            current_settings = {}
            try:
                settings_elem = root.find(".//CurrentSettings")
                if settings_elem is not None:
                    # Extract ProjectSettings
                    project_settings_elem = settings_elem.find("ProjectSettings")
                    if project_settings_elem is not None:
                        for child in project_settings_elem:
                            current_settings[child.tag] = child.text
                    
                    # Extract Folders
                    folders_elem = settings_elem.find("Folders")
                    if folders_elem is not None:
                        for child in folders_elem:
                            current_settings[child.tag] = child.text
                            
                data['current_settings'][0] = current_settings
            except Exception as e:
                logger.warning(f"Error extracting current settings: {e}")
            
            # Create DataFrame
            df = pd.DataFrame(data)
            logger.info(f"Successfully parsed RASMapper file: {rasmap_path}")
            return df
            
        except Exception as e:
            logger.error(f"Unexpected error processing RASMapper file {rasmap_path}: {e}")
            # Create a single row DataFrame with all empty values
            return pd.DataFrame({
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            })
    
    @staticmethod
    @log_call
    def get_rasmap_path(ras_object=None) -> Optional[Path]:
        """
        Get the path to the .rasmap file based on the current project.
        
        Args:
            ras_object: Optional RAS object instance.
            
        Returns:
            Optional[Path]: Path to the .rasmap file if found, None otherwise.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        project_name = ras_obj.project_name
        project_folder = ras_obj.project_folder
        rasmap_path = project_folder / f"{project_name}.rasmap"
        
        if not rasmap_path.exists():
            logger.warning(f"RASMapper file not found: {rasmap_path}")
            return None
        
        return rasmap_path
    
    @staticmethod
    @log_call
    def initialize_rasmap_df(ras_object=None) -> pd.DataFrame:
        """
        Initialize the rasmap_df as part of project initialization.
        
        Args:
            ras_object: Optional RAS object instance.
            
        Returns:
            pd.DataFrame: DataFrame containing information from the .rasmap file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        rasmap_path = RasMap.get_rasmap_path(ras_obj)
        if rasmap_path is None:
            logger.warning("No .rasmap file found for this project. Creating empty rasmap_df.")
            # Create a single row DataFrame with all empty values
            return pd.DataFrame({
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            })
        
        return RasMap.parse_rasmap(rasmap_path, ras_obj)

    @staticmethod
    @log_call
    def get_terrain_names(rasmap_path: Union[str, Path]) -> List[str]:
        """
        Extracts terrain layer names from a given .rasmap file.
        
        Args:
            rasmap_path (Union[str, Path]): Path to the .rasmap file.

        Returns:
            List[str]: A list of terrain names.
        
        Raises:
            FileNotFoundError: If the rasmap file does not exist.
            ValueError: If the file is not a valid XML or lacks a 'Terrains' section.
        """
        rasmap_path = Path(rasmap_path)
        if not rasmap_path.is_file():
            raise FileNotFoundError(f"The file '{rasmap_path}' does not exist.")

        try:
            tree = ET.parse(rasmap_path)
            root = tree.getroot()
        except ET.ParseError as e:
            raise ValueError(f"Failed to parse the RASMAP file. Ensure it is a valid XML file. Error: {e}")

        terrains_element = root.find('Terrains')
        if terrains_element is None:
            logger.warning("The RASMAP file does not contain a 'Terrains' section.")
            return []

        terrain_names = [layer.get('Name') for layer in terrains_element.findall('Layer') if layer.get('Name')]
        logger.info(f"Extracted terrain names: {terrain_names}")
        return terrain_names


    @staticmethod
    @log_call
    def postprocess_stored_maps(
        plan_number: Union[str, List[str]],
        specify_terrain: Optional[str] = None,
        layers: Union[str, List[str]] = None,
        ras_object: Optional[Any] = None,
        auto_click_compute: bool = True
    ) -> bool:
        """
        Automates the generation of stored floodplain map outputs (e.g., .tif files).

        This function modifies the plan and .rasmap files to generate floodplain maps
        for one or more plans, then restores the original files.

        Args:
            plan_number (Union[str, List[str]]): Plan number(s) to generate maps for.
            specify_terrain (Optional[str]): The name of a specific terrain to use.
            layers (Union[str, List[str]], optional): A list of map layers to generate.
                Defaults to ['WSEL', 'Velocity', 'Depth'].
            ras_object (Optional[Any]): The RAS project object.
            auto_click_compute (bool, optional): If True, uses GUI automation to automatically
                click "Run > Unsteady Flow Analysis" and "Compute" button. If False, just
                opens HEC-RAS and waits for manual execution. Defaults to True.

        Returns:
            bool: True if the process completed successfully, False otherwise.

        Notes:
            - auto_click_compute=True: Automated GUI workflow (clicks menu and Compute button)
            - auto_click_compute=False: Manual workflow (user must click Compute)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        if layers is None:
            layers = ['WSEL', 'Velocity', 'Depth']
        elif isinstance(layers, str):
            layers = [layers]

        # Convert plan_number to list if it's a string
        plan_number_list = [plan_number] if isinstance(plan_number, str) else plan_number

        rasmap_path = ras_obj.project_folder / f"{ras_obj.project_name}.rasmap"
        rasmap_backup_path = rasmap_path.with_suffix(f"{rasmap_path.suffix}.storedmap.bak")

        # Store plan paths and their backups
        plan_paths = []
        plan_backup_paths = []
        plan_results_folders = {}  # Map plan_num to results folder name

        for plan_num in plan_number_list:
            plan_path = Path(RasPlan.get_plan_path(plan_num, ras_obj))
            plan_backup_path = plan_path.with_suffix(f"{plan_path.suffix}.storedmap.bak")
            plan_paths.append(plan_path)
            plan_backup_paths.append(plan_backup_path)

            # Get the Short Identifier for this plan to determine results folder
            plan_df = ras_obj.plan_df
            plan_info = plan_df[plan_df['plan_number'] == plan_num]
            if not plan_info.empty:
                short_id = plan_info.iloc[0]['Short Identifier']
                if pd.notna(short_id) and short_id:
                    plan_results_folders[plan_num] = short_id
                else:
                    # Fallback: use plan number if no Short Identifier
                    plan_results_folders[plan_num] = f"Plan_{plan_num}"
                    logger.warning(f"Plan {plan_num} has no Short Identifier, using 'Plan_{plan_num}' as folder name")
            else:
                plan_results_folders[plan_num] = f"Plan_{plan_num}"
                logger.warning(f"Could not find plan {plan_num} in plan_df, using 'Plan_{plan_num}' as folder name")

        def _create_map_element(name, map_type, results_folder, profile_name="Max"):
            # Generate filename: "WSE (Max).vrt", "Depth (Max).vrt", etc.
            filename = f"{name} ({profile_name}).vrt"
            relative_path = f".\\{results_folder}\\{filename}"

            map_params = {
                "MapType": map_type,
                "OutputMode": "Stored Current Terrain",
                "StoredFilename": relative_path,  # Required for stored maps
                "ProfileIndex": "2147483647",
                "ProfileName": profile_name
            }

            # Create Layer element with Filename attribute
            layer_elem = ET.Element(
                'Layer',
                Name=name,
                Type="RASResultsMap",
                Checked="True",
                Filename=relative_path  # Required for stored maps
            )

            map_params_elem = ET.SubElement(layer_elem, 'MapParameters')
            for k, v in map_params.items():
                map_params_elem.set(k, str(v))
            return layer_elem

        try:
            # --- 1. Backup and Modify Plan Files ---
            for plan_num, plan_path, plan_backup_path in zip(plan_number_list, plan_paths, plan_backup_paths):
                logger.info(f"Backing up plan file {plan_path} to {plan_backup_path}")
                shutil.copy2(plan_path, plan_backup_path)
                
                logger.info(f"Updating plan run flags for floodplain mapping for plan {plan_num}...")
                RasPlan.update_run_flags(
                    plan_num,
                    geometry_preprocessor=False,
                    unsteady_flow_simulation=False,
                    post_processor=False,
                    floodplain_mapping=True, # Note: True maps to 0, which means "Run"
                    ras_object=ras_obj
                )

            # --- 2. Backup and Modify RASMAP File ---
            logger.info(f"Backing up rasmap file {rasmap_path} to {rasmap_backup_path}")
            shutil.copy2(rasmap_path, rasmap_backup_path)

            tree = ET.parse(rasmap_path)
            root = tree.getroot()
            
            results_section = root.find('Results')
            if results_section is None:
                raise ValueError(f"No <Results> section found in {rasmap_path}")

            # Process each plan's results layer
            for plan_num in plan_number_list:
                plan_hdf_part = f".p{plan_num}.hdf"
                results_layer = None
                for layer in results_section.findall("Layer[@Type='RASResults']"):
                    filename = layer.get("Filename")
                    if filename and plan_hdf_part.lower() in filename.lower():
                        results_layer = layer
                        break

                if results_layer is None:
                    logger.warning(f"Could not find RASResults layer for plan ending in '{plan_hdf_part}' in {rasmap_path}")
                    continue
                
                # Map user-provided layer names to HEC-RAS variable names and map types
                # Note: "WSE" is the correct HEC-RAS convention (not "WSEL")
                map_definitions = {
                    "WSE": "elevation",
                    "WSEL": "elevation",  # Accept both for backward compatibility, but use "WSE" in output
                    "Velocity": "velocity",
                    "Depth": "depth"
                }

                # Get the results folder for this plan
                results_folder = plan_results_folders.get(plan_num, f"Plan_{plan_num}")

                for layer_name in layers:
                    if layer_name in map_definitions:
                        map_type = map_definitions[layer_name]

                        # Convert WSEL to WSE for output (HEC-RAS convention)
                        output_name = "WSE" if layer_name == "WSEL" else layer_name

                        map_elem = _create_map_element(output_name, map_type, results_folder)
                        results_layer.append(map_elem)
                        logger.info(f"Added '{output_name}' stored map to results layer for plan {plan_num}.")

            if specify_terrain:
                terrains_elem = root.find('Terrains')
                if terrains_elem is not None:
                    for layer in list(terrains_elem):
                        if layer.get('Name') != specify_terrain:
                            terrains_elem.remove(layer)
                    logger.info(f"Filtered terrains, keeping only '{specify_terrain}'.")

            tree.write(rasmap_path, encoding='utf-8', xml_declaration=True)
            
            # --- 3. Execute HEC-RAS ---
            if auto_click_compute:
                # Use GUI automation to automatically click menu and Compute button
                logger.info("Using GUI automation to run floodplain mapping...")

                # Note: For multiple plans, we run the first plan's automation
                # The user can manually run additional plans if needed
                first_plan = plan_number_list[0]

                success = RasGuiAutomation.open_and_compute(
                    plan_number=first_plan,
                    ras_object=ras_obj,
                    auto_click_compute=True,
                    wait_for_user=True
                )

                if len(plan_number_list) > 1:
                    logger.info(f"Note: GUI automation ran plan {first_plan}. "
                               f"Please manually run remaining plans: {', '.join(plan_number_list[1:])}")

                if not success:
                    logger.error("Floodplain mapping computation failed.")
                    return False

            else:
                # Manual mode: Just open HEC-RAS and wait for user to execute
                logger.info("Opening HEC-RAS...")
                ras_exe = ras_obj.ras_exe_path
                prj_path = f'"{str(ras_obj.prj_file)}"'
                command = f"{ras_exe} {prj_path}"

                try:
                    import sys
                    import subprocess
                    if sys.platform == "win32":
                        hecras_process = subprocess.Popen(command)
                    else:
                        hecras_process = subprocess.Popen([ras_exe, prj_path])

                    logger.info(f"HEC-RAS opened with Process ID: {hecras_process.pid}")
                    logger.info(f"Please run plan(s) {', '.join(plan_number_list)} using the 'Compute Multiple' window in HEC-RAS to generate floodplain mapping results.")

                    # Wait for HEC-RAS to close
                    logger.info("Waiting for HEC-RAS to close...")
                    hecras_process.wait()
                    logger.info("HEC-RAS has closed")

                    success = True

                except Exception as e:
                    logger.error(f"Failed to launch HEC-RAS: {e}")
                    success = False

                if not success:
                    logger.error("Floodplain mapping computation failed.")
                    return False

            logger.info("Floodplain mapping computation successful.")
            return True
        
        except Exception as e:
            logger.error(f"Error in postprocess_stored_maps: {e}")
            return False

        finally:
            # --- 4. Restore Files ---
            for plan_path, plan_backup_path in zip(plan_paths, plan_backup_paths):
                if plan_backup_path.exists():
                    logger.info(f"Restoring original plan file from {plan_backup_path}")
                    shutil.move(plan_backup_path, plan_path)
            if rasmap_backup_path.exists():
                logger.info(f"Restoring original rasmap file from {rasmap_backup_path}")
                shutil.move(rasmap_backup_path, rasmap_path)

    @staticmethod
    @log_call
    def get_results_folder(plan_number: Union[str, int, float], ras_object=None) -> Path:
        """
        Get the folder path containing raster results for a specified plan.

        HEC-RAS creates output folders based on the plan's Short Identifier.
        Windows folder naming replaces special characters with underscores.

        Args:
            plan_number (Union[str, int, float]): Plan number (accepts flexible formats like 1, "01", "001").
            ras_object: Optional RAS object instance.

        Returns:
            Path: Path to the mapping output folder.

        Raises:
            ValueError: If the plan number is not found or output folder doesn't exist.

        Examples:
            >>> folder = RasMap.get_results_folder("01")
            >>> folder = RasMap.get_results_folder(1)
            >>> folder = RasMap.get_results_folder("08", ras_object=my_project)

        Notes:
            - Normalizes plan number to two-digit format ("01", "02", etc.)
            - Retrieves Short Identifier from plan_df
            - Normalizes Short ID for Windows folder naming (special chars -> underscores)
            - Searches project folder for matching output directory
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan number to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)

        # Get plan metadata from plan_df
        plan_df = ras_obj.plan_df
        plan_info = plan_df[plan_df['plan_number'] == plan_number]

        if plan_info.empty:
            raise ValueError(
                f"Plan {plan_number} not found in project. "
                f"Available plans: {list(plan_df['plan_number'])}"
            )

        short_id = plan_info.iloc[0]['Short Identifier']

        if pd.isna(short_id) or not short_id:
            raise ValueError(
                f"Plan {plan_number} does not have a Short Identifier. "
                "Check the plan file for missing metadata."
            )

        # Normalize Short ID to match Windows folder naming
        # RASMapper replaces special characters for Windows compatibility
        replacements = {
            '/': '_', '\\': '_', ':': '_', '*': '_',
            '?': '_', '"': '_', '<': '_', '>': '_',
            '|': '_', '+': '_', ' ': '_'
        }

        normalized = short_id
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)

        # Remove trailing underscores
        normalized = normalized.rstrip('_')

        # Search for output folder in project directory
        project_folder = ras_obj.project_folder

        # Try exact match with Short ID
        exact_match = project_folder / short_id
        if exact_match.exists() and exact_match.is_dir():
            logger.info(f"Found output folder (exact match): {exact_match}")
            return exact_match

        # Try normalized name
        normalized_match = project_folder / normalized
        if normalized_match.exists() and normalized_match.is_dir():
            logger.info(f"Found output folder (normalized): {normalized_match}")
            return normalized_match

        # Try partial match (contains)
        for item in project_folder.iterdir():
            if not item.is_dir():
                continue
            folder_name = item.name
            # Check if short_id is contained in folder name or vice versa
            if short_id in folder_name or folder_name in short_id:
                logger.info(f"Found output folder (partial match): {item}")
                return item
            # Check normalized version
            if normalized in folder_name or folder_name in normalized:
                logger.info(f"Found output folder (normalized partial match): {item}")
                return item

        # No folder found
        raise ValueError(
            f"Output folder not found for plan {plan_number} (Short ID: '{short_id}'). "
            f"Expected folder name: '{normalized}' in {project_folder}. "
            "Ensure the plan has been run and RASMapper has exported results."
        )

    @staticmethod
    @log_call
    def get_results_raster(
        plan_number: Union[str, int, float],
        variable_name: str,
        ras_object=None
    ) -> Path:
        """
        Get the .vrt file path for a specified plan and variable name.

        This function locates VRT (Virtual Raster) files exported by RASMapper
        for a specific hydraulic variable (e.g., WSE, Depth, Velocity).

        Args:
            plan_number (Union[str, int, float]): Plan number (accepts flexible formats).
            variable_name (str): Variable name to search for in VRT filenames (e.g., "WSE", "Depth", "Velocity").
            ras_object: Optional RAS object instance.

        Returns:
            Path: Path to the matching .vrt file.

        Raises:
            ValueError: If no matching files or multiple matching files are found.

        Examples:
            >>> vrt = RasMap.get_results_raster("01", "WSE")
            >>> vrt = RasMap.get_results_raster(1, "Depth")
            >>> vrt = RasMap.get_results_raster("08", "WSE (Max)", ras_object=my_project)

        Notes:
            - Uses get_results_folder() to locate the output directory
            - Searches for .vrt files containing the variable_name (case-insensitive)
            - If multiple files match, lists all matches and raises an error
            - User should make variable_name more specific to narrow results
            - VRT files are lightweight virtual rasters that reference underlying .tif tiles
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the mapping folder for this plan
        mapping_folder = RasMap.get_results_folder(plan_number, ras_obj)

        # List all .vrt files in the folder
        vrt_files = list(mapping_folder.glob("*.vrt"))

        if not vrt_files:
            raise ValueError(
                f"No .vrt files found in mapping folder: {mapping_folder}. "
                "Ensure RASMapper has exported raster results for this plan."
            )

        # Filter files containing variable_name (case-insensitive)
        matching_files = [
            f for f in vrt_files
            if variable_name.lower() in f.name.lower()
        ]

        # Handle results
        if len(matching_files) == 0:
            available_files = [f.name for f in vrt_files]
            raise ValueError(
                f"No .vrt files found matching variable name '{variable_name}' in {mapping_folder}. "
                f"Available files: {available_files}. "
                "Try making variable_name more specific or check for typos."
            )
        elif len(matching_files) == 1:
            logger.info(f"Found matching VRT file: {matching_files[0]}")
            return matching_files[0]
        else:
            # Multiple matches - print list and raise error
            logger.error(f"Multiple .vrt files match '{variable_name}':")
            for i, f in enumerate(matching_files, 1):
                logger.error(f"  {i}. {f.name}")

            raise ValueError(
                f"Multiple .vrt files ({len(matching_files)}) match variable name '{variable_name}'. "
                f"Matching files: {[f.name for f in matching_files]}. "
                "Please make variable_name more specific (e.g., 'WSE (Max)' instead of 'WSE')."
            )
==================================================

File: c:\GH\ras-commander\ras_commander\RasPlan.py
==================================================
"""
RasPlan - Operations for handling plan files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasPlan:
- set_geom(): Set the geometry for a specified plan
- set_steady(): Apply a steady flow file to a plan file
- set_unsteady(): Apply an unsteady flow file to a plan file
- set_num_cores(): Update the maximum number of cores to use
- set_geom_preprocessor(): Update geometry preprocessor settings
- clone_plan(): Create a new plan file based on a template
- clone_unsteady(): Copy unsteady flow files from a template
- clone_steady(): Copy steady flow files from a template
- clone_geom(): Copy geometry files from a template
- get_next_number(): Determine the next available number from a list
- get_plan_value(): Retrieve a specific value from a plan file
- get_results_path(): Get the results file path for a plan
- get_plan_path(): Get the full path for a plan number
- get_flow_path(): Get the full path for a flow number
- get_unsteady_path(): Get the full path for an unsteady number
- get_geom_path(): Get the full path for a geometry number
- update_run_flags(): Update various run flags in a plan file
- update_plan_intervals(): Update computation and output intervals
- update_plan_description(): Update the description in a plan file
- read_plan_description(): Read the description from a plan file
- update_simulation_date(): Update simulation start and end dates
- get_shortid(): Get the Short Identifier from a plan file
- set_shortid(): Set the Short Identifier in a plan file
- get_plan_title(): Get the Plan Title from a plan file
- set_plan_title(): Set the Plan Title in a plan file


        
"""
import os
import re
import logging
from pathlib import Path
import shutil
from typing import Union, Optional
from numbers import Number
import pandas as pd
from .RasPrj import RasPrj, ras
from .RasUtils import RasUtils
from pathlib import Path
from typing import Union, Any
from datetime import datetime

import logging
import re
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasPlan:
    """
    A class for operations on HEC-RAS plan files.
    """
    
    @staticmethod
    @log_call
    def set_geom(plan_number: Union[str, Number], new_geom: Union[str, Number], ras_object=None) -> pd.DataFrame:
        """
        Set the geometry for the specified plan by updating only the plan file.

        Parameters:
            plan_number (Union[str, Number]): The plan number to update (accepts int, float, numpy types, etc.).
            new_geom (Union[str, Number]): The new geometry number to set (accepts int, float, numpy types, etc.).
            ras_object: An optional RAS object instance.

        Returns:
            pd.DataFrame: The updated geometry DataFrame.

        Example:
            updated_geom_df = RasPlan.set_geom('02', '03')

        Note:
            This function updates the Geom File= line in the plan file and 
            updates the ras object's dataframes without modifying the PRJ file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan and geometry numbers to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        new_geom = RasUtils.normalize_ras_number(new_geom)

        # Update all dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        
        if new_geom not in ras_obj.geom_df['geom_number'].values:
            logger.error(f"Geometry {new_geom} not found in project.")
            raise ValueError(f"Geometry {new_geom} not found in project.")

        # Get the plan file path
        plan_file_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
        if not plan_file_path.exists():
            logger.error(f"Plan file not found: {plan_file_path}")
            raise ValueError(f"Plan file not found: {plan_file_path}")
        
        # Read the plan file and update the Geom File line
        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()
            
            for i, line in enumerate(lines):
                if line.startswith("Geom File="):
                    lines[i] = f"Geom File=g{new_geom}\n"
                    logger.info(f"Updated Geom File in plan file to g{new_geom} for plan {plan_number}")
                    break
                
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)
        except Exception as e:
            logger.error(f"Error updating plan file: {e}")
            raise
        # Update the plan_df without reinitializing
        mask = ras_obj.plan_df['plan_number'] == plan_number
        ras_obj.plan_df.loc[mask, 'geom_number'] = new_geom
        ras_obj.plan_df.loc[mask, 'geometry_number'] = new_geom  # Update geometry_number column
        ras_obj.plan_df.loc[mask, 'Geom File'] = f"g{new_geom}"
        geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom}"
        ras_obj.plan_df.loc[mask, 'Geom Path'] = str(geom_path)

        logger.info(f"Geometry for plan {plan_number} set to {new_geom}")
        logger.debug("Updated plan DataFrame:")
        logger.debug(ras_obj.plan_df)

        return ras_obj.plan_df

    @staticmethod
    @log_call
    def set_steady(plan_number: Union[str, Number], new_steady_flow_number: Union[str, Number], ras_object=None):
        """
        Apply a steady flow file to a plan file.

        Parameters:
        plan_number (Union[str, Number]): Plan number (e.g., '02', 2, or 2.0)
        new_steady_flow_number (Union[str, Number]): Steady flow number to apply (e.g., '01', 1, or 1.0)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If the specified steady flow number is not found in the project file
        FileNotFoundError: If the specified plan file is not found

        Example:
        >>> RasPlan.set_steady('02', '01')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan and flow numbers to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        new_steady_flow_number = RasUtils.normalize_ras_number(new_steady_flow_number)

        ras_obj.flow_df = ras_obj.get_flow_entries()

        if new_steady_flow_number not in ras_obj.flow_df['flow_number'].values:
            raise ValueError(f"Steady flow number {new_steady_flow_number} not found in project file.")
        
        plan_file_path = RasPlan.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}")
        
        try:
            RasUtils.update_file(plan_file_path, RasPlan._update_steady_in_file, new_steady_flow_number)
            
            # Update all dataframes
            ras_obj.plan_df = ras_obj.get_plan_entries()
            
            # Update flow-related columns
            mask = ras_obj.plan_df['plan_number'] == plan_number
            flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{new_steady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow File'] = f"f{new_steady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow Path'] = str(flow_path)
            ras_obj.plan_df.loc[mask, 'unsteady_number'] = None
            
            # Update remaining dataframes
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            
        except Exception as e:
            raise IOError(f"Failed to update steady flow file: {e}")

    @staticmethod
    def _update_steady_in_file(lines, new_steady_flow_number):
        return [f"Flow File=f{new_steady_flow_number}\n" if line.startswith("Flow File=f") else line for line in lines]

    @staticmethod
    @log_call
    def set_unsteady(plan_number: Union[str, Number], new_unsteady_flow_number: Union[str, Number], ras_object=None):
        """
        Apply an unsteady flow file to a plan file.

        Parameters:
        plan_number (Union[str, Number]): Plan number (e.g., '04', 4, or 4.0)
        new_unsteady_flow_number (Union[str, Number]): Unsteady flow number to apply (e.g., '01', 1, or 1.0)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If the specified unsteady number is not found in the project file
        FileNotFoundError: If the specified plan file is not found

        Example:
        >>> RasPlan.set_unsteady('04', '01')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan and unsteady flow numbers to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        new_unsteady_flow_number = RasUtils.normalize_ras_number(new_unsteady_flow_number)

        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        if new_unsteady_flow_number not in ras_obj.unsteady_df['unsteady_number'].values:
            raise ValueError(f"Unsteady number {new_unsteady_flow_number} not found in project file.")
        
        plan_file_path = RasPlan.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}")
        
        try:
            # Read the plan file
            with open(plan_file_path, 'r') as f:
                lines = f.readlines()

            # Update the Flow File line
            for i, line in enumerate(lines):
                if line.startswith("Flow File="):
                    lines[i] = f"Flow File=u{new_unsteady_flow_number}\n"
                    break
            
            # Write back to the plan file
            with open(plan_file_path, 'w') as f:
                f.writelines(lines)
            
            # Update all dataframes
            ras_obj.plan_df = ras_obj.get_plan_entries()
            
            # Update flow-related columns
            mask = ras_obj.plan_df['plan_number'] == plan_number
            flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow File'] = f"u{new_unsteady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow Path'] = str(flow_path)
            ras_obj.plan_df.loc[mask, 'unsteady_number'] = new_unsteady_flow_number
            
            # Update remaining dataframes
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            
        except Exception as e:
            raise IOError(f"Failed to update unsteady flow file: {e}")

    @staticmethod
    def _update_unsteady_in_file(lines, new_unsteady_flow_number):
        return [f"Unsteady File=u{new_unsteady_flow_number}\n" if line.startswith("Unsteady File=u") else line for line in lines]
    
    @staticmethod
    @log_call
    def set_num_cores(plan_number: Union[str, Number], num_cores: int, ras_object=None):
        """
        Update the maximum number of cores to use in the HEC-RAS plan file.

        Parameters:
        plan_number (Union[str, Number]): Plan number (e.g., '02', 2, or 2.0) or full path to the plan file
        num_cores (int): Maximum number of cores to use
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Number of cores is controlled by the following parameters in the plan file corresponding to 1D, 2D, Pipe Systems and Pump Stations:
        UNET D1 Cores=  
        UNET D2 Cores=
        PS Cores=

        Where a value of "0" is used for "All Available" cores, and values of 1 or more are used to specify the number of cores to use.
        For complex 1D/2D models with pipe systems, a more complex approach may be needed to optimize performance.  (Suggest writing a custom function based on this code).
        This function simply sets the "num_cores" parameter for ALL instances of the above parameters in the plan file.


        Notes on setting num_cores in HEC-RAS:
        The recommended setting for num_cores is 2 (most efficient) to 8 (most performant)
        More details in the HEC-Commander Repository Blog "Benchmarking is All You Need"
        https://github.com/billk-FM/HEC-Commander/blob/main/Blog/7._Benchmarking_Is_All_You_Need.md
        
        Microsoft Windows has a maximum of 64 cores that can be allocated to a single Ras.exe process. 

        Example:
        >>> # Using plan number
        >>> RasPlan.set_num_cores('02', 4)
        >>> # Using full path to plan file
        >>> RasPlan.set_num_cores('/path/to/project.p02', 4)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        plan_file_path = RasUtils.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}. Please provide a valid plan number or path.")
        
        def update_num_cores(lines):
            updated_lines = []
            for line in lines:
                if any(param in line for param in ["UNET D1 Cores=", "UNET D2 Cores=", "PS Cores="]):
                    param_name = line.split("=")[0]
                    updated_lines.append(f"{param_name}= {num_cores}\n")
                else:
                    updated_lines.append(line)
            return updated_lines
        
        try:
            RasUtils.update_file(plan_file_path, update_num_cores)
        except Exception as e:
            raise IOError(f"Failed to update number of cores in plan file: {e}")
        
        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def set_geom_preprocessor(file_path, run_htab, use_ib_tables, ras_object=None):
        """
        Update the simulation plan file to modify the `Run HTab` and `UNET Use Existing IB Tables` settings.
        
        Parameters:
        file_path (str): Path to the simulation plan file (.p06 or similar) that you want to modify.
        run_htab (int): Value for the `Run HTab` setting:
            - `0` : Do not run the geometry preprocessor, use existing geometry tables.
            - `-1` : Run the geometry preprocessor, forcing a recomputation of the geometry tables.
        use_ib_tables (int): Value for the `UNET Use Existing IB Tables` setting:
            - `0` : Use existing interpolation/boundary (IB) tables without recomputing them.
            - `-1` : Do not use existing IB tables, force a recomputation.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If `run_htab` or `use_ib_tables` are not integers or not within the accepted values (`0` or `-1`).
        FileNotFoundError: If the specified file does not exist.
        IOError: If there is an error reading or writing the file.

        Example:
        >>> RasPlan.set_geom_preprocessor('/path/to/project.p06', run_htab=-1, use_ib_tables=0)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        if run_htab not in [-1, 0]:
            raise ValueError("Invalid value for `Run HTab`. Expected `0` or `-1`.")
        if use_ib_tables not in [-1, 0]:
            raise ValueError("Invalid value for `UNET Use Existing IB Tables`. Expected `0` or `-1`.")
        
        def update_geom_preprocessor(lines, run_htab, use_ib_tables):
            updated_lines = []
            for line in lines:
                if line.lstrip().startswith("Run HTab="):
                    updated_lines.append(f"Run HTab= {run_htab} \n")
                elif line.lstrip().startswith("UNET Use Existing IB Tables="):
                    updated_lines.append(f"UNET Use Existing IB Tables= {use_ib_tables} \n")
                else:
                    updated_lines.append(line)
            return updated_lines
        
        try:
            RasUtils.update_file(file_path, update_geom_preprocessor, run_htab, use_ib_tables)
        except FileNotFoundError:
            raise FileNotFoundError(f"The file '{file_path}' does not exist.")
        except IOError as e:
            raise IOError(f"An error occurred while reading or writing the file: {e}")

        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def get_results_path(plan_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Retrieve the results file path for a given HEC-RAS plan number.

        Args:
            plan_number (Union[str, Number]): The HEC-RAS plan number for which to find the results path (e.g., '02', 2, or 2.0).
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[str]: The full path to the results file if found and the file exists, or None if not found.

        Raises:
            RuntimeError: If the project is not initialized.

        Example:
            >>> ras_plan = RasPlan()
            >>> results_path = ras_plan.get_results_path('01')
            >>> if results_path:
            ...     print(f"Results file found at: {results_path}")
            ... else:
            ...     print("Results file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Update the plan dataframe in the ras instance to ensure it is current
        ras_obj.plan_df = ras_obj.get_plan_entries()

        # Normalize plan number to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        
        plan_entry = ras_obj.plan_df[ras_obj.plan_df['plan_number'] == plan_number]
        if not plan_entry.empty:
            results_path = plan_entry['HDF_Results_Path'].iloc[0]
            if results_path and Path(results_path).exists():
                return results_path
            else:
                return None
        else:
            return None

    @staticmethod
    @log_call
    def get_plan_path(plan_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given plan number.

        This method ensures that the latest plan entries are included by refreshing
        the plan dataframe before searching for the requested plan number.

        Args:
        plan_number (Union[str, Number]): The plan number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the plan file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> plan_path = ras_plan.get_plan_path('01')
        >>> if plan_path:
        ...     print(f"Plan file found at: {plan_path}")
        ... else:
        ...     print("Plan file not found.")
        >>> # Integer input also works
        >>> plan_path = ras_plan.get_plan_path(1)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan number to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        
        plan_df = ras_obj.get_plan_entries()
        
        plan_path = plan_df[plan_df['plan_number'] == plan_number]
        
        if not plan_path.empty:
            if 'full_path' in plan_path.columns and not pd.isna(plan_path['full_path'].iloc[0]):
                return plan_path['full_path'].iloc[0]
            else:
                # Fallback to constructing path
                return str(ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}")
        return None

    @staticmethod
    @log_call
    def get_flow_path(flow_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given flow number.

        Args:
        flow_number (Union[str, Number]): The flow number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the flow file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> flow_path = ras_plan.get_flow_path('01')
        >>> if flow_path:
        ...     print(f"Flow file found at: {flow_path}")
        ... else:
        ...     print("Flow file not found.")
        >>> # Integer input also works
        >>> flow_path = ras_plan.get_flow_path(1)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize flow number to two-digit format
        flow_number = RasUtils.normalize_ras_number(flow_number)
        
        # Use updated flow dataframe
        ras_obj.flow_df = ras_obj.get_prj_entries('Flow')
        
        flow_path = ras_obj.flow_df[ras_obj.flow_df['flow_number'] == flow_number]
        if not flow_path.empty:
            full_path = flow_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_unsteady_path(unsteady_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given unsteady number.

        Args:
        unsteady_number (Union[str, Number]): The unsteady number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the unsteady file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> unsteady_path = ras_plan.get_unsteady_path('01')
        >>> if unsteady_path:
        ...     print(f"Unsteady file found at: {unsteady_path}")
        ... else:
        ...     print("Unsteady file not found.")
        >>> # Integer input also works
        >>> unsteady_path = ras_plan.get_unsteady_path(1)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize unsteady number to two-digit format
        unsteady_number = RasUtils.normalize_ras_number(unsteady_number)
        
        # Use updated unsteady dataframe
        ras_obj.unsteady_df = ras_obj.get_prj_entries('Unsteady')
        
        unsteady_path = ras_obj.unsteady_df[ras_obj.unsteady_df['unsteady_number'] == unsteady_number]
        if not unsteady_path.empty:
            full_path = unsteady_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_geom_path(geom_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given geometry number.

        Args:
        geom_number (Union[str, Number]): The geometry number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the geometry file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> geom_path = ras_plan.get_geom_path('01')
        >>> if geom_path:
        ...     print(f"Geometry file found at: {geom_path}")
        ... else:
        ...     print("Geometry file not found.")
        >>> # Integer input also works
        >>> geom_path = ras_plan.get_geom_path(1)
        """
        logger = get_logger(__name__)

        if geom_number is None:
            logger.warning("Provided geometry number is None")
            return None

        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()

            # Normalize geometry number to two-digit format
            geom_number = RasUtils.normalize_ras_number(geom_number)
            
            # Use updated geom dataframe
            ras_obj.geom_df = ras_obj.get_prj_entries('Geom')
            
            # Find the geometry file path
            geom_path = ras_obj.geom_df[ras_obj.geom_df['geom_number'] == geom_number]
            if not geom_path.empty:
                if 'full_path' in geom_path.columns and pd.notna(geom_path['full_path'].iloc[0]):
                    full_path = geom_path['full_path'].iloc[0]
                    logger.info(f"Found geometry path: {full_path}")
                    return full_path
                else:
                    # Fallback to constructing path
                    constructed_path = str(ras_obj.project_folder / f"{ras_obj.project_name}.g{geom_number}")
                    logger.info(f"Constructed geometry path: {constructed_path}")
                    return constructed_path
            else:
                logger.warning(f"No geometry file found with number: {geom_number}")
                return None
        except Exception as e:
            logger.error(f"Error in get_geom_path: {str(e)}")
            return None

    # Clone Functions to copy unsteady, flow, and geometry files from templates

    @staticmethod
    @log_call
    def clone_plan(template_plan: Union[str, Number], new_shortid=None, new_plan_shortid=None, new_title=None, ras_object=None):
        """
        Create a new plan file based on a template and update the project file.

        Parameters:
        template_plan (Union[str, Number]): Plan number to use as template (e.g., '01', 1, or 1.0)
        new_shortid (str, optional): New short identifier for the plan file (max 24 chars).
                                     If not provided, appends '_copy' to original.
                                     Alias: new_plan_shortid (for improved clarity)
        new_plan_shortid (str, optional): Alias for new_shortid. If both are provided,
                                          new_plan_shortid takes precedence.
        new_title (str, optional): New plan title (max 32 chars, updates "Plan Title=" line).
                                   If not provided, keeps original title.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New plan number

        Example:
        >>> # Clone with default shortid and title (string input)
        >>> new_plan = RasPlan.clone_plan('01')
        >>>
        >>> # Clone with integer input
        >>> new_plan = RasPlan.clone_plan(1)
        >>>
        >>> # Clone with custom shortid and title (either parameter name works)
        >>> new_plan = RasPlan.clone_plan('01', new_shortid='Steady_v41',
        ...                               new_title='Steady Flow - HEC-RAS 4.1')
        >>> new_plan = RasPlan.clone_plan('01', new_plan_shortid='Steady_v41',
        ...                               new_title='Steady Flow - HEC-RAS 4.1')

        Note:
            Both new_shortid and new_title are optional.
            new_plan_shortid is an alias for new_shortid for improved clarity.
            This function updates the ras object's dataframes after modifying the project structure.
        """
        # Handle parameter aliasing: new_plan_shortid takes precedence if both provided
        if new_plan_shortid is not None:
            new_shortid = new_plan_shortid

        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan number to two-digit format
        template_plan = RasUtils.normalize_ras_number(template_plan)

        # Validate new_title length if provided
        if new_title is not None and len(new_title) > 32:
            raise ValueError(
                f"Plan title must be 32 characters or less. "
                f"Got {len(new_title)} characters: '{new_title}'"
            )

        # Update plan entries without reinitializing the entire project
        ras_obj.plan_df = ras_obj.get_prj_entries('Plan')

        new_plan_num = RasPlan.get_next_number(ras_obj.plan_df['plan_number'])
        template_plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{template_plan}"
        new_plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{new_plan_num}"

        def update_plan_metadata(lines):
            """Update both Plan Title and Short Identifier"""
            title_pattern = re.compile(r'^Plan Title=(.*)$', re.IGNORECASE)
            shortid_pattern = re.compile(r'^Short Identifier=(.*)$', re.IGNORECASE)

            for i, line in enumerate(lines):
                # Update Plan Title if new_title provided
                title_match = title_pattern.match(line.strip())
                if title_match and new_title is not None:
                    lines[i] = f"Plan Title={new_title[:32]}\n"
                    continue

                # Update Short Identifier
                shortid_match = shortid_pattern.match(line.strip())
                if shortid_match:
                    current_shortid = shortid_match.group(1)
                    if new_shortid is None:
                        new_shortid_value = (current_shortid + "_copy")[:24]
                    else:
                        new_shortid_value = new_shortid[:24]
                    lines[i] = f"Short Identifier={new_shortid_value}\n"

            return lines

        # Use RasUtils to clone the file and update metadata
        RasUtils.clone_file(template_plan_path, new_plan_path, update_plan_metadata)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Plan', new_plan_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_plan_num

    @staticmethod
    @log_call
    def clone_unsteady(template_unsteady: Union[str, Number], new_title=None, ras_object=None):
        """
        Copy unsteady flow files from a template, find the next unsteady number,
        and update the project file accordingly.

        Parameters:
        template_unsteady (Union[str, Number]): Unsteady flow number to use as template (e.g., '01', 1, or 1.0)
        new_title (str, optional): New flow title (max 32 chars, updates "Flow Title=" line)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New unsteady flow number (e.g., '03')

        Example:
        >>> # String input
        >>> new_unsteady_num = RasPlan.clone_unsteady('01',
        ...                                           new_title='Unsteady - HEC-RAS 4.1')
        >>> print(f"New unsteady flow file created: u{new_unsteady_num}")
        >>> # Integer input also works
        >>> new_unsteady_num = RasPlan.clone_unsteady(1)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize unsteady number to two-digit format
        template_unsteady = RasUtils.normalize_ras_number(template_unsteady)

        # Validate new_title length if provided
        if new_title is not None and len(new_title) > 32:
            raise ValueError(
                f"Flow title must be 32 characters or less. "
                f"Got {len(new_title)} characters: '{new_title}'"
            )

        # Update unsteady entries without reinitializing the entire project
        ras_obj.unsteady_df = ras_obj.get_prj_entries('Unsteady')

        new_unsteady_num = RasPlan.get_next_number(ras_obj.unsteady_df['unsteady_number'])
        template_unsteady_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{template_unsteady}"
        new_unsteady_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_num}"

        def update_flow_title(lines):
            """Update Flow Title if new_title provided"""
            if new_title is None:
                return lines

            title_pattern = re.compile(r'^Flow Title=(.*)$', re.IGNORECASE)
            for i, line in enumerate(lines):
                title_match = title_pattern.match(line.strip())
                if title_match:
                    lines[i] = f"Flow Title={new_title[:32]}\n"
                    break
            return lines

        # Use RasUtils to clone the file and update flow title
        RasUtils.clone_file(template_unsteady_path, new_unsteady_path, update_flow_title)

        # Copy the corresponding .hdf file if it exists
        template_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{template_unsteady}.hdf"
        new_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_num}.hdf"
        if template_hdf_path.exists():
            shutil.copy(template_hdf_path, new_hdf_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Unsteady', new_unsteady_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_unsteady_num


    @staticmethod
    @log_call
    def clone_steady(template_flow: Union[str, Number], new_title=None, ras_object=None):
        """
        Copy steady flow files from a template, find the next flow number,
        and update the project file accordingly.

        Parameters:
        template_flow (Union[str, Number]): Flow number to use as template (e.g., '01', 1, or 1.0)
        new_title (str, optional): New flow title (max 32 chars, updates "Flow Title=" line)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New flow number (e.g., '03')

        Example:
        >>> # String input
        >>> new_flow_num = RasPlan.clone_steady('01',
        ...                                      new_title='Steady Flow - HEC-RAS 4.1')
        >>> print(f"New steady flow file created: f{new_flow_num}")
        >>> # Integer input also works
        >>> new_flow_num = RasPlan.clone_steady(1)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize flow number to two-digit format
        template_flow = RasUtils.normalize_ras_number(template_flow)

        # Validate new_title length if provided
        if new_title is not None and len(new_title) > 32:
            raise ValueError(
                f"Flow title must be 32 characters or less. "
                f"Got {len(new_title)} characters: '{new_title}'"
            )

        # Update flow entries without reinitializing the entire project
        ras_obj.flow_df = ras_obj.get_prj_entries('Flow')

        new_flow_num = RasPlan.get_next_number(ras_obj.flow_df['flow_number'])
        template_flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{template_flow}"
        new_flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{new_flow_num}"

        def update_flow_title(lines):
            """Update Flow Title if new_title provided"""
            if new_title is None:
                return lines

            title_pattern = re.compile(r'^Flow Title=(.*)$', re.IGNORECASE)
            for i, line in enumerate(lines):
                title_match = title_pattern.match(line.strip())
                if title_match:
                    lines[i] = f"Flow Title={new_title[:32]}\n"
                    break
            return lines

        # Use RasUtils to clone the file and update flow title
        RasUtils.clone_file(template_flow_path, new_flow_path, update_flow_title)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Flow', new_flow_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)
        
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        return new_flow_num

    @staticmethod
    @log_call
    def clone_geom(template_geom: Union[str, Number], ras_object=None):
        """
        Copy geometry files from a template, find the next geometry number,
        and update the project file accordingly.

        Parameters:
        template_geom (Union[str, Number]): Geometry number to use as template (e.g., '01', 1, or 1.0)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New geometry number (e.g., '03')

        Example:
        >>> # String input
        >>> new_geom_num = RasPlan.clone_geom('01')
        >>> # Integer input also works
        >>> new_geom_num = RasPlan.clone_geom(1)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize geometry number to two-digit format
        template_geom = RasUtils.normalize_ras_number(template_geom)

        # Update geometry entries without reinitializing the entire project
        ras_obj.geom_df = ras_obj.get_prj_entries('Geom')

        new_geom_num = RasPlan.get_next_number(ras_obj.geom_df['geom_number'])
        template_geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{template_geom}"
        new_geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom_num}"

        # Use RasUtils to clone the file
        RasUtils.clone_file(template_geom_path, new_geom_path)

        # Handle HDF file copy
        template_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{template_geom}.hdf"
        new_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom_num}.hdf"
        if template_hdf_path.is_file():
            RasUtils.clone_file(template_hdf_path, new_hdf_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Geom', new_geom_num, ras_object=ras_obj)

        # Update all dataframes in the ras object
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_geom_num

    @staticmethod
    @log_call
    def get_next_number(existing_numbers):
        """
        Determine the next available number from a list of existing numbers.
        
        Parameters:
        existing_numbers (list): List of existing numbers as strings
        
        Returns:
        str: Next available number as a zero-padded string
        
        Example:
        >>> existing_numbers = ['01', '02', '04']
        >>> RasPlan.get_next_number(existing_numbers)
        '03'
        >>> existing_numbers = ['01', '02', '03']
        >>> RasPlan.get_next_number(existing_numbers)
        '04'
        """
        existing_numbers = sorted(int(num) for num in existing_numbers)
        next_number = 1
        for num in existing_numbers:
            if num == next_number:
                next_number += 1
            else:
                break
        return f"{next_number:02d}"

    @staticmethod
    @log_call
    def get_plan_value(
        plan_number_or_path: Union[str, Path],
        key: str,
        ras_object=None
    ) -> Any:
        """
        Retrieve a specific value from a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        key (str): The key to retrieve from the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Any: The value associated with the specified key

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading the plan file

        Available keys and their expected types:
        - 'Computation Interval' (str): Time value for computational time step (e.g., '5SEC', '2MIN')
        - 'DSS File' (str): Name of the DSS file used
        - 'Flow File' (str): Name of the flow input file
        - 'Friction Slope Method' (int): Method selection for friction slope (e.g., 1, 2)
        - 'Geom File' (str): Name of the geometry input file
        - 'Mapping Interval' (str): Time interval for mapping output
        - 'Plan File' (str): Name of the plan file
        - 'Plan Title' (str): Title of the simulation plan
        - 'Program Version' (str): Version number of HEC-RAS
        - 'Run HTab' (int): Flag to run HTab module (-1 or 1)
        - 'Run Post Process' (int): Flag to run post-processing (-1 or 1)
        - 'Run Sediment' (int): Flag to run sediment transport module (0 or 1)
        - 'Run UNET' (int): Flag to run unsteady network module (-1 or 1)
        - 'Run WQNET' (int): Flag to run water quality module (0 or 1)
        - 'Short Identifier' (str): Short name or ID for the plan
        - 'Simulation Date' (str): Start and end dates/times for simulation
        - 'UNET D1 Cores' (int): Number of cores used in 1D calculations
        - 'UNET D2 Cores' (int): Number of cores used in 2D calculations
        - 'PS Cores' (int): Number of cores used in parallel simulation
        - 'UNET Use Existing IB Tables' (int): Flag for using existing internal boundary tables (-1, 0, or 1)
        - 'UNET 1D Methodology' (str): 1D calculation methodology
        - 'UNET D2 Solver Type' (str): 2D solver type
        - 'UNET D2 Name' (str): Name of the 2D area
        - 'Run RASMapper' (int): Flag to run RASMapper for floodplain mapping (-1 for off, 0 for on)
        
        Note: 
        Writing Multi line keys like 'Description' are not supported by this function.

        Example:
        >>> computation_interval = RasPlan.get_plan_value("01", "Computation Interval")
        >>> print(f"Computation interval: {computation_interval}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        supported_plan_keys = {
            'Description', 'Computation Interval', 'DSS File', 'Flow File', 'Friction Slope Method',
            'Geom File', 'Mapping Interval', 'Plan File', 'Plan Title', 'Program Version',
            'Run HTab', 'Run Post Process', 'Run Sediment', 'Run UNET', 'Run WQNET',
            'Short Identifier', 'Simulation Date', 'UNET D1 Cores', 'UNET D2 Cores', 'PS Cores',
            'UNET Use Existing IB Tables', 'UNET 1D Methodology', 'UNET D2 Solver Type', 
            'UNET D2 Name', 'Run RASMapper', 'Run HTab', 'Run UNET'
        }

        if key not in supported_plan_keys:
            logger = logging.getLogger(__name__)
            logger.warning(f"Unknown key: {key}. Valid keys are: {', '.join(supported_plan_keys)}\n Add more keys and explanations in get_plan_value() as needed.")

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()
        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        # Handle core settings specially to convert to integers
        core_keys = {'UNET D1 Cores', 'UNET D2 Cores', 'PS Cores'}
        if key in core_keys:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                try:
                    return int(match.group(1).strip())
                except ValueError:
                    logger = logging.getLogger(__name__)
                    logger.error(f"Could not convert {key} value to integer")
                    return None
            else:
                logger = logging.getLogger(__name__)
                logger.error(f"Key '{key}' not found in the plan file.")
                return None
        elif key == 'Description':
            match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            return match.group(1).strip() if match else None
        else:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()
            else:
                logger = logging.getLogger(__name__)
                logger.error(f"Key '{key}' not found in the plan file.")
                return None





    @staticmethod
    @log_call
    def update_run_flags(
        plan_number_or_path: Union[str, Path],
        geometry_preprocessor: bool = None,
        unsteady_flow_simulation: bool = None,
        run_sediment: bool = None,
        post_processor: bool = None,
        floodplain_mapping: bool = None,
        ras_object=None
    ) -> None:
        """
        Update the run flags in a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        geometry_preprocessor (bool, optional): Set Geometry Preprocessor (Run HTab, -1 = ON, 0 = OFF)
        unsteady_flow_simulation (bool, optional): Set Unsteady Flow (Run UNet, -1 = ON, 0 = OFF)
        run_sediment (bool, optional): Set Run Sediment (Run Sediment, -1 = ON, 0 = OFF)
        post_processor (bool, optional): Set Post Processor (Run PostProcess, -1 = ON, 0 = OFF)
        floodplain_mapping (bool, optional): Set Floodplain Mapping (Run RASMapper, -1 = ON, 0 = OFF)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading or writing the plan file

        Notes:
        - -1 is ON, 0 is OFF
        - Lines affected in plan file:
            Run HTab= -1           # geometry_preprocessor
            Run UNet= -1           # unsteady_flow_simulation
            Run Sediment= 0        # run_sediment
            Run PostProcess= -1    # post_processor
            Run RASMapper= 0       # floodplain_mapping

        Example:
        >>> RasPlan.update_run_flags("01", geometry_preprocessor=True, unsteady_flow_simulation=True, run_sediment=False, post_processor=True, floodplain_mapping=False)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        # Map arguments to plan keys (string in file : argument, ON=-1, OFF=0)
        flag_map = [
            ("Run HTab", geometry_preprocessor),
            ("Run UNet", unsteady_flow_simulation),
            ("Run Sediment", run_sediment),
            ("Run PostProcess", post_processor),
            ("Run RASMapper", floodplain_mapping)
        ]

        try:
            with open(plan_file_path, 'r') as f:
                lines = f.readlines()

            # Annotate which flags got edited for logger
            updated_lines = 0

            for flag, value in flag_map:
                if value is not None:
                    # Find and update the line
                    found = False
                    for idx, line in enumerate(lines):
                        if line.strip().startswith(f"{flag}="):
                            lines[idx] = f"{flag}= {-1 if value else 0}\n"
                            updated_lines += 1
                            found = True
                            break
                    if not found:
                        # If not present, add the line at end (optional; original HEC-RAS behavior retains missing as OFF)
                        lines.append(f"{flag}= {-1 if value else 0}\n")
                        updated_lines += 1

            with open(plan_file_path, 'w') as f:
                f.writelines(lines)

            logger = get_logger(__name__)
            logger.info(
                f"Successfully updated run flags in plan file: {plan_file_path} "
                f"(flags modified: {updated_lines})"
            )

        except IOError as e:
            logger = get_logger(__name__)
            logger.error(f"Error updating run flags in plan file {plan_file_path}: {e}")
            raise


    @staticmethod
    @log_call
    def update_plan_intervals(
        plan_number_or_path: Union[str, Path],
        computation_interval: Optional[str] = None,
        output_interval: Optional[str] = None,
        instantaneous_interval: Optional[str] = None,
        mapping_interval: Optional[str] = None,
        ras_object=None
    ) -> None:
        """
        Update the computation and output intervals in a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        computation_interval (Optional[str]): The new computation interval. Valid entries include:
            '1SEC', '2SEC', '3SEC', '4SEC', '5SEC', '6SEC', '10SEC', '15SEC', '20SEC', '30SEC',
            '1MIN', '2MIN', '3MIN', '4MIN', '5MIN', '6MIN', '10MIN', '15MIN', '20MIN', '30MIN',
            '1HOUR', '2HOUR', '3HOUR', '4HOUR', '6HOUR', '8HOUR', '12HOUR', '1DAY'
        output_interval (Optional[str]): The new output interval. Valid entries are the same as computation_interval.
        instantaneous_interval (Optional[str]): The new instantaneous interval. Valid entries are the same as computation_interval.
        mapping_interval (Optional[str]): The new mapping interval. Valid entries are the same as computation_interval.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
        ValueError: If the plan file is not found or if an invalid interval is provided
        IOError: If there's an error reading or writing the plan file

        Note: This function does not check if the intervals are equal divisors. Ensure you use valid values from HEC-RAS.

        Example:
        >>> RasPlan.update_plan_intervals("01", computation_interval="5SEC", output_interval="1MIN", instantaneous_interval="1HOUR", mapping_interval="5MIN")
        >>> RasPlan.update_plan_intervals("/path/to/plan.p01", computation_interval="10SEC", output_interval="30SEC")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        valid_intervals = [
            '1SEC', '2SEC', '3SEC', '4SEC', '5SEC', '6SEC', '10SEC', '15SEC', '20SEC', '30SEC',
            '1MIN', '2MIN', '3MIN', '4MIN', '5MIN', '6MIN', '10MIN', '15MIN', '20MIN', '30MIN',
            '1HOUR', '2HOUR', '3HOUR', '4HOUR', '6HOUR', '8HOUR', '12HOUR', '1DAY'
        ]

        interval_mapping = {
            'Computation Interval': computation_interval,
            'Output Interval': output_interval,
            'Instantaneous Interval': instantaneous_interval,
            'Mapping Interval': mapping_interval
        }

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            for i, line in enumerate(lines):
                for key, value in interval_mapping.items():
                    if value is not None:
                        if value.upper() not in valid_intervals:
                            raise ValueError(f"Invalid {key}: {value}. Must be one of {valid_intervals}")
                        if line.strip().startswith(key):
                            lines[i] = f"{key}={value.upper()}\n"

            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger = logging.getLogger(__name__)
            logger.info(f"Successfully updated intervals in plan file: {plan_file_path}")

        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error updating intervals in plan file {plan_file_path}: {e}")
            raise
     
     




    @staticmethod
    @log_call
    def read_plan_description(plan_number_or_path: Union[str, Path], ras_object: Optional['RasPrj'] = None) -> str:
        """
        Read the description from the plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The description from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.
        """
        logger = logging.getLogger(__name__)

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()
        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        description_lines = []
        in_description = False
        description_found = False
        for line in lines:
            if line.strip() == "BEGIN DESCRIPTION:":
                in_description = True
                description_found = True
            elif line.strip() == "END DESCRIPTION:":
                break
            elif in_description:
                description_lines.append(line.strip())

        if not description_found:
            logger.warning(f"No description found in plan file: {plan_file_path}")
            return ""

        description = '\n'.join(description_lines)
        logger.info(f"Read description from plan file: {plan_file_path}")
        return description


    @staticmethod
    @log_call
    def update_plan_description(plan_number: Union[str, Number], description: str, ras_object=None):
        """
        Update or insert plan description in the correct location within a plan file.

        The description block will be placed after initial plan parameters
        (Plan Title, Program Version, Short Identifier, Simulation Date, Geom File,
        Flow File, and flow type) but before the Computation Interval line.

        Parameters:
        -----------
        plan_number : Union[str, Number]
            Plan number to update (e.g., '01', 1, or 1.0)
        description : str
            Description text to insert. Will be automatically wrapped in
            BEGIN DESCRIPTION/END DESCRIPTION blocks.
        ras_object : RasPrj, optional
            RAS project object. If None, uses global 'ras' object.
        
        Returns:
        --------
        bool : True if successful, False otherwise
        
        Examples:
        ---------
        >>> RasPlan.update_plan_description('02', 
        ...     'Atlas 14 Uncertainty Analysis\\n' +
        ...     'AEP: 100 years\\n' +
        ...     'Duration: 24 hours\\n' +
        ...     'Confidence Level: upper')
        True
        """
        try:
            # Get the RAS object
            if ras_object is None:
                ras_obj = ras
            else:
                ras_obj = ras_object
            
            # Get plan path
            plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_obj)
            
            # Read the plan file
            with open(plan_path, 'r') as f:
                lines = f.readlines()
            
            # Find existing description block if it exists
            desc_start_idx = None
            desc_end_idx = None
            
            for i, line in enumerate(lines):
                if line.strip().upper().startswith('BEGIN DESCRIPTION'):
                    desc_start_idx = i
                elif line.strip().upper().startswith('END DESCRIPTION'):
                    desc_end_idx = i
                    break
            
            # Find the correct insertion point (before Computation Interval)
            insertion_idx = None
            
            # Primary method: Find Computation Interval line
            for i, line in enumerate(lines):
                if line.strip().startswith('Computation Interval='):
                    insertion_idx = i
                    break
            
            # Fallback method 1: Look for common parameter lines that come after description
            if insertion_idx is None:
                fallback_markers = [
                    'K Sum by GR=',
                    'Std Step Tol=',
                    'Critical Tol=',
                    'Num of Std Step Trials=',
                    'Max Error Tol=',
                    'Flow Tol Ratio=',
                    'Split Flow NTrial=',
                    'Split Flow Tol=',
                    'Split Flow Ratio=',
                    'Log Output Level=',
                    'Friction Slope Method=',
                    'Unsteady Friction Slope Method='
                ]
                
                for i, line in enumerate(lines):
                    for marker in fallback_markers:
                        if line.strip().startswith(marker):
                            insertion_idx = i
                            break
                    if insertion_idx is not None:
                        break
            
            # Fallback method 2: Insert after initial parameters and flow type
            if insertion_idx is None:
                # Find the last of the initial parameters
                initial_params = [
                    'Plan Title=',
                    'Program Version=',
                    'Short Identifier=',
                    'Simulation Date=',
                    'Geom File=',
                    'Flow File='
                ]
                
                last_param_idx = 0
                for i, line in enumerate(lines):
                    for param in initial_params:
                        if line.strip().startswith(param):
                            last_param_idx = max(last_param_idx, i)
                
                # Check for flow type lines after Flow File
                flow_types = ['Subcritical Flow', 'Mixed Flow', 'Supercritical Flow']
                for i in range(last_param_idx + 1, min(last_param_idx + 5, len(lines))):
                    if i < len(lines) and lines[i].strip() in flow_types:
                        last_param_idx = i
                
                insertion_idx = last_param_idx + 1
            
            # Prepare the new description block
            # Ensure description doesn't have trailing newline for proper formatting
            description_clean = description.rstrip()

            description_block = [
                'Begin DESCRIPTION\n',
                description_clean + '\n',
                'END DESCRIPTION\n'
            ]
            
            # Build the new file content
            if desc_start_idx is not None and desc_end_idx is not None:
                # Replace existing description block
                # Keep it in its current location if it's already in the right place
                # Otherwise move it to the correct location
                if desc_start_idx < insertion_idx:
                    # Description is already before insertion point, replace in place
                    new_lines = lines[:desc_start_idx] + description_block + lines[desc_end_idx + 1:]
                else:
                    # Description is after insertion point, need to move it
                    # Remove old description
                    lines_without_desc = lines[:desc_start_idx] + lines[desc_end_idx + 1:]
                    # Insert at correct location
                    new_lines = lines_without_desc[:insertion_idx] + description_block + lines_without_desc[insertion_idx:]
            else:
                # No existing description, insert new one
                new_lines = lines[:insertion_idx] + description_block + lines[insertion_idx:]
            
            # Write the modified content back to the file
            with open(plan_path, 'w') as f:
                f.writelines(new_lines)
            
            # Validate the result (optional debug check)
            if __debug__:  # Only in debug mode
                with open(plan_path, 'r') as f:
                    content = f.read()
                
                # Check that description comes before Computation Interval
                if 'Begin DESCRIPTION' in content and 'Computation Interval=' in content:
                    desc_pos = content.find('Begin DESCRIPTION')
                    comp_pos = content.find('Computation Interval=')
                    if desc_pos > comp_pos:
                        print(f"Warning: Description block may be in wrong position in plan {plan_number}")
            
            return True
            
        except FileNotFoundError:
            print(f"Error: Plan file not found for plan {plan_number}")
            return False
        except IOError as e:
            print(f"Error: IO error updating plan {plan_number}: {e}")
            return False
        except Exception as e:
            print(f"Error: Unexpected error updating plan {plan_number}: {e}")
            import traceback
            traceback.print_exc()
            return False






    




    @staticmethod
    @log_call
    def update_simulation_date(plan_number_or_path: Union[str, Number, Path], start_date: datetime, end_date: datetime, ras_object: Optional['RasPrj'] = None) -> None:
        """
        Update the simulation date for a given plan.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            start_date (datetime): The start date and time for the simulation.
            end_date (datetime): The end date and time for the simulation.
            ras_object (Optional['RasPrj']): The RAS project object. Defaults to None.

        Raises:
            ValueError: If the plan file is not found or if there's an error updating the file.
        """

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        # Format the dates
        formatted_date = f"{start_date.strftime('%d%b%Y').upper()},{start_date.strftime('%H%M')},{end_date.strftime('%d%b%Y').upper()},{end_date.strftime('%H%M')}"

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Simulation Date line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Simulation Date="):
                    lines[i] = f"Simulation Date={formatted_date}\n"
                    updated = True
                    break

            # If Simulation Date line not found, add it at the end
            if not updated:
                lines.append(f"Simulation Date={formatted_date}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated simulation date in plan file: {plan_file_path}")

        except IOError as e:
            logger.error(f"Error updating simulation date in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating simulation date: {e}")

        # Refresh RasPrj dataframes
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()
            ras_object.unsteady_df = ras_object.get_unsteady_entries()

    @staticmethod
    @log_call
    def get_shortid(plan_number_or_path: Union[str, Number, Path], ras_object=None) -> str:
        """
        Get the Short Identifier from a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The Short Identifier from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.

        Example:
            >>> shortid = RasPlan.get_shortid('01')
            >>> print(f"Plan's Short Identifier: {shortid}")
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the Short Identifier using get_plan_value
        shortid = RasPlan.get_plan_value(plan_number_or_path, "Short Identifier", ras_obj)
        
        if shortid is None:
            logger.warning(f"Short Identifier not found in plan: {plan_number_or_path}")
            return ""
        
        logger.info(f"Retrieved Short Identifier: {shortid}")
        return shortid

    @staticmethod
    @log_call
    def set_shortid(plan_number_or_path: Union[str, Number, Path], new_shortid: str, ras_object=None) -> None:
        """
        Set the Short Identifier in a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            new_shortid (str): The new Short Identifier to set (max 24 characters).
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Raises:
            ValueError: If the plan file is not found or if new_shortid is too long.
            IOError: If there's an error updating the plan file.

        Example:
            >>> RasPlan.set_shortid('01', 'NewShortIdentifier')
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Ensure new_shortid is not too long (HEC-RAS limits short identifiers to 24 characters)
        if len(new_shortid) > 24:
            logger.warning(f"Short Identifier too long (24 char max). Truncating: {new_shortid}")
            new_shortid = new_shortid[:24]

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Short Identifier line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Short Identifier="):
                    lines[i] = f"Short Identifier={new_shortid}\n"
                    updated = True
                    break

            # If Short Identifier line not found, add it after Plan Title
            if not updated:
                for i, line in enumerate(lines):
                    if line.startswith("Plan Title="):
                        lines.insert(i+1, f"Short Identifier={new_shortid}\n")
                        updated = True
                        break
                
                # If Plan Title not found either, add at the beginning
                if not updated:
                    lines.insert(0, f"Short Identifier={new_shortid}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated Short Identifier in plan file to: {new_shortid}")

        except IOError as e:
            logger.error(f"Error updating Short Identifier in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating Short Identifier: {e}")

        # Refresh RasPrj dataframes if ras_object provided
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()

    @staticmethod
    @log_call
    def get_plan_title(plan_number_or_path: Union[str, Number, Path], ras_object=None) -> str:
        """
        Get the Plan Title from a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The Plan Title from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.

        Example:
            >>> title = RasPlan.get_plan_title('01')
            >>> print(f"Plan Title: {title}")
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the Plan Title using get_plan_value
        title = RasPlan.get_plan_value(plan_number_or_path, "Plan Title", ras_obj)
        
        if title is None:
            logger.warning(f"Plan Title not found in plan: {plan_number_or_path}")
            return ""
        
        logger.info(f"Retrieved Plan Title: {title}")
        return title

    @staticmethod
    @log_call
    def set_plan_title(plan_number_or_path: Union[str, Number, Path], new_title: str, ras_object=None) -> None:
        """
        Set the Plan Title in a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            new_title (str): The new Plan Title to set.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error updating the plan file.

        Example:
            >>> RasPlan.set_plan_title('01', 'Updated Plan Scenario')
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Plan Title line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Plan Title="):
                    lines[i] = f"Plan Title={new_title}\n"
                    updated = True
                    break

            # If Plan Title line not found, add it at the beginning
            if not updated:
                lines.insert(0, f"Plan Title={new_title}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated Plan Title in plan file to: {new_title}")

        except IOError as e:
            logger.error(f"Error updating Plan Title in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating Plan Title: {e}")

        # Refresh RasPrj dataframes if ras_object provided
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()
==================================================

File: c:\GH\ras-commander\ras_commander\RasPrj.py
==================================================
"""
RasPrj.py - Manages HEC-RAS projects within the ras-commander library

This module provides a class for managing HEC-RAS projects.

Classes:
    RasPrj: A class for managing HEC-RAS projects.

Functions:
    init_ras_project: Initialize a RAS project.
    get_ras_exe: Determine the HEC-RAS executable path based on the input.

DEVELOPER NOTE:
This class is used to initialize a RAS project and is used in conjunction with the RasCmdr class to manage the execution of RAS plans.
By default, the RasPrj class is initialized with the global 'ras' object.
However, you can create multiple RasPrj instances to manage multiple projects.
Do not mix and match global 'ras' object instances and custom instances of RasPrj - it will cause errors.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function():
        
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are class methods and are designed to be used with instances of the class.

List of Functions in RasPrj:    
- initialize()
- _load_project_data()
- _get_geom_file_for_plan()
- _parse_plan_file()
- _parse_unsteady_file()
- _get_prj_entries()
- _parse_boundary_condition()
- is_initialized (property)
- check_initialized()
- find_ras_prj()
- get_project_name()
- get_prj_entries()
- get_plan_entries()
- get_flow_entries()
- get_unsteady_entries()
- get_geom_entries()
- get_hdf_entries()
- print_data()
- get_plan_value()
- get_boundary_conditions()
        
Functions in RasPrj that are not part of the class:        
- init_ras_project()
- get_ras_exe()

        
        
        
"""
import os
import re
from pathlib import Path
import pandas as pd
from typing import Union, Any, List, Dict, Tuple
import logging
from ras_commander.LoggingConfig import get_logger
from ras_commander.Decorators import log_call

logger = get_logger(__name__)

def read_file_with_fallback_encoding(file_path, encodings=['utf-8', 'latin1', 'cp1252', 'iso-8859-1']):
    """
    Attempt to read a file using multiple encodings.
    
    Args:
        file_path (str or Path): Path to the file to read
        encodings (list): List of encodings to try, in order of preference
    
    Returns:
        tuple: (content, encoding) or (None, None) if all encodings fail
    """
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                content = file.read()
                return content, encoding
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"Error reading file {file_path} with {encoding} encoding: {e}")
            continue
    
    logger.error(f"Failed to read file {file_path} with any of the attempted encodings: {encodings}")
    return None, None

class RasPrj:
    
    def __init__(self):
        self.initialized = False
        self.boundaries_df = None  # New attribute to store boundary conditions
        self.suppress_logging = False  # Add suppress_logging as instance variable

    @log_call
    def initialize(self, project_folder, ras_exe_path, suppress_logging=True, prj_file=None):
        """
        Initialize a RasPrj instance with project folder and RAS executable path.

        IMPORTANT: External users should use init_ras_project() function instead of this method.
        This method is intended for internal use only.

        Args:
            project_folder (str or Path): Path to the HEC-RAS project folder.
            ras_exe_path (str or Path): Path to the HEC-RAS executable.
            suppress_logging (bool, default=True): If True, suppresses initialization logging messages.
            prj_file (str or Path, optional): If provided, use this specific .prj file instead of searching.
                                              This is used when user specifies a .prj file directly.

        Raises:
            ValueError: If no HEC-RAS project file is found in the specified folder,
                        or if the specified prj_file doesn't exist or is invalid.

        Note:
            This method sets up the RasPrj instance by:
            1. Finding the project file (.prj) or using the provided prj_file
            2. Loading project data (plans, geometries, flows)
            3. Extracting boundary conditions
            4. Setting the initialization flag
            5. Loading RASMapper data (.rasmap)
        """
        self.suppress_logging = suppress_logging  # Store suppress_logging state
        self.project_folder = Path(project_folder)

        # If user specified a .prj file directly, use it (Phase 2 optimization)
        if prj_file is not None:
            self.prj_file = Path(prj_file).resolve()
            if not self.prj_file.exists():
                logger.error(f"Specified .prj file does not exist: {self.prj_file}")
                raise ValueError(f"Specified .prj file does not exist: {self.prj_file}. Please check the path and try again.")
            logger.debug(f"Using specified .prj file: {self.prj_file}")
        else:
            # Search for .prj file (existing behavior)
            self.prj_file = self.find_ras_prj(self.project_folder)
            if self.prj_file is None:
                logger.error(f"No HEC-RAS project file found in {self.project_folder}")
                raise ValueError(f"No HEC-RAS project file found in {self.project_folder}. Please check the path and try again.")

        self.project_name = Path(self.prj_file).stem
        self.ras_exe_path = ras_exe_path
        
        # Set initialized to True before loading project data
        self.initialized = True
        
        # Now load the project data
        self._load_project_data()
        self.boundaries_df = self.get_boundary_conditions()
        
        # Load RASMapper data if available
        try:
            # Import here to avoid circular imports
            from .RasMap import RasMap
            self.rasmap_df = RasMap.initialize_rasmap_df(self)
        except ImportError:
            logger.warning("RasMap module not available. RASMapper data will not be loaded.")
            self.rasmap_df = pd.DataFrame(columns=['projection_path', 'profile_lines_path', 'soil_layer_path', 
                                                'infiltration_hdf_path', 'landcover_hdf_path', 'terrain_hdf_path', 
                                                'current_settings'])
        except Exception as e:
            logger.error(f"Error initializing RASMapper data: {e}")
            self.rasmap_df = pd.DataFrame(columns=['projection_path', 'profile_lines_path', 'soil_layer_path', 
                                                'infiltration_hdf_path', 'landcover_hdf_path', 'terrain_hdf_path', 
                                                'current_settings'])

        if not suppress_logging:
            logger.info(f"Initialization complete for project: {self.project_name}")
            logger.info(f"Plan entries: {len(self.plan_df)}, Flow entries: {len(self.flow_df)}, "
                         f"Unsteady entries: {len(self.unsteady_df)}, Geometry entries: {len(self.geom_df)}, "
                         f"Boundary conditions: {len(self.boundaries_df)}")
            logger.info(f"Geometry HDF files found: {self.plan_df['Geom_File'].notna().sum()}")
            logger.info(f"RASMapper data loaded: {not self.rasmap_df.empty}")

    @log_call
    def _load_project_data(self):
        """
        Load project data from the HEC-RAS project file.

        This internal method:
        1. Initializes DataFrames for plan, flow, unsteady, and geometry entries
        2. Ensures all required columns are present with appropriate default values
        3. Sets file paths for all components (geometries, flows, plans)

        Raises:
            Exception: If there's an error loading or processing project data.
        """
        try:
            # Load data frames
            self.unsteady_df = self._get_prj_entries('Unsteady')
            self.plan_df = self._get_prj_entries('Plan')
            self.flow_df = self._get_prj_entries('Flow')
            self.geom_df = self.get_geom_entries()
            
            # Ensure required columns exist
            self._ensure_required_columns()
            
            # Set paths for geometry and flow files
            self._set_file_paths()

            # Make sure all plan paths are properly set
            self._set_plan_paths()

            # Add flow_type column for deterministic steady/unsteady identification
            if not self.plan_df.empty and 'unsteady_number' in self.plan_df.columns:
                self.plan_df['flow_type'] = self.plan_df['unsteady_number'].apply(
                    lambda x: 'Unsteady' if pd.notna(x) else 'Steady'
                )
            else:
                if not self.plan_df.empty:
                    self.plan_df['flow_type'] = 'Unknown'

        except Exception as e:
            logger.error(f"Error loading project data: {e}")
            raise

    def _ensure_required_columns(self):
        """Ensure all required columns exist in plan_df."""
        required_columns = [
            'plan_number', 'unsteady_number', 'geometry_number',
            'Geom File', 'Geom Path', 'Flow File', 'Flow Path', 'full_path'
        ]
        
        for col in required_columns:
            if col not in self.plan_df.columns:
                self.plan_df[col] = None
        
        if not self.plan_df['full_path'].any():
            self.plan_df['full_path'] = self.plan_df['plan_number'].apply(
                lambda x: str(self.project_folder / f"{self.project_name}.p{x}")
            )

    def _set_file_paths(self):
        """Set geometry and flow paths in plan_df."""
        for idx, row in self.plan_df.iterrows():
            try:
                self._set_geom_path(idx, row)
                self._set_flow_path(idx, row)
                
                if not self.suppress_logging:
                    logger.info(f"Plan {row['plan_number']} paths set up")
            except Exception as e:
                logger.error(f"Error processing plan file {row['plan_number']}: {e}")

    def _set_geom_path(self, idx: int, row: pd.Series):
        """Set geometry path for a plan entry."""
        if pd.notna(row['Geom File']):
            geom_path = self.project_folder / f"{self.project_name}.g{row['Geom File']}"
            self.plan_df.at[idx, 'Geom Path'] = str(geom_path)

    def _set_flow_path(self, idx: int, row: pd.Series):
        """Set flow path for a plan entry."""
        if pd.notna(row['Flow File']):
            prefix = 'u' if pd.notna(row['unsteady_number']) else 'f'
            flow_path = self.project_folder / f"{self.project_name}.{prefix}{row['Flow File']}"
            self.plan_df.at[idx, 'Flow Path'] = str(flow_path)

    def _set_plan_paths(self):
        """Set full path information for plan files and their associated geometry and flow files."""
        if self.plan_df.empty:
            logger.debug("Plan DataFrame is empty, no paths to set")
            return
        
        # Ensure full path is set for all plan entries
        if 'full_path' not in self.plan_df.columns or self.plan_df['full_path'].isna().any():
            self.plan_df['full_path'] = self.plan_df['plan_number'].apply(
                lambda x: str(self.project_folder / f"{self.project_name}.p{x}")
            )
        
        # Create the Geom Path and Flow Path columns if they don't exist
        if 'Geom Path' not in self.plan_df.columns:
            self.plan_df['Geom Path'] = None
        if 'Flow Path' not in self.plan_df.columns:
            self.plan_df['Flow Path'] = None
        
        # Update paths for each plan entry
        for idx, row in self.plan_df.iterrows():
            try:
                # Set geometry path if Geom File exists and Geom Path is missing or invalid
                if pd.notna(row['Geom File']):
                    geom_path = self.project_folder / f"{self.project_name}.g{row['Geom File']}"
                    self.plan_df.at[idx, 'Geom Path'] = str(geom_path)
                
                # Set flow path if Flow File exists and Flow Path is missing or invalid
                if pd.notna(row['Flow File']):
                    # Determine the prefix (u for unsteady, f for steady flow)
                    prefix = 'u' if pd.notna(row['unsteady_number']) else 'f'
                    flow_path = self.project_folder / f"{self.project_name}.{prefix}{row['Flow File']}"
                    self.plan_df.at[idx, 'Flow Path'] = str(flow_path)
                
                if not self.suppress_logging:
                    logger.debug(f"Plan {row['plan_number']} paths set up")
            except Exception as e:
                logger.error(f"Error setting paths for plan {row.get('plan_number', idx)}: {e}")

    def _get_geom_file_for_plan(self, plan_number):
        """
        Get the geometry file path for a given plan number.
        
        Args:
            plan_number (str): The plan number to find the geometry file for.
        
        Returns:
            str: The full path to the geometry HDF file, or None if not found.
        """
        plan_file_path = self.project_folder / f"{self.project_name}.p{plan_number}"
        content, encoding = read_file_with_fallback_encoding(plan_file_path)
        
        if content is None:
            return None
        
        try:
            for line in content.splitlines():
                if line.startswith("Geom File="):
                    geom_file = line.strip().split('=')[1]
                    geom_hdf_path = self.project_folder / f"{self.project_name}.{geom_file}.hdf"
                    if geom_hdf_path.exists():
                        return str(geom_hdf_path)
                    else:
                        return None
        except Exception as e:
            logger.error(f"Error reading plan file for geometry: {e}")
        return None


    @staticmethod
    @log_call
    def get_plan_value(
        plan_number_or_path: Union[str, Path],
        key: str,
        ras_object=None
    ) -> Any:
        """
        Retrieve a specific value from a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        key (str): The key to retrieve from the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Any: The value associated with the specified key

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading the plan file
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # These must exactly match the keys in supported_plan_keys from _parse_plan_file
        valid_keys = {
            'Computation Interval',
            'DSS File',
            'Flow File',
            'Friction Slope Method',
            'Geom File',
            'Mapping Interval',
            'Plan Title',
            'Program Version',
            'Run HTab',
            'Run PostProcess',
            'Run Sediment',
            'Run UNet',
            'Run WQNet',
            'Short Identifier',
            'Simulation Date',
            'UNET D1 Cores',
            'UNET D2 Cores',
            'PS Cores',
            'UNET Use Existing IB Tables',
            'UNET 1D Methodology',
            'UNET D2 SolverType',
            'UNET D2 Name',
            'description'  # Special case for description block
        }

        if key not in valid_keys:
            logger.warning(f"Unknown key: {key}. Valid keys are: {', '.join(sorted(valid_keys))}")
            return None

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()
        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        if key == 'description':
            match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            return match.group(1).strip() if match else None
        else:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                value = match.group(1).strip()
                # Convert core values to integers
                if key in ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']:
                    try:
                        return int(value)
                    except ValueError:
                        logger.warning(f"Could not convert {key} value '{value}' to integer")
                        return None
                return value
            
            # Use DEBUG level for missing core values, ERROR for other missing keys
            if key in ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']:
                logger.debug(f"Core setting '{key}' not found in plan file")
            else:
                logger.error(f"Key '{key}' not found in the plan file")
            return None

    def _parse_plan_file(self, plan_file_path):
        """
        Parse a plan file and extract critical information.
        
        Args:
            plan_file_path (Path): Path to the plan file.
        
        Returns:
            dict: Dictionary containing extracted plan information.
        """
        plan_info = {}
        content, encoding = read_file_with_fallback_encoding(plan_file_path)
        
        if content is None:
            logger.error(f"Could not read plan file {plan_file_path} with any supported encoding")
            return plan_info
        
        try:
            # Extract description
            description_match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            if description_match:
                plan_info['description'] = description_match.group(1).strip()
            
            # BEGIN Exception to Style Guide, this is needed to keep the key names consistent with the plan file keys.
            
            # Extract other critical information
            supported_plan_keys = {
                'Computation Interval': r'Computation Interval=(.+)',
                'DSS File': r'DSS File=(.+)',
                'Flow File': r'Flow File=(.+)',
                'Friction Slope Method': r'Friction Slope Method=(.+)',
                'Geom File': r'Geom File=(.+)',
                'Mapping Interval': r'Mapping Interval=(.+)',
                'Plan Title': r'Plan Title=(.+)',
                'Program Version': r'Program Version=(.+)',
                'Run HTab': r'Run HTab=(.+)',
                'Run PostProcess': r'Run PostProcess=(.+)',
                'Run Sediment': r'Run Sediment=(.+)',
                'Run UNet': r'Run UNet=(.+)',
                'Run WQNet': r'Run WQNet=(.+)',
                'Short Identifier': r'Short Identifier=(.+)',
                'Simulation Date': r'Simulation Date=(.+)',
                'UNET D1 Cores': r'UNET D1 Cores=(.+)',
                'UNET D2 Cores': r'UNET D2 Cores=(.+)',
                'PS Cores': r'PS Cores=(.+)',
                'UNET Use Existing IB Tables': r'UNET Use Existing IB Tables=(.+)',
                'UNET 1D Methodology': r'UNET 1D Methodology=(.+)',
                'UNET D2 SolverType': r'UNET D2 SolverType=(.+)',
                'UNET D2 Name': r'UNET D2 Name=(.+)'
            }
            
            # END Exception to Style Guide
            
            # First, explicitly set None for core values
            core_keys = ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']
            for key in core_keys:
                plan_info[key] = None
            
            for key, pattern in supported_plan_keys.items():
                match = re.search(pattern, content)
                if match:
                    value = match.group(1).strip()
                    # Convert core values to integers if they exist
                    if key in core_keys and value:
                        try:
                            value = int(value)
                        except ValueError:
                            logger.warning(f"Could not convert {key} value '{value}' to integer in plan file {plan_file_path}")
                            value = None
                    plan_info[key] = value
                elif key in core_keys:
                    logger.debug(f"Core setting '{key}' not found in plan file {plan_file_path}")
            
            logger.debug(f"Parsed plan file: {plan_file_path} using {encoding} encoding")
        except Exception as e:
            logger.error(f"Error parsing plan file {plan_file_path}: {e}")
        
        return plan_info

    @log_call
    def _get_prj_entries(self, entry_type):
        """
        Extract entries of a specific type from the HEC-RAS project file.
        
        Args:
            entry_type (str): The type of entry to extract (e.g., 'Plan', 'Flow', 'Unsteady', 'Geom').
        
        Returns:
            pd.DataFrame: A DataFrame containing the extracted entries.
        
        Raises:
            Exception: If there's an error reading or processing the project file.
        """
        entries = []
        pattern = re.compile(rf"{entry_type} File=(\w+)")

        try:
            with open(self.prj_file, 'r', encoding='utf-8') as file:
                for line in file:
                    match = pattern.match(line.strip())
                    if match:
                        file_name = match.group(1)
                        full_path = str(self.project_folder / f"{self.project_name}.{file_name}")
                        entry_number = file_name[1:]
                        
                        entry = {
                            f'{entry_type.lower()}_number': entry_number,
                            'full_path': full_path
                        }
                        
                        # Handle Unsteady entries
                        if entry_type == 'Unsteady':
                            entry.update(self._process_unsteady_entry(entry_number, full_path))
                        else:
                            entry.update(self._process_default_entry())
                        
                        # Handle Plan entries
                        if entry_type == 'Plan':
                            entry.update(self._process_plan_entry(entry_number, full_path))
                        
                        entries.append(entry)
            
            df = pd.DataFrame(entries)
            return self._format_dataframe(df, entry_type)
        
        except Exception as e:
            logger.error(f"Error in _get_prj_entries for {entry_type}: {e}")
            raise

    def _process_unsteady_entry(self, entry_number: str, full_path: str) -> dict:
        """Process unsteady entry data."""
        entry = {'unsteady_number': entry_number}
        unsteady_info = self._parse_unsteady_file(Path(full_path))
        entry.update(unsteady_info)
        return entry

    def _process_default_entry(self) -> dict:
        """Process default entry data."""
        return {
            'unsteady_number': None,
            'geometry_number': None
        }

    def _process_plan_entry(self, entry_number: str, full_path: str) -> dict:
        """Process plan entry data."""
        entry = {}
        plan_info = self._parse_plan_file(Path(full_path))
        
        if plan_info:
            entry.update(self._process_flow_file(plan_info))
            entry.update(self._process_geom_file(plan_info))
            
            # Add remaining plan info
            for key, value in plan_info.items():
                if key not in ['Flow File', 'Geom File']:
                    entry[key] = value
            
            # Add HDF results path
            hdf_results_path = self.project_folder / f"{self.project_name}.p{entry_number}.hdf"
            entry['HDF_Results_Path'] = str(hdf_results_path) if hdf_results_path.exists() else None
        
        return entry

    def _process_flow_file(self, plan_info: dict) -> dict:
        """Process flow file information from plan info."""
        flow_file = plan_info.get('Flow File')
        if flow_file and flow_file.startswith('u'):
            return {
                'unsteady_number': flow_file[1:],
                'Flow File': flow_file[1:]
            }
        return {
            'unsteady_number': None,
            'Flow File': flow_file[1:] if flow_file and flow_file.startswith('f') else None
        }

    def _process_geom_file(self, plan_info: dict) -> dict:
        """Process geometry file information from plan info."""
        geom_file = plan_info.get('Geom File')
        if geom_file and geom_file.startswith('g'):
            return {
                'geometry_number': geom_file[1:],
                'Geom File': geom_file[1:]
            }
        return {
            'geometry_number': None,
            'Geom File': None
        }

    def _parse_unsteady_file(self, unsteady_file_path):
        """
        Parse an unsteady flow file and extract critical information.
        
        Args:
            unsteady_file_path (Path): Path to the unsteady flow file.
        
        Returns:
            dict: Dictionary containing extracted unsteady flow information.
        """
        unsteady_info = {}
        content, encoding = read_file_with_fallback_encoding(unsteady_file_path)
        
        if content is None:
            return unsteady_info
        
        try:
            # BEGIN Exception to Style Guide, this is needed to keep the key names consistent with the unsteady file keys.
            
            supported_unsteady_keys = {
                'Flow Title': r'Flow Title=(.+)',
                'Program Version': r'Program Version=(.+)',
                'Use Restart': r'Use Restart=(.+)',
                'Precipitation Mode': r'Precipitation Mode=(.+)',
                'Wind Mode': r'Wind Mode=(.+)',
                'Met BC=Precipitation|Mode': r'Met BC=Precipitation\|Mode=(.+)',
                'Met BC=Evapotranspiration|Mode': r'Met BC=Evapotranspiration\|Mode=(.+)',
                'Met BC=Precipitation|Expanded View': r'Met BC=Precipitation\|Expanded View=(.+)',
                'Met BC=Precipitation|Constant Units': r'Met BC=Precipitation\|Constant Units=(.+)',
                'Met BC=Precipitation|Gridded Source': r'Met BC=Precipitation\|Gridded Source=(.+)'
            }
            
            # END Exception to Style Guide
            
            for key, pattern in supported_unsteady_keys.items():
                match = re.search(pattern, content)
                if match:
                    unsteady_info[key] = match.group(1).strip()
        
        except Exception as e:
            logger.error(f"Error parsing unsteady file {unsteady_file_path}: {e}")
        
        return unsteady_info

    @property
    def is_initialized(self):
        """
        Check if the RasPrj instance has been initialized.

        Returns:
            bool: True if the instance has been initialized, False otherwise.
        """
        return self.initialized

    @log_call
    def check_initialized(self):
        """
        Ensure that the RasPrj instance has been initialized before operations.

        Raises:
            RuntimeError: If the project has not been initialized with init_ras_project().

        Note:
            This method is called by other methods to validate the project state before
            performing operations. Users typically don't need to call this directly.
        """
        if not self.initialized:
            raise RuntimeError("Project not initialized. Call init_ras_project() first.")

    @staticmethod
    @log_call
    def find_ras_prj(folder_path):
        """
        Find the appropriate HEC-RAS project file (.prj) in the given folder.
        
        This method uses several strategies to locate the correct project file:
        1. If only one .prj file exists, it is selected
        2. If multiple .prj files exist, it tries to match with .rasmap file names
        3. As a last resort, it scans files for "Proj Title=" content
        
        Args:
            folder_path (str or Path): Path to the folder containing HEC-RAS files.
        
        Returns:
            Path: The full path of the selected .prj file or None if no suitable file is found.
        
        Example:
            >>> project_file = RasPrj.find_ras_prj("/path/to/ras_project")
            >>> if project_file:
            ...     print(f"Found project file: {project_file}")
            ... else:
            ...     print("No project file found")
        """
        folder_path = Path(folder_path)
        prj_files = list(folder_path.glob("*.prj"))
        rasmap_files = list(folder_path.glob("*.rasmap"))
        if len(prj_files) == 1:
            return prj_files[0].resolve()
        if len(prj_files) > 1:
            if len(rasmap_files) == 1:
                base_filename = rasmap_files[0].stem
                prj_file = folder_path / f"{base_filename}.prj"
                if prj_file.exists():
                    return prj_file.resolve()
            for prj_file in prj_files:
                try:
                    with open(prj_file, 'r') as file:
                        content = file.read()
                        if "Proj Title=" in content:
                            return prj_file.resolve()
                except Exception:
                    continue
        return None


    @log_call
    def get_project_name(self):
        """
        Get the name of the HEC-RAS project (without file extension).

        Returns:
            str: The name of the project.

        Raises:
            RuntimeError: If the project has not been initialized.

        Example:
            >>> project_name = ras.get_project_name()
            >>> print(f"Working with project: {project_name}")
        """
        self.check_initialized()
        return self.project_name

    @log_call
    def set_current_plan(self, plan_number: Union[str, int]):
        """
        Set the current plan in the HEC-RAS project file (.prj).

        This ensures that when HEC-RAS opens, it opens with the specified plan active.

        Args:
            plan_number (Union[str, int]): The plan number to set as current (e.g., "01" or 1)

        Raises:
            RuntimeError: If the project has not been initialized
            ValueError: If the plan number is invalid or not found
            IOError: If there's an error reading or writing the project file

        Example:
            >>> ras.set_current_plan("01")
            >>> # HEC-RAS will now open with plan 01 active
        """
        self.check_initialized()

        # Normalize plan number to 2-digit string with leading zero
        from .RasUtils import RasUtils
        plan_number_str = RasUtils.normalize_ras_number(plan_number)

        # Verify plan exists in plan_df
        if plan_number_str not in self.plan_df['plan_number'].values:
            available_plans = ', '.join(self.plan_df['plan_number'].values)
            raise ValueError(
                f"Plan {plan_number_str} not found in project. "
                f"Available plans: {available_plans}"
            )

        try:
            # Read the project file
            with open(self.prj_file, 'r') as f:
                lines = f.readlines()

            # Find and update the Current Plan line
            updated = False
            for i, line in enumerate(lines):
                if line.strip().startswith('Current Plan='):
                    lines[i] = f"Current Plan=p{plan_number_str}\n"
                    updated = True
                    break

            # If Current Plan line doesn't exist, add it after Proj Title
            if not updated:
                for i, line in enumerate(lines):
                    if line.strip().startswith('Proj Title='):
                        lines.insert(i + 1, f"Current Plan=p{plan_number_str}\n")
                        updated = True
                        break

            if not updated:
                raise ValueError("Could not find 'Proj Title=' or 'Current Plan=' in project file")

            # Write back to file
            with open(self.prj_file, 'w') as f:
                f.writelines(lines)

            logger.info(f"Set current plan to p{plan_number_str} in {self.prj_file}")

        except IOError as e:
            logger.error(f"Error updating current plan in {self.prj_file}: {e}")
            raise

    @log_call
    def get_prj_entries(self, entry_type):
        """
        Get entries of a specific type from the HEC-RAS project.

        This method extracts files of the specified type from the project file,
        parses their content, and returns a structured DataFrame.

        Args:
            entry_type (str): The type of entry to retrieve ('Plan', 'Flow', 'Unsteady', or 'Geom').

        Returns:
            pd.DataFrame: A DataFrame containing the requested entries with appropriate columns.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> # Get all geometry files in the project
            >>> geom_entries = ras.get_prj_entries('Geom')
            >>> print(f"Project contains {len(geom_entries)} geometry files")
        
        Note:
            This is a generic method. For specific file types, use the dedicated methods:
            get_plan_entries(), get_flow_entries(), get_unsteady_entries(), get_geom_entries()
        """
        self.check_initialized()
        return self._get_prj_entries(entry_type)

    @log_call
    def get_plan_entries(self):
        """
        Get all plan entries from the HEC-RAS project.
        
        Returns a DataFrame containing all plan files (.p*) in the project
        with their associated properties, paths and settings.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'plan_number', 'full_path',
                          'unsteady_number', 'geometry_number', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> plan_entries = ras.get_plan_entries()
            >>> print(f"Project contains {len(plan_entries)} plan files")
            >>> # Display the first plan's properties
            >>> if not plan_entries.empty:
            ...     print(plan_entries.iloc[0])
        """
        self.check_initialized()
        return self._get_prj_entries('Plan')

    @log_call
    def get_flow_entries(self):
        """
        Get all flow entries from the HEC-RAS project.
        
        Returns a DataFrame containing all flow files (.f*) in the project
        with their associated properties and paths.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'flow_number', 'full_path', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> flow_entries = ras.get_flow_entries()
            >>> print(f"Project contains {len(flow_entries)} flow files")
            >>> # Display the first flow file's properties
            >>> if not flow_entries.empty:
            ...     print(flow_entries.iloc[0])
        """
        self.check_initialized()
        return self._get_prj_entries('Flow')

    @log_call
    def get_unsteady_entries(self):
        """
        Get all unsteady flow entries from the HEC-RAS project.
        
        Returns a DataFrame containing all unsteady flow files (.u*) in the project
        with their associated properties and paths.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'unsteady_number', 'full_path', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> unsteady_entries = ras.get_unsteady_entries()
            >>> print(f"Project contains {len(unsteady_entries)} unsteady flow files")
            >>> # Display the first unsteady file's properties
            >>> if not unsteady_entries.empty:
            ...     print(unsteady_entries.iloc[0])
        """
        self.check_initialized()
        return self._get_prj_entries('Unsteady')

    @log_call
    def get_geom_entries(self):
        """
        Get all geometry entries from the HEC-RAS project.
        
        Returns a DataFrame containing all geometry files (.g*) in the project
        with their associated properties, paths and HDF links.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'geom_number', 'full_path', 
                          'hdf_path', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> geom_entries = ras.get_geom_entries()
            >>> print(f"Project contains {len(geom_entries)} geometry files")
            >>> # Display the first geometry file's properties
            >>> if not geom_entries.empty:
            ...     print(geom_entries.iloc[0])
        """
        self.check_initialized()
        geom_pattern = re.compile(r'Geom File=(\w+)')
        geom_entries = []

        try:
            with open(self.prj_file, 'r') as f:
                for line in f:
                    match = geom_pattern.search(line)
                    if match:
                        geom_entries.append(match.group(1))
        
            geom_df = pd.DataFrame({'geom_file': geom_entries})
            geom_df['geom_number'] = geom_df['geom_file'].str.extract(r'(\d+)$')
            geom_df['full_path'] = geom_df['geom_file'].apply(lambda x: str(self.project_folder / f"{self.project_name}.{x}"))
            geom_df['hdf_path'] = geom_df['full_path'] + ".hdf"
            
            if not self.suppress_logging:  # Only log if suppress_logging is False
                logger.info(f"Found {len(geom_df)} geometry entries")
            return geom_df
        except Exception as e:
            logger.error(f"Error reading geometry entries from project file: {e}")
            raise
    
    @log_call
    def get_hdf_entries(self):
        """
        Get all plan entries that have associated HDF results files.
        
        This method identifies which plans have been successfully computed
        and have HDF results available for further analysis.
        
        Returns:
            pd.DataFrame: A DataFrame containing plan entries with HDF results.
                          Returns an empty DataFrame if no results are found.
        
        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> hdf_entries = ras.get_hdf_entries()
            >>> if hdf_entries.empty:
            ...     print("No computed results found. Run simulations first.")
            ... else:
            ...     print(f"Found results for {len(hdf_entries)} plans")
        
        Note:
            This is useful for identifying which plans have been successfully computed
            and can be used for further results analysis.
        """
        self.check_initialized()
        
        hdf_entries = self.plan_df[self.plan_df['HDF_Results_Path'].notna()].copy()
        
        if hdf_entries.empty:
            return pd.DataFrame(columns=self.plan_df.columns)
        
        return hdf_entries
    
    
    @log_call
    def print_data(self):
        """
        Print a comprehensive summary of all RAS Object data for this instance.
        
        This method outputs:
        - Project information (name, folder, file paths)
        - Summary of plans, flows, geometries, and unsteady files
        - HDF results availability
        - Boundary conditions
        
        Useful for debugging, validation, and exploring project structure.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> ras.print_data()  # Displays complete project overview
        """
        self.check_initialized()
        logger.info(f"--- Data for {self.project_name} ---")
        logger.info(f"Project folder: {self.project_folder}")
        logger.info(f"PRJ file: {self.prj_file}")
        logger.info(f"HEC-RAS executable: {self.ras_exe_path}")
        logger.info("Plan files:")
        logger.info(f"\n{self.plan_df}")
        logger.info("Flow files:")
        logger.info(f"\n{self.flow_df}")
        logger.info("Unsteady flow files:")
        logger.info(f"\n{self.unsteady_df}")
        logger.info("Geometry files:")
        logger.info(f"\n{self.geom_df}")
        logger.info("HDF entries:")
        logger.info(f"\n{self.get_hdf_entries()}")
        logger.info("Boundary conditions:")
        logger.info(f"\n{self.boundaries_df}")
        logger.info("----------------------------")

    @log_call
    def get_boundary_conditions(self) -> pd.DataFrame:
        """
        Extract boundary conditions from unsteady flow files into a structured DataFrame.

        This method:
        1. Parses all unsteady flow files to extract boundary condition information
        2. Creates a structured DataFrame with boundary locations, types and parameters
        3. Links boundary conditions to their respective unsteady flow files

        Supported boundary condition types include:
        - Flow Hydrograph
        - Stage Hydrograph
        - Normal Depth
        - Lateral Inflow Hydrograph
        - Uniform Lateral Inflow Hydrograph
        - Gate Opening

        Returns:
            pd.DataFrame: A DataFrame containing detailed boundary condition information.
                              Returns an empty DataFrame if no unsteady flow files are present.
        
        Example:
            >>> boundaries = ras.get_boundary_conditions()
            >>> if not boundaries.empty:
            ...     print(f"Found {len(boundaries)} boundary conditions")
            ...     # Show flow hydrographs only
            ...     flow_hydrographs = boundaries[boundaries['bc_type'] == 'Flow Hydrograph']
            ...     print(f"Project has {len(flow_hydrographs)} flow hydrographs")
        
        Note:
            To see unparsed boundary condition lines for debugging, set logging to DEBUG:
            import logging
            logging.getLogger().setLevel(logging.DEBUG)
        """
        boundary_data = []
        
        # Check if unsteady_df is empty
        if self.unsteady_df.empty:
            logger.info("No unsteady flow files found in the project.")
            return pd.DataFrame()  # Return an empty DataFrame
        
        for _, row in self.unsteady_df.iterrows():
            unsteady_file_path = row['full_path']
            unsteady_number = row['unsteady_number']
            
            try:
                with open(unsteady_file_path, 'r') as file:
                    content = file.read()
            except IOError as e:
                logger.error(f"Error reading unsteady file {unsteady_file_path}: {e}")
                continue
                
            bc_blocks = re.split(r'(?=Boundary Location=)', content)[1:]
            
            for i, block in enumerate(bc_blocks, 1):
                bc_info, unparsed_lines = self._parse_boundary_condition(block, unsteady_number, i)
                boundary_data.append(bc_info)
                
                if unparsed_lines:
                    logger.debug(f"Unparsed lines for boundary condition {i} in unsteady file {unsteady_number}:\n{unparsed_lines}")
        
        if not boundary_data:
            logger.info("No boundary conditions found in unsteady flow files.")
            return pd.DataFrame()  # Return an empty DataFrame if no boundary conditions were found
        
        boundaries_df = pd.DataFrame(boundary_data)
        
        # Merge with unsteady_df to get relevant unsteady flow file information
        merged_df = pd.merge(boundaries_df, self.unsteady_df, 
                             left_on='unsteady_number', right_on='unsteady_number', how='left')
        
        return merged_df

    def _parse_boundary_condition(self, block: str, unsteady_number: str, bc_number: int) -> Tuple[Dict, str]:
        lines = block.split('\n')
        bc_info = {
            'unsteady_number': unsteady_number,
            'boundary_condition_number': bc_number
        }
        
        parsed_lines = set()
        
        # Parse Boundary Location
        boundary_location = lines[0].split('=')[1].strip()
        fields = [field.strip() for field in boundary_location.split(',')]
        bc_info.update({
            'river_reach_name': fields[0] if len(fields) > 0 else '',
            'river_station': fields[1] if len(fields) > 1 else '',
            'storage_area_name': fields[2] if len(fields) > 2 else '',
            'pump_station_name': fields[3] if len(fields) > 3 else ''
        })
        parsed_lines.add(0)
        
        # Determine BC Type
        bc_types = {
            'Flow Hydrograph=': 'Flow Hydrograph',
            'Lateral Inflow Hydrograph=': 'Lateral Inflow Hydrograph',
            'Uniform Lateral Inflow Hydrograph=': 'Uniform Lateral Inflow Hydrograph',
            'Stage Hydrograph=': 'Stage Hydrograph',
            'Friction Slope=': 'Normal Depth',
            'Gate Name=': 'Gate Opening'
        }
        
        bc_info['bc_type'] = 'Unknown'
        bc_info['hydrograph_type'] = None
        for i, line in enumerate(lines[1:], 1):
            for key, bc_type in bc_types.items():
                if line.startswith(key):
                    bc_info['bc_type'] = bc_type
                    if 'Hydrograph' in bc_type:
                        bc_info['hydrograph_type'] = bc_type
                    parsed_lines.add(i)
                    break
            if bc_info['bc_type'] != 'Unknown':
                break
        
        # Parse other fields
        known_fields = ['Interval', 'DSS Path', 'Use DSS', 'Use Fixed Start Time', 'Fixed Start Date/Time',
                        'Is Critical Boundary', 'Critical Boundary Flow', 'DSS File']
        for i, line in enumerate(lines):
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                if key in known_fields:
                    bc_info[key] = value.strip()
                    parsed_lines.add(i)
        
        # Handle hydrograph values
        bc_info['hydrograph_num_values'] = 0
        if bc_info['hydrograph_type']:
            hydrograph_key = f"{bc_info['hydrograph_type']}="
            hydrograph_line = next((line for i, line in enumerate(lines) if line.startswith(hydrograph_key)), None)
            if hydrograph_line:
                hydrograph_index = lines.index(hydrograph_line)
                values_count = int(hydrograph_line.split('=')[1].strip())
                bc_info['hydrograph_num_values'] = values_count
                if values_count > 0:
                    values = ' '.join(lines[hydrograph_index + 1:]).split()[:values_count]
                    bc_info['hydrograph_values'] = values
                    parsed_lines.update(range(hydrograph_index, hydrograph_index + (values_count // 5) + 2))
        
        # Collect unparsed lines
        unparsed_lines = '\n'.join(line for i, line in enumerate(lines) if i not in parsed_lines and line.strip())
        
        if unparsed_lines:
            logger.debug(f"Unparsed lines for boundary condition {bc_number} in unsteady file {unsteady_number}:\n{unparsed_lines}")
        
        return bc_info, unparsed_lines

    @log_call
    def _format_dataframe(self, df, entry_type):
        """
        Format the DataFrame according to the desired column structure.
        
        Args:
            df (pd.DataFrame): The DataFrame to format.
            entry_type (str): The type of entry (e.g., 'Plan', 'Flow', 'Unsteady', 'Geom').
        
        Returns:
            pd.DataFrame: The formatted DataFrame.
        """
        if df.empty:
            return df
        
        if entry_type == 'Plan':
            # Set required column order
            first_cols = ['plan_number', 'unsteady_number', 'geometry_number']
            
            # Standard plan key columns in the exact order specified
            plan_key_cols = [
                'Plan Title', 'Program Version', 'Short Identifier', 'Simulation Date',
                'Std Step Tol', 'Computation Interval', 'Output Interval', 'Instantaneous Interval',
                'Mapping Interval', 'Run HTab', 'Run UNet', 'Run Sediment', 'Run PostProcess',
                'Run WQNet', 'Run RASMapper', 'UNET Use Existing IB Tables', 'HDF_Results_Path',
                'UNET 1D Methodology', 'Write IC File', 'Write IC File at Fixed DateTime',
                'IC Time', 'Write IC File Reoccurance', 'Write IC File at Sim End'
            ]
            
            # Additional convenience columns
            file_path_cols = ['Geom File', 'Geom Path', 'Flow File', 'Flow Path']
            
            # Special columns that must be preserved
            special_cols = ['HDF_Results_Path']
            
            # Build the final column list
            all_cols = first_cols.copy()
            
            # Add plan key columns if they exist
            for col in plan_key_cols:
                if col in df.columns and col not in all_cols and col not in special_cols:
                    all_cols.append(col)
            
            # Add any remaining columns not explicitly specified
            other_cols = [col for col in df.columns if col not in all_cols + file_path_cols + special_cols + ['full_path']]
            all_cols.extend(other_cols)
            
            # Add HDF_Results_Path if it exists (ensure it comes before file paths)
            for special_col in special_cols:
                if special_col in df.columns and special_col not in all_cols:
                    all_cols.append(special_col)
            
            # Add file path columns at the end
            all_cols.extend(file_path_cols)
            
            # Rename plan_number column
            df = df.rename(columns={f'{entry_type.lower()}_number': 'plan_number'})
            
            # Fill in missing columns with None
            for col in all_cols:
                if col not in df.columns:
                    df[col] = None
            
            # Make sure full_path column is preserved and included
            if 'full_path' in df.columns and 'full_path' not in all_cols:
                all_cols.append('full_path')
            
            # Return DataFrame with specified column order
            cols_to_return = [col for col in all_cols if col in df.columns]
            return df[cols_to_return]
        
        return df

    @log_call
    def _get_prj_entries(self, entry_type):
        """
        Extract entries of a specific type from the HEC-RAS project file.
        """
        entries = []
        pattern = re.compile(rf"{entry_type} File=(\w+)")

        try:
            with open(self.prj_file, 'r') as file:
                for line in file:
                    match = pattern.match(line.strip())
                    if match:
                        file_name = match.group(1)
                        full_path = str(self.project_folder / f"{self.project_name}.{file_name}")
                        entry = self._create_entry(entry_type, file_name, full_path)
                        entries.append(entry)
        
            return self._format_dataframe(pd.DataFrame(entries), entry_type)
        
        except Exception as e:
            logger.error(f"Error in _get_prj_entries for {entry_type}: {e}")
            raise

    def _create_entry(self, entry_type, file_name, full_path):
        """Helper method to create entry dictionary."""
        entry_number = file_name[1:]
        entry = {
            f'{entry_type.lower()}_number': entry_number,
            'full_path': full_path,
            'unsteady_number': None,
            'geometry_number': None
        }
        
        if entry_type == 'Unsteady':
            entry['unsteady_number'] = entry_number
            entry.update(self._parse_unsteady_file(Path(full_path)))
        elif entry_type == 'Plan':
            self._update_plan_entry(entry, entry_number, full_path)
        
        return entry

    def _update_plan_entry(self, entry, entry_number, full_path):
        """Helper method to update plan entry with additional information."""
        plan_info = self._parse_plan_file(Path(full_path))
        if plan_info:
            # Handle Flow File
            flow_file = plan_info.get('Flow File')
            if flow_file:
                if flow_file.startswith('u'):
                    entry.update({'unsteady_number': flow_file[1:], 'Flow File': flow_file[1:]})
                else:
                    entry['Flow File'] = flow_file[1:] if flow_file.startswith('f') else None
            
            # Handle Geom File
            geom_file = plan_info.get('Geom File')
            if geom_file and geom_file.startswith('g'):
                entry.update({'geometry_number': geom_file[1:], 'Geom File': geom_file[1:]})
            
            # Add remaining plan info
            entry.update({k: v for k, v in plan_info.items() if k not in ['Flow File', 'Geom File']})
            
            # Add HDF results path
            hdf_path = self.project_folder / f"{self.project_name}.p{entry_number}.hdf"
            entry['HDF_Results_Path'] = str(hdf_path) if hdf_path.exists() else None


# Create a global instance named 'ras'
# Defining the global instance allows the init_ras_project function to initialize the project.
# This only happens on the library initialization, not when the user calls init_ras_project.
ras = RasPrj()

# END OF CLASS DEFINITION


# START OF FUNCTION DEFINITIONS

@log_call
def init_ras_project(ras_project_folder, ras_version=None, ras_object=None):
    """
    Initialize a RAS project for use with the ras-commander library.

    This is the primary function for setting up a HEC-RAS project. It:
    1. Finds the project file (.prj) in the specified folder OR uses the provided .prj file
    2. Validates .prj files by checking for "Proj Title=" marker
    3. Identifies the appropriate HEC-RAS executable
    4. Loads project data (plans, geometries, flows)
    5. Creates dataframes containing project components

    Args:
        ras_project_folder (str or Path): Path to the RAS project folder OR direct path to a .prj file.
                                          If a .prj file is provided:
                                          - File is validated to have .prj extension
                                          - File content is checked for "Proj Title=" marker
                                          - Parent folder is used as the project folder
        ras_version (str, optional): The version of RAS to use (e.g., "6.6") OR
                                     a full path to the Ras.exe file (e.g., "D:/Programs/HEC/HEC-RAS/6.6/Ras.exe").
                                     If None, will attempt to detect from plan files.
        ras_object (RasPrj, optional): If None, updates the global 'ras' object.
                                       If a RasPrj instance, updates that instance.
                                       If any other value, creates and returns a new RasPrj instance.

    Returns:
        RasPrj: An initialized RasPrj instance.

    Raises:
        FileNotFoundError: If the specified project folder or .prj file doesn't exist.
        ValueError: If the provided file is not a .prj file, does not contain "Proj Title=",
                    or if no HEC-RAS project file is found in the folder.

    Example:
        >>> # Initialize using project folder (existing behavior)
        >>> init_ras_project("/path/to/project", "6.6")
        >>> print(f"Initialized project: {ras.project_name}")
        >>>
        >>> # Initialize using direct .prj file path (new feature)
        >>> init_ras_project("/path/to/project/MyModel.prj", "6.6")
        >>> print(f"Initialized project: {ras.project_name}")
        >>>
        >>> # Create a new RasPrj instance with .prj file
        >>> my_project = init_ras_project("/path/to/project/MyModel.prj", "6.6", "new")
        >>> print(f"Created project instance: {my_project.project_name}")
    """
    # Convert to Path object for consistent handling
    input_path = Path(ras_project_folder).resolve()

    # Detect if input is a file or folder
    if input_path.is_file():
        # User provided a .prj file path
        if input_path.suffix.lower() != '.prj':
            error_msg = f"The provided file is not a HEC-RAS project file (.prj): {input_path}"
            logger.error(error_msg)
            raise ValueError(f"{error_msg}. Please provide either a project folder or a .prj file.")

        # Enhanced validation: Check if file contains "Proj Title=" to verify it's a HEC-RAS project file
        try:
            content, encoding = read_file_with_fallback_encoding(input_path)
            if content is None or "Proj Title=" not in content:
                error_msg = f"The file does not appear to be a valid HEC-RAS project file (missing 'Proj Title='): {input_path}"
                logger.error(error_msg)
                raise ValueError(f"{error_msg}. Please provide a valid HEC-RAS .prj file.")
            logger.debug(f"Validated .prj file contains 'Proj Title=' marker")
        except Exception as e:
            error_msg = f"Error validating .prj file: {e}"
            logger.error(error_msg)
            raise ValueError(f"{error_msg}. Please ensure the file is a valid HEC-RAS project file.")

        # Extract the parent folder to use as project_folder
        project_folder = input_path.parent
        specified_prj_file = input_path  # Store for optimization
        logger.debug(f"User provided .prj file: {input_path}")
        logger.debug(f"Using parent folder as project_folder: {project_folder}")

    elif input_path.is_dir():
        # User provided a folder path (existing behavior)
        project_folder = input_path
        specified_prj_file = None
        logger.debug(f"User provided folder path: {project_folder}")

    else:
        # Path doesn't exist
        if input_path.suffix.lower() == '.prj':
            error_msg = f"The specified .prj file does not exist: {input_path}"
            logger.error(error_msg)
            raise FileNotFoundError(f"{error_msg}. Please check the path and try again.")
        else:
            error_msg = f"The specified RAS project folder does not exist: {input_path}"
            logger.error(error_msg)
            raise FileNotFoundError(f"{error_msg}. Please check the path and try again.")

    # Determine which RasPrj instance to use
    if ras_object is None:
        # Use the global 'ras' object
        logger.debug("Initializing global 'ras' object via init_ras_project function.")
        ras_object = ras
    elif not isinstance(ras_object, RasPrj):
        # Create a new RasPrj instance
        logger.debug("Creating a new RasPrj instance.")
        ras_object = RasPrj()
    
    ras_exe_path = None
    
    # Use version specified by user if provided
    if ras_version is not None:
        ras_exe_path = get_ras_exe(ras_version)
        if ras_exe_path == "Ras.exe" and ras_version != "Ras.exe":
            logger.warning(f"HEC-RAS Version {ras_version} was not found. Running HEC-RAS will fail.")
    else:
        # No version specified, try to detect from plan files
        detected_version = None
        logger.info("No HEC-RAS Version Specified.Attempting to detect HEC-RAS version from plan files.")
        
        # Look for .pXX files in project folder
        logger.info(f"Searching for plan files in {project_folder}")
        # Search for any file with .p01 through .p99 extension, regardless of base name
        plan_files = list(project_folder.glob("*.p[0-9][0-9]"))
        
        if not plan_files:
            logger.info(f"No plan files found in {project_folder}")
        
        for plan_file in plan_files:
            logger.info(f"Found plan file: {plan_file.name}")
            content, encoding = read_file_with_fallback_encoding(plan_file)
            
            if not content:
                logger.info(f"Could not read content from {plan_file.name}")
                continue
                
            logger.info(f"Successfully read plan file with {encoding} encoding")
            
            # Look for Program Version in plan file
            for line in content.splitlines():
                if line.startswith("Program Version="):
                    version = line.split("=")[1].strip()
                    logger.info(f"Found Program Version={version} in {plan_file.name}")
                    
                    # Replace 00 in version string if present
                    if "00" in version:
                        version = version.replace("00", "0")
                    
                    # Try to get RAS executable for this version
                    test_exe_path = get_ras_exe(version)
                    logger.info(f"Checking RAS executable path: {test_exe_path}")
                    
                    if test_exe_path != "Ras.exe":
                        detected_version = version
                        ras_exe_path = test_exe_path
                        logger.debug(f"Found valid HEC-RAS version {version} in plan file {plan_file.name}")
                        break
                    else:
                        logger.info(f"Version {version} not found in default installation path")
            
            if detected_version:
                break
        
        if not detected_version:
            logger.error("No valid HEC-RAS version found in any plan files.")
            ras_exe_path = "Ras.exe"
            logger.warning("No valid HEC-RAS version was detected. Running HEC-RAS will fail.")
    
    # Initialize or re-initialize with the determined executable path
    # Pass specified_prj_file to avoid re-searching when user provided .prj file directly
    if specified_prj_file is not None:
        ras_object.initialize(project_folder, ras_exe_path, prj_file=specified_prj_file)
    else:
        ras_object.initialize(project_folder, ras_exe_path)

    # Store version for RasControl (legacy COM interface support)
    ras_object.ras_version = ras_version if ras_version else detected_version

    # Always update the global ras object as well
    if ras_object is not ras:
        if specified_prj_file is not None:
            ras.initialize(project_folder, ras_exe_path, prj_file=specified_prj_file)
        else:
            ras.initialize(project_folder, ras_exe_path)
        # Also store version in global ras object
        ras.ras_version = ras_version if ras_version else detected_version
        logger.debug("Global 'ras' object also updated to match the new project.")

    logger.debug(f"Project initialized. Project folder: {ras_object.project_folder}")
    logger.debug(f"Using HEC-RAS executable: {ras_exe_path}")
    return ras_object

@log_call
def get_ras_exe(ras_version=None):
    """
    Determine the HEC-RAS executable path based on the input.
    
    This function attempts to find the HEC-RAS executable in the following order:
    1. If ras_version is a valid file path to an .exe file, use that path directly
       (useful for non-standard installations or non-C: drive installations)
    2. If ras_version is a known version number, use default installation path (on C: drive)
    3. If global 'ras' object has ras_exe_path, use that
    4. As a fallback, return "Ras.exe" but log an error
    
    Args:
        ras_version (str, optional): Either a version number (e.g., "6.6") or 
                                     a full path to the HEC-RAS executable 
                                     (e.g., "D:/Programs/HEC/HEC-RAS/6.6/Ras.exe").
    
    Returns:
        str: The full path to the HEC-RAS executable or "Ras.exe" if not found.
    
    Note:
        - HEC-RAS version numbers include: "6.6", "6.5", "6.4.1", "6.3", etc.
        - The default installation path follows: C:/Program Files (x86)/HEC/HEC-RAS/{version}/Ras.exe
        - For non-standard installations, provide the full path to Ras.exe
        - Returns "Ras.exe" if no valid path is found, with error logged
        - Allows the library to function even without HEC-RAS installed
    """
    if ras_version is None:
        if hasattr(ras, 'ras_exe_path') and ras.ras_exe_path:
            logger.debug(f"Using HEC-RAS executable from global 'ras' object: {ras.ras_exe_path}")
            return ras.ras_exe_path
        else:
            default_path = "Ras.exe"
            logger.debug(f"No HEC-RAS version specified and global 'ras' object not initialized or missing ras_exe_path.")
            logger.warning(f"HEC-RAS is not installed or version not specified. Running HEC-RAS will fail unless a valid installed version is specified.")
            return default_path
    
    # ACTUAL folder names in C:/Program Files (x86)/HEC/HEC-RAS/
    # This list matches the exact folder names on disk (verified 2025-10-30)
    ras_version_folders = [
        "6.7 Beta 4", "6.6", "6.5", "6.4.1", "6.3.1", "6.3", "6.2", "6.1", "6.0",
        "5.0.7", "5.0.6", "5.0.5", "5.0.4", "5.0.3", "5.0.1", "5.0",
        "4.1.0", "4.0"
    ]

    # User-friendly aliases (user_input → actual_folder_name)
    # Allows users to pass "4.1" and find "4.1.0" folder, or "66" to find "6.6"
    version_aliases = {
        # 4.x aliases
        "4.1": "4.1.0",      # User passes "4.1" → finds "4.1.0" folder
        "41": "4.1.0",       # Compact format
        "410": "4.1.0",      # Full compact
        "40": "4.0",         # Compact format for 4.0

        # 5.0.x aliases
        "50": "5.0",         # Compact format
        "501": "5.0.1",      # Compact format for 5.0.1
        "503": "5.0.3",      # Compact format
        "504": "5.0.4",      # Compact format for 5.0.4
        "505": "5.0.5",
        "506": "5.0.6",
        "507": "5.0.7",

        # 6.x aliases
        "60": "6.0",
        "61": "6.1",
        "62": "6.2",
        "63": "6.3",
        "631": "6.3.1",
        "6.4": "6.4.1",      # No 6.4 folder, use 6.4.1
        "64": "6.4.1",
        "641": "6.4.1",
        "65": "6.5",
        "66": "6.6",
        "6.7": "6.7 Beta 4", # User passes "6.7" → finds "6.7 Beta 4"
        "67": "6.7 Beta 4",
    }

    # Check if input is a direct path to an executable
    hecras_path = Path(ras_version)
    if hecras_path.is_file() and hecras_path.suffix.lower() == '.exe':
        logger.debug(f"HEC-RAS executable found at specified path: {hecras_path}")
        return str(hecras_path)

    version_str = str(ras_version)

    # Check if there's an alias for this version
    if version_str in version_aliases:
        actual_folder = version_aliases[version_str]
        logger.debug(f"Mapped version '{version_str}' to folder '{actual_folder}'")
        version_str = actual_folder

    # Check if this is a known folder name
    if version_str in ras_version_folders:
        default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{version_str}/Ras.exe")
        if default_path.is_file():
            logger.debug(f"HEC-RAS executable found at default path: {default_path}")
            return str(default_path)
        else:
            error_msg = f"HEC-RAS Version {version_str} folder exists but Ras.exe not found at expected path. Running HEC-RAS will fail."
            logger.error(error_msg)
            return "Ras.exe"
    
    # Final fallback: Try to find a matching version from folder list
    try:
        # Try to find a matching version from our list
        for known_folder in ras_version_folders:
            # Check for partial matches or compact formats
            if version_str in known_folder or known_folder.replace('.', '') == version_str:
                default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{known_folder}/Ras.exe")
                if default_path.is_file():
                    logger.debug(f"HEC-RAS executable found via fuzzy match: {default_path}")
                    return str(default_path)

        # Try direct path construction for newer versions
        if '.' in version_str:
            default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{version_str}/Ras.exe")
            if default_path.is_file():
                logger.debug(f"HEC-RAS executable found at path: {default_path}")
                return str(default_path)
    except Exception as e:
        logger.error(f"Error parsing version or finding path: {e}")

    error_msg = f"HEC-RAS Version {ras_version} is not recognized or installed. Running HEC-RAS will fail unless a valid installed version is specified."
    logger.error(error_msg)
    return "Ras.exe"

==================================================

File: c:\GH\ras-commander\ras_commander\RasUnsteady.py
==================================================
"""
RasUnsteady - Operations for handling unsteady flow files in HEC-RAS projects.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function():
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasUnsteady:
- update_flow_title()
- update_restart_settings()
- extract_boundary_and_tables()
- print_boundaries_and_tables()
- identify_tables()
- parse_fixed_width_table()
- extract_tables()
- write_table_to_file()
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call
import pandas as pd
import numpy as np
import re
from typing import Union, Optional, Any, Tuple, Dict, List



logger = get_logger(__name__)

# Module code starts here

class RasUnsteady:
    """
    Class for all operations related to HEC-RAS unsteady flow files.
    """
    @staticmethod
    @log_call
    def update_flow_title(unsteady_file: str, new_title: str, ras_object: Optional[Any] = None) -> None:
        """
        Update the Flow Title in an unsteady flow file (.u*).

        The Flow Title provides a descriptive identifier for unsteady flow scenarios in HEC-RAS. 
        It appears in the HEC-RAS interface and helps differentiate between different flow files.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file or unsteady flow number
            new_title (str): New flow title (max 24 characters, will be truncated if longer)
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            None: The function modifies the file in-place and updates the ras object's unsteady dataframe

        Example:
            # Clone an existing unsteady flow file
            new_unsteady_number = RasPlan.clone_unsteady("02")
            
            # Get path to the new unsteady flow file
            new_unsteady_file = RasPlan.get_unsteady_path(new_unsteady_number)
            
            # Update the flow title
            new_title = "Modified Flow Scenario"
            RasUnsteady.update_flow_title(new_unsteady_file, new_title)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        new_title = new_title[:24]  # Truncate to 24 characters if longer
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        for i, line in enumerate(lines):
            if line.startswith("Flow Title="):
                old_title = line.strip().split('=')[1]
                lines[i] = f"Flow Title={new_title}\n"
                updated = True
                logger.info(f"Updated Flow Title from '{old_title}' to '{new_title}'")
                break
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied Flow Title modification to {unsteady_file}")
        else:
            logger.warning(f"Flow Title not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def update_restart_settings(unsteady_file: str, use_restart: bool, restart_filename: Optional[str] = None, ras_object: Optional[Any] = None) -> None:
        """
        Update the restart file settings in an unsteady flow file.

        Restart files in HEC-RAS allow simulations to continue from a previously saved state,
        which is useful for long simulations or when making downstream changes.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            use_restart (bool): Whether to use a restart file (True) or not (False)
            restart_filename (str, optional): Path to the restart file (.rst)
                                             Required if use_restart is True
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            None: The function modifies the file in-place and updates the ras object's unsteady dataframe

        Example:
            # Enable restart file for an unsteady flow
            unsteady_file = RasPlan.get_unsteady_path("03")
            RasUnsteady.update_restart_settings(
                unsteady_file, 
                use_restart=True, 
                restart_filename="model_restart.rst"
            )
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        restart_line_index = None
        for i, line in enumerate(lines):
            if line.startswith("Use Restart="):
                restart_line_index = i
                old_value = line.strip().split('=')[1]
                new_value = "-1" if use_restart else "0"
                lines[i] = f"Use Restart={new_value}\n"
                updated = True
                logger.info(f"Updated Use Restart from {old_value} to {new_value}")
                break
        
        if use_restart:
            if not restart_filename:
                logger.error("Restart filename must be specified when enabling restart.")
                raise ValueError("Restart filename must be specified when enabling restart.")
            if restart_line_index is not None:
                lines.insert(restart_line_index + 1, f"Restart Filename={restart_filename}\n")
                logger.info(f"Added Restart Filename: {restart_filename}")
            else:
                logger.warning("Could not find 'Use Restart' line to insert 'Restart Filename'")
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied restart settings modification to {unsteady_file}")
        else:
            logger.warning(f"Use Restart setting not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def extract_boundary_and_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> pd.DataFrame:
        """
        Extract boundary conditions and their associated tables from an unsteady flow file.

        Boundary conditions in HEC-RAS define time-varying inputs like flow hydrographs,
        stage hydrographs, gate operations, and lateral inflows. This function parses these
        conditions and their data tables from the unsteady flow file.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            pd.DataFrame: DataFrame containing boundary conditions with the following columns:
                - River Name, Reach Name, River Station: Location information
                - DSS File: Associated DSS file path if any
                - Tables: Dictionary containing DataFrames of time-series values

        Example:
            # Get the path to unsteady flow file "02"
            unsteady_file = RasPlan.get_unsteady_path("02")
            
            # Extract boundary conditions and tables
            boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)
            print(f"Extracted {len(boundaries_df)} boundary conditions from the file.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        table_types = [
            'Flow Hydrograph=', 
            'Gate Openings=', 
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=', 
            'Lateral Inflow Hydrograph='
        ]
        
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Initialize variables
        boundary_data = []
        current_boundary = None
        current_tables = {}
        current_table = None
        table_values = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # Check for Boundary Location line
            if line.startswith("Boundary Location="):
                # Save previous boundary if it exists
                if current_boundary is not None:
                    if current_table and table_values:
                        # Process any remaining table
                        try:
                            df = pd.DataFrame({'Value': table_values})
                            current_tables[current_table_name] = df
                        except Exception as e:
                            logger.warning(f"Error processing table {current_table_name}: {e}")
                    current_boundary['Tables'] = current_tables
                    boundary_data.append(current_boundary)
                
                # Start new boundary
                current_boundary = {
                    'Boundary Location': line.split('=', 1)[1].strip(),
                    'DSS File': '',
                    'Tables': {}
                }
                current_tables = {}
                current_table = None
                table_values = []
                
            # Check for DSS File line
            elif line.startswith("DSS File=") and current_boundary is not None:
                current_boundary['DSS File'] = line.split('=', 1)[1].strip()
                
            # Check for table headers
            elif any(line.startswith(t) for t in table_types) and current_boundary is not None:
                # If we were processing a table, save it
                if current_table and table_values:
                    try:
                        df = pd.DataFrame({'Value': table_values})
                        current_tables[current_table_name] = df
                    except Exception as e:
                        logger.warning(f"Error processing previous table: {e}")
                
                # Start new table
                try:
                    current_table = line.split('=')
                    current_table_name = current_table[0].strip()
                    num_values = int(current_table[1])
                    table_values = []
                    
                    # Read the table values
                    rows_needed = (num_values + 9) // 10  # Round up division
                    for _ in range(rows_needed):
                        i += 1
                        if i >= len(lines):
                            break
                        row = lines[i].strip()
                        # Parse fixed-width values (8 characters each)
                        j = 0
                        while j < len(row):
                            value_str = row[j:j+8].strip()
                            if value_str:
                                try:
                                    value = float(value_str)
                                    table_values.append(value)
                                except ValueError:
                                    # Try splitting merged values
                                    parts = re.findall(r'-?\d+\.?\d*', value_str)
                                    table_values.extend([float(p) for p in parts])
                            j += 8
                
                except (ValueError, IndexError) as e:
                    logger.error(f"Error processing table at line {i}: {e}")
                    current_table = None
                    
            i += 1
        
        # Add the last boundary if it exists
        if current_boundary is not None:
            if current_table and table_values:
                try:
                    df = pd.DataFrame({'Value': table_values})
                    current_tables[current_table_name] = df
                except Exception as e:
                    logger.warning(f"Error processing final table: {e}")
            current_boundary['Tables'] = current_tables
            boundary_data.append(current_boundary)
        
        # Create DataFrame
        boundaries_df = pd.DataFrame(boundary_data)
        if not boundaries_df.empty:
            # Split boundary location into components
            location_columns = ['River Name', 'Reach Name', 'River Station', 
                              'Downstream River Station', 'Storage Area Connection',
                              'Storage Area Name', 'Pump Station Name', 
                              'Blank 1', 'Blank 2']
            split_locations = boundaries_df['Boundary Location'].str.split(',', expand=True)
            # Ensure we have the right number of columns
            for i, col in enumerate(location_columns):
                if i < split_locations.shape[1]:
                    boundaries_df[col] = split_locations[i].str.strip()
                else:
                    boundaries_df[col] = ''
            boundaries_df = boundaries_df.drop(columns=['Boundary Location'])
        
        logger.info(f"Successfully extracted boundaries and tables from {unsteady_path}")
        return boundaries_df

    @staticmethod
    @log_call
    def print_boundaries_and_tables(boundaries_df: pd.DataFrame) -> None:
        """
        Print boundary conditions and their associated tables in a formatted, readable way.

        This function is useful for quickly visualizing the complex nested structure of 
        boundary conditions extracted by extract_boundary_and_tables().

        Parameters:
            boundaries_df (pd.DataFrame): DataFrame containing boundary information and 
                                         nested tables data from extract_boundary_and_tables()

        Returns:
            None: Output is printed to console

        Example:
            # Extract boundary conditions and tables
            boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)
            
            # Print in a formatted way
            print("Detailed boundary conditions and tables:")
            RasUnsteady.print_boundaries_and_tables(boundaries_df)
        """
        pd.set_option('display.max_columns', None)
        pd.set_option('display.max_rows', None)
        print("\nBoundaries and Tablesin boundaries_df:")
        for idx, row in boundaries_df.iterrows():
            print(f"\nBoundary {idx+1}:")
            print(f"River Name: {row['River Name']}")
            print(f"Reach Name: {row['Reach Name']}")
            print(f"River Station: {row['River Station']}")
            print(f"DSS File: {row['DSS File']}")
            
            if row['Tables']:
                print("\nTables for this boundary:")
                for table_name, table_df in row['Tables'].items():
                    print(f"\n{table_name}:")
                    print(table_df.to_string())
            print("-" * 80)





# Additional functions from the AWS webinar where the code was developed
# Need to add examples

    @staticmethod
    @log_call
    def identify_tables(lines: List[str]) -> List[Tuple[str, int, int]]:
        """
        Identify the start and end line numbers of tables in an unsteady flow file.

        HEC-RAS unsteady flow files contain numeric tables in a fixed-width format.
        This function locates these tables within the file and provides their positions.

        Parameters:
            lines (List[str]): List of file lines (typically from file.readlines())

        Returns:
            List[Tuple[str, int, int]]: List of tuples where each tuple contains:
                - table_name (str): The type of table (e.g., 'Flow Hydrograph=')
                - start_line (int): Line number where the table data begins
                - end_line (int): Line number where the table data ends

        Example:
            # Read the unsteady flow file
            with open(new_unsteady_file, 'r') as f:
                lines = f.readlines()
                
            # Identify tables in the file
            tables = RasUnsteady.identify_tables(lines)
            print(f"Identified {len(tables)} tables in the unsteady flow file.")
        """
        table_types = [
            'Flow Hydrograph=', 
            'Gate Openings=', 
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=', 
            'Lateral Inflow Hydrograph='
        ]
        tables = []
        current_table = None
        
        for i, line in enumerate(lines):
            if any(table_type in line for table_type in table_types):
                if current_table:
                    tables.append((current_table[0], current_table[1], i-1))
                table_name = line.strip().split('=')[0] + '='
                try:
                    num_values = int(line.strip().split('=')[1])
                    current_table = (table_name, i+1, num_values)
                except (ValueError, IndexError) as e:
                    logger.error(f"Error parsing table header at line {i}: {e}")
                    continue
        
        if current_table:
            tables.append((current_table[0], current_table[1], 
                          current_table[1] + (current_table[2] + 9) // 10))
        
        logger.debug(f"Identified {len(tables)} tables in the file")
        return tables

    @staticmethod
    @log_call
    def parse_fixed_width_table(lines: List[str], start: int, end: int) -> pd.DataFrame:
        """
        Parse a fixed-width table from an unsteady flow file into a pandas DataFrame.

        HEC-RAS uses a fixed-width format (8 characters per value) for numeric tables.
        This function converts this format into a DataFrame for easier manipulation.

        Parameters:
            lines (List[str]): List of file lines (from file.readlines())
            start (int): Starting line number for table data
            end (int): Ending line number for table data

        Returns:
            pd.DataFrame: DataFrame with a single column 'Value' containing the parsed numeric values

        Example:
            # Identify tables in the file
            tables = RasUnsteady.identify_tables(lines)
            
            # Parse a specific table (e.g., first flow hydrograph)
            table_name, start_line, end_line = tables[0]
            table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)
        """
        data = []
        for line in lines[start:end]:
            # Skip empty lines or lines that don't contain numeric data
            if not line.strip() or not any(c.isdigit() for c in line):
                continue
                
            # Split the line into 8-character columns and process each value
            values = []
            for i in range(0, len(line.rstrip()), 8):
                value_str = line[i:i+8].strip()
                if value_str:  # Only process non-empty strings
                    try:
                        # Handle special cases where numbers are run together
                        if len(value_str) > 8:
                            # Use regex to find all numbers in the string
                            parts = re.findall(r'-?\d+\.?\d*', value_str)
                            values.extend([float(p) for p in parts])
                        else:
                            values.append(float(value_str))
                    except ValueError:
                        # If conversion fails, try to extract any valid numbers from the string
                        parts = re.findall(r'-?\d+\.?\d*', value_str)
                        if parts:
                            values.extend([float(p) for p in parts])
                        else:
                            logger.debug(f"Skipping non-numeric value: {value_str}")
                            continue
            
            # Only add to data if we found valid numeric values
            if values:
                data.extend(values)
        
        if not data:
            logger.warning("No numeric data found in table section")
            return pd.DataFrame(columns=['Value'])
            
        return pd.DataFrame(data, columns=['Value'])
    
    @staticmethod
    @log_call
    def extract_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> Dict[str, pd.DataFrame]:
        """
        Extract all tables from an unsteady flow file and return them as DataFrames.

        This function combines identify_tables() and parse_fixed_width_table() to extract
        all tables from an unsteady flow file in a single operation.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            Dict[str, pd.DataFrame]: Dictionary where:
                - Keys are table names (e.g., 'Flow Hydrograph=')
                - Values are DataFrames with a 'Value' column containing numeric data

        Example:
            # Extract all tables from the unsteady flow file
            all_tables = RasUnsteady.extract_tables(new_unsteady_file)
            print(f"Extracted {len(all_tables)} tables from the file.")
            
            # Access a specific table
            flow_tables = [name for name in all_tables.keys() if 'Flow Hydrograph=' in name]
            if flow_tables:
                flow_df = all_tables[flow_tables[0]]
                print(f"Flow table has {len(flow_df)} values")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Fix: Use RasUnsteady.identify_tables 
        tables = RasUnsteady.identify_tables(lines)
        extracted_tables = {}
        
        for table_name, start, end in tables:
            df = RasUnsteady.parse_fixed_width_table(lines, start, end)
            extracted_tables[table_name] = df
            logger.debug(f"Extracted table '{table_name}' with {len(df)} values")
        
        return extracted_tables

    @staticmethod
    @log_call
    def write_table_to_file(unsteady_file: str, table_name: str, df: pd.DataFrame, 
                           start_line: int, ras_object: Optional[Any] = None) -> None:
        """
        Write an updated table back to an unsteady flow file in the required fixed-width format.

        This function takes a modified DataFrame and writes it back to the unsteady flow file,
        preserving the 8-character fixed-width format that HEC-RAS requires.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            table_name (str): Name of the table to update (e.g., 'Flow Hydrograph=')
            df (pd.DataFrame): DataFrame containing the updated values with a 'Value' column
            start_line (int): Line number where the table data begins in the file
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            None: The function modifies the file in-place

        Example:
            # Identify tables in the unsteady flow file
            tables = RasUnsteady.identify_tables(lines)
            table_name, start_line, end_line = tables[0]
            
            # Parse and modify the table
            table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)
            table_df['Value'] = table_df['Value'] * 0.75  # Scale values to 75%
            
            # Write modified table back to the file
            RasUnsteady.write_table_to_file(new_unsteady_file, table_name, table_df, start_line)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Format values into fixed-width strings
        formatted_values = []
        for i in range(0, len(df), 10):
            row = df['Value'].iloc[i:i+10]
            formatted_row = ''.join(f'{value:8.2f}' for value in row)
            formatted_values.append(formatted_row + '\n')
        
        # Replace old table with new formatted values
        lines[start_line:start_line+len(formatted_values)] = formatted_values
        
        try:
            with open(unsteady_path, 'w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated table '{table_name}' in {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            raise
        except IOError as e:
            logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            raise








'''



Flow Title=Single 2D Area with Bridges
Program Version=6.60
Use Restart= 0 
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DSNormalDepth                   ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalD                      ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,Upstream Inflow                 ,                                
Interval=1HOUR
Flow Hydrograph= 200 
    1000    3000    6500    8000    9500   11000   12500   14000   15500   17000
   18500   20000   22000   24000   26000   28000   30000   34000   38000   42000
   46000   50000   54000   58000   62000   66000   70000   73000   76000   79000
   82000   85000   87200   89400   91600   93800   96000   96800   97600   98400
   99200  100000   99600   99200   98800   98400   98000   96400   94800   93200
   91600   90000   88500   87000   85500   84000   82500   81000   79500   78000
   76500   75000   73500   7200070666.6669333.34   6800066666.6665333.33   64000
62666.6761333.33   6000058666.6757333.33   5600054666.6753333.33   5200050666.67
49333.33   4800046666.6745333.33   4400042666.6741333.33   4000039166.6738333.33
   3750036666.6735833.33   3500034166.6733333.33   3250031666.6730833.33   30000
29166.6728333.33   2750026666.6725833.33   2500024166.6723333.33   2250021666.67
20833.33   2000019655.1719310.3518965.5218620.6918275.8617931.0417586.2117241.38
16896.5516551.72 16206.915862.0715517.2415172.4114827.5914482.7614137.93 13793.1
13448.2813103.4512758.6212413.7912068.9711724.1411379.3111034.4810689.6610344.83
   10000 9915.25 9830.51 9745.76 9661.02 9576.27 9491.53 9406.78 9322.03 9237.29
 9152.54  9067.8 8983.05 8898.31 8813.56 8728.81 8644.07 8559.32 8474.58 8389.83
 8305.09 8220.34 8135.59 8050.85  7966.1 7881.36 7796.61 7711.86 7627.12 7542.37
 7457.63 7372.88 7288.14 7203.39 7118.64  7033.9 6949.15 6864.41 6779.66 6694.92
 6610.17 6525.42 6440.68 6355.93 6271.19 6186.44  6101.7 6016.95  5932.2 5847.46
 5762.71 5677.97 5593.22 5508.48 5423.73 5338.98 5254.24 5169.49 5084.75    5000
Stage Hydrograph TW Check=0
Flow Hydrograph QMult= 0.5 
Flow Hydrograph Slope= 0.0005 
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,Sayers Dam      ,                ,                ,                                ,                                
Gate Name=Gate #1     
Gate DSS Path=
Gate Use DSS=False
Gate Time Interval=1HOUR
Gate Use Fixed Start Time=False
Gate Fixed Start Date/Time=,
Gate Openings= 100 
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalDepth                  ,                                
Friction Slope=0.0003,0
Met Point Raster Parameters=,,,,
Precipitation Mode=Disable
Wind Mode=No Wind Forces
Air Density Mode=
Wave Mode=No Wave Forcing
Met BC=Precipitation|Expanded View=0
Met BC=Precipitation|Point Interpolation=Nearest
Met BC=Precipitation|Gridded Source=DSS
Met BC=Precipitation|Gridded Interpolation=
Met BC=Evapotranspiration|Expanded View=0
Met BC=Evapotranspiration|Point Interpolation=Nearest
Met BC=Evapotranspiration|Gridded Source=DSS
Met BC=Evapotranspiration|Gridded Interpolation=
Met BC=Wind Speed|Expanded View=0
Met BC=Wind Speed|Constant Units=ft/s
Met BC=Wind Speed|Point Interpolation=Nearest
Met BC=Wind Speed|Gridded Source=DSS
Met BC=Wind Speed|Gridded Interpolation=
Met BC=Wind Direction|Expanded View=0
Met BC=Wind Direction|Point Interpolation=Nearest
Met BC=Wind Direction|Gridded Source=DSS
Met BC=Wind Direction|Gridded Interpolation=
Met BC=Wind Velocity X|Expanded View=0
Met BC=Wind Velocity X|Constant Units=ft/s
Met BC=Wind Velocity X|Point Interpolation=Nearest
Met BC=Wind Velocity X|Gridded Source=DSS
Met BC=Wind Velocity X|Gridded Interpolation=
Met BC=Wind Velocity Y|Expanded View=0
Met BC=Wind Velocity Y|Constant Units=ft/s
Met BC=Wind Velocity Y|Point Interpolation=Nearest
Met BC=Wind Velocity Y|Gridded Source=DSS
Met BC=Wind Velocity Y|Gridded Interpolation=
Met BC=Wave Forcing X|Expanded View=0
Met BC=Wave Forcing X|Point Interpolation=Nearest
Met BC=Wave Forcing X|Gridded Source=DSS
Met BC=Wave Forcing X|Gridded Interpolation=
Met BC=Wave Forcing Y|Expanded View=0
Met BC=Wave Forcing Y|Point Interpolation=Nearest
Met BC=Wave Forcing Y|Gridded Source=DSS
Met BC=Wave Forcing Y|Gridded Interpolation=
Met BC=Air Density|Mode=Constant
Met BC=Air Density|Expanded View=0
Met BC=Air Density|Constant Value=1.225
Met BC=Air Density|Constant Units=kg/m3
Met BC=Air Density|Point Interpolation=Nearest
Met BC=Air Density|Gridded Source=DSS
Met BC=Air Density|Gridded Interpolation=
Met BC=Air Temperature|Expanded View=0
Met BC=Air Temperature|Point Interpolation=Nearest
Met BC=Air Temperature|Gridded Source=DSS
Met BC=Air Temperature|Gridded Interpolation=
Met BC=Humidity|Expanded View=0
Met BC=Humidity|Point Interpolation=Nearest
Met BC=Humidity|Gridded Source=DSS
Met BC=Humidity|Gridded Interpolation=
Met BC=Air Pressure|Mode=Constant
Met BC=Air Pressure|Expanded View=0
Met BC=Air Pressure|Constant Value=1013.2
Met BC=Air Pressure|Constant Units=mb
Met BC=Air Pressure|Point Interpolation=Nearest
Met BC=Air Pressure|Gridded Source=DSS
Met BC=Air Pressure|Gridded Interpolation=
Non-Newtonian Method= 0 , 
Non-Newtonian Constant Vol Conc=0
Non-Newtonian Yield Method= 0 , 
Non-Newtonian Yield Coef=0, 0
User Yeild=   0
Non-Newtonian Sed Visc= 0 , 
Non-Newtonian Obrian B=0
User Viscosity=0
User Viscosity Ratio=0
Herschel-Bulkley Coef=0, 0
Clastic Method= 0 , 
Coulomb Phi=0
Voellmy X=0
Non-Newtonian Hindered FV= 0 
Non-Newtonian FV K=0
Non-Newtonian ds=0
Non-Newtonian Max Cv=0
Non-Newtonian Bulking Method= 0 , 
Non-Newtonian High C Transport= 0 , 
Lava Activation= 0 
Temperature=1300,15,,15,14,980
Heat Ballance=1,1200,0.5,1,70,0.95
Viscosity=1000,,,
Yield Strength=,,,
Consistency Factor=,,,
Profile Coefficient=4,1.3,
Lava Param=,2500,




'''





==================================================

File: c:\GH\ras-commander\ras_commander\RasUtils.py
==================================================
"""
RasUtils - Utility functions for the ras-commander library

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function():
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasUtils:
- create_directory()
- find_files_by_extension()
- get_file_size()
- get_file_modification_time()
- normalize_ras_number()
- get_plan_path()
- remove_with_retry()
- update_plan_file()
- check_file_access()
- convert_to_dataframe()
- save_to_excel()
- calculate_rmse()
- calculate_percent_bias()
- calculate_error_metrics()
- update_file()
- get_next_number()
- clone_file()
- update_project_file()
- decode_byte_strings()
- perform_kdtree_query()
- find_nearest_neighbors()
- consolidate_dataframe()
- find_nearest_value()
- horizontal_distance()
    
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from typing import Union, Optional, Dict, Callable, List, Tuple, Any
import pandas as pd
import numpy as np
import shutil
import re
from scipy.spatial import KDTree
import datetime
import time
import h5py
from datetime import timedelta
from numbers import Number
from .LoggingConfig import get_logger
from .Decorators import log_call


logger = get_logger(__name__)
# Module code starts here

class RasUtils:
    """
    A class containing utility functions for the ras-commander library.
    When integrating new functions that do not clearly fit into other classes, add them here.
    """

    @staticmethod
    @log_call
    def create_directory(directory_path: Path, ras_object=None) -> Path:
        """
        Ensure that a directory exists, creating it if necessary.

        Parameters:
        directory_path (Path): Path to the directory
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Path to the ensured directory

        Example:
        >>> ensured_dir = RasUtils.create_directory(Path("output"))
        >>> print(f"Directory ensured: {ensured_dir}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(directory_path)
        try:
            path.mkdir(parents=True, exist_ok=True)
            logger.info(f"Directory ensured: {path}")
        except Exception as e:
            logger.error(f"Failed to create directory {path}: {e}")
            raise
        return path

    @staticmethod
    @log_call
    def find_files_by_extension(extension: str, ras_object=None) -> list:
        """
        List all files in the project directory with a specific extension.

        Parameters:
        extension (str): File extension to filter (e.g., '.prj')
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        list: List of file paths matching the extension

        Example:
        >>> prj_files = RasUtils.find_files_by_extension('.prj')
        >>> print(f"Found {len(prj_files)} .prj files")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            files = list(ras_obj.project_folder.glob(f"*{extension}"))
            file_list = [str(file) for file in files]
            logger.info(f"Found {len(file_list)} files with extension '{extension}' in {ras_obj.project_folder}")
            return file_list
        except Exception as e:
            logger.error(f"Failed to find files with extension '{extension}': {e}")
            raise

    @staticmethod
    @log_call
    def get_file_size(file_path: Path, ras_object=None) -> Optional[int]:
        """
        Get the size of a file in bytes.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[int]: Size of the file in bytes, or None if the file does not exist

        Example:
        >>> size = RasUtils.get_file_size(Path("project.prj"))
        >>> print(f"File size: {size} bytes")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                size = path.stat().st_size
                logger.info(f"Size of {path}: {size} bytes")
                return size
            except Exception as e:
                logger.error(f"Failed to get size for {path}: {e}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def get_file_modification_time(file_path: Path, ras_object=None) -> Optional[float]:
        """
        Get the last modification time of a file.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[float]: Last modification time as a timestamp, or None if the file does not exist

        Example:
        >>> mtime = RasUtils.get_file_modification_time(Path("project.prj"))
        >>> print(f"Last modified: {mtime}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                mtime = path.stat().st_mtime
                logger.info(f"Last modification time of {path}: {mtime}")
                return mtime
            except Exception as e:
                logger.exception(f"Failed to get modification time for {path}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def normalize_ras_number(ras_number: Union[str, int, float, Path, Number]) -> str:
        """
        Normalize RAS file numbers to two-digit string format.

        HEC-RAS uses two-digit file extensions for plans (.p01), geometries (.g02),
        flows (.f03), etc. This function standardizes various input formats to ensure
        consistent file path construction.

        Parameters:
        ras_number (Union[str, int, float, Path, Number]): Input number in various formats:
            - int: 1, 2, 3, etc.
            - str: "1", "01", "001", etc.
            - float: 1.0, 2.0 (must be whole numbers)
            - Path: Path("project.p05") - extracts number from extension
            - Number: numpy.int64(1), etc.

        Returns:
        str: Normalized two-digit format ("01", "02", ..., "99")

        Raises:
        ValueError: If the number is not between 1 and 99, or cannot be converted
        TypeError: If the input type is invalid

        Examples:
        >>> RasUtils.normalize_ras_number(1)
        '01'
        >>> RasUtils.normalize_ras_number("1")
        '01'
        >>> RasUtils.normalize_ras_number("01")
        '01'
        >>> RasUtils.normalize_ras_number("001")
        '01'
        >>> RasUtils.normalize_ras_number(np.int64(5))
        '05'
        >>> RasUtils.normalize_ras_number(Path("project.p02"))
        '02'

        Notes:
        - Used for plan numbers, geometry numbers, flow file numbers, etc.
        - Ensures consistent handling across all RAS file types
        - Prevents file path construction errors from unnormalized inputs
        """
        # Handle Path objects - extract number from file extension
        if isinstance(ras_number, Path):
            # Extract from extensions like .p01, .g02, .f03, etc.
            suffix = ras_number.suffix  # e.g., ".p01"
            if len(suffix) >= 2 and suffix[0] == '.':
                # Try to extract number after the letter (e.g., "01" from ".p01")
                number_part = suffix[2:]  # Skip "." and letter
                if number_part.isdigit():
                    ras_number = number_part
                else:
                    raise ValueError(
                        f"Cannot extract RAS number from Path extension: {ras_number}. "
                        f"Expected format like 'project.p01' or 'geom.g02'"
                    )
            else:
                raise ValueError(
                    f"Cannot extract RAS number from Path: {ras_number}. "
                    f"Expected file with RAS extension like .p01, .g02, etc."
                )

        # Convert to integer for validation
        try:
            # Handle string inputs - strip leading zeros before conversion
            if isinstance(ras_number, str):
                stripped = ras_number.lstrip('0')
                if not stripped or not stripped.isdigit():
                    # Handle edge cases like "0", "00", or non-numeric strings
                    if not stripped:  # Was all zeros
                        ras_int = 0
                    else:
                        raise ValueError(f"Cannot convert '{ras_number}' to integer")
                else:
                    ras_int = int(stripped)
            else:
                # Handle numeric types (int, float, numpy types, etc.)
                ras_int = int(ras_number)

                # Check if float had decimal component
                if isinstance(ras_number, (float, np.floating)) and ras_number != ras_int:
                    raise ValueError(
                        f"RAS numbers must be integers, got float with decimals: {ras_number}"
                    )

        except (ValueError, TypeError) as e:
            raise ValueError(
                f"Cannot convert RAS number '{ras_number}' (type: {type(ras_number).__name__}) "
                f"to integer: {e}"
            ) from e

        # Validate range (1-99 for HEC-RAS files)
        if not 1 <= ras_int <= 99:
            raise ValueError(
                f"RAS file number must be between 1 and 99, got: {ras_int}"
            )

        # Return normalized two-digit format
        normalized = f"{ras_int:02d}"
        logger.debug(f"Normalized RAS number '{ras_number}' to '{normalized}'")
        return normalized

    @staticmethod
    @log_call
    def get_plan_path(current_plan_number_or_path: Union[str, Number, Path], ras_object=None) -> Path:
        """
        Get the path for a plan file with a given plan number or path.

        Parameters:
        current_plan_number_or_path (Union[str, Number, Path]): The plan number (e.g., '01', 1, or 1.0) or full path to the plan file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Full path to the plan file

        Raises:
        ValueError: If plan number is not between 1 and 99
        TypeError: If input type is invalid
        FileNotFoundError: If the plan file does not exist

        Example:
        >>> plan_path = RasUtils.get_plan_path(1)
        >>> print(f"Plan file path: {plan_path}")
        >>> plan_path = RasUtils.get_plan_path("01")
        >>> print(f"Plan file path: {plan_path}")
        >>> plan_path = RasUtils.get_plan_path("path/to/plan.p01")
        >>> print(f"Plan file path: {plan_path}")
        """
        # Validate RAS object
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Handle direct file path input
        plan_path = Path(current_plan_number_or_path)
        if plan_path.is_file():
            logger.info(f"Using provided plan file path: {plan_path}")
            return plan_path

        # Handle plan number input - use centralized normalization
        try:
            current_plan_number = RasUtils.normalize_ras_number(current_plan_number_or_path)
            logger.debug(f"Normalized plan number to: {current_plan_number}")
        except (ValueError, TypeError) as e:
            logger.error(f"Invalid plan number: {current_plan_number_or_path}. {e}")
            raise
        
        # Construct and validate plan path
        plan_name = f"{ras_obj.project_name}.p{current_plan_number}"
        full_plan_path = ras_obj.project_folder / plan_name
        
        if not full_plan_path.exists():
            logger.error(f"Plan file does not exist: {full_plan_path}")
            raise FileNotFoundError(f"Plan file does not exist: {full_plan_path}")
        
        logger.info(f"Constructed plan file path: {full_plan_path}")
        return full_plan_path

    @staticmethod
    @log_call
    def remove_with_retry(
        path: Path,
        max_attempts: int = 5,
        initial_delay: float = 1.0,
        is_folder: bool = True,
        ras_object=None
    ) -> bool:
        """
        Attempts to remove a file or folder with retry logic and exponential backoff.

        Parameters:
        path (Path): Path to the file or folder to be removed.
        max_attempts (int): Maximum number of removal attempts.
        initial_delay (float): Initial delay between attempts in seconds.
        is_folder (bool): If True, the path is treated as a folder; if False, it's treated as a file.
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        bool: True if the file or folder was successfully removed, False otherwise.

        Example:
        >>> success = RasUtils.remove_with_retry(Path("temp_folder"), is_folder=True)
        >>> print(f"Removal successful: {success}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        path = Path(path)
        for attempt in range(1, max_attempts + 1):
            try:
                if path.exists():
                    if is_folder:
                        shutil.rmtree(path)
                        logger.info(f"Folder removed: {path}")
                    else:
                        path.unlink()
                        logger.info(f"File removed: {path}")
                else:
                    logger.info(f"Path does not exist, nothing to remove: {path}")
                return True
            except PermissionError as pe:
                if attempt < max_attempts:
                    delay = initial_delay * (2 ** (attempt - 1))  # Exponential backoff
                    logger.warning(
                        f"PermissionError on attempt {attempt} to remove {path}: {pe}. "
                        f"Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(
                        f"Failed to remove {path} after {max_attempts} attempts due to PermissionError: {pe}. Skipping."
                    )
                    return False
            except Exception as e:
                logger.exception(f"Failed to remove {path} on attempt {attempt}")
                return False
        return False

    @staticmethod
    @log_call
    def update_plan_file(
        plan_number_or_path: Union[str, Path],
        file_type: str,
        entry_number: int,
        ras_object=None
    ) -> None:
        """
        Update a plan file with a new file reference.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        file_type (str): Type of file to update ('Geom', 'Flow', or 'Unsteady')
        entry_number (int): Number (from 1 to 99) to set
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Raises:
        ValueError: If an invalid file_type is provided
        FileNotFoundError: If the plan file doesn't exist

        Example:
        >>> RasUtils.update_plan_file(1, "Geom", 2)
        >>> RasUtils.update_plan_file("path/to/plan.p01", "Geom", 2)
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        valid_file_types = {'Geom': 'g', 'Flow': 'f', 'Unsteady': 'u'}
        if file_type not in valid_file_types:
            logger.error(
                f"Invalid file_type '{file_type}'. Expected one of: {', '.join(valid_file_types.keys())}"
            )
            raise ValueError(
                f"Invalid file_type. Expected one of: {', '.join(valid_file_types.keys())}"
            )

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise FileNotFoundError(f"Plan file not found: {plan_file_path}")
        
        file_prefix = valid_file_types[file_type]
        search_pattern = f"{file_type} File="
        formatted_entry_number = f"{int(entry_number):02d}"  # Ensure two-digit format

        try:
            RasUtils.check_file_access(plan_file_path, 'r')
            with plan_file_path.open('r') as file:
                lines = file.readlines()
        except Exception as e:
            logger.exception(f"Failed to read plan file {plan_file_path}")
            raise

        updated = False
        for i, line in enumerate(lines):
            if line.startswith(search_pattern):
                lines[i] = f"{search_pattern}{file_prefix}{formatted_entry_number}\n"
                logger.info(
                    f"Updated {file_type} File in {plan_file_path} to {file_prefix}{formatted_entry_number}"
                )
                updated = True
                break

        if not updated:
            logger.warning(
                f"Search pattern '{search_pattern}' not found in {plan_file_path}. No update performed."
            )

        try:
            with plan_file_path.open('w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated plan file: {plan_file_path}")
        except Exception as e:
            logger.exception(f"Failed to write updates to plan file {plan_file_path}")
            raise

        # Refresh RasPrj dataframes
        try:
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            logger.info("RAS object dataframes have been refreshed.")
        except Exception as e:
            logger.exception("Failed to refresh RasPrj dataframes")
            raise

    @staticmethod
    @log_call
    def check_file_access(file_path: Path, mode: str = 'r') -> None:
        """
        Check if the file can be accessed with the specified mode.

        Parameters:
        file_path (Path): Path to the file
        mode (str): Mode to check ('r' for read, 'w' for write, etc.)

        Raises:
        FileNotFoundError: If the file does not exist
        PermissionError: If the required permissions are not met
        """
        
        path = Path(file_path)
        if not path.exists():
            logger.error(f"File not found: {file_path}")
            raise FileNotFoundError(f"File not found: {file_path}")
        
        if mode in ('r', 'rb'):
            if not os.access(path, os.R_OK):
                logger.error(f"Read permission denied for file: {file_path}")
                raise PermissionError(f"Read permission denied for file: {file_path}")
            else:
                logger.debug(f"Read access granted for file: {file_path}")
        
        if mode in ('w', 'wb', 'a', 'ab'):
            parent_dir = path.parent
            if not os.access(parent_dir, os.W_OK):
                logger.error(f"Write permission denied for directory: {parent_dir}")
                raise PermissionError(f"Write permission denied for directory: {parent_dir}")
            else:
                logger.debug(f"Write access granted for directory: {parent_dir}")


    @staticmethod
    @log_call
    def convert_to_dataframe(data_source: Union[pd.DataFrame, Path], **kwargs) -> pd.DataFrame:
        """
        Converts input to a pandas DataFrame. Supports existing DataFrames or file paths (CSV, Excel, TSV, Parquet).

        Args:
            data_source (Union[pd.DataFrame, Path]): The input to convert to a DataFrame. Can be a file path or an existing DataFrame.
            **kwargs: Additional keyword arguments to pass to pandas read functions.

        Returns:
            pd.DataFrame: The resulting DataFrame.

        Raises:
            NotImplementedError: If the file type is unsupported or input type is invalid.

        Example:
            >>> df = RasUtils.convert_to_dataframe(Path("data.csv"))
            >>> print(type(df))
            <class 'pandas.core.frame.DataFrame'>
        """
        if isinstance(data_source, pd.DataFrame):
            logger.debug("Input is already a DataFrame, returning a copy.")
            return data_source.copy()
        elif isinstance(data_source, Path):
            ext = data_source.suffix.replace('.', '', 1)
            logger.info(f"Converting file with extension '{ext}' to DataFrame.")
            if ext == 'csv':
                return pd.read_csv(data_source, **kwargs)
            elif ext.startswith('x'):
                return pd.read_excel(data_source, **kwargs)
            elif ext == "tsv":
                return pd.read_csv(data_source, sep="\t", **kwargs)
            elif ext in ["parquet", "pq", "parq"]:
                return pd.read_parquet(data_source, **kwargs)
            else:
                logger.error(f"Unsupported file type: {ext}")
                raise NotImplementedError(f"Unsupported file type {ext}. Should be one of csv, tsv, parquet, or xlsx.")
        else:
            logger.error(f"Unsupported input type: {type(data_source)}")
            raise NotImplementedError(f"Unsupported type {type(data_source)}. Only file path / existing DataFrame supported at this time")

    @staticmethod
    @log_call
    def save_to_excel(dataframe: pd.DataFrame, excel_path: Path, **kwargs) -> None:
        """
        Saves a pandas DataFrame to an Excel file with retry functionality.

        Args:
            dataframe (pd.DataFrame): The DataFrame to save.
            excel_path (Path): The path to the Excel file where the DataFrame will be saved.
            **kwargs: Additional keyword arguments passed to `DataFrame.to_excel()`.

        Raises:
            IOError: If the file cannot be saved after multiple attempts.

        Example:
            >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
            >>> RasUtils.save_to_excel(df, Path('output.xlsx'))
        """
        saved = False
        max_attempts = 3
        attempt = 0

        while not saved and attempt < max_attempts:
            try:
                dataframe.to_excel(excel_path, **kwargs)
                logger.info(f'DataFrame successfully saved to {excel_path}')
                saved = True
            except IOError as e:
                attempt += 1
                if attempt < max_attempts:
                    logger.warning(f"Error saving file. Attempt {attempt} of {max_attempts}. Please close the Excel document if it's open.")
                else:
                    logger.error(f"Failed to save {excel_path} after {max_attempts} attempts.")
                    raise IOError(f"Failed to save {excel_path} after {max_attempts} attempts. Last error: {str(e)}")

    @staticmethod
    @log_call
    def calculate_rmse(observed_values: np.ndarray, predicted_values: np.ndarray, normalized: bool = True) -> float:
        """
        Calculate the Root Mean Squared Error (RMSE) between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            normalized (bool, optional): Whether to normalize RMSE to a percentage of observed_values. Defaults to True.

        Returns:
            float: The calculated RMSE value.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_rmse(observed, predicted)
            0.06396394
        """
        rmse = np.sqrt(np.mean((predicted_values - observed_values) ** 2))
        
        if normalized:
            rmse = rmse / np.abs(np.mean(observed_values))
        
        logger.debug(f"Calculated RMSE: {rmse}")
        return rmse

    @staticmethod
    @log_call
    def calculate_percent_bias(observed_values: np.ndarray, predicted_values: np.ndarray, as_percentage: bool = False) -> float:
        """
        Calculate the Percent Bias between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            as_percentage (bool, optional): If True, return bias as a percentage. Defaults to False.

        Returns:
            float: The calculated Percent Bias.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_percent_bias(observed, predicted, as_percentage=True)
            3.33333333
        """
        multiplier = 100 if as_percentage else 1
        
        percent_bias = multiplier * (np.mean(predicted_values) - np.mean(observed_values)) / np.mean(observed_values)
        
        logger.debug(f"Calculated Percent Bias: {percent_bias}")
        return percent_bias

    @staticmethod
    @log_call
    def calculate_error_metrics(observed_values: np.ndarray, predicted_values: np.ndarray) -> Dict[str, float]:
        """
        Compute a trio of error metrics: correlation, RMSE, and Percent Bias.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.

        Returns:
            Dict[str, float]: A dictionary containing correlation ('cor'), RMSE ('rmse'), and Percent Bias ('pb').

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_error_metrics(observed, predicted)
            {'cor': 0.9993, 'rmse': 0.06396, 'pb': 0.03333}
        """
        correlation = np.corrcoef(observed_values, predicted_values)[0, 1]
        rmse = RasUtils.calculate_rmse(observed_values, predicted_values)
        percent_bias = RasUtils.calculate_percent_bias(observed_values, predicted_values)
        
        metrics = {'cor': correlation, 'rmse': rmse, 'pb': percent_bias}
        logger.info(f"Calculated error metrics: {metrics}")
        return metrics

    
    @staticmethod
    @log_call
    def update_file(file_path: Path, update_function: Callable, *args) -> None:
        """
        Generic method to update a file.

        Parameters:
        file_path (Path): Path to the file to be updated
        update_function (Callable): Function to update the file contents
        *args: Additional arguments to pass to the update_function

        Raises:
        Exception: If there's an error updating the file

        Example:
        >>> def update_content(lines, new_value):
        ...     lines[0] = f"New value: {new_value}\\n"
        ...     return lines
        >>> RasUtils.update_file(Path("example.txt"), update_content, "Hello")
        """
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            updated_lines = update_function(lines, *args) if args else update_function(lines)
            
            with open(file_path, 'w') as f:
                f.writelines(updated_lines)
            logger.info(f"Successfully updated file: {file_path}")
        except Exception as e:
            logger.exception(f"Failed to update file {file_path}")
            raise

    @staticmethod
    @log_call
    def get_next_number(existing_numbers: list) -> str:
        """
        Determine the next available number from a list of existing numbers.

        Parameters:
        existing_numbers (list): List of existing numbers as strings

        Returns:
        str: Next available number as a zero-padded string

        Example:
        >>> RasUtils.get_next_number(["01", "02", "04"])
        "05"
        """
        existing_numbers = sorted(int(num) for num in existing_numbers)
        next_number = max(existing_numbers, default=0) + 1
        return f"{next_number:02d}"

    @staticmethod
    @log_call
    def clone_file(template_path: Path, new_path: Path, update_function: Optional[Callable] = None, *args) -> None:
        """
        Generic method to clone a file and optionally update it.

        Parameters:
        template_path (Path): Path to the template file
        new_path (Path): Path where the new file will be created
        update_function (Optional[Callable]): Function to update the cloned file
        *args: Additional arguments to pass to the update_function

        Raises:
        FileNotFoundError: If the template file doesn't exist

        Example:
        >>> def update_content(lines, new_value):
        ...     lines[0] = f"New value: {new_value}\\n"
        ...     return lines
        >>> RasUtils.clone_file(Path("template.txt"), Path("new.txt"), update_content, "Hello")
        """
        if not template_path.exists():
            logger.error(f"Template file '{template_path}' does not exist.")
            raise FileNotFoundError(f"Template file '{template_path}' does not exist.")

        shutil.copy(template_path, new_path)
        logger.info(f"File cloned from {template_path} to {new_path}")

        if update_function:
            RasUtils.update_file(new_path, update_function, *args)
    @staticmethod
    @log_call
    def update_project_file(prj_file: Path, file_type: str, new_num: str, ras_object=None) -> None:
        """
        Update the project file with a new entry.

        Parameters:
        prj_file (Path): Path to the project file
        file_type (str): Type of file being added (e.g., 'Plan', 'Geom')
        new_num (str): Number of the new file entry
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Example:
        >>> RasUtils.update_project_file(Path("project.prj"), "Plan", "02")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            with open(prj_file, 'r') as f:
                lines = f.readlines()
            
            new_line = f"{file_type} File={file_type[0].lower()}{new_num}\n"
            lines.append(new_line)
            
            with open(prj_file, 'w') as f:
                f.writelines(lines)
            logger.info(f"Project file updated with new {file_type} entry: {new_num}")
        except Exception as e:
            logger.exception(f"Failed to update project file {prj_file}")
            raise
        
  
        
        
    # From FunkShuns
        
    @staticmethod
    @log_call
    def decode_byte_strings(dataframe: pd.DataFrame) -> pd.DataFrame:
        """
        Decodes byte strings in a DataFrame to regular string objects.

        This function converts columns with byte-encoded strings (e.g., b'string') into UTF-8 decoded strings.

        Args:
            dataframe (pd.DataFrame): The DataFrame containing byte-encoded string columns.

        Returns:
            pd.DataFrame: The DataFrame with byte strings decoded to regular strings.

        Example:
            >>> df = pd.DataFrame({'A': [b'hello', b'world'], 'B': [1, 2]})
            >>> decoded_df = RasUtils.decode_byte_strings(df)
            >>> print(decoded_df)
                A  B
            0  hello  1
            1  world  2
        """
        str_df = dataframe.select_dtypes(['object'])
        str_df = str_df.stack().str.decode('utf-8').unstack()
        for col in str_df:
            dataframe[col] = str_df[col]
        return dataframe

    @staticmethod
    @log_call
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = RasUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    @log_call
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = RasUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped

    @staticmethod
    @log_call
    def consolidate_dataframe(
        dataframe: pd.DataFrame,
        group_by: Optional[Union[str, List[str]]] = None,
        pivot_columns: Optional[Union[str, List[str]]] = None,
        level: Optional[int] = None,
        n_dimensional: bool = False,
        aggregation_method: Union[str, Callable] = 'list'
    ) -> pd.DataFrame:
        """
        Consolidate rows in a DataFrame by merging duplicate values into lists or using a specified aggregation function.

        Args:
            dataframe (pd.DataFrame): The DataFrame to consolidate.
            group_by (Optional[Union[str, List[str]]]): Columns or indices to group by.
            pivot_columns (Optional[Union[str, List[str]]]): Columns to pivot.
            level (Optional[int]): Level of multi-index to group by.
            n_dimensional (bool): If True, use a pivot table for N-Dimensional consolidation.
            aggregation_method (Union[str, Callable]): Aggregation method, e.g., 'list' to aggregate into lists.

        Returns:
            pd.DataFrame: The consolidated DataFrame.

        Example:
            >>> df = pd.DataFrame({'A': [1, 1, 2], 'B': [4, 5, 6], 'C': [7, 8, 9]})
            >>> result = RasUtils.consolidate_dataframe(df, group_by='A')
            >>> print(result)
            B         C
            A            
            1  [4, 5]  [7, 8]
            2  [6]     [9]
        """
        if aggregation_method == 'list':
            agg_func = lambda x: tuple(x)
        else:
            agg_func = aggregation_method

        if n_dimensional:
            result = dataframe.pivot_table(group_by, pivot_columns, aggfunc=agg_func)
        else:
            result = dataframe.groupby(group_by, level=level).agg(agg_func).applymap(list)

        return result

    @staticmethod
    @log_call
    def find_nearest_value(array: Union[list, np.ndarray], target_value: Union[int, float]) -> Union[int, float]:
        """
        Finds the nearest value in a NumPy array to the specified target value.

        Args:
            array (Union[list, np.ndarray]): The array to search within.
            target_value (Union[int, float]): The value to find the nearest neighbor to.

        Returns:
            Union[int, float]: The nearest value in the array to the specified target value.

        Example:
            >>> arr = np.array([1, 3, 5, 7, 9])
            >>> result = RasUtils.find_nearest_value(arr, 6)
            >>> print(result)
            5
        """
        array = np.asarray(array)
        idx = (np.abs(array - target_value)).argmin()
        return array[idx]
    
    @classmethod
    @log_call
    def horizontal_distance(cls, coord1: np.ndarray, coord2: np.ndarray) -> float:
        """
        Calculate the horizontal distance between two coordinate points.
        
        Args:
            coord1 (np.ndarray): First coordinate point [X, Y].
            coord2 (np.ndarray): Second coordinate point [X, Y].
        
        Returns:
            float: Horizontal distance.
        
        Example:
            >>> distance = RasUtils.horizontal_distance(np.array([0, 0]), np.array([3, 4]))
            >>> print(distance)
            5.0
        """
        return np.linalg.norm(coord2 - coord1)
    
    
    
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\_hec_monolith.py
==================================================
"""
HEC Monolith Downloader

Replicates dssrip2's monolith installation approach.
Downloads HEC Monolith JARs and native libraries from HEC Nexus repository.

Based on: https://github.com/mkoohafkan/dssrip2
"""

import os
import sys
import platform
import hashlib
import zipfile
from pathlib import Path
from typing import List, Dict, Optional
import requests
from tqdm import tqdm


class HecMonolithDownloader:
    """
    Download and manage HEC Monolith libraries.

    Replicates dssrip2's requirements.yaml and download logic.
    """

    # HEC Nexus repository
    NEXUS_BASE = "https://www.hec.usace.army.mil/nexus/repository/maven-public"
    MAVEN_BASE = "https://repo.maven.apache.org/maven2"

    # Requirements (from dssrip2's requirements.yaml)
    COMMON_JARS = [
        {"artifactId": "hec-monolith", "group": "mil.army.usace.hec", "version": "3.3.27"},
        {"artifactId": "hec-monolith-compat", "group": "mil.army.usace.hec", "version": "3.3.27"},
        {"artifactId": "hec-nucleus-data", "group": "mil.army.usace.hec", "version": "2.0.1"},
        {"artifactId": "hec-nucleus-metadata", "group": "mil.army.usace.hec", "version": "2.0.1"},
        {"artifactId": "hecnf", "group": "mil.army.usace.hec.hecnf", "version": "7.2.0"},
        {"artifactId": "flogger", "group": "com.google.flogger", "version": "0.5.1", "source": "maven"},
        {"artifactId": "flogger-system-backend", "group": "com.google.flogger", "version": "0.5.1", "source": "maven"},
    ]

    # Platform-specific native libraries
    NATIVE_LIBS = {
        "Windows": {"artifactId": "javaHeclib", "group": "mil.army.usace.hec",
                    "version": "7-IU-8-win-x86_64", "extension": "zip"},
        "Linux": {"artifactId": "javaHeclib", "group": "mil.army.usace.hec",
                  "version": "7-IU-8-linux-x86_64", "extension": "zip"},
        "Darwin": {"artifactId": "javaHeclib", "group": "mil.army.usace.hec",
                   "version": "7-IU-8-macOS-x86_64", "extension": "zip"},
    }

    def __init__(self, cache_dir: Optional[Path] = None):
        """
        Initialize downloader.

        Args:
            cache_dir: Directory to cache downloads.
                      Defaults to ~/.ras-commander/dss/
        """
        if cache_dir is None:
            cache_dir = Path.home() / ".ras-commander" / "dss"

        self.cache_dir = Path(cache_dir)
        self.jar_dir = self.cache_dir / "jar"
        self.lib_dir = self.cache_dir / "lib"

        # Create directories
        self.jar_dir.mkdir(parents=True, exist_ok=True)
        self.lib_dir.mkdir(parents=True, exist_ok=True)

    def is_installed(self) -> bool:
        """Check if HEC Monolith is already installed."""
        # Check for key JARs
        required_jars = ["hec-monolith", "hec-nucleus-data"]
        for jar_name in required_jars:
            if not list(self.jar_dir.glob(f"{jar_name}*.jar")):
                return False

        # Check for native library
        platform_name = platform.system()
        if platform_name == "Windows":
            lib_file = "javaHeclib.dll"
        elif platform_name == "Linux":
            lib_file = "libjavaHeclib.so"
        elif platform_name == "Darwin":
            lib_file = "libjavaHeclib.dylib"
        else:
            return False

        return (self.lib_dir / lib_file).exists()

    def get_download_url(self, artifact: Dict, source: str = "nexus") -> str:
        """
        Construct Maven download URL.

        Args:
            artifact: Artifact specification dict
            source: "nexus" or "maven"

        Returns:
            Download URL
        """
        group = artifact["group"].replace(".", "/")
        artifact_id = artifact["artifactId"]
        version = artifact["version"]
        extension = artifact.get("extension", "jar")

        filename = f"{artifact_id}-{version}.{extension}"

        if source == "maven":
            base = self.MAVEN_BASE
        else:
            base = self.NEXUS_BASE

        url = f"{base}/{group}/{artifact_id}/{version}/{filename}"
        return url

    def download_file(self, url: str, dest: Path, description: str = "") -> Path:
        """
        Download file with progress bar.

        Args:
            url: URL to download
            dest: Destination file path
            description: Description for progress bar

        Returns:
            Path to downloaded file
        """
        if dest.exists():
            print(f"  Using cached: {dest.name}")
            return dest

        print(f"  Downloading: {description or dest.name}")

        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size = int(response.headers.get('content-length', 0))

        with open(dest, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    pbar.update(len(chunk))

        return dest

    def verify_sha1(self, filepath: Path, expected_sha1: Optional[str] = None) -> bool:
        """
        Verify SHA1 checksum of file.

        Args:
            filepath: File to verify
            expected_sha1: Expected SHA1 hash (if None, tries to download .sha1 file)

        Returns:
            True if checksum matches
        """
        if expected_sha1 is None:
            # Try to download .sha1 file
            return True  # Skip verification if no checksum available

        sha1 = hashlib.sha1()
        with open(filepath, 'rb') as f:
            while chunk := f.read(8192):
                sha1.update(chunk)

        computed = sha1.hexdigest()
        return computed == expected_sha1

    def download_jars(self):
        """Download all required JAR files."""
        print("\nDownloading HEC Monolith JAR files...")

        for artifact in self.COMMON_JARS:
            source = artifact.get("source", "nexus")
            url = self.get_download_url(artifact, source)
            filename = f"{artifact['artifactId']}-{artifact['version']}.jar"
            dest = self.jar_dir / filename

            self.download_file(url, dest, artifact['artifactId'])

    def download_native_library(self):
        """Download platform-specific native library."""
        platform_name = platform.system()

        if platform_name not in self.NATIVE_LIBS:
            raise RuntimeError(f"Unsupported platform: {platform_name}")

        print(f"\nDownloading native library for {platform_name}...")

        artifact = self.NATIVE_LIBS[platform_name]
        url = self.get_download_url(artifact)
        filename = f"{artifact['artifactId']}-{artifact['version']}.zip"
        dest = self.lib_dir / filename

        # Download ZIP
        self.download_file(url, dest, f"Native library ({platform_name})")

        # Extract ZIP
        print(f"  Extracting native library...")
        with zipfile.ZipFile(dest, 'r') as zip_ref:
            zip_ref.extractall(self.lib_dir)

        # Remove ZIP file
        dest.unlink()

        print(f"  [OK] Native library installed")

    def install(self, force: bool = False):
        """
        Download and install HEC Monolith.

        Args:
            force: Force re-download even if already installed
        """
        if self.is_installed() and not force:
            print("HEC Monolith already installed")
            return

        if force:
            print("Forcing re-download of HEC Monolith...")

        print("="*80)
        print("Installing HEC Monolith Libraries")
        print("="*80)
        print(f"Install location: {self.cache_dir}")

        # Download JARs
        self.download_jars()

        # Download native library
        self.download_native_library()

        print("\n" + "="*80)
        print("[SUCCESS] HEC Monolith installation complete!")
        print("="*80)

    def get_classpath(self) -> List[str]:
        """
        Get list of JAR paths for JVM classpath.

        Returns:
            List of absolute JAR file paths
        """
        if not self.is_installed():
            raise RuntimeError("HEC Monolith not installed. Call install() first.")

        jar_files = list(self.jar_dir.glob("*.jar"))
        return [str(jar.resolve()) for jar in sorted(jar_files)]

    def get_library_path(self) -> str:
        """
        Get path to native library directory.

        Returns:
            Absolute path to lib directory containing native libraries
        """
        if not self.is_installed():
            raise RuntimeError("HEC Monolith not installed. Call install() first.")

        return str(self.lib_dir.resolve())

    def get_info(self) -> Dict:
        """
        Get information about installation.

        Returns:
            Dict with installation details
        """
        jars = list(self.jar_dir.glob("*.jar"))
        libs = list(self.lib_dir.glob("*"))

        total_size = sum(f.stat().st_size for f in jars + libs)

        return {
            "installed": self.is_installed(),
            "cache_dir": str(self.cache_dir),
            "num_jars": len(jars),
            "jar_files": [j.name for j in sorted(jars)],
            "lib_files": [l.name for l in sorted(libs)],
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "platform": platform.system(),
        }


if __name__ == "__main__":
    """Test installation"""
    downloader = HecMonolithDownloader()

    print("HEC Monolith Downloader Test")
    print("="*80)

    # Check current status
    info = downloader.get_info()
    print(f"\nInstalled: {info['installed']}")
    print(f"Cache dir: {info['cache_dir']}")

    if not info['installed']:
        # Install
        downloader.install()

        # Show info
        info = downloader.get_info()
        print(f"\nInstallation complete:")
        print(f"  JARs: {info['num_jars']}")
        print(f"  Total size: {info['total_size_mb']} MB")
        print(f"\nClasspath:")
        for jar in downloader.get_classpath():
            print(f"  - {Path(jar).name}")
        print(f"\nLibrary path: {downloader.get_library_path()}")
    else:
        print("\nAlready installed!")
        print(f"  JARs: {info['num_jars']}")
        print(f"  Total size: {info['total_size_mb']} MB")

==================================================

File: c:\GH\ras-commander\ras_commander\__init__.py
==================================================
"""
ras-commander: A Python library for automating HEC-RAS operations
"""

from importlib.metadata import version, PackageNotFoundError
from .LoggingConfig import setup_logging, get_logger
from .Decorators import log_call, standardize_input

try:
    __version__ = version("ras-commander")
except PackageNotFoundError:
    # package is not installed
    __version__ = "0.83.1"

# Set up logging
setup_logging()

# Core functionality
from .RasPrj import RasPrj, init_ras_project, get_ras_exe, ras
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasGeometry import RasGeometry
from .RasGeometryUtils import RasGeometryUtils
from .RasUnsteady import RasUnsteady
from .RasUtils import RasUtils
from .RasExamples import RasExamples
from .RasCmdr import RasCmdr
from .RasControl import RasControl
from .RasMap import RasMap
from .RasGuiAutomation import RasGuiAutomation
from .HdfFluvialPluvial import HdfFluvialPluvial

# HDF handling
from .HdfBase import HdfBase
from .HdfBndry import HdfBndry
from .HdfMesh import HdfMesh
from .HdfPlan import HdfPlan
from .HdfResultsMesh import HdfResultsMesh
from .HdfResultsPlan import HdfResultsPlan
from .HdfResultsXsec import HdfResultsXsec
from .HdfStruc import HdfStruc
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfPump import HdfPump
from .HdfPipe import HdfPipe
from .HdfInfiltration import HdfInfiltration
from .HdfHydraulicTables import HdfHydraulicTables
from .HdfResultsBreach import HdfResultsBreach
from .RasBreach import RasBreach

# Plotting functionality
from .HdfPlot import HdfPlot
from .HdfResultsPlot import HdfResultsPlot

# Define __all__ to specify what should be imported when using "from ras_commander import *"
__all__ = [
    # Core functionality
    'RasPrj', 'init_ras_project', 'get_ras_exe', 'ras',
    'RasPlan', 'RasGeo', 'RasGeometry', 'RasGeometryUtils', 'RasUnsteady', 'RasUtils',
    'RasExamples', 'RasCmdr', 'RasControl', 'RasMap', 'RasGuiAutomation', 'HdfFluvialPluvial',
    
    # HDF handling
    'HdfBase', 'HdfBndry', 'HdfMesh', 'HdfPlan',
    'HdfResultsMesh', 'HdfResultsPlan', 'HdfResultsXsec',
    'HdfStruc', 'HdfUtils', 'HdfXsec', 'HdfPump',
    'HdfPipe', 'HdfInfiltration', 'HdfHydraulicTables', 'HdfResultsBreach', 'RasBreach',
    
    # Plotting functionality
    'HdfPlot', 'HdfResultsPlot',
    
    # Utilities
    'get_logger', 'log_call', 'standardize_input',
]

==================================================

