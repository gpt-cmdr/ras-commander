{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remote Execution with PsExec\n",
        "\n",
        "This notebook demonstrates how to execute HEC-RAS plans on remote Windows machines using PsExec.\n",
        "\n",
        "**Features:**\n",
        "- Distributed execution across multiple remote machines\n",
        "- Automatic project deployment via network shares\n",
        "- Parallel execution with configurable workers\n",
        "- Result collection and consolidation\n",
        "- **Automatic PsExec.exe download** (no manual setup required)\n",
        "\n",
        "**Requirements:**\n",
        "- Remote machine(s) configured per REMOTE_WORKER_SETUP_GUIDE.md (see feature_dev_notes/RasRemote/)\n",
        "- Network share accessible from control machine\n",
        "- HEC-RAS installed on remote machine(s)\n",
        "\n",
        "**Note:** PsExec.exe will be automatically downloaded to `C:\\Users\\{username}\\psexec\\` if not found.\n",
        "\n",
        "**Author:** William (Bill) Katzenmeyer, P.E., C.F.M.\n",
        "\n",
        "**Date:** 2025-11-24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Configure Remote Workers\n\nLoad worker configurations from `RemoteWorkers.json` file using `load_workers_from_json()`.\n\n**First time setup:**\n1. Copy `RemoteWorkers.json.template` to `RemoteWorkers.json`\n2. Edit `RemoteWorkers.json` with your remote machine details\n3. The JSON file is in `.gitignore` for security (credentials won't be committed)\n\n**JSON Format:**\n```json\n{\n  \"workers\": [\n    {\n      \"name\": \"Local Compute\",\n      \"worker_type\": \"local\",\n      \"worker_folder\": \"C:\\\\RasRemote\",\n      \"process_priority\": \"low\",\n      \"queue_priority\": 0,\n      \"cores_total\": 4,\n      \"cores_per_plan\": 2,\n      \"enabled\": true\n    },\n    {\n      \"name\": \"Remote Workstation\",\n      \"worker_type\": \"psexec\",\n      \"hostname\": \"192.168.1.100\",\n      \"share_path\": \"\\\\\\\\192.168.1.100\\\\RasRemote\",\n      \"worker_folder\": \"C:\\\\RasRemote\",\n      \"username\": \"your_username\",\n      \"password\": \"your_password\",\n      \"session_id\": 2,\n      \"process_priority\": \"low\",\n      \"queue_priority\": 1,\n      \"cores_total\": 16,\n      \"cores_per_plan\": 4,\n      \"enabled\": true\n    }\n  ]\n}\n```\n\n**Key Changes (v0.85.0):**\n- `ras_exe_path` is no longer required - automatically obtained from the initialized RAS project\n- Use `load_workers_from_json()` to load all workers from a JSON file\n- `worker_type` field is now required in each worker configuration\n- `worker_folder` replaces `local_path` - specifies where temp folders are created\n\n**Configuration Fields:**\n- `worker_type`: Required - \"psexec\", \"local\", \"ssh\", etc.\n- `worker_folder`: Local path where temporary worker folders are created during execution\n- `share_path`: (psexec only) UNC path to network share that maps to worker_folder\n- `process_priority`: OS process priority for HEC-RAS execution\n  - Valid values: `\"low\"` (default, recommended), `\"below normal\"`, `\"normal\"`\n- `queue_priority`: Execution queue priority (0-9)\n  - Lower values execute first (0 = highest priority)\n- `cores_total`: Total CPU cores on the remote machine (enables parallel execution)\n- `cores_per_plan`: Cores allocated to each HEC-RAS plan\n- **Parallel plans**: cores_total / cores_per_plan (e.g., 16/4 = 4 plans in parallel)\n\n**Session ID:** Use `query user` on remote machine to find (typically 2)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 enabled worker(s) in RemoteWorkers.json:\n",
            "  - Local Compute (localhost)\n",
            "    Type: local\n",
            "    Cores: 4 total, 2 per plan \u2192 2 plans in parallel\n",
            "    Process Priority: low, Queue Priority: 0\n",
            "  - CLB-04 (192.168.3.8)\n",
            "    Type: psexec\n",
            "    Cores: 8 total, 2 per plan \u2192 4 plans in parallel\n",
            "    Process Priority: low, Queue Priority: 0\n",
            "  - CLB-05 (192.168.3.24)\n",
            "    Type: psexec\n",
            "    Cores: 8 total, 2 per plan \u2192 4 plans in parallel\n",
            "    Process Priority: low, Queue Priority: 0\n",
            "  - CLB-03-VM1 (192.168.3.21)\n",
            "    Type: psexec\n",
            "    Cores: 2 total, 2 per plan \u2192 1 plans in parallel\n",
            "    Process Priority: low, Queue Priority: 0\n",
            "\n",
            "NOTE: Workers will be loaded after init_ras_project() to get ras_exe_path automatically\n"
          ]
        }
      ],
      "source": [
        "# Load remote worker configurations using the new load_workers_from_json() function\n",
        "# Note: Workers are loaded AFTER init_ras_project() so ras_exe_path is obtained automatically\n",
        "\n",
        "config_file = Path(\"RemoteWorkers.json\")\n",
        "\n",
        "if not config_file.exists():\n",
        "    print(\"ERROR: RemoteWorkers.json not found!\")\n",
        "    print()\n",
        "    print(\"First time setup:\")\n",
        "    print(\"1. Copy RemoteWorkers.json.template to RemoteWorkers.json\")\n",
        "    print(\"2. Edit RemoteWorkers.json with your remote machine details\")\n",
        "    print(\"3. Run this cell again\")\n",
        "    print()\n",
        "    print(\"The RemoteWorkers.json file should be in the same folder as this notebook.\")\n",
        "    raise FileNotFoundError(\"RemoteWorkers.json not found. See instructions above.\")\n",
        "\n",
        "# Preview the JSON configuration (without loading workers yet)\n",
        "import json\n",
        "with open(config_file, 'r') as f:\n",
        "    worker_configs = json.load(f)\n",
        "\n",
        "# Get enabled workers for display\n",
        "enabled_configs = [w for w in worker_configs[\"workers\"] if w.get(\"enabled\", True)]\n",
        "\n",
        "print(f\"Found {len(enabled_configs)} enabled worker(s) in RemoteWorkers.json:\")\n",
        "for w in enabled_configs:\n",
        "    cores_total = w.get('cores_total', 'Not set')\n",
        "    cores_per_plan = w.get('cores_per_plan', 4)\n",
        "    process_priority = w.get('process_priority', 'low')\n",
        "    queue_priority = w.get('queue_priority', 0)\n",
        "    \n",
        "    if w.get('cores_total'):\n",
        "        max_parallel = w['cores_total'] // cores_per_plan\n",
        "        parallel_info = f\"{max_parallel} plans in parallel\"\n",
        "    else:\n",
        "        parallel_info = \"Sequential execution\"\n",
        "\n",
        "    print(f\"  - {w.get('name', 'unnamed')} ({w.get('hostname', 'localhost')})\")\n",
        "    print(f\"    Type: {w.get('worker_type', 'unknown')}\")\n",
        "    print(f\"    Cores: {cores_total} total, {cores_per_plan} per plan \u2192 {parallel_info}\")\n",
        "    print(f\"    Process Priority: {process_priority}, Queue Priority: {queue_priority}\")\n",
        "\n",
        "print()\n",
        "print(\"NOTE: Workers will be loaded after init_ras_project() to get ras_exe_path automatically\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Example 1: Execute Single Plan (Muncie)\n",
        "\n",
        "Simple example executing one plan from the Muncie example project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n",
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - Project 'Muncie' already exists. Deleting existing folder...\n",
            "2025-12-02 11:20:12 - ras_commander.RasExamples - INFO - Existing folder for project 'Muncie' has been deleted.\n",
            "2025-12-02 11:20:13 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n",
            "2025-12-02 11:20:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project extracted to: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n",
            "Project initialized: Muncie\n",
            "Available plans: [0, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "# Extract Muncie example project\n",
        "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
        "print(f\"Project extracted to: {muncie_path}\")\n",
        "\n",
        "# Initialize project (updates global ras object)\n",
        "init_ras_project(muncie_path, \"6.6\")\n",
        "print(f\"Project initialized: {ras.project_name}\")\n",
        "print(f\"Available plans: {list(ras.plan_df.index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Initializing local worker\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - ERROR - Failed to initialize worker 'Local Compute': LocalWorker.__init__() got an unexpected keyword argument 'process_priority'\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Initializing psexec worker\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO - Initializing PsExec worker for 192.168.3.8\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO - PsExec worker configured:\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Hostname: 192.168.3.8\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Share path: \\\\192.168.3.8\\RasRemote\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Local path: C:\\RasRemote\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   User: .\\bill\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   System account: False\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Session ID: 2\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Process Priority: low\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Queue Priority: 0\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - WARNING - Validation deferred - share access and remote execution will be tested during actual plan execution\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Loaded worker: CLB-04 (psexec)\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Initializing psexec worker\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO - Initializing PsExec worker for 192.168.3.24\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO - PsExec worker configured:\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Hostname: 192.168.3.24\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Share path: \\\\192.168.3.24\\RasRemote\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Local path: C:\\RasRemote\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   User: .\\bill\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   System account: False\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Session ID: 2\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Process Priority: low\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Queue Priority: 0\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - WARNING - Validation deferred - share access and remote execution will be tested during actual plan execution\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Loaded worker: CLB-05 (psexec)\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Initializing psexec worker\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO - Initializing PsExec worker for 192.168.3.21\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO - PsExec worker configured:\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Hostname: 192.168.3.21\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Share path: \\\\192.168.3.21\\RasRemote\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Local path: C:\\RasRemote\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   User: .\\bill\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   System account: False\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Session ID: 2\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Process Priority: low\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - INFO -   Queue Priority: 0\n",
            "2025-12-02 11:20:13 - ras_commander.remote.PsexecWorker - WARNING - Validation deferred - share access and remote execution will be tested during actual plan execution\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Loaded worker: CLB-03-VM1 (psexec)\n",
            "2025-12-02 11:20:13 - ras_commander.remote.RasWorker - INFO - Loaded 3 workers from RemoteWorkers.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 3 worker(s):\n",
            "  - CLB-04 (psexec)\n",
            "    Hostname: 192.168.3.8\n",
            "    RAS Exe: C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\n",
            "    Session ID: 2\n",
            "    Process Priority: low\n",
            "    Queue Priority: 0\n",
            "    Parallel Capacity: 4 plans simultaneously\n",
            "\n",
            "  - CLB-05 (psexec)\n",
            "    Hostname: 192.168.3.24\n",
            "    RAS Exe: C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\n",
            "    Session ID: 2\n",
            "    Process Priority: low\n",
            "    Queue Priority: 0\n",
            "    Parallel Capacity: 4 plans simultaneously\n",
            "\n",
            "  - CLB-03-VM1 (psexec)\n",
            "    Hostname: 192.168.3.21\n",
            "    RAS Exe: C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\n",
            "    Session ID: 2\n",
            "    Process Priority: low\n",
            "    Queue Priority: 0\n",
            "\n",
            "Using worker for examples: CLB-04\n"
          ]
        }
      ],
      "source": [
        "# Load workers from JSON - ras_exe_path is automatically obtained from the ras object\n",
        "# This must be called AFTER init_ras_project() so the RAS executable path is known\n",
        "\n",
        "workers = load_workers_from_json(\"RemoteWorkers.json\")\n",
        "\n",
        "print(f\"Loaded {len(workers)} worker(s):\")\n",
        "for w in workers:\n",
        "    print(f\"  - {w.worker_id} ({w.worker_type})\")\n",
        "    print(f\"    Hostname: {w.hostname}\")\n",
        "    print(f\"    RAS Exe: {w.ras_exe_path}\")\n",
        "    print(f\"    Session ID: {getattr(w, 'session_id', 'N/A')}\")\n",
        "    print(f\"    Process Priority: {getattr(w, 'process_priority', 'N/A')}\")\n",
        "    print(f\"    Queue Priority: {getattr(w, 'queue_priority', 'N/A')}\")\n",
        "    if hasattr(w, 'max_parallel_plans') and w.max_parallel_plans > 1:\n",
        "        print(f\"    Parallel Capacity: {w.max_parallel_plans} plans simultaneously\")\n",
        "    print()\n",
        "\n",
        "# Use first worker for single-plan examples\n",
        "if workers:\n",
        "    worker = workers[0]\n",
        "    print(f\"Using worker for examples: {worker.worker_id}\")\n",
        "else:\n",
        "    raise ValueError(\"No workers loaded from RemoteWorkers.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Execute Plan 01 remotely\n# autoclean=True (default) deletes worker folders after execution\n# Set autoclean=False for debugging to preserve worker folders on the remote machine\n\nprint(\"Executing Plan 01 on remote machine...\")\nprint(\"This will take ~30-60 seconds\")\n\nstart_time = time.time()\n\nresults = compute_parallel_remote(\n    plan_numbers=\"01\",\n    workers=[worker],\n    num_cores=4,\n    autoclean=True  # Default is True - deletes temp folders after execution\n)\n\nelapsed = time.time() - start_time\n\nprint(f\"\\nExecution complete in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\nprint(f\"\\nResults:\")\nfor plan_num, result in results.items():\n    if result.success:\n        print(f\"  Plan {plan_num}: SUCCESS\")\n        print(f\"    HDF Path: {result.hdf_path}\")\n        print(f\"    Execution Time: {result.execution_time:.1f}s\")\n    else:\n        print(f\"  Plan {plan_num}: FAILED - {result.error_message}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Reading computation messages from HDF: Muncie.p01.hdf\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Successfully extracted 2577 characters from HDF\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n",
            "2025-12-02 11:21:08 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MUNCIE PLAN 01 - RESULT VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "HDF File: Muncie.p01.hdf\n",
            "Size: 3.81 MB\n",
            "\n",
            "Compute Status: \u2705 Successful\n",
            "\n",
            "Compute Messages (last 250 chars):\n",
            "Preprocessing Geometry\t<1\n",
            "Unsteady Flow Computations\t       7\n",
            "Post-Processing\t<1\n",
            "Generating Time Series Post Process\t<1\n",
            "Complete Process\t      10\n",
            "\n",
            "Computation Speed\tSimulation/Runtime\n",
            "Unsteady Flow Computations\t11058x\n",
            "Complete Process\t8560x\n",
            "\n",
            "\n",
            "\n",
            "Volume Accounting: Available (1 entries)\n",
            "      Error  Error Percent  Total Boundary Flux of Water In  \\\n",
            "0 -0.277472        0.00075                     36674.503906   \n",
            "\n",
            "   Total Boundary Flux of Water Out Vol Accounting in  Volume Ending  \\\n",
            "0                      33463.007812         Acre Feet    3533.462646   \n",
            "\n",
            "   Volume Starting  \n",
            "0       322.244659  \n",
            "\n",
            "\u2705 Remote execution verified - HDF results successfully collected!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify Muncie results using HDF analysis\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "hdf_path = Path(muncie_path) / \"Muncie.p01.hdf\"\n",
        "\n",
        "if hdf_path.exists():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MUNCIE PLAN 01 - RESULT VERIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    \n",
        "    # Get basic info\n",
        "    size_mb = hdf_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"HDF File: {hdf_path.name}\")\n",
        "    print(f\"Size: {size_mb:.2f} MB\")\n",
        "    print()\n",
        "    \n",
        "    # Get compute messages (static method)\n",
        "    msgs = HdfResultsPlan.get_compute_messages(hdf_path)\n",
        "    \n",
        "    if \"completed successfully\" in msgs.lower() or \"complete process\" in msgs.lower():\n",
        "        print(\"Compute Status: \u2705 Successful\")\n",
        "    else:\n",
        "        print(\"Compute Status: \u26a0\ufe0f Check messages\")\n",
        "    \n",
        "    # Show last part of compute messages\n",
        "    print(\"\\nCompute Messages (last 250 chars):\")\n",
        "    print(msgs[-250:])\n",
        "    print()\n",
        "    \n",
        "    # Get steady flow results\n",
        "    is_steady = HdfResultsPlan.is_steady_plan(hdf_path)\n",
        "    if is_steady:\n",
        "        profiles = HdfResultsPlan.get_steady_profile_names(hdf_path)\n",
        "        print(f\"Steady Flow Profiles: {profiles}\")\n",
        "        \n",
        "        # Get WSE for first profile\n",
        "        if profiles:\n",
        "            wse_df = HdfResultsPlan.get_steady_wse(hdf_path, profiles[0])\n",
        "            if wse_df is not None and len(wse_df) > 0:\n",
        "                print(f\"Cross Sections: {len(wse_df)}\")\n",
        "                print(f\"WSE Range: {wse_df['W.S. Elev'].min():.2f} to {wse_df['W.S. Elev'].max():.2f} ft\")\n",
        "    \n",
        "    # Get volume accounting\n",
        "    try:\n",
        "        vol = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "        if vol is not None:\n",
        "            print(f\"\\nVolume Accounting: Available ({len(vol)} entries)\")\n",
        "            print(vol)\n",
        "    except:\n",
        "        print(\"\\nVolume Accounting: Not available\")\n",
        "    \n",
        "    print()\n",
        "    print(\"\u2705 Remote execution verified - HDF results successfully collected!\")\n",
        "    print()\n",
        "else:\n",
        "    print(\"\u274c HDF file not found - execution may have failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 11:21:08 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 11:21:08 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n",
            "2025-12-02 11:21:08 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n",
            "2025-12-02 11:21:08 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n",
            "2025-12-02 11:21:09 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-12-02 11:21:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project extracted to: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "Project initialized: BaldEagleDamBrk\n",
            "Available plans: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ],
      "source": [
        "# Extract BaldEagleCrkMulti2D project\n",
        "baldeagle_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\")\n",
        "print(f\"Project extracted to: {baldeagle_path}\")\n",
        "\n",
        "# Initialize project (updates global ras object)\n",
        "init_ras_project(baldeagle_path, \"6.6\")\n",
        "print(f\"Project initialized: {ras.project_name}\")\n",
        "print(f\"Available plans: {list(ras.plan_df.index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Execute expanded set of plans to test queue priority and parallel execution\n# Plans 03, 04, 06, 13, 15, 17, 18, 19 - 8 plans total\n# This tests the queue-aware scheduling with multiple sub-workers\n\ntest_plans = [\"03\", \"04\", \"06\", \"13\", \"15\", \"17\", \"18\", \"19\"]\nprint(f\"Executing {len(test_plans)} plans on remote machine: {test_plans}\")\nprint(\"These are 2D unsteady models - may take 10-20 minutes total\")\nprint(\"Watch the logs to observe queue priority and wave scheduling\")\n\nstart_time = time.time()\n\nresults = compute_parallel_remote(\n    plan_numbers=test_plans,\n    workers=[worker],\n    num_cores=4,\n    autoclean=True  # Default is True - deletes temp folders after execution\n)\n\nelapsed = time.time() - start_time\n\nprint(f\"\\nExecution complete in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\nprint(f\"\\nResults:\")\nsuccess_count = 0\nfor plan_num, result in results.items():\n    if result.success:\n        print(f\"  Plan {plan_num}: SUCCESS ({result.execution_time:.1f}s)\")\n        success_count += 1\n    else:\n        print(f\"  Plan {plan_num}: FAILED - {result.error_message}\")\n\nprint(f\"\\nSummary: {success_count}/{len(results)} plans succeeded\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Reading computation messages from HDF: BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Successfully extracted 1786 characters from HDF\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p19.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p19.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Reading computation messages from HDF: BaldEagleDamBrk.p19.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Successfully extracted 1789 characters from HDF\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p19.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p19.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Using existing Path object HDF file: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p19.hdf\n",
            "2025-12-02 11:33:10 - ras_commander.HdfResultsPlan - INFO - Final validated file path: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p19.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BALDEAGLE PLANS 06 & 19 - RESULT VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "Plan 06:\n",
            "  HDF Size: 577.57 MB\n",
            "  Status: \u2705 Computation successful\n",
            "  Unsteady Summary: Available\n",
            "  Volume Accounting: 1 entries\n",
            "\n",
            "Plan 19:\n",
            "  HDF Size: 11.07 MB\n",
            "  Status: \u2705 Computation successful\n",
            "  Unsteady Summary: Available\n",
            "  Volume Accounting: 1 entries\n",
            "\n",
            "\u2705 Remote execution verified - 2D model results successfully collected!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify BaldEagle results using HDF analysis\n",
        "from ras_commander import HdfResultsPlan, HdfResultsMesh\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"BALDEAGLE PLANS 06 & 19 - RESULT VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "for plan_num in [\"06\", \"19\"]:\n",
        "    hdf_path = Path(baldeagle_path) / f\"BaldEagleDamBrk.p{plan_num}.hdf\"\n",
        "    \n",
        "    if hdf_path.exists():\n",
        "        print(f\"Plan {plan_num}:\")\n",
        "        size_mb = hdf_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  HDF Size: {size_mb:.2f} MB\")\n",
        "        \n",
        "        # Get compute messages (static method)\n",
        "        msgs = HdfResultsPlan.get_compute_messages(hdf_path)\n",
        "        if \"completed successfully\" in msgs.lower() or \"complete process\" in msgs.lower():\n",
        "            print(f\"  Status: \u2705 Computation successful\")\n",
        "        else:\n",
        "            print(f\"  Status: \u26a0\ufe0f Check compute messages\")\n",
        "        \n",
        "        # Get unsteady summary\n",
        "        try:\n",
        "            summary = HdfResultsPlan.get_unsteady_summary(hdf_path)\n",
        "            if summary is not None:\n",
        "                print(f\"  Unsteady Summary: Available\")\n",
        "        except:\n",
        "            print(f\"  Unsteady Summary: Not available\")\n",
        "        \n",
        "        # Get volume accounting\n",
        "        try:\n",
        "            vol = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "            if vol is not None and len(vol) > 0:\n",
        "                print(f\"  Volume Accounting: {len(vol)} entries\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Get mesh timesteps for 2D\n",
        "        try:\n",
        "            mesh_times = HdfResultsMesh.get_output_times(hdf_path)\n",
        "            if mesh_times is not None:\n",
        "                print(f\"  Output Timesteps: {len(mesh_times)}\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print()\n",
        "    else:\n",
        "        print(f\"Plan {plan_num}: \u274c HDF file not found\")\n",
        "        print()\n",
        "\n",
        "print(\"\u2705 Remote execution verified - 2D model results successfully collected!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Example 3: Multiple Remote Workers (Parallel)\n",
        "\n",
        "Execute plans across multiple remote machines simultaneously.\n",
        "\n",
        "**Note:** This example uses ALL enabled workers from `RemoteWorkers.json`.\n",
        "To use multiple machines, add additional workers to the JSON file and set `enabled: true`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Execute multiple plans across all loaded workers\n# Plans will be distributed based on queue_priority (0 first, then 1, etc.)\n\n# Workers were already loaded in cell-7 using load_workers_from_json()\nif len(workers) > 1:\n    print(f\"Executing plans across {len(workers)} worker(s)...\")\n    for w in workers:\n        print(f\"  - {w.worker_id} ({w.hostname}) - Queue {getattr(w, 'queue_priority', 0)}\")\n    \n    start_time = time.time()\n    \n    results = compute_parallel_remote(\n        plan_numbers=[\"06\", \"19\"],\n        workers=workers,\n        num_cores=4,\n        clear_geompre=False,\n        autoclean=True  # Default is True - deletes temp folders after execution\n    )\n    \n    elapsed = time.time() - start_time\n    \n    print(f\"\\nTotal execution time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n    print(f\"\\nResults:\")\n    for plan_num, result in results.items():\n        status = \"SUCCESS\" if result.success else f\"FAILED: {result.error_message}\"\n        print(f\"  Plan {plan_num}: {status}\")\n    \n    # Calculate speedup\n    successful = sum(1 for r in results.values() if r.success)\n    print(f\"\\nSummary: {successful}/{len(results)} plans succeeded\")\nelse:\n    print(f\"Only 1 worker loaded - skipping multi-worker example\")\n    print(f\"To test parallel execution:\")\n    print(f\"  1. Add more workers to RemoteWorkers.json\")\n    print(f\"  2. Set enabled=true for each\")\n    print(f\"  3. Re-run the notebook from the beginning\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Alternative: Manually initialize a worker without JSON file\n# This demonstrates the init_ras_worker() function directly\n# Note: ras_exe_path is automatically obtained from the ras object\n\nmanual_worker = init_ras_worker(\n    \"psexec\",\n    hostname=\"192.168.3.8\",  # Replace with your hostname\n    share_path=r\"\\\\192.168.3.8\\RasRemote\",  # Replace with your share path\n    worker_folder=r\"C:\\RasRemote\",  # Local path on remote machine corresponding to share_path\n    credentials={\n        \"username\": \".\\\\bill\",  # Replace with your username\n        \"password\": \"YourPassword\"  # Replace with your password\n    },\n    # ras_exe_path is NOT required - obtained from ras object automatically\n    session_id=2,\n    process_priority=\"low\",\n    queue_priority=0,\n    cores_total=8,\n    cores_per_plan=2\n)\n\nprint(f\"Manual worker initialized:\")\nprint(f\"  Worker ID: {manual_worker.worker_id}\")\nprint(f\"  Hostname: {manual_worker.hostname}\")\nprint(f\"  Worker Folder: {manual_worker.worker_folder}\")\nprint(f\"  RAS Exe: {manual_worker.ras_exe_path}\")  # Automatically set from ras object\nprint(f\"  Parallel Capacity: {manual_worker.max_parallel_plans} plans\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Verify Results\n",
        "\n",
        "Check that HDF files were created and results collected properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan HDF files (.pXX.hdf) in results folder: 8\n",
            "  BaldEagleDamBrk.p03.hdf: 59.07 MB\n",
            "  BaldEagleDamBrk.p04.hdf: 81.73 MB\n",
            "  BaldEagleDamBrk.p06.hdf: 577.59 MB\n",
            "  BaldEagleDamBrk.p13.hdf: 17.00 MB\n",
            "  BaldEagleDamBrk.p15.hdf: 87.78 MB\n",
            "  BaldEagleDamBrk.p17.hdf: 47.00 MB\n",
            "  BaldEagleDamBrk.p18.hdf: 23.26 MB\n",
            "  BaldEagleDamBrk.p19.hdf: 11.07 MB\n"
          ]
        }
      ],
      "source": [
        "# List only .pXX.hdf files in results folder (plan result HDFs)\n",
        "import re\n",
        "\n",
        "results_path = Path(baldeagle_path).parent / \"multi_worker_results\" / \"BaldEagleDamBrk\"\n",
        "\n",
        "pattern = re.compile(r\"\\.p\\d{2}\\.hdf$\", re.IGNORECASE)\n",
        "\n",
        "if results_path.exists():\n",
        "    hdf_files = [hdf for hdf in results_path.glob(\"*.hdf\") if pattern.search(hdf.name)]\n",
        "    print(f\"Plan HDF files (.pXX.hdf) in results folder: {len(hdf_files)}\")\n",
        "    for hdf in hdf_files:\n",
        "        size_mb = hdf.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  {hdf.name}: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"Results folder not found: {results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Configuration\n",
        "\n",
        "### Session ID Determination\n",
        "\n",
        "Find the active session ID on a remote machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timeout querying sessions\n"
          ]
        }
      ],
      "source": [
        "# Query active sessions on remote machine\n",
        "# Uses the first loaded worker to get psexec_path and credentials\n",
        "import subprocess\n",
        "\n",
        "if workers:\n",
        "    w = workers[0]\n",
        "    psexec = getattr(w, 'psexec_path', None)\n",
        "    \n",
        "    if psexec and hasattr(w, 'credentials') and w.credentials:\n",
        "        cmd = [\n",
        "            psexec,\n",
        "            f\"\\\\\\\\{w.hostname}\",\n",
        "            \"-u\", w.credentials.get(\"username\", \"\"),\n",
        "            \"-p\", w.credentials.get(\"password\", \"\"),\n",
        "            \"-accepteula\",\n",
        "            \"cmd\", \"/c\", \"query\", \"user\"\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
        "            print(\"Active sessions on remote machine:\")\n",
        "            print(result.stdout)\n",
        "            print(\"\\nLook for the ID column - typically 2 for workstations\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"Timeout querying sessions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not query sessions: {e}\")\n",
        "    else:\n",
        "        print(\"Worker doesn't have psexec_path or credentials set\")\n",
        "        print(\"Try session_id=2 (most common for single-user workstations)\")\n",
        "else:\n",
        "    print(\"No workers loaded - run previous cells first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Priority Levels\n",
        "\n",
        "Control OS process priority for remote HEC-RAS execution:\n",
        "\n",
        "- `\"low\"` - Low priority (recommended for background work, minimal impact on remote user)\n",
        "- `\"below normal\"` - Below normal priority\n",
        "- `\"normal\"` - Normal priority (default Windows priority)\n",
        "\n",
        "**Note:** Higher priorities (above normal, high, realtime) are NOT supported to avoid impacting remote user operations.\n",
        "\n",
        "### Queue Priority\n",
        "\n",
        "Control execution order across workers:\n",
        "\n",
        "- `queue_priority` is an integer from 0-9 (lower = higher priority)\n",
        "- Workers at queue level 0 are filled before queue level 1, etc.\n",
        "- Within each queue level, wave scheduling applies (one plan per machine first, then additional)\n",
        "- Use for tiered bursting: local workers (queue 0) execute first, then remote (queue 1), then cloud (queue 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worker: CLB-04\n",
            "  Process Priority: low\n",
            "  Queue Priority: 0\n",
            "  RAS Exe Path: C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\n",
            "\n",
            "To change settings, edit RemoteWorkers.json and reload workers:\n",
            "  workers = load_workers_from_json('RemoteWorkers.json')\n"
          ]
        }
      ],
      "source": [
        "# Example: Viewing worker configuration with low process priority\n",
        "# Workers loaded from JSON already have these settings applied\n",
        "\n",
        "if workers:\n",
        "    w = workers[0]\n",
        "    print(f\"Worker: {w.worker_id}\")\n",
        "    print(f\"  Process Priority: {getattr(w, 'process_priority', 'N/A')}\")\n",
        "    print(f\"  Queue Priority: {getattr(w, 'queue_priority', 'N/A')}\")\n",
        "    print(f\"  RAS Exe Path: {w.ras_exe_path}\")\n",
        "    print()\n",
        "    print(\"To change settings, edit RemoteWorkers.json and reload workers:\")\n",
        "    print(\"  workers = load_workers_from_json('RemoteWorkers.json')\")\n",
        "else:\n",
        "    print(\"No workers loaded - run previous cells first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Troubleshooting (Optional)\n",
        "\n",
        "### Test Remote Connections using psexec\n",
        "\n",
        "Change the cell below to a code cell, enter your username and password for use in testing. \n",
        "\n",
        "Don't leave your passwords here, it can get synced back to git.  Use RemoteWorkers.json, it is already in the .gitignore for this repo.  \n",
        "Use the code cell below for testing only, not as a design pattern for production usage: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REMOTE_CONFIG built from worker: CLB-04\n",
            "  Hostname: 192.168.3.8\n",
            "  Share Path: \\\\192.168.3.8\\RasRemote\n",
            "  Session ID: 2\n"
          ]
        }
      ],
      "source": [
        "# Build REMOTE_CONFIG from the first psexec worker in workers list\n",
        "# This uses the credentials already loaded from RemoteWorkers.json\n",
        "\n",
        "REMOTE_CONFIG = None\n",
        "\n",
        "if workers:\n",
        "    # Find first psexec worker\n",
        "    for w in workers:\n",
        "        if w.worker_type == \"psexec\":\n",
        "            REMOTE_CONFIG = {\n",
        "                \"hostname\": w.hostname,\n",
        "                \"share_path\": w.share_path,\n",
        "                \"username\": w.credentials.get(\"username\", \"\") if hasattr(w, 'credentials') and w.credentials else \"\",\n",
        "                \"password\": w.credentials.get(\"password\", \"\") if hasattr(w, 'credentials') and w.credentials else \"\",\n",
        "                \"ras_exe_path\": w.ras_exe_path,\n",
        "                \"session_id\": getattr(w, 'session_id', 2)\n",
        "            }\n",
        "            print(f\"REMOTE_CONFIG built from worker: {w.worker_id}\")\n",
        "            print(f\"  Hostname: {REMOTE_CONFIG['hostname']}\")\n",
        "            print(f\"  Share Path: {REMOTE_CONFIG['share_path']}\")\n",
        "            print(f\"  Session ID: {REMOTE_CONFIG['session_id']}\")\n",
        "            break\n",
        "\n",
        "if REMOTE_CONFIG is None:\n",
        "    print(\"WARNING: No psexec workers found in workers list.\")\n",
        "    print(\"Define REMOTE_CONFIG manually or add psexec workers to RemoteWorkers.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 11:38:03 - ras_commander.remote.RasWorker - INFO - Initializing psexec worker\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO - Initializing PsExec worker for 192.168.3.8\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO - PsExec worker configured:\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   Hostname: 192.168.3.8\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   Share path: \\\\192.168.3.8\\RasRemote\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   Local path: C:\\RasRemote\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   User: .\\bill\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   System account: False\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   Session ID: 2\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   Process Priority: low\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - INFO -   Queue Priority: 0\n",
            "2025-12-02 11:38:03 - ras_commander.remote.PsexecWorker - WARNING - Validation deferred - share access and remote execution will be tested during actual plan execution\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FAIL] Connection timeout - check firewall and services\n"
          ]
        }
      ],
      "source": [
        "# Test basic PsExec connectivity\n",
        "import subprocess\n",
        "\n",
        "if REMOTE_CONFIG is None:\n",
        "    print(\"REMOTE_CONFIG not set - run the cell above first\")\n",
        "else:\n",
        "    # Get psexec path from the initialized worker\n",
        "    try:\n",
        "        temp_worker = init_ras_worker(\n",
        "            \"psexec\",\n",
        "            hostname=REMOTE_CONFIG[\"hostname\"],\n",
        "            share_path=REMOTE_CONFIG[\"share_path\"],\n",
        "            credentials={\n",
        "                \"username\": REMOTE_CONFIG[\"username\"],\n",
        "                \"password\": REMOTE_CONFIG[\"password\"]\n",
        "            },\n",
        "            session_id=REMOTE_CONFIG[\"session_id\"]\n",
        "        )\n",
        "        psexec_path = temp_worker.psexec_path\n",
        "\n",
        "        test_cmd = [\n",
        "            psexec_path,\n",
        "            f\"\\\\\\\\{REMOTE_CONFIG['hostname']}\",\n",
        "            \"-u\", REMOTE_CONFIG[\"username\"],\n",
        "            \"-p\", REMOTE_CONFIG[\"password\"],\n",
        "            \"-i\", str(REMOTE_CONFIG[\"session_id\"]),\n",
        "            \"-accepteula\",\n",
        "            \"cmd\", \"/c\", \"echo\", \"SUCCESS\"\n",
        "        ]\n",
        "\n",
        "        result = subprocess.run(test_cmd, capture_output=True, text=True, timeout=30)\n",
        "        if \"SUCCESS\" in result.stdout:\n",
        "            print(\"[OK] PsExec connection successful!\")\n",
        "        else:\n",
        "            print(\"[WARNING] Unexpected output:\")\n",
        "            print(result.stdout)\n",
        "            print(result.stderr)\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"[FAIL] Connection timeout - check firewall and services\")\n",
        "    except Exception as e:\n",
        "        print(f\"[FAIL] Connection error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Share Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Share accessible: \\\\192.168.3.8\\RasRemote\\\n",
            "     Contents: 44 items\n"
          ]
        }
      ],
      "source": [
        "# Test if share is accessible\n",
        "from pathlib import WindowsPath\n",
        "\n",
        "if REMOTE_CONFIG is None:\n",
        "    print(\"REMOTE_CONFIG not set - run the 'Build REMOTE_CONFIG' cell first\")\n",
        "else:\n",
        "    share_path = Path(REMOTE_CONFIG[\"share_path\"])\n",
        "\n",
        "    try:\n",
        "        # This may fail without authenticated session - that's OK\n",
        "        if share_path.exists():\n",
        "            print(f\"[OK] Share accessible: {share_path}\")\n",
        "            files = list(share_path.iterdir())[:5]\n",
        "            print(f\"     Contents: {len(list(share_path.iterdir()))} items\")\n",
        "        else:\n",
        "            print(f\"[INFO] Share not accessible via Path.exists() (authentication may be required)\")\n",
        "            print(f\"      This is normal - share will be accessed during execution with credentials\")\n",
        "    except Exception as e:\n",
        "        print(f\"[INFO] Cannot test share access: {e}\")\n",
        "        print(f\"      This is normal - share will be accessed during execution with credentials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Notes and Best Practices\n",
        "\n",
        "### Remote Worker Configuration:\n",
        "- Credentials stored in `RemoteWorkers.json` (not committed to git)\n",
        "- See **REMOTE_WORKERS_README.md** for JSON format and setup\n",
        "- Template provided: `RemoteWorkers.json.template`\n",
        "\n",
        "### Remote Worker Requirements:\n",
        "1. \u2705 Network share created and accessible\n",
        "2. \u2705 User in local Administrators group\n",
        "3. \u2705 Group Policy: User added to network access, local logon, batch job policies\n",
        "4. \u2705 Registry: LocalAccountTokenFilterPolicy = 1\n",
        "5. \u2705 Remote Registry service running\n",
        "6. \u2705 Windows Firewall configured\n",
        "7. \u2705 Machine rebooted after changes\n",
        "\n",
        "### Session ID:\n",
        "- Session ID 2 is typical for single-user workstations\n",
        "- Use `query user` on remote machine to verify\n",
        "- User must be logged in for session to be active\n",
        "- Session ID can change if user logs off/on\n",
        "\n",
        "### HEC-RAS Considerations:\n",
        "- HEC-RAS is a GUI application\n",
        "- MUST use session-based execution (`system_account=False`)\n",
        "- NEVER use SYSTEM account (`system_account=True`) for HEC-RAS\n",
        "- HEC-RAS window will start on the desktop of the remote desktop\n",
        "- Ensure HEC-RAS version matches on all workers, and TOS has been accepted.\n",
        "\n",
        "### Performance:\n",
        "- Network share speed affects file transfer\n",
        "- Use Gigabit Ethernet for best performance\n",
        "- 2-4 workers per machine optimal (depends on cores/RAM)\n",
        "- Plans execute sequentially on each worker\n",
        "- Multiple workers enable true parallel execution\n",
        "\n",
        "### Security:\n",
        "- Credentials in `RemoteWorkers.json` (in .gitignore)\n",
        "- Never commit credentials to git\n",
        "- See setup instructions for required group policy and registry changes\n",
        "\n",
        "### Debugging:\n",
        "- Check logs in ras_commander.log\n",
        "- Inspect compute messages: `project.p##.computeMsgs.txt`\n",
        "- Verify temp folders on remote share\n",
        "- Test PsExec manually with provided batch files\n",
        "\n",
        "---\n",
        "\n",
        "**For complete setup instructions, see:**\n",
        "- `feature_dev_notes/RasRemote/REMOTE_WORKER_SETUP_GUIDE.md` - Remote machine setup\n",
        "- `REMOTE_WORKERS_README.md` - JSON credential file format"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## 10. Cleanup Remote Worker Folders\n\nThe `autoclean=True` parameter (default) automatically deletes worker folders after execution.\nHowever, if you used `autoclean=False` for debugging or if executions were interrupted,\nyou may have leftover folders on the remote shares.\n\n**All files in the RasRemote share are considered temporary** and can be safely deleted\nto preserve disk space on the remote machines.\n\nRun the cells below to manually clean up any remaining worker folders.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# List and optionally clean up worker folders on remote machines\n# This cleans ALL files in the RasRemote share - all contents are temporary\n\ndef cleanup_remote_shares(workers, dry_run=True):\n    \"\"\"\n    Clean up worker folders from remote shares.\n    \n    Args:\n        workers: List of worker objects with share_path attribute\n        dry_run: If True, only list folders without deleting (default True for safety)\n    \n    Returns:\n        dict: {hostname: {\"folders\": count, \"size_mb\": total_size}}\n    \"\"\"\n    import shutil\n    \n    results = {}\n    seen_shares = set()\n    \n    for w in workers:\n        if not hasattr(w, 'share_path') or not w.share_path:\n            continue\n            \n        share_path = Path(w.share_path)\n        share_key = str(share_path)\n        \n        # Skip if we've already processed this share\n        if share_key in seen_shares:\n            continue\n        seen_shares.add(share_key)\n        \n        hostname = getattr(w, 'hostname', 'unknown')\n        \n        try:\n            if not share_path.exists():\n                print(f\"Share not accessible: {share_path}\")\n                continue\n                \n            folders = [f for f in share_path.iterdir() if f.is_dir()]\n            total_size = 0\n            \n            print(f\"\\n{'='*60}\")\n            print(f\"Share: {share_path} ({hostname})\")\n            print(f\"{'='*60}\")\n            \n            if not folders:\n                print(\"  No folders found - share is clean\")\n                results[hostname] = {\"folders\": 0, \"size_mb\": 0}\n                continue\n                \n            for folder in folders:\n                # Calculate folder size\n                folder_size = sum(f.stat().st_size for f in folder.rglob('*') if f.is_file())\n                folder_size_mb = folder_size / (1024 * 1024)\n                total_size += folder_size_mb\n                \n                if dry_run:\n                    print(f\"  [WOULD DELETE] {folder.name} ({folder_size_mb:.1f} MB)\")\n                else:\n                    print(f\"  [DELETING] {folder.name} ({folder_size_mb:.1f} MB)\")\n                    shutil.rmtree(folder, ignore_errors=True)\n            \n            results[hostname] = {\"folders\": len(folders), \"size_mb\": total_size}\n            \n            if dry_run:\n                print(f\"\\n  Summary: {len(folders)} folders, {total_size:.1f} MB total\")\n                print(f\"  Set dry_run=False to delete these folders\")\n            else:\n                print(f\"\\n  Deleted: {len(folders)} folders, {total_size:.1f} MB freed\")\n                \n        except Exception as e:\n            print(f\"Error accessing {share_path}: {e}\")\n            \n    return results\n\n# DRY RUN - List folders without deleting\nprint(\"=\" * 70)\nprint(\"CLEANUP PREVIEW (dry_run=True)\")\nprint(\"=\" * 70)\ncleanup_results = cleanup_remote_shares(workers, dry_run=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ACTUALLY DELETE - Uncomment and run to delete all worker folders\n# WARNING: This permanently deletes all folders in the RasRemote shares!\n\n# print(\"=\" * 70)\n# print(\"CLEANUP EXECUTION (dry_run=False)\")\n# print(\"=\" * 70)\n# cleanup_results = cleanup_remote_shares(workers, dry_run=False)\n# print(\"\\nCleanup complete!\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}