{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AEP Storm Analysis with RAS-Commander\n",
        "\n",
        "This notebook automates the end-to-end process of analyzing multiple storm events with different Annual Exceedance Probabilities (AEP) in HEC-RAS. It covers:\n",
        "\n",
        "1. Generating hyetographs from NOAA Atlas 14 precipitation data\n",
        "2. Creating HEC-RAS plan files for each AEP event\n",
        "3. Creating unsteady flow files with the generated hyetographs\n",
        "4. Executing multiple plans in parallel\n",
        "5. Analyzing and visualizing the results\n",
        "\n",
        "This automation is particularly useful for analyzing how a drainage system performs under different storm frequencies, from common events (e.g., 2-year) to rare events (e.g., 100-year)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Import Libraries\n",
        "\n",
        "First, we'll import all the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "\n",
        "# Install ras-commander if not already installed\n",
        "# Uncomment this line if you need to install the package\n",
        "# !pip install ras-commander\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RasExamples.extract_project([\"Davis\"])\n",
        "# This loads the project in fresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Hyetograph Generation Functions\n",
        "\n",
        "These functions handle reading precipitation frequency data from NOAA Atlas 14 and generating balanced storm hyetographs using the Alternating Block Method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_duration(duration_str):\n",
        "    \"\"\"\n",
        "    Parses a duration string and converts it to hours.\n",
        "    Examples: \"5-min:\" -> 0.0833 hours, \"2-hr:\" -> 2 hours, \"2-day:\" -> 48 hours\n",
        "    \"\"\"\n",
        "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
        "    if not match:\n",
        "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
        "    value, unit = match.groups()\n",
        "    value = int(value)\n",
        "    unit = unit.lower()\n",
        "    if unit in ['min', 'minute', 'minutes']:\n",
        "        hours = value / 60.0\n",
        "    elif unit in ['hr', 'hour', 'hours']:\n",
        "        hours = value\n",
        "    elif unit in ['day', 'days']:\n",
        "        hours = value * 24\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
        "    return hours\n",
        "\n",
        "def read_precipitation_data(csv_file):\n",
        "    \"\"\"\n",
        "    Reads the precipitation frequency CSV and returns a DataFrame\n",
        "    with durations in hours as the index and ARIs as columns.\n",
        "    \"\"\"\n",
        "    with open(csv_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    header_line_idx = None\n",
        "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
        "\n",
        "    # Locate the header line\n",
        "    for idx, line in enumerate(lines):\n",
        "        if header_pattern.match(line.strip().lower()):\n",
        "            header_line_idx = idx\n",
        "            break\n",
        "\n",
        "    if header_line_idx is None:\n",
        "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
        "\n",
        "    # Extract the ARI headers from the header line\n",
        "    header_line = lines[header_line_idx].strip()\n",
        "    headers = [item.strip() for item in header_line.split(',')]\n",
        "    \n",
        "    if len(headers) < 2:\n",
        "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
        "\n",
        "    aris = headers[1:]  # Exclude the first column which is the duration\n",
        "\n",
        "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
        "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
        "\n",
        "    # Initialize lists to store durations and corresponding depths\n",
        "    durations = []\n",
        "    depths = {ari: [] for ari in aris}\n",
        "\n",
        "    # Iterate over the lines following the header to extract data\n",
        "    for line in lines[header_line_idx + 1:]:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # Skip empty lines\n",
        "        if not duration_pattern.match(line):\n",
        "            break  # Stop if the line does not match the duration pattern\n",
        "        parts = [part.strip() for part in line.split(',')]\n",
        "        if len(parts) != len(headers):\n",
        "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
        "        duration_str = parts[0]\n",
        "        try:\n",
        "            duration_hours = parse_duration(duration_str)\n",
        "        except ValueError as ve:\n",
        "            print(f\"Skipping line due to error: {ve}\")\n",
        "            continue  # Skip lines with invalid duration formats\n",
        "        durations.append(duration_hours)\n",
        "        for ari, depth_str in zip(aris, parts[1:]):\n",
        "            try:\n",
        "                depth = float(depth_str)\n",
        "            except ValueError:\n",
        "                depth = np.nan  # Assign NaN for invalid depth values\n",
        "            depths[ari].append(depth)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(depths, index=durations)\n",
        "    df.index.name = 'Duration_hours'\n",
        "\n",
        "    # Drop any rows with NaN values\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "def interpolate_depths(df, total_duration):\n",
        "    \"\"\"\n",
        "    Interpolates precipitation depths for each ARI on a log-log scale\n",
        "    for each hour up to the total storm duration.\n",
        "    \"\"\"\n",
        "    T = total_duration\n",
        "    t_hours = np.arange(1, T+1)\n",
        "    D = {}\n",
        "    for ari in df.columns:\n",
        "        durations = df.index.values\n",
        "        depths = df[ari].values\n",
        "        # Ensure all depths are positive\n",
        "        if np.any(depths <= 0):\n",
        "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
        "        # Log-log interpolation\n",
        "        log_durations = np.log(durations)\n",
        "        log_depths = np.log(depths)\n",
        "        log_t = np.log(t_hours)\n",
        "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
        "        D_t = np.exp(log_D_t)\n",
        "        D[ari] = D_t\n",
        "    return D\n",
        "\n",
        "def compute_incremental_depths(D, total_duration):\n",
        "    \"\"\"\n",
        "    Computes incremental precipitation depths for each hour.\n",
        "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
        "    \"\"\"\n",
        "    incremental_depths = {}\n",
        "    for ari, D_t in D.items():\n",
        "        I_t = np.empty(total_duration)\n",
        "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
        "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
        "        incremental_depths[ari] = I_t\n",
        "    return incremental_depths\n",
        "\n",
        "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
        "    \"\"\"\n",
        "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
        "    \"\"\"\n",
        "    hyetograph = [0.0] * T\n",
        "    hyetograph[central_index] = max_depth\n",
        "    remaining_depths = sorted_depths.copy()\n",
        "    remaining_depths.remove(max_depth)\n",
        "    left = central_index - 1\n",
        "    right = central_index + 1\n",
        "    toggle = True  # Start assigning to the right\n",
        "    for depth in remaining_depths:\n",
        "        if toggle and right < T:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif not toggle and left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        elif right < T:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        else:\n",
        "            print(\"Warning: Not all incremental depths assigned.\")\n",
        "            break\n",
        "        toggle = not toggle\n",
        "    return hyetograph\n",
        "\n",
        "def generate_hyetograph(incremental_depths, position_percent, T):\n",
        "    \"\"\"\n",
        "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
        "    \"\"\"\n",
        "    max_depth = np.max(incremental_depths)\n",
        "    incremental_depths_list = incremental_depths.tolist()\n",
        "    central_index = int(round(T * position_percent / 100)) - 1\n",
        "    central_index = max(0, min(central_index, T - 1))\n",
        "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
        "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
        "    return hyetograph\n",
        "\n",
        "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
        "    \"\"\"\n",
        "    Saves the hyetograph to a CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'Time_hour': np.arange(1, total_duration + 1),\n",
        "        'Precipitation_in': hyetograph\n",
        "    })\n",
        "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
        "    output_file = os.path.join(output_dir, filename)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
        "    \"\"\"\n",
        "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    for ari in aris:\n",
        "        # Ensure ARI is a string for consistent filename formatting\n",
        "        ari_str = str(ari)\n",
        "        \n",
        "        # Construct the filename based on the naming convention\n",
        "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        \n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Warning: File '{filename}' does not exist in the directory '{output_dir}'. Skipping this ARI.\")\n",
        "            continue\n",
        "        \n",
        "        # Read the hyetograph data\n",
        "        try:\n",
        "            hyetograph_df = pd.read_csv(filepath)\n",
        "            print(f\"Successfully read the hyetograph data from '{filename}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading the hyetograph CSV file '{filename}': {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Plot the hyetograph\n",
        "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
        "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
        "    \n",
        "    # Customize the plot\n",
        "    plt.xlabel('Time (Hour)', fontsize=14)\n",
        "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
        "    plt.title(f'Comparison of Hyetographs for ARIs {aris}\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 24)))  # Adjust x-ticks based on duration\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate AEP Hydrographs\n",
        "\n",
        "This cell orchestrates the entire AEP analysis process, generating hyetographs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Main function to run the entire AEP analysis process.\n",
        "\"\"\"\n",
        "# Set the paths and parameters\n",
        "input_csv = 'data/PF_Depth_English_PDS_DavisCA.csv'  # Path to NOAA Atlas 14 data\n",
        "output_dir = 'hyetographs'  # Directory for saving hyetographs\n",
        "position_percent = 50  # Position percentage for the maximum incremental depth block\n",
        "total_duration = 24  # Storm duration in hours\n",
        "base_plan = \"02\"  # Base plan to clone\n",
        "\n",
        "# Set the AEP events (return periods in years)\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "\n",
        "# Ensure the output directory exists\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory is set to: {output_dir}\")\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "# Step 1: Generate hyetographs for each AEP event\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 1: Generating hyetographs for each AEP event...\")\n",
        "\n",
        "try:\n",
        "    # Read precipitation data\n",
        "    df = read_precipitation_data(input_csv)\n",
        "    print(\"Successfully read the input CSV file.\")\n",
        "    \n",
        "    # Display the first few rows of the DataFrame to verify\n",
        "    print(\"\\nPrecipitation Frequency Data from Atlas 14:\")\n",
        "    display.display(df.head())\n",
        "    \n",
        "    # Interpolate depths\n",
        "    D = interpolate_depths(df, total_duration)\n",
        "    print(\"Successfully interpolated precipitation depths.\")\n",
        "\n",
        "    print(\"Array D with interpolated depths\")\n",
        "    display.display(D)\n",
        "    \n",
        "    # Compute incremental depths\n",
        "    inc_depths = compute_incremental_depths(D, total_duration)\n",
        "    print(\"Successfully computed incremental depths.\")\n",
        "    \n",
        "    # Show Incremental Depths\n",
        "    print(\"Array inc_depths Contents \")\n",
        "    display.display(inc_depths)\n",
        "\n",
        "    # Generate and save hyetographs for each AEP\n",
        "    hyetograph_files = {}\n",
        "    for ari in aep_events:\n",
        "        ari_str = str(ari)\n",
        "        if ari_str in inc_depths:\n",
        "            hyetograph = generate_hyetograph(inc_depths[ari_str], position_percent, total_duration)\n",
        "            file_path = save_hyetograph(hyetograph, ari_str, output_dir, position_percent, total_duration)\n",
        "            hyetograph_files[ari_str] = file_path\n",
        "        else:\n",
        "            print(f\"Warning: ARI {ari_str} not found in the data. Skipping.\")\n",
        "    \n",
        "    print(\"\\nAll hyetographs have been generated and saved.\")\n",
        "    \n",
        "    # Plot the hyetographs for comparison\n",
        "    plot_multiple_hyetographs(aep_events, position_percent, total_duration, output_dir)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error generating hyetographs: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Initialize the HEC-RAS project\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 2: Initializing the HEC-RAS ras...\")\n",
        "\n",
        "# Define the path to the Davis project\n",
        "current_dir = Path.cwd()\n",
        "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
        "\n",
        "# Check if the project exists\n",
        "if not pipes_ex_path.exists():\n",
        "    # Extract the project if needed\n",
        "    RasExamples.extract_project([\"Davis\"])\n",
        "\n",
        "# Initialize the RAS project\n",
        "init_ras_project(pipes_ex_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the existing plans\n",
        "print(\"\\nExisting plans in the project:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ras.boundaries_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"01\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"01\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsteady_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract boundary conditions and tables\n",
        "boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\n",
        "print(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "boundaries_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the contents of Unsteady File\n",
        "with open(unsteady_file, 'r') as f:\n",
        "    unsteady_contents = f.read()\n",
        "print(f\"Contents of unsteady flow file {unsteady_file}:\")\n",
        "print(\"-\" * 80)\n",
        "print(unsteady_contents)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To implement AEP event hydrographs, we will edit the Precipitation Hydrograph table\n",
        "We will need to edit both the number of values, as well as replacing the existing fixed-width table.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define HEC-RAS Plan and Unsteady Flow File Functions\n",
        "\n",
        "These functions handle creating HEC-RAS plan files and unsteady flow files for each AEP event. They apply the generated hyetographs to the boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_plan_for_aep(base_plan, aep_years, duration_hours, hyetograph_file, project):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a specific AEP event.\n",
        "    \"\"\"\n",
        "    # Create plan name and short ID\n",
        "    plan_name = f\"{aep_years}YR-{duration_hours}HR\"\n",
        "    \n",
        "    print(f\"Creating new plan '{plan_name}'...\")\n",
        "    \n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_plan_shortid=plan_name, ras_object=project)\n",
        "    print(f\"Created new plan: {new_plan_number}\")\n",
        "    \n",
        "    # Clone the unsteady flow file from the base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            \n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    \n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "    print(f\"Created new unsteady flow file: {new_unsteady_number}\")\n",
        "    \n",
        "    # Update the unsteady flow file with the hyetograph data\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "    \n",
        "    \n",
        "    # Update the flow title to reflect the AEP event\n",
        "    new_title = f\"{aep_years}YR-{duration_hours}HR Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "    print(f\"Updated unsteady flow title to: {new_title}\")\n",
        "    \n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    success = modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, project)\n",
        "    if success:\n",
        "        print(f\"Successfully applied hyetograph data from {hyetograph_file} to unsteady flow file\")\n",
        "    else:\n",
        "        print(f\"Warning: Failed to apply hyetograph data. Unsteady flow file may need manual modification.\")\n",
        "    \n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "    print(f\"Assigned unsteady flow file {new_unsteady_number} to plan {new_plan_number}\")\n",
        "    '''\n",
        "    # Update the plan description\n",
        "    description = f\"AEP {aep_years}-year, {duration_hours}-hour storm\\n\"\n",
        "    description += f\"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "    description += f\"Based on plan {base_plan}\\n\"\n",
        "    description += f\"Hyetograph from: {os.path.basename(hyetograph_file)}\"\n",
        "    \n",
        "    RasPlan.update_plan_description(new_plan_number, description, ras_object=project)\n",
        "    print(f\"Updated plan description for plan {new_plan_number}\")\n",
        "    \n",
        "    return new_plan_number, new_unsteady_number\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, project):\n",
        "    \"\"\"\n",
        "    Modifies an unsteady flow file to incorporate hyetograph data as precipitation.\n",
        "    \n",
        "    Parameters:\n",
        "    - unsteady_file_path: Path to the unsteady flow file\n",
        "    - hyetograph_file: Path to the hyetograph data CSV\n",
        "    - project: RAS project object\n",
        "    \n",
        "    Returns:\n",
        "    - Boolean indicating success\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the hyetograph data\n",
        "        hyetograph_df = pd.read_csv(hyetograph_file)\n",
        "        print(f\"Loaded hyetograph from {hyetograph_file} with {len(hyetograph_df)} values\")\n",
        "        \n",
        "        # Read the unsteady flow file\n",
        "        with open(unsteady_file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        \n",
        "        # Find the sections that need to be modified\n",
        "        precip_hydrograph_index = None\n",
        "        \n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Precipitation Hydrograph=\"):\n",
        "                precip_hydrograph_index = i\n",
        "                break\n",
        "        \n",
        "        if precip_hydrograph_index is None:\n",
        "            print(\"Cannot find Precipitation Hydrograph section in unsteady file.\")\n",
        "            return False\n",
        "        \n",
        "        # Get the time interval from the hyetograph\n",
        "        time_interval = \"1HOUR\"  # Default\n",
        "        if \"Time_hour\" in hyetograph_df.columns and len(hyetograph_df) > 1:\n",
        "            hour_diff = hyetograph_df[\"Time_hour\"].iloc[1] - hyetograph_df[\"Time_hour\"].iloc[0]\n",
        "            time_interval = f\"{int(hour_diff)}HOUR\" if hour_diff >= 1 else f\"{int(hour_diff*60)}MIN\"\n",
        "        \n",
        "        # Format the precipitation values for the hydrograph\n",
        "        precipitation_values = hyetograph_df[\"Precipitation_in\"].values\n",
        "        \n",
        "        # Create the Precipitation Hydrograph line\n",
        "        precip_line = f\"Precipitation Hydrograph= {len(precipitation_values)} \\n\"\n",
        "        \n",
        "        # Format the values in groups of 10 per line\n",
        "        value_lines = []\n",
        "        for i in range(0, len(precipitation_values), 10):\n",
        "            row_values = precipitation_values[i:i+10]\n",
        "            row_line = \"\".join([f\"{value:8.2f}\" for value in row_values]) + \"\\n\"\n",
        "            value_lines.append(row_line)\n",
        "            \n",
        "        # Remove old hydrograph data - find end of current hydrograph\n",
        "        current_line = precip_hydrograph_index + 1\n",
        "        while current_line < len(lines) and not any(lines[current_line].startswith(prefix) for prefix in [\"DSS Path=\", \"Use DSS=\", \"Use Fixed Start Time=\"]):\n",
        "            current_line += 1\n",
        "            \n",
        "        # Replace the hydrograph section\n",
        "        lines[precip_hydrograph_index:current_line] = [precip_line] + value_lines\n",
        "            \n",
        "        # Write the modified file back\n",
        "        with open(unsteady_file_path, 'w') as file:\n",
        "            file.writelines(lines)\n",
        "            \n",
        "        print(f\"Successfully applied hyetograph data from {hyetograph_file} to unsteady flow file.\")\n",
        "        print(f\"Added {len(precipitation_values)} precipitation values with interval {time_interval}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error modifying unsteady flow file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Create new plans for each AEP event\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 3: Creating new plans for each AEP event...\")\n",
        "\n",
        "new_plan_numbers = []\n",
        "\n",
        "for ari in aep_events:\n",
        "    ari_str = str(ari)\n",
        "    if ari_str in hyetograph_files:\n",
        "        try:\n",
        "            # Create a new plan for this AEP event\n",
        "            new_plan_number, _ = create_plan_for_aep(\n",
        "                base_plan=base_plan,\n",
        "                aep_years=ari_str,\n",
        "                duration_hours=total_duration,\n",
        "                hyetograph_file=hyetograph_files[ari_str],\n",
        "                project=ras\n",
        "            )\n",
        "            new_plan_numbers.append(new_plan_number)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plan for AEP {ari_str}: {e}\")\n",
        "\n",
        "# Display the updated plans\n",
        "print(\"\\nUpdated plans in the project:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"03\"\n",
        "unsteady_file_rev = RasPlan.get_unsteady_path(\"03\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file_rev}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsteady_file_rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(unsteady_file_rev, 'r') as f:\n",
        "    unsteady_contents_rev = f.read()\n",
        "print(f\"Contents of unsteady flow file for plan 03 ({unsteady_file_rev}):\")\n",
        "print(\"-\" * 80)\n",
        "print(unsteady_contents_rev)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import difflib\n",
        "\n",
        "def show_file_diff(file1_path, file2_path, context_lines=3):\n",
        "    \"\"\"\n",
        "    Shows the differences between two files with context.\n",
        "    \n",
        "    Parameters:\n",
        "    - file1_path: Path to the first file\n",
        "    - file2_path: Path to the second file\n",
        "    - context_lines: Number of context lines to show around differences\n",
        "    \"\"\"\n",
        "    # Read the file contents\n",
        "    with open(file1_path, 'r') as file1:\n",
        "        file1_lines = file1.readlines()\n",
        "    \n",
        "    with open(file2_path, 'r') as file2:\n",
        "        file2_lines = file2.readlines()\n",
        "    \n",
        "    # Create a differ object\n",
        "    differ = difflib.unified_diff(\n",
        "        file1_lines, \n",
        "        file2_lines,\n",
        "        fromfile=str(file1_path),\n",
        "        tofile=str(file2_path),\n",
        "        n=context_lines\n",
        "    )\n",
        "    \n",
        "    # Convert differ output to a string\n",
        "    diff_text = ''.join(differ)\n",
        "    \n",
        "    # If no differences found\n",
        "    if not diff_text:\n",
        "        print(f\"No differences found between {file1_path} and {file2_path}\")\n",
        "        return\n",
        "    \n",
        "    # Print the differences\n",
        "    print(f\"Differences between {file1_path} and {file2_path}:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(diff_text)\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Count added, removed, and modified lines\n",
        "    added = sum(1 for line in diff_text.splitlines() if line.startswith('+') and not line.startswith('+++'))\n",
        "    removed = sum(1 for line in diff_text.splitlines() if line.startswith('-') and not line.startswith('---'))\n",
        "    \n",
        "    print(f\"Summary: {added} additions, {removed} removals\")\n",
        "\n",
        "# Show differences between the unsteady flow files\n",
        "if 'unsteady_file' in locals() and 'unsteady_file_rev' in locals():\n",
        "    show_file_diff(unsteady_file, unsteady_file_rev)\n",
        "else:\n",
        "    print(\"Error: One or both unsteady flow file variables not defined.\")\n",
        "    print(\"Please run the cells that define unsteady_file and unsteady_file_rev first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Parallel Execution and Results Analysis Functions\n",
        "\n",
        "These functions manage parallel plan execution with resource optimization and extract, analyze, and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_plans_in_parallel(plan_numbers, project, max_workers=None, cores_per_worker=2):\n",
        "    \"\"\"\n",
        "    Executes multiple plans in parallel.\n",
        "    \"\"\"\n",
        "    # Calculate optimal number of workers if not provided\n",
        "    if max_workers is None:\n",
        "        physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
        "        max_workers = max(1, physical_cores // cores_per_worker)\n",
        "    \n",
        "    print(f\"Executing {len(plan_numbers)} plans in parallel with {max_workers} workers, \" + \n",
        "          f\"each using {cores_per_worker} cores...\")\n",
        "    \n",
        "    # Create compute folder\n",
        "    compute_folder = Path(project.project_folder) / \"compute_aep_parallel\"\n",
        "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Execute plans in parallel\n",
        "    start_time = time.time()\n",
        "    \n",
        "    results = RasCmdr.compute_parallel(\n",
        "        plan_number=plan_numbers,\n",
        "        max_workers=max_workers,\n",
        "        num_cores=cores_per_worker,\n",
        "        dest_folder=compute_folder,\n",
        "        clear_geompre=True,\n",
        "        overwrite_dest=True,\n",
        "        ras_object=project\n",
        "    )\n",
        "    \n",
        "    end_time = time.time()\n",
        "    total_duration = end_time - start_time\n",
        "    \n",
        "    print(f\"Parallel execution completed in {total_duration:.2f} seconds\")\n",
        "    \n",
        "    # Create a DataFrame from the execution results\n",
        "    results_df = pd.DataFrame([\n",
        "        {\"Plan\": plan, \"Success\": success}\n",
        "        for plan, success in results.items()\n",
        "    ])\n",
        "    \n",
        "    # Sort by plan number\n",
        "    results_df = results_df.sort_values(\"Plan\")\n",
        "    \n",
        "    print(\"\\nExecution Results:\")\n",
        "    display.display(results_df)\n",
        "    \n",
        "    return results, compute_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"01\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"01\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Execute all plans in parallel\n",
        "#-------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set computation parameters for better performance\n",
        "for plan_number in new_plan_numbers:\n",
        "    RasPlan.set_num_cores(plan_number, 2, ras_object=ras)\n",
        "    RasPlan.update_plan_intervals(\n",
        "        plan_number,\n",
        "        computation_interval=\"15MIN\",\n",
        "        output_interval=\"30MIN\",\n",
        "        mapping_interval=\"1HOUR\",\n",
        "        ras_object=ras\n",
        "    )\n",
        "    print(f\"Updated computation settings for plan {plan_number}\")\n",
        "\n",
        "# Execute plans in parallel\n",
        "results, compute_folder = execute_plans_in_parallel(\n",
        "    plan_numbers=new_plan_numbers,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\nStep 4: Executing all plans in parallel...\")\n",
        "\n",
        "results, compute_folder = execute_plans_in_parallel(\n",
        "    plan_numbers=new_plan_numbers,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df = HdfResultsPlan.get_runtime_data(\"02\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(\"02\", variable=\"Pipes/Pipe Flow DS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws = HdfPipe.get_pipe_network_timeseries(\"02\", variable=\"Nodes/Water Surface\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_results(results, compute_folder, project):\n",
        "    \"\"\"\n",
        "    Analyzes the results from multiple plans.\n",
        "    \"\"\"\n",
        "    print(\"Analyzing results from parallel execution...\")\n",
        "    \n",
        "    # Initialize a RAS project in the compute folder\n",
        "    compute_project = RasPrj()\n",
        "    compute_project = init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "    print(f\"Initialized compute project: {compute_project.project_name}\")\n",
        "    \n",
        "    # Check which plans have results\n",
        "    plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "    print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
        "    display.display(plans_with_results[['plan_number', 'Short Identifier', 'HDF_Results_Path']])\n",
        "    \n",
        "    # Initialize a dictionary to store analysis results\n",
        "    analysis_results = {}\n",
        "    \n",
        "    # Analyze each plan's results\n",
        "    for idx, row in plans_with_results.iterrows():\n",
        "        plan_number = row['plan_number']\n",
        "        plan_name = row['Short Identifier']\n",
        "        hdf_path = row['HDF_Results_Path']\n",
        "        \n",
        "        print(f\"\\nAnalyzing results for plan {plan_number} ({plan_name})...\")\n",
        "        \n",
        "        try:\n",
        "            # Get runtime data\n",
        "            runtime_df = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "            \n",
        "            if runtime_df is not None and not runtime_df.empty:\n",
        "                # Extract key metrics\n",
        "                sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
        "                compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
        "                compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
        "                \n",
        "                # Get pipe network results\n",
        "                try:\n",
        "                    # Get pipe flow data\n",
        "                    pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Pipes/Pipe Flow DS\")\n",
        "                    node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "                    \n",
        "                    # Convert xarray DataArrays to numpy arrays and compute statistics\n",
        "                    pipe_flow_array = pipe_flow_ds.values\n",
        "                    node_ws_array = node_ws.values\n",
        "                    \n",
        "                    # Calculate maximum flows and water surfaces\n",
        "                    max_flows = np.nanmax(pipe_flow_array, axis=0)  # Max over time for each location\n",
        "                    avg_max_flow = np.nanmean(max_flows)\n",
        "                    max_max_flow = np.nanmax(max_flows)\n",
        "                    \n",
        "                    max_ws = np.nanmax(node_ws_array, axis=0)  # Max over time for each node\n",
        "                    avg_max_ws = np.nanmean(max_ws)\n",
        "                    max_max_ws = np.nanmax(max_ws)\n",
        "                    \n",
        "                    # Store results in the dictionary\n",
        "                    analysis_results[plan_name] = {\n",
        "                        'Plan Number': plan_number,\n",
        "                        'Simulation Duration (s)': sim_duration,\n",
        "                        'Compute Time (hr)': compute_time,\n",
        "                        'Compute Speed (hr/hr)': compute_speed,\n",
        "                        'Average Max Pipe Flow (cfs)': avg_max_flow,\n",
        "                        'Maximum Pipe Flow (cfs)': max_max_flow,\n",
        "                        'Average Max Node Water Surface (ft)': avg_max_ws,\n",
        "                        'Maximum Node Water Surface (ft)': max_max_ws,\n",
        "                        'HDF Path': hdf_path\n",
        "                    }\n",
        "                    \n",
        "                    print(f\"  Simulation Duration: {sim_duration:.2f} seconds\")\n",
        "                    print(f\"  Computation Time: {compute_time:.5f} hours\")\n",
        "                    print(f\"  Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
        "                    print(f\"  Average Max Pipe Flow: {avg_max_flow:.2f} cfs\")\n",
        "                    print(f\"  Maximum Pipe Flow: {max_max_flow:.2f} cfs\")\n",
        "                    print(f\"  Average Max Node Water Surface: {avg_max_ws:.2f} ft\")\n",
        "                    print(f\"  Maximum Node Water Surface: {max_max_ws:.2f} ft\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"  Error analyzing pipe network data: {str(e)}\")\n",
        "                    analysis_results[plan_name] = {\n",
        "                        'Plan Number': plan_number,\n",
        "                        'Simulation Duration (s)': sim_duration,\n",
        "                        'Compute Time (hr)': compute_time,\n",
        "                        'Compute Speed (hr/hr)': compute_speed,\n",
        "                        'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                        'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                        'Average Max Node Water Surface (ft)': np.nan,\n",
        "                        'Maximum Node Water Surface (ft)': np.nan,\n",
        "                        'HDF Path': hdf_path\n",
        "                    }\n",
        "            else:\n",
        "                print(\"  No runtime data found.\")\n",
        "                analysis_results[plan_name] = {\n",
        "                    'Plan Number': plan_number,\n",
        "                    'Simulation Duration (s)': np.nan,\n",
        "                    'Compute Time (hr)': np.nan,\n",
        "                    'Compute Speed (hr/hr)': np.nan,\n",
        "                    'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                    'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                    'Average Max Node Water Surface (ft)': np.nan,\n",
        "                    'Maximum Node Water Surface (ft)': np.nan,\n",
        "                    'HDF Path': hdf_path\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error analyzing plan {plan_number}: {str(e)}\")\n",
        "            analysis_results[plan_name] = {\n",
        "                'Plan Number': plan_number,\n",
        "                'Simulation Duration (s)': np.nan,\n",
        "                'Compute Time (hr)': np.nan,\n",
        "                'Compute Speed (hr/hr)': np.nan,\n",
        "                'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                'Average Max Node Water Surface (ft)': np.nan,\n",
        "                'Maximum Node Water Surface (ft)': np.nan,\n",
        "                'HDF Path': hdf_path\n",
        "            }\n",
        "    \n",
        "    # Create a DataFrame from the analysis results\n",
        "    analysis_df = pd.DataFrame.from_dict(analysis_results, orient='index')\n",
        "    \n",
        "    # Extract AEP years and handle NaN values\n",
        "    analysis_df['AEP_Years'] = analysis_df.index.str.extract(r'(\\d+)YR').astype(float)\n",
        "    \n",
        "    # Sort by AEP years, handling the base plan\n",
        "    analysis_df = analysis_df.sort_values('AEP_Years', na_position='first')\n",
        "    \n",
        "    # Drop the temporary column used for sorting\n",
        "    analysis_df = analysis_df.drop(columns=['AEP_Years'])\n",
        "    \n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    display.display(analysis_df)\n",
        "    \n",
        "    return analysis_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------------\n",
        "# Step 5: Analyze the results\n",
        "#---------------------------------------------------------------------\n",
        "print(\"\\nStep 5: Analyzing the results...\")\n",
        "\n",
        "analysis_df = analyze_results(\n",
        "    results=results,\n",
        "    compute_folder=compute_folder,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_results(analysis_df):\n",
        "    \"\"\"\n",
        "    Plots the results from the analysis.\n",
        "    \"\"\"\n",
        "    # Extract AEP values from the index (plan names), skipping non-AEP plans\n",
        "    aep_values = []\n",
        "    aep_data = pd.DataFrame()\n",
        "    \n",
        "    for name in analysis_df.index:\n",
        "        if 'YR' in name:\n",
        "            try:\n",
        "                aep_year = int(name.split('YR')[0])\n",
        "                aep_values.append(aep_year)\n",
        "                aep_data = pd.concat([aep_data, analysis_df.loc[[name]]])\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    if len(aep_values) == 0:\n",
        "        print(\"No valid AEP plans found to plot\")\n",
        "        return\n",
        "        \n",
        "    # Create a figure with multiple subplots\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Plot 1: Maximum Pipe Flow vs AEP\n",
        "    axs[0].semilogx(aep_values, aep_data['Maximum Pipe Flow (cfs)'], 'o-', marker='o', markersize=8)\n",
        "    axs[0].set_title('Maximum Pipe Flow vs Return Period', fontsize=16)\n",
        "    axs[0].set_xlabel('Return Period (years)', fontsize=14)\n",
        "    axs[0].set_ylabel('Maximum Pipe Flow (cfs)', fontsize=14)\n",
        "    axs[0].grid(True)\n",
        "    \n",
        "    # Add data labels\n",
        "    for i, txt in enumerate(aep_values):\n",
        "        axs[0].annotate(f\"{txt} yr\", \n",
        "                      (aep_values[i], aep_data['Maximum Pipe Flow (cfs)'].iloc[i]),\n",
        "                      textcoords=\"offset points\", \n",
        "                      xytext=(0, 10), \n",
        "                      ha='center')\n",
        "    \n",
        "    # Plot 2: Maximum Node Water Surface vs AEP\n",
        "    axs[1].semilogx(aep_values, aep_data['Maximum Node Water Surface (ft)'], 'o-', marker='s', markersize=8, color='green')\n",
        "    axs[1].set_title('Maximum Node Water Surface vs Return Period', fontsize=16)\n",
        "    axs[1].set_xlabel('Return Period (years)', fontsize=14)\n",
        "    axs[1].set_ylabel('Maximum Node Water Surface (ft)', fontsize=14)\n",
        "    axs[1].grid(True)\n",
        "    \n",
        "    # Add data labels\n",
        "    for i, txt in enumerate(aep_values):\n",
        "        axs[1].annotate(f\"{txt} yr\", \n",
        "                      (aep_values[i], aep_data['Maximum Node Water Surface (ft)'].iloc[i]),\n",
        "                      textcoords=\"offset points\", \n",
        "                      xytext=(0, 10), \n",
        "                      ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Plot time series for each return period\n",
        "    try:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        \n",
        "        # Create a color map for different return periods\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(aep_values)))\n",
        "        \n",
        "        # Plot each return period\n",
        "        for i, name in enumerate(aep_data.index):\n",
        "            # Get HDF path for this return period\n",
        "            hdf_path = aep_data.loc[name, 'HDF Path']\n",
        "            \n",
        "            # Get pipe network timeseries data\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "            \n",
        "            # Get data for location 61\n",
        "            loc_61_ws = node_ws.sel(location=61)\n",
        "            \n",
        "            # Plot the time series\n",
        "            plt.plot(loc_61_ws.time.values, loc_61_ws.values, \n",
        "                    label=f'{name}', \n",
        "                    color=colors[i],\n",
        "                    linewidth=2)\n",
        "        \n",
        "        plt.title('Water Surface Elevation Time Series by Return Period - Location 61', fontsize=16)\n",
        "        plt.xlabel('Time', fontsize=14)\n",
        "        plt.ylabel('Water Surface Elevation (ft)', fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.legend(fontsize=12)\n",
        "        \n",
        "        # Format x-axis dates\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Could not create detailed heatmap: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 6: Plotting the results...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\billk\\AppData\\Local\\Temp\\ipykernel_25136\\3654224487.py:26: UserWarning: marker is redundantly defined by the 'marker' keyword argument and the fmt string \"o-\" (-> marker='o'). The keyword argument will take precedence.\n",
            "  axs[0].semilogx(aep_values, aep_data['Maximum Pipe Flow (cfs)'], 'o-', marker='o', markersize=8)\n",
            "C:\\Users\\billk\\AppData\\Local\\Temp\\ipykernel_25136\\3654224487.py:41: UserWarning: marker is redundantly defined by the 'marker' keyword argument and the fmt string \"o-\" (-> marker='o'). The keyword argument will take precedence.\n",
            "  axs[1].semilogx(aep_values, aep_data['Maximum Node Water Surface (ft)'], 'o-', marker='s', markersize=8, color='green')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1400x1200 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Using HDF file from direct string path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p01.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Final validated HDF file path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p01.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Using HDF file from direct string path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p03.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Final validated HDF file path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p03.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Using HDF file from direct string path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p04.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Final validated HDF file path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p04.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Using HDF file from direct string path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p05.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Final validated HDF file path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p05.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Using HDF file from direct string path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p06.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Final validated HDF file path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p06.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Using HDF file from direct string path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p07.hdf\n",
            "2025-03-20 11:53:52 - ras_commander.HdfPipe - INFO - Final validated HDF file path: c:\\Users\\billk\\Desktop\\RAS Commander Examples HOLDBACK\\example_projects\\Davis\\compute_aep_parallel\\DavisStormSystem.p07.hdf\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#---------------------------------------------------------------------\n",
        "# Step 6: Plot the results\n",
        "#---------------------------------------------------------------------\n",
        "print(\"\\nStep 6: Plotting the results...\")\n",
        "\n",
        "plot_results(analysis_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "node_ws = HdfPipe.get_pipe_network_timeseries(\"04\", variable=\"Nodes/Water Surface\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div><pre>xarray.DataArray with Unknown dimensions\n[Full xarray output truncated during preprocessing]</pre></div>",
            "text/plain": "xarray.DataArray with Unknown dimensions\n[Full xarray output truncated during preprocessing]"
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(\"04\", variable=\"Pipes/Pipe Flow DS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div><pre>xarray.DataArray with Unknown dimensions\n[Full xarray output truncated during preprocessing]</pre></div>",
            "text/plain": "xarray.DataArray with Unknown dimensions\n[Full xarray output truncated during preprocessing]"
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe_flow_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot time series for each event"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot time series for each AEP event at the same downstream location\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import pandas as pd\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "# Set up the figure with a larger size for better visibility\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Set up color map for different ARI events\n",
        "cmap = plt.cm.viridis\n",
        "colors = [cmap(i) for i in np.linspace(0, 0.9, len(new_plan_numbers))]\n",
        "\n",
        "# Dictionary to store all dataframes for later use\n",
        "all_ts_data = {}\n",
        "\n",
        "# Loop through each plan we created\n",
        "for i, plan_number in enumerate(new_plan_numbers):\n",
        "    try:\n",
        "        # Get results HDF path for this plan\n",
        "        results_path = RasPlan.get_results_path(plan_number, ras_object=ras)\n",
        "        \n",
        "        if not results_path.exists():\n",
        "            print(f\"No results found for Plan {plan_number}. Skipping.\")\n",
        "            continue\n",
        "            \n",
        "        # Extract the AEP value from the plan name\n",
        "        plan_info = ras.plan_df.loc[int(plan_number)]\n",
        "        plan_name = plan_info[\"Title\"]\n",
        "        aep_label = f\"{plan_info['ShortID']} ({plan_name})\"\n",
        "        \n",
        "        # Get reference point time series (using the same 'pipe_flow_ds' point)\n",
        "        ref_ts = HdfResultsPlan.reference_points_timeseries_output(results_path)\n",
        "        \n",
        "        # Check if pipe_flow_ds exists in the results\n",
        "        if 'pipe_flow_ds' not in ref_ts:\n",
        "            print(f\"Warning: 'pipe_flow_ds' not found in results for Plan {plan_number}\")\n",
        "            continue\n",
        "        \n",
        "        # Get the time series data for pipe_flow_ds\n",
        "        ts_data = ref_ts['pipe_flow_ds']\n",
        "        \n",
        "        # Store data for potential further use\n",
        "        all_ts_data[plan_number] = ts_data\n",
        "        \n",
        "        # Plot the stage (water surface elevation)\n",
        "        if 'Stage' in ts_data.columns:\n",
        "            plt.plot(ts_data.index, ts_data['Stage'], \n",
        "                     label=aep_label, \n",
        "                     color=colors[i], \n",
        "                     linewidth=2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing Plan {plan_number}: {e}\")\n",
        "\n",
        "# Add grid, legend and labels\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.title('Water Surface Elevation Time Series at Downstream Point', fontsize=14)\n",
        "plt.xlabel('Time', fontsize=12)\n",
        "plt.ylabel('Stage (m)', fontsize=12)\n",
        "\n",
        "# Format the x-axis to show dates nicely\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust y-axis to show reasonable number of ticks\n",
        "plt.gca().yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
        "\n",
        "# Add a secondary plot with flow if it exists in the data\n",
        "if all(('Flow' in df.columns) for df in all_ts_data.values() if not df.empty):\n",
        "    ax2 = plt.gca().twinx()\n",
        "    \n",
        "    for i, (plan_number, ts_data) in enumerate(all_ts_data.items()):\n",
        "        if 'Flow' in ts_data.columns:\n",
        "            ax2.plot(ts_data.index, ts_data['Flow'], \n",
        "                     linestyle='--', \n",
        "                     color=colors[i], \n",
        "                     alpha=0.5)\n",
        "    \n",
        "    ax2.set_ylabel('Flow (m\u00b3/s)', color='gray', fontsize=12)\n",
        "    ax2.tick_params(axis='y', labelcolor='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a table of peak values for each plan\n",
        "print(\"\\nPeak Values Summary:\")\n",
        "peak_data = []\n",
        "\n",
        "for plan_number, ts_data in all_ts_data.items():\n",
        "    plan_info = ras.plan_df.loc[int(plan_number)]\n",
        "    aep_label = f\"{plan_info['ShortID']} ({plan_info['Title']})\"\n",
        "    \n",
        "    if 'Stage' in ts_data.columns:\n",
        "        peak_stage = ts_data['Stage'].max()\n",
        "        peak_time = ts_data['Stage'].idxmax()\n",
        "        \n",
        "        peak_flow = None\n",
        "        if 'Flow' in ts_data.columns:\n",
        "            peak_flow = ts_data['Flow'].max()\n",
        "        \n",
        "        peak_data.append({\n",
        "            'Plan': aep_label,\n",
        "            'Peak Stage (m)': round(peak_stage, 2),\n",
        "            'Time of Peak': peak_time,\n",
        "            'Peak Flow (m\u00b3/s)': round(peak_flow, 2) if peak_flow is not None else 'N/A'\n",
        "        })\n",
        "\n",
        "if peak_data:\n",
        "    peak_df = pd.DataFrame(peak_data)\n",
        "    display(peak_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a comprehensive workflow for automated AEP storm analysis using RAS-Commander. The key benefits of this approach include:\n",
        "\n",
        "1. **Efficiency**: Automating repetitive tasks saves time and reduces errors\n",
        "2. **Consistency**: Ensures consistent methodology across all return periods\n",
        "3. **Parallel Execution**: Makes optimal use of computational resources\n",
        "4. **Comprehensive Analysis**: Extracts and visualizes key metrics across return periods\n",
        "5. **Reproducibility**: The entire workflow is documented and repeatable\n",
        "\n",
        "This approach can be extended to include additional analyses, such as:\n",
        "\n",
        "- Comparing different storm patterns (e.g., position of peak intensity)\n",
        "- Analyzing climate change scenarios by adjusting precipitation depths\n",
        "- Evaluating infrastructure improvements by comparing baseline and modified geometries\n",
        "- Generating frequency curves for key hydraulic parameters\n",
        "\n",
        "By leveraging the power of RAS-Commander, engineers can focus on interpreting results and making design decisions rather than managing model configurations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}