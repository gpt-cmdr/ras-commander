{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Atlas 14 Uncertainty Analysis with Multi-Project Support\n",
        "\n",
        "This notebook performs comprehensive uncertainty analysis of precipitation-driven flooding by:\n",
        "\n",
        "1. **Processing Multiple Durations**: Analyzes 1-hr, 2-hr, 3-hr, 6-hr, 12-hr, 24-hr, and 2-day storms\n",
        "2. **Including Confidence Intervals**: Runs upper and lower confidence bounds for each scenario\n",
        "3. **Quantifying Uncertainty**: Shows how precipitation uncertainty propagates through flood models\n",
        "4. **Comprehensive Visualization**: Creates confidence envelope plots and uncertainty heatmaps\n",
        "5. **Multi-Project Management**: Automatically handles HEC-RAS 99-plan limit by distributing scenarios across multiple project copies\n",
        "\n",
        "## Methodology\n",
        "\n",
        "**Confidence Interval Estimation:**\n",
        "- NOAA Atlas 14 precipitation estimates have inherent uncertainty\n",
        "- Upper confidence bound \u2248 1.4 \u00d7 point estimate\n",
        "- Lower confidence bound \u2248 0.7 \u00d7 point estimate\n",
        "- These factors represent approximate 90% confidence intervals\n",
        "\n",
        "**Scenario Matrix:**\n",
        "- 6 AEP events (2, 5, 10, 25, 50, 100 years)\n",
        "- 7 durations (1hr, 2hr, 3hr, 6hr, 12hr, 24hr, 2day)\n",
        "- 3 confidence levels (lower, point, upper)\n",
        "- **Total: 126 scenarios**\n",
        "\n",
        "**99-Plan Limit Solution:**\n",
        "- HEC-RAS projects limited to 99 plans maximum\n",
        "- Automated distribution across multiple project copies\n",
        "- Preserves original project (all work done in copies)\n",
        "- Results automatically aggregated from all projects\n",
        "\n",
        "**Analysis Outputs:**\n",
        "- Confidence envelopes for peak water surfaces\n",
        "- Uncertainty quantification by duration and location\n",
        "- Design recommendations considering uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "from IPython import display\n",
        "import psutil\n",
        "from itertools import product\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract and Initialize Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize project\n",
        "current_dir = Path.cwd()\n",
        "pipes_ex_path = current_dir / \"A14_Examples\" / \"Davis\"\n",
        "rasexamples_extract_path = current_dir / \"A14_Examples\"\n",
        "\n",
        "if not pipes_ex_path.exists():\n",
        "    RasExamples.extract_project([\"Davis\"], output_path=rasexamples_extract_path)\n",
        "\n",
        "init_ras_project(pipes_ex_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "print(f\"\\nBase plan configuration:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Hyetograph Generation with Confidence Intervals\n",
        "\n",
        "These functions extend the original hyetograph generation to support:\n",
        "- Multiple durations (not just 24-hour)\n",
        "- Confidence interval calculations\n",
        "- Systematic scenario organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_duration(duration_str):\n",
        "    \"\"\"\n",
        "    Parses a duration string and converts it to hours.\n",
        "    Examples: \"5-min:\" -> 0.0833 hours, \"2-hr:\" -> 2 hours, \"2-day:\" -> 48 hours\n",
        "    \"\"\"\n",
        "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
        "    if not match:\n",
        "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
        "    value, unit = match.groups()\n",
        "    value = int(value)\n",
        "    unit = unit.lower()\n",
        "    if unit in ['min', 'minute', 'minutes']:\n",
        "        hours = value / 60.0\n",
        "    elif unit in ['hr', 'hour', 'hours']:\n",
        "        hours = value\n",
        "    elif unit in ['day', 'days']:\n",
        "        hours = value * 24\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
        "    return hours\n",
        "\n",
        "def read_precipitation_data(csv_file):\n",
        "    \"\"\"\n",
        "    Reads the precipitation frequency CSV and returns a DataFrame\n",
        "    with durations in hours as the index and ARIs as columns.\n",
        "    \"\"\"\n",
        "    with open(csv_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    header_line_idx = None\n",
        "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
        "\n",
        "    # Locate the header line\n",
        "    for idx, line in enumerate(lines):\n",
        "        if header_pattern.match(line.strip().lower()):\n",
        "            header_line_idx = idx\n",
        "            break\n",
        "\n",
        "    if header_line_idx is None:\n",
        "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
        "\n",
        "    # Extract the ARI headers from the header line\n",
        "    header_line = lines[header_line_idx].strip()\n",
        "    headers = [item.strip() for item in header_line.split(',')]\n",
        "    \n",
        "    if len(headers) < 2:\n",
        "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
        "\n",
        "    aris = headers[1:]  # Exclude the first column which is the duration\n",
        "\n",
        "    # Define the pattern for data lines\n",
        "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
        "\n",
        "    # Initialize lists to store durations and corresponding depths\n",
        "    durations = []\n",
        "    depths = {ari: [] for ari in aris}\n",
        "\n",
        "    # Iterate over the lines following the header to extract data\n",
        "    for line in lines[header_line_idx + 1:]:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if not duration_pattern.match(line):\n",
        "            break\n",
        "        parts = [part.strip() for part in line.split(',')]\n",
        "        if len(parts) != len(headers):\n",
        "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
        "        duration_str = parts[0]\n",
        "        try:\n",
        "            duration_hours = parse_duration(duration_str)\n",
        "        except ValueError as ve:\n",
        "            print(f\"Skipping line due to error: {ve}\")\n",
        "            continue\n",
        "        durations.append(duration_hours)\n",
        "        for ari, depth_str in zip(aris, parts[1:]):\n",
        "            try:\n",
        "                depth = float(depth_str)\n",
        "            except ValueError:\n",
        "                depth = np.nan\n",
        "            depths[ari].append(depth)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(depths, index=durations)\n",
        "    df.index.name = 'Duration_hours'\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_confidence_intervals(df_point, upper_factor=1.4, lower_factor=0.7):\n",
        "    \"\"\"\n",
        "    Creates upper and lower confidence interval DataFrames from point estimates.\n",
        "    \n",
        "    Parameters:\n",
        "    - df_point: DataFrame with point estimates\n",
        "    - upper_factor: Multiplier for upper CI (default 1.4 for ~90% CI)\n",
        "    - lower_factor: Multiplier for lower CI (default 0.7 for ~90% CI)\n",
        "    \n",
        "    Returns:\n",
        "    - Tuple of (df_lower, df_point, df_upper)\n",
        "    \"\"\"\n",
        "    df_upper = df_point * upper_factor\n",
        "    df_lower = df_point * lower_factor\n",
        "    \n",
        "    return df_lower, df_point, df_upper\n",
        "\n",
        "def get_time_interval(duration_hrs):\n",
        "    \"\"\"\n",
        "    Determines the appropriate time interval based on storm duration.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    duration_hrs : float\n",
        "        Storm duration in hours\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    float : Time interval in hours\n",
        "    \"\"\"\n",
        "    if duration_hrs >= 24:\n",
        "        return 1.0  # 1 hour for 24+ hour storms\n",
        "    elif duration_hrs >= 12:\n",
        "        return 0.5  # 30 minutes for 12-hour storms\n",
        "    elif duration_hrs >= 6:\n",
        "        return 0.25  # 15 minutes for 6-hour storms\n",
        "    else:\n",
        "        return 5.0 / 60.0  # 5 minutes for storms less than 6 hours\n",
        "\n",
        "def interpolate_depths(df, total_duration):\n",
        "    \"\"\"\n",
        "    Interpolates precipitation depths for each ARI on a log-log scale\n",
        "    using appropriate time intervals based on duration.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Precipitation frequency data with durations as index and ARIs as columns\n",
        "    total_duration : float\n",
        "        Total storm duration in hours\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (dict of interpolated depths, time array in hours)\n",
        "    \"\"\"\n",
        "    # Determine time interval based on duration\n",
        "    dt = get_time_interval(total_duration)\n",
        "    \n",
        "    # Create time array with appropriate interval\n",
        "    t_hours = np.arange(dt, total_duration + dt/2, dt)\n",
        "    \n",
        "    D = {}\n",
        "    for ari in df.columns:\n",
        "        durations = df.index.values\n",
        "        depths = df[ari].values\n",
        "        if np.any(depths <= 0):\n",
        "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
        "        \n",
        "        # Log-log interpolation\n",
        "        log_durations = np.log(durations)\n",
        "        log_depths = np.log(depths)\n",
        "        log_t = np.log(t_hours)\n",
        "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
        "        D_t = np.exp(log_D_t)\n",
        "        D[ari] = D_t\n",
        "    \n",
        "    return D, t_hours\n",
        "\n",
        "def compute_incremental_depths(D, t_hours):\n",
        "    \"\"\"\n",
        "    Computes incremental precipitation depths for each time interval.\n",
        "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D : dict\n",
        "        Dictionary of cumulative depths for each ARI\n",
        "    t_hours : array\n",
        "        Time array in hours\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary of incremental depths for each ARI\n",
        "    \"\"\"\n",
        "    incremental_depths = {}\n",
        "    for ari, D_t in D.items():\n",
        "        num_intervals = len(t_hours)\n",
        "        I_t = np.empty(num_intervals)\n",
        "        I_t[0] = D_t[0]\n",
        "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
        "        incremental_depths[ari] = I_t\n",
        "    return incremental_depths\n",
        "\n",
        "def assign_alternating_block(sorted_depths, max_depth, central_index, num_intervals):\n",
        "    \"\"\"\n",
        "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sorted_depths : list\n",
        "        Sorted incremental depths (descending)\n",
        "    max_depth : float\n",
        "        Maximum depth value\n",
        "    central_index : int\n",
        "        Index for peak position\n",
        "    num_intervals : int\n",
        "        Total number of time intervals\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    list : Hyetograph array\n",
        "    \"\"\"\n",
        "    hyetograph = [0.0] * num_intervals\n",
        "    hyetograph[central_index] = max_depth\n",
        "    remaining_depths = sorted_depths.copy()\n",
        "    remaining_depths.remove(max_depth)\n",
        "    left = central_index - 1\n",
        "    right = central_index + 1\n",
        "    toggle = True\n",
        "    for depth in remaining_depths:\n",
        "        if toggle and right < num_intervals:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif not toggle and left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        elif right < num_intervals:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        else:\n",
        "            print(\"Warning: Not all incremental depths assigned.\")\n",
        "            break\n",
        "        toggle = not toggle\n",
        "    return hyetograph\n",
        "\n",
        "def generate_hyetograph(incremental_depths, position_percent, num_intervals):\n",
        "    \"\"\"\n",
        "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    incremental_depths : array\n",
        "        Incremental depths for each time interval\n",
        "    position_percent : float\n",
        "        Peak position as percentage (e.g., 50 for middle)\n",
        "    num_intervals : int\n",
        "        Total number of time intervals\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    list : Hyetograph array\n",
        "    \"\"\"\n",
        "    max_depth = np.max(incremental_depths)\n",
        "    incremental_depths_list = incremental_depths.tolist()\n",
        "    central_index = int(round(num_intervals * position_percent / 100)) - 1\n",
        "    central_index = max(0, min(central_index, num_intervals - 1))\n",
        "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
        "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, num_intervals)\n",
        "    return hyetograph\n",
        "\n",
        "def save_hyetograph(hyetograph, t_hours, ari, duration_hrs, ci_level, output_dir, position_percent):\n",
        "    \"\"\"\n",
        "    Saves the hyetograph to a CSV file with clear naming convention.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    hyetograph : array\n",
        "        Hyetograph values\n",
        "    t_hours : array\n",
        "        Time array in hours\n",
        "    ari : int or str\n",
        "        Annual recurrence interval\n",
        "    duration_hrs : float\n",
        "        Storm duration in hours\n",
        "    ci_level : str\n",
        "        Confidence level ('lower', 'point', 'upper')\n",
        "    output_dir : str\n",
        "        Output directory path\n",
        "    position_percent : float\n",
        "        Peak position percentage\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    str : Path to saved file\n",
        "    \"\"\"\n",
        "    # Determine time interval\n",
        "    dt = get_time_interval(duration_hrs)\n",
        "    \n",
        "    # Create DataFrame with appropriate time units\n",
        "    if dt >= 1.0:\n",
        "        time_col_name = 'Time_hour'\n",
        "        time_values = t_hours\n",
        "    elif dt >= 1.0/60.0:\n",
        "        time_col_name = 'Time_min'\n",
        "        time_values = t_hours * 60  # Convert to minutes\n",
        "    else:\n",
        "        time_col_name = 'Time_hour'\n",
        "        time_values = t_hours\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        time_col_name: time_values,\n",
        "        'Precipitation_in': hyetograph\n",
        "    })\n",
        "    \n",
        "    # Format duration string\n",
        "    if duration_hrs >= 24:\n",
        "        dur_str = f\"{int(duration_hrs/24)}day\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration_hrs)}hr\"\n",
        "    \n",
        "    filename = f'hyetograph_ARI_{ari}_DUR_{dur_str}_CI_{ci_level}.csv'\n",
        "    output_file = os.path.join(output_dir, filename)\n",
        "    \n",
        "    # Add metadata as header comment\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(f\"# NOAA Atlas 14 Hyetograph\\n\")\n",
        "        f.write(f\"# ARI: {ari} years\\n\")\n",
        "        f.write(f\"# Duration: {duration_hrs} hours\\n\")\n",
        "        f.write(f\"# Time Interval: {dt*60:.1f} minutes\\n\")\n",
        "        f.write(f\"# Confidence Level: {ci_level}\\n\")\n",
        "        f.write(f\"# Peak Position: {position_percent}%\\n\")\n",
        "        f.write(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        df.to_csv(f, index=False)\n",
        "    \n",
        "    return output_file\n",
        "\n",
        "print(\"Hyetograph generation functions defined (with variable time intervals)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Complete Scenario Matrix\n",
        "\n",
        "Create hyetographs for all combinations of:\n",
        "- AEP events: 2, 5, 10, 25, 50, 100 years\n",
        "- Durations: 1hr, 2hr, 3hr, 6hr, 12hr, 24hr, 2day\n",
        "- Confidence levels: lower, point, upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "input_csv = 'data/PF_Depth_English_PDS_DavisCA.csv'\n",
        "output_dir = 'hyetographs_uncertainty'\n",
        "position_percent = 50\n",
        "base_plan = \"02\"\n",
        "\n",
        "# Define scenario parameters\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "durations = [1, 2, 3, 6, 12, 24, 48]  # hours\n",
        "ci_levels = ['lower', 'point', 'upper']\n",
        "ci_factors = {'lower': 0.7, 'point': 1.0, 'upper': 1.4}\n",
        "\n",
        "# Create output directory\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"\\nScenario Matrix:\")\n",
        "print(f\"  AEP Events: {aep_events}\")\n",
        "print(f\"  Durations: {durations} hours\")\n",
        "print(f\"  CI Levels: {ci_levels}\")\n",
        "print(f\"  Total Scenarios: {len(aep_events) * len(durations) * len(ci_levels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read precipitation data\n",
        "print(\"Reading NOAA Atlas 14 data...\")\n",
        "df_point = read_precipitation_data(input_csv)\n",
        "print(f\"Successfully read data with {len(df_point)} durations and {len(df_point.columns)} ARI values\")\n",
        "\n",
        "# Display the data\n",
        "print(\"\\nPrecipitation Frequency Data (Point Estimates):\")\n",
        "display.display(df_point.head(10))\n",
        "\n",
        "# Create confidence interval DataFrames\n",
        "print(\"\\nGenerating confidence intervals...\")\n",
        "df_lower, df_point, df_upper = create_confidence_intervals(df_point)\n",
        "print(\"Confidence intervals created:\")\n",
        "print(f\"  Lower CI factor: {ci_factors['lower']}\")\n",
        "print(f\"  Point estimate factor: {ci_factors['point']}\")\n",
        "print(f\"  Upper CI factor: {ci_factors['upper']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all hyetographs\n",
        "print(\"Generating hyetographs with variable time intervals...\\n\")\n",
        "print(\"Time Interval Rules:\")\n",
        "print(\"  \u2022 24+ hour storms: 1-hour intervals\")\n",
        "print(\"  \u2022 12-hour storms: 30-minute intervals\")\n",
        "print(\"  \u2022 6-hour storms: 15-minute intervals\")\n",
        "print(\"  \u2022 < 6-hour storms: 5-minute intervals\\n\")\n",
        "\n",
        "scenario_list = []\n",
        "hyetograph_count = 0\n",
        "\n",
        "for ari in aep_events:\n",
        "    ari_str = str(ari)\n",
        "    \n",
        "    # Check if this ARI is in the data\n",
        "    if ari_str not in df_point.columns:\n",
        "        print(f\"Warning: ARI {ari_str} not found in data. Skipping.\")\n",
        "        continue\n",
        "    \n",
        "    for duration in durations:\n",
        "        # Get time interval for this duration\n",
        "        dt = get_time_interval(duration)\n",
        "        print(f\"Processing {ari}-yr, {duration}-hr storm (dt={dt*60:.1f} min)...\")\n",
        "        \n",
        "        # Process each confidence level\n",
        "        for ci_level in ci_levels:\n",
        "            # Select the appropriate DataFrame\n",
        "            if ci_level == 'lower':\n",
        "                df_current = df_lower\n",
        "            elif ci_level == 'upper':\n",
        "                df_current = df_upper\n",
        "            else:\n",
        "                df_current = df_point\n",
        "            \n",
        "            # Interpolate depths with appropriate time interval\n",
        "            D, t_hours = interpolate_depths(df_current, duration)\n",
        "            \n",
        "            # Compute incremental depths\n",
        "            inc_depths = compute_incremental_depths(D, t_hours)\n",
        "            \n",
        "            # Generate hyetograph\n",
        "            num_intervals = len(t_hours)\n",
        "            hyetograph = generate_hyetograph(inc_depths[ari_str], position_percent, num_intervals)\n",
        "            \n",
        "            # Save hyetograph\n",
        "            file_path = save_hyetograph(hyetograph, t_hours, ari_str, duration, ci_level, \n",
        "                                       output_dir, position_percent)\n",
        "            \n",
        "            # Record scenario\n",
        "            scenario_list.append({\n",
        "                'ari': ari,\n",
        "                'duration_hrs': duration,\n",
        "                'time_interval_min': dt * 60,\n",
        "                'num_intervals': num_intervals,\n",
        "                'ci_level': ci_level,\n",
        "                'hyetograph_file': file_path,\n",
        "                'total_depth_in': sum(hyetograph)\n",
        "            })\n",
        "            \n",
        "            hyetograph_count += 1\n",
        "\n",
        "# Create scenario DataFrame\n",
        "scenario_df = pd.DataFrame(scenario_list)\n",
        "\n",
        "print(f\"\\n\u2713 Generated {hyetograph_count} hyetographs\")\n",
        "print(f\"\\nScenario Summary (first 12 rows):\")\n",
        "display.display(scenario_df.head(12))\n",
        "\n",
        "# Save scenario list\n",
        "scenario_csv = 'scenarios_uncertainty.csv'\n",
        "scenario_df.to_csv(scenario_csv, index=False)\n",
        "print(f\"\\nScenario list saved to: {scenario_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precip-depth-header",
      "metadata": {},
      "source": [
        "## Total Precipitation Depth Analysis\n",
        "\n",
        "Visualize total precipitation depths across all scenarios with confidence intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precip-depth-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your data is in scenario_df with columns: duration_hrs, ari, ci_level, total_depth_in\n",
        "\n",
        "# Create the plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Define your ARI events (adjust based on your data)\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "\n",
        "for idx, ari in enumerate(aep_events):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Extract data for this specific ARI\n",
        "    ari_data = scenario_df[scenario_df['ari'] == ari].copy()\n",
        "\n",
        "    # Get unique durations\n",
        "    durations = sorted(ari_data['duration_hrs'].unique())\n",
        "\n",
        "    # Initialize lists to store values\n",
        "    lower_vals = []\n",
        "    upper_vals = []\n",
        "    point_vals = []\n",
        "    ci_percentages = []\n",
        "\n",
        "    # Process each duration\n",
        "    for dur in durations:\n",
        "        # Get data for this specific duration\n",
        "        dur_data = ari_data[ari_data['duration_hrs'] == dur]\n",
        "\n",
        "        # Extract values for each CI level\n",
        "        lower_val = dur_data[dur_data['ci_level'] == 'lower']['total_depth_in'].values\n",
        "        upper_val = dur_data[dur_data['ci_level'] == 'upper']['total_depth_in'].values\n",
        "        point_val = dur_data[dur_data['ci_level'] == 'point']['total_depth_in'].values\n",
        "\n",
        "        # Handle missing data\n",
        "        if len(lower_val) > 0 and len(upper_val) > 0 and len(point_val) > 0:\n",
        "            lower = float(lower_val[0])\n",
        "            upper = float(upper_val[0])\n",
        "            point = float(point_val[0])\n",
        "\n",
        "            # Calculate CI width as percentage of point estimate\n",
        "            # Using point estimate as the reference (you could also use mean of upper/lower)\n",
        "            if point > 0:\n",
        "                ci_width = upper - lower\n",
        "                ci_pct = (ci_width / point) * 100\n",
        "            else:\n",
        "                ci_pct = np.nan\n",
        "        else:\n",
        "            lower = upper = point = ci_pct = np.nan\n",
        "\n",
        "        lower_vals.append(lower)\n",
        "        upper_vals.append(upper)\n",
        "        point_vals.append(point)\n",
        "        ci_percentages.append(ci_pct)\n",
        "\n",
        "    # Convert to arrays\n",
        "    lower_vals = np.array(lower_vals)\n",
        "    upper_vals = np.array(upper_vals)\n",
        "    point_vals = np.array(point_vals)\n",
        "    ci_percentages = np.array(ci_percentages)\n",
        "\n",
        "    # Plot the data\n",
        "    ax.plot(durations, point_vals,\n",
        "            'ko-', linewidth=2, markersize=6, label='Point Estimate')\n",
        "    ax.fill_between(durations,\n",
        "                    lower_vals,\n",
        "                    upper_vals,\n",
        "                    alpha=0.3, color='gray', label='90% CI')\n",
        "\n",
        "    # Calculate spacing for annotations\n",
        "    y_range = np.nanmax(upper_vals) - np.nanmin(lower_vals)\n",
        "\n",
        "    # Add annotations for each point\n",
        "    for i, (x, y_lower, y_upper, y_point, ci_pct) in enumerate(zip(\n",
        "            durations, lower_vals, upper_vals, point_vals, ci_percentages)):\n",
        "\n",
        "        if not np.isnan(y_lower):\n",
        "            # Annotate lower bound\n",
        "            ax.text(x, y_lower - 0.02 * y_range, f'{y_lower:.2f}',\n",
        "                    fontsize=8, ha='center', va='top')\n",
        "\n",
        "        if not np.isnan(y_upper):\n",
        "            # Annotate upper bound\n",
        "            ax.text(x, y_upper + 0.02 * y_range, f'{y_upper:.2f}',\n",
        "                    fontsize=8, ha='center', va='bottom')\n",
        "\n",
        "        if not np.isnan(y_point):\n",
        "            # Annotate point estimate\n",
        "            ax.text(x, y_point, f'{y_point:.2f}',\n",
        "                    fontsize=8, color='darkgray', ha='left', va='bottom',\n",
        "                    fontweight='bold')\n",
        "\n",
        "        if not np.isnan(ci_pct):\n",
        "            # Annotate CI percentage in the middle of the CI band\n",
        "            y_mid = (y_lower + y_upper) / 2\n",
        "            ax.text(x, y_mid, f'{ci_pct:.1f}%',\n",
        "                    fontsize=8, color='royalblue', ha='center', va='center',\n",
        "                    bbox=dict(boxstyle='round,pad=0.2', facecolor='white',\n",
        "                             edgecolor='none', alpha=0.7))\n",
        "\n",
        "    # Set labels and formatting\n",
        "    ax.set_xlabel('Duration (hours)', fontsize=10)\n",
        "    ax.set_ylabel('Total Precipitation (inches)', fontsize=10)\n",
        "    ax.set_title(f'{ari}-Year Event', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "    # Set x-axis to log scale\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xticks(durations)\n",
        "    ax.set_xticklabels([str(d) for d in durations])\n",
        "\n",
        "    # Adjust y-limits to accommodate annotations\n",
        "    y_min = np.nanmin(lower_vals) - 0.1 * y_range\n",
        "    y_max = np.nanmax(upper_vals) + 0.1 * y_range\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "\n",
        "plt.suptitle('Total Precipitation Depth vs Duration with Confidence Intervals\\n(CI width as % of point estimate at each duration)',\n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Also print out the actual CI percentages to verify\n",
        "print(\"\\nConfidence Interval Width as % of Point Estimate:\")\n",
        "print(\"=\"*60)\n",
        "for ari in aep_events:\n",
        "    print(f\"\\n{ari}-Year Event:\")\n",
        "    ari_data = scenario_df[scenario_df['ari'] == ari].copy()\n",
        "    durations = sorted(ari_data['duration_hrs'].unique())\n",
        "\n",
        "    for dur in durations:\n",
        "        dur_data = ari_data[ari_data['duration_hrs'] == dur]\n",
        "        lower = dur_data[dur_data['ci_level'] == 'lower']['total_depth_in'].values\n",
        "        upper = dur_data[dur_data['ci_level'] == 'upper']['total_depth_in'].values\n",
        "        point = dur_data[dur_data['ci_level'] == 'point']['total_depth_in'].values\n",
        "\n",
        "        if len(lower) > 0 and len(upper) > 0 and len(point) > 0:\n",
        "            ci_width = upper[0] - lower[0]\n",
        "            ci_pct = (ci_width / point[0]) * 100\n",
        "            print(f\"  {dur:2d} hr: {ci_pct:.1f}% (L:{lower[0]:.2f}, P:{point[0]:.2f}, U:{upper[0]:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Sample Hyetographs\n",
        "\n",
        "Show how confidence intervals affect hyetograph shapes for a few example scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyeto-plot-functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_hyetograph_with_ci(ari, duration, output_dir, save_dir=None):\n",
        "    \"\"\"\n",
        "    Plot hyetograph showing lower CI, Atlas 14 estimate, and upper CI.\n",
        "    Handles variable time intervals (hours or minutes) based on duration.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ari : int\n",
        "        Annual recurrence interval (return period)\n",
        "    duration : int\n",
        "        Storm duration in hours\n",
        "    output_dir : str\n",
        "        Directory containing hyetograph CSV files\n",
        "    save_dir : str, optional\n",
        "        Directory to save PNG files. If None, only displays plot.\n",
        "    \"\"\"\n",
        "    # Format duration string\n",
        "    if duration >= 24:\n",
        "        dur_str = f\"{int(duration/24)}day\"\n",
        "        dur_label = f\"{int(duration/24)}-day\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration)}hr\"\n",
        "        dur_label = f\"{int(duration)}-hr\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Define colors - using distinct, professional colors without transparency\n",
        "    colors = {\n",
        "        'lower': '#4472C4',   # Professional blue\n",
        "        'point': '#2C2C2C',   # Dark gray/black\n",
        "        'upper': '#C55A5A'    # Professional red\n",
        "    }\n",
        "\n",
        "    # Store data for proper ordering\n",
        "    ci_data = {}\n",
        "\n",
        "    # Read hyetographs for each CI level\n",
        "    for ci_level in ['lower', 'point', 'upper']:\n",
        "        filename = f'hyetograph_ARI_{ari}_DUR_{dur_str}_CI_{ci_level}.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Skip header lines\n",
        "        df = pd.read_csv(filepath, comment='#')\n",
        "        ci_data[ci_level] = df\n",
        "\n",
        "    # Determine time column name and units\n",
        "    if 'Time_hour' in ci_data['point'].columns:\n",
        "        time_col = 'Time_hour'\n",
        "        time_label = 'Time (hours)'\n",
        "        time_values = ci_data['point'][time_col]\n",
        "        bar_width = 0.8  # Standard width for hourly data\n",
        "    elif 'Time_min' in ci_data['point'].columns:\n",
        "        time_col = 'Time_min'\n",
        "        time_label = 'Time (minutes)'\n",
        "        time_values = ci_data['point'][time_col]\n",
        "        # Adjust bar width based on time interval\n",
        "        time_interval = time_values.iloc[1] - time_values.iloc[0] if len(time_values) > 1 else 5\n",
        "        bar_width = time_interval * 0.8  # 80% of interval width\n",
        "    else:\n",
        "        raise ValueError(\"Could not find time column (Time_hour or Time_min) in CSV\")\n",
        "\n",
        "    # Get the data arrays\n",
        "    x_time = ci_data['point'][time_col]\n",
        "    lower_precip = ci_data['lower']['Precipitation_in']\n",
        "    point_precip = ci_data['point']['Precipitation_in']\n",
        "    upper_precip = ci_data['upper']['Precipitation_in']\n",
        "\n",
        "    # Calculate the incremental heights for stacking\n",
        "    height_lower = lower_precip\n",
        "    height_point_increment = point_precip - lower_precip\n",
        "    height_upper_increment = upper_precip - point_precip\n",
        "\n",
        "    # Plot stacked bars\n",
        "    ax.bar(x_time, height_lower,\n",
        "           width=bar_width,\n",
        "           alpha=1.0,\n",
        "           color=colors['lower'],\n",
        "           label='90% Lower CI',\n",
        "           edgecolor='none')\n",
        "\n",
        "    ax.bar(x_time, height_point_increment,\n",
        "           width=bar_width,\n",
        "           bottom=height_lower,\n",
        "           alpha=1.0,\n",
        "           color=colors['point'],\n",
        "           label='NOAA Atlas 14 Estimate',\n",
        "           edgecolor='none')\n",
        "\n",
        "    ax.bar(x_time, height_upper_increment,\n",
        "           width=bar_width,\n",
        "           bottom=point_precip,\n",
        "           alpha=1.0,\n",
        "           color=colors['upper'],\n",
        "           label='90% Upper CI',\n",
        "           edgecolor='none')\n",
        "\n",
        "    ax.set_xlabel(time_label, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Incremental Precipitation (inches)', fontsize=12, fontweight='bold')\n",
        "\n",
        "    title = f'{ari}-Year {dur_label} Design Storm\\nNOAA Atlas 14 Precipitation with 90% Confidence Bounds'\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    ax.legend(fontsize=10, loc='upper right', framealpha=0.95)\n",
        "    ax.grid(axis='y', alpha=0.3, zorder=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if directory specified\n",
        "    if save_dir is not None:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        filename = f'{ari}yr_{dur_str}_storm_CI.png'\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Hyetograph plotting functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYETOGRAPH VISUALIZATION WITH CONFIDENCE INTERVALS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"HYETOGRAPH VISUALIZATION WITH CONFIDENCE INTERVALS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Generate ALL hyetographs and save to folder (without displaying)\n",
        "print(\"\\nGenerating and saving all hyetograph plots...\")\n",
        "print(\"(Only displaying 10-Year 6-Hour and 100-Year 24-Hour storms)\\n\")\n",
        "\n",
        "# Define all scenarios\n",
        "ari_values = [2, 5, 10, 25, 50, 100]\n",
        "duration_values = [1, 2, 3, 6, 12, 24, 48]  # 48 = 2 days\n",
        "save_dir = \"Atlas 14 Storm Hyetographs\"\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "total_plots = len(ari_values) * len(duration_values)\n",
        "current = 0\n",
        "generated_count = 0\n",
        "\n",
        "for ari in ari_values:\n",
        "    for duration in duration_values:\n",
        "        current += 1\n",
        "\n",
        "        # Format duration string\n",
        "        if duration >= 24:\n",
        "            dur_str = f\"{int(duration/24)}day\"\n",
        "        else:\n",
        "            dur_str = f\"{int(duration)}hr\"\n",
        "\n",
        "        # Check if files exist\n",
        "        filename = f'hyetograph_ARI_{ari}_DUR_{dur_str}_CI_point.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"[{current}/{total_plots}] Skipping {ari}-yr, {dur_str} - files not found\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Only DISPLAY these two specific plots\n",
        "            if (ari == 10 and duration == 6) or (ari == 100 and duration == 24):\n",
        "                print(f\"\\n[{current}/{total_plots}] Displaying {ari}-yr, {dur_str} storm...\")\n",
        "                plot_hyetograph_with_ci(ari, duration, output_dir, save_dir)\n",
        "            else:\n",
        "                # Generate and save without displaying\n",
        "                print(f\"[{current}/{total_plots}] Generating {ari}-yr, {dur_str} storm...\", end='')\n",
        "\n",
        "                # Call the function but close the plot immediately to avoid display\n",
        "                import matplotlib\n",
        "                matplotlib.use('Agg')  # Use non-interactive backend\n",
        "                plot_hyetograph_with_ci(ari, duration, output_dir, save_dir)\n",
        "                plt.close('all')  # Close all figures\n",
        "                matplotlib.use('module://matplotlib_inline.backend_inline')  # Restore interactive backend\n",
        "\n",
        "                print(\" saved.\")\n",
        "\n",
        "            generated_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(f\"COMPLETE\")\n",
        "print(f\"{'=' * 70}\")\n",
        "print(f\"\u2713 Generated and saved {generated_count} hyetograph plots to: {save_dir}\")\n",
        "print(f\"\u2713 Displayed 2 key scenarios (10-Year 6-Hour and 100-Year 24-Hour)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Total Depths Across Scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyetograph visualization completed in cell above\n",
        "# All hyetograph plots have been generated and saved to \"Atlas 14 Storm Hyetographs\" directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize HEC-RAS Project and Create Plans\n",
        "\n",
        "Now we'll create a plan for each scenario in our matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plan Creation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_simulation_duration_from_plan(plan_number, project):\n",
        "    \"\"\"\n",
        "    Get the simulation duration in hours from a plan file.\n",
        "\n",
        "    Parameters:\n",
        "    - plan_number: Plan number\n",
        "    - project: RAS project object\n",
        "\n",
        "    Returns:\n",
        "    - tuple: (start_datetime, end_datetime, duration_hours)\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=project)\n",
        "\n",
        "    with open(plan_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Find Simulation Date line\n",
        "    for line in lines:\n",
        "        if line.startswith('Simulation Date='):\n",
        "            # Format: Simulation Date=10JAN2000,1200,11JAN2000,2400\n",
        "            parts = line.strip().split('=')[1].split(',')\n",
        "\n",
        "            start_date = parts[0]\n",
        "            start_time = parts[1]\n",
        "            end_date = parts[2]\n",
        "            end_time = parts[3]\n",
        "\n",
        "            # Parse dates (format: DDMMMYYYY, time: HHMM)\n",
        "            start_dt = datetime.strptime(f\"{start_date}{start_time}\", \"%d%b%Y%H%M\")\n",
        "            \n",
        "            # Handle special case: HEC-RAS uses \"2400\" to mean midnight (end of day)\n",
        "            if end_time == \"2400\":\n",
        "                end_dt = datetime.strptime(f\"{end_date}0000\", \"%d%b%Y%H%M\")\n",
        "                end_dt = end_dt + timedelta(days=1)\n",
        "            else:\n",
        "                end_dt = datetime.strptime(f\"{end_date}{end_time}\", \"%d%b%Y%H%M\")\n",
        "\n",
        "            # Calculate duration in hours\n",
        "            duration_hours = (end_dt - start_dt).total_seconds() / 3600\n",
        "\n",
        "            return start_dt, end_dt, duration_hours\n",
        "\n",
        "    # Default fallback\n",
        "    return None, None, 24.0\n",
        "\n",
        "\n",
        "def format_interval_for_hecras(interval_hours):\n",
        "    \"\"\"\n",
        "    Convert interval in hours to HEC-RAS format string.\n",
        "\n",
        "    Parameters:\n",
        "    - interval_hours: Time interval in hours (e.g., 0.0833 for 5 minutes)\n",
        "\n",
        "    Returns:\n",
        "    - str: HEC-RAS format (e.g., \"5MIN\", \"15MIN\", \"1HOUR\")\n",
        "    \"\"\"\n",
        "    interval_minutes = interval_hours * 60\n",
        "\n",
        "    if interval_minutes < 1.0:\n",
        "        # Less than 1 minute - shouldn't happen but handle it\n",
        "        seconds = interval_minutes * 60\n",
        "        return f\"{int(seconds)}SEC\"\n",
        "    elif interval_minutes < 60:\n",
        "        # Minutes\n",
        "        return f\"{int(interval_minutes)}MIN\"\n",
        "    else:\n",
        "        # Hours\n",
        "        hours = interval_hours\n",
        "        if hours == int(hours):\n",
        "            return f\"{int(hours)}HOUR\"\n",
        "        else:\n",
        "            # Fractional hours - convert to minutes\n",
        "            return f\"{int(interval_minutes)}MIN\"\n",
        "\n",
        "\n",
        "def modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, plan_number=None, project=None):\n",
        "    \"\"\"\n",
        "    Modifies an unsteady flow file to incorporate hyetograph data as precipitation.\n",
        "    Handles time intervals and pads the hyetograph to cover the full simulation window.\n",
        "\n",
        "    Parameters:\n",
        "    - unsteady_file_path: Path to the unsteady flow file\n",
        "    - hyetograph_file: Path to the hyetograph data CSV\n",
        "    - plan_number: Plan number (for getting simulation duration)\n",
        "    - project: RAS project object\n",
        "\n",
        "    Returns:\n",
        "    - Boolean indicating success\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the hyetograph data (skip comment lines)\n",
        "        hyetograph_df = pd.read_csv(hyetograph_file, comment='#')\n",
        "\n",
        "        # Get the time interval from the hyetograph\n",
        "        if 'Time_hour' in hyetograph_df.columns:\n",
        "            time_values = hyetograph_df['Time_hour'].values\n",
        "        elif 'Time_min' in hyetograph_df.columns:\n",
        "            time_values = hyetograph_df['Time_min'].values / 60.0  # Convert to hours\n",
        "        else:\n",
        "            raise ValueError(\"Could not find time column in hyetograph file\")\n",
        "\n",
        "        # Calculate interval\n",
        "        if len(time_values) > 1:\n",
        "            interval_hours = time_values[1] - time_values[0]\n",
        "        else:\n",
        "            interval_hours = time_values[0]\n",
        "\n",
        "        # Get storm duration in hours\n",
        "        storm_duration_hours = time_values[-1]\n",
        "\n",
        "        # Get simulation duration from plan file\n",
        "        if plan_number and project:\n",
        "            start_dt, end_dt, sim_duration_hours = get_simulation_duration_from_plan(plan_number, project)\n",
        "        else:\n",
        "            # Default: assume 36-hour simulation\n",
        "            sim_duration_hours = 36.0\n",
        "\n",
        "        print(f\"  Storm duration: {storm_duration_hours:.2f} hours\")\n",
        "        print(f\"  Time interval: {interval_hours*60:.1f} minutes\")\n",
        "        print(f\"  Simulation duration: {sim_duration_hours:.1f} hours\")\n",
        "\n",
        "        # Calculate total number of intervals needed\n",
        "        total_intervals = int(sim_duration_hours / interval_hours)\n",
        "\n",
        "        # Get precipitation values from hyetograph\n",
        "        precip_values = hyetograph_df[\"Precipitation_in\"].values\n",
        "\n",
        "        # Calculate where to place the storm (at the beginning of simulation)\n",
        "        warmup_hours = 0.0\n",
        "        warmup_intervals = int(warmup_hours / interval_hours)\n",
        "\n",
        "        # Create full precipitation array with zeros\n",
        "        full_precip = np.zeros(total_intervals)\n",
        "\n",
        "        # Insert storm values after warmup period\n",
        "        storm_intervals = len(precip_values)\n",
        "        if warmup_intervals + storm_intervals <= total_intervals:\n",
        "            full_precip[warmup_intervals:warmup_intervals + storm_intervals] = precip_values\n",
        "        else:\n",
        "            # If storm doesn't fit, place at start\n",
        "            end_idx = min(storm_intervals, total_intervals)\n",
        "            full_precip[0:end_idx] = precip_values[0:end_idx]\n",
        "\n",
        "        print(f\"  Total intervals in simulation: {total_intervals}\")\n",
        "        print(f\"  Storm placed at interval {warmup_intervals} (at simulation start)\")\n",
        "\n",
        "        # Read the unsteady flow file\n",
        "        with open(unsteady_file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        # Find the Interval line and update it\n",
        "        interval_line_idx = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Interval=\"):\n",
        "                interval_line_idx = i\n",
        "                break\n",
        "\n",
        "        if interval_line_idx is not None:\n",
        "            # Update the interval\n",
        "            interval_str = format_interval_for_hecras(interval_hours)\n",
        "            lines[interval_line_idx] = f\"Interval={interval_str}\\n\"\n",
        "            print(f\"  Updated Interval to: {interval_str}\")\n",
        "\n",
        "        # Find the Precipitation Hydrograph section\n",
        "        precip_hydrograph_index = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Precipitation Hydrograph=\"):\n",
        "                precip_hydrograph_index = i\n",
        "                break\n",
        "\n",
        "        if precip_hydrograph_index is None:\n",
        "            print(\"Cannot find Precipitation Hydrograph section in unsteady file.\")\n",
        "            return False\n",
        "\n",
        "        # Create the Precipitation Hydrograph line\n",
        "        precip_line = f\"Precipitation Hydrograph= {len(full_precip)} \\n\"\n",
        "\n",
        "        # Format the values in groups of 10 per line\n",
        "        value_lines = []\n",
        "        for i in range(0, len(full_precip), 10):\n",
        "            row_values = full_precip[i:i+10]\n",
        "            row_line = \"\".join([f\"{value:8.2f}\" for value in row_values]) + \"\\n\"\n",
        "            value_lines.append(row_line)\n",
        "\n",
        "        # Find end of current hydrograph\n",
        "        current_line = precip_hydrograph_index + 1\n",
        "        end_markers = [\"DSS Path=\", \"Use DSS=\", \"Use Fixed Start Time=\"]\n",
        "        while current_line < len(lines) and not any(lines[current_line].startswith(marker) for marker in end_markers):\n",
        "            current_line += 1\n",
        "\n",
        "        # Replace the hydrograph section\n",
        "        lines[precip_hydrograph_index:current_line] = [precip_line] + value_lines\n",
        "\n",
        "        # Write the modified file back\n",
        "        with open(unsteady_file_path, 'w') as file:\n",
        "            file.writelines(lines)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error modifying unsteady flow file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "\n",
        "def create_plan_for_scenario(scenario_row, base_plan, project):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a specific scenario.\n",
        "\n",
        "    Parameters:\n",
        "    - scenario_row: Row from scenario DataFrame\n",
        "    - base_plan: Base plan number to clone from\n",
        "    - project: RAS project object\n",
        "\n",
        "    Returns:\n",
        "    - new_plan_number\n",
        "    \"\"\"\n",
        "    ari = scenario_row['ari']\n",
        "    duration = scenario_row['duration_hrs']\n",
        "    ci_level = scenario_row['ci_level']\n",
        "    hyetograph_file = scenario_row['hyetograph_file']\n",
        "\n",
        "    # Format duration string\n",
        "    if duration >= 24:\n",
        "        dur_str = f\"{int(duration/24)}D\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration)}H\"\n",
        "\n",
        "    # Create plan name (keep it short for HEC-RAS)\n",
        "    ci_abbrev = ci_level[0].upper()  # L, P, or U\n",
        "    plan_name = f\"{ari}YR-{dur_str}-{ci_abbrev}\"\n",
        "\n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_shortid=plan_name, ras_object=project)\n",
        "\n",
        "    # Get unsteady number from base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            break\n",
        "\n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    # Clone the unsteady flow file\n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "\n",
        "    # Get unsteady file path\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "\n",
        "    # Update the flow title\n",
        "    new_title = f\"{ari}YR-{dur_str}-{ci_level.upper()} Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "\n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    # Pass plan number and project so we can get simulation duration\n",
        "    hyetograph_file_abs = Path(hyetograph_file).absolute()\n",
        "    success = modify_unsteady_flow_with_hyetograph(\n",
        "        unsteady_file_path,\n",
        "        hyetograph_file_abs,\n",
        "        plan_number=new_plan_number,\n",
        "        project=project\n",
        "    )\n",
        "    if not success:\n",
        "        raise RuntimeError(f\"Failed to apply hyetograph data to plan {new_plan_number}\")\n",
        "\n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "\n",
        "    return new_plan_number\n",
        "\n",
        "print(\"Plan creation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling the 99-Plan Limit in HEC-RAS\n",
        "\n",
        "**IMPORTANT**: HEC-RAS projects can only contain a maximum of 99 plans. Since this analysis requires 126 scenarios (6 AEPs \u00d7 7 durations \u00d7 3 confidence levels), we need to distribute scenarios across multiple project copies.\n",
        "\n",
        "**Solution Approach:**\n",
        "1. **Automatic Project Distribution**: The `distribute_scenarios_across_projects()` function automatically:\n",
        "   - Checks existing plan count in the base project\n",
        "   - Calculates available slots (99 - existing plans)\n",
        "   - Creates multiple project copies as needed\n",
        "   - Groups scenarios logically (by AEP) to keep related runs together\n",
        "   \n",
        "2. **Project Naming**: New projects are created with meaningful names:\n",
        "   - Format: `{project_name}{suffix}_{number}`\n",
        "   - Example: `Davis_atlas14_01`, `Davis_atlas14_02`, etc.\n",
        "   - The suffix is user-configurable (default: `_atlas14`)\n",
        "\n",
        "3. **Plan Descriptions**: Each plan's description includes:\n",
        "   - AEP (return period)\n",
        "   - Storm duration\n",
        "   - Confidence level\n",
        "   - Analysis set identifier (the folder suffix)\n",
        "   \n",
        "4. **Original Project Preservation**: The base project is never modified - all new plans are created in project copies\n",
        "\n",
        "**Configuration Options:**\n",
        "- `folder_suffix`: Change the suffix added to project folders (default: `\"_atlas14\"`)\n",
        "- Modify the grouping strategy in `distribute_scenarios_across_projects()` if needed\n",
        "  (currently groups by AEP to keep related scenarios together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Multi-Project Management Functions for 99-Plan Limit\n",
        "\n",
        "import shutil\n",
        "\n",
        "def copy_ras_project(source_folder, dest_folder, suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Copy a HEC-RAS project to a new folder.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    source_folder : Path or str\n",
        "        Source project folder\n",
        "    dest_folder : Path or str\n",
        "        Destination folder\n",
        "    suffix : str, optional\n",
        "        Suffix to add to project name\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Path : Path to destination folder\n",
        "    \"\"\"\n",
        "    source_folder = Path(source_folder)\n",
        "    dest_folder = Path(dest_folder)\n",
        "\n",
        "    # Create destination\n",
        "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Copy all files\n",
        "    for item in source_folder.iterdir():\n",
        "        if item.is_file():\n",
        "            shutil.copy2(item, dest_folder / item.name)\n",
        "        elif item.is_dir() and item.name not in ['compute_uncertainty', '.ipynb_checkpoints']:\n",
        "            shutil.copytree(item, dest_folder / item.name, dirs_exist_ok=True)\n",
        "\n",
        "    print(f\"Copied project to: {dest_folder}\")\n",
        "    return dest_folder\n",
        "\n",
        "\n",
        "def distribute_scenarios_across_projects(scenario_df, base_project_path, base_plan=\"02\",\n",
        "                                         folder_suffix=\"_atlas14\", ras_version=\"6.6\"):\n",
        "    \"\"\"\n",
        "    Distribute scenarios across multiple HEC-RAS project copies to handle 99-plan limit.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    scenario_df : DataFrame\n",
        "        Scenarios to process\n",
        "    base_project_path : Path or str\n",
        "        Original project path\n",
        "    base_plan : str\n",
        "        Base plan number to clone from\n",
        "    folder_suffix : str\n",
        "        Suffix for project folders (default: \"_atlas14\")\n",
        "    ras_version : str\n",
        "        HEC-RAS version\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame : scenario_df with added columns: project_folder, project_object, plan_number\n",
        "    \"\"\"\n",
        "    base_project_path = Path(base_project_path)\n",
        "\n",
        "    # Check base project existing plans\n",
        "    temp_proj = RasPrj()\n",
        "    init_ras_project(base_project_path, ras_version, ras_object=temp_proj)\n",
        "    existing_plans_in_base = len(temp_proj.plan_df)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DISTRIBUTING SCENARIOS ACROSS PROJECTS (99-PLAN LIMIT)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBase project has {existing_plans_in_base} existing plans\")\n",
        "    print(f\"Maximum plans per project: 99\")\n",
        "    print(f\"Total scenarios to distribute: {len(scenario_df)}\\n\")\n",
        "\n",
        "    # Calculate how many scenarios we can fit per project\n",
        "    max_scenarios_per_project = 99 - existing_plans_in_base\n",
        "\n",
        "    # Group scenarios by ARI to keep related scenarios together\n",
        "    ari_groups = scenario_df.groupby('ari')\n",
        "\n",
        "    results_list = []\n",
        "    current_project_num = 1\n",
        "    current_project_path = None\n",
        "    current_project_obj = None\n",
        "    current_project_plan_count = 0\n",
        "\n",
        "    for ari, ari_df in ari_groups:\n",
        "        num_scenarios_in_group = len(ari_df)\n",
        "\n",
        "        print(f\"Processing {ari}-year AEP ({num_scenarios_in_group} scenarios)...\")\n",
        "\n",
        "        # Check if we need to create a new project\n",
        "        if current_project_obj is None or (current_project_plan_count + num_scenarios_in_group) > max_scenarios_per_project:\n",
        "            # Create a new project\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Creating Project #{current_project_num}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Create project folder with meaningful name\n",
        "            project_folder_name = f\"{base_project_path.name}{folder_suffix}_{current_project_num:02d}\"\n",
        "            current_project_path = base_project_path.parent / project_folder_name\n",
        "\n",
        "            # Copy the project\n",
        "            copy_ras_project(base_project_path, current_project_path)\n",
        "\n",
        "            # Initialize the project\n",
        "            current_project_obj = RasPrj()\n",
        "            init_ras_project(current_project_path, ras_version, ras_object=current_project_obj)\n",
        "\n",
        "            # Reset counter to existing plans in the new copy\n",
        "            current_project_plan_count = len(current_project_obj.plan_df)\n",
        "            print(f\"Project initialized with {current_project_plan_count} existing plans\")\n",
        "            print(f\"Available slots: {99 - current_project_plan_count}\")\n",
        "\n",
        "            current_project_num += 1\n",
        "\n",
        "        # Add all scenarios from this ARI group to the current project\n",
        "        for idx, scenario in ari_df.iterrows():\n",
        "            scenario['project_folder'] = str(current_project_path)\n",
        "            scenario['project_object'] = current_project_obj\n",
        "            scenario['project_suffix'] = folder_suffix\n",
        "            results_list.append(scenario)\n",
        "            current_project_plan_count += 1\n",
        "\n",
        "        print(f\"  Added {num_scenarios_in_group} scenarios to project {current_project_path.name}\")\n",
        "        print(f\"  Current plan count: {current_project_plan_count}/{99}\")\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"DISTRIBUTION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total scenarios: {len(results_df)}\")\n",
        "    print(f\"Projects created: {current_project_num - 1}\")\n",
        "    print(f\"\\nScenarios per project:\")\n",
        "    for proj_folder in results_df['project_folder'].unique():\n",
        "        count = len(results_df[results_df['project_folder'] == proj_folder])\n",
        "        print(f\"  {Path(proj_folder).name}: {count} scenarios\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def create_plan_with_description(scenario_row, base_plan, project, folder_suffix):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a scenario with description including folder suffix.\n",
        "\n",
        "    Parameters:\n",
        "    - scenario_row: Row from scenario DataFrame\n",
        "    - base_plan: Base plan number to clone from\n",
        "    - project: RAS project object\n",
        "    - folder_suffix: Folder suffix to add to description\n",
        "\n",
        "    Returns:\n",
        "    - new_plan_number\n",
        "    \"\"\"\n",
        "    ari = scenario_row['ari']\n",
        "    duration = scenario_row['duration_hrs']\n",
        "    ci_level = scenario_row['ci_level']\n",
        "    hyetograph_file = scenario_row['hyetograph_file']\n",
        "\n",
        "    # Format duration string\n",
        "    if duration >= 24:\n",
        "        dur_str = f\"{int(duration/24)}D\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration)}H\"\n",
        "\n",
        "    # Create plan name (keep it short for HEC-RAS)\n",
        "    ci_abbrev = ci_level[0].upper()  # L, P, or U\n",
        "    plan_name = f\"{ari}YR-{dur_str}-{ci_abbrev}\"\n",
        "\n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_shortid=plan_name, ras_object=project)\n",
        "\n",
        "    # Update description with folder suffix\n",
        "    description_text = f\"Atlas 14 Uncertainty Analysis\\n\"\n",
        "    description_text += f\"AEP: {ari} years\\n\"\n",
        "    description_text += f\"Duration: {duration} hours\\n\"\n",
        "    description_text += f\"Confidence Level: {ci_level}\\n\"\n",
        "    description_text += f\"Analysis Set: {folder_suffix}\"\n",
        "\n",
        "    #RasPlan.update_plan_description(new_plan_number, description_text, ras_object=project)\n",
        "\n",
        "    # Get unsteady number from base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            break\n",
        "\n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    # Clone the unsteady flow file\n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "\n",
        "    # Get unsteady file path\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "\n",
        "    # Update the flow title\n",
        "    new_title = f\"{ari}YR-{dur_str}-{ci_level.upper()} Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "\n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    # Need to ensure hyetograph path is accessible from new project location\n",
        "    hyetograph_file_abs = Path(hyetograph_file).absolute()\n",
        "    success = modify_unsteady_flow_with_hyetograph(\n",
        "        unsteady_file_path,\n",
        "        hyetograph_file_abs,\n",
        "        plan_number=new_plan_number,\n",
        "        project=project\n",
        "    )\n",
        "    if not success:\n",
        "        raise RuntimeError(f\"Failed to apply hyetograph data to plan {new_plan_number}\")\n",
        "\n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "\n",
        "    return new_plan_number\n",
        "\n",
        "\n",
        "print(\"Multi-project management functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribute scenarios across projects and create plans\n",
        "print(\"=\"*70)\n",
        "print(\"CREATING PLANS WITH 99-PLAN LIMIT HANDLING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal scenarios to process: {len(scenario_df)}\")\n",
        "print(f\"Base project: {pipes_ex_path}\\n\")\n",
        "\n",
        "# Distribute scenarios across multiple projects as needed\n",
        "scenario_df_distributed = distribute_scenarios_across_projects(\n",
        "    scenario_df, \n",
        "    base_project_path=pipes_ex_path,\n",
        "    base_plan=base_plan,\n",
        "    folder_suffix=\"_atlas14\",\n",
        "    ras_version=\"6.6\"\n",
        ")\n",
        "\n",
        "# Now create plans in each project\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CREATING PLANS IN DISTRIBUTED PROJECTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "plan_numbers = []\n",
        "failed_scenarios = []\n",
        "\n",
        "# Group by project\n",
        "project_groups = scenario_df_distributed.groupby('project_folder')\n",
        "\n",
        "for proj_folder, proj_scenarios in project_groups:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing Project: {Path(proj_folder).name}\")\n",
        "    print(f\"Scenarios: {len(proj_scenarios)}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Get the project object (they should all be the same within a group)\n",
        "    proj_obj = proj_scenarios.iloc[0]['project_object']\n",
        "    folder_suffix = proj_scenarios.iloc[0]['project_suffix']\n",
        "    \n",
        "    for idx, row in proj_scenarios.iterrows():\n",
        "        try:\n",
        "            new_plan = create_plan_with_description(row, base_plan, proj_obj, folder_suffix)\n",
        "            plan_numbers.append(new_plan)\n",
        "            \n",
        "            # Update the row with the plan number\n",
        "            scenario_df_distributed.at[idx, 'plan_number'] = new_plan\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error creating plan for scenario {idx}: {e}\")\n",
        "            failed_scenarios.append(idx)\n",
        "            plan_numbers.append(None)\n",
        "            scenario_df_distributed.at[idx, 'plan_number'] = None\n",
        "    \n",
        "    # Show progress\n",
        "    successful = len([p for p in plan_numbers if p is not None])\n",
        "    print(f\"\\nProject {Path(proj_folder).name}: Created {len(proj_scenarios)} plans\")\n",
        "\n",
        "# Remove failed scenarios\n",
        "scenario_df = scenario_df_distributed[scenario_df_distributed['plan_number'].notna()].copy()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PLAN CREATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\u2713 Successfully created {len(plan_numbers) - len(failed_scenarios)} plans\")\n",
        "if failed_scenarios:\n",
        "    print(f\"\u2717 Failed to create {len(failed_scenarios)} plans\")\n",
        "\n",
        "print(f\"\\nPlan Summary (first 15 rows):\")\n",
        "display.display(scenario_df[['ari', 'duration_hrs', 'ci_level', 'plan_number', 'project_folder']].head(15))\n",
        "\n",
        "# Save updated scenario list\n",
        "scenario_df.to_csv('scenarios_with_plans.csv', index=False)\n",
        "print(f\"\\nScenario list with plan numbers saved to: scenarios_with_plans.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set computation parameters for all new plans across all projects\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURING COMPUTATION PARAMETERS ACROSS ALL PROJECTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Group by project\n",
        "project_groups = scenario_df.groupby('project_folder')\n",
        "\n",
        "total_configured = 0\n",
        "\n",
        "for proj_folder, proj_scenarios in project_groups:\n",
        "    print(f\"\\nConfiguring {len(proj_scenarios)} plans in: {Path(proj_folder).name}\")\n",
        "    \n",
        "    # Get the project object\n",
        "    proj_obj = proj_scenarios.iloc[0]['project_object']\n",
        "    \n",
        "    for idx, row in proj_scenarios.iterrows():\n",
        "        plan_number = row['plan_number']\n",
        "        \n",
        "        try:\n",
        "            RasPlan.set_num_cores(plan_number, 2, ras_object=proj_obj)\n",
        "#            RasPlan.update_plan_intervals(\n",
        "#                plan_number,\n",
        "#                computation_interval=\"15MIN\",\n",
        "#                output_interval=\"30MIN\",\n",
        "#                mapping_interval=\"1HOUR\",\n",
        "#                ras_object=proj_obj\n",
        "#            )\n",
        "            total_configured += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error configuring plan {plan_number}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"CONFIGURATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\u2713 Configured {total_configured} plans across {len(project_groups)} projects\")\n",
        "print(\"  - Cores per plan: 2\")\n",
        "#print(\"  - Computation interval: 15 MIN\")\n",
        "#print(\"  - Output interval: 30 MIN\")\n",
        "#print(\"  - Mapping interval: 1 HOUR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_scenarios_in_batches(scenario_df, batch_size=20, max_workers=4, cores_per_worker=2, ras_object=None):\n",
        "    \"\"\"\n",
        "    Execute scenarios in batches with progress tracking.\n",
        "    \n",
        "    Parameters:\n",
        "    - scenario_df: DataFrame with scenario information\n",
        "    - batch_size: Number of plans per batch\n",
        "    - max_workers: Number of parallel workers\n",
        "    - cores_per_worker: Cores per worker\n",
        "    - ras_object: RAS project object (uses global 'ras' if None)\n",
        "    \n",
        "    Returns:\n",
        "    - results_dict: Dictionary of execution results\n",
        "    - compute_folder: Path to compute folder\n",
        "    \"\"\"\n",
        "    # Use provided ras_object or try to get from DataFrame\n",
        "    if ras_object is None:\n",
        "        if 'project_object' in scenario_df.columns:\n",
        "            ras_obj = scenario_df.iloc[0]['project_object']\n",
        "        else:\n",
        "            ras_obj = ras  # Fall back to global\n",
        "    else:\n",
        "        ras_obj = ras_object\n",
        "    \n",
        "    # Create compute folder\n",
        "    compute_folder = Path(ras_obj.project_folder) / \"compute_uncertainty\"\n",
        "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Get all plan numbers\n",
        "    all_plans = scenario_df['plan_number'].tolist()\n",
        "    \n",
        "    # Split into batches\n",
        "    batches = [all_plans[i:i+batch_size] for i in range(0, len(all_plans), batch_size)]\n",
        "    \n",
        "    print(f\"Executing {len(all_plans)} plans in {len(batches)} batches\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Max workers: {max_workers}\")\n",
        "    print(f\"Cores per worker: {cores_per_worker}\")\n",
        "    print(f\"Compute folder: {compute_folder}\\n\")\n",
        "    \n",
        "    all_results = {}\n",
        "    overall_start = time.time()\n",
        "\n",
        "    from ras_commander import RasPlan  # Make sure RasPlan is imported\n",
        "\n",
        "    for batch_idx, batch_plans in enumerate(batches, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Batch {batch_idx}/{len(batches)}: Processing {len(batch_plans)} plans\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Set plan title for each plan in the batch, showing scenario information from scenario_df\n",
        "        for plan_number in batch_plans:\n",
        "            # Find the scenario row for this plan number\n",
        "            scenario_row = scenario_df.loc[scenario_df['plan_number'] == plan_number].squeeze()\n",
        "            # Compose a scenario title string\n",
        "            # This can be customized as needed. Here, include ARI, Duration, Project (if columns exist).\n",
        "            scenario_title_parts = []\n",
        "            for field in ['ARI', 'Duration', 'project_folder']:\n",
        "                if field in scenario_row:\n",
        "                    value = scenario_row[field]\n",
        "                    if pd.notna(value):\n",
        "                        scenario_title_parts.append(f\"{field}:{value}\")\n",
        "            # Join up to 32 chars (RAS limit)\n",
        "            scenario_title = \" | \".join(scenario_title_parts)[:32] or f\"Plan {plan_number}\"\n",
        "            # Set plan title using RasPlan\n",
        "            RasPlan.set_plan_title(plan_number, scenario_title, ras_object=ras_obj)\n",
        "\n",
        "        batch_start = time.time()\n",
        "        \n",
        "        # Execute batch\n",
        "        results = RasCmdr.compute_parallel(\n",
        "            plan_number=batch_plans,\n",
        "            max_workers=max_workers,\n",
        "            num_cores=cores_per_worker,\n",
        "            dest_folder=compute_folder,\n",
        "            clear_geompre=True,\n",
        "            overwrite_dest=True,\n",
        "            ras_object=ras_obj\n",
        "        )\n",
        "        \n",
        "        batch_duration = time.time() - batch_start\n",
        "        \n",
        "        # Update results\n",
        "        all_results.update(results)\n",
        "        \n",
        "        # Report batch results\n",
        "        success_count = sum(1 for v in results.values() if v)\n",
        "        print(f\"\\nBatch {batch_idx} completed in {batch_duration:.1f} seconds\")\n",
        "        print(f\"Success: {success_count}/{len(batch_plans)}\")\n",
        "        \n",
        "        # Progress summary\n",
        "        total_completed = batch_idx * batch_size\n",
        "        if total_completed > len(all_plans):\n",
        "            total_completed = len(all_plans)\n",
        "        print(f\"Overall progress: {total_completed}/{len(all_plans)} plans completed\")\n",
        "    \n",
        "    overall_duration = time.time() - overall_start\n",
        "\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ALL BATCHES COMPLETED\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total time: {overall_duration/60:.1f} minutes\")\n",
        "    print(f\"Average per plan: {overall_duration/len(all_plans):.1f} seconds\")\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame([\n",
        "        {\"plan_number\": plan, \"success\": success}\n",
        "        for plan, success in all_results.items()\n",
        "    ])\n",
        "    \n",
        "    success_rate = (results_df['success'].sum() / len(results_df)) * 100\n",
        "    print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
        "    print(f\"Successful: {results_df['success'].sum()}\")\n",
        "    print(f\"Failed: {(~results_df['success']).sum()}\")\n",
        "    \n",
        "    return all_results, compute_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute scenarios across all projects\n",
        "print(\"=\"*70)\n",
        "print(\"EXECUTING SCENARIOS ACROSS ALL PROJECTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_results = {}\n",
        "all_compute_folders = {}\n",
        "\n",
        "# Group by project\n",
        "project_groups = scenario_df.groupby('project_folder')\n",
        "\n",
        "for proj_folder, proj_scenarios in project_groups:\n",
        "    proj_name = Path(proj_folder).name\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing Project: {proj_name}\")\n",
        "    print(f\"Scenarios: {len(proj_scenarios)}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Get the project object\n",
        "    proj_obj = proj_scenarios.iloc[0]['project_object']\n",
        "    \n",
        "    # Execute scenarios for this project in batches\n",
        "    results, compute_folder = execute_scenarios_in_batches(\n",
        "        proj_scenarios,\n",
        "        batch_size=21,  # 3 CI levels \u00d7 7 durations = 21 plans per ARI\n",
        "        max_workers=4,\n",
        "        cores_per_worker=2\n",
        "    )\n",
        "    \n",
        "    # Store results and compute folder for later processing\n",
        "    all_results.update(results)\n",
        "    all_compute_folders[proj_folder] = compute_folder\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ALL PROJECTS EXECUTION COMPLETE\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total scenarios executed: {len(all_results)}\")\n",
        "print(f\"Projects: {len(all_compute_folders)}\")\n",
        "\n",
        "# Save compute folder mapping\n",
        "compute_folders_df = pd.DataFrame([\n",
        "    {'project_folder': pf, 'compute_folder': str(cf)}\n",
        "    for pf, cf in all_compute_folders.items()\n",
        "])\n",
        "compute_folders_df.to_csv('compute_folders.csv', index=False)\n",
        "print(f\"\\nCompute folder mapping saved to: compute_folders.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize compute projects and extract results from all projects\n",
        "print(\"=\"*70)\n",
        "print(\"EXTRACTING RESULTS FROM ALL PROJECTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_plans_with_results = []\n",
        "\n",
        "# Process each project's compute folder\n",
        "for proj_folder, compute_folder in all_compute_folders.items():\n",
        "    proj_name = Path(proj_folder).name\n",
        "    print(f\"\\nProcessing results from: {proj_name}\")\n",
        "    print(f\"Compute folder: {compute_folder}\")\n",
        "\n",
        "    # Initialize compute project to access results\n",
        "    compute_project = RasPrj()\n",
        "    compute_project = init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "\n",
        "    # Check which plans have results\n",
        "    plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()].copy()\n",
        "    plans_with_results['source_project'] = proj_folder\n",
        "    plans_with_results['compute_folder'] = str(compute_folder)\n",
        "\n",
        "    all_plans_with_results.append(plans_with_results)\n",
        "\n",
        "    print(f\"  Found {len(plans_with_results)} plans with HDF results\")\n",
        "\n",
        "# Combine all results\n",
        "if len(all_plans_with_results) > 0:\n",
        "    combined_results_df = pd.concat(all_plans_with_results, ignore_index=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"RESULTS EXTRACTION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total plans with results: {len(combined_results_df)}\")\n",
        "\n",
        "    # Debug: Show what columns we have before merge\n",
        "    print(f\"\\nColumns in combined_results_df: {list(combined_results_df.columns)}\")\n",
        "    print(f\"Columns in scenario_df: {list(scenario_df.columns)}\")\n",
        "\n",
        "    # Merge with scenario information\n",
        "    results_df = scenario_df.merge(\n",
        "        combined_results_df[['plan_number', 'Short Identifier', 'HDF_Results_Path', 'source_project', 'compute_folder']],\n",
        "        on='plan_number',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nAfter merge:\")\n",
        "    print(f\"  Total rows in results_df: {len(results_df)}\")\n",
        "    print(f\"  Rows with HDF paths: {len(results_df[results_df['HDF_Results_Path'].notna()])}\")\n",
        "    print(f\"  Columns in results_df: {list(results_df.columns)}\")\n",
        "\n",
        "    # Check if scenario info is preserved\n",
        "    if 'ari' in results_df.columns:\n",
        "        print(f\"  \u2713 'ari' column preserved\")\n",
        "    else:\n",
        "        print(f\"  \u2717 'ari' column MISSING!\")\n",
        "\n",
        "    if 'duration_hrs' in results_df.columns:\n",
        "        print(f\"  \u2713 'duration_hrs' column preserved\")\n",
        "    else:\n",
        "        print(f\"  \u2717 'duration_hrs' column MISSING!\")\n",
        "\n",
        "    print(f\"\\nResults summary (first 15 rows):\")\n",
        "    display.display(results_df[['ari', 'duration_hrs', 'ci_level', 'plan_number', 'HDF_Results_Path']].head(15))\n",
        "\n",
        "    # Save results summary\n",
        "    results_df.to_csv('results_uncertainty_scenarios.csv', index=False)\n",
        "    print(f\"\\nResults summary saved to: results_uncertainty_scenarios.csv\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: No plans with results found!\")\n",
        "    results_df = scenario_df.copy()\n",
        "    results_df['HDF_Results_Path'] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Metrics from All Scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diagnostic-check",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DIAGNOSTIC: Check DataFrame Columns\n",
        "# ============================================================================\n",
        "# Run this cell if you encounter KeyError: 'ari' in later cells\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATAFRAME DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check scenario_df\n",
        "if 'scenario_df' in dir():\n",
        "    print(f\"\\n1. scenario_df:\")\n",
        "    print(f\"   Shape: {scenario_df.shape}\")\n",
        "    print(f\"   Columns: {list(scenario_df.columns)}\")\n",
        "\n",
        "    required = ['ari', 'duration_hrs', 'ci_level', 'plan_number']\n",
        "    missing = [c for c in required if c not in scenario_df.columns]\n",
        "    if missing:\n",
        "        print(f\"   \u26a0\ufe0f  Missing: {missing}\")\n",
        "    else:\n",
        "        print(f\"   \u2713 Has all required scenario columns\")\n",
        "else:\n",
        "    print(f\"\\n1. scenario_df: NOT FOUND\")\n",
        "\n",
        "# Check results_df\n",
        "if 'results_df' in dir():\n",
        "    print(f\"\\n2. results_df:\")\n",
        "    print(f\"   Shape: {results_df.shape}\")\n",
        "    print(f\"   Columns: {list(results_df.columns)}\")\n",
        "\n",
        "    required = ['ari', 'duration_hrs', 'ci_level', 'plan_number', 'HDF_Results_Path']\n",
        "    missing = [c for c in required if c not in results_df.columns]\n",
        "    if missing:\n",
        "        print(f\"   \u26a0\ufe0f  Missing: {missing}\")\n",
        "    else:\n",
        "        print(f\"   \u2713 Has all required columns\")\n",
        "\n",
        "    if 'HDF_Results_Path' in results_df.columns:\n",
        "        with_hdf = results_df['HDF_Results_Path'].notna().sum()\n",
        "        print(f\"   Rows with HDF paths: {with_hdf}/{len(results_df)}\")\n",
        "else:\n",
        "    print(f\"\\n2. results_df: NOT FOUND\")\n",
        "\n",
        "# Check metrics_df\n",
        "if 'metrics_df' in dir():\n",
        "    print(f\"\\n3. metrics_df:\")\n",
        "    print(f\"   Shape: {metrics_df.shape}\")\n",
        "    print(f\"   Columns: {list(metrics_df.columns)}\")\n",
        "\n",
        "    required = ['ari', 'duration_hrs', 'ci_level']\n",
        "    missing = [c for c in required if c not in metrics_df.columns]\n",
        "    if missing:\n",
        "        print(f\"   \u26a0\ufe0f  Missing: {missing}\")\n",
        "        print(f\"   \\n   ACTION REQUIRED:\")\n",
        "        print(f\"   The metrics extraction didn't preserve scenario columns.\")\n",
        "        print(f\"   This means results_df was missing them.\")\n",
        "        print(f\"   Re-run the results extraction cell (Cell 28)\")\n",
        "    else:\n",
        "        print(f\"   \u2713 Has all required columns\")\n",
        "else:\n",
        "    print(f\"\\n3. metrics_df: NOT CREATED YET\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"END DIAGNOSTIC\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_scenario_metrics(results_df):\n",
        "    \"\"\"\n",
        "    Extract key metrics from all scenarios.\n",
        "    \"\"\"\n",
        "    metrics_list = []\n",
        "\n",
        "    print(\"Extracting metrics from all scenarios...\\n\")\n",
        "\n",
        "    # SAFETY CHECK: Verify required columns exist\n",
        "    required_cols = ['ari', 'duration_hrs', 'ci_level', 'total_depth_in', 'HDF_Results_Path', 'plan_number']\n",
        "    missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: results_df is missing required columns: {missing_cols}\")\n",
        "        print(f\"Available columns: {list(results_df.columns)}\")\n",
        "        print(f\"\\nCannot extract metrics without scenario information!\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    for idx, row in results_df.iterrows():\n",
        "        hdf_path = row.get('HDF_Results_Path')\n",
        "\n",
        "        if pd.isna(hdf_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Get runtime data\n",
        "            runtime_df = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "\n",
        "            # Get pipe network results\n",
        "            pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Pipes/Pipe Flow DS\")\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "\n",
        "            # Calculate statistics\n",
        "            pipe_flow_array = pipe_flow_ds.values\n",
        "            node_ws_array = node_ws.values\n",
        "\n",
        "            max_flows = np.nanmax(pipe_flow_array, axis=0)\n",
        "            max_ws = np.nanmax(node_ws_array, axis=0)\n",
        "\n",
        "            metrics = {\n",
        "                'plan_number': row['plan_number'],\n",
        "                'ari': row['ari'],\n",
        "                'duration_hrs': row['duration_hrs'],\n",
        "                'ci_level': row['ci_level'],\n",
        "                'total_precip_in': row['total_depth_in'],\n",
        "                'compute_time_hr': runtime_df['Complete Process (hr)'].iloc[0] if not runtime_df.empty else np.nan,\n",
        "                'avg_max_pipe_flow_cfs': np.nanmean(max_flows),\n",
        "                'max_pipe_flow_cfs': np.nanmax(max_flows),\n",
        "                'avg_max_node_ws_ft': np.nanmean(max_ws),\n",
        "                'max_node_ws_ft': np.nanmax(max_ws),\n",
        "                'hdf_path': hdf_path\n",
        "            }\n",
        "\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting metrics for plan {row.get('plan_number', 'unknown')}: {e}\")\n",
        "            continue\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_list)\n",
        "\n",
        "    if len(metrics_df) > 0:\n",
        "        print(f\"\\n\u2713 Extracted metrics from {len(metrics_df)} scenarios\")\n",
        "        print(f\"Columns in metrics_df: {list(metrics_df.columns)}\")\n",
        "    else:\n",
        "        print(\"\\n\u2717 No metrics extracted!\")\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "# SAFETY CHECK: Verify results_df has required columns before extraction\n",
        "print(\"=\"*70)\n",
        "print(\"PRE-EXTRACTION SAFETY CHECKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "required_cols = ['ari', 'duration_hrs', 'ci_level', 'total_depth_in', 'plan_number', 'HDF_Results_Path']\n",
        "print(f\"\\nChecking results_df for required columns...\")\n",
        "print(f\"Required: {required_cols}\")\n",
        "\n",
        "if 'results_df' in dir():\n",
        "    print(f\"\\nresults_df columns: {list(results_df.columns)}\")\n",
        "    missing = [col for col in required_cols if col not in results_df.columns]\n",
        "\n",
        "    if missing:\n",
        "        print(f\"\\n\u26a0\ufe0f  WARNING: Missing columns: {missing}\")\n",
        "        print(\"\\nThis will cause KeyError in uncertainty analysis!\")\n",
        "        print(\"\\nDEBUGGING INFO:\")\n",
        "        print(f\"  - results_df shape: {results_df.shape}\")\n",
        "        print(f\"  - rows with HDF paths: {results_df['HDF_Results_Path'].notna().sum() if 'HDF_Results_Path' in results_df.columns else 'N/A'}\")\n",
        "        print(\"\\nLikely cause: Merge in results extraction didn't preserve scenario_df columns\")\n",
        "        print(\"Solution: Re-run results extraction cell (Cell 28)\")\n",
        "    else:\n",
        "        print(f\"\\n\u2713 All required columns present!\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  results_df not found in namespace\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Extract metrics\n",
        "metrics_df = extract_scenario_metrics(results_df)\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nMetrics Summary:\")\n",
        "if len(metrics_df) > 0:\n",
        "    display.display(metrics_df.head(15))\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_df.to_csv('metrics_uncertainty_analysis.csv', index=False)\n",
        "    print(\"\\nMetrics saved to: metrics_uncertainty_analysis.csv\")\n",
        "else:\n",
        "    print(\"\\nNo metrics to display. Check errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainty Quantification\n",
        "\n",
        "Calculate uncertainty metrics for each ARI-Duration combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_uncertainty_metrics(metrics_df):\n",
        "    \"\"\"\n",
        "    Calculate uncertainty metrics for each ARI-Duration combination.\n",
        "    \"\"\"\n",
        "    # SAFETY CHECK: Verify required columns exist\n",
        "    required_cols = ['ari', 'duration_hrs', 'ci_level']\n",
        "    missing_cols = [col for col in required_cols if col not in metrics_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: metrics_df is missing required columns: {missing_cols}\")\n",
        "        print(f\"Available columns: {list(metrics_df.columns)}\")\n",
        "        print(f\"Cannot calculate uncertainty without these columns!\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    if len(metrics_df) == 0:\n",
        "        print(\"ERROR: metrics_df is empty!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    uncertainty_list = []\n",
        "\n",
        "    for ari in metrics_df['ari'].unique():\n",
        "        for duration in metrics_df['duration_hrs'].unique():\n",
        "            # Filter data\n",
        "            subset = metrics_df[\n",
        "                (metrics_df['ari'] == ari) &\n",
        "                (metrics_df['duration_hrs'] == duration)\n",
        "            ]\n",
        "\n",
        "            if len(subset) < 3:\n",
        "                continue\n",
        "\n",
        "            # Get values by CI level\n",
        "            lower = subset[subset['ci_level'] == 'lower']\n",
        "            point = subset[subset['ci_level'] == 'point']\n",
        "            upper = subset[subset['ci_level'] == 'upper']\n",
        "\n",
        "            if len(lower) == 0 or len(point) == 0 or len(upper) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate uncertainty for max pipe flow\n",
        "            flow_point = point['max_pipe_flow_cfs'].values[0]\n",
        "            flow_lower = lower['max_pipe_flow_cfs'].values[0]\n",
        "            flow_upper = upper['max_pipe_flow_cfs'].values[0]\n",
        "            flow_range = flow_upper - flow_lower\n",
        "            flow_rel_uncertainty = (flow_range / flow_point) * 100 if flow_point > 0 else np.nan\n",
        "\n",
        "            # Calculate uncertainty for max node WS\n",
        "            ws_point = point['max_node_ws_ft'].values[0]\n",
        "            ws_lower = lower['max_node_ws_ft'].values[0]\n",
        "            ws_upper = upper['max_node_ws_ft'].values[0]\n",
        "            ws_range = ws_upper - ws_lower\n",
        "            ws_rel_uncertainty = (ws_range / ws_point) * 100 if ws_point > 0 else np.nan\n",
        "\n",
        "            uncertainty_list.append({\n",
        "                'ari': ari,\n",
        "                'duration_hrs': duration,\n",
        "                'max_flow_point_cfs': flow_point,\n",
        "                'max_flow_lower_cfs': flow_lower,\n",
        "                'max_flow_upper_cfs': flow_upper,\n",
        "                'max_flow_range_cfs': flow_range,\n",
        "                'max_flow_rel_uncertainty_pct': flow_rel_uncertainty,\n",
        "                'max_ws_point_ft': ws_point,\n",
        "                'max_ws_lower_ft': ws_lower,\n",
        "                'max_ws_upper_ft': ws_upper,\n",
        "                'max_ws_range_ft': ws_range,\n",
        "                'max_ws_rel_uncertainty_pct': ws_rel_uncertainty\n",
        "            })\n",
        "\n",
        "    uncertainty_df = pd.DataFrame(uncertainty_list)\n",
        "    return uncertainty_df\n",
        "\n",
        "# Calculate uncertainties\n",
        "uncertainty_df = calculate_uncertainty_metrics(metrics_df)\n",
        "\n",
        "if len(uncertainty_df) > 0:\n",
        "    print(\"Uncertainty Analysis:\")\n",
        "    display.display(uncertainty_df)\n",
        "\n",
        "    # Save uncertainty metrics\n",
        "    uncertainty_df.to_csv('uncertainty_metrics.csv', index=False)\n",
        "    print(\"\\nUncertainty metrics saved to: uncertainty_metrics.csv\")\n",
        "else:\n",
        "    print(\"\\nCannot perform uncertainty analysis - metrics_df has missing or invalid data\")\n",
        "    print(\"Please check the results extraction and metrics extraction steps above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Peak Flow vs Duration with Confidence Envelopes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Peak Water Surface vs Duration with Confidence Envelopes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot peak water surface vs duration for each ARI\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, ari in enumerate(sorted(metrics_df['ari'].unique())):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get data for this ARI\n",
        "    ari_data = uncertainty_df[uncertainty_df['ari'] == ari].sort_values('duration_hrs')\n",
        "    \n",
        "    if len(ari_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Plot point estimate\n",
        "    ax.plot(ari_data['duration_hrs'], ari_data['max_ws_point_ft'], \n",
        "            'ko-', linewidth=2, markersize=6, label='Point Estimate', zorder=3)\n",
        "    \n",
        "    # Plot confidence envelope\n",
        "    ax.fill_between(ari_data['duration_hrs'], \n",
        "                     ari_data['max_ws_lower_ft'], \n",
        "                     ari_data['max_ws_upper_ft'],\n",
        "                     alpha=0.3, color='green', label='90% CI', zorder=1)\n",
        "    \n",
        "    ax.set_xlabel('Duration (hours)', fontsize=11)\n",
        "    ax.set_ylabel('Peak Water Surface (ft)', fontsize=11)\n",
        "    ax.set_title(f'{ari}-Year Event', fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(alpha=0.3, zorder=0)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xticks(durations)\n",
        "    ax.set_xticklabels(durations)\n",
        "\n",
        "plt.suptitle('Peak Water Surface vs Storm Duration with Confidence Intervals', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.savefig('peak_ws_uncertainty.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: peak_ws_uncertainty.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Relative Uncertainty Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create heatmaps for relative uncertainty\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Pivot data for heatmaps\n",
        "flow_uncertainty_pivot = uncertainty_df.pivot(\n",
        "    index='ari',\n",
        "    columns='duration_hrs',\n",
        "    values='max_flow_rel_uncertainty_pct'\n",
        ")\n",
        "\n",
        "ws_uncertainty_pivot = uncertainty_df.pivot(\n",
        "    index='ari',\n",
        "    columns='duration_hrs',\n",
        "    values='max_ws_rel_uncertainty_pct'\n",
        ")\n",
        "\n",
        "# Plot flow uncertainty heatmap\n",
        "sns.heatmap(flow_uncertainty_pivot, annot=True, fmt='.1f', cmap='YlOrRd', \n",
        "            cbar_kws={'label': 'Relative Uncertainty (%)'}, ax=ax1)\n",
        "ax1.set_title('Relative Uncertainty in Peak Flow', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Duration (hours)', fontsize=12)\n",
        "ax1.set_ylabel('Return Period (years)', fontsize=12)\n",
        "\n",
        "# Plot WS uncertainty heatmap\n",
        "sns.heatmap(ws_uncertainty_pivot, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "            cbar_kws={'label': 'Relative Uncertainty (%)'}, ax=ax2)\n",
        "ax2.set_title('Relative Uncertainty in Peak Water Surface', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Duration (hours)', fontsize=12)\n",
        "ax2.set_ylabel('Return Period (years)', fontsize=12)\n",
        "\n",
        "plt.suptitle('Precipitation Uncertainty Propagation Through Flood Model', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('uncertainty_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: uncertainty_heatmaps.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Time Series with Confidence Envelope at Critical Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_timeseries_with_ci(ari, duration, location_id=61):\n",
        "    \"\"\"\n",
        "    Plot time series showing confidence envelope at a specific location.\n",
        "    \"\"\"\n",
        "    # Get scenarios for this ARI and duration\n",
        "    scenarios = metrics_df[\n",
        "        (metrics_df['ari'] == ari) & \n",
        "        (metrics_df['duration_hrs'] == duration)\n",
        "    ]\n",
        "    \n",
        "    if len(scenarios) == 0:\n",
        "        print(f\"No data found for {ari}-year, {duration}-hour storm\")\n",
        "        return\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    \n",
        "    # Extract time series for each CI level\n",
        "    for ci_level in ['lower', 'point', 'upper']:\n",
        "        scenario = scenarios[scenarios['ci_level'] == ci_level]\n",
        "        \n",
        "        if len(scenario) == 0:\n",
        "            continue\n",
        "        \n",
        "        hdf_path = scenario['hdf_path'].values[0]\n",
        "        \n",
        "        try:\n",
        "            # Get node water surface time series\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "            \n",
        "            # Extract data for specified location\n",
        "            loc_ws = node_ws.sel(location=location_id)\n",
        "            \n",
        "            # Plot\n",
        "            if ci_level == 'point':\n",
        "                ax.plot(loc_ws.time.values, loc_ws.values, \n",
        "                       'k-', linewidth=2, label='Point Estimate', zorder=3)\n",
        "            elif ci_level == 'lower':\n",
        "                lower_data = loc_ws.values\n",
        "                lower_time = loc_ws.time.values\n",
        "            elif ci_level == 'upper':\n",
        "                upper_data = loc_ws.values\n",
        "                upper_time = loc_ws.time.values\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {ci_level} data: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Plot confidence envelope\n",
        "    if 'lower_data' in locals() and 'upper_data' in locals():\n",
        "        ax.fill_between(lower_time, lower_data, upper_data, \n",
        "                       alpha=0.3, color='blue', label='90% CI', zorder=1)\n",
        "    \n",
        "    ax.set_xlabel('Time', fontsize=12)\n",
        "    ax.set_ylabel('Water Surface Elevation (ft)', fontsize=12)\n",
        "    ax.set_title(f'Water Surface at Node {location_id}\\n' +\n",
        "                f'{ari}-Year, {duration}-Hour Storm with Confidence Interval', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(alpha=0.3, zorder=0)\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot time series for selected scenarios\n",
        "print(\"Time Series with Confidence Envelopes\\n\")\n",
        "plot_timeseries_with_ci(10, 24, location_id=61)\n",
        "plot_timeseries_with_ci(100, 24, location_id=61)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_all_aeps_for_duration(duration, location_id=61):\n",
        "    \"\"\"\n",
        "    Plot all AEP/ARI levels for a specific duration with confidence intervals.\n",
        "    Creates one plot showing all storm frequencies for the given duration.\n",
        "    \"\"\"\n",
        "    # Get all unique ARIs for this duration\n",
        "    duration_data = metrics_df[metrics_df['duration_hrs'] == duration]\n",
        "    aris = sorted(duration_data['ari'].unique())\n",
        "    \n",
        "    if len(aris) == 0:\n",
        "        print(f\"No data found for {duration}-hour duration\")\n",
        "        return\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    \n",
        "    # Color palette for different ARIs\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(aris)))\n",
        "    \n",
        "    for idx, ari in enumerate(aris):\n",
        "        scenarios = duration_data[duration_data['ari'] == ari]\n",
        "        \n",
        "        lower_data = None\n",
        "        upper_data = None\n",
        "        point_data = None\n",
        "        time_data = None\n",
        "        \n",
        "        # Extract time series for each CI level\n",
        "        for ci_level in ['lower', 'point', 'upper']:\n",
        "            scenario = scenarios[scenarios['ci_level'] == ci_level]\n",
        "            \n",
        "            if len(scenario) == 0:\n",
        "                continue\n",
        "            \n",
        "            hdf_path = scenario['hdf_path'].values[0]\n",
        "            \n",
        "            try:\n",
        "                # Get node water surface time series\n",
        "                node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "                \n",
        "                # Extract data for specified location\n",
        "                loc_ws = node_ws.sel(location=location_id)\n",
        "                \n",
        "                if ci_level == 'point':\n",
        "                    point_data = loc_ws.values\n",
        "                    time_data = loc_ws.time.values\n",
        "                elif ci_level == 'lower':\n",
        "                    lower_data = loc_ws.values\n",
        "                elif ci_level == 'upper':\n",
        "                    upper_data = loc_ws.values\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting {ari}-year {ci_level} data: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Plot confidence envelope and point estimate\n",
        "        if point_data is not None and time_data is not None:\n",
        "            # Plot confidence envelope first (lower z-order)\n",
        "            if lower_data is not None and upper_data is not None:\n",
        "                ax.fill_between(time_data, lower_data, upper_data, \n",
        "                               alpha=0.2, color=colors[idx], zorder=1)\n",
        "            \n",
        "            # Plot point estimate on top\n",
        "            ax.plot(time_data, point_data, \n",
        "                   color=colors[idx], linewidth=2.5, \n",
        "                   label=f'{ari}-Year (90% CI)', zorder=2)\n",
        "    \n",
        "    ax.set_xlabel('Time', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel('Water Surface Elevation (ft)', fontsize=13, fontweight='bold')\n",
        "    ax.set_title(f'Water Surface at Node {location_id}\\n' +\n",
        "                f'{duration}-Hour Duration - All Return Periods with 90% Confidence Intervals', \n",
        "                fontsize=15, fontweight='bold')\n",
        "    ax.legend(fontsize=11, loc='best', framealpha=0.9)\n",
        "    ax.grid(alpha=0.3, linestyle='--', zorder=0)\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_all_durations_all_aeps(location_id=61):\n",
        "    \"\"\"\n",
        "    Create a separate plot for each duration showing all AEP levels.\n",
        "    \"\"\"\n",
        "    # Get all unique durations\n",
        "    durations = sorted(metrics_df['duration_hrs'].unique())\n",
        "    \n",
        "    print(f\"Creating plots for {len(durations)} durations at Node {location_id}\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for duration in durations:\n",
        "        print(f\"\\n{duration}-Hour Duration Storm\")\n",
        "        print(\"-\" * 70)\n",
        "        plot_all_aeps_for_duration(duration, location_id)\n",
        "\n",
        "\n",
        "# Generate all plots\n",
        "plot_all_durations_all_aeps(location_id=61)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics and Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"UNCERTAINTY ANALYSIS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. OVERALL UNCERTAINTY STATISTICS\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Average relative uncertainty in peak flow: {uncertainty_df['max_flow_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"Average relative uncertainty in peak WSE: {uncertainty_df['max_ws_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"Range of uncertainty in peak flow: {uncertainty_df['max_flow_rel_uncertainty_pct'].min():.1f}% - {uncertainty_df['max_flow_rel_uncertainty_pct'].max():.1f}%\")\n",
        "print(f\"Range of uncertainty in peak WSE: {uncertainty_df['max_ws_rel_uncertainty_pct'].min():.1f}% - {uncertainty_df['max_ws_rel_uncertainty_pct'].max():.1f}%\")\n",
        "\n",
        "print(\"\\n2. SCENARIOS WITH HIGHEST UNCERTAINTY\")\n",
        "print(\"-\" * 70)\n",
        "top_flow_uncertainty = uncertainty_df.nlargest(5, 'max_flow_rel_uncertainty_pct')\n",
        "print(\"\\nTop 5 - Peak Flow Uncertainty:\")\n",
        "for idx, row in top_flow_uncertainty.iterrows():\n",
        "    print(f\"  {row['ari']}-year, {row['duration_hrs']}-hour: {row['max_flow_rel_uncertainty_pct']:.1f}%\")\n",
        "\n",
        "top_ws_uncertainty = uncertainty_df.nlargest(5, 'max_ws_rel_uncertainty_pct')\n",
        "print(\"\\nTop 5 - Water Surface Uncertainty:\")\n",
        "for idx, row in top_ws_uncertainty.iterrows():\n",
        "    print(f\"  {row['ari']}-year, {row['duration_hrs']}-hour: {row['max_ws_rel_uncertainty_pct']:.1f}%\")\n",
        "\n",
        "print(\"\\n3. UNCERTAINTY BY RETURN PERIOD\")\n",
        "print(\"-\" * 70)\n",
        "by_ari = uncertainty_df.groupby('ari').agg({\n",
        "    'max_flow_rel_uncertainty_pct': 'mean',\n",
        "    'max_ws_rel_uncertainty_pct': 'mean'\n",
        "}).round(1)\n",
        "print(by_ari)\n",
        "\n",
        "print(\"\\n4. UNCERTAINTY BY DURATION\")\n",
        "print(\"-\" * 70)\n",
        "by_duration = uncertainty_df.groupby('duration_hrs').agg({\n",
        "    'max_flow_rel_uncertainty_pct': 'mean',\n",
        "    'max_ws_rel_uncertainty_pct': 'mean'\n",
        "}).round(1)\n",
        "print(by_duration)\n",
        "\n",
        "print(\"\\n5. DESIGN IMPLICATIONS\")\n",
        "print(\"-\" * 70)\n",
        "print(\"Based on the uncertainty analysis:\")\n",
        "print(f\"  \u2022 Precipitation uncertainty of \u00b140% results in:\")\n",
        "print(f\"    - Peak flow uncertainty: {uncertainty_df['max_flow_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"    - Peak WSE uncertainty: {uncertainty_df['max_ws_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"  \u2022 Higher uncertainties observed for:\")\n",
        "most_uncertain_duration = by_duration['max_flow_rel_uncertainty_pct'].idxmax()\n",
        "print(f\"    - Duration: {most_uncertain_duration} hours\")\n",
        "most_uncertain_ari = by_ari['max_flow_rel_uncertainty_pct'].idxmax()\n",
        "print(f\"    - Return period: {most_uncertain_ari} years\")\n",
        "print(f\"  \u2022 Designers should consider upper bound scenarios for critical infrastructure\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Analysis complete. All results saved to CSV files.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "This comprehensive uncertainty analysis demonstrates:\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Uncertainty Propagation**: Precipitation uncertainty of approximately \u00b140% (90% confidence interval) propagates through the hydraulic model with varying impacts depending on storm characteristics.\n",
        "\n",
        "2. **Duration Effects**: Different storm durations exhibit varying sensitivity to precipitation uncertainty, with some durations showing amplified uncertainty in hydraulic responses.\n",
        "\n",
        "3. **Return Period Patterns**: The relationship between return period and uncertainty provides insights into which design events have the most variability.\n",
        "\n",
        "4. **Spatial Considerations**: Peak water surface elevations and pipe flows show different uncertainty patterns, important for infrastructure design decisions.\n",
        "\n",
        "### Design Recommendations\n",
        "\n",
        "- **Use upper confidence bounds** for critical infrastructure design\n",
        "- **Consider duration sensitivity** when selecting design storms\n",
        "- **Account for uncertainty** in flood risk communication\n",
        "- **Evaluate multiple scenarios** rather than single point estimates\n",
        "\n",
        "### Methodology Advantages\n",
        "\n",
        "- **Comprehensive Coverage**: All practical durations analyzed\n",
        "- **Systematic Approach**: Consistent methodology across all scenarios\n",
        "- **Quantified Uncertainty**: Clear metrics for decision-making\n",
        "- **Reproducible**: Fully documented workflow\n",
        "\n",
        "### Future Enhancements\n",
        "\n",
        "This framework can be extended to include:\n",
        "- Climate change adjustments to precipitation\n",
        "- Infiltration parameter uncertainty\n",
        "- Manning's n coefficient uncertainty\n",
        "- Combined uncertainty analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}