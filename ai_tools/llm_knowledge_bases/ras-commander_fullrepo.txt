Project Structure (files included):
├── .gitignore
├── AGENTS.md
├── CLAUDE.md
├── Comprehensive_Library_Guide.md
├── LICENSE
├── README.md
├── STYLE_GUIDE.md
├── api-hdf.md
├── api-ras.md
├── api.md
├── examples
│   ├── 00_Using_RasExamples.ipynb
│   ├── 01_project_initialization.ipynb
│   ├── 02_plan_and_geometry_operations.ipynb
│   ├── 03_unsteady_flow_operations.ipynb
│   ├── 04_multiple_project_operations.ipynb
│   ├── 05_single_plan_execution.ipynb
│   ├── 06_executing_plan_sets.ipynb
│   ├── 07_sequential_plan_execution.ipynb
│   ├── 08_parallel_execution.ipynb
│   ├── 09_plan_parameter_operations.ipynb
│   ├── 101_Core_Sensitivity.ipynb
│   ├── 102_benchmarking_versions_6.1_to_6.6.ipynb
│   ├── 103_Running_AEP_Events_from_Atlas_14.ipynb
│   ├── 104_Atlas14_AEP_Multi_Project.ipynb
│   ├── 105_mannings_sensitivity_bulk_analysis.ipynb
│   ├── 106_mannings_sensitivity_multi-interval.ipynb
│   ├── 10_1d_hdf_data_extraction.ipynb
│   ├── 11_2d_hdf_data_extraction.ipynb
│   ├── 12_2d_hdf_data_extraction pipes and pumps.ipynb
│   ├── 13_2d_detail_face_data_extraction.ipynb
│   ├── 14_fluvial_pluvial_delineation.ipynb
│   ├── 15_stored_map_generation.ipynb
│   ├── 16_automating_ras_with_win32com.ipynb
│   ├── 17_extracting_profiles_with_hecrascontroller and RasControl.ipynb
│   ├── 18_breach_results_extraction.ipynb
│   ├── 19_steady_flow_analysis.ipynb
│   ├── 20_plaintext_geometry_operations.ipynb
│   ├── 21_rasmap_raster_exports.ipynb
│   ├── 22_dss_boundary_extraction.ipynb
│   ├── 23_remote_execution_psexec.ipynb
│   ├── AGENTS.md
│   ├── README.md
│   ├── REMOTE_WORKERS_README.md
│   ├── RemoteWorkers.json
│   ├── RemoteWorkers.json.template
│   ├── flood_polygons.geojson
│   └── hyetographs
│       ├── hyetograph_ARI_100_years_pos50pct_24hr.csv
│       ├── hyetograph_ARI_10_years_pos50pct_24hr.csv
│       ├── hyetograph_ARI_25_years_pos50pct_24hr.csv
│       ├── hyetograph_ARI_2_years_pos50pct_24hr.csv
│       ├── hyetograph_ARI_50_years_pos50pct_24hr.csv
│       └── hyetograph_ARI_5_years_pos50pct_24hr.csv
├── pyproject.toml
├── ras_commander
│   ├── AGENTS.md
│   ├── Decorators.py
│   ├── LoggingConfig.py
│   ├── M3Model.py
│   ├── RasBreach.py
│   ├── RasCmdr.py
│   ├── RasControl.py
│   ├── RasExamples.py
│   ├── RasGeo.py
│   ├── RasGeometry.py
│   ├── RasGeometryUtils.py
│   ├── RasGuiAutomation.py
│   ├── RasMap.py
│   ├── RasPlan.py
│   ├── RasPrj.py
│   ├── RasUnsteady.py
│   ├── RasUtils.py
│   ├── __init__.py
│   ├── dss
│   │   ├── AGENTS.md
│   │   ├── RasDss.py
│   │   ├── __init__.py
│   │   └── _hec_monolith.py
│   ├── geom
│   │   ├── AGENTS.md
│   │   ├── GeomBridge.py
│   │   ├── GeomCrossSection.py
│   │   ├── GeomCulvert.py
│   │   ├── GeomInlineWeir.py
│   │   ├── GeomLandCover.py
│   │   ├── GeomLateral.py
│   │   ├── GeomParser.py
│   │   ├── GeomPreprocessor.py
│   │   ├── GeomStorage.py
│   │   └── __init__.py
│   ├── hdf
│   │   ├── AGENTS.md
│   │   ├── HdfBase.py
│   │   ├── HdfBndry.py
│   │   ├── HdfFluvialPluvial.py
│   │   ├── HdfHydraulicTables.py
│   │   ├── HdfInfiltration.py
│   │   ├── HdfMesh.py
│   │   ├── HdfPipe.py
│   │   ├── HdfPlan.py
│   │   ├── HdfPlot.py
│   │   ├── HdfPump.py
│   │   ├── HdfResultsBreach.py
│   │   ├── HdfResultsMesh.py
│   │   ├── HdfResultsPlan.py
│   │   ├── HdfResultsPlot.py
│   │   ├── HdfResultsXsec.py
│   │   ├── HdfStruc.py
│   │   ├── HdfUtils.py
│   │   ├── HdfXsec.py
│   │   └── __init__.py
│   └── remote
│       ├── AGENTS.md
│       ├── AwsEc2Worker.py
│       ├── AzureFrWorker.py
│       ├── DockerWorker.py
│       ├── Execution.py
│       ├── LocalWorker.py
│       ├── PsexecWorker.py
│       ├── RasWorker.py
│       ├── RemoteWorkers.json.template
│       ├── SlurmWorker.py
│       ├── SshWorker.py
│       ├── Utils.py
│       ├── WinrmWorker.py
│       └── __init__.py
├── settings.db
├── setup.py
└── troubleshooting
    ├── TCC_road.u01
    ├── VERIFICATION_SUMMARY.md
    ├── demo_precipitation_hydrograph_parsing.ipynb
    ├── demo_precipitation_hydrograph_parsing_executed.ipynb
    ├── execute_notebook.py
    ├── executed_02.ipynb
    ├── execution_results.json
    └── test_precip_hydrograph.py

File: C:\GH\ras-commander\.gitignore
==================================================
# Ignore the example_projects folder and all its subfolders
examples/example_projects/

# Ignore workspace, projects, and my_projects folders
workspace/
projects/
my_projects/
test_projects/
custom_extraction_folder/
multi_test/
unsteady_examples/
special_projects/

# Ignore FEMA BLE Models
examples/FEMA_BLE_Models/
examples/hdf_example_data/

# ignore tools/stored_map_assistant build folders
tools/stored_map_assistant/build/
tools/stored_map_assistant/dist/

# Ignore library assistant config
library_assistant/config/

# Ignore Python egg info
*.egg-info/
.eggs/
.conda/
# Ignore the Example_Projects_6_5.zip file
Example_Projects_6_5.zip

# Ignore the misc folder and all its subfolders
misc/

# Ignore Python cache files
__pycache__/
*.py[cod]

# Ignore compiled Python files
*.so

# Ignore distribution / packaging
dist/
build/

# Ignore test cache
.pytest_cache/

# Ignore virtual environments
.venv/
venv/

# Ignore IDE-specific files (optional, uncomment if needed)
# .vscode/
# .idea/

# Ignore OS-specific files
.DS_Store
Thumbs.db
Example_Projects_6_6.zip
/tests/example_projects
/example_projects

# Research folder - ignore outputs but keep source code
# Keep: Python scripts, notebooks, README, AGENTS.md, pyproject.toml
# Ignore: CSV, log files, temp folders, generated markdown findings
research/**/findings/*.csv
research/**/findings/*.log
research/**/outputs/
research/**/temp/
research/**/.ipynb_checkpoints/
research/**/*.pyc
research/**/__pycache__/
/feature_dev_notes
/research
/tools
/examples/working
/working/
/examples/hyetographs
examples/flood_polygons.geojson
ras-commander_logo.png
/tests
/.claude
nul
examples/example_projects.csv

# Ignore remote worker credentials
examples/RemoteWorkers.json
RemoteWorkers.json
/troubleshooting

==================================================

File: C:\GH\ras-commander\AGENTS.md
==================================================
**Purpose**
- Orient AI agents to use Ras Commander effectively without relying on external knowledge bases or executing heavyweight notebooks in-place. This file governs the entire repository. Subfolder files add context-specific guidance.

**Do This First**
- Ignore `ai_tools/` and any generated knowledge bases. These are for maintainers; agents should not read, build, or depend on them.
- Prefer “cleaned” notebooks (no images, truncated outputs). If a cleaned copy is not present, treat notebooks as reference only and disable plotting-heavy cells.
- Work in local, ignored folders such as `working/`, `scripts/`, or `ras_agent/` at the repo root. Create them if missing. It is safe to add these to `.gitignore` locally.

**Environment (uv-managed)**
- Python 3.10+ is required.
- Create a single virtual environment at the repo root and install the package in editable mode:
  - `uv venv .venv`
  - `uv pip install -e .`
  - Quick check: `uv run python -c "import ras_commander as ras; print(ras.__version__)"`
- Build artifacts when needed only: `python setup.py sdist bdist_wheel` (invokes a maintainer script in `ai_tools/`; agents should not rely on it).

**HEC-RAS Requirement**
- HEC-RAS 6.x must be installed to execute plans. Pass an explicit `Ras.exe` path when needed, for example:
  - `from ras_commander import init_ras_project`
  - `init_ras_project(<project_folder>, r"D:/Programs/HEC/HEC-RAS/6.6/Ras.exe")`

**Repository Layout**
- `ras_commander/` — core library (e.g., `RasCmdr`, `RasPrj`, `RasPlan`, `RasMap`, `Hdf*`, `Ras*`). One class per module is typical (e.g., `HdfMesh.py`). See `ras_commander/AGENTS.md` for coding-level details.
- `examples/` — Jupyter notebooks and sample data for scenario validation. See `examples/AGENTS.md` for a detailed, agent-friendly index.
- Top-level docs: `README.md`, `STYLE_GUIDE.md`, `Comprehensive_Library_Guide.md`, `api.md`.
- Packaging: `setup.py`, `pyproject.toml`.

**Coding Style**
- Follow `STYLE_GUIDE.md` (PEP 8, 4-space indents, ≤79-char lines, imports: stdlib → third-party → local).
- Names: `snake_case` for functions/variables; `PascalCase` for classes; `UPPER_CASE` for constants.
- Logging pattern:
  - `from ras_commander import get_logger, log_call`
  - `logger = get_logger(__name__)`
  - Decorate public methods with `@log_call`.
- Many classes act as static “namespaces” (`Hdf*`, `Ras*`). Keep new modules consistent with this pattern.

**Notebooks: Agent Workflow**
- Purpose: Notebooks illustrate usage for humans; agents should extract logic into scripts.
- Agent steps when using notebooks:
  1) Read and analyze relevant code cells (skip long outputs, images).
  2) Understand the goal and dependencies of the sequence.
  3) Extract and adapt to a standalone Python script or CLI tool.
     - Replace `!pip install` or ad‑hoc installs with `uv pip install ...`.
     - Replace environment-specific paths with parameters.
  4) Execute the script via `uv run ...` with inputs and output folders in `working/`.
- The examples index at `examples/AGENTS.md` highlights unique logic that lives only in notebooks (e.g., nearest-cell/face search, Atlas 14 workflows).

**Core Execution Examples**
- Initialize a project and run a plan to a clean folder:
  - `from ras_commander import init_ras_project, RasCmdr`
  - `init_ras_project(<project_folder>, <path_to_Ras.exe>)`
  - `RasCmdr.compute_plan("01", dest_folder=r"C:\\temp\\ras_out", overwrite_dest=True)`
- Parallel execution across plans: `RasCmdr.compute_parallel([...])` (see `RasCmdr` for details and CPU core advice).

**Testing & Data Hygiene**
- Scenario validation is performed with official HEC-RAS example projects (see `examples/`).
- Do not commit large datasets or generated outputs. Use writable temp folders (e.g., `C:\\temp\\ras_out` or `working/`).
- If adding unit tests, prefer `pytest` with `tests/test_*.py`; run with `pytest`.

**PR & Commit Guidance**
- Commits: imperative mood, concise subject, optional scope. Example: `HdfXsec: fix ineffective-flow handling`.
- PRs: include summary, motivation, linked issues, reproduction steps, and before/after notes.
- Update documentation when APIs change; keep diffs focused and consistent.

**HEC Commander (companion repo)**
- Apply this `AGENTS.md` standard there as well. Remove outdated custom GPT files. Republishing as standardized CLB agents can follow separately. This repo’s `AGENTS.md` is self-sufficient for Ras Commander.


==================================================

File: C:\GH\ras-commander\api-hdf.md
==================================================
# RAS Commander API Documentation - HDF Classes

This document provides detailed reference for HDF data extraction and analysis classes in the `ras_commander` library. For HEC-RAS project management and plan execution classes, see [api-ras.md](api-ras.md).

## Overview

The HDF classes provide methods for extracting and analyzing data from HEC-RAS HDF5 files (.hdf), including:
- Geometry data (cross sections, 2D mesh, structures, boundaries)
- Simulation results (water surface, velocity, depth, flow)
- Plan information and metadata

All methods use the `@standardize_input` decorator for flexible file path handling and the `@log_call` decorator for operation logging.

---

## Class: HdfBase

Contains fundamental static methods for interacting with HEC-RAS HDF files. Used by other `Hdf*` classes. Requires an open `h5py.File` object or uses `@standardize_input`.

### `HdfBase.get_simulation_start_time(hdf_file)`

*   **Purpose:** Extracts the simulation start time attribute from the Plan Information group.
*   **Parameters:**
    *   `hdf_file` (`h5py.File`): Open HDF file object.
*   **Returns:** (`datetime`): Simulation start time.
*   **Raises:** `ValueError` if path not found or time parsing fails.

### `HdfBase.get_unsteady_timestamps(hdf_file)`

*   **Purpose:** Extracts the list of unsteady output timestamps (usually in milliseconds format) and converts them to datetime objects.
*   **Parameters:**
    *   `hdf_file` (`h5py.File`): Open HDF file object.
*   **Returns:** `List[datetime]`: List of datetime objects for each output time step.

### `HdfBase.get_2d_flow_area_names_and_counts(hdf_path)`

*   **Purpose:** Gets the names and cell counts of all 2D Flow Areas defined in the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry HDF).
*   **Returns:** `List[Tuple[str, int]]`: List of tuples `(area_name, cell_count)`.
*   **Raises:** `ValueError` on read errors.

### `HdfBase.get_projection(hdf_path)`

*   **Purpose:** Retrieves the spatial projection information (WKT string) from the HDF file attributes or associated `.rasmap` file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file.
*   **Returns:** (`str` or `None`): Well-Known Text (WKT) string of the projection, or `None` if not found.

### `HdfBase.get_attrs(hdf_file, attr_path)`

*   **Purpose:** Retrieves all attributes from a specific group or dataset within the HDF file.
*   **Parameters:**
    *   `hdf_file` (`h5py.File`): Open HDF file object.
    *   `attr_path` (`str`): Internal HDF path to the group/dataset (e.g., "Plan Data/Plan Information").
*   **Returns:** `Dict[str, Any]`: Dictionary of attributes. Returns empty dict if path not found.

### `HdfBase.get_dataset_info(file_path, group_path='/')`

*   **Purpose:** Prints a recursive listing of the structure (groups, datasets, attributes, shapes, dtypes) within an HDF5 file, starting from `group_path`.
*   **Parameters:**
    *   `file_path` (Input handled by `@standardize_input`): Path identifier for the HDF file.
    *   `group_path` (`str`, optional): Internal HDF path to start exploration from. Default is root ('/').
*   **Returns:** `None`. Prints to console.

### `HdfBase.get_polylines_from_parts(hdf_path, path, info_name="Polyline Info", parts_name="Polyline Parts", points_name="Polyline Points")`

*   **Purpose:** Reconstructs Shapely LineString or MultiLineString geometries from HEC-RAS's standard polyline representation in HDF (using Info, Parts, Points datasets).
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file.
    *   `path` (`str`): Internal HDF base path containing the polyline datasets (e.g., "Geometry/River Centerlines").
    *   `info_name` (`str`, optional): Name of the dataset containing polyline start/count info. Default "Polyline Info".
    *   `parts_name` (`str`, optional): Name of the dataset defining parts for multi-part lines. Default "Polyline Parts".
    *   `points_name` (`str`, optional): Name of the dataset containing all point coordinates. Default "Polyline Points".
*   **Returns:** `List[LineString or MultiLineString]`: List of reconstructed Shapely geometries.

### `HdfBase.print_attrs(name, obj)`

*   **Purpose:** Helper method to print the attributes of an HDF5 object (Group or Dataset) during exploration (used by `get_dataset_info`).
*   **Parameters:**
    *   `name` (`str`): Name of the HDF5 object.
    *   `obj` (`h5py.Group` or `h5py.Dataset`): The HDF5 object.
*   **Returns:** `None`. Prints to console.

---

## Class: HdfBndry

Contains static methods for extracting boundary-related *geometry* features (BC Lines, Breaklines, Refinement Regions, Reference Lines/Points) from HEC-RAS HDF files (typically geometry HDF). Returns GeoDataFrames.

### `HdfBndry.get_bc_lines(hdf_path)`

*   **Purpose:** Extracts 2D Flow Area Boundary Condition Lines.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier (usually geometry HDF).
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries and attributes (Name, SA-2D, Type, etc.).

### `HdfBndry.get_breaklines(hdf_path)`

*   **Purpose:** Extracts 2D Flow Area Break Lines. Skips invalid (zero-length, single-point) breaklines.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier (usually geometry HDF).
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString/MultiLineString geometries and attributes (bl_id, Name).

### `HdfBndry.get_refinement_regions(hdf_path)`

*   **Purpose:** Extracts 2D Flow Area Refinement Regions.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier (usually geometry HDF).
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Polygon/MultiPolygon geometries and attributes (rr_id, Name).

### `HdfBndry.get_reference_lines(hdf_path, mesh_name=None)`

*   **Purpose:** Extracts Reference Lines used for profile output, optionally filtering by mesh name.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier (usually geometry HDF).
    *   `mesh_name` (`str`, optional): Filter results to this specific mesh area.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString/MultiLineString geometries and attributes (refln_id, Name, mesh_name, Type).

### `HdfBndry.get_reference_points(hdf_path, mesh_name=None)`

*   **Purpose:** Extracts Reference Points used for point output, optionally filtering by mesh name.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier (usually geometry HDF).
    *   `mesh_name` (`str`, optional): Filter results to this specific mesh area.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Point geometries and attributes (refpt_id, Name, mesh_name, Cell Index).

---

## Class: HdfFluvialPluvial

Contains static methods for analyzing fluvial-pluvial boundaries based on simulation results.

### `HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(hdf_path, delta_t=12, min_line_length=None)`

*   **Purpose:** Calculates the boundary line between areas dominated by fluvial (riverine) vs. pluvial (rainfall/local) flooding, based on the timing difference of maximum water surface elevation between adjacent 2D cells. Attempts to join adjacent boundary segments.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF file.
    *   `delta_t` (`float`, optional): Time difference threshold in hours. Adjacent cells with max WSE time differences greater than this are considered part of the boundary. Default is 12.
    *   `min_line_length` (`float`, optional): Minimum length (in CRS units) for boundary lines to be included. Lines shorter than this will be dropped. Default is None (no filtering).
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame containing LineString geometries representing the calculated boundary. CRS matches the input HDF.
*   **Raises:** `ValueError` if required mesh or results data is missing.

### `HdfFluvialPluvial.generate_fluvial_pluvial_polygons(hdf_path, delta_t=12, temporal_tolerance_hours=1.0, min_polygon_area_acres=None)`

*   **Purpose:** Generates dissolved polygons representing fluvial, pluvial, and ambiguous flood zones. Classifies each wetted cell using iterative region growth based on maximum water surface elevation timing, then merges cells into three distinct regions: 'fluvial', 'pluvial', and 'ambiguous'.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF file.
    *   `delta_t` (`float`, optional): The time difference (in hours) between adjacent cells that defines the initial boundary between fluvial and pluvial zones. Default is 12.
    *   `temporal_tolerance_hours` (`float`, optional): The maximum time difference (in hours) for a cell to be considered part of an expanding region during iterative growth. Default is 1.0.
    *   `min_polygon_area_acres` (`float`, optional): Minimum polygon area (in acres). For fluvial or pluvial polygons smaller than this threshold, they are reclassified to the opposite type and merged with adjacent polygons. Ambiguous polygons are not affected. Default is None (no filtering).
*   **Returns:** `gpd.GeoDataFrame`: A GeoDataFrame with dissolved polygons for 'fluvial', 'pluvial', and 'ambiguous' zones. CRS matches the input HDF.
*   **Raises:** `ValueError` if required mesh or results data is missing.
*   **Notes:** 
    - Uses iterative region growth algorithm to expand from initial boundary seeds
    - Handles conflicts by marking cells as 'ambiguous' when both fluvial and pluvial regions compete for the same cell
    - Area-based filtering requires projected CRS; geographic CRS will skip this step with a warning

---

## Class: HdfInfiltration

Contains static methods for handling infiltration data within HEC-RAS HDF files (typically geometry HDF).

### `HdfInfiltration.get_infiltration_baseoverrides(hdf_path: Path) -> Optional[pd.DataFrame]`

*   **Purpose:** Retrieves current infiltration parameters from a HEC-RAS geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `Optional[pd.DataFrame]`: DataFrame containing infiltration parameters if successful, None if operation fails.

### `HdfInfiltration.get_infiltration_layer_data(hdf_path: Path) -> Optional[pd.DataFrame]`

*   **Purpose:** Retrieves current infiltration parameters from a HEC-RAS infiltration layer HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the infiltration layer HDF.
*   **Returns:** `Optional[pd.DataFrame]`: DataFrame containing infiltration parameters if successful, None if operation fails.

### `HdfInfiltration.set_infiltration_layer_data(hdf_path: Path, infiltration_df: pd.DataFrame) -> Optional[pd.DataFrame]`

*   **Purpose:** Sets infiltration layer data in the infiltration layer HDF file directly from the provided DataFrame.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the infiltration layer HDF.
    *   `infiltration_df` (`pd.DataFrame`): DataFrame containing infiltration parameters.
*   **Returns:** `Optional[pd.DataFrame]`: The infiltration DataFrame if successful, None if operation fails.

### `HdfInfiltration.scale_infiltration_data(hdf_path: Path, infiltration_df: pd.DataFrame, scale_factors: Dict[str, float]) -> Optional[pd.DataFrame]`

*   **Purpose:** Updates infiltration parameters in the HDF file with scaling factors.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `infiltration_df` (`pd.DataFrame`): DataFrame containing infiltration parameters.
    *   `scale_factors` (`Dict[str, float]`): Dictionary mapping column names to their scaling factors.
*   **Returns:** `Optional[pd.DataFrame]`: The updated infiltration DataFrame if successful, None if operation fails.

### `HdfInfiltration.get_infiltration_map(hdf_path: Path = None, ras_object: Any = None) -> dict`

*   **Purpose:** Reads the infiltration raster map from HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file. If not provided, uses first infiltration_hdf_path from rasmap_df.
    *   `ras_object` (`RasPrj`, optional): Specific RAS object to use. If None, uses the global ras instance.
*   **Returns:** `dict`: Dictionary mapping raster values to mukeys.

### `HdfInfiltration.calculate_soil_statistics(zonal_stats: list, raster_map: dict) -> pd.DataFrame`

*   **Purpose:** Calculates soil statistics from zonal statistics.
*   **Parameters:**
    *   `zonal_stats` (`list`): List of zonal statistics.
    *   `raster_map` (`dict`): Dictionary mapping raster values to mukeys.
*   **Returns:** `pd.DataFrame`: DataFrame with soil statistics including percentages and areas.

### `HdfInfiltration.get_significant_mukeys(soil_stats: pd.DataFrame, threshold: float = 1.0) -> pd.DataFrame`

*   **Purpose:** Gets mukeys with percentage greater than threshold.
*   **Parameters:**
    *   `soil_stats` (`pd.DataFrame`): DataFrame with soil statistics.
    *   `threshold` (`float`, optional): Minimum percentage threshold. Default 1.0.
*   **Returns:** `pd.DataFrame`: DataFrame with significant mukeys and their statistics.

### `HdfInfiltration.calculate_total_significant_percentage(significant_mukeys: pd.DataFrame) -> float`

*   **Purpose:** Calculates total percentage covered by significant mukeys.
*   **Parameters:**
    *   `significant_mukeys` (`pd.DataFrame`): DataFrame of significant mukeys.
*   **Returns:** `float`: Total percentage covered by significant mukeys.

### `HdfInfiltration.save_statistics(soil_stats: pd.DataFrame, output_path: Path, include_timestamp: bool = True)`

*   **Purpose:** Saves soil statistics to CSV.
*   **Parameters:**
    *   `soil_stats` (`pd.DataFrame`): DataFrame with soil statistics.
    *   `output_path` (`Path`): Path to save CSV file.
    *   `include_timestamp` (`bool`, optional): Whether to include timestamp in filename. Default True.
*   **Returns:** None

### `HdfInfiltration.get_infiltration_parameters(hdf_path: Path = None, mukey: str = None, ras_object: Any = None) -> dict`

*   **Purpose:** Gets infiltration parameters for a specific mukey from HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file. If not provided, uses first infiltration_hdf_path from rasmap_df.
    *   `mukey` (`str`): Mukey identifier.
    *   `ras_object` (`RasPrj`, optional): Specific RAS object to use. If None, uses the global ras instance.
*   **Returns:** `dict`: Dictionary of infiltration parameters.

### `HdfInfiltration.calculate_weighted_parameters(soil_stats: pd.DataFrame, infiltration_params: dict) -> dict`

*   **Purpose:** Calculates weighted infiltration parameters based on soil statistics.
*   **Parameters:**
    *   `soil_stats` (`pd.DataFrame`): DataFrame with soil statistics.
    *   `infiltration_params` (`dict`): Dictionary of infiltration parameters by mukey.
*   **Returns:** `dict`: Dictionary of weighted average infiltration parameters.

---

## Class: HdfMesh

Contains static methods for extracting 2D mesh geometry information from HEC-RAS HDF files (typically geometry or plan HDF).

### `HdfMesh.get_mesh_area_names(hdf_path)`

*   **Purpose:** Retrieves the names of all 2D Flow Areas defined in the HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file.
*   **Returns:** `List[str]`: List of 2D Flow Area names.

### `HdfMesh.get_mesh_areas(hdf_path)`

*   **Purpose:** Extracts the outer perimeter polygons for each 2D Flow Area.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Polygon geometries and 'mesh_name' attribute.

### `HdfMesh.get_mesh_cell_polygons(hdf_path)`

*   **Purpose:** Reconstructs the individual cell polygons for all 2D Flow Areas by assembling cell faces.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Polygon geometries and attributes 'mesh_name', 'cell_id'.

### `HdfMesh.get_mesh_cell_points(hdf_path)`

*   **Purpose:** Extracts the center point coordinates for each cell in all 2D Flow Areas.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Point geometries and attributes 'mesh_name', 'cell_id'.

### `HdfMesh.get_mesh_cell_faces(hdf_path)`

*   **Purpose:** Extracts the face line segments that form the boundaries of the mesh cells.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries and attributes 'mesh_name', 'face_id'.

### `HdfMesh.get_mesh_area_attributes(hdf_path)`

*   **Purpose:** Retrieves the main attributes associated with the 2D Flow Areas group in the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `pd.DataFrame`: DataFrame containing the attributes (e.g., Manning's n values).

### `HdfMesh.get_mesh_face_property_tables(hdf_path)`

*   **Purpose:** Extracts the detailed hydraulic property tables (Elevation vs. Area, Wetted Perimeter, Roughness) associated with each *face* in each 2D Flow Area.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `Dict[str, pd.DataFrame]`: Dictionary mapping mesh names to DataFrames. Each DataFrame contains columns ['Face ID', 'Z', 'Area', 'Wetted Perimeter', "Manning's n"].

### `HdfMesh.get_mesh_cell_property_tables(hdf_path)`

*   **Purpose:** Extracts the detailed hydraulic property tables (Elevation vs. Volume, Surface Area) associated with each *cell* in each 2D Flow Area.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `Dict[str, pd.DataFrame]`: Dictionary mapping mesh names to DataFrames. Each DataFrame contains columns ['Cell ID', 'Z', 'Volume', 'Surface Area'].

---

## Class: HdfPipe

Contains static methods for handling pipe network geometry and results data from HEC-RAS HDF files.

### `HdfPipe.get_pipe_conduits(hdf_path, crs="EPSG:4326")`

*   **Purpose:** Extracts pipe conduit centerlines, attributes, and terrain profiles from the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry or plan HDF).
    *   `crs` (`str`, optional): Coordinate Reference System string. Default "EPSG:4326".
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries ('Polyline'), attributes, and 'Terrain_Profiles' (list of (station, elevation) tuples).

### `HdfPipe.get_pipe_nodes(hdf_path)`

*   **Purpose:** Extracts pipe node locations and attributes from the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry or plan HDF).
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Point geometries and attributes.

### `HdfPipe.get_pipe_network(hdf_path, pipe_network_name=None, crs="EPSG:4326")`

*   **Purpose:** Extracts the detailed geometry of a specific pipe network, including cell polygons, faces, nodes, and connectivity information from the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry or plan HDF).
    *   `pipe_network_name` (`str`, optional): Name of the network. If `None`, uses the first network found.
    *   `crs` (`str`, optional): Coordinate Reference System string. Default "EPSG:4326".
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame primarily representing cells (Polygon geometry), with related face and node info included as attributes or object columns.
*   **Raises:** `ValueError` if `pipe_network_name` not found.

### `HdfPipe.get_pipe_profile(hdf_path, conduit_id)`

*   **Purpose:** Extracts the station-elevation terrain profile data for a specific pipe conduit from the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry or plan HDF).
    *   `conduit_id` (`int`): Zero-based index of the conduit.
*   **Returns:** `pd.DataFrame`: DataFrame with columns ['Station', 'Elevation'].
*   **Raises:** `KeyError`, `IndexError`.

### `HdfPipe.get_pipe_network_timeseries(hdf_path, variable)`

*   **Purpose:** Extracts time series results for a specified variable across all elements (cells, faces, pipes, nodes) of a pipe network.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `variable` (`str`): The results variable name (e.g., "Cell Water Surface", "Pipes/Pipe Flow DS", "Nodes/Depth").
*   **Returns:** `xr.DataArray`: DataArray with dimensions ('time', 'location') containing the time series values. Includes units attribute.
*   **Raises:** `ValueError` for invalid variable name, `KeyError`.

### `HdfPipe.get_pipe_network_summary(hdf_path)`

*   **Purpose:** Extracts summary statistics (min/max values, timing) for pipe network results from the plan results HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `pd.DataFrame`: DataFrame containing the summary statistics. Returns empty DataFrame if data not found.
*   **Raises:** `KeyError`.

### `HdfPipe.extract_timeseries_for_node(plan_hdf_path, node_id)`

*   **Purpose:** Extracts time series data specifically for a single pipe node (Depth, Drop Inlet Flow, Water Surface).
*   **Parameters:**
    *   `plan_hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `node_id` (`int`): Zero-based index of the node.
*   **Returns:** `Dict[str, xr.DataArray]`: Dictionary mapping variable names to their respective DataArrays (time dimension only).

### `HdfPipe.extract_timeseries_for_conduit(plan_hdf_path, conduit_id)`

*   **Purpose:** Extracts time series data specifically for a single pipe conduit (Flow US/DS, Velocity US/DS).
*   **Parameters:**
    *   `plan_hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `conduit_id` (`int`): Zero-based index of the conduit.
*   **Returns:** `Dict[str, xr.DataArray]`: Dictionary mapping variable names to their respective DataArrays (time dimension only).

---

## Class: HdfPlan

Contains static methods for extracting general plan-level information and attributes from HEC-RAS HDF files (plan or geometry HDF).

### `HdfPlan.get_plan_start_time(hdf_path)`

*   **Purpose:** Gets the simulation start time from the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan HDF.
*   **Returns:** (`datetime`): Simulation start time.
*   **Raises:** `ValueError`.

### `HdfPlan.get_plan_end_time(hdf_path)`

*   **Purpose:** Gets the simulation end time from the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan HDF.
*   **Returns:** (`datetime`): Simulation end time.
*   **Raises:** `ValueError`.

### `HdfPlan.get_plan_timestamps_list(hdf_path)`

*   **Purpose:** Gets the list of simulation output timestamps from the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan HDF.
*   **Returns:** `List[datetime]`: List of output datetime objects.
*   **Raises:** `ValueError`.

### `HdfPlan.get_plan_information(hdf_path)`

*   **Purpose:** Extracts all attributes from the 'Plan Data/Plan Information' group in the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan HDF.
*   **Returns:** `Dict[str, Any]`: Dictionary of plan information attributes.
*   **Raises:** `ValueError`.

### `HdfPlan.get_plan_parameters(hdf_path)`

*   **Purpose:** Extracts all attributes from the 'Plan Data/Plan Parameters' group in the plan HDF file and returns them as a DataFrame. Includes the plan number extracted from the filename.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan HDF.
*   **Returns:** `pd.DataFrame`: DataFrame with columns ['Plan', 'Parameter', 'Value'].
*   **Raises:** `ValueError`.

### `HdfPlan.get_plan_met_precip(hdf_path)`

*   **Purpose:** Extracts precipitation attributes from the 'Event Conditions/Meteorology/Precipitation' group in the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan HDF.
*   **Returns:** `Dict[str, Any]`: Dictionary of precipitation attributes. Returns empty dict if not found.

### `HdfPlan.get_geometry_information(hdf_path)`

*   **Purpose:** Extracts root-level attributes (like Version, Units, Projection) from the 'Geometry' group in a geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `pd.DataFrame`: DataFrame with columns ['Value'] and index ['Attribute Name'].
*   **Raises:** `ValueError`.

---

## Class: HdfPlot

Contains static methods for creating basic plots from HEC-RAS HDF data using `matplotlib`.

### `HdfPlot.plot_mesh_cells(cell_polygons_df, projection, title='2D Flow Area Mesh Cells', figsize=(12, 8))`

*   **Purpose:** Plots 2D mesh cell outlines from a GeoDataFrame.
*   **Parameters:**
    *   `cell_polygons_df` (`gpd.GeoDataFrame`): GeoDataFrame containing cell polygons (requires 'geometry' column).
    *   `projection` (`str`): CRS string to assign if `cell_polygons_df` doesn't have one.
    *   `title` (`str`, optional): Plot title. Default '2D Flow Area Mesh Cells'.
    *   `figsize` (`Tuple[int, int]`, optional): Figure size. Default (12, 8).
*   **Returns:** (`gpd.GeoDataFrame` or `None`): The input GeoDataFrame (with CRS possibly assigned), or `None` if input was empty. Displays the plot.

### `HdfPlot.plot_time_series(df, x_col, y_col, title=None, figsize=(12, 6))`

*   **Purpose:** Creates a simple line plot for time series data from a DataFrame.
*   **Parameters:**
    *   `df` (`pd.DataFrame`): DataFrame containing the data.
    *   `x_col` (`str`): Column name for the x-axis (usually time).
    *   `y_col` (`str`): Column name for the y-axis.
    *   `title` (`str`, optional): Plot title. Default `None`.
    *   `figsize` (`Tuple[int, int]`, optional): Figure size. Default (12, 6).
*   **Returns:** `None`. Displays the plot.

---

## Class: HdfPump

Contains static methods for handling pump station geometry and results data from HEC-RAS HDF files.

### `HdfPump.get_pump_stations(hdf_path)`

*   **Purpose:** Extracts pump station locations and attributes from the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry or plan HDF).
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with Point geometries and attributes including 'station_id'.
*   **Raises:** `KeyError`.

### `HdfPump.get_pump_groups(hdf_path)`

*   **Purpose:** Extracts pump group attributes and efficiency curve data from the geometry HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`): Path identifier for the HDF file (usually geometry or plan HDF).
*   **Returns:** `pd.DataFrame`: DataFrame containing pump group attributes and 'efficiency_curve' data (list of values).
*   **Raises:** `KeyError`.

### `HdfPump.get_pump_station_timeseries(hdf_path, pump_station)`

*   **Purpose:** Extracts time series results (Flow, Stage HW, Stage TW, Pumps On) for a specific pump station from the plan results HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `pump_station` (`str`): Name of the pump station as defined in HEC-RAS.
*   **Returns:** `xr.DataArray`: DataArray with dimensions ('time', 'variable') containing the time series. Includes units attribute.
*   **Raises:** `KeyError`, `ValueError` if pump station not found.

### `HdfPump.get_pump_station_summary(hdf_path)`

*   **Purpose:** Extracts summary statistics (min/max values, volumes, durations) for all pump stations from the plan results HDF.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `pd.DataFrame`: DataFrame containing the summary statistics. Returns empty DataFrame if data not found.
*   **Raises:** `KeyError`.

### `HdfPump.get_pump_operation_timeseries(hdf_path, pump_station)`

*   **Purpose:** Extracts detailed pump operation time series data (similar to `get_pump_station_timeseries` but often from a different HDF group, potentially DSS Profile Output) for a specific pump station. Returns as a DataFrame.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `pump_station` (`str`): Name of the pump station.
*   **Returns:** `pd.DataFrame`: DataFrame with columns ['Time', 'Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on'].
*   **Raises:** `KeyError`, `ValueError` if pump station not found.

---

## Class: HdfResultsMesh

Contains static methods for extracting and analyzing 2D mesh *results* data from HEC-RAS plan HDF files.

### `HdfResultsMesh.get_mesh_summary(hdf_path, var, round_to="100ms")`

*   **Purpose:** Extracts summary output (e.g., max/min values and times) for a specific variable across all cells/faces in all 2D areas. Merges with geometry (points for cells, lines for faces).
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `var` (`str`): The summary variable name (e.g., "Maximum Water Surface", "Maximum Face Velocity", "Cell Last Iteration").
    *   `round_to` (`str`, optional): Time rounding precision for timestamps. Default "100ms".
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame containing the summary results, geometry, and mesh/element IDs.
*   **Raises:** `ValueError`.

### `HdfResultsMesh.get_mesh_timeseries(hdf_path, mesh_name, var, truncate=True)`

*   **Purpose:** Extracts the full time series for a specific variable for all cells or faces within a *single* specified 2D mesh area.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `mesh_name` (`str`): Name of the 2D Flow Area.
    *   `var` (`str`): Results variable name (e.g., "Water Surface", "Face Velocity", "Depth").
    *   `truncate` (`bool`, optional): If `True`, remove trailing zero-value time steps. Default `True`.
*   **Returns:** `xr.DataArray`: DataArray with dimensions ('time', 'cell_id' or 'face_id') containing the time series. Includes units attribute.
*   **Raises:** `ValueError`.

### `HdfResultsMesh.get_mesh_faces_timeseries(hdf_path, mesh_name)`

*   **Purpose:** Extracts time series for all standard *face-based* variables ("Face Velocity", "Face Flow") for a specific mesh area.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `mesh_name` (`str`): Name of the 2D Flow Area.
*   **Returns:** `xr.Dataset`: Dataset containing DataArrays for each face variable, indexed by time and face_id.

### `HdfResultsMesh.get_mesh_cells_timeseries(hdf_path, mesh_names=None, var=None, truncate=False, ras_object=None)`

*   **Purpose:** Extracts time series for specified (or all) *cell-based* variables for specified (or all) mesh areas.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `mesh_names` (`str` or `List[str]`, optional): Name(s) of mesh area(s). If `None`, processes all.
    *   `var` (`str`, optional): Specific variable name. If `None`, retrieves all available cell and face variables.
    *   `truncate` (`bool`, optional): Remove trailing zero time steps. Default `False`.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `Dict[str, xr.Dataset]`: Dictionary mapping mesh names to Datasets containing the requested variable(s) as DataArrays, indexed by time and cell_id/face_id.
*   **Raises:** `ValueError`.

### `HdfResultsMesh.get_mesh_last_iter(hdf_path)`

*   **Purpose:** Shortcut to get the summary output for "Cell Last Iteration".
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `pd.DataFrame`: DataFrame containing the last iteration count for each cell (via `get_mesh_summary`).

### `HdfResultsMesh.get_mesh_max_ws(hdf_path, round_to="100ms")`

*   **Purpose:** Shortcut to get the summary output for "Maximum Water Surface".
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `round_to` (`str`, optional): Time rounding precision. Default "100ms".
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame containing max WSE and time for each cell (via `get_mesh_summary`).

### `HdfResultsMesh.get_mesh_min_ws(hdf_path, round_to="100ms")`

*   **Purpose:** Shortcut to get the summary output for "Minimum Water Surface".
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `round_to` (`str`, optional): Time rounding precision. Default "100ms".
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame containing min WSE and time for each cell (via `get_mesh_summary`).

### `HdfResultsMesh.get_mesh_max_face_v(hdf_path, round_to="100ms")`

*   **Purpose:** Shortcut to get the summary output for "Maximum Face Velocity".
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `round_to` (`str`, optional): Time rounding precision. Default "100ms".
*   **Returns:** `pd.DataFrame`: DataFrame containing max velocity and time for each face (via `get_mesh_summary`).

### `HdfResultsMesh.get_mesh_min_face_v(hdf_path, round_to="100ms")`

*   **Purpose:** Shortcut to get the summary output for "Minimum Face Velocity".
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `round_to` (`str`, optional): Time rounding precision. Default "100ms".
*   **Returns:** `pd.DataFrame`: DataFrame containing min velocity and time for each face (via `get_mesh_summary`).

### `HdfResultsMesh.get_mesh_max_ws_err(hdf_path, round_to="100ms")`

*   **Purpose:** Shortcut to get the summary output for "Cell Maximum Water Surface Error".
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `round_to` (`str`, optional): Time rounding precision. Default "100ms".
*   **Returns:** `pd.DataFrame`: DataFrame containing max WSE error and time for each cell (via `get_mesh_summary`).

### `HdfResultsMesh.get_mesh_max_iter(hdf_path, round_to="100ms")`

*   **Purpose:** Shortcut to get the summary output for "Cell Last Iteration" (often used as max iteration indicator).
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `round_to` (`str`, optional): Time rounding precision. Default "100ms".
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame containing max iteration count and time for each cell (via `get_mesh_summary`).

### `HdfResultsMesh.get_boundary_conditions_timeseries(hdf_path)`

*   **Purpose:** Extracts timeseries data for all boundary conditions as a single combined xarray Dataset with stage, flow, and per-face data.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `xr.Dataset`: Dataset containing boundary condition data with dimensions (time, bc_name, face_id) and variables (stage, flow, flow_per_face, stage_per_face).
*   **Raises:** `ValueError`.

---

## Class: HdfResultsPlan

Contains static methods for extracting general plan-level *results* and summary information from HEC-RAS plan HDF files.

### `HdfResultsPlan.get_unsteady_info(hdf_path)`

*   **Purpose:** Extracts attributes from the 'Results/Unsteady' group in the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `pd.DataFrame`: Single-row DataFrame containing the unsteady results attributes.
*   **Raises:** `FileNotFoundError`, `KeyError`, `RuntimeError`.

### `HdfResultsPlan.get_unsteady_summary(hdf_path)`

*   **Purpose:** Extracts attributes from the 'Results/Unsteady/Summary' group in the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `pd.DataFrame`: Single-row DataFrame containing the unsteady summary attributes.
*   **Raises:** `FileNotFoundError`, `KeyError`, `RuntimeError`.

### `HdfResultsPlan.get_volume_accounting(hdf_path)`

*   **Purpose:** Extracts attributes from the 'Results/Unsteady/Summary/Volume Accounting' group in the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** (`pd.DataFrame` or `None`): Single-row DataFrame containing volume accounting attributes, or `None` if the group doesn't exist.
*   **Raises:** `FileNotFoundError`, `RuntimeError`.

### `HdfResultsPlan.get_runtime_data(hdf_path)`

*   **Purpose:** Extracts detailed computational performance metrics (durations, speeds) for different simulation processes (Geometry, Preprocessing, Unsteady Flow) from the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** (`pd.DataFrame` or `None`): Single-row DataFrame containing runtime statistics, or `None` if data is missing or parsing fails.

### `HdfResultsPlan.get_compute_messages(hdf_path)`

*   **Purpose:** Extracts computation messages from HDF file with automatic fallback to .txt file extraction. Computation messages contain detailed information about the computation process, including warnings, errors, convergence information, and performance metrics.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF. Can be a plan number (e.g., "01"), which the decorator resolves to the HDF file path.
*   **Returns:** `str`: String containing computation messages, or empty string if unavailable.
*   **Raises:** No exceptions raised. Returns empty string gracefully if messages unavailable.
*   **HDF Path:** Reads from `/Results/Summary/Compute Messages (text)` dataset in the HDF file structure.
*   **Fallback Behavior:** If HDF path not found, automatically falls back to extracting from .txt files via `RasControl.get_comp_msgs()`. Logging messages (WARNING level) indicate when fallback occurs.
*   **Note:** Function naming follows modern HDF structure conventions (`compute_messages` matching the HDF dataset name), as opposed to `RasControl.get_comp_msgs()` which follows legacy naming.

**Example:**
```python
from ras_commander import init_ras_project
from ras_commander.HdfResultsPlan import HdfResultsPlan

# Initialize project with modern HEC-RAS version
init_ras_project(r"C:/models/BaldEagle/BaldEagle.prj", "6.5")

# Extract computation messages using plan number
msgs = HdfResultsPlan.get_compute_messages("01")

if msgs:
    print("Computation Messages:")
    print(msgs)

    # Can also parse specific information
    lines = msgs.split('\r\n')
    for line in lines:
        if 'error' in line.lower() or 'warning' in line.lower():
            print(f"⚠ {line}")
else:
    print("No computation messages available")
```

### `HdfResultsPlan.get_reference_timeseries(hdf_path, reftype)`

*   **Purpose:** Extracts time series results for all Reference Lines or Reference Points from the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `reftype` (`str`): Type of reference feature ('lines' or 'points').
*   **Returns:** `pd.DataFrame`: DataFrame containing time series data for the specified reference type. Each column represents a reference feature, indexed by time step. Returns empty DataFrame if data not found.

### `HdfResultsPlan.get_reference_summary(hdf_path, reftype)`

*   **Purpose:** Extracts summary results (e.g., max/min values) for all Reference Lines or Reference Points from the plan HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
    *   `reftype` (`str`): Type of reference feature ('lines' or 'points').
*   **Returns:** `pd.DataFrame`: DataFrame containing summary data for the specified reference type. Returns empty DataFrame if data not found.

---

## Class: HdfResultsPlot

Contains static methods for plotting specific HEC-RAS *results* data using `matplotlib`.

### `HdfResultsPlot.plot_results_max_wsel(max_ws_df)`

*   **Purpose:** Creates a scatter plot showing the spatial distribution of maximum water surface elevation (WSE) per mesh cell.
*   **Parameters:**
    *   `max_ws_df` (`gpd.GeoDataFrame`): GeoDataFrame containing max WSE results (requires 'geometry' and 'maximum_water_surface' columns, typically from `HdfResultsMesh.get_mesh_max_ws`).
*   **Returns:** `None`. Displays the plot.

### `HdfResultsPlot.plot_results_max_wsel_time(max_ws_df)`

*   **Purpose:** Creates a scatter plot showing the spatial distribution of the *time* at which maximum water surface elevation occurred for each mesh cell. Also prints timing statistics.
*   **Parameters:**
    *   `max_ws_df` (`gpd.GeoDataFrame`): GeoDataFrame containing max WSE results (requires 'geometry' and 'maximum_water_surface_time' columns, typically from `HdfResultsMesh.get_mesh_max_ws`).
*   **Returns:** `None`. Displays the plot and prints statistics.

### `HdfResultsPlot.plot_results_mesh_variable(variable_df, variable_name, colormap='viridis', point_size=10)`

*   **Purpose:** Creates a generic scatter plot for visualizing any scalar mesh variable (e.g., max depth, max velocity) spatially across cell points.
*   **Parameters:**
    *   `variable_df` (`gpd.GeoDataFrame` or `pd.DataFrame`): (Geo)DataFrame containing the variable data and either a 'geometry' column (Point) or 'x', 'y' columns.
    *   `variable_name` (`str`): The name of the column in `variable_df` containing the data to plot and label.
    *   `colormap` (`str`, optional): Matplotlib colormap name. Default 'viridis'.
    *   `point_size` (`int`, optional): Size of scatter plot points. Default 10.
*   **Returns:** `None`. Displays the plot.
*   **Raises:** `ValueError` if coordinates or variable column are missing.

---

## Class: HdfResultsXsec

Contains static methods for extracting 1D cross-section and related *results* data from HEC-RAS plan HDF files.

### `HdfResultsXsec.get_xsec_timeseries(hdf_path)`

*   **Purpose:** Extracts time series results (Water Surface, Velocity, Flow, etc.) for all 1D cross-sections. Includes cross-section attributes (River, Reach, Station) and calculated maximum values as coordinates/variables.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `xr.Dataset`: Dataset containing DataArrays for each variable, indexed by time and cross_section name/identifier. Includes coordinates for attributes and max values.
*   **Raises:** `KeyError`.

### `HdfResultsXsec.get_ref_lines_timeseries(hdf_path)`

*   **Purpose:** Extracts time series results (Flow, Velocity, Water Surface) for all 1D Reference Lines.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `xr.Dataset`: Dataset containing DataArrays for each variable, indexed by time and reference line ID/name. Returns empty dataset if data not found.
*   **Raises:** `FileNotFoundError`, `KeyError`.

### `HdfResultsXsec.get_ref_points_timeseries(hdf_path)`

*   **Purpose:** Extracts time series results (Flow, Velocity, Water Surface) for all 1D Reference Points.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='plan_hdf'`): Path identifier for the plan results HDF.
*   **Returns:** `xr.Dataset`: Dataset containing DataArrays for each variable, indexed by time and reference point ID/name. Returns empty dataset if data not found.
*   **Raises:** `FileNotFoundError`, `KeyError`.

---

## Class: HdfStruc

Contains static methods for extracting hydraulic structure *geometry* data from HEC-RAS HDF files (typically geometry HDF).

### `HdfStruc.get_structures(hdf_path, datetime_to_str=False)`

*   **Purpose:** Extracts geometry and attributes for all structures (bridges, culverts, inline structures, lateral structures) defined in the geometry HDF. Includes centerline geometry, profile data, and other specific attributes like bridge coefficients.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `datetime_to_str` (`bool`, optional): Convert datetime attributes to ISO strings. Default `False`.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries (centerlines) and numerous attribute columns, including nested profile data ('Profile_Data'). Returns empty GeoDataFrame if no structures found.

### `HdfStruc.get_geom_structures_attrs(hdf_path)`

*   **Purpose:** Extracts the top-level attributes associated with the 'Geometry/Structures' group in the geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
*   **Returns:** `pd.DataFrame`: Single-row DataFrame containing the group attributes. Returns empty DataFrame if group not found.

---

## Class: HdfUtils

Contains general static utility methods used for HDF processing, data conversion, and calculations.

### `HdfUtils.convert_ras_string(value)`

*   **Purpose:** Converts byte strings or regular strings potentially containing HEC-RAS specific formats (dates, durations, booleans) into appropriate Python objects (`bool`, `datetime`, `List[datetime]`, `timedelta`, `str`).
*   **Parameters:**
    *   `value` (`str` or `bytes`): Input string or byte string.
*   **Returns:** (`bool`, `datetime`, `List[datetime]`, `timedelta`, `str`): Converted Python object.

### `HdfUtils.convert_ras_hdf_value(value)`

*   **Purpose:** General converter for values read directly from HDF datasets (handles `np.nan`, byte strings, numpy types).
*   **Parameters:**
    *   `value` (`Any`): Value read from HDF.
*   **Returns:** (`None`, `bool`, `str`, `List[str]`, `int`, `float`, `List[int]`, `List[float]`): Converted Python object.

### `HdfUtils.convert_df_datetimes_to_str(df)`

*   **Purpose:** Converts all columns of dtype `datetime64` in a DataFrame to ISO format strings (`YYYY-MM-DD HH:MM:SS`).
*   **Parameters:**
    *   `df` (`pd.DataFrame`): Input DataFrame.
*   **Returns:** `pd.DataFrame`: DataFrame with datetime columns converted to strings.

### `HdfUtils.convert_hdf5_attrs_to_dict(attrs, prefix=None)`

*   **Purpose:** Converts HDF5 attributes (from `.attrs`) into a Python dictionary, applying `convert_ras_hdf_value` to each value.
*   **Parameters:**
    *   `attrs` (`h5py.AttributeManager` or `Dict`): Attributes object or dictionary.
    *   `prefix` (`str`, optional): Prefix to add to keys in the resulting dictionary.
*   **Returns:** `Dict[str, Any]`: Dictionary of converted attributes.

### `HdfUtils.convert_timesteps_to_datetimes(timesteps, start_time, time_unit="days", round_to="100ms")`

*   **Purpose:** Converts an array of numeric time steps (relative to a start time) into a pandas `DatetimeIndex`.
*   **Parameters:**
    *   `timesteps` (`np.ndarray`): Array of time step values.
    *   `start_time` (`datetime`): The reference start datetime.
    *   `time_unit` (`str`, optional): Unit of the `timesteps` ('days' or 'hours'). Default 'days'.
    *   `round_to` (`str`, optional): Pandas frequency string for rounding. Default '100ms'.
*   **Returns:** `pd.DatetimeIndex`: Index of datetime objects.

### `HdfUtils.perform_kdtree_query(reference_points, query_points, max_distance=2.0)`

*   **Purpose:** Finds nearest point in `reference_points` for each point in `query_points` using KDTree, within `max_distance`. Returns index or -1. (See `RasUtils` for identical function).
*   **Parameters:** See `RasUtils.perform_kdtree_query`.
*   **Returns:** (`np.ndarray`): Array of indices or -1.

### `HdfUtils.find_nearest_neighbors(points, max_distance=2.0)`

*   **Purpose:** Finds nearest neighbor for each point within the same dataset using KDTree, excluding self and points beyond `max_distance`. Returns index or -1. (See `RasUtils` for identical function).
*   **Parameters:** See `RasUtils.find_nearest_neighbors`.
*   **Returns:** (`np.ndarray`): Array of indices or -1.

### `HdfUtils.parse_ras_datetime(datetime_str)`

*   **Purpose:** Parses HEC-RAS standard datetime string format ("ddMMMYYYY HH:MM:SS").
*   **Parameters:**
    *   `datetime_str` (`str`): String to parse.
*   **Returns:** (`datetime`): Parsed datetime object.

### `HdfUtils.parse_ras_window_datetime(datetime_str)`

*   **Purpose:** Parses HEC-RAS simulation window datetime string format ("ddMMMYYYY HHMM").
*   **Parameters:**
    *   `datetime_str` (`str`): String to parse.
*   **Returns:** (`datetime`): Parsed datetime object.

### `HdfUtils.parse_duration(duration_str)`

*   **Purpose:** Parses HEC-RAS duration string format ("HH:MM:SS").
*   **Parameters:**
    *   `duration_str` (`str`): String to parse.
*   **Returns:** (`timedelta`): Parsed timedelta object.

### `HdfUtils.parse_ras_datetime_ms(datetime_str)`

*   **Purpose:** Parses HEC-RAS datetime string format that includes milliseconds ("ddMMMYYYY HH:MM:SS:fff").
*   **Parameters:**
    *   `datetime_str` (`str`): String to parse.
*   **Returns:** (`datetime`): Parsed datetime object with microseconds.

### `HdfUtils.parse_run_time_window(window)`

*   **Purpose:** Parses a HEC-RAS time window string ("datetime1 to datetime2") into start and end datetime objects.
*   **Parameters:**
    *   `window` (`str`): Time window string.
*   **Returns:** `Tuple[datetime, datetime]`: Tuple containing (start_datetime, end_datetime).


---

## Class: HdfXsec

Contains static methods for extracting 1D cross-section *geometry* data from HEC-RAS HDF files (typically geometry HDF).

### `HdfXsec.get_cross_sections(hdf_path, datetime_to_str=True, ras_object=None)`

*   **Purpose:** Extracts detailed cross-section geometry, attributes, station-elevation data, Manning's n values, and ineffective flow areas from the geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `datetime_to_str` (`bool`, optional): Convert datetime attributes to strings. Default `True`.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries (cross-section cut lines) and numerous attributes including nested lists/dicts for profile data ('station_elevation'), roughness ('mannings_n'), and ineffective areas ('ineffective_blocks').

### `HdfXsec.get_river_centerlines(hdf_path, datetime_to_str=False)`

*   **Purpose:** Extracts river centerline geometries and attributes from the geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `datetime_to_str` (`bool`, optional): Convert datetime attributes to strings. Default `False`.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries and attributes like 'River Name', 'Reach Name', 'length'.

### `HdfXsec.get_river_stationing(centerlines_gdf)`

*   **Purpose:** Calculates stationing values along river centerlines, interpolating points and determining direction based on upstream/downstream connections.
*   **Parameters:**
    *   `centerlines_gdf` (`gpd.GeoDataFrame`): GeoDataFrame obtained from `get_river_centerlines`.
*   **Returns:** `gpd.GeoDataFrame`: The input GeoDataFrame with added columns: 'station_start', 'station_end', 'stations' (array), 'points' (array of Shapely Points).

### `HdfXsec.get_river_reaches(hdf_path, datetime_to_str=False)`

*   **Purpose:** Extracts 1D river reach lines (often identical to centerlines but potentially simplified) from the geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `datetime_to_str` (`bool`, optional): Convert datetime attributes to strings. Default `False`.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries and attributes.

### `HdfXsec.get_river_edge_lines(hdf_path, datetime_to_str=False)`

*   **Purpose:** Extracts river edge lines (representing the extent of the 1D river schematic) from the geometry HDF file. Usually includes Left and Right edges.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `datetime_to_str` (`bool`, optional): Convert datetime attributes to strings. Default `False`.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries and attributes including 'bank_side' ('Left'/'Right').

### `HdfXsec.get_river_bank_lines(hdf_path, datetime_to_str=False)`

*   **Purpose:** Extracts river bank lines (defining the main channel within the cross-section) from the geometry HDF file.
*   **Parameters:**
    *   `hdf_path` (Input handled by `@standardize_input`, `file_type='geom_hdf'`): Path identifier for the geometry HDF.
    *   `datetime_to_str` (`bool`, optional): Convert datetime attributes to strings. Default `False`.
*   **Returns:** `gpd.GeoDataFrame`: GeoDataFrame with LineString geometries and attributes 'bank_id', 'bank_side'.

---

## Logging Configuration Functions

### `get_logger(name)`

*   **Purpose:** Retrieves a configured logger instance for use within the library or user scripts. Ensures logging is set up.
*   **Parameters:**
    *   `name` (`str`): Name for the logger (typically `__name__`).
*   **Returns:** (`logging.Logger`): A standard Python logger instance.

---

## Class: RasMap

Contains static methods for parsing and accessing information from HEC-RAS mapper configuration files (.rasmap) and automating post-processing tasks.

### `RasMap.parse_rasmap(rasmap_path: Union[str, Path], ras_object=None) -> pd.DataFrame`

*   **Purpose:** Parse a .rasmap file and extract relevant information, including paths to terrain, soil layers, land cover, and other spatial datasets.
*   **Parameters:**
    *   `rasmap_path` (`Union[str, Path]`): Path to the .rasmap file.
    *   `ras_object` (`RasPrj`, optional): Specific RAS object to use. If None, uses the global ras instance.
*   **Returns:** `pd.DataFrame`: A single-row DataFrame containing extracted information from the .rasmap file.
*   **Raises:** Various exceptions for file access or parsing failures.

### `RasMap.get_rasmap_path(ras_object=None) -> Optional[Path]`

*   **Purpose:** Get the path to the .rasmap file based on the current project.
*   **Parameters:**
    *   `ras_object` (`RasPrj`, optional): Specific RAS object to use. If None, uses the global ras instance.
*   **Returns:** `Optional[Path]`: Path to the .rasmap file if found, None otherwise.

### `RasMap.initialize_rasmap_df(ras_object=None) -> pd.DataFrame`

*   **Purpose:** Initialize the `rasmap_df` as part of project initialization. This is typically called internally by `init_ras_project`.
*   **Parameters:**
    *   `ras_object` (`RasPrj`, optional): Specific RAS object to use. If None, uses the global ras instance.
*   **Returns:** `pd.DataFrame`: DataFrame containing information from the .rasmap file.

### `RasMap.get_terrain_names(rasmap_path: Union[str, Path]) -> List[str]`
*   **Purpose:** Extracts all terrain layer names from a given `.rasmap` file.
*   **Parameters:**
    *   `rasmap_path` (`Union[str, Path]`): Path to the `.rasmap` file.
*   **Returns:** (`List[str]`): A list of terrain names.
*   **Raises:** `FileNotFoundError`, `ValueError`.

### `RasMap.postprocess_stored_maps(plan_number: Union[str, List[str]], specify_terrain: Optional[str] = None, layers: Union[str, List[str]] = None, ras_object: Optional[Any] = None) -> bool`
*   **Purpose:** Automates the generation of stored floodplain map outputs (e.g., `.tif` files) for a specific plan.
*   **Parameters:**
    *   `plan_number` (`Union[str, List[str]]`): Plan number(s) to generate maps for. Can be a single plan number as a string or a list of plan numbers for batch processing.
    *   `specify_terrain` (`Optional[str]`): The name of a specific terrain to use for mapping. If provided, other terrains are temporarily ignored.
    *   `layers` (`Union[str, List[str]]`, optional): A list of map layers to generate. Defaults to `['WSEL', 'Velocity', 'Depth']`.
    *   `ras_object` (`RasPrj`, optional): The RAS project object to use.
*   **Returns:** (`bool`): `True` if the process completed successfully, `False` otherwise.
*   **Workflow:**
    1.  Backs up the original plan and `.rasmap` files.
    2.  Modifies the plan file to only run floodplain mapping.
    3.  Modifies the `.rasmap` file to include the specified stored map layers.
    4.  Opens HEC-RAS GUI and waits for the user to manually execute the plan(s) using the 'Compute Multiple' window.
    5.  Completely restores the original plan and `.rasmap` files to their previous state after HEC-RAS closes.
==================================================

File: C:\GH\ras-commander\api-ras.md
==================================================
# RAS Commander API Documentation

This document provides a detailed reference for the public Application Programming Interface (API) of the `ras_commander` library. It lists all public classes and functions available for interacting with HEC-RAS projects.

## Introduction to Decorators

Many functions within the `ras_commander` library utilize decorators to provide common functionality like logging and input standardization. Understanding these decorators is key to using the API effectively.

### `@log_call`

*   **Purpose:** Automatically logs the entry and exit of a function call at the DEBUG level using the library's configured logger.
*   **Usage:** Applied to most public methods in the `Ras*` and `Hdf*` classes.
*   **Benefit:** Reduces boilerplate logging code and provides a consistent way to trace function execution for debugging purposes. You can configure the overall logging level using `logging.getLogger('ras_commander').setLevel(logging.LEVEL)`.

### `@standardize_input(file_type='plan_hdf'|'geom_hdf')`

*   **Purpose:** Standardizes the input for functions that operate on HEC-RAS HDF files (`.hdf`). It ensures that the function receives a validated `pathlib.Path` object pointing to the correct HDF file, regardless of the input format provided by the user.
*   **Usage:** Primarily used by methods within the `Hdf*` classes.
*   **Accepted Inputs:** The decorator can handle various input types for the HDF file path argument (usually the first argument or `hdf_path` keyword):
    *   `str`: A plan/geometry number (e.g., "01"), a plan prefix number (e.g., "p01"), or a full file path.
    *   `int`: A plan/geometry number (e.g., 1).
    *   `pathlib.Path`: A Path object pointing to the HDF file.
    *   `h5py.File`: An opened h5py File object (the decorator extracts the filename).
*   **`file_type` Argument:**
    *   `'plan_hdf'`: When resolving numbers, the decorator looks for the corresponding plan results HDF file (e.g., `ProjectName.p01.hdf`). This is the default.
    *   `'geom_hdf'`: When resolving numbers, the decorator looks for the corresponding geometry HDF file (e.g., `ProjectName.g01.hdf`).
*   **RAS Object Context:** The decorator uses the provided `ras_object` (or the global `ras` instance) to look up file paths when numbers are given as input. Ensure the relevant `RasPrj` object is initialized.
*   **Validation:** The decorator verifies that the resulting path points to an existing file before passing it to the decorated function.

---

## Class: RasPrj

Manages HEC-RAS project data and state. Provides access to project files, plans, geometries, flows, and boundary conditions. Can be used as a global `ras` object or instantiated for multi-project workflows.

### `RasPrj.__init__()`

*   **Purpose:** Constructor for RasPrj class. Creates an uninitialized instance.
*   **Parameters:** None.
*   **Returns:** `None`. Creates a new RasPrj instance with `initialized=False`.
*   **Note:** Call `initialize()` or use `init_ras_project()` to fully initialize the instance with project data.

### `RasPrj.initialize(project_folder, ras_exe_path, suppress_logging=True)`

*   **Purpose:** Initializes a `RasPrj` instance. **Note:** Users should typically call `init_ras_project()` instead.
*   **Parameters:**
    *   `project_folder` (`str` or `Path`): Path to the HEC-RAS project folder.
    *   `ras_exe_path` (`str` or `Path`): Path to the HEC-RAS executable.
    *   `suppress_logging` (`bool`, optional, default=`True`): Suppresses detailed initialization logs if True.
*   **Returns:** `None`. Modifies the instance in place.
*   **Raises:** `ValueError` if no `.prj` file is found.

### `RasPrj.check_initialized()`

*   **Purpose:** Checks if the `RasPrj` instance has been initialized.
*   **Parameters:** None.
*   **Returns:** `None`.
*   **Raises:** `RuntimeError` if the project is not initialized.

### `RasPrj.find_ras_prj(folder_path)`

*   **Purpose:** (Static method) Finds the main HEC-RAS project file (`.prj`) within a folder using various heuristics.
*   **Parameters:**
    *   `folder_path` (`str` or `Path`): Path to the folder to search.
*   **Returns:** `Path` object for the found `.prj` file, or `None` if not found.

### `RasPrj.get_project_name()`

*   **Purpose:** Gets the name of the initialized HEC-RAS project (filename without extension).
*   **Parameters:** None.
*   **Returns:** (`str`): The project name.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_prj_entries(entry_type)`

*   **Purpose:** Retrieves entries (plans, flows, geoms, unsteady) listed in the project file (`.prj`).
*   **Parameters:**
    *   `entry_type` (`str`): Type of entry ('Plan', 'Flow', 'Unsteady', 'Geom').
*   **Returns:** `pd.DataFrame`: DataFrame containing information about the specified entry type found in the project.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_plan_entries()`

*   **Purpose:** Retrieves all plan file entries (`.p*`) listed in the project file.
*   **Parameters:** None.
*   **Returns:** `pd.DataFrame`: DataFrame of plan entries with details parsed from plan files.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_flow_entries()`

*   **Purpose:** Retrieves all steady flow file entries (`.f*`) listed in the project file.
*   **Parameters:** None.
*   **Returns:** `pd.DataFrame`: DataFrame of steady flow entries.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_unsteady_entries()`

*   **Purpose:** Retrieves all unsteady flow file entries (`.u*`) listed in the project file.
*   **Parameters:** None.
*   **Returns:** `pd.DataFrame`: DataFrame of unsteady flow entries with details parsed from unsteady files.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_geom_entries()`

*   **Purpose:** Retrieves all geometry file entries (`.g*`) listed in the project file.
*   **Parameters:** None.
*   **Returns:** `pd.DataFrame`: DataFrame of geometry entries including paths to associated `.hdf` files.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_hdf_entries()`

*   **Purpose:** Retrieves plan entries that have a corresponding HDF results file (`.p*.hdf`).
*   **Parameters:** None.
*   **Returns:** `pd.DataFrame`: Filtered DataFrame of plan entries with existing HDF results files.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.print_data()`

*   **Purpose:** Prints a summary of the initialized project data (paths, file counts, dataframes) to the log (INFO level).
*   **Parameters:** None.
*   **Returns:** `None`.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj.get_plan_value(plan_number_or_path, key, ras_object=None)`

*   **Purpose:** (Static method, but often called on instance) Retrieves a specific value for a given key from a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number (e.g., "01") or full path to the plan file.
    *   `key` (`str`): The keyword in the plan file (e.g., 'Computation Interval', 'Short Identifier').
    *   `ras_object` (`RasPrj`, optional): Instance to use context from (project path). Defaults to global `ras`.
*   **Returns:** (`Any`): The value associated with the key, or `None` if not found. Type depends on the key (str, int).
*   **Raises:** `ValueError` if plan file not found, `IOError` on read error.

### `RasPrj.get_boundary_conditions()`

*   **Purpose:** Parses all unsteady flow files in the project to extract and structure boundary condition information.
*   **Parameters:** None.
*   **Returns:** `pd.DataFrame`: DataFrame containing detailed boundary condition data (location, type, parameters, associated unsteady file info). Returns empty DataFrame if no unsteady files or boundaries are found.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPrj` Attributes

*   `project_folder` (`Path`): Path to the project folder.
*   `project_name` (`str`): Name of the project.
*   `prj_file` (`Path`): Path to the project file.
*   `ras_exe_path` (`str`): Path to the HEC-RAS executable.
*   `plan_df` (`pd.DataFrame`): DataFrame containing plan file information.
*   `flow_df` (`pd.DataFrame`): DataFrame containing flow file information.
*   `unsteady_df` (`pd.DataFrame`): DataFrame containing unsteady flow file information.
*   `geom_df` (`pd.DataFrame`): DataFrame containing geometry file information.
*   `boundaries_df` (`pd.DataFrame`): DataFrame containing boundary condition information.
*   `rasmap_df` (`pd.DataFrame`): DataFrame containing RASMapper configuration data including paths to terrain, soil layer, infiltration, and land cover data.
*   `is_initialized` (`bool`): Read-only property indicating whether the RasPrj instance has been properly initialized with project data.

---

## Standalone Functions

These functions are available directly under the `ras_commander` import.

### `init_ras_project(ras_project_folder, ras_version=None, ras_object=None)`

*   **Purpose:** Primary function to initialize a `RasPrj` object (either global `ras` or a custom instance) for a specific HEC-RAS project.
*   **Parameters:**
    *   `ras_project_folder` (`str` or `Path`): Path to the HEC-RAS project folder.
    *   `ras_version` (`str`, optional): HEC-RAS version (e.g., "6.6") or full path to `Ras.exe`. Defaults to auto-detection or global setting.
    *   `ras_object` (`RasPrj`, optional): If `None`, initializes the global `ras` object. If a `RasPrj` instance, initializes that instance. If any other value (e.g., a string like "new"), creates and returns a *new* `RasPrj` instance. **Also updates the global `ras` object regardless.**
*   **Returns:** (`RasPrj`): The initialized `RasPrj` instance (either the one passed in, the global `ras`, or a newly created one).
*   **Raises:** `FileNotFoundError` if folder doesn't exist, `ValueError` if `.prj` file not found.

### `get_ras_exe(ras_version=None)`

*   **Purpose:** Determines the full path to the HEC-RAS executable based on version number or explicit path.
*   **Parameters:**
    *   `ras_version` (`str`, optional): Version string (e.g., "6.5") or full path to `Ras.exe`. If `None`, uses global `ras` object's path or defaults to "Ras.exe".
*   **Returns:** (`str`): Full path to the `Ras.exe` executable. Returns "Ras.exe" if lookup fails.

---

## Class: RasPlan

Contains static methods for operating on HEC-RAS plan files (`.p*`). Assumes a `RasPrj` object (defaulting to global `ras`) is initialized for context.

### `RasPlan.set_geom(plan_number, new_geom, ras_object=None)`

*   **Purpose:** Updates a plan file to use a different geometry file.
*   **Parameters:**
    *   `plan_number` (`str` or `int`): Plan number to modify (e.g., "01", 1).
    *   `new_geom` (`str` or `int`): Geometry number to assign (e.g., "02", 2).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `pd.DataFrame`: The updated *geometry* DataFrame of the `ras_object`.
*   **Raises:** `ValueError` if `new_geom` not found, `FileNotFoundError`, `IOError`.

### `RasPlan.set_steady(plan_number, new_steady_flow_number, ras_object=None)`

*   **Purpose:** Updates a plan file to use a specific steady flow file.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number (e.g., "01").
    *   `new_steady_flow_number` (`str`): Steady flow number (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and updates the `ras_object`.
*   **Raises:** `ValueError` if `new_steady_flow_number` not found, `FileNotFoundError`, `IOError`.

### `RasPlan.set_unsteady(plan_number, new_unsteady_flow_number, ras_object=None)`

*   **Purpose:** Updates a plan file to use a specific unsteady flow file.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number (e.g., "01").
    *   `new_unsteady_flow_number` (`str`): Unsteady flow number (e.g., "02").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and updates the `ras_object`.
*   **Raises:** `ValueError` if `new_unsteady_flow_number` not found, `FileNotFoundError`, `IOError`.

### `RasPlan.set_num_cores(plan_number_or_path, num_cores, ras_object=None)`

*   **Purpose:** Sets the number of cores (`UNET D1 Cores`, `UNET D2 Cores`, `PS Cores`) in a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `num_cores` (`int`): Number of cores to set (0 for 'All Available').
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and updates the `ras_object`.
*   **Raises:** `FileNotFoundError`, `IOError`.

### `RasPlan.set_geom_preprocessor(file_path, run_htab, use_ib_tables, ras_object=None)`

*   **Purpose:** Modifies the `Run HTab` and `UNET Use Existing IB Tables` settings in a plan file.
*   **Parameters:**
    *   `file_path` (`str` or `Path`): Full path to the plan file.
    *   `run_htab` (`int`): `0` (use existing) or `-1` (force recompute).
    *   `use_ib_tables` (`int`): `0` (use existing) or `-1` (force recompute).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and updates the `ras_object`.
*   **Raises:** `ValueError` for invalid flag values, `FileNotFoundError`, `IOError`.

### `RasPlan.clone_plan(template_plan, new_plan_shortid=None, ras_object=None)`

*   **Purpose:** Creates a new plan file by copying a template plan, assigns the next available plan number, optionally updates the Short Identifier, and updates the project file.
*   **Parameters:**
    *   `template_plan` (`str`): Plan number to use as template (e.g., "01").
    *   `new_plan_shortid` (`str`, optional): New Short Identifier (max 24 chars). If `None`, appends "_copy".
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The number of the newly created plan (e.g., "03").
*   **Raises:** `FileNotFoundError` if template not found, `IOError`.

### `RasPlan.clone_unsteady(template_unsteady, ras_object=None)`

*   **Purpose:** Creates a new unsteady flow file (`.u*` and associated `.hdf`) by copying a template, assigns the next available number, and updates the project file.
*   **Parameters:**
    *   `template_unsteady` (`str`): Unsteady flow number to use as template (e.g., "02").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The number of the newly created unsteady flow file (e.g., "03").
*   **Raises:** `FileNotFoundError` if template not found, `IOError`.

### `RasPlan.clone_steady(template_flow, ras_object=None)`

*   **Purpose:** Creates a new steady flow file (`.f*`) by copying a template, assigns the next available number, and updates the project file.
*   **Parameters:**
    *   `template_flow` (`str`): Steady flow number to use as template (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The number of the newly created steady flow file (e.g., "02").
*   **Raises:** `FileNotFoundError` if template not found, `IOError`.

### `RasPlan.clone_geom(template_geom, ras_object=None)`

*   **Purpose:** Creates a new geometry file (`.g*` and associated `.hdf`) by copying a template, assigns the next available number, and updates the project file.
*   **Parameters:**
    *   `template_geom` (`str`): Geometry number to use as template (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The number of the newly created geometry file (e.g., "03").
*   **Raises:** `FileNotFoundError` if template not found, `IOError`.

### `RasPlan.get_next_number(existing_numbers)`

*   **Purpose:** (Static utility) Finds the smallest unused positive integer number given a list of existing numbers (as strings), returned as a zero-padded string.
*   **Parameters:**
    *   `existing_numbers` (`list` of `str`): List of existing numbers (e.g., ['01', '03']).
*   **Returns:** (`str`): The next available number (e.g., "02").

### `RasPlan.get_plan_value(plan_number_or_path, key, ras_object=None)`

*   **Purpose:** Retrieves a specific value for a given key from a plan file. (See also `RasPrj.get_plan_value`).
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `key` (`str`): Keyword in the plan file (e.g., 'Computation Interval').
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`Any`): The value associated with the key, or `None` if not found. Type depends on the key.
*   **Raises:** `ValueError` if plan file not found, `IOError`.

### `RasPlan.get_results_path(plan_number, ras_object=None)`

*   **Purpose:** Gets the expected path to the HDF results file (`.p*.hdf`) for a given plan number. Checks if the file exists.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str` or `None`): Full path to the HDF results file if it exists, otherwise `None`.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPlan.get_plan_path(plan_number, ras_object=None)`

*   **Purpose:** Gets the full path to a plan file (`.p*`) given its number.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str` or `None`): Full path to the plan file, or `None` if not found in the project.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPlan.get_flow_path(flow_number, ras_object=None)`

*   **Purpose:** Gets the full path to a steady flow file (`.f*`) given its number.
*   **Parameters:**
    *   `flow_number` (`str`): Steady flow number (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str` or `None`): Full path to the flow file, or `None` if not found.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPlan.get_unsteady_path(unsteady_number, ras_object=None)`

*   **Purpose:** Gets the full path to an unsteady flow file (`.u*`) given its number.
*   **Parameters:**
    *   `unsteady_number` (`str`): Unsteady flow number (e.g., "02").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str` or `None`): Full path to the unsteady file, or `None` if not found.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPlan.get_geom_path(geom_number, ras_object=None)`

*   **Purpose:** Gets the full path to a geometry file (`.g*`) given its number.
*   **Parameters:**
    *   `geom_number` (`str`): Geometry number (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str` or `None`): Full path to the geometry file, or `None` if not found.
*   **Raises:** `RuntimeError` if not initialized.

### `RasPlan.update_run_flags(plan_number_or_path, geometry_preprocessor=None, unsteady_flow_simulation=None, run_sediment=None, post_processor=None, floodplain_mapping=None, ras_object=None)`

*   **Purpose:** Updates the run flags (e.g., `Run HTab`, `Run UNet`, `Run RASMapper`) in a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `geometry_preprocessor` (`bool`, optional): Set `Run HTab` (True=1, False=0).
    *   `unsteady_flow_simulation` (`bool`, optional): Set `Run UNet` (True=1, False=0).
    *   `run_sediment` (`bool`, optional): Set `Run Sediment` (True=1, False=0).
    *   `post_processor` (`bool`, optional): Set `Run PostProcess` (True=1, False=0).
    *   `floodplain_mapping` (`bool`, optional): Set `Run RASMapper` (True=0, False=-1). **Note inverted logic for RASMapper**.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.update_plan_intervals(plan_number_or_path, computation_interval=None, output_interval=None, instantaneous_interval=None, mapping_interval=None, ras_object=None)`

*   **Purpose:** Updates time intervals (computation, output, mapping, etc.) in a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `computation_interval` (`str`, optional): E.g., "1MIN", "10SEC", "1HOUR".
    *   `output_interval` (`str`, optional): E.g., "1HOUR", "30MIN".
    *   `instantaneous_interval` (`str`, optional): E.g., "1HOUR", "15MIN".
    *   `mapping_interval` (`str`, optional): E.g., "1HOUR", "15MIN".
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file.
*   **Raises:** `ValueError` if plan not found or interval invalid, `IOError`.

### `RasPlan.update_plan_description(plan_number_or_path, description, ras_object=None)`

*   **Purpose:** Updates the multi-line description block within a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `description` (`str`): The new description text (can be multi-line).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and updates the `ras_object`.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.read_plan_description(plan_number_or_path, ras_object=None)`

*   **Purpose:** Reads the multi-line description block from a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The description text, or "" if not found.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.update_simulation_date(plan_number_or_path, start_date, end_date, ras_object=None)`

*   **Purpose:** Updates the simulation start and end date/time in a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `start_date` (`datetime`): Simulation start datetime object.
    *   `end_date` (`datetime`): Simulation end datetime object.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and updates the `ras_object`.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.get_shortid(plan_number_or_path, ras_object=None)`

*   **Purpose:** Gets the 'Short Identifier' value from a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The Short Identifier, or "" if not found.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.set_shortid(plan_number_or_path, new_shortid, ras_object=None)`

*   **Purpose:** Sets the 'Short Identifier' value in a plan file (max 24 chars).
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `new_shortid` (`str`): New identifier (will be truncated if > 24 chars).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.get_plan_title(plan_number_or_path, ras_object=None)`

*   **Purpose:** Gets the 'Plan Title' value from a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`str`): The Plan Title, or "" if not found.
*   **Raises:** `ValueError` if plan not found, `IOError`.

### `RasPlan.set_plan_title(plan_number_or_path, new_title, ras_object=None)`

*   **Purpose:** Sets the 'Plan Title' value in a plan file.
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `new_title` (`str`): New title.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file.
*   **Raises:** `ValueError` if plan not found, `IOError`.

---

## Class: RasGeo

Contains static methods for operating on HEC-RAS geometry files (`.g*`) and associated preprocessor files. Assumes a `RasPrj` object (defaulting to global `ras`) is initialized.

### `RasGeo.clear_geompre_files(plan_files=None, ras_object=None)`

*   **Purpose:** Deletes geometry preprocessor files (`.c*`) associated with specified plan files. This forces HEC-RAS to recompute hydraulic tables based on the geometry. **Note:** Does not currently clear IB tables or HDF geometry tables.
*   **Parameters:**
    *   `plan_files` (`str`, `Path`, `List[Union[str, Path]]`, optional): Plan file path(s) or number(s). If `None`, clears for all plans in the project.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Deletes files and updates the `ras_object`'s geometry DataFrame.
*   **Raises:** `PermissionError`, `OSError`.

### `RasGeo.get_mannings_baseoverrides(geom_file_path)`

*   **Purpose:** Reads the base Manning's n table from a HEC-RAS geometry file.
*   **Parameters:**
    *   `geom_file_path` (Input handled by `@standardize_input`): Path identifier for the geometry file (.g##).
*   **Returns:** `pd.DataFrame`: DataFrame with Table Number, Land Cover Name, and Base Manning's n Value.

### `RasGeo.get_mannings_regionoverrides(geom_file_path)`

*   **Purpose:** Reads the Manning's n region overrides from a HEC-RAS geometry file.
*   **Parameters:**
    *   `geom_file_path` (Input handled by `@standardize_input`): Path identifier for the geometry file (.g##).
*   **Returns:** `pd.DataFrame`: DataFrame with Table Number, Land Cover Name, MainChannel value, and Region Name.

### `RasGeo.set_mannings_baseoverrides(geom_file_path, mannings_data)`

*   **Purpose:** Writes base Manning's n values to a HEC-RAS geometry file.
*   **Parameters:**
    *   `geom_file_path` (Input handled by `@standardize_input`): Path identifier for the geometry file (.g##).
    *   `mannings_data` (`pd.DataFrame`): DataFrame with columns 'Table Number', 'Land Cover Name', and 'Base Manning\'s n Value'.
*   **Returns:** `bool`: True if successful.

### `RasGeo.set_mannings_regionoverrides(geom_file_path, mannings_data)`

*   **Purpose:** Writes regional Manning's n overrides to a HEC-RAS geometry file.
*   **Parameters:**
    *   `geom_file_path` (Input handled by `@standardize_input`): Path identifier for the geometry file (.g##).
    *   `mannings_data` (`pd.DataFrame`): DataFrame with columns 'Table Number', 'Land Cover Name', 'MainChannel', and 'Region Name'.
*   **Returns:** `bool`: True if successful.

---

## Class: RasUnsteady

Contains static methods for operating on HEC-RAS unsteady flow files (`.u*`). Assumes a `RasPrj` object (defaulting to global `ras`) is initialized.

### `RasUnsteady.update_flow_title(unsteady_file, new_title, ras_object=None)`

*   **Purpose:** Updates the 'Flow Title' line within an unsteady flow file (max 24 chars).
*   **Parameters:**
    *   `unsteady_file` (`str` or `Path`): Unsteady flow number or full path.
    *   `new_title` (`str`): The new title (will be truncated if > 24 chars).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the file and updates the `ras_object`.
*   **Raises:** `FileNotFoundError`, `PermissionError`, `IOError`.

### `RasUnsteady.update_restart_settings(unsteady_file, use_restart, restart_filename=None, ras_object=None)`

*   **Purpose:** Enables or disables the use of a restart file (`.rst`) in an unsteady flow file.
*   **Parameters:**
    *   `unsteady_file` (`str` or `Path`): Unsteady flow number or full path.
    *   `use_restart` (`bool`): `True` to enable restart, `False` to disable.
    *   `restart_filename` (`str`, optional): Path to the `.rst` file (required if `use_restart` is `True`).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the file and updates the `ras_object`.
*   **Raises:** `ValueError` if `restart_filename` missing, `FileNotFoundError`, `PermissionError`, `IOError`.

### `RasUnsteady.extract_boundary_and_tables(unsteady_file, ras_object=None)`

*   **Purpose:** Parses an unsteady flow file to extract boundary condition definitions and associated time-series data tables (e.g., Flow Hydrograph).
*   **Parameters:**
    *   `unsteady_file` (`str` or `Path`): Unsteady flow number or full path.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `pd.DataFrame`: DataFrame where each row represents a boundary condition. Includes columns for location (`River Name`, `Reach Name`, etc.), `DSS File`, and a `Tables` column containing a dictionary of `pd.DataFrame` objects for each time-series table found.
*   **Raises:** `FileNotFoundError`, `PermissionError`.

### `RasUnsteady.print_boundaries_and_tables(boundaries_df)`

*   **Purpose:** Prints the boundary conditions and tables (extracted by `extract_boundary_and_tables`) to the console in a readable format.
*   **Parameters:**
    *   `boundaries_df` (`pd.DataFrame`): DataFrame returned by `extract_boundary_and_tables`.
*   **Returns:** `None`.

### `RasUnsteady.identify_tables(lines)`

*   **Purpose:** (Static utility) Scans lines from an unsteady file and identifies the name and line ranges of data tables.
*   **Parameters:**
    *   `lines` (`List[str]`): List of lines read from the unsteady file.
*   **Returns:** `List[Tuple[str, int, int]]`: List of tuples `(table_name, start_line_index, end_line_index)`.

### `RasUnsteady.parse_fixed_width_table(lines, start, end)`

*   **Purpose:** (Static utility) Parses the fixed-width numeric data within identified table lines.
*   **Parameters:**
    *   `lines` (`List[str]`): List of lines read from the unsteady file.
    *   `start` (`int`): Starting line index (inclusive) of the table data.
    *   `end` (`int`): Ending line index (exclusive) of the table data.
*   **Returns:** `pd.DataFrame`: DataFrame with a single 'Value' column containing the numeric data.

### `RasUnsteady.extract_tables(unsteady_file, ras_object=None)`

*   **Purpose:** Extracts all numeric data tables (like hydrographs, gate openings) from an unsteady file into a dictionary of DataFrames.
*   **Parameters:**
    *   `unsteady_file` (`str` or `Path`): Unsteady flow number or full path.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `Dict[str, pd.DataFrame]`: Dictionary mapping table names (e.g., 'Flow Hydrograph=') to DataFrames containing the table values.
*   **Raises:** `FileNotFoundError`, `PermissionError`.

### `RasUnsteady.write_table_to_file(unsteady_file, table_name, df, start_line, ras_object=None)`

*   **Purpose:** Writes a modified DataFrame back into an unsteady flow file, formatting it correctly into the fixed-width structure required by HEC-RAS.
*   **Parameters:**
    *   `unsteady_file` (`str` or `Path`): Unsteady flow number or full path.
    *   `table_name` (`str`): Name of the table being written (must match the header in the file).
    *   `df` (`pd.DataFrame`): DataFrame containing the new values (must have a 'Value' column).
    *   `start_line` (`int`): Line index where the original table data started.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the file.
*   **Raises:** `FileNotFoundError`, `PermissionError`, `IOError`.

---

## Class: RasUtils

Contains general static utility functions used across the `ras_commander` library.

### `RasUtils.create_directory(directory_path, ras_object=None)`

*   **Purpose:** Ensures a directory exists, creating it (and any parent directories) if necessary.
*   **Parameters:**
    *   `directory_path` (`Path`): The directory path to ensure.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`Path`): The ensured directory path.
*   **Raises:** `OSError` on creation failure.

### `RasUtils.find_files_by_extension(extension, ras_object=None)`

*   **Purpose:** Lists all files within the initialized project directory matching a specific extension.
*   **Parameters:**
    *   `extension` (`str`): The file extension (e.g., '.prj', '.p*').
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`list` of `str`): List of full file paths.

### `RasUtils.get_file_size(file_path, ras_object=None)`

*   **Purpose:** Gets the size of a file in bytes.
*   **Parameters:**
    *   `file_path` (`Path`): Path to the file.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`int` or `None`): File size in bytes, or `None` if file not found.

### `RasUtils.get_file_modification_time(file_path, ras_object=None)`

*   **Purpose:** Gets the last modification timestamp of a file.
*   **Parameters:**
    *   `file_path` (`Path`): Path to the file.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`float` or `None`): Unix timestamp of last modification, or `None` if file not found.

### `RasUtils.get_plan_path(current_plan_number_or_path, ras_object=None)`

*   **Purpose:** Resolves a plan number or path string into a full, validated `Path` object for a plan file.
*   **Parameters:**
    *   `current_plan_number_or_path` (`str` or `Path`): Plan number (1-99) or full path.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`Path`): The validated path to the plan file.
*   **Raises:** `ValueError`, `TypeError`, `FileNotFoundError`.

### `RasUtils.remove_with_retry(path, max_attempts=5, initial_delay=1.0, is_folder=True, ras_object=None)`

*   **Purpose:** Safely removes a file or folder, retrying with exponential backoff if a `PermissionError` occurs.
*   **Parameters:**
    *   `path` (`Path`): Path to remove.
    *   `max_attempts` (`int`, optional): Max removal attempts. Default is 5.
    *   `initial_delay` (`float`, optional): Initial delay in seconds. Default is 1.0.
    *   `is_folder` (`bool`, optional): `True` if path is a folder, `False` if a file. Default is `True`.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** (`bool`): `True` if removal succeeded, `False` otherwise.

### `RasUtils.update_plan_file(plan_number_or_path, file_type, entry_number, ras_object=None)`

*   **Purpose:** Updates a line in a plan file to reference a different associated file (geometry, flow, unsteady).
*   **Parameters:**
    *   `plan_number_or_path` (`str` or `Path`): Plan number or full path.
    *   `file_type` (`str`): Type of file link to update ('Geom', 'Flow', 'Unsteady').
    *   `entry_number` (`int`): The number (1-99) of the new associated file.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the plan file and refreshes the `ras_object`.
*   **Raises:** `ValueError`, `FileNotFoundError`.

### `RasUtils.check_file_access(file_path, mode='r')`

*   **Purpose:** Verifies if a file exists and can be accessed with the specified mode ('r', 'w', etc.).
*   **Parameters:**
    *   `file_path` (`Path`): Path to the file.
    *   `mode` (`str`, optional): Access mode to check. Default is 'r'.
*   **Returns:** `None`.
*   **Raises:** `FileNotFoundError`, `PermissionError`.

### `RasUtils.convert_to_dataframe(data_source, **kwargs)`

*   **Purpose:** Converts various inputs (existing DataFrame, CSV, Excel, TSV, Parquet file path) into a pandas DataFrame.
*   **Parameters:**
    *   `data_source` (`pd.DataFrame` or `Path`): Input data or file path.
    *   `**kwargs`: Additional arguments for pandas read functions (e.g., `sheet_name` for Excel).
*   **Returns:** `pd.DataFrame`.
*   **Raises:** `NotImplementedError` for unsupported types.

### `RasUtils.save_to_excel(dataframe, excel_path, **kwargs)`

*   **Purpose:** Saves a DataFrame to an Excel file, with retries to handle potential file locking issues.
*   **Parameters:**
    *   `dataframe` (`pd.DataFrame`): DataFrame to save.
    *   `excel_path` (`Path`): Output Excel file path.
    *   `**kwargs`: Additional arguments for `DataFrame.to_excel()`.
*   **Returns:** `None`.
*   **Raises:** `IOError` if saving fails after retries.

### `RasUtils.calculate_rmse(observed_values, predicted_values, normalized=True)`

*   **Purpose:** Calculates Root Mean Squared Error (RMSE), optionally normalized.
*   **Parameters:**
    *   `observed_values` (`np.ndarray`): Array of actual values.
    *   `predicted_values` (`np.ndarray`): Array of predicted values.
    *   `normalized` (`bool`, optional): If `True`, normalize by the mean of observed values. Default is `True`.
*   **Returns:** (`float`): Calculated RMSE.

### `RasUtils.calculate_percent_bias(observed_values, predicted_values, as_percentage=False)`

*   **Purpose:** Calculates Percent Bias (PBIAS).
*   **Parameters:**
    *   `observed_values` (`np.ndarray`): Array of actual values.
    *   `predicted_values` (`np.ndarray`): Array of predicted values.
    *   `as_percentage` (`bool`, optional): If `True`, returns result multiplied by 100. Default is `False`.
*   **Returns:** (`float`): Calculated Percent Bias.

### `RasUtils.calculate_error_metrics(observed_values, predicted_values)`

*   **Purpose:** Calculates a dictionary of common error metrics (correlation, RMSE, Percent Bias).
*   **Parameters:**
    *   `observed_values` (`np.ndarray`): Array of actual values.
    *   `predicted_values` (`np.ndarray`): Array of predicted values.
*   **Returns:** `Dict[str, float]`: Dictionary with keys 'cor', 'rmse', 'pb'.

### `RasUtils.update_file(file_path, update_function, *args)`

*   **Purpose:** Generic function to read a file, apply a modification function to its lines, and write it back.
*   **Parameters:**
    *   `file_path` (`Path`): Path to the file.
    *   `update_function` (`Callable`): A function that takes a list of lines (and optionally `*args`) and returns a modified list of lines.
    *   `*args`: Additional arguments passed to `update_function`.
*   **Returns:** `None`.
*   **Raises:** Exceptions from file I/O or `update_function`.

### `RasUtils.get_next_number(existing_numbers)`

*   **Purpose:** Finds the smallest unused positive integer number given a list of existing numbers (as strings), returned as a zero-padded string. (Same as `RasPlan.get_next_number`)
*   **Parameters:**
    *   `existing_numbers` (`list` of `str`): List of existing numbers (e.g., ['01', '03']).
*   **Returns:** (`str`): The next available number (e.g., "02").

### `RasUtils.clone_file(template_path, new_path, update_function=None, *args)`

*   **Purpose:** Copies a template file to a new path and optionally applies a modification function to the new file's content.
*   **Parameters:**
    *   `template_path` (`Path`): Path to the source file.
    *   `new_path` (`Path`): Path for the new copied file.
    *   `update_function` (`Callable`, optional): Function to modify the lines of the new file. Takes a list of lines (and optionally `*args`).
    *   `*args`: Additional arguments for `update_function`.
*   **Returns:** `None`.
*   **Raises:** `FileNotFoundError` if template doesn't exist.

### `RasUtils.update_project_file(prj_file, file_type, new_num, ras_object=None)`

*   **Purpose:** Appends a new file entry line (e.g., `Plan File=p03`) to the project file (`.prj`).
*   **Parameters:**
    *   `prj_file` (`Path`): Path to the `.prj` file.
    *   `file_type` (`str`): Type of entry ('Plan', 'Geom', 'Flow', 'Unsteady').
    *   `new_num` (`str`): The two-digit number for the new entry (e.g., "03").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`. Modifies the project file.

### `RasUtils.decode_byte_strings(dataframe)`

*   **Purpose:** Decodes all byte string (`b'...'`) columns in a DataFrame to UTF-8 strings.
*   **Parameters:**
    *   `dataframe` (`pd.DataFrame`): Input DataFrame.
*   **Returns:** `pd.DataFrame`: DataFrame with byte strings decoded.

### `RasUtils.perform_kdtree_query(reference_points, query_points, max_distance=2.0)`

*   **Purpose:** Finds the nearest point in `reference_points` for each point in `query_points` using KDTree, within a maximum distance.
*   **Parameters:**
    *   `reference_points` (`np.ndarray`): NxD array of reference points.
    *   `query_points` (`np.ndarray`): MxD array of query points.
    *   `max_distance` (`float`, optional): Maximum distance for a match. Default 2.0.
*   **Returns:** (`np.ndarray`): Array of length M containing indices of nearest reference points. Index is -1 if no point found within `max_distance`.

### `RasUtils.find_nearest_neighbors(points, max_distance=2.0)`

*   **Purpose:** Finds the nearest neighbor for each point within the same dataset using KDTree, excluding self-matches and points beyond `max_distance`.
*   **Parameters:**
    *   `points` (`np.ndarray`): NxD array of points.
    *   `max_distance` (`float`, optional): Maximum distance for a match. Default 2.0.
*   **Returns:** (`np.ndarray`): Array of length N containing indices of the nearest neighbor for each point. Index is -1 if no neighbor found within `max_distance`.

### `RasUtils.consolidate_dataframe(dataframe, group_by=None, pivot_columns=None, level=None, n_dimensional=False, aggregation_method='list')`

*   **Purpose:** Aggregates rows in a DataFrame based on grouping criteria, typically merging values into lists.
*   **Parameters:**
    *   `dataframe` (`pd.DataFrame`): Input DataFrame.
    *   `group_by` (`str` or `List[str]`, optional): Column(s) or index level(s) to group by.
    *   `pivot_columns` (`str` or `List[str]`, optional): Column(s) to use for pivoting (if `n_dimensional`).
    *   `level` (`int`, optional): Index level to group by.
    *   `n_dimensional` (`bool`, optional): Use `pivot_table` if `True`. Default `False`.
    *   `aggregation_method` (`str` or `Callable`, optional): How to aggregate ('list', 'sum', 'mean', etc.). Default 'list'.
*   **Returns:** `pd.DataFrame`: The consolidated DataFrame.

### `RasUtils.find_nearest_value(array, target_value)`

*   **Purpose:** Finds the element in an array that is numerically closest to a target value.
*   **Parameters:**
    *   `array` (`list` or `np.ndarray`): Array of numbers to search within.
    *   `target_value` (`int` or `float`): The value to find the nearest match for.
*   **Returns:** (`int` or `float`): The value from the array closest to `target_value`.

### `RasUtils.horizontal_distance(coord1, coord2)`

*   **Purpose:** Calculates the 2D Euclidean distance between two points.
*   **Parameters:**
    *   `coord1` (`np.ndarray`): [X, Y] coordinates of the first point.
    *   `coord2` (`np.ndarray`): [X, Y] coordinates of the second point.
*   **Returns:** (`float`): The horizontal distance.

---

## Class: RasExamples

Provides methods to download, manage, and access HEC-RAS example projects included with the official HEC-RAS releases. Useful for testing and demonstration.

### `RasExamples.get_example_projects(version_number='6.6')`

*   **Purpose:** Downloads the example projects zip file for a specific HEC-RAS version if it doesn't already exist locally. Initializes the class to read the zip file structure.
*   **Parameters:**
    *   `version_number` (`str`, optional): HEC-RAS version string (e.g., "6.6", "6.5"). Default is "6.6".
*   **Returns:** (`Path`): Path to the directory where projects will be extracted (`example_projects`).
*   **Raises:** `ValueError` for invalid version, `requests.exceptions.RequestException` on download failure.

### `RasExamples.list_categories()`

*   **Purpose:** Lists the categories (top-level folders) available in the example projects zip file.
*   **Parameters:** None.
*   **Returns:** (`List[str]`): List of category names.

### `RasExamples.list_projects(category=None)`

*   **Purpose:** Lists the project names available, optionally filtered by category.
*   **Parameters:**
    *   `category` (`str`, optional): If provided, lists projects only within this category.
*   **Returns:** (`List[str]`): List of project names.

### `RasExamples.extract_project(project_names, output_path=None)`

*   **Purpose:** Extracts one or more specified projects from the zip file. Overwrites if already extracted.
*   **Parameters:**
    *   `project_names` (`str` or `List[str]`): Name(s) of the project(s) to extract.
    *   `output_path` (`str` or `Path`, optional): Path where the project folder will be placed. Can be a relative path (creates subfolder in current directory) or an absolute path. If `None`, uses default `example_projects` folder.
*   **Returns:** (`Path` or `List[Path]`): Path(s) to the extracted project folder(s). Returns a single `Path` if one name was given, a list otherwise.
*   **Raises:** `ValueError` if a project name is not found.

### `RasExamples.is_project_extracted(project_name)`

*   **Purpose:** Checks if a specific project has already been extracted into the `example_projects` directory.
*   **Parameters:**
    *   `project_name` (`str`): Name of the project to check.
*   **Returns:** (`bool`): `True` if the project folder exists, `False` otherwise.

### `RasExamples.clean_projects_directory()`

*   **Purpose:** Removes the entire `example_projects` directory and its contents, then recreates the empty directory.
*   **Parameters:** None.
*   **Returns:** `None`.

### `RasExamples.download_fema_ble_model(huc8, output_dir=None)`

*   **Purpose:** (Placeholder) Intended to download FEMA Base Level Engineering models. *Currently not implemented.*
*   **Parameters:**
    *   `huc8` (`str`): 8-digit HUC code.
    *   `output_dir` (`str`, optional): Directory to save files.
*   **Returns:** (`str`): Path to the extracted model directory.

---

## Class: RasControl

Provides legacy HEC-RAS version support via the HECRASController COM interface. Wraps the COM API with ras-commander style conventions (plan numbers instead of file paths). Supports HEC-RAS versions 3.1, 4.1, 5.0.x (501-507), 6.0, 6.3, 6.6. **Requires version specification when initializing project with `init_ras_project()`.**

### Key Features

*   **Plan Number Interface**: Use plan numbers (e.g., "02") instead of file paths
*   **Auto-sets Current Plan**: Automatically sets the current plan before operations
*   **Steady & Unsteady Support**: Extract both steady state profiles and unsteady time series
*   **Open-Operate-Close Pattern**: Opens HEC-RAS, performs operation, closes completely (no GUI left open)
*   **Blocking Execution**: `run_plan()` waits for computation to complete before returning
*   **Version Migration**: Perfect for validating model migration across HEC-RAS versions

### `RasControl.run_plan(plan_number, ras_object=None)`

*   **Purpose:** Runs a HEC-RAS plan using the COM interface. Automatically sets the plan as current, starts computation, waits for completion, and closes HEC-RAS.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number to run (e.g., "01", "02").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`. **Must be initialized with a version string** (e.g., `init_ras_project(path, "4.1")`).
*   **Returns:** `Tuple[bool, List[str]]`: `(success, messages)` where `success` is `True` if computation completed, `messages` is a list of computation messages from HEC-RAS.
*   **Raises:** `RuntimeError` if RAS not initialized with version, `ValueError` if plan not found, `Exception` from COM interface errors.
*   **Note:** Blocks until computation completes. Terminates any remaining ras.exe processes after closing.

### `RasControl.get_steady_results(plan_number, ras_object=None)`

*   **Purpose:** Extracts steady state profile results (WSE, velocity, flow, etc.) from a completed HEC-RAS plan using the COM interface.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number to extract results from (e.g., "02").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `pd.DataFrame`: DataFrame with columns: `river`, `reach`, `node_id`, `profile`, `wsel`, `min_ch_el`, `velocity`, `flow`, `froude`, `energy`, `max_depth`. One row per cross-section per profile.
*   **Raises:** `RuntimeError` if RAS not initialized with version, `ValueError` if plan not found or is not steady, `Exception` from COM interface errors.
*   **Note:** Automatically sets the plan as current before extraction. Closes HEC-RAS after extraction.

### `RasControl.get_unsteady_results(plan_number, max_times=None, ras_object=None)`

*   **Purpose:** Extracts unsteady time series results from a completed HEC-RAS plan using the COM interface. Includes special "Max WS" timestep containing maximum values from any computational timestep.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number to extract results from (e.g., "01").
    *   `max_times` (`int`, optional): Maximum number of timesteps to extract. If `None`, extracts all output times. **Note:** This limits extracted timesteps, not computational timesteps.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `pd.DataFrame`: DataFrame with columns: `river`, `reach`, `node_id`, `time_index`, `time_string`, `wsel`, `min_ch_el`, `velocity`, `flow`, `froude`, `energy`, `max_depth`. First row (time_index=1) is always "Max WS" containing peak values from entire simulation.
*   **Raises:** `RuntimeError` if RAS not initialized with version, `ValueError` if plan not found or is not unsteady, `Exception` from COM interface errors.
*   **Note:** Automatically sets the plan as current before extraction. Closes HEC-RAS after extraction. Filter `time_string != 'Max WS'` for time series plotting.

### `RasControl.get_output_times(plan_number, ras_object=None)`

*   **Purpose:** Retrieves the list of output times available for an unsteady plan.
*   **Parameters:**
    *   `plan_number` (`str`): Plan number to query (e.g., "01").
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `List[str]`: List of time strings (e.g., `["Max WS", "18FEB1999 0000", "18FEB1999 0200", ...]`). First entry is always "Max WS".
*   **Raises:** `RuntimeError` if RAS not initialized with version, `ValueError` if plan not found, `Exception` from COM interface errors.
*   **Note:** Automatically sets the plan as current. Closes HEC-RAS after query.

### `RasControl.set_current_plan(plan_title, ras_object=None)`

*   **Purpose:** Sets the current plan in HEC-RAS by plan title (as shown in the GUI).
*   **Parameters:**
    *   `plan_title` (`str`): The "Plan Title" from the plan file (not the plan number or short ID).
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `None`.
*   **Raises:** `RuntimeError` if RAS not initialized with version, `ValueError` if plan title not found, `Exception` from COM interface errors.
*   **Note:** This is a low-level method. Most users should rely on automatic plan setting in `run_plan()`, `get_steady_results()`, and `get_unsteady_results()`.

### `RasControl.get_comp_msgs(plan, ras_object=None)`

*   **Purpose:** Extracts computation messages from .txt file with automatic fallback to HDF extraction. Computation messages contain detailed information about the computation process, including warnings, errors, convergence information, and performance metrics.
*   **Parameters:**
    *   `plan` (`str` or `Path`): Plan number (e.g., "01", "02") or path to .prj file.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
*   **Returns:** `str`: String containing computation messages, or empty string if unavailable.
*   **Raises:** No exceptions raised. Returns empty string gracefully if messages unavailable.
*   **Version Support:**
    *   HEC-RAS 3.x-5.x: Reads from `.comp_msgs.txt` file
    *   HEC-RAS 6.x+: Reads from `.computeMsgs.txt` file
    *   Falls back to HDF extraction if .txt files not found
*   **Note:** This method checks for both .txt naming patterns and automatically falls back to HDF if neither exists. Logging messages (WARNING level) indicate when fallback occurs. Function naming follows legacy HECRASController conventions (`comp_msgs` abbreviation).

**Example:**
```python
from ras_commander import init_ras_project, RasControl

# Initialize project with legacy version
init_ras_project(r"C:/models/BaldEagle/BaldEagle.prj", "4.1")

# Extract computation messages
msgs = RasControl.get_comp_msgs("01")

if msgs:
    print("Computation Messages:")
    print(msgs)
else:
    print("No computation messages available")
```

### Version Support and Initialization

RasControl requires the HEC-RAS version to be specified when initializing a project:

```python
# Initialize with version string
init_ras_project(project_path, "4.1")     # HEC-RAS 4.1
init_ras_project(project_path, "5.0.6")   # HEC-RAS 5.0.6
init_ras_project(project_path, "6.6")     # HEC-RAS 6.6

# Flexible version formats accepted
init_ras_project(project_path, "41")      # Same as "4.1"
init_ras_project(project_path, "506")     # Same as "5.0.6"
init_ras_project(project_path, "66")      # Same as "6.6"
```

**Supported Versions:** 3.1, 4.1, 5.0.1-5.0.7, 6.0, 6.3, 6.6

### Understanding "Max WS" in Unsteady Results

When extracting unsteady results, the first row always contains `time_string="Max WS"` and `time_index=1`. This special timestep contains the **maximum values that occurred at ANY computational timestep** during the simulation, not just at output intervals.

**Why this matters:**
- Output intervals (e.g., every 1 hour) may miss the peak
- Computational timesteps (e.g., every 30 seconds) capture the true maximum
- "Max WS" provides the absolute peak values for the entire simulation

**Usage pattern:**
```python
# Extract unsteady results
df_unsteady = RasControl.get_unsteady_results("01", max_times=20)

# Separate Max WS from time series
max_ws = df_unsteady[df_unsteady['time_string'] == 'Max WS']
timeseries = df_unsteady[df_unsteady['time_string'] != 'Max WS']

# Plot time series with Max WS as reference
import matplotlib.pyplot as plt
xs_data = timeseries[timeseries['node_id'] == '12345']
plt.plot(xs_data['time_index'], xs_data['wsel'], label='WSE')
plt.axhline(max_ws[max_ws['node_id'] == '12345']['wsel'].iloc[0],
            color='r', linestyle='--', label='Max WS')
```

### When to Use RasControl vs RasCmdr

| Use RasControl | Use RasCmdr |
|----------------|-------------|
| HEC-RAS 3.x - 4.x | HEC-RAS 6.0+ |
| No HDF file support needed | Need HDF data access |
| Version migration validation | Modern workflows |
| Legacy model support | Better performance |
| Steady profile extraction | 2D mesh analysis |

For HEC-RAS 6.0+, prefer using HDF-based methods (`HdfResultsPlan`, `HdfResultsXsec`) for better performance and more detailed data access.

---

## Class: RasCmdr

Contains static methods for executing HEC-RAS simulations via command line interface. Assumes a `RasPrj` object (defaulting to global `ras`) is initialized.

### `RasCmdr.compute_plan(plan_number, dest_folder=None, ras_object=None, clear_geompre=False, num_cores=None, overwrite_dest=False)`

*   **Purpose:** Executes a single HEC-RAS plan computation using the command line. Optionally copies the project to a destination folder first.
*   **Parameters:**
    *   `plan_number` (`str` or `Path`): Plan number (e.g., "01") or full path to plan file.
    *   `dest_folder` (`str` or `Path`, optional): Folder name (relative to project parent) or full path for computation. If `None`, runs in the original project folder.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
    *   `clear_geompre` (`bool`, optional): Clear `.c*` files before running. Default `False`.
    *   `num_cores` (`int`, optional): Number of cores to set in the plan file before running. Default `None` (use plan's current setting).
    *   `overwrite_dest` (`bool`, optional): If `True`, overwrite `dest_folder` if it exists. Default `False`.
*   **Returns:** (`bool`): `True` if execution succeeded (process completed without error), `False` otherwise.
*   **Raises:** `ValueError` if `dest_folder` exists and `overwrite_dest` is False, `FileNotFoundError`, `PermissionError`, `subprocess.CalledProcessError`.

### `RasCmdr.compute_parallel(plan_number=None, max_workers=2, num_cores=2, clear_geompre=False, ras_object=None, dest_folder=None, overwrite_dest=False)`

*   **Purpose:** Executes multiple HEC-RAS plans in parallel by creating temporary worker copies of the project. Consolidates results into a final destination folder.
*   **Parameters:**
    *   `plan_number` (`str` or `List[str]`, optional): Plan number(s) to run. If `None`, runs all plans in the project.
    *   `max_workers` (`int`, optional): Max number of parallel HEC-RAS instances. Default 2.
    *   `num_cores` (`int`, optional): Cores assigned to *each* worker instance. Default 2.
    *   `clear_geompre` (`bool`, optional): Clear `.c*` files in worker folders. Default `False`.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
    *   `dest_folder` (`str` or `Path`, optional): Final folder for consolidated results. If `None`, creates `ProjectName [Computed]` folder.
    *   `overwrite_dest` (`bool`, optional): Overwrite `dest_folder` if it exists. Default `False`.
*   **Returns:** `Dict[str, bool]`: Dictionary mapping plan numbers to their execution success status (`True`/`False`).
*   **Raises:** `ValueError`, `FileNotFoundError`, `PermissionError`, `RuntimeError`.

### `RasCmdr.compute_test_mode(plan_number=None, dest_folder_suffix="[Test]", clear_geompre=False, num_cores=None, ras_object=None, overwrite_dest=False)`

*   **Purpose:** Executes specified HEC-RAS plans sequentially within a dedicated test folder (a copy of the project).
*   **Parameters:**
    *   `plan_number` (`str` or `List[str]`, optional): Plan number(s) to run. If `None`, runs all plans.
    *   `dest_folder_suffix` (`str`, optional): Suffix for the test folder name (e.g., `ProjectName [Test]`). Default "[Test]".
    *   `clear_geompre` (`bool`, optional): Clear `.c*` files before running each plan. Default `False`.
    *   `num_cores` (`int`, optional): Cores to set for each plan execution. Default `None`.
    *   `ras_object` (`RasPrj`, optional): Instance for context. Defaults to global `ras`.
    *   `overwrite_dest` (`bool`, optional): Overwrite test folder if it exists. Default `False`.
*   **Returns:** `Dict[str, bool]`: Dictionary mapping plan numbers to their execution success status (`True`/`False`).
*   **Raises:** `ValueError`, `FileNotFoundError`, `PermissionError`.

---


==================================================

File: C:\GH\ras-commander\api.md
==================================================
# RAS Commander API Documentation

The `ras_commander` library API documentation has been split into two files for easier navigation:

## API Documentation Files

### [api-ras.md](api-ras.md) - HEC-RAS Classes and Functions
Complete reference for HEC-RAS project management, plan execution, and file operations:
- **RasPrj**: Project initialization and management
- **RasPlan**: Plan file operations and modifications
- **RasGeo**: Geometry file operations
- **RasUnsteady**: Unsteady flow file management
- **RasUtils**: Utility functions
- **RasExamples**: Example project management
- **RasCmdr**: Plan execution and computation
- **RasMap**: RASMapper configuration and post-processing
- **Standalone Functions**: `init_ras_project()`, `get_ras_exe()`

### [api-hdf.md](api-hdf.md) - HDF Data Extraction and Analysis Classes
Complete reference for extracting and analyzing data from HEC-RAS HDF files:
- **HdfBase**: Core HDF file operations
- **HdfBndry**: Boundary condition geometry
- **HdfFluvialPluvial**: Fluvial-pluvial boundary analysis
- **HdfInfiltration**: Infiltration data handling
- **HdfMesh**: 2D mesh geometry extraction
- **HdfPipe**: Pipe network geometry and results
- **HdfPlan**: Plan-level information and metadata
- **HdfPlot**: Basic plotting utilities
- **HdfPump**: Pump station geometry and results
- **HdfResultsMesh**: 2D mesh results extraction
- **HdfResultsPlan**: Plan-level results (both unsteady and **steady state**)
- **HdfResultsPlot**: Results plotting utilities
- **HdfResultsXsec**: 1D cross-section results
- **HdfStruc**: Hydraulic structure geometry
- **HdfUtils**: HDF utility functions
- **HdfXsec**: 1D cross-section geometry

## Quick Start

For getting started with the library, see the [README.md](README.md) and example notebooks in the `examples/` directory.

## New Features

### Steady State Support
The library now includes full support for steady state flow analysis. See the new steady state methods in `HdfResultsPlan`:
- `is_steady_plan()` - Check if HDF contains steady state results
- `get_steady_profile_names()` - Extract profile names
- `get_steady_wse()` - Extract water surface elevations
- `get_steady_info()` - Extract steady flow metadata

For usage examples, see the steady state example notebook in `examples/`.

==================================================

File: C:\GH\ras-commander\CLAUDE.md
==================================================
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## AGENTS.md Standard (Read Me First)

This repository now uses the AGENTS.md standard alongside CLAUDE.md.

- Treat any `AGENTS.md` in the current folder (and its parents up to the repo root) the same way you treat `CLAUDE.md` files.
- Scope and precedence:
  - The `AGENTS.md` at a folder applies to that folder and all subfolders.
  - More-deeply-nested `AGENTS.md` files override parent guidance when conflicts arise.
  - Direct, explicit instructions in the task or doc you’re reading take precedence.
- Always read both `CLAUDE.md` and `AGENTS.md` for the working directory before editing code or running tools.
- Key locations with scoped guidance:
  - `/AGENTS.md` (root)
  - `/ras_commander/AGENTS.md` (library API and coding conventions)
  - `/ras_commander/remote/AGENTS.md` (remote execution subpackage)
  - `/examples/AGENTS.md` (notebook index and extraction workflow)
  - `/examples/data/AGENTS.md` (small reference datasets; read-only)
  - `/examples/example_projects/AGENTS.md` (large HEC‑RAS examples; do not commit extractions)
  - `/ai_tools/AGENTS.md` (maintainer-only; ignore when using the library)

Important highlights from AGENTS.md:
- Ignore `ai_tools/` and any generated knowledge bases; they are maintainer-only.
- Prefer cleaned notebooks; otherwise treat notebooks as reference and extract logic into scripts.
- Use a single uv-managed venv at the repo root and install in editable mode (`uv venv .venv`, `uv pip install -e .`).

## Project Overview

**ras-commander** is a Python library for automating HEC-RAS (Hydrologic Engineering Center's River Analysis System) operations. It provides a comprehensive API for interacting with HEC-RAS project files, executing simulations, and processing results through HDF data analysis.

## Development Environment

### Build Commands
- **Build package**: `python setup.py sdist bdist_wheel`
- **Clean build**: Remove `build/`, `dist/`, and `*.egg-info` directories before building
- **Install locally**: `pip install -e .` (for development)
- **Install from build**: `pip install ras-commander`

### Testing Strategy
- Uses **Test Driven Development** with HEC-RAS example projects instead of traditional unit tests
- Example projects are located in `examples/` and `tests/example_projects/`
- Test scripts are in `tests/` directory (e.g., `test_ras_examples_initialization.py`)
- All example notebooks in `examples/` serve as both documentation and functional tests
- Run example tests: `python test_ras_examples_initialization.py`

### Dependencies
- **Python**: Requires 3.10+
- **Core packages**: h5py, numpy, pandas, geopandas, matplotlib, shapely, scipy, xarray, tqdm, requests, rasterstats, rtree
- **Legacy support**: pywin32>=227 (COM interface), psutil>=5.6.6 (process management)
- **Optional**: tkinterdnd2 (for GUI applications with drag-and-drop)
- Install dependencies: `pip install h5py numpy pandas requests tqdm scipy xarray geopandas matplotlib shapely pathlib rasterstats rtree pywin32 psutil`

### Environment Management
- Supports both pip and uv for package management
- Development pattern: Clone repo and use `sys.path.append()` method (see examples for import pattern)
- Create virtual environment recommended: `python -m venv venv` or `conda create`

## Architecture Overview

### Core Classes and Execution Model

**Primary Execution Class**: `RasCmdr` - Static class for HEC-RAS plan execution
- `RasCmdr.compute_plan(plan_number, dest_folder=None, ras_object=None, clear_geompre=False, num_cores=None, overwrite_dest=False)`
- `RasCmdr.compute_parallel()` - Parallel execution across multiple worker processes
- `RasCmdr.compute_test_mode()` - Sequential execution in test environment

**Legacy Version Support**: `RasControl` - COM interface for HEC-RAS 3.x-4.x
- Uses ras-commander style API (plan numbers, not file paths)
- Integrates with `init_ras_project()` and `ras` object
- Open-operate-close pattern prevents conflicts with modern workflows
- Supported versions: 3.1, 4.1, 5.0.x (501, 503, 505, 506), 6.0, 6.3, 6.6
- Public methods (ras-commander style):
  - `init_ras_project(path, "4.1")` - Initialize with version
  - `RasControl.run_plan("02")` - Run plan by number
  - `RasControl.get_steady_results("02")` - Extract steady profiles
  - `RasControl.get_unsteady_results("01", max_times=10)` - Extract time series
  - `RasControl.get_output_times("01")` - List unsteady timesteps
  - `RasControl.set_current_plan("Steady Flow Run")` - Switch active plan
- Private methods handle all COM interface details
- See `examples/17_extracting_profiles_with_hecrascontroller.ipynb` for complete usage

**Project Management**: `RasPrj` class and global `ras` object
- Initialize projects: `init_ras_project(path, ras_version, ras_object=None)`
- Global `ras` object available for single-project workflows
- Multiple project support via separate `RasPrj` instances

**File Operations Classes**:
- `RasPlan` - Plan file operations and modifications
- `RasGeo` - Geometry file operations (2D Manning's n land cover)
- `RasGeometry` - Geometry parsing (1D cross sections, storage, connections)
- `RasGeometryUtils` - Geometry parsing utilities (fixed-width, count interpretation)
- `RasStruct` - Inline structure parsing (bridges, culverts, inline weirs) **NEW**
- `RasBreach` - Breach parameter modification in plan files
- `RasUnsteady` - Unsteady flow file management
- `RasUtils` - Utility functions for file operations
- `RasMap` - RASMapper configuration parsing
- `RasExamples` - HEC-RAS example project management and extraction

**HDF Data Processing Classes**:
- `HdfBase`, `HdfPlan`, `HdfMesh` - Core HDF file operations
- `HdfResults*` classes - Results processing and analysis (unsteady and **steady state**)
- `HdfStruc` - Structure data and SA/2D connection listings
- `HdfResultsBreach` - Dam breach results extraction from HDF files **NEW**
- `HdfHydraulicTables` - Cross section property tables (HTAB) **NEW**
- `HdfPipe`, `HdfPump` - Infrastructure analysis
- `HdfPlot`, `HdfResultsPlot` - Visualization utilities

**New Steady State Support** (as of v0.80.3+):
- `HdfResultsPlan.is_steady_plan()` - Check if HDF contains steady state results
- `HdfResultsPlan.get_steady_profile_names()` - Extract steady state profile names
- `HdfResultsPlan.get_steady_wse()` - Extract water surface elevations for profiles
- `HdfResultsPlan.get_steady_info()` - Extract steady flow metadata
- See `examples/19_steady_flow_analysis.ipynb` for complete usage examples

**New Geometry Parsing Support** (as of v0.81.0+):
- `RasGeometry` - Comprehensive geometry parsing and modification
  - Cross sections: `get_cross_sections()`, `get_station_elevation()`, `set_station_elevation()`
  - Manning's n: `get_mannings_n()`, bank stations, expansion/contraction coefficients
  - Storage areas: `get_storage_areas()`, `get_storage_elevation_volume()`
  - Lateral structures: `get_lateral_structures()`, `get_lateral_weir_profile()`
  - SA/2D connections: `get_connections()`, `get_connection_weir_profile()`, `get_connection_gates()`
- `HdfHydraulicTables` - Extract property tables (HTAB) from preprocessed geometry HDF
  - `get_xs_htab()` - Area, conveyance, wetted perimeter vs elevation
  - Enables rating curves without re-running HEC-RAS
- **Critical Features**: Automatic bank station interpolation, 450 point limit enforcement
- See `research/geometry file parsing/api-geom.md` for complete API reference
- See `research/geometry file parsing/example_notebooks/02_complete_geometry_operations.ipynb`

**Dam Breach Operations** (as of v0.81.0+):
- **Architectural Pattern**: Plain text (Ras*) vs HDF (Hdf*) separation
- `RasBreach` - Breach PARAMETERS in plan files (.p##)
  - `list_breach_structures_plan()` - List structures from plan file
  - `read_breach_block()` - Read breach parameters
  - `update_breach_block()` - Modify breach parameters
- `HdfResultsBreach` - Breach RESULTS from HDF files (.p##.hdf)
  - `get_breach_timeseries()` - Complete time series (flow, stage, geometry)
  - `get_breach_summary()` - Summary statistics (peaks, timing)
  - `get_breaching_variables()` - Breach geometry evolution
  - `get_structure_variables()` - Structure flow variables
- **Important**: Use plan file names for parameters, HDF may have different naming
- See `examples/18_breach_results_extraction.ipynb` for workflow examples

**DSS File Operations** (as of v0.82.0+):
- `RasDss` - Read HEC-DSS files (V6 and V7) for boundary conditions **NEW**
  - `get_catalog(dss_file)` - List all paths in DSS file
  - `read_timeseries(dss_file, pathname)` - Read time series as DataFrame
  - `read_multiple_timeseries(dss_file, pathnames)` - Batch read multiple paths
  - `extract_boundary_timeseries(boundaries_df, ras_object)` - Auto-extract all DSS boundary data
  - `get_info(dss_file)` - File summary and statistics
- **Technology**: HEC Monolith libraries (auto-downloaded on first use, ~17 MB)
- **Java Bridge**: pyjnius (pip installable, lazy loaded)
- **Dependencies**: Requires `pip install pyjnius` and Java 8+ (JRE or JDK)
- **Lazy Loading**: No overhead unless DSS methods are called
- **Tested**: 84 DSS files (6.64 GB), 88% success rate, handles files up to 1.3 GB
- See `examples/22_dss_boundary_extraction.ipynb` for complete workflow

**Inline Structure Parsing** (as of v0.84.0+):
- `RasStruct` - Parse inline structures from geometry files (.g##) **NEW**
  - Inline Weirs: `get_inline_weirs()`, `get_inline_weir_profile()`, `get_inline_weir_gates()`
  - Bridges: `get_bridges()`, `get_bridge_deck()`, `get_bridge_piers()`, `get_bridge_abutment()`
  - Bridge Details: `get_bridge_approach_sections()`, `get_bridge_coefficients()`, `get_bridge_htab()`
  - Culverts: `get_culverts()`, `get_all_culverts()`
- **Culvert Shape Codes**: 1=Circular, 2=Box, 3=Pipe Arch, 4=Ellipse, 5=Arch, 6=Semi-Circle, 7=Low Profile Arch, 8=High Profile Arch, 9=Con Span
- **Parsing**: Uses 8-character fixed-width and comma-separated formats
- See `research/geometry file parsing/api-geom.md` for complete API reference

### Execution Modes
1. **Single Plan**: `RasCmdr.compute_plan()` - Execute one plan with full parameter control
2. **Parallel**: `RasCmdr.compute_parallel()` - Run multiple plans simultaneously using worker folders
3. **Sequential**: `RasCmdr.compute_test_mode()` - Run multiple plans in order in test folder
4. **Distributed**: `compute_parallel_remote()` - Execute plans across remote machines via PsExec/SSH/cloud

**Remote Execution Subpackage**: `ras_commander.remote` (as of v0.85.0+)
- Refactored from single module to subpackage with lazy loading
- `init_ras_worker()` - Factory function to create and validate remote workers
- `compute_parallel_remote()` - Execute plans across distributed worker pool
- **PsexecWorker** - Windows remote execution via PsExec over network shares (✓ implemented)
- **Future workers**: SshWorker, LocalWorker, WinrmWorker, DockerWorker, SlurmWorker, AwsEc2Worker, AzureFrWorker
- Worker abstraction enables heterogeneous execution (local + remote + cloud simultaneously)
- Queue-aware wave scheduling across all worker types
- **Lazy loading**: Remote module only loaded when `init_ras_worker` or workers are accessed
- **Optional dependencies**: `pip install ras-commander[remote-ssh]`, `[remote-aws]`, `[remote-all]`
- See `examples/23_remote_execution_psexec.ipynb` for complete usage
- See `feature_dev_notes/RasRemote/REMOTE_WORKER_SETUP_GUIDE.md` for setup instructions
- See `ras_commander/remote/AGENTS.md` for subpackage-specific guidance

**Critical for HEC-RAS Remote Execution:**
- HEC-RAS is a GUI application and requires session-based execution
- Use `session_id=2` (typical for workstations), NOT `system_account=True`
- Remote machine requires Group Policy configuration (network access, local logon, batch job rights)
- Registry key `LocalAccountTokenFilterPolicy=1` required
- Remote Registry service must be running
- User must be in Administrators group

## Key Development Patterns

### Static Class Pattern
- Most classes use static methods with `@log_call` decorators
- No instantiation required: `RasCmdr.compute_plan()` not `RasCmdr().compute_plan()`
- Pass `ras_object` parameter when working with multiple projects

### Import Flexibility Pattern
```python
# Flexible imports for development vs installed package
try:
    from ras_commander import init_ras_project, RasCmdr, RasPlan
except ImportError:
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    from ras_commander import init_ras_project, RasCmdr, RasPlan
```

### Path Handling
- Use `pathlib.Path` objects consistently
- Support both string paths and Path objects in function parameters
- Handle Windows paths with proper escaping

### Error Handling and Logging
- Comprehensive logging using centralized `LoggingConfig`
- Use `@log_call` decorator for automatic function call logging
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs written to both console and rotating file (`ras_commander.log`)

### Naming Conventions
- **Functions/Variables**: snake_case (`compute_plan`, `plan_number`)
- **Classes**: PascalCase (`RasCmdr`, `HdfBase`)
- **Constants**: UPPER_CASE
- **Abbreviations**: ras, prj, geom, geompre, num, init, BC, IC, TW

### Function Naming: Technological Lineage

Function names in ras-commander reflect the conventions of their underlying technology source. This creates consistency with the data structures and APIs they interact with.

#### RasControl Class - Legacy HEC-RAS Conventions
- **Naming Style**: Abbreviated, matching HECRASController COM interface
- **Example**: `get_comp_msgs()` uses "comp_msgs" to match legacy .txt filename convention
- **Rationale**: Maintains consistency with pre-5.x HEC-RAS terminology and file naming
- **Versions**: Targets HEC-RAS 3.x-4.x (with 5.x/6.x fallback support)

#### HdfResultsPlan Class - Modern HDF Structure Conventions
- **Naming Style**: Descriptive, matching HDF dataset/group names
- **Example**: `get_compute_messages()` uses "compute_messages" to match HDF path naming
- **Rationale**: Function names mirror the actual HDF structure: `Results/Summary/Compute Messages (text)`
- **Pattern**: Read HDF file structure and derive Python method names from group/dataset names
- **Versions**: Targets HEC-RAS 6.x+ HDF-based results

#### Why Names Differ Between Classes
The naming differences are **intentional and contextually appropriate**:
- `RasControl.get_comp_msgs()` - reflects `.comp_msgs.txt` and `.computeMsgs.txt` file conventions
- `HdfResultsPlan.get_compute_messages()` - reflects `Compute Messages (text)` HDF dataset naming

This is not inconsistency, but **technological lineage** - each reflects its underlying data source conventions. Users benefit from predictable mapping between Python methods and the data structures they access.

#### Guideline for New Functions
When adding new functions:
1. **For RasControl**: Match legacy HEC-RAS/HECRASController naming conventions
2. **For HdfResultsPlan**: Inspect HDF structure and mirror group/dataset names
3. **For other classes**: Follow snake_case with descriptive, unabbreviated names

### Documentation Standards
- Comprehensive docstrings with Args, Returns, Raises, Examples sections
- Include usage examples in all major functions
- Document parameter types and optional parameters clearly

## Working with HEC-RAS Projects

### Project Initialization
```python
# Single project (uses global ras object)
init_ras_project(r"/path/to/project", "6.5")

# Multiple projects
project1 = RasPrj()
init_ras_project(r"/path/to/project1", "6.5", ras_object=project1)
```

### Accessing Project Data
- `ras.plan_df` - Plans dataframe
- `ras.geom_df` - Geometry files dataframe  
- `ras.flow_df` - Flow files dataframe
- `ras.unsteady_df` - Unsteady files dataframe
- `ras.boundaries_df` - Boundary conditions dataframe

### Plan Execution Parameters
- `plan_number`: Plan ID (string, e.g., "01", "02")
- `dest_folder`: Optional computation folder (None = modify original)
- `clear_geompre`: Clear geometry preprocessor files (Boolean)
- `num_cores`: Number of processing cores (Integer)
- `overwrite_dest`: Overwrite existing destination (Boolean)

### Example Project Management with RasExamples
```python
# Extract HEC-RAS example projects for testing
# Default extraction to 'example_projects' folder
path = RasExamples.extract_project("Muncie")

# Extract to custom location (relative or absolute path)
path = RasExamples.extract_project("Dam Breaching", output_path="my_tests")
paths = RasExamples.extract_project(["Muncie", "Balde Eagle Creek"], output_path="C:/tests")

# List available projects and categories
all_projects = RasExamples.list_projects()
categories = RasExamples.list_categories()
category_projects = RasExamples.list_projects("1D Unsteady Flow Hydraulics")
```

## GUI Application Development

### Existing GUI Pattern
- See `tools/1D Mannings to L-MC-R/1D_Mannings_to_L-MC-R.py` for tkinter GUI example
- Uses tkinterdnd2 for drag-and-drop functionality
- Progress bars and real-time logging display
- Error handling with user-friendly messages
- PyInstaller compilation for exe distribution

### Recommended GUI Libraries
- **tkinter** - Built-in, good for simple applications
- **tkinterdnd2** - Add drag-and-drop support
- **PyQt5/PySide2** - For more advanced GUIs
- **PyInstaller** - Create standalone executables

## File Structure Conventions

### Project Organization
- Main library code in `ras_commander/`
- Examples and documentation in `examples/`
- Test projects in `tests/example_projects/`
- GUI applications in `apps/` (create if needed)
- Utility tools in `tools/`
- AI/LLM resources in `ai_tools/`

### Creating New Applications
- Create subdirectory in `apps/` or `tools/`
- Include separate `requirements.txt` or `pyproject.toml`
- Follow existing naming patterns (snake_case for directories)
- Include README.md with installation and usage instructions
- Use uv for environment management when possible

## AI/LLM Integration Notes

- Repository has extensive AI tooling in `ai_tools/`
- Knowledge bases generated automatically during build process
- Cursor IDE integration with `.cursorrules`
- Optimized for LLM code assistance and generation
- Examples serve dual purpose as documentation and tests to reduce hallucination

## Common Pitfalls to Avoid

- Don't instantiate static classes like `RasCmdr()` 
- Always specify `ras_object` parameter when working with multiple projects
- Use pathlib.Path for all path operations
- Don't forget `@log_call` decorator on new functions
- Ensure HEC-RAS project is initialized before calling compute functions
- Handle file permissions carefully on Windows systems
- Test with actual HEC-RAS example projects, not synthetic data

==================================================

File: C:\GH\ras-commander\Comprehensive_Library_Guide.md
==================================================
# Comprehensive RAS-Commander Library Guide

## Introduction

RAS-Commander (`ras_commander`) is a Python library designed to automate and streamline operations with HEC-RAS projects. It provides a suite of tools for managing projects, executing simulations, and handling results. This guide offers a comprehensive overview of the library's key concepts, modules, best practices, and advanced usage patterns. RAS-Commander is designed to be flexible, robust, and AI-accessible, making it an ideal tool for both manual and automated HEC-RAS workflows.

RAS-Commander can be installed with the following commands:
```bash
# Install dependencies first for potentially smoother installation
pip install h5py numpy pandas requests tqdm scipy xarray geopandas matplotlib psutil shapely fiona pathlib rtree

# Install or update ras-commander
pip install --upgrade ras-commander
```

## Key Concepts

1.  **RAS Objects (`RasPrj`)**:
    *   Represent HEC-RAS projects containing information about plans, geometries, and flow files.
    *   Support both a global `ras` object (imported from `ras_commander`) and custom named `RasPrj` instances to handle multiple projects.
    *   Include `rasmap_df` attribute providing access to spatial datasets referenced in RASMapper.

2.  **Project Initialization**:
    *   Use `init_ras_project()` to initialize projects and set up `RasPrj` objects.
    *   Handles project file (`.prj`) discovery, HEC-RAS executable path determination, and data structure setup.
    *   Supports multiple ways to specify the HEC-RAS executable location:
        - **Version number**: Use a version string (e.g., `"6.5"`) to use the default installation path
          ```python
          init_ras_project("/path/to/project", "6.5")
          ```
        - **Custom executable path**: Provide the full path to Ras.exe for non-standard installations
          ```python
          init_ras_project("/path/to/project", r"D:/Programs/HEC/HEC-RAS/6.5/Ras.exe")
          ```
        - **Auto-detection**: Omit the version parameter to attempt detection from plan files
          ```python
          init_ras_project("/path/to/project")
          ```
        - This flexibility is particularly useful when HEC-RAS is installed on a drive other than C:, or in a non-standard location.

3.  **File Handling**:
    *   Utilizes `pathlib.Path` for consistent, platform-independent file paths.
    *   Adheres to HEC-RAS file naming conventions (`.prj`, `.p01`, `.g01`, `.f01`, `.u01`, `.hdf`).

4.  **Data Management**:
    *   Employs Pandas DataFrames (accessible via `ras_object.plan_df`, `geom_df`, `flow_df`, `unsteady_df`, `boundaries_df`) to manage structured data about project components.
    *   Provides methods for accessing and updating project files, which subsequently refresh these DataFrames.

5.  **Execution Modes (`RasCmdr`)**:
    *   **Single Plan Execution**: Run individual plans with `RasCmdr.compute_plan()`.
    *   **Sequential Execution**: Run multiple plans in sequence in a test folder with `RasCmdr.compute_test_mode()`.
    *   **Parallel Execution**: Run multiple independent plans concurrently using worker folders with `RasCmdr.compute_parallel()`.

6.  **Example Projects (`RasExamples`)**:
    *   The `RasExamples` class offers functionality to download and manage official HEC-RAS example projects for testing and learning.

7.  **Utility Functions (`RasUtils`)**:
    *   Provides common utility functions for file operations (cloning, updating, removing), path handling, data conversion, error metrics calculation, and spatial queries.

8.  **AI-Accessibility**:
    *   Structured, consistent codebase with clear documentation (like this guide and docstrings) intended to facilitate easier learning and usage by AI models and developers. Code aims for predictability.

9.  **Boundary Conditions (`RasPrj.boundaries_df`)**:
    *   Represent input conditions (flow/stage hydrographs, normal depth, etc.).
    *   The `RasPrj` class extracts and manages boundary condition data parsed from unsteady flow files (`.u*`).

10. **Flexibility and Modularity**:
    *   Functions operating on projects accept an optional `ras_object` parameter, allowing use with the global `ras` or custom `RasPrj` instances.
    *   Clear separation of concerns: project management (`RasPrj`), execution (`RasCmdr`), file operations (`RasPlan`, `RasGeo`, `RasUnsteady`), HDF data retrieval (`Hdf*` classes), utilities (`RasUtils`), and examples (`RasExamples`).

11. **Error Handling and Logging**:
    *   Emphasis on robust error checking (e.g., `check_initialized`) and informative logging.
    *   Uses the `LoggingConfig` module for consistent setup (console and optional file output).
    *   `@log_call` decorator applied to most public functions for automatic call tracing at the DEBUG level.

12. **HDF Support (`Hdf*` classes)**:
    *   Comprehensive HDF file handling through specialized classes (`HdfBase`, `HdfMesh`, `HdfPlan`, `HdfResultsMesh`, `HdfResultsPlan`, `HdfResultsXsec`, `HdfStruc`, `HdfPipe`, `HdfPump`, `HdfFluvialPluvial`, `HdfInfiltration`, `HdfBndry`, `HdfXsec`, `HdfPlot`, `HdfResultsPlot`).
    *   Support for mesh, plan, cross-section geometry and results.
    *   Advanced data extraction, analysis, and plotting capabilities.

13. **Infrastructure Support (`HdfPipe`, `HdfPump`, `HdfStruc`)**:
    *   Tools for accessing geometry and results data related to pipe networks, pump stations, and other hydraulic structures from HDF files.

## Core Features

1.  **Project Management**: Initialize, load, and inspect HEC-RAS projects (`RasPrj`, `init_ras_project`).
2.  **Plan Execution**: Run single or multiple HEC-RAS plans sequentially or in parallel (`RasCmdr`).
3.  **File Operations**: Clone, modify, and manage HEC-RAS file types (plans, geometries, flows) programmatically (`RasPlan`, `RasGeo`, `RasUnsteady`, `RasUtils`).
4.  **Data Extraction**: Retrieve and process simulation results and geometry from HDF files (`Hdf*` classes).
5.  **Boundary Condition Management**: Extract and analyze boundary conditions from unsteady flow files (`RasPrj.get_boundary_conditions`).
6.  **Parallel Processing**: Optimize simulation time for multiple independent plans (`RasCmdr.compute_parallel`).
7.  **Example Project Handling**: Download and manage HEC-RAS example projects (`RasExamples`).
8.  **Utility Functions**: Perform common tasks like file manipulation, path resolution, error calculation, and spatial queries (`RasUtils`).
9.  **HDF File Handling**: Specialized classes for structured access to HEC-RAS HDF data.
10. **Infrastructure Analysis**: Tools for analyzing pipe networks, pump stations, and structures via HDF data (`HdfPipe`, `HdfPump`, `HdfStruc`).
11. **Visualization**: Basic plotting capabilities for HDF results (`HdfPlot`, `HdfResultsPlot`).

## Module Overview

1.  **RasPrj**: Manages HEC-RAS project state, including file discovery and dataframes for plans, geoms, flows, unsteady files, and boundary conditions.
2.  **RasCmdr**: Handles execution of HEC-RAS simulations via command line (single, sequential, parallel).
3.  **RasPlan**: Provides functions for plan file (`.p*`) operations (cloning, modifying parameters like geometry, flow files, cores, intervals, description).
4.  **RasGeo**: Manages geometry-related operations, primarily clearing preprocessor files (`.c*`).
5.  **RasUnsteady**: Handles unsteady flow file (`.u*`) operations (updating title, restart settings, extracting boundary tables).
6.  **RasUtils**: Offers general utility functions (file handling, path finding, data conversion, error metrics, spatial queries).
7.  **RasExamples**: Manages downloading and extracting official HEC-RAS example projects.
8.  **RasMap**: Parses HEC-RAS mapper configuration files (.rasmap) to extract paths to terrain, soil layer, land cover data, and other spatial datasets. Provides functions to automate post-processing of stored floodplain maps.
9.  **HdfBase**: Provides base functionality for HDF file operations (time parsing, attribute access, projection).
10. **HdfBndry**: Handles boundary *geometry* features (BC lines, breaklines, etc.) from geometry HDF files.
11. **HdfMesh**: Manages mesh *geometry* data (cell polygons, points, faces, attributes) from HDF files.
12. **HdfPlan**: Handles plan-level information (simulation times, parameters) from plan HDF files.
13. **HdfResultsMesh**: Processes mesh *results* (WSE, velocity, depth timeseries, summaries) from plan HDF files.
14. **HdfResultsPlan**: Handles plan-level *results* (volume accounting, runtime stats) from plan HDF files.
15. **HdfResultsXsec**: Processes 1D cross-section *results* (WSE, flow, velocity timeseries) from plan HDF files.
16. **HdfStruc**: Manages structure *geometry* data (centerlines, profiles) from geometry HDF files.
17. **HdfUtils**: Provides utility functions specifically for HDF data handling (data type conversions, spatial queries).
18. **HdfXsec**: Handles 1D cross-section and river *geometry* (cut lines, centerlines, banks) from geometry HDF files.
19. **HdfPipe**: Handles pipe network geometry and results data from HDF files.
20. **HdfPump**: Manages pump station geometry and results data from HDF files.
21. **HdfFluvialPluvial**: Analyzes fluvial vs. pluvial boundaries based on results timing in plan HDF files.
22. **HdfInfiltration**: Handles infiltration layer data (parameters, maps) from geometry or `.tif.hdf` files.
23. **HdfPlot & HdfResultsPlot**: Basic visualization utilities for HDF data and results.
24. **Decorators**: Contains `@log_call` and `@standardize_input`.
25. **LoggingConfig**: Sets up and provides access to the library's logging system.

## Best Practices

### 1. RAS Object Usage (`RasPrj`)

*   **Single Project Scripts**:
    *   Use the global `ras` object for simplicity after initializing it.
    ```python
    from ras_commander import ras, init_ras_project

    # Initialize the global 'ras' object
    init_ras_project("/path/to/project", "6.5")
    # Use ras object for operations
    print(ras.plan_df)
    ```

*   **Multiple Projects**:
    *   Create separate `RasPrj` instances for each project using `init_ras_project` with the `ras_object` parameter set to a new instance or a unique identifier string. Always pass the specific `ras_object` to functions that need project context.
    ```python
    from ras_commander import RasPrj, init_ras_project, RasCmdr

    # Initialize two separate projects using new instances
    project1 = init_ras_project("/path/to/project1", "6.5", ras_object=RasPrj())
    project2 = init_ras_project("/path/to/project2", "6.5", ras_object=RasPrj())

    # Pass the specific object when calling functions
    RasCmdr.compute_plan("01", ras_object=project1)
    RasCmdr.compute_plan("02", ras_object=project2)
    ```

*   **Consistency**:
    *   Avoid mixing global `ras` usage and custom `RasPrj` instances within the same logical part of your script to prevent confusion. When using multiple projects, *always* pass the `ras_object` parameter.

### 2. Plan Specification

*   Use plan numbers as strings (e.g., `"01"`, `"02"`) for consistency with file naming conventions when calling functions like `compute_plan`.
    ```python
    RasCmdr.compute_plan("01")
    ```

*   Check available plans before specifying plan numbers using the `plan_df` attribute.
    ```python
    print(ras.plan_df) # Displays available plans and their details
    available_plans = ras.plan_df['plan_number'].tolist()
    print(f"Available plans: {available_plans}")
    ```

### 3. Geometry Preprocessor Files (`.c*`)

*   Clear geometry preprocessor files (`.c*`) using `RasGeo.clear_geompre_files()` *before* running a plan if the underlying geometry (`.g*`) file has been significantly modified. This forces HEC-RAS to recalculate hydraulic tables.
    ```python
    from ras_commander import RasPlan, RasGeo
    plan_path = RasPlan.get_plan_path("01") # Get path if needed
    RasGeo.clear_geompre_files(plan_path) # Clear for a specific plan
    # Or clear for all plans: RasGeo.clear_geompre_files()
    ```
*   Alternatively, use `clear_geompre=True` in `RasCmdr` functions for a clean computation environment, though calling `RasGeo.clear_geompre_files` directly offers more control.
    ```python
    RasCmdr.compute_plan("01", clear_geompre=True)
    ```

### 4. Parallel Execution (`RasCmdr.compute_parallel`)

*   Adjust `max_workers` (number of parallel HEC-RAS instances) based on available **physical CPU cores** and **RAM**. Each worker needs significant RAM (2-4GB+).
*   Set `num_cores` (cores *per worker*) to balance single-plan speed vs. overall throughput. Common values are 2-4 cores per worker.
    ```python
    import psutil
    physical_cores = psutil.cpu_count(logical=False)
    cores_per_worker = 2
    max_workers_cpu = max(1, physical_cores // cores_per_worker)
    # Consider RAM constraints as well - may need fewer workers than CPU allows
    max_workers = min(max_workers_cpu, 4) # Example: Limit to 4 workers regardless of CPU

    RasCmdr.compute_parallel(max_workers=max_workers, num_cores=cores_per_worker)
    ```

*   Use `dest_folder` to organize outputs from parallel runs and prevent conflicts with the original project or other runs. Use `overwrite_dest=True` cautiously.
    ```python
    results_dir = Path("./parallel_run_output")
    RasCmdr.compute_parallel(dest_folder=results_dir, overwrite_dest=True)
    ```

### 5. Error Handling and Logging

Proper error handling and logging are crucial for robust `ras_commander` scripts.

1.  **Logging Setup**:
    The library automatically sets up basic console logging at the INFO level. You can change the level or add file logging.
    ```python
    import logging
    from ras_commander import setup_logging, get_logger

    # Change console log level to DEBUG
    logging.getLogger('ras_commander').setLevel(logging.DEBUG)

    # Or, setup file logging as well
    # setup_logging(log_file='my_ras_script.log', log_level=logging.DEBUG)

    # Get a logger for your script
    script_logger = get_logger(__name__)
    script_logger.info("Script starting...")
    ```

2.  **Using the `@log_call` Decorator**:
    Most public library functions already use `@log_call`, automatically logging function entry/exit at the DEBUG level.

3.  **Custom Logging**:
    Use the logger obtained via `get_logger()` for more detailed script-specific logging.
    ```python
    logger = get_logger(__name__)
    def my_custom_ras_step():
        logger.info("Starting custom step...")
        try:
            # Perform operations using ras_commander functions
            result = RasCmdr.compute_plan("01")
            if not result:
                logger.warning("Plan 01 computation reported failure.")
            # Process result...
            logger.info("Custom step completed.")
        except Exception as e:
            logger.error(f"Custom step failed: {str(e)}", exc_info=True) # Log exception details
            # Handle error appropriately
    ```

4.  **Error Handling Best Practices**:
    *   Use `try...except` blocks around library calls that might fail (e.g., file operations, computations).
    *   Catch specific exceptions (`FileNotFoundError`, `ValueError`, `subprocess.CalledProcessError`) where possible.
    *   Check return values from functions like `compute_plan` which return `True`/`False`.
    *   Provide informative error messages in your logs or exceptions.
    ```python
    try:
        plan_path = RasPlan.get_plan_path("99") # Plan likely doesn't exist
        if plan_path is None:
             raise ValueError("Plan 99 does not exist in the project.")
        # ... more operations
    except ValueError as ve:
        script_logger.error(f"Configuration error: {ve}")
    except FileNotFoundError as fnf:
        script_logger.error(f"File access error: {fnf}")
    except Exception as ex:
        script_logger.critical(f"An unexpected error occurred: {ex}", exc_info=True)
    ```

### 6. File Path Handling

*   Use `pathlib.Path` objects for robust file and directory operations within your scripts. Library functions generally accept `str` or `Path`.
    ```python
    from pathlib import Path
    project_dir = Path("/path/to/my/project")
    results_dir = project_dir.parent / "results"
    # Pass Path objects to library functions
    init_ras_project(project_dir, "6.5")
    RasCmdr.compute_plan("01", dest_folder=results_dir)
    ```

### 7. Type Hinting

*   Apply type hints in your own functions that use `ras_commander` to improve code readability, maintainability, and IDE support.
    ```python
    from ras_commander import RasPrj, RasCmdr
    from typing import Dict, List

    def run_specific_plans(project: RasPrj, plans_to_run: List[str]) -> Dict[str, bool]:
        """Runs a list of plans for the given project."""
        results = RasCmdr.compute_parallel(
            plan_number=plans_to_run,
            ras_object=project,
            max_workers=2,
            num_cores=2
        )
        return results
    ```

## Usage Patterns

### Initializing a Project (Global `ras`)

```python
from ras_commander import init_ras_project, ras

# Initializes the global 'ras' object
init_ras_project("/path/to/project", "6.5")
print(f"Working with project: {ras.project_name}")
print(f"Using HEC-RAS at: {ras.ras_exe_path}")

# Or use a custom path to Ras.exe (for non-C: drive installations)
init_ras_project("/path/to/project", r"D:/Programs/HEC/HEC-RAS/6.5/Ras.exe")
print(f"Working with project: {ras.project_name}")
print(f"Using custom HEC-RAS path: {ras.ras_exe_path}")

# Or let the library try to detect the version from plan files
init_ras_project("/path/to/project")  # Auto-detection (if plan files exist)
print(f"Available plans:\n{ras.plan_df}")
```

### Cloning a Plan

```python
from ras_commander import RasPlan, ras, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Clone plan "01", automatically assigns next number (e.g., "02")
new_plan_number = RasPlan.clone_plan("01", new_plan_shortid="Cloned Plan Test")
print(f"Created new plan: {new_plan_number}")

# The 'ras' object is automatically refreshed after cloning
print(f"Updated plan list:\n{ras.plan_df}")
```

### Executing Plans (`RasCmdr`)

*   **Single Plan Execution**:
    ```python
    from ras_commander import RasCmdr, init_ras_project

    init_ras_project("/path/to/project", "6.5")
    success = RasCmdr.compute_plan("01", num_cores=2)
    print(f"Plan '01' execution {'successful' if success else 'failed'}")
    ```

*   **Parallel Execution of Multiple Plans**:
    ```python
    from ras_commander import RasCmdr, init_ras_project
    from pathlib import Path

    init_ras_project("/path/to/project", "6.5")
    results_folder = Path("./parallel_results")

    results = RasCmdr.compute_parallel(
        plan_number=["01", "02", "03"],
        max_workers=3,
        num_cores=2, # 2 cores per worker
        dest_folder=results_folder,
        overwrite_dest=True,
        clear_geompre=False
    )

    print("Parallel execution results:")
    for plan, success in results.items():
        print(f"  Plan {plan}: {'Successful' if success else 'Failed'}")
    ```
*   **Sequential Execution in Test Folder**:
    ```python
    from ras_commander import RasCmdr, init_ras_project

    init_ras_project("/path/to/project", "6.5")

    results = RasCmdr.compute_test_mode(
        plan_number=["01", "02"],
        dest_folder_suffix="[SequentialTest]",
        num_cores=4,
        overwrite_dest=True
    )
    print("Sequential execution results:")
    for plan, success in results.items():
        print(f"  Plan {plan}: {'Successful' if success else 'Failed'}")
    ```

### Working with Multiple Projects

```python
from ras_commander import RasPrj, init_ras_project, RasCmdr
from pathlib import Path

# Initialize two separate projects using new instances
project1 = init_ras_project("/path/to/project1", "6.5", ras_object=RasPrj())
project2 = init_ras_project("/path/to/project2", "6.5", ras_object=RasPrj())

results_folder1 = Path("./results_proj1")
results_folder2 = Path("./results_proj2")

# Perform operations on each project, passing the correct object
print(f"Running plan 01 for {project1.project_name}")
RasCmdr.compute_plan("01", ras_object=project1, dest_folder=results_folder1, overwrite_dest=True)

print(f"Running plan 02 for {project2.project_name}")
RasCmdr.compute_plan("02", ras_object=project2, dest_folder=results_folder2, overwrite_dest=True)

# Compare results (example: number of plans)
print(f"{project1.project_name} has {len(project1.plan_df)} plans.")
print(f"{project2.project_name} has {len(project2.plan_df)} plans.")

# Access HDF results (assuming computations were successful)
# Note: Need to re-initialize RasPrj for the output folders to access results easily,
# or construct HDF paths manually.
# Example: Re-initialize to read results from project 1's output
results_proj1 = init_ras_project(results_folder1, "6.5", ras_object=RasPrj())
hdf_df1 = results_proj1.get_hdf_entries()
print(f"HDF results found for Project 1 run:\n{hdf_df1}")
```

## Advanced Usage

### Working with HDF Files

`ras_commander` provides extensive support for reading HEC-RAS HDF files through specialized `Hdf*` classes. These methods typically accept a path identifier (string, Path, number) which is standardized by the `@standardize_input` decorator.

1.  **Exploring HDF Structure**:
    Use `HdfBase.get_dataset_info()` to print the internal structure of an HDF file.
    ```python
    from ras_commander import HdfBase, init_ras_project, RasPlan

    init_ras_project("/path/to/project", "6.5")
    # Assume plan 01 has been computed and has results
    hdf_results_path = RasPlan.get_results_path("01")
    if hdf_results_path:
        print(f"Exploring HDF file: {hdf_results_path}")
        HdfBase.get_dataset_info(hdf_results_path, group_path="/Results/Unsteady/Output/Output Blocks/Base Output/Summary Output")
    else:
        print("No HDF results found for plan 01.")
    ```

2.  **Extracting Mesh Results (`HdfResultsMesh`)**:
    Use `HdfResultsMesh` methods to get timeseries or summary results for 2D areas.
    ```python
    from ras_commander import HdfResultsMesh, init_ras_project, RasPlan
    import xarray as xr

    init_ras_project("/path/to/project", "6.5")
    hdf_results_path = RasPlan.get_results_path("01") # Path to .p01.hdf

    if hdf_results_path:
        try:
            # Get Water Surface time series for the first mesh area
            mesh_names = HdfMesh.get_mesh_area_names(hdf_results_path)
            if mesh_names:
                first_mesh = mesh_names[0]
                ws_timeseries: xr.DataArray = HdfResultsMesh.get_mesh_timeseries(
                    hdf_results_path, first_mesh, "Water Surface"
                )
                print(f"Water Surface timeseries for mesh '{first_mesh}':\n{ws_timeseries}")

                # Get Max Water Surface summary for all meshes
                max_ws_summary = HdfResultsMesh.get_mesh_max_ws(hdf_results_path)
                print(f"\nMax Water Surface Summary:\n{max_ws_summary}")
            else:
                print("No mesh areas found in HDF.")
        except Exception as e:
            print(f"Error reading mesh results: {e}")
    else:
        print("No HDF results found for plan 01.")
    ```

3.  **Working with Plan Results (`HdfResultsPlan`)**:
    Use `HdfResultsPlan` for plan-level results like runtime or volume accounting.
    ```python
    from ras_commander import HdfResultsPlan, init_ras_project, RasPlan

    init_ras_project("/path/to/project", "6.5")
    hdf_results_path = RasPlan.get_results_path("01")

    if hdf_results_path:
        runtime_data = HdfResultsPlan.get_runtime_data(hdf_results_path)
        print(f"Runtime Data:\n{runtime_data}")

        volume_accounting = HdfResultsPlan.get_volume_accounting(hdf_results_path)
        print(f"\nVolume Accounting:\n{volume_accounting}")
    else:
        print("No HDF results found for plan 01.")
    ```

4.  **Cross-Section Results (`HdfResultsXsec`)**:
    Extract 1D cross-section time series results using `HdfResultsXsec`.
    ```python
    from ras_commander import HdfResultsXsec, init_ras_project, RasPlan
    import xarray as xr

    init_ras_project("/path/to/project", "6.5")
    hdf_results_path = RasPlan.get_results_path("01")

    if hdf_results_path:
        xsec_results: xr.Dataset = HdfResultsXsec.get_xsec_timeseries(hdf_results_path)
        print(f"Cross Section Results Dataset:\n{xsec_results}")
        # Example: Access Water Surface for the first cross section
        # first_xs_name = xsec_results['cross_section'][0].item()
        # ws_first_xs = xsec_results['Water_Surface'].sel(cross_section=first_xs_name)
        # print(f"\nWater Surface for first cross section ({first_xs_name}):\n{ws_first_xs}")
    else:
        print("No HDF results found for plan 01.")
    ```

### Working with Pipe Networks and Pump Stations

`ras_commander` provides specialized classes (`HdfPipe`, `HdfPump`) for handling pipe network and pump station data from HEC-RAS HDF files.

1.  **Pipe Network Operations (`HdfPipe`)**:
    Extract geometry and results for pipe networks.
    ```python
    from ras_commander import HdfPipe, init_ras_project, RasPlan, HdfMesh
    from pathlib import Path

    init_ras_project("/path/to/project_with_pipes", "6.5")
    # Assume plan 01 computed results for a project with pipes
    hdf_plan_path = RasPlan.get_results_path("01")
    # Geometry HDF is needed for geometric data
    hdf_geom_path = HdfMesh.get_mesh_cell_polygons(hdf_plan_path) # Infer geom HDF from plan

    if hdf_plan_path and hdf_geom_path:
        # Extract pipe conduit geometry
        pipe_conduits_gdf = HdfPipe.get_pipe_conduits(hdf_geom_path)
        print(f"Pipe Conduits:\n{pipe_conduits_gdf}")

        # Extract pipe node geometry
        pipe_nodes_gdf = HdfPipe.get_pipe_nodes(hdf_geom_path)
        print(f"\nPipe Nodes:\n{pipe_nodes_gdf}")

        # Get pipe network timeseries results (e.g., Node Depth)
        node_depth_ts = HdfPipe.get_pipe_network_timeseries(hdf_plan_path, "Nodes/Depth")
        print(f"\nNode Depth Timeseries:\n{node_depth_ts}")

        # Get pipe network summary results
        summary_df = HdfPipe.get_pipe_network_summary(hdf_plan_path)
        print(f"\nPipe Network Summary:\n{summary_df}")

        # Get profile for a specific conduit (e.g., conduit index 0)
        # profile_df = HdfPipe.get_pipe_profile(hdf_geom_path, conduit_id=0)
        # print(f"\nProfile for Conduit 0:\n{profile_df}")
    else:
        print("Could not find HDF plan or geometry files.")
    ```

2.  **Pump Station Operations (`HdfPump`)**:
    Work with pump station geometry and results data.
    ```python
    from ras_commander import HdfPump, init_ras_project, RasPlan, HdfMesh

    init_ras_project("/path/to/project_with_pumps", "6.5")
    # Assume plan 01 computed results
    hdf_plan_path = RasPlan.get_results_path("01")
    hdf_geom_path = HdfMesh.get_mesh_cell_polygons(hdf_plan_path) # Infer geom HDF

    if hdf_plan_path and hdf_geom_path:
        # Extract pump station locations
        pump_stations_gdf = HdfPump.get_pump_stations(hdf_geom_path)
        print(f"Pump Stations:\n{pump_stations_gdf}")

        # Get pump group details (like efficiency curves)
        pump_groups_df = HdfPump.get_pump_groups(hdf_geom_path)
        print(f"\nPump Groups:\n{pump_groups_df}")

        # Get pump station timeseries results (replace "Pump Station 1" with actual name)
        try:
            pump_ts = HdfPump.get_pump_station_timeseries(hdf_plan_path, "Pump Station 1")
            print(f"\nTimeseries for Pump Station 1:\n{pump_ts}")
        except ValueError as e:
            print(f"\nError getting timeseries: {e}")


        # Get pump station summary results
        summary_df = HdfPump.get_pump_station_summary(hdf_plan_path)
        print(f"\nPump Station Summary:\n{summary_df}")

        # Get pump operation timeseries (replace "Pump Station 1" with actual name)
        try:
            operation_df = HdfPump.get_pump_operation_timeseries(hdf_plan_path, "Pump Station 1")
            print(f"\nOperation Timeseries for Pump Station 1:\n{operation_df}")
        except ValueError as e:
             print(f"\nError getting operation timeseries: {e}")
    else:
        print("Could not find HDF plan or geometry files.")
    ```

### Working with Multiple HEC-RAS Projects

Manage multiple projects by creating separate `RasPrj` instances.

```python
from ras_commander import RasPrj, init_ras_project, RasCmdr, RasPlan
import pandas as pd

# Initialize multiple project instances
# Use ras_object=RasPrj() to ensure distinct instances
project1 = init_ras_project("/path/to/project1", "6.5", ras_object=RasPrj())
project2 = init_ras_project("/path/to/project2", "6.6", ras_object=RasPrj())

# This allows RasPrj Instances to be accessed independently
print(f"Project 1: {project1.project_name} ({len(project1.plan_df)} plans)")
print(f"Project 2: {project2.project_name} ({len(project2.plan_df)} plans)")

# --- Best Practices ---
# 1. Clear Naming: Use descriptive variable names (e.g., bald_eagle_proj, muncie_proj).
# 2. Pass Objects: Always pass the correct ras_object to functions.
# 3. Avoid Global 'ras': Don't rely on the global 'ras' when managing multiple projects.
# 4. Separate Outputs: Use distinct dest_folder paths for computations.
# 5. Resource Awareness: Monitor CPU/RAM when running computations for multiple projects, especially in parallel.

# --- Example Workflow ---
# Function to compare basic project structures
def compare_project_structures(ras_obj1: RasPrj, name1: str, ras_obj2: RasPrj, name2: str) -> pd.DataFrame:
    """Compare the structures of two HEC-RAS projects."""
    comparison = {
        'Project Name': [ras_obj1.project_name, ras_obj2.project_name],
        'Plan Count': [len(ras_obj1.plan_df), len(ras_obj2.plan_df)],
        'Geometry Count': [len(ras_obj1.geom_df), len(ras_obj2.geom_df)],
        'Flow Count': [len(ras_obj1.flow_df), len(ras_obj2.flow_df)],
        'Unsteady Count': [len(ras_obj1.unsteady_df), len(ras_obj2.unsteady_df)]
    }
    return pd.DataFrame(comparison, index=[name1, name2])

# Perform operations on each project
RasCmdr.compute_plan("01", ras_object=project1, dest_folder="./proj1_run")
RasCmdr.compute_plan("01", ras_object=project2, dest_folder="./proj2_run")

# Compare structures
comparison_df = compare_project_structures(project1, "Project 1", project2, "Project 2")
print("\nProject Structure Comparison:")
print(comparison_df)

# --- Application Examples ---
# 1. Model Comparison: Run same scenarios on different river models.
# 2. Basin-wide Analysis: Process connected or related models.
# 3. Parameter Sweep: Test parameter variations across multiple baseline models.
# 4. Batch Processing: Automate runs for a large inventory of models.
```

### Plan Execution Modes (`RasCmdr`)

`ras_commander` offers three modes for running HEC-RAS plans:

#### Single Plan Execution (`compute_plan`)

Runs one plan, optionally in a separate destination folder. Best for targeted runs or when immediate results are needed.

```python
from ras_commander import RasCmdr, init_ras_project

init_ras_project("/path/to/project", "6.5")

success = RasCmdr.compute_plan(
    plan_number="01",              # Plan to execute
    dest_folder="/path/to/single_run_results", # Optional: Where to run
    num_cores=4,                   # Optional: Cores for this run
    clear_geompre=True,            # Optional: Force geometry preprocess
    overwrite_dest=True            # Optional: Overwrite dest if exists
)
print(f"Plan 01 execution status: {success}")
```

#### Sequential Execution (`compute_test_mode`)

Runs multiple plans one after another in a dedicated test folder (copy of the project). Best for plans with dependencies or for controlled resource usage.

```python
from ras_commander import RasCmdr, init_ras_project

init_ras_project("/path/to/project", "6.5")

results = RasCmdr.compute_test_mode(
    plan_number=["01", "02", "03"], # Plans to run in order
    dest_folder_suffix="[SequentialRun]", # Suffix for test folder name
    clear_geompre=True,            # Optional: Clear before each plan
    num_cores=4,                   # Optional: Cores for each plan
    overwrite_dest=True            # Optional: Overwrite test folder
)
print("Sequential results:", results)
```

#### Parallel Execution (`compute_parallel`)

Runs multiple independent plans concurrently using temporary worker folders, consolidating results afterward. Best for maximizing speed on multi-core systems with independent plans.

```python
from ras_commander import RasCmdr, init_ras_project

init_ras_project("/path/to/project", "6.5")

results = RasCmdr.compute_parallel(
    plan_number=["01", "02", "03"], # Plans to run in parallel
    max_workers=3,                 # Max concurrent HEC-RAS instances
    num_cores=2,                   # Cores assigned to each worker
    dest_folder="/path/to/parallel_results", # Final results location
    clear_geompre=False,           # Optional: Clear in worker folders
    overwrite_dest=True            # Optional: Overwrite final results folder
)
print("Parallel results:", results)
```

#### Choosing the Right Mode

*   **Dependency:** Sequential (`compute_test_mode`) for dependent plans; Parallel (`compute_parallel`) for independent plans.
*   **Resources:** Single (`compute_plan`) or Sequential for limited hardware; Parallel for multi-core systems.
*   **Speed:** Parallel is usually fastest overall for multiple plans; Single is fastest for one specific plan.
*   **Debugging:** Single or Sequential are often easier to debug.
*   **Isolation:** `compute_test_mode` and `compute_parallel` (with `dest_folder`) provide isolated run environments.

#### Return Values

*   `compute_plan()`: Returns `bool` (success/failure).
*   `compute_test_mode()`: Returns `Dict[str, bool]` mapping plan number to success status.
*   `compute_parallel()`: Returns `Dict[str, bool]` mapping plan number to success status.

```python
# Example checking results from parallel run
results = RasCmdr.compute_parallel(plan_number=["01", "02", "03"])
for plan_num, success in results.items():
    print(f"Plan {plan_num}: {'Success' if success else 'Failed'}")
    if not success:
        print(f"  Check logs related to plan {plan_num} execution.")
```

### Plan Parameter Operations (`RasPlan`)

Modify plan file (`.p*`) settings programmatically without the HEC-RAS GUI.

#### Retrieving Plan Values (`get_plan_value`)

Read specific parameters directly from a plan file.

```python
from ras_commander import RasPlan, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Get the computation interval for plan 01
interval = RasPlan.get_plan_value("01", "Computation Interval")
print(f"Plan 01 Computation Interval: {interval}")

# Get the number of cores assigned (0 means 'all available')
cores = RasPlan.get_plan_value("01", "UNET D2 Cores") # For 2D cores
print(f"Plan 01 UNET D2 Cores setting: {cores}")

# Get the associated geometry file number
geom_file_str = RasPlan.get_plan_value("01", "Geom File") # Returns e.g., "g01"
geom_num = geom_file_str[1:] if geom_file_str else "N/A"
print(f"Plan 01 uses Geometry: {geom_num}")
```

Common keys include: `Computation Interval`, `Short Identifier`, `Simulation Date`, `UNET D1 Cores`, `UNET D2 Cores`, `Plan Title`, `Geom File`, `Flow File` (or `Unsteady File`), `Friction Slope Method`, `Run HTab`, `UNET Use Existing IB Tables`.

#### Updating Run Flags (`update_run_flags`)

Control which simulation components HEC-RAS executes.

```python
from ras_commander import RasPlan, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Example: Enable geometry preprocessing and unsteady sim, disable others
RasPlan.update_run_flags(
    "01",
    geometry_preprocessor=True,    # Run HTab = 1
    unsteady_flow_simulation=True, # Run UNet = 1
    post_processor=False,          # Run PostProcess = 0
    floodplain_mapping=False       # Run RASMapper = -1 (Note: False maps to -1 for RASMapper)
)
print("Updated run flags for plan 01.")
```

#### Setting Time Intervals (`update_plan_intervals`)

Modify simulation time steps and output frequencies.

```python
from ras_commander import RasPlan, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Set computation to 10 sec, output to 1 min, mapping to 15 min
RasPlan.update_plan_intervals(
    "01",
    computation_interval="10SEC",
    output_interval="1MIN",
    mapping_interval="15MIN"
    # instantaneous_interval="1HOUR" # Also available
)
print("Updated time intervals for plan 01.")
```

Valid interval values (must match HEC-RAS exactly): `1SEC`..`30SEC`, `1MIN`..`30MIN`, `1HOUR`..`12HOUR`, `1DAY`.

#### Working with Simulation Dates (`update_simulation_date`)

Change the simulation window (start and end times).

```python
from ras_commander import RasPlan, init_ras_project
from datetime import datetime

init_ras_project("/path/to/project", "6.5")

start_dt = datetime(2024, 1, 1, 12, 0) # Jan 1, 2024, 12:00 PM
end_dt = datetime(2024, 1, 5, 0, 0)   # Jan 5, 2024, 00:00 AM

RasPlan.update_simulation_date("01", start_date=start_dt, end_date=end_dt)
print(f"Updated simulation dates for plan 01 to {start_dt} - {end_dt}.")
```

Ensure the simulation window covers your boundary condition data and includes appropriate warm-up/cool-down periods.

#### Managing Plan Descriptions (`read_plan_description`, `update_plan_description`)

Read or modify the multi-line description block in the plan file.

```python
from ras_commander import RasPlan, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Read current description
current_desc = RasPlan.read_plan_description("01")
print(f"Current description for plan 01:\n{current_desc}\n-----------------")

# Update the description
new_desc = f"""
Run Date: {datetime.now().strftime('%Y-%m-%d')}
Scenario: Test with updated Manning's values.
Source Geometry: g02
Source Flow: u03
Notes: Increased roughness in floodplain by 15%.
"""
RasPlan.update_plan_description("01", new_desc)
print("Updated description for plan 01.")
```

#### Core Allocation (`set_num_cores`)

Configure the number of processor cores a plan should use (sets `UNET D1 Cores`, `UNET D2 Cores`, `PS Cores` simultaneously).

```python
from ras_commander import RasPlan, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Set plan 01 to use 4 cores
RasPlan.set_num_cores("01", 4)
print("Set plan 01 to use 4 cores.")

# Set plan 02 to use all available cores (value 0)
RasPlan.set_num_cores("02", 0)
print("Set plan 02 to use all available cores.")
```

Consider system resources. 2-8 cores are typically effective. Using too many can decrease performance.

### Performance Optimization

Strategies to improve execution speed and resource usage.

1.  **Parallel Execution (`RasCmdr.compute_parallel`)**:
    *   Run independent plans simultaneously.
    *   Balance `max_workers` and `num_cores` based on CPU/RAM.
    ```python
    # Example: 8 physical cores, run 4 plans in parallel, 2 cores each
    RasCmdr.compute_parallel(plan_number=["01", "02", "03", "04"], max_workers=4, num_cores=2)
    ```

2.  **Optimized Geometry Preprocessing**:
    *   Avoid redundant calculations if multiple plans use the same geometry. Preprocess once, then run simulations.
    ```python
    from ras_commander import RasPlan, RasCmdr, init_ras_project

    init_ras_project("/path/to/project", "6.5")
    plan_to_preprocess = "01" # Plan using the geometry to preprocess
    geom_file_to_use = RasPlan.get_plan_value(plan_to_preprocess, "Geom File") # e.g., "g01"

    # Step 1: Force geometry preprocessing for the target geometry via one plan
    print(f"Preprocessing geometry {geom_file_to_use} using plan {plan_to_preprocess}...")
    RasPlan.update_run_flags(
        plan_to_preprocess,
        geometry_preprocessor=True,    # Run HTab = 1
        unsteady_flow_simulation=False # Run UNet = 0 (or -1 if available)
        # Set other flags as needed (e.g., post_processor=False)
    )
    RasCmdr.compute_plan(plan_to_preprocess) # This run primarily generates .c* files

    # Step 2: Run actual simulations using the preprocessed geometry
    plans_using_geom = ["01", "03", "05"] # Plans that use the same geometry
    for plan_num in plans_using_geom:
        print(f"Running simulation for plan {plan_num} using preprocessed geometry...")
        RasPlan.update_run_flags(
            plan_num,
            geometry_preprocessor=False,   # Run HTab = 0 (use existing tables)
            unsteady_flow_simulation=True  # Run UNet = 1
            # Set other flags as needed
        )
        # Run the plan (can be single, parallel, or test mode)
        RasCmdr.compute_plan(plan_num)
    ```

3.  **Memory Management (Large Datasets)**:
    *   When reading large HDF results (especially 2D time series), process data in chunks if memory becomes an issue. `xarray` (returned by `get_mesh_timeseries`) supports lazy loading and chunking.
    ```python
    import xarray as xr
    # ws_timeseries = HdfResultsMesh.get_mesh_timeseries(...)
    # If ws_timeseries is too large:
    # ws_chunked = ws_timeseries.chunk({'time': 100, 'cell_id': 10000}) # Example chunking
    # result = ws_chunked.mean(dim='cell_id').compute() # Perform computation
    ```

4.  **I/O Optimization**:
    *   Minimize repeated file opening/closing when reading multiple datasets from the *same* HDF file. Open it once with `h5py.File`.
    ```python
    import h5py
    from ras_commander import HdfBase, HdfResultsMesh # ... and other Hdf classes

    hdf_path = "/path/to/results.p01.hdf"
    try:
        with h5py.File(hdf_path, 'r') as hdf_file:
            # Perform multiple reads within this block
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            timestamps = HdfBase.get_unsteady_timestamps(hdf_file)
            # summary_ws = HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Water Surface") # Requires adapting func to take h5py.File
            print(f"Start time: {start_time}, Found {len(timestamps)} timestamps.")
            # ... more operations using the open hdf_file
    except Exception as e:
        print(f"Error accessing HDF file {hdf_path}: {e}")
    ```
    *(Note: Most `Hdf*` methods currently use `@standardize_input` which handles file opening/closing internally. Adapting them to accept open `h5py.File` objects might be needed for extreme I/O optimization).*

5.  **Profiling**:
    *   Use Python's `cProfile` to identify bottlenecks in your scripts.
    ```python
    import cProfile
    from ras_commander import RasCmdr, init_ras_project

    init_ras_project("/path/to/project", "6.5")
    # Profile a specific function call
    # cProfile.run('RasCmdr.compute_plan("01", num_cores=2)')
    ```

### Working with Boundary Conditions (`RasPrj.boundaries_df`)

Access and analyze boundary conditions extracted from unsteady flow files.

1.  **Accessing Boundary Conditions**:
    The `boundaries_df` attribute holds the parsed data.
    ```python
    from ras_commander import init_ras_project, ras

    init_ras_project("/path/to/project", "6.5")
    boundary_conditions = ras.boundaries_df
    if boundary_conditions is not None and not boundary_conditions.empty:
        print(f"Found {len(boundary_conditions)} boundary conditions:")
        print(boundary_conditions.head())
    else:
        print("No boundary conditions found or project not initialized correctly.")
    ```

2.  **Filtering Boundary Conditions**:
    Use standard pandas filtering.
    ```python
    if boundary_conditions is not None and not boundary_conditions.empty:
        # Get all flow hydrographs
        flow_hydrographs = boundary_conditions[boundary_conditions['bc_type'] == 'Flow Hydrograph']
        print(f"\nFlow Hydrographs:\n{flow_hydrographs[['river_reach_name', 'river_station', 'hydrograph_num_values']]}")

        # Get boundary conditions for a specific river/reach
        # main_river_boundaries = boundary_conditions[boundary_conditions['river_reach_name'] == 'Main River']
    ```

3.  **Analyzing Boundary Condition Data**:
    Access columns for details. Hydrograph values are often stored as strings or lists.
    ```python
    if 'flow_hydrographs' in locals() and not flow_hydrographs.empty:
        for index, bc in flow_hydrographs.iterrows():
            print(f"\n--- BC {index} ---")
            print(f"  Location: {bc['river_reach_name']} @ RS {bc['river_station']}")
            print(f"  Num Values: {bc['hydrograph_num_values']}")
            # Note: 'hydrograph_values' might be a list of strings or numbers depending on parsing
            # print(f"  Values (first 5): {bc.get('hydrograph_values', [])[:5]}")
    ```

4.  **Modifying Boundary Conditions**:
    *Direct modification via the library is generally NOT supported.* The `boundaries_df` is read-only representation. Modifying boundary conditions typically requires:
    *   Using `RasUnsteady` functions (`extract_tables`, `write_table_to_file`) to modify numeric tables.
    *   Manually editing the `.u*` files for structural changes.
    *   Creating custom functions to parse/rewrite specific parts of the `.u*` file.

5.  **Visualizing Boundary Conditions**:
    Use pandas and matplotlib with the extracted table data.
    ```python
    from ras_commander import RasUnsteady # Needed for table extraction
    import matplotlib.pyplot as plt

    if 'flow_hydrographs' in locals() and not flow_hydrographs.empty:
        # Example for the first flow hydrograph found
        first_flow_bc = flow_hydrographs.iloc[0]
        unsteady_file_path = first_flow_bc['full_path'] # Get path from merged df
        
        # Extract tables for this unsteady file
        tables = RasUnsteady.extract_tables(unsteady_file_path)
        
        # Find the correct table (assuming one flow hydrograph per location)
        flow_table_name = 'Flow Hydrograph='
        if flow_table_name in tables:
            flow_data = tables[flow_table_name]
            
            # Need time interval to create time axis (get from boundary_df or parse file)
            interval_str = first_flow_bc.get('Interval', '1HOUR') # Example: Get interval
            # Convert interval_str to timedelta (requires parsing logic, simplified here)
            # time_delta = pd.Timedelta(interval_str) # Simplistic example
            # time_axis = [i * time_delta for i in range(len(flow_data))]

            plt.figure(figsize=(10, 5))
            plt.plot(flow_data['Value']) # Plot against index if time axis is complex
            plt.title(f"Flow Hydrograph: {first_flow_bc['river_reach_name']} RS {first_flow_bc['river_station']}")
            plt.xlabel("Time Step Index")
            plt.ylabel("Flow")
            plt.grid(True)
            plt.show()
        else:
            print(f"Could not find table '{flow_table_name}' in {unsteady_file_path}")

    ```

### Advanced Data Processing with RasUtils

`RasUtils` provides tools beyond basic file operations.

1.  **Data Conversion (`convert_to_dataframe`)**:
    Load data from various file types into pandas DataFrames.
    ```python
    from ras_commander import RasUtils
    from pathlib import Path

    try:
        csv_df = RasUtils.convert_to_dataframe(Path("results.csv"))
        excel_df = RasUtils.convert_to_dataframe(Path("data.xlsx"), sheet_name="Sheet1")
        print("DataFrames loaded successfully.")
    except FileNotFoundError:
        print("Input file not found.")
    except NotImplementedError as e:
        print(f"Error: {e}")
    ```

2.  **Statistical Analysis (Error Metrics)**:
    Calculate common metrics for model calibration/validation.
    ```python
    from ras_commander import RasUtils
    import numpy as np

    observed = np.array([100, 120, 140, 160, 180])
    predicted = np.array([105, 125, 135, 165, 175])

    rmse = RasUtils.calculate_rmse(observed, predicted, normalized=False)
    percent_bias = RasUtils.calculate_percent_bias(observed, predicted, as_percentage=True)
    metrics = RasUtils.calculate_error_metrics(observed, predicted)

    print(f"RMSE: {rmse:.2f}")
    print(f"Percent Bias: {percent_bias:.2f}%")
    print(f"All Metrics: {metrics}")
    ```

3.  **Spatial Operations (KDTree)**:
    Perform nearest neighbor searches efficiently.
    ```python
    from ras_commander import RasUtils
    import numpy as np

    # Find nearest point in 'points' for each point in 'query_points' within 5 units
    points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
    query_points = np.array([[0.5, 0.5], [5, 5], [9, 9]])
    nearest_indices = RasUtils.perform_kdtree_query(points, query_points, max_distance=5.0)
    # Returns indices from 'points': e.g., [1, -1, 3] (-1 if no point within max_distance)
    print(f"Nearest point indices: {nearest_indices}")

    # Find nearest neighbor within the 'points' dataset itself (excluding self)
    # neighbors_indices = RasUtils.find_nearest_neighbors(points, max_distance=3.0)
    # print(f"Nearest neighbor indices within dataset: {neighbors_indices}")
    ```

4.  **Data Consolidation (`consolidate_dataframe`)**:
    Group and aggregate DataFrame rows, often merging values into lists.
    ```python
    from ras_commander import RasUtils
    import pandas as pd

    df = pd.DataFrame({'Group': ['A', 'A', 'B', 'B', 'A'],
                       'Value': [10, 20, 30, 40, 50],
                       'Type': ['X', 'Y', 'X', 'Y', 'X']})

    # Consolidate by 'Group', merging 'Value' and 'Type' into lists
    consolidated = RasUtils.consolidate_dataframe(df, group_by='Group', aggregation_method='list')
    print("Consolidated DataFrame:")
    print(consolidated)
    # Output might look like:
    #              Value       Type
    # Group
    # A      [10, 20, 50]  [X, Y, X]
    # B          [30, 40]     [X, Y]
    ```

### Optimizing Parallel Execution (`RasCmdr.compute_parallel`)

Fine-tune parallel runs based on goals and resources.

#### Strategy 1: Efficiency (Maximize Throughput)

Use fewer cores per worker to run more workers simultaneously (if RAM allows). Good for many small/medium independent plans.

```python
from ras_commander import RasCmdr
import psutil

physical_cores = psutil.cpu_count(logical=False)
cores_per_worker = 2 # Minimal cores per HEC-RAS instance
max_workers = max(1, physical_cores // cores_per_worker)
# Check available RAM and potentially reduce max_workers if needed

print(f"Efficiency Mode: Running up to {max_workers} workers with {cores_per_worker} cores each.")
RasCmdr.compute_parallel(
    plan_number=["01", "02", "03", "04", "05", "06"], # Example plans
    max_workers=max_workers,
    num_cores=cores_per_worker
)
```

#### Strategy 2: Performance (Minimize Single Plan Runtime)

Assign more cores per worker, reducing the number of concurrent workers. Good if individual plan speed is critical or for few, large plans.

```python
from ras_commander import RasCmdr
import psutil

physical_cores = psutil.cpu_count(logical=False)
cores_per_worker = max(4, min(8, physical_cores // 2)) # Use 4-8 cores, but not more than half the system
max_workers = max(1, physical_cores // cores_per_worker)

print(f"Performance Mode: Running up to {max_workers} workers with {cores_per_worker} cores each.")
RasCmdr.compute_parallel(
    plan_number=["LargePlan01", "LargePlan02"], # Example large plans
    max_workers=max_workers,
    num_cores=cores_per_worker
)
```

#### Strategy 3: Background Run (Balanced)

Limit total core usage to leave resources free for other tasks.

```python
from ras_commander import RasCmdr
import psutil

physical_cores = psutil.cpu_count(logical=False)
max_total_cores = int(physical_cores * 0.75) # Use up to 75% of physical cores
cores_per_worker = 2
max_workers = max(1, max_total_cores // cores_per_worker)

print(f"Background Mode: Running up to {max_workers} workers, using max {max_total_cores} total cores.")
RasCmdr.compute_parallel(
    max_workers=max_workers,
    num_cores=cores_per_worker
)
```

#### Optimizing Geometry Preprocessing in Parallel Runs

If many plans share the same geometry, preprocess it *once* before the parallel run, then run the parallel simulations without geometry preprocessing.

1.  **Preprocess Geometry (Single Run)**:
    ```python
    from ras_commander import RasPlan, RasCmdr, init_ras_project

    init_ras_project("/path/to/project", "6.5")
    plan_for_preprocessing = "01" # Choose one plan that uses the target geometry

    print("Preprocessing geometry...")
    RasPlan.update_run_flags(
        plan_for_preprocessing,
        geometry_preprocessor=True,
        unsteady_flow_simulation=False # Don't run the full simulation yet
    )
    RasCmdr.compute_plan(plan_for_preprocessing)
    print("Geometry preprocessing complete.")
    ```

2.  **Run Parallel Simulations (Without Preprocessing)**:
    ```python
    plans_to_run = ["01", "03", "05"] # Plans sharing the preprocessed geometry

    print("Running parallel simulations without geometry preprocessing...")
    RasCmdr.compute_parallel(
        plan_number=plans_to_run,
        max_workers=3,
        num_cores=2,
        clear_geompre=False # Important: Don't clear the files we just created
        # Ensure update_run_flags is set correctly *inside* compute_parallel
        # if needed, or ensure plans are pre-configured correctly.
        # Best practice: Ensure plans are saved with Run HTab = 0 before parallel run.
    )
    print("Parallel simulations complete.")
    ```
    *Self-Correction:* `compute_parallel` doesn't directly take `update_run_flags`. The flags should be set *before* calling `compute_parallel`. It might be better to modify the *template* plans to have `Run HTab=0` before the parallel run, or modify them in the worker folders (more complex). The easiest is often to ensure the `.p*` files are saved correctly beforehand.


### Working with RASMapper Data

RAS Commander now provides access to RASMapper configuration data through the `rasmap_df` attribute of the `RasPrj` class, which is initialized automatically when a project is loaded. This enables integration with spatial datasets referenced in RASMapper.

When you run init_ras_project, rasmap_df is populated with data from the project's .rasmap file

The `rasmap_df` contains paths to:
- Terrain data (DEM)
- Soil layers (Hydrologic Soil Groups)  
- Land cover datasets
- Infiltration data
- Profile lines and other features
- Project settings and current visualization state

This allows programmatic access to the same spatial data being used in RASMapper visualizations.

### Working with RASMapper Data and Post-Processing

The `RasMap` class provides tools to interact with `.rasmap` files, enabling automation of tasks typically done in the RASMapper interface, such as generating stored floodplain maps.

#### 1. Getting Available Terrains

Before generating maps, you might need to know which terrain layers are available in the project.

```python
from ras_commander import init_ras_project, RasMap, ras

# Initialize the project
init_ras_project(r"/path/to/your/project", "6.6")

# Get the path to the .rasmap file
rasmap_file = RasMap.get_rasmap_path()

if rasmap_file:
    # Get a list of terrain names
    terrains = RasMap.get_terrain_names(rasmap_file)
    print(f"Available terrains: {terrains}")
else:
    print("No .rasmap file found.")
```

#### 2. Automating Stored Map Generation (`postprocess_stored_maps`)

This powerful function automates the creation of stored map `.tif` files (e.g., for Depth, WSEL, Velocity) after a primary simulation has been run. It temporarily modifies plan and mapper files to run HEC-RAS in a mapping-only mode, then restores them.

**Workflow:**

1.  **Run a primary simulation** to generate the main HDF results (e.g., using `RasCmdr.compute_plan`).
2.  **Call `postprocess_stored_maps`** on the computed plan to generate the `.tif` files.

```python
from ras_commander import init_ras_project, RasCmdr, RasMap, RasPlan, ras

# Initialize the project
init_ras_project(r"/path/to/project", "6.6")
plan_to_map = "01"

# Step 1: Ensure the primary simulation has been run
hdf_path = RasPlan.get_results_path(plan_to_map)
if not hdf_path:
    print(f"Running initial simulation for plan {plan_to_map}...")
    RasCmdr.compute_plan(plan_to_map)

# Step 2: Post-process to generate stored maps
print(f"Generating stored maps for plan {plan_to_map}...")
success = RasMap.postprocess_stored_maps(
    plan_number=plan_to_map,
    specify_terrain="Terrain50",  # Optional: specify a terrain
    layers=["Depth", "WSEL"]      # Optional: specify which maps to generate
)

if success:
    print("Stored maps generated successfully.")
    # You can now find the .tif files in a subfolder named after the plan's Short ID
else:
    print("Failed to generate stored maps.")
```

This function is ideal for batch-processing workflows where you need to generate floodplain maps for multiple scenarios without manual intervention in RASMapper.

### Modifying Manning's n Values

RAS Commander provides functions for reading and writing Manning's n values in geometry files through the `RasGeo` class. This allows automation of roughness coefficient adjustments for calibration and sensitivity analysis.

```python
from ras_commander import RasGeo, RasPlan, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Get the geometry file path
geom_path = RasPlan.get_geom_path("01")

# Read base Manning's n values
mannings_df = RasGeo.get_mannings_baseoverrides(geom_path)
print(f"Current Manning's n values:\n{mannings_df}")

# Read region-specific Manning's n overrides
region_df = RasGeo.get_mannings_regionoverrides(geom_path)
print(f"Regional Manning's n overrides:\n{region_df}")

# Modify Manning's n values (example: increase all values by 20%)
mannings_df['Base Manning\'s n Value'] *= 1.2
RasGeo.set_mannings_baseoverrides(geom_path, mannings_df)
print("Updated Manning's n values in geometry file")

# Clear preprocessor files to ensure geometry changes take effect
RasGeo.clear_geompre_files()
```

Common applications include:
- Automated calibration workflows
- Sensitivity analysis by batch-modifying roughness values
- Scenario analysis using different roughness sets
- Seasonal roughness adjustments

### Advanced Infiltration Data Handling

The enhanced `HdfInfiltration` class provides comprehensive tools for working with soil and infiltration data in HEC-RAS projects.

```python
from ras_commander import HdfInfiltration, init_ras_project

init_ras_project("/path/to/project", "6.5")

# Get the infiltration layer HDF path from the RASMapper configuration
infiltration_path = ras.rasmap_df['infiltration_hdf_path'][0][0]

# Read current infiltration parameters
infil_df = HdfInfiltration.get_infiltration_baseoverrides(infiltration_path)
print(f"Current infiltration parameters:\n{infil_df}")

# Scale infiltration parameters
scale_factors = {
    'Maximum Deficit': 1.2,  # Increase by 20%
    'Initial Deficit': 1.0,  # No change
    'Potential Percolation Rate': 0.8  # Decrease by 20%
}
updated_df = HdfInfiltration.scale_infiltration_data(
    infiltration_path, infil_df, scale_factors
)
print("Updated infiltration parameters")

# Get infiltration map (raster value to mukey mapping)
infil_map = HdfInfiltration.get_infiltration_map()

# Calculate weighted parameters based on soil coverage
significant_soils = HdfInfiltration.get_significant_mukeys(soil_stats, threshold=1.0)
weighted_params = HdfInfiltration.calculate_weighted_parameters(
    significant_soils, infiltration_params
)
print(f"Weighted infiltration parameters:\n{weighted_params}")
```

Key capabilities include:
- Retrieving and modifying infiltration parameters
- Scaling parameter values for sensitivity analysis
- Calculating soil statistics from raster data
- Computing area-weighted infiltration parameters
- Extracting and analyzing soil map unit data


## Approaching Your End User Needs with Ras Commander

### Understanding Data Sources and Strategies

RAS Commander interacts primarily with HEC-RAS project definition files (ASCII text: `.prj`, `.p*`, `.g*`, `.u*`, `.f*`) and HDF output files (`.hdf`), aiming for accessibility and automation without needing the complexities of the HEC-RAS GUI or DSS manipulation.

1.  **Data Sources in HEC-RAS Projects**:
    *   ASCII input files (plans, unsteady flows, geometry definitions, project structure).
    *   DSS (Data Storage System) files (often used for time-series inputs like hydrographs, observed data).
    *   HDF (Hierarchical Data Format) files (contain detailed geometry tables and simulation results).

2.  **RAS Commander's Focus**:
    *   **Reading/Writing ASCII:** Parses `.prj` for structure. Reads/writes parameters in `.p*`, `.u*`, `.g*` files using `RasPlan`, `RasUnsteady`, `RasGeo`.
    *   **Reading HDF:** Extensive capabilities to read geometry and results from `.g*.hdf` and `.p*.hdf` files using `Hdf*` classes.
    *   **Avoiding Direct DSS Manipulation:** The library generally avoids reading from or writing to DSS files directly due to their binary format complexity and reliance on HEC libraries.

3.  **Strategy for Handling DSS Inputs**:
    *   **Option 1 (Recommended): Define Time Series in ASCII:** Instead of referencing DSS paths in your unsteady flow file (`.u*`), define hydrographs directly within the file using the fixed-width table format. You can use `RasUnsteady.extract_tables` to read existing tables and `RasUnsteady.write_table_to_file` to write modified/new ones.
    *   **Option 2 (Workaround): Read DSS Data via HDF:** If a simulation *using* DSS inputs has already been run, the HDF results file often contains the time-series data that was originally sourced from DSS (e.g., boundary condition flows). You can extract this from the HDF using `ras_commander` (e.g., via `HdfResultsXsec`) and potentially use it to construct ASCII tables for future runs.

4.  **Accessing Project Data**:
    *   **Project Structure:** `ras.plan_df`, `ras.geom_df`, `ras.flow_df`, `ras.unsteady_df` provide DataFrames parsed from the `.prj` file and associated plan/unsteady files.
    *   **Plan/Unsteady Parameters:** Use `RasPlan.get_plan_value` or read specific lines via `RasPlan`/`RasUnsteady` functions. Modify using `set_`/`update_` functions.
    *   **Geometry Data:** Detailed geometry (mesh, cross-sections, structures) is best accessed from the HDF geometry file (`.g*.hdf`) using `HdfMesh`, `HdfXsec`, `HdfStruc`, `HdfPipe`, `HdfPump`.
    *   **Results Data:** Simulation outputs are read from the HDF results file (`.p*.hdf`) using `HdfResultsMesh`, `HdfResultsPlan`, `HdfResultsXsec`, etc.
    *   **Boundary Conditions:** Parsed summary and table data available in `ras.boundaries_df` and via `RasUnsteady.extract_tables`.

### Working with RAS Commander

1.  **Initialization**: Start with `init_ras_project()` to load the project structure into a `RasPrj` object (usually the global `ras`).
2.  **Inspection**: Use the `.df` attributes (`ras.plan_df`, etc.) and `get_*_entries()` methods to understand the project components.
3.  **Modification**: Use `RasPlan`, `RasUnsteady`, `RasGeo` methods to change parameters, clone components, or update file references. Remember these often refresh the `ras` object's DataFrames.
4.  **Execution**: Use `RasCmdr` methods (`compute_plan`, `compute_parallel`, `compute_test_mode`) to run simulations.
5.  **Results Analysis**: After successful computation, use `Hdf*` classes to read geometry and results data from the relevant `.hdf` files (geometry or plan results). Use `RasPlan.get_results_path()` to find the results HDF.

### Example Workflow: Modifying and Running a Boundary Condition

```python
from ras_commander import (
    init_ras_project, ras, RasPlan, RasUnsteady, RasCmdr, HdfResultsXsec
)
import pandas as pd

# 1. Initialize Project
init_ras_project("/path/to/project", "6.5")

# 2. Identify and Clone Components
template_plan = "01"
template_unsteady = ras.plan_df.loc[ras.plan_df['plan_number'] == template_plan, 'unsteady_number'].iloc[0]

new_plan_num = RasPlan.clone_plan(template_plan, new_plan_shortid="Scaled_Flow_Test")
new_unsteady_num = RasPlan.clone_unsteady(template_unsteady)

# 3. Link Cloned Components
RasPlan.set_unsteady(new_plan_num, new_unsteady_num) # Link new unsteady to new plan
print(f"Plan {new_plan_num} created, using Unsteady {new_unsteady_num}")

# 4. Modify Unsteady Flow Data
new_unsteady_path = RasPlan.get_unsteady_path(new_unsteady_num)

# Find the flow hydrograph table within the new unsteady file
try:
    with open(new_unsteady_path, 'r') as f:
        lines = f.readlines()
    tables_info = RasUnsteady.identify_tables(lines)
    
    flow_table_info = None
    for name, start, end in tables_info:
        if "Flow Hydrograph" in name:
            flow_table_info = (name, start, end)
            break
            
    if not flow_table_info:
        raise ValueError("Flow Hydrograph table not found in unsteady file.")

    flow_table_name, flow_start_line, flow_end_line = flow_table_info
    
    # Parse the table
    flow_df = RasUnsteady.parse_fixed_width_table(lines, flow_start_line, flow_end_line)
    
    # Modify the values (e.g., scale by 1.2)
    flow_df['Value'] = flow_df['Value'] * 1.2
    print(f"Scaled {len(flow_df)} flow values by 1.2")
    
    # Write the modified table back
    RasUnsteady.write_table_to_file(new_unsteady_path, flow_table_name, flow_df, flow_start_line)
    print("Modified unsteady flow file saved.")

except Exception as e:
    print(f"Error modifying unsteady file: {e}")
    # Handle error exit

# 5. Execute the Modified Plan
print(f"Running plan {new_plan_num}...")
success = RasCmdr.compute_plan(new_plan_num, num_cores=2)

# 6. Analyze Results (if successful)
if success:
    print("Computation successful. Analyzing results...")
    hdf_path = RasPlan.get_results_path(new_plan_num)
    if hdf_path:
        xsec_results = HdfResultsXsec.get_xsec_timeseries(hdf_path)
        max_flow = xsec_results['Maximum_Flow'].max().item() # Get overall max flow from results
        print(f"Overall maximum cross-section flow in results: {max_flow:.2f}")
    else:
        print("Could not find HDF results file.")
else:
    print("Computation failed.")

```

### Best Practices for Workflow Development

1.  **Understand Your Data**: Know where key information resides (ASCII vs. HDF) and how HEC-RAS uses it.
2.  **Leverage HDF**: Use the `Hdf*` classes for reading detailed geometry and results – it's often easier than parsing ASCII geometry or complex results formats.
3.  **Iterate**: Start simple. Manually perform a step in HEC-RAS, understand the file changes, then automate that step using `ras_commander`. Verify with the GUI.
4.  **Isolate Runs**: Use `dest_folder` in `RasCmdr` functions or `compute_test_mode` to avoid modifying your original project during testing and development.
5.  **Log Extensively**: Use `logger.info()`, `logger.debug()`, etc., in your scripts to track progress and diagnose issues. Configure logging levels appropriately.
6.  **Use AI Assistance**: Leverage the AI-friendly structure. Provide relevant code snippets, this guide, or specific examples to an AI assistant (like ChatGPT, Claude) to help generate code for your specific tasks using the `ras_commander` API.

By following these strategies, you can effectively use `ras_commander` to automate complex HEC-RAS workflows, even navigating around limitations like direct DSS interaction by focusing on ASCII parameter files and HDF data extraction.

## RAS-Commander Dataframe Examples

After initializing a HEC-RAS project with `init_ras_project()`, `ras_commander` provides several pandas DataFrames accessible via the `RasPrj` object (e.g., `ras.plan_df`) to inspect the project's structure and components.

### Project Information

Basic info stored directly on the `RasPrj` object:

```python
from ras_commander import ras, init_ras_project
init_ras_project(r"C:\path\to\your\Muncie_Project", "6.5") # Example path

print(f"Project Name: {ras.project_name}")
print(f"Project Folder: {ras.project_folder}")
print(f"PRJ File: {ras.prj_file}")
print(f"HEC-RAS Executable Path: {ras.ras_exe_path}")
```
*Example output:*
```
Project Name: Muncie
Project Folder: C:\path\to\your\Muncie_Project
PRJ File: C:\path\to\your\Muncie_Project\Muncie.prj
HEC-RAS Executable Path: C:\Program Files (x86)\HEC\HEC-RAS\6.5\Ras.exe
```

### Plan Files DataFrame (`ras.plan_df`)

Contains information parsed from the `.prj` file and individual `.p*` files about each plan.

```python
print(f"\nPlan Files DataFrame ({len(ras.plan_df)} plans):")
display(ras.plan_df) # Use display() in notebooks for better formatting
```

*Key columns include:*
*   `plan_number`: Plan identifier ("01", "02", ...).
*   `full_path`: Path to the `.p*` file.
*   `Short Identifier`: User-defined short name.
*   `Plan Title`: User-defined full title.
*   `Geom File`: Geometry file number used (`gXX`).
*   `Flow File`: Flow file number used (`fXX` or `uXX`).
*   `unsteady_number`: Unsteady flow number if used (`uXX`), else `None`.
*   `geometry_number`: Geometry number used (`gXX`).
*   `Simulation Date`: Start/end dates/times string.
*   `Computation Interval`: Time step (e.g., "2MIN").
*   `Run HTab`, `Run UNet`, etc.: Run flags (parsed value).
*   `UNET D1 Cores`, `UNET D2 Cores`: Core settings (parsed integer).
*   `HDF_Results_Path`: Path to `.p*.hdf` results file, if it exists.
*   `Geom Path`, `Flow Path`: Calculated paths to associated geometry and flow files.

*(See original guide for example table structure)*

### Flow Files DataFrame (`ras.flow_df`)

Lists steady flow files (`.f*`) found in the `.prj` file.

```python
print(f"\nSteady Flow Files DataFrame ({len(ras.flow_df)} files):")
display(ras.flow_df)
```
*Key columns:*
*   `flow_number`: Flow file identifier ("01", "02", ...).
*   `full_path`: Path to the `.f*` file.

*(See original guide for example table structure)*

### Unsteady Flow Files DataFrame (`ras.unsteady_df`)

Lists unsteady flow files (`.u*`) found in the `.prj` file, with metadata parsed from the files.

```python
print(f"\nUnsteady Flow Files DataFrame ({len(ras.unsteady_df)} files):")
display(ras.unsteady_df)
```
*Key columns:*
*   `unsteady_number`: Unsteady file identifier ("01", "02", ...).
*   `full_path`: Path to the `.u*` file.
*   `Flow Title`: Title parsed from the `.u*` file.
*   `Program Version`: Version parsed from the `.u*` file.
*   `Use Restart`: Restart flag parsed from the `.u*` file.
*   Other parsed metadata (Precipitation, Wind modes etc.).

*(See original guide for example table structure)*

### Geometry Files DataFrame (`ras.geom_df`)

Lists geometry files (`.g*`) found in the `.prj` file.

```python
print(f"\nGeometry Files DataFrame ({len(ras.geom_df)} files):")
display(ras.geom_df)
```
*Key columns:*
*   `geom_number`: Geometry file identifier ("01", "02", ...).
*   `full_path`: Path to the `.g*` file.
*   `geom_file`: Base name (`gXX`).
*   `hdf_path`: Calculated path to the corresponding geometry HDF file (`.g*.hdf`).

*(See original guide for example table structure)*

### HDF Entries DataFrame (`ras.get_hdf_entries()`)

Filters `plan_df` to show only plans where the HDF results file exists.

```python
hdf_entries_df = ras.get_hdf_entries()
print(f"\nHDF Entries DataFrame ({len(hdf_entries_df)} plans with results):")
display(hdf_entries_df)
```
*Structure:* Same columns as `plan_df`, but only includes rows where `HDF_Results_Path` points to an existing file.

*(See original guide for example table structure)*

### Boundary Conditions DataFrame (`ras.boundaries_df`)

Detailed information about boundary conditions parsed from *all* unsteady flow files (`.u*`) in the project.

```python
print(f"\nBoundary Conditions DataFrame ({len(ras.boundaries_df)} conditions):")
display(ras.boundaries_df)
```
*Key columns include:*
*   `unsteady_number`: Links to the `.u*` file.
*   `boundary_condition_number`: Sequential ID within the unsteady file.
*   `river_reach_name`, `river_station`: Location (for river boundaries).
*   `storage_area_name`, `pump_station_name`: Location (for SA/Pump boundaries).
*   `bc_type`: Type of boundary (e.g., "Flow Hydrograph", "Normal Depth", "Gate Opening").
*   `hydrograph_type`: Specific type if it's a hydrograph.
*   `Interval`: Time interval for hydrograph data.
*   `hydrograph_num_values`: Number of points in the hydrograph table.
*   `hydrograph_values`: List of hydrograph values (often as strings or numbers).
*   Columns inherited from `unsteady_df` via merge (`Flow Title`, `Program Version`, etc.).

*(See original guide for example table structure)*

### Accessing and Using Dataframes

Use standard pandas operations to query and analyze this structured project data.

```python
import pandas as pd
# Assuming 'ras' is initialized

# Find all plans using geometry "01"
g01_plans = ras.plan_df[ras.plan_df['geometry_number'] == '01']
print(f"\nPlans using Geometry 01: {g01_plans['plan_number'].tolist()}")

# Get details for a specific plan
plan_02_details = ras.plan_df[ras.plan_df['plan_number'] == '02'].iloc[0]
print(f"\nDetails for Plan 02 - Short ID: {plan_02_details['Short Identifier']}")

# Count boundary conditions by type across the whole project
if ras.boundaries_df is not None and not ras.boundaries_df.empty:
    bc_counts = ras.boundaries_df['bc_type'].value_counts()
    print("\nBoundary Condition Counts:")
    print(bc_counts)
else:
    print("\nNo boundary conditions found in project.")

# Find unsteady files using a restart file
restart_files = ras.unsteady_df[ras.unsteady_df['Use Restart'] == 'True'] # Check actual parsed value
print(f"\nUnsteady files using restart: {restart_files['unsteady_number'].tolist()}")
```

These DataFrames provide a powerful way to programmatically understand and interact with the structure and components of your HEC-RAS projects.

## Conclusion

The RAS-Commander (`ras_commander`) library provides a robust and flexible Python interface for automating HEC-RAS workflows. By leveraging its classes and functions for project management, execution, file operations, and HDF data extraction, users can significantly streamline their modeling processes. Adhering to the best practices outlined in this guide, particularly regarding RAS object management, file handling, and error checking, will lead to more efficient and reliable automation scripts.

Remember to consult the specific docstrings of functions for detailed parameter information and refer to the library's source code for the most up-to-date implementation details.

For further assistance, bug reports, or feature requests, please refer to the library's [GitHub repository](https://github.com/billk-FM/ras-commander) and issue tracker.

**Happy Modeling!**


==================================================

File: C:\GH\ras-commander\LICENSE
==================================================
MIT License

Copyright (c) 2024 William M. Katzenmeyer

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so.

==================================================

File: C:\GH\ras-commander\pyproject.toml
==================================================
[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta:__legacy__"

==================================================

File: C:\GH\ras-commander\README.md
==================================================
# RAS Commander (ras-commander)

<p align="center">
  <img src="ras-commander_logo.svg" width=70%>
</p>

RAS Commander is a Python library for automating HEC-RAS operations, providing a set of tools to interact with HEC-RAS project files, execute simulations, and manage project data. This library was initially conceptualized in the Australian Water School course "AI Tools for Modelling Innovation", and subsequently expanded to cover much of the basic functionality of the HECRASController COM32 interface using open-source python libraries.  This library uses a Test Driven Development strategy, leveraging the publicly-available HEC-RAS Example projects to create repeatable demonstration examples.  The "Commmander" moniker is inspired by the "Command Line is All You Need" approach to HEC-RAS automation that was first implemented in the HEC-Commander Tools repository. 

*[Check out the ASFPM Presentation on RAS-Commander Here](https://drive.google.com/file/d/1kX0twae8NrpLwR0iQ0Dmd8zAXdq-pYXD/view)*

## Repository Author

**[William Katzenmeyer, P.E., C.F.M.](https://engineeringwithllms.info)**  
Owner & Vice President, [CLB Engineering Corporation](https://clbengineering.com/)  


## Don't Ask Me, Ask a GPT!

This repository has several methods of interaction with Large Language Models and LLM-Assisted Coding built right in: 

1. **[Purpose-Built Knowledge Base Summaries](https://github.com/gpt-cmdr/ras-commander/tree/main/ai_tools/llm_knowledge_bases)**: Up-to-date compilations of the documentation and codebase for use with large language models like Claude, ChatGPT, Gemini or Grok. Look in 'ai_tools/assistant_knowledge_bases/' in the repo.  The repo's codebase (without documentation and examples) has been curated to stay within the current ~200k context window limitations of frontier models, and for tasks that do not need an understanding of the underlying code, the Comprehensive Library Guide and any relevant examples from the example folder should be adequate context for leveraging the ras-commander API to complete tasks. 

2. **[Cursor IDE Integration](https://github.com/gpt-cmdr/ras-commander/blob/main/.cursorrules)**: Custom rules(.cursor/rules) for the Cursor IDE to provide context-aware suggestions and documentation.  Just open the repository folder in Cursor to recognize these instructions.  You can create your own folders "/workspace/, "/projects/", or "my_projects/" as these are already in the .gitignore, and place your custom scripts there for your projects.  This will allow easy referencing of the ras-commander documents and individual repo files, the automatic loading of the .cursorrules file.  Alternatvely, download the github repo into your projects folder to easily load documents and use cursor rules files.

3. **[RAS-Commander library as indexed by Deepwiki](https://deepwiki.com/gpt-cmdr/ras-commander)** An LLM-generated summary of the repostiory with diagrams and analysis of the library, as well as an integrated chat assistant with deep research.

4. **[RAS Commander Library Assistant on ChatGPT](https://chatgpt.com/g/g-TZRPR3oAO-ras-commander-library-assistant)**: A specialized ChatGPT "GPT" with access to the ras-commander codebase and library, available for answering queries and providing code suggestions.   You can even upload your own plan, unsteady and HDF files to inspect and help determine how to automate your workflows or visualize your results.  _NOTE: GPT's are still quite limited by OpenAI's GPT frameworks and may not be useful for long conversations.  Code interpreter cannot run HEC-RAS but can [open and view smaller HDF files and projects for demonstration purposes](https://chatgpt.com/share/67e7cdb7-49e0-8010-bbac-61d2c54d473f)_


## Background
The ras-commander library emerged from the initial test-bed of AI-driven coding represented by the [HEC-Commander tools](https://github.com/gpt-cmdr/HEC-Commander) Python notebooks. These notebooks served as a proof of concept, demonstrating the value proposition of automating HEC-RAS operations. In 2024, I taught a series of progressively more complex webinars demonstrating how to use simple prompting, example projects and natural language instruction to effectively code HEC-RAS automation workflows, culminating in a 6 hour course.  The library published for utilization in that course, [awsrastools](https://github.com/gpt-cmdr/awsrastools) served as a foundation of examples which were iteratively extended into the full RAS-Commander library.  Unlike the original notebook by the same name, this library is not focused on parallel execution across multiple machines.  Instead, it is focused on providing a general-purpose python API for interacting with HEC-RAS projects, and building an AI-friendly library that will allow new users to quickly scaffold their own workflows into a python script. Example notebooks are provided, but the intention is to empower engineers, software developers, GIS personnel and data analysts to more easily access and interact with HEC-RAS data in a python environment.  Also, by publishing these examples publicly, with complete working code examples and LLM optimization, future users can readily rewrite they key functions of the library for inclusion in into their own preferred libraries, languages or return formats.

## Features

If you've ever read the book "Breaking the HEC-RAS Code" by Chris Goodell, this library is intended to be an AI-coded, pythonic library that provides a modern alternative to the HECRASController API.  By leveraginging modern python features libraries such as pandas, geopandas and H5Py (favoring HDF data sources wherever practicable) this library builds functionality around HEC-RAS 6.2+ while maintaining as much forward compatibilty as possible with HEC-RAS 2025.  

HEC-RAS Project Management & Execution
- Multi-project handling with parallel and sequential execution
- Command-line execution integration
- Project folder management and organization
- Multi-core processing optimization
- Progress tracking and logging
- Execution error handling and recovery

Legacy Version Support (NEW in v0.81.0)
- **RasControl class** for HEC-RAS 3.x-4.x via COM interface (HECRASController)
- ras-commander style API - use plan numbers, not file paths
- Extract steady state profiles AND unsteady time series
- Run plans with automatic current plan setting
- Supports versions: 3.1, 4.1, 5.0.x (501-507), 6.0, 6.3, 6.6
- Version migration validation and comparison
- Open-operate-close pattern prevents conflicts with modern workflows
- Seamless integration with ras-commander project management
- See `examples/17_extracting_profiles_with_hecrascontroller.ipynb` for complete usage

HDF Data Access & Analysis
- 2D mesh results processing (depths, velocities, WSE)
- Cross-section data extraction
- Boundary condition analysis
- Structure data (bridges, culverts, gates)
- Pipe network and pump station analysis
- Fluvial-pluvial boundary calculations
- Infiltration and precipitation data handling
- Infiltration and soil data handling
- Land cover and terrain data integration
- Weighted parameter calculations for hydrologic modeling

RASMapper Data Integration
- RASMapper configuration parsing (.rasmap files)
- Terrain, soil, and land cover HDF paths
- Profile line paths

Manning's n Coefficient Management
- Base Manning's n table extraction and modification
- Regional overrides for spatially-varied roughness
- Direct editing of geometry file Manning values

Infiltration & Soil Analysis
- Soil statistics calculation and analysis
- Infiltration parameter management and scaling
- Weighted average parameter calculation
- Raster-based soil data processing

RAS ASCII File Operations
- Plan file creation and modification
- Geometry file parsing examples 
- Unsteady flow file management
- Project file updates and validation  

Note about support for Pipe Networks:  As a relatively new feature, only read access to Pipe Network geometry and results data has been included.  Users will need to code their own methods to modify/add pipe network data, and pull requests are always welcome to incorporate this capability.

Note about version support: The modern HDF-based features target HEC-RAS 6.2+ for optimal compatibility. For legacy versions (3.1, 4.1, 5.0.x), use the RasControl class which provides COM-based access to steady state profile extraction and plan execution (see example notebook 17).

## Installation

First, create a virtual environment with conda or venv (ask ChatGPT if you need help).  

#### Install via Pip

In your virtual environment, install ras-commander using pip:
```
pip install --upgrade ras-commander
```
If you have dependency issues with pip (especially if you have errors with numpy), try clearing your local pip packages 'C:\Users\your_username\AppData\Roaming\Python\' and then creating a new virtual environment.  

### Library Dependencies

These are the core dependencies required for ras-commander library functionality:

```bash
# Core library dependencies
pip install h5py numpy pandas requests tqdm scipy xarray geopandas matplotlib shapely rasterstats rtree

# Windows-specific (for RasControl COM interface and GUI automation)
pip install pywin32 psutil
```

### Notebook Dependencies

Additional packages needed to run the example notebooks in the `examples/` folder:

```bash
# For raster visualization (notebooks 15, 21)
pip install rasterio

# For coordinate system operations (notebook 14)
pip install pyproj
```

### Optional Dependencies

For specific features that aren't required for core functionality:

```bash
# DSS file operations (requires Java JRE/JDK 8+)
pip install pyjnius

# Remote execution backends
pip install paramiko      # SSH remote execution
pip install pywinrm       # WinRM remote execution
pip install docker        # Docker container execution
pip install boto3         # AWS EC2 execution
pip install azure-identity azure-mgmt-compute  # Azure execution

# Or install all remote backends at once:
pip install ras-commander[remote-all]
```

Note: `pathlib` is built into Python 3.4+ and does not need to be installed separately.


#### Work in a Local Copy

If you want to make revisions and work actively in your local version of ras-commander, just skip the pip install rascommander step above and clone a fork of the repo to your local machine using Git (ask ChatGPT if you need help).  Most of the notebooks and examples in this repo have a code segment similar to the one below, that works as long as the script is located in a first-level subfolder of the ras-commander repository:
```
# Flexible imports to allow for development without installation
try:
    # Try to import from the installed package
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
except ImportError:
    # If the import fails, add the parent directory to the Python path
    current_file = Path(__file__).resolve()
    parent_directory = current_file.parent.parent
    sys.path.append(str(parent_directory))
    # Alternately, you can just define a path sys.path.append(r"c:/path/to/rascommander/rascommander)")
    
    # Now try to import again
    from ras_commander import init_ras_project, RasExamples, RasCmdr, RasPlan, RasGeo, RasUnsteady, RasUtils, ras
```
It is highly suggested to fork this repository before going this route, and using Git to manage your changes!  This allows any revisions to the ras-commander classes and functions to be actively edited and developed by end users. The folders "/workspace/, "/projects/", or "my_projects/" are included in the .gitignore, so users can place you custom scripts there for any project data they don't want to be tracked by git.

## Quick Start Guide

```
from ras_commander import init_ras_project, RasCmdr, RasPlan
```

### Initialize a project (single project)
```python
# Basic initialization using default HEC-RAS location (on C: drive)
init_ras_project(r"/path/to/project", "6.5")

# Specifying a custom path to Ras.exe (useful if HEC-RAS is not installed on C: drive)
init_ras_project(r"/path/to/project", r"D:/Programs/HEC/HEC-RAS/6.5/Ras.exe")
```

### Initialize a project (multiple projects)
```python
your_ras_project = RasPrj()
init_ras_project(r"/path/to/project", "6.5", ras_object=your_ras_project)
```

## Accessing Plan, Unsteady and Boundary Conditions Dataframes
Using the default 'ras" object, othewise substitute your_ras_project for muli-project scripts
```
print("\nPlan Files DataFrame:")
ras.plan_df
```
```
print("\nFlow Files DataFrame:")
ras.flow_df
```
```
print("\nUnsteady Flow Files DataFrame:")
ras.unsteady_df
```
```
print("\nGeometry Files DataFrame:")
ras.geom_df
```
```
print("\nBoundary Conditions DataFrame:")
ras.boundaries_df
```
```
print("\nHDF Entries DataFrame:")
ras.get_hdf_entries()
```



### Execute a single plan
```
RasCmdr.compute_plan("01", dest_folder=r"/path/to/results", overwrite_dest=True)
```

### Execute plans in parallel
```
results = RasCmdr.compute_parallel(
    plan_number=["01", "02"],
    max_workers=2,
    num_cores=2,
    dest_folder=r"/path/to/results",
    overwrite_dest=True
)
```

### Modify a plan
```
RasPlan.set_geom("01", "02")
```

### Execution Modes

RAS Commander provides multiple methods for executing HEC-RAS plans:

#### Modern Command-Line Execution (HEC-RAS 6.0+)

**Single Plan Execution:**
```python
# Execute a single plan
success = RasCmdr.compute_plan("01", dest_folder=r"/path/to/results")
print(f"Plan execution {'successful' if success else 'failed'}")
```

**Sequential Execution of Multiple Plans:**
```python
# Execute multiple plans in sequence in a test folder
results = RasCmdr.compute_test_mode(
    plan_number=["01", "02", "03"],
    dest_folder_suffix="[Test]"
)
for plan, success in results.items():
    print(f"Plan {plan}: {'Successful' if success else 'Failed'}")
```

**Parallel Execution of Multiple Plans:**
```python
# Execute multiple plans concurrently
results = RasCmdr.compute_parallel(
    plan_number=["01", "02", "03"],
    max_workers=3,
    num_cores=2
)
for plan, success in results.items():
    print(f"Plan {plan}: {'Successful' if success else 'Failed'}")
```

#### Legacy COM Interface Execution (HEC-RAS 3.x-6.x)

**For older HEC-RAS versions**, use the RasControl class:

```python
# Initialize with version
init_ras_project(project_path, "4.1")  # or "41", "5.0.6", "506", "6.6", etc.

# Run a plan (auto-sets as current, blocks until complete)
success, messages = RasControl.run_plan("02")

# Extract steady state results
df_steady = RasControl.get_steady_results("02")
print(f"Extracted {len(df_steady)} rows with {df_steady['profile'].nunique()} profiles")

# Extract unsteady time series (includes "Max WS" special timestep)
df_unsteady = RasControl.get_unsteady_results("01", max_times=20)
times = RasControl.get_output_times("01")
```

**Key RasControl Features:**
- Uses plan numbers, not file paths (ras-commander style)
- Automatically sets current plan before operations
- Supports steady AND unsteady extraction
- Works with versions 3.1, 4.1, 5.0.x, 6.0, 6.3, 6.6
- Open-operate-close pattern (no HEC-RAS window left open)
- Perfect for version migration validation

### Working with Multiple Projects

RAS Commander allows working with multiple HEC-RAS projects simultaneously:

```python
# Initialize multiple projects
project1 = RasPrj()
init_ras_project(path1, "6.6", ras_object=project1)
project2 = RasPrj()
init_ras_project(path2, "6.6", ras_object=project2)

# Perform operations on each project
RasCmdr.compute_plan("01", ras_object=project1, dest_folder=folder1)
RasCmdr.compute_plan("01", ras_object=project2, dest_folder=folder2)

# Compare results between projects
print(f"Project 1: {project1.project_name}")
print(f"Project 2: {project2.project_name}")

# Always specify the ras_object parameter when working with multiple projects
# to avoid confusion with the global 'ras' object
```

This is useful for comparing different river systems, running scenario analyses across multiple watersheds, or managing a suite of related models.

#### Core HEC-RAS Automation Classes

- `RasPrj`: Manages HEC-RAS projects, handling initialization and data loading
- `RasCmdr`: Handles execution of HEC-RAS simulations via command line
- `RasControl`: Legacy version support via COM interface for HEC-RAS 3.x-6.x
- `RasPlan`: Provides functions for modifying and updating plan files
- `RasGeo`: Handles 2D Manning's n land cover operations
- `RasGeometry`: **NEW** Comprehensive 1D geometry parsing (cross sections, storage, connections)
- `RasGeometryUtils`: **NEW** Geometry parsing utilities (FORTRAN fixed-width, count interpretation)
- `RasBreach`: Dam breach parameter modification in plan files
- `RasUnsteady`: Manages unsteady flow file operations
- `RasUtils`: Contains utility functions for file operations and data management
- `RasMap`: Parses RASMapper configuration files and automates floodplain mapping
- `RasExamples`: Manages and loads HEC-RAS example projects

#### HDF Data Access Classes
- `HdfBase`: Core functionality for HDF file operations
- `HdfBndry`: Enhanced boundary condition handling
- `HdfMesh`: Comprehensive mesh data management
- `HdfPlan`: Plan data extraction and analysis
- `HdfResultsMesh`: Advanced mesh results processing
- `HdfResultsPlan`: Plan results analysis
- `HdfResultsXsec`: Cross-section results processing
- `HdfStruc`: Structure data and SA/2D connection management
- `HdfResultsBreach`: **NEW** Dam breach results extraction from HDF files
- `HdfHydraulicTables`: **NEW** Cross section property tables (HTAB) for rating curves
- `HdfPipe`: Pipe network analysis tools
- `HdfPump`: Pump station analysis capabilities
- `HdfFluvialPluvial`: Fluvial-pluvial boundary analysis
- `HdfPlot` & `HdfResultsPlot`: Specialized plotting utilities

### Project Organization Diagram

```
ras_commander
├── ai_tools
│   └── [AI Knowledge Bases](https://github.com/gpt-cmdr/ras-commander/tree/main/ai_tools/llm_knowledge_bases)
├── examples
│   └── [Examples Notebooks](https://github.com/gpt-cmdr/ras-commander/tree/main/ras_commander)
├── ras_commander
│   ├── __init__.py
│   ├── _version.py
│   ├── Decorators.py
│   ├── LoggingConfig.py
│   ├── RasCmdr.py
│   ├── RasExamples.py
│   ├── RasGeo.py
│   ├── RasPlan.py
│   ├── RasPrj.py
│   ├── RasUnsteady.py
│   ├── RasUtils.py
│   ├── HdfBase.py
│   ├── HdfBndry.py
│   ├── HdfMesh.py
│   ├── HdfPlan.py
│   ├── HdfResultsMesh.py
│   ├── HdfResultsPlan.py
│   ├── HdfResultsXsec.py
│   ├── HdfStruc.py
│   ├── HdfPipe.py
│   ├── HdfPump.py
│   ├── HdfFluvialPluvial.py
│   ├── HdfPlot.py
│   └── HdfResultsPlot.py
├── .gitignore
├── LICENSE
├── README.md
├── STYLE_GUIDE.md
├── Comprehensive_Library_Guide.md
├── pyproject.toml
├── setup.py
```

### Accessing HEC Examples through RasExamples

The `RasExamples` class provides functionality for quickly loading and managing HEC-RAS example projects. This is particularly useful for testing and development purposes.  All examples in the ras-commander repository currently utilize HEC example projects to provide fully running scripts and notebooks for end user testing, demonstration and adaption. 

Key features:
- Download and extract HEC-RAS example projects
- List available project categories and projects
- Extract specific projects for use
- Manage example project data efficiently

Example usage:
from ras_commander import RasExamples

```
categories = ras_examples.list_categories()
projects = ras_examples.list_projects("Steady Flow")
extracted_paths = ras_examples.extract_project(["Bald Eagle Creek", "Muncie"])
```

The RasExamples class is used to provide an alternative to traditional unit testing, with example notebooks doubling as tests and in-context examples for the end user.  This increases interpretability by LLM's, reducing hallucinations.  

### RasPrj

The `RasPrj` class is central to managing HEC-RAS projects within the ras-commander library. It handles project initialization, data loading, and provides access to project components.

Key features:
- Initialize HEC-RAS projects
- Load and manage project data (plans, geometries, flows, etc.)
- Provide easy access to project files and information

Note: While a global `ras` object is available for convenience, you can create multiple `RasPrj` instances to manage several projects simultaneously.

Example usage:
```
from ras_commander import RasPrj, init_ras_project
```

#### Using the global ras object
```
init_ras_project("/path/to/project", "6.5")
```

#### Creating a custom RasPrj instance
```
custom_project = RasPrj()
init_ras_project("/path/to/another_project", "6.5", ras_instance=custom_project)
```

### RasHdf

The `RasHdf` class provides utilities for working with HDF files in HEC-RAS projects, enabling easy access to simulation results and model data.

Example usage:

```python
from ras_commander import RasHdf, init_ras_project, RasPrj

# Initialize project with a custom ras object
custom_ras = RasPrj()
init_ras_project("/path/to/project", "6.5", ras_instance=custom_ras)

# Get runtime data for a specific plan
plan_number = "01"
runtime_data = RasHdf.get_runtime_data(plan_number, ras_object=custom_ras)
print(runtime_data)
```
This class simplifies the process of extracting and analyzing data from HEC-RAS HDF output files, supporting tasks such as post-processing and result visualization.

#### Infrastructure Analysis
```python
from ras_commander import HdfPipe, HdfPump

# Analyze pipe network
pipe_network = HdfPipe.get_pipe_network(hdf_path)
conduits = HdfPipe.get_pipe_conduits(hdf_path)

# Analyze pump stations
pump_stations = HdfPump.get_pump_stations(hdf_path)
pump_performance = HdfPump.get_pump_station_summary(hdf_path)
```

#### Advanced Results Analysis
```python
from ras_commander import HdfResultsMesh

# Get maximum water surface and velocity
max_ws = HdfResultsMesh.get_mesh_max_ws(hdf_path)
max_vel = HdfResultsMesh.get_mesh_max_face_v(hdf_path)

# Visualize results
from ras_commander import HdfResultsPlot
HdfResultsPlot.plot_results_max_wsel(max_ws)
```

#### Fluvial-Pluvial Analysis
```python
from ras_commander import HdfFluvialPluvial

boundary = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(
    hdf_path,
    delta_t=12  # Time threshold in hours
)
```

## Examples

Check out the examples in the repository to learn how to use RAS Commander:

### Project Setup
- `00_Using_RasExamples.ipynb`: Download and extract HEC-RAS example projects
- `01_project_initialization.ipynb`: Initialize HEC-RAS projects and explore their components

### File Operations
- `02_plan_and_geometry_operations.ipynb`: Clone and modify plan and geometry files
- `03_unsteady_flow_operations.ipynb`: Extract and modify boundary conditions
- `09_plan_parameter_operations.ipynb`: Retrieve and update plan parameters

### Execution Modes
- `05_single_plan_execution.ipynb`: Execute a single plan with specific options
- `06_executing_plan_sets.ipynb`: Different ways to specify and execute plan sets
- `07_sequential_plan_execution.ipynb`: Run multiple plans in sequence
- `08_parallel_execution.ipynb`: Run multiple plans in parallel

### Legacy Version Support
- `17_extracting_profiles_with_hecrascontroller.ipynb`: Using RasControl for HEC-RAS 3.x-6.x via COM interface

### Advanced Operations
- `04_multiple_project_operations.ipynb`: Work with multiple HEC-RAS projects simultaneously

These examples demonstrate practical applications of RAS Commander for automating HEC-RAS workflows, from basic operations to advanced scenarios.

## Documentation

For detailed usage instructions and API documentation, please refer to the [Comprehensive Library Guide](Comprehensive_Library_Guide.md).

## Future Development

The ras-commander library is an ongoing project. Future plans include:
- Integration of more advanced AI-driven features
- Expansion of HMS and DSS functionalities
- Community-driven development of new modules and features

## Related Resources

- [HEC-Commander Blog](https://github.com/gpt-cmdr/HEC-Commander/tree/main/Blog)
- [GPT-Commander YouTube Channel](https://www.youtube.com/@GPT_Commander)
- [ChatGPT Examples for Water Resources Engineers](https://github.com/gpt-cmdr/HEC-Commander/tree/main/ChatGPT%20Examples)


## Style Guide

This project follows a specific style guide to maintain consistency across the codebase. Please refer to the [Style Guide](STYLE_GUIDE.md) for details on coding conventions, documentation standards, and best practices.

## Acknowledgments

RAS Commander is based on the HEC-Commander project's "Command Line is All You Need" approach, leveraging the HEC-RAS command-line interface for automation. The initial development of this library was presented in the HEC-Commander Tools repository. In a 2024 Australian Water School webinar, Bill demonstrated the derivation of basic HEC-RAS automation functions from plain language instructions. Leveraging the previously developed code and AI tools, the library was created. The primary tools used for this initial development were Anthropic's Claude, GPT-4, Google's Gemini Experimental models, and the Cursor AI Coding IDE.

Additionally, we would like to acknowledge the following notable contributions and attributions for open source projects which significantly influenced the development of RAS Commander:

1. Contributions: Sean Micek's [`funkshuns`](https://github.com/openSourcerer9000/funkshuns), [`TXTure`](https://github.com/openSourcerer9000/TXTure), and [`RASmatazz`](https://github.com/openSourcerer9000/RASmatazz) libraries provided inspiration, code examples and utility functions which were adapted with AI for use in RAS Commander. Sean has also contributed heavily to 

- Development of additional HDF functions for detailed analysis and mapping of HEC-RAS results within the RasHdf class.
- Development of the prototype `RasCmdr` class for executing HEC-RAS simulations.

2. Attribution: The [`pyHMT2D`](https://github.com/psu-efd/pyHMT2D/) project by Xiaofeng Liu, which provided insights into HDF file handling methods for HEC-RAS outputs.  Many of the functions in the [Ras_2D_Data.py](https://github.com/psu-efd/pyHMT2D/blob/main/pyHMT2D/Hydraulic_Models_Data/RAS_2D/RAS_2D_Data.py) file were adapted with AI for use in RAS Commander. 

   Xiaofeng Liu, Ph.D., P.E.,    Associate Professor, Department of Civil and Environmental Engineering
   Institute of Computational and Data Sciences, Penn State University

3. Attribution: The [ffrd\rashdf'](https://github.com/fema-ffrd/rashdf) project by FEMA-FFRD (FEMA Future of Flood Risk Data) was incorporated, revised, adapted and extended in rascommander's RasHDF libaries (where noted). 

These acknowledgments recognize the contributions and inspirations that have helped shape RAS Commander, ensuring proper attribution for the ideas and code that have influenced its development.

4. Chris Goodell, "Breaking the HEC-RAS Code" - Studied and used as a reference for understanding the inner workings of HEC-RAS, providing valuable insights into the software's functionality and structure.

5. [HEC-Commander Tools](https://github.com/gpt-cmdr/HEC-Commander) - Inspiration and initial code base for the development of RAS Commander.

## Official RAS Commander AI-Generated Songs:

[No More Wait and See (Bluegrass)](https://suno.com/song/16889f3e-50f1-4afe-b779-a41738d7617a)  
  
  
[No More Wait and See (Cajun Zydeco)](https://suno.com/song/4441c45d-f6cd-47b9-8fbc-1f7b277ee8ed)  
  
## Other Resources

Notebook version of RAS-Commander: [RAS-Commander Notebook in the HEC-Commander Tools Repository](https://github.com/gpt-cmdr/HEC-Commander/tree/main/RAS-Commander)  

Youtube Tutorials for HEC-Commander Tools and RAS-Commander: [GPT-Commander on YouTube](https://www.youtube.com/@GPT_Commander/videos)

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to submit pull requests, report issues, and suggest improvements.

## LICENSE

This software is released under the MIT license.

## Contact

For questions, suggestions, or support, please contact:  
William Katzenmeyer, P.E., C.F.M. - heccommander@gmail.com

==================================================

File: C:\GH\ras-commander\settings.db
==================================================
SQLite format 3   @                                                                     .v  n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N)eindexix_settings_idsettingsCREATE INDEX ix_settings_id ON settings (id)VtablesettingssettingsCREATE TABLE settings (
	id VARCHAR NOT NULL, 
	anthropic_api_key TEXT, 
	openai_api_key TEXT, 
	selected_model VARCHAR, 
	context_mode VARCHAR, 
	omit_folders TEXT, 
	omit_extensions TEXT, 
	omit_files TEXT, 
	chunk_level VARCHAR, 
	initial_chunk_size INTEGER, 
	followup_chunk_size INTEGER, 
	PRIMARY KEY (id)
)/C indexsqlite_autoindex_settings_1settings                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
==================================================

File: C:\GH\ras-commander\setup.py
==================================================
from setuptools import setup, find_packages
from setuptools.command.build_py import build_py
import subprocess
from pathlib import Path

class CustomBuildPy(build_py):
    def run(self):
        # Clean up __pycache__ folders
        root_dir = Path(__file__).parent
        for pycache_dir in root_dir.rglob('__pycache__'):
            if pycache_dir.is_dir():
                for cache_file in pycache_dir.iterdir():
                    cache_file.unlink()  # Delete each file
                pycache_dir.rmdir()      # Delete the empty directory
                print(f"Cleaned up: {pycache_dir}")

        # Run the summary_knowledge_bases.py script
        script_path = Path(__file__).parent / 'ai_tools' / 'generate_llm_knowledge_bases.py'
        try:
            subprocess.run(['python', str(script_path)], check=True)
        except subprocess.CalledProcessError:
            print("Warning: Knowledge base generation script failed, continuing with build")
        except FileNotFoundError:
            print("Warning: Knowledge base generation script not found, continuing with build")
        
        # Continue with the regular build process
        super().run()

setup(
    name="ras-commander",
    version="0.86.0",
    packages=find_packages(),
    include_package_data=True,
    python_requires='>=3.10',
    author="William M. Katzenmeyer, P.E., C.F.M.",
    author_email="heccommander@gmail.com",
    description="A Python library for automating HEC-RAS 6.x operations",
    long_description=open('README.md').read(),
    long_description_content_type="text/markdown",
    url="https://github.com/gpt-cmdr/ras-commander",
    cmdclass={
        'build_py': CustomBuildPy,
    },
    install_requires=[
        'h5py',
        'numpy',
        'pandas',
        'requests',
        'tqdm',
        'scipy',
        'xarray',
        'geopandas',
        'matplotlib',
        'shapely',
        'rasterstats',
        'rtree',
        'pywin32>=227',    # Required for RasControl COM interface
        'psutil>=5.6.6',   # Required for RasControl process management
    ],
    extras_require={
        # Remote execution backends (PsExec worker has no extra deps)
        'remote': [],  # Base remote - PsExec only, no additional deps
        'remote-ssh': ['paramiko>=3.0'],
        'remote-winrm': ['pywinrm>=0.4.3'],
        'remote-docker': ['docker>=6.0'],
        'remote-aws': ['boto3>=1.28'],
        'remote-azure': ['azure-identity>=1.14', 'azure-mgmt-compute>=30.0'],
        'remote-all': [
            'paramiko>=3.0',
            'pywinrm>=0.4.3',
            'docker>=6.0',
            'boto3>=1.28',
            'azure-identity>=1.14',
            'azure-mgmt-compute>=30.0',
        ],
    })

"""
ras-commander setup.py

This file is used to build and publish the ras-commander package to PyPI.

To build and publish this package, follow these steps:

1. Ensure you have the latest versions of setuptools, wheel, and twine installed:
   pip install --upgrade setuptools wheel twine

2. Update the version number in ras_commander/__init__.py (if not using automatic versioning)

3. Create source distribution and wheel:
   python setup.py sdist bdist_wheel

4. Check the distribution:
   twine check dist/*

5. Upload to Test PyPI (optional):
   twine upload --repository testpypi dist/*

6. Install from Test PyPI to verify (optional):
   pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple ras-commander

7. Upload to PyPI:
   twine upload dist/* --username __token__ --password <your_api_key>


8. Install from PyPI to verify:
   pip install ras-commander

Note: Ensure you have the necessary credentials and access rights to upload to PyPI.
For more information, visit: https://packaging.python.org/tutorials/packaging-projects/

"""

==================================================

File: C:\GH\ras-commander\STYLE_GUIDE.md
==================================================
# RAS Commander (ras-commander) Style Guide

## Table of Contents
1. [Naming Conventions](#1-naming-conventions)
2. [Code Structure and Organization](#2-code-structure-and-organization)
3. [Documentation and Comments](#3-documentation-and-comments)
4. [Code Style](#4-code-style)
5. [Error Handling](#5-error-handling)
6. [Testing](#6-testing)
7. [Version Control](#7-version-control)
8. [Type Hinting](#8-type-hinting)
9. [Project-Specific Conventions](#9-project-specific-conventions)
10. [Inheritance](#10-inheritance)
11. [RasUtils Usage](#11-rasutils-usage)
12. [Working with RasExamples](#12-working-with-rasexamples)
13. [Logging](#13-logging)
14. [Decorator Usage](#14-decorator-usage)
15. [Static Class Pattern](#15-static-class-pattern)

## 1. Naming Conventions

### 1.1 General Rules
- Use `snake_case` for all function and variable names
- Use `PascalCase` for class names
- Use `UPPER_CASE` for constants

### 1.2 Library-Specific Naming
- Informal Name: RAS Commander
- Package Name and GitHub Library Name: ras-commander (with a hyphen)
- Import Name: ras_commander (with an underscore)
- Main Class of functions for HEC-RAS Automation: RasCmdr

### 1.3 Function Naming
- Start function names with a verb describing the action
- Use clear, descriptive names
- Common verbs and their uses:
  - `get_`: retrieve data
  - `set_`: set values or properties
  - `compute_`: execute or calculate
  - `clone_`: copy
  - `clear_`: remove or reset data
  - `find_`: search
  - `update_`: modify existing data

### 1.4 Abbreviations
Use the following abbreviations consistently throughout the codebase:

- ras: HEC-RAS
- prj: Project
- geom: Geometry
- pre: Preprocessor
- geompre: Geometry Preprocessor
- num: Number
- init: Initialize
- XS: Cross Section
- DSS: Data Storage System
- GIS: Geographic Information System
- BC: Boundary Condition
- IC: Initial Condition
- TW: Tailwater

Use these abbreviations in lowercase for function and variable names (e.g., `geom`, not `Geom` or `GEOM`).

### 1.5 Class Naming
- Use `PascalCase` for class names (e.g., `FileOperations`, `PlanOperations`, `RasCmdr`)
- Class names should be nouns or noun phrases

### 1.6 Variable Naming
- Use descriptive names indicating purpose or content
- Prefix boolean variables with `is_`, `has_`, or similar

### 1.7 HDF Class Method Naming

For HDF class methods, follow these naming conventions:
- `get_` prefix for methods that extract data from HDF files
- Use specific entity names in method names: `get_mesh_max_ws()` instead of just `get_max_ws()`
- For methods that return GeoDataFrame objects, include the relevant geometry type in the name: `get_mesh_cell_polygons()`
- Prefix private helper methods with underscore: `_get_mesh_timeseries_output_path()`
- Use consistent suffixes for related methods: `get_mesh_max_ws()`, `get_mesh_min_ws()`

## 2. Code Structure and Organization

### 2.1 File Organization
- Group related functions into appropriate classes
- Keep each class in its own file, named after the class

### 2.2 Function Organization
- Order functions logically within a class
- Place common or important functions at the top of the class

### 2.3 Module Structure
- Use the following order for module contents:
  1. Module-level docstring
  2. Imports (grouped and ordered)
  3. Constants
  4. Classes
  5. Functions

## 3. Documentation and Comments

### 3.1 Docstrings
- Use docstrings for all modules, classes, methods, and functions
- Follow Google Python Style Guide format
- Include parameters, return values, and a brief description
- For complex functions, include examples in the docstring

### 3.2 Comments
- Use inline comments sparingly, only for complex logic
- Keep comments up-to-date with code changes
- Use TODO comments for future work, formatted as: `# TODO: description`

## 4. Code Style

### 4.1 Imports
- Order imports as follows:
  1. Standard library imports
  2. Third-party library imports
  3. Local application imports
- Use absolute imports
- Use `import ras_commander as ras` for shortening the library name in examples

### 4.2 Whitespace
- Follow PEP 8 guidelines
- Use 4 spaces for indentation (no tabs)
- Use blank lines to separate logical sections of code

### 4.3 Line Length
- Limit lines to 79 characters for code, 72 for comments and docstrings
- Use parentheses for line continuation in long expressions

## 5. Error Handling

**Use Logging Instead of Prints**
Ensure that every operation that can fail or needs to provide feedback to the user is logged instead of using `print`. This will help in debugging and improve monitoring during execution.

   ```python
   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   ```

   Example of replacing a `print` with logging:
   ```python
   logging.info('Starting HEC-RAS simulation...')
   ```

- Use explicit exception handling with try/except blocks
- Raise custom exceptions when appropriate, with descriptive messages
- Use logging for error reporting and debugging information
- Use specific exception types when raising errors (e.g., `ValueError`, `FileNotFoundError`)
- Provide informative error messages that include relevant details
- Implement proper cleanup in finally blocks when necessary
- For user-facing functions, consider wrapping internal exceptions in custom exceptions specific to ras-commander

Example:
```python
try:
    result = compute_plan(plan_number)
except FileNotFoundError as e:
    raise RasCommanderError(f"Plan file not found: {e}")
except ValueError as e:
    raise RasCommanderError(f"Invalid plan parameter: {e}")
except Exception as e:
    raise RasCommanderError(f"Unexpected error during plan computation: {e}")
```

### 5.1 Logging Configuration

- Always use the module-specific logger from `LoggingConfig`:
  ```python
  from .LoggingConfig import get_logger
  logger = get_logger(__name__)
  ```

- Use appropriate log levels:
  - DEBUG: Detailed information useful for debugging
  - INFO: Confirmation of expected operations
  - WARNING: Something unexpected happened but execution continues
  - ERROR: An operation failed but the program can continue
  - CRITICAL: Program cannot continue

- Include informative context in log messages:
  ```python
  logger.info(f"Processing file: {file_path}")
  logger.error(f"Failed to read HDF file {hdf_path}: {str(e)}")
  ```

- Use the `@log_call` decorator for automatic function entry/exit logging

## 6. Testing

- The RasExamples() Class is provided for testing directly with HEC Example projects
- The library eschews traditional unit testing in favor of this approach
- The unit tests double as useful examples that can be extended by end users 
- Any notebooks in the repo should include a working model (hosted elsewhere) 

## 7. Version Control

- Use meaningful commit messages that clearly describe the changes made
- Create feature branches for new features or significant changes
- Submit pull requests for code review before merging into the main branch
- Keep commits focused and atomic (one logical change per commit)
- Use git tags for marking releases
- Follow semantic versioning for release numbering

## 8. Type Hinting

- Use type hints for all function parameters and return values
- Use the `typing` module for complex types (e.g., `List`, `Dict`, `Optional`)
- Include type hints in function signatures and docstrings
- Use `Union` for parameters that can accept multiple types
- For methods that don't return a value, use `-> None`

Example:
```python
from typing import List, Optional

def process_plans(plan_numbers: List[str], max_workers: Optional[int] = None) -> bool:
    # Function implementation
    return True
```

## 9. Project-Specific Conventions

### 9.1 RAS Instance Handling
- Design functions to accept an optional `ras_object` parameter:
  ```python
  def some_function(param1, param2, ras_object=None):
      ras_obj = ras_object or ras
      ras_obj.check_initialized()
      # Function implementation
  ```

### 9.2 File Path Handling
- Use `pathlib.Path` for file and directory path manipulations
- Convert string paths to Path objects at the beginning of functions

### 9.3 DataFrame Handling
- Use pandas for data manipulation and storage where appropriate
- Prefer method chaining for pandas operations to improve readability

### 9.4 Parallel Execution
- Follow the guidelines in the "Benchmarking is All You Need" blog post for optimal core usage in parallel plan execution

### 9.5 Function Return Values
- Prefer returning meaningful values over modifying global state
- Use tuple returns for multiple values instead of modifying input parameters
- Use consistent return types across related functions:
  - Use GeoDataFrame for functions returning spatial data
  - Use pandas DataFrame for tabular data
  - Use xarray DataArray/Dataset for multi-dimensional data with coordinates
  - Use Optional[Type] for functions that might return None

- Provide clear error handling for function returns:
  - Return empty DataFrame/GeoDataFrame instead of None when appropriate
  - Use Optional typing for functions that might return None
  - Document possible return values in docstrings


## 10. RasUtils Usage

- Use RasUtils for general-purpose utility functions that don't fit into other specific classes
- When adding new utility functions, ensure they are static methods of the RasUtils class
- Keep utility functions focused and single-purpose
- Document utility functions thoroughly, including examples of usage

Example:
```python
class RasUtils:
    @staticmethod
    def create_backup(file_path: Path, backup_suffix: str = "_backup") -> Path:
        """
        Create a backup of the specified file.

        Args:
            file_path (Path): Path to the file to be backed up
            backup_suffix (str): Suffix to append to the backup file name

        Returns:
            Path: Path to the created backup file

        Example:
            >>> backup_path = RasUtils.create_backup(Path("project.prj"))
            >>> print(f"Backup created at: {backup_path}")
        """
        # Function implementation
```

## 11. Working with RasExamples

- Use RasExamples for managing and loading example HEC-RAS projects
- Always check if example projects are already downloaded before attempting to download them again
- Use the `list_categories()` and `list_projects()` methods to explore available examples
- When extracting projects, use meaningful names and keep track of extracted paths
- Clean up extracted projects when they are no longer needed using `clean_projects_directory()`

Example:
```python
ras_examples = RasExamples()
if not ras_examples.is_project_extracted("Bald Eagle Creek"):
    extracted_path = ras_examples.extract_project("Bald Eagle Creek")[0]
    # Use the extracted project
    # ...
    # Clean up when done
    RasUtils.remove_with_retry(extracted_path, is_folder=True)
```

Remember, consistency is key. When in doubt, prioritize readability and clarity in your code. Always consider the maintainability and extensibility of the codebase when making design decisions.

## 12. Logging

Instructions for setting up a minimal logging decorator and applying it to functions:

1. Create logging_config.py:
```python
import logging
import functools

def setup_logging(level=logging.INFO):
    logging.basicConfig(level=level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def log_call(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        logger.info(f"Calling {func.__name__}")
        return func(*args, **kwargs)
    return wrapper

setup_logging()
```

2. In each module file (e.g., RasPrj.py, RasPlan.py):
   - Add at the top: `from ras_commander.logging_config import log_call`
   - Remove all existing logging configurations and logger instantiations

3. Apply the decorator to functions:
   - Replace existing logging statements with the `@log_call` decorator
   - Remove any manual logging within the function body

Example changes to functions:

Before:
```python
def compute_plan(plan_number, dest_folder=None, ras_object=None, clear_geompre=False, num_cores=None):
    logging.info(f"Computing plan {plan_number}")
    # ... function logic ...
    logging.info(f"Plan {plan_number} computation complete")
    return result
```

After:
```python
@log_call
def compute_plan(plan_number, dest_folder=None, ras_object=None, clear_geompre=False, num_cores=None):
    # ... function logic ...
    return result
```

Apply this pattern across all functions in the library. This approach will significantly reduce the code footprint while maintaining basic logging functionality.

## 13. Decorator Usage for Ras* and Hdf* Classes to Simplify Inputs

- Use the `@log_call` decorator for all public methods to enable automatic logging of function entry and exit
- Use the `@standardize_input` decorator for methods that accept HDF file paths to ensure consistent handling
- Place decorators on separate lines for better readability when multiple decorators are used
- Order decorators consistently: `@staticmethod` first, followed by `@log_call`, then `@standardize_input`

Example:
```python
@staticmethod
@log_call
@standardize_input(file_type='plan_hdf')
def get_mesh_max_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
    # Function implementation
```

## 15. Static Class Pattern

Many classes in the ras-commander library follow a static method pattern where:
- All methods are decorated with `@staticmethod`
- The class serves as a namespace for related functionality
- No instantiation is required to use the methods

When implementing such classes:
- Include a note in the class docstring stating "All methods in this class are static and designed to be used without instantiation"
- Use consistent method organization, typically starting with core methods followed by helper methods
- Private helper methods should still use the underscore prefix (e.g., `_parse_file`)
- Avoid storing state in class variables unless absolutely necessary

Example:
```python
class HdfMesh:
    """
    A class for handling mesh-related operations on HEC-RAS HDF files.
    
    All methods in this class are static and designed to be used without instantiation.
    """
    
    @staticmethod
    @log_call
    def get_mesh_area_names(hdf_path: Path) -> List[str]:
        # Method implementation
        
    @staticmethod
    def _parse_mesh_data(data: np.ndarray) -> Dict:
        # Helper method implementation
```
==================================================

File: C:\GH\ras-commander\examples\00_Using_RasExamples.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:36:37.154554Z",
          "iopub.status.busy": "2025-11-17T17:36:37.154336Z",
          "iopub.status.idle": "2025-11-17T17:36:39.149660Z",
          "shell.execute_reply": "2025-11-17T17:36:39.149051Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:37 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using RASExamples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Method for Calling HEC-RAS Example Projects by Folder Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:36:39.152344Z",
          "iopub.status.busy": "2025-11-17T17:36:39.152025Z",
          "iopub.status.idle": "2025-11-17T17:37:05.377294Z",
          "shell.execute_reply": "2025-11-17T17:37:05.376750Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Found zip file: C:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:39 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Project 'Muncie' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Existing folder for project 'Muncie' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Extracting project 'Davis'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Project 'Davis' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Existing folder for project 'Davis' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Successfully extracted project 'Davis' to C:\\GH\\ras-commander\\examples\\example_projects\\Davis\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Special Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Extracting special project 'NewOrleansMetro'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - Downloading special project from: https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299502039/299502111/1/1747692522764/NewOrleansMetroPipesExample.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:36:40 - ras_commander.RasExamples - INFO - This may take a few moments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:03 - ras_commander.RasExamples - INFO - Downloaded special project zip file to C:\\GH\\ras-commander\\examples\\example_projects\\NewOrleansMetro_temp.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:04 - ras_commander.RasExamples - INFO - Successfully extracted special project 'NewOrleansMetro' to C:\\GH\\ras-commander\\examples\\example_projects\\NewOrleansMetro\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:04 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Special Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:04 - ras_commander.RasExamples - INFO - Extracting special project 'BeaverLake'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:04 - ras_commander.RasExamples - INFO - Downloading special project from: https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299501780/299502090/1/1747692179014/BeaverLake-SWMM-Import-Solution.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:04 - ras_commander.RasExamples - INFO - This may take a few moments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Downloaded special project zip file to C:\\GH\\ras-commander\\examples\\example_projects\\BeaverLake_temp.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Successfully extracted special project 'BeaverLake' to C:\\GH\\ras-commander\\examples\\example_projects\\BeaverLake\n"
          ]
        },
        {
          "data": {
            "text/plain": "[\"[WindowsPath('C:/GH/ras-commander/examples/example_projects/Balde Eagle Creek'),\\n\", \" WindowsPath('C:/GH/ras-commander/examples/example_projects/BaldEagleCrkMulti2D'),\\n\", \" WindowsPath('C:/GH/ras-commander/examples/example_projects/Muncie'),\\n\", \" WindowsPath('C:/GH/ras-commander/examples/example_projects/Davis'),\\n\", \" WindowsPath('C:/GH/ras-commander/examples/example_projects/NewOrleansMetro'),\\n\", \" WindowsPath('C:/GH/ras-commander/examples/example_projects/BeaverLake')]\"]"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This Code Cell is All You Need\n",
        "# This is what this Class was intended to do: Help me make repeatable workflows around HEC-RAS Example Projects for testing and demonstration purposes. \n",
        "\n",
        "# Extract specific projects\n",
        "RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\", \"Davis\", \"NewOrleansMetro\", \"BeaverLake\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RasExamples will not download a new .zip file if one already exists, this allows you to replace the Example_Projects_6_x.zip with your own zip file (with the same folder format as the HEC-RAS examples) and you will be able to load them by folder name for repeatable Test Driven Development\n",
        "\n",
        "Just make sure all project folders have unique folder names. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:05.379849Z",
          "iopub.status.busy": "2025-11-17T17:37:05.379645Z",
          "iopub.status.idle": "2025-11-17T17:37:05.384262Z",
          "shell.execute_reply": "2025-11-17T17:37:05.383590Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example projects are already downloaded.\n",
            "RasExamples.folder_df:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<property at 0x22070ad3f10>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check if example projects are already downloaded\n",
        "if RasExamples.projects_dir.exists():\n",
        "    print(\"Example projects are already downloaded.\")\n",
        "    print(\"RasExamples.folder_df:\")\n",
        "    display(RasExamples.folder_df)\n",
        "else:\n",
        "    print(\"Downloading example projects...\")\n",
        "    RasExamples.get_example_projects()\n",
        "    print(\"RasExamples.folder_df:\")\n",
        "    display(RasExamples.folder_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:05.386668Z",
          "iopub.status.busy": "2025-11-17T17:37:05.386326Z",
          "iopub.status.idle": "2025-11-17T17:37:05.390346Z",
          "shell.execute_reply": "2025-11-17T17:37:05.389909Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Available categories: 1D Sediment Transport, 1D Steady Flow Hydraulics, 1D Unsteady Flow Hydraulics, 2D Sediment Transport, 2D Unsteady Flow Hydraulics, Applications Guide, Pipes (beta), Water Quality\n"
          ]
        }
      ],
      "source": [
        "# List all categories\n",
        "categories = RasExamples.list_categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:05.392502Z",
          "iopub.status.busy": "2025-11-17T17:37:05.392312Z",
          "iopub.status.idle": "2025-11-17T17:37:05.396489Z",
          "shell.execute_reply": "2025-11-17T17:37:05.395948Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available categories:\n"
          ]
        },
        {
          "data": {
            "text/plain": "[\"['1D Sediment Transport',\\n\", \" '1D Steady Flow Hydraulics',\\n\", \" '1D Unsteady Flow Hydraulics',\\n\", \" '2D Sediment Transport',\\n\", \" '2D Unsteady Flow Hydraulics',\\n\", \" 'Applications Guide',\\n\", \" 'Pipes (beta)',\\n\", \" 'Water Quality']\"]"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nAvailable categories:\")\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:05.398659Z",
          "iopub.status.busy": "2025-11-17T17:37:05.398387Z",
          "iopub.status.idle": "2025-11-17T17:37:05.404171Z",
          "shell.execute_reply": "2025-11-17T17:37:05.403544Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Projects in category '1D Sediment Transport': BSTEM - Simple Example, Dredging Example, Reservoir Video Tutorial, SIAM Example, Simple Sediment Transport Example, Unsteady Sediment with Concentration Rules, Video Tutorial (Sediment Intro)\n"
          ]
        },
        {
          "data": {
            "text/plain": "[\"['BSTEM - Simple Example',\\n\", \" 'Dredging Example',\\n\", \" 'Reservoir Video Tutorial',\\n\", \" 'SIAM Example',\\n\", \" 'Simple Sediment Transport Example',\\n\", \" 'Unsteady Sediment with Concentration Rules',\\n\", \" 'Video Tutorial (Sediment Intro)']\"]"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# List projects in a specific category\n",
        "projects = RasExamples.list_projects(\"1D Sediment Transport\")\n",
        "projects\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:05.406270Z",
          "iopub.status.busy": "2025-11-17T17:37:05.406089Z",
          "iopub.status.idle": "2025-11-17T17:37:05.410665Z",
          "shell.execute_reply": "2025-11-17T17:37:05.410113Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - All available projects: BSTEM - Simple Example, Dredging Example, Reservoir Video Tutorial, SIAM Example, Simple Sediment Transport Example, Unsteady Sediment with Concentration Rules, Video Tutorial (Sediment Intro), Baxter RAS Mapper, Chapter 4 Example Data, ConSpan Culvert, Mixed Flow Regime Channel, Wailupe GeoRAS, Balde Eagle Creek, Bridge Hydraulics, ContractionExpansionMinorLosses, Culvert Hydraulics, Culverts with Flap Gates, Dam Breaching, Elevation Controled Gates, Inline Structure with Gated Spillways, Internal Stage and Flow Boundary Condition, JunctionHydraulics, Lateral Strcuture with Gates, Lateral Structure connected to a River Reach, Lateral Structure Overflow Weir, Lateral Structure with Culverts and Gates, Lateral Structure with Culverts, Levee Breaching, Mixed Flow Regime, Multiple Reaches with Hydraulic Structures, NavigationDam, Pumping Station with Rules, Pumping Station, Rule Operations, Simplified Physical Breaching, Storage Area Hydraulic Connection, UngagedAreaInflows, Unsteady Flow Encroachment Analysis, Chippewa_2D, Weise_2D, BaldEagleCrkMulti2D, Muncie, Example 1 - Critical Creek, Example 10 - Stream Junction, Example 11 - Bridge Scour, Example 12 - Inline Structure, Example 13 - Singler Bridge (WSPRO), Example 14 - Ice Covered River, Example 15 - Split Flow Junction with Lateral Weir, Example 16 - Channel Modification, Example 17 - Unsteady Flow Application, Example 18 - Advanced Inline Structure, Example 19 - Hydrologic Routing - ModPuls, Example 2 - Beaver Creek, Example 20 - HagerLatWeir, Example 21 - Overflow Gates, Example 22 - Groundwater Interflow, Example 23 - Urban Modeling, Example 24 - Mannings-n-Calibration, Example 3 - Single Culvert, Example 4 - Multiple Culverts, Example 5 - Multiple Openings, Example 6 - Floodway Determination, Example 7 - Multiple Plans, Example 8 - Looped Network, Example 9 - Mixed Flow Analysis, Davis, Nutrient Example, NewOrleansMetro, BeaverLake\n"
          ]
        },
        {
          "data": {
            "text/plain": "[\"['BSTEM - Simple Example',\\n\", \" 'Dredging Example',\\n\", \" 'Reservoir Video Tutorial',\\n\", \" 'SIAM Example',\\n\", \" 'Simple Sediment Transport Example',\\n\", \" 'Unsteady Sediment with Concentration Rules',\\n\", \" 'Video Tutorial (Sediment Intro)',\\n\", \" 'Baxter RAS Mapper',\\n\", \" 'Chapter 4 Example Data',\\n\", \" 'ConSpan Culvert',\\n\", \" 'Mixed Flow Regime Channel',\\n\", \" 'Wailupe GeoRAS',\\n\", \" 'Balde Eagle Creek',\\n\", \" 'Bridge Hydraulics',\\n\", \" 'ContractionExpansionMinorLosses',\\n\", \" 'Culvert Hydraulics',\\n\", \" 'Culverts with Flap Gates',\\n\", \" 'Dam Breaching',\\n\", \" 'Elevation Controled Gates',\\n\", \" 'Inline Structure with Gated Spillways',\\n\", \" 'Internal Stage and Flow Boundary Condition',\\n\", \" 'JunctionHydraulics',\\n\", \" 'Lateral Strcuture with Gates',\\n\", \" 'Lateral Structure connected to a River Reach',\\n\", \" 'Lateral Structure Overflow Weir',\\n\", \" 'Lateral Structure with Culverts and Gates',\\n\", \" 'Lateral Structure with Culverts',\\n\", \" 'Levee Breaching',\\n\", \" 'Mixed Flow \n...\n[Output truncated, 2482 characters total]"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# List all projects\n",
        "all_projects = RasExamples.list_projects()\n",
        "all_projects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:05.412579Z",
          "iopub.status.busy": "2025-11-17T17:37:05.412309Z",
          "iopub.status.idle": "2025-11-17T17:37:06.967896Z",
          "shell.execute_reply": "2025-11-17T17:37:06.967474Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:05 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Project 'Muncie' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Existing folder for project 'Muncie' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n"
          ]
        }
      ],
      "source": [
        "# Extract specific projects\n",
        "projects_to_extract = [\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"]\n",
        "extracted_paths = RasExamples.extract_project(projects_to_extract)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note about New Pipes and Conduits Version 6.6 Example Project\n",
        "\n",
        "Use project name \"Davis\" to explore pipes and conduits (introduced in version 6.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NEW Example Projects:\n",
        "#### The \"BeaverLake\" and \"NewOrleansMetro\" were published to the HEC-RAS Sample Datasets page.  These sample datasets are specifically for Pipe Systems introduced in HEC-RAS 6.6\n",
        "\n",
        "https://www.hec.usace.army.mil/confluence/rasdocs/hgt/latest/sample-datasets/small-city-urban-drainage-davis-ca\n",
        "\n",
        "https://www.hec.usace.army.mil/confluence/rasdocs/hgt/latest/sample-datasets/small-neighborhood-drainage-beaver-lake\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:06.969867Z",
          "iopub.status.busy": "2025-11-17T17:37:06.969691Z",
          "iopub.status.idle": "2025-11-17T17:37:08.359092Z",
          "shell.execute_reply": "2025-11-17T17:37:08.358479Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Special Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Extracting special project 'BeaverLake'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Special project 'BeaverLake' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Existing folder for project 'BeaverLake' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - Downloading special project from: https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299501780/299502090/1/1747692179014/BeaverLake-SWMM-Import-Solution.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:06 - ras_commander.RasExamples - INFO - This may take a few moments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - Downloaded special project zip file to C:\\GH\\ras-commander\\examples\\example_projects\\BeaverLake_temp.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - Successfully extracted special project 'BeaverLake' to C:\\GH\\ras-commander\\examples\\example_projects\\BeaverLake\n"
          ]
        }
      ],
      "source": [
        "extracted_paths = RasExamples.extract_project(\"BeaverLake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:08.361532Z",
          "iopub.status.busy": "2025-11-17T17:37:08.361271Z",
          "iopub.status.idle": "2025-11-17T17:37:29.155193Z",
          "shell.execute_reply": "2025-11-17T17:37:29.154530Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Special Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - Extracting special project 'NewOrleansMetro'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - Special project 'NewOrleansMetro' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - Existing folder for project 'NewOrleansMetro' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - Downloading special project from: https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299502039/299502111/1/1747692522764/NewOrleansMetroPipesExample.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:08 - ras_commander.RasExamples - INFO - This may take a few moments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:28 - ras_commander.RasExamples - INFO - Downloaded special project zip file to C:\\GH\\ras-commander\\examples\\example_projects\\NewOrleansMetro_temp.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:29 - ras_commander.RasExamples - INFO - Successfully extracted special project 'NewOrleansMetro' to C:\\GH\\ras-commander\\examples\\example_projects\\NewOrleansMetro\n"
          ]
        }
      ],
      "source": [
        "extracted_paths = RasExamples.extract_project(\"NewOrleansMetro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:29.157603Z",
          "iopub.status.busy": "2025-11-17T17:37:29.157426Z",
          "iopub.status.idle": "2025-11-17T17:37:30.557762Z",
          "shell.execute_reply": "2025-11-17T17:37:30.557186Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:29 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Special Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:29 - ras_commander.RasExamples - INFO - Extracting special project 'BeaverLake'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:29 - ras_commander.RasExamples - INFO - Downloading special project from: https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299501780/299502090/1/1747692179014/BeaverLake-SWMM-Import-Solution.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:29 - ras_commander.RasExamples - INFO - This may take a few moments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Downloaded special project zip file to C:\\GH\\ras-commander\\examples\\special_projects\\BeaverLake_temp.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Successfully extracted special project 'BeaverLake' to C:\\GH\\ras-commander\\examples\\special_projects\\BeaverLake\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special project extracted to: C:\\GH\\ras-commander\\examples\\special_projects\\BeaverLake\n"
          ]
        }
      ],
      "source": [
        "# Example 4: Special projects also work with custom output paths\n",
        "beaverlake_path = RasExamples.extract_project(\"BeaverLake\", output_path=\"special_projects\")\n",
        "print(f\"Special project extracted to: {beaverlake_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:30.559758Z",
          "iopub.status.busy": "2025-11-17T17:37:30.559596Z",
          "iopub.status.idle": "2025-11-17T17:37:30.619583Z",
          "shell.execute_reply": "2025-11-17T17:37:30.618869Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\unsteady_examples\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Extracting project 'Pumping Station'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Project 'Pumping Station' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Existing folder for project 'Pumping Station' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Successfully extracted project 'Pumping Station' to C:\\GH\\ras-commander\\examples\\unsteady_examples\\Pumping Station\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 2 projects:\n",
            "  - C:\\GH\\ras-commander\\examples\\unsteady_examples\\Balde Eagle Creek\n",
            "  - C:\\GH\\ras-commander\\examples\\unsteady_examples\\Pumping Station\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Extract multiple projects to a custom location\n",
        "projects = [\"Balde Eagle Creek\", \"Pumping Station\"]\n",
        "extracted_paths = RasExamples.extract_project(projects, output_path=\"unsteady_examples\")\n",
        "print(f\"Extracted {len(extracted_paths)} projects:\")\n",
        "for path in extracted_paths:\n",
        "    print(f\"  - {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:30.621749Z",
          "iopub.status.busy": "2025-11-17T17:37:30.621562Z",
          "iopub.status.idle": "2025-11-17T17:37:30.722815Z",
          "shell.execute_reply": "2025-11-17T17:37:30.722091Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Extracting project 'Dam Breaching'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Project 'Dam Breaching' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Existing folder for project 'Dam Breaching' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Successfully extracted project 'Dam Breaching' to C:\\HEC_RAS_Projects\\testing\\Dam Breaching\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to: C:\\HEC_RAS_Projects\\testing\\Dam Breaching\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Extract to an absolute path\n",
        "from pathlib import Path\n",
        "custom_path = Path(\"C:/HEC_RAS_Projects/testing\")\n",
        "dam_breach_path = RasExamples.extract_project(\"Dam Breaching\", output_path=custom_path)\n",
        "print(f\"Extracted to: {dam_breach_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:30.724773Z",
          "iopub.status.busy": "2025-11-17T17:37:30.724567Z",
          "iopub.status.idle": "2025-11-17T17:37:30.961765Z",
          "shell.execute_reply": "2025-11-17T17:37:30.961384Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:37:30 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to C:\\GH\\ras-commander\\examples\\my_projects\\Muncie\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to: C:\\GH\\ras-commander\\examples\\my_projects\\Muncie\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Extract to a relative path (creates subfolder in current directory)\n",
        "muncie_path = RasExamples.extract_project(\"Muncie\", output_path=\"my_projects\")\n",
        "print(f\"Extracted to: {muncie_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Projects to Custom Locations\n",
        "\n",
        "By default, projects are extracted to the `example_projects` subfolder. However, you can specify a custom output location using the `output_path` parameter. This is useful when organizing projects for different workflows or when you need to extract projects to a specific directory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\01_project_initialization.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:52.089029Z",
          "iopub.status.busy": "2025-11-17T17:37:52.088811Z",
          "iopub.status.idle": "2025-11-17T17:37:53.423339Z",
          "shell.execute_reply": "2025-11-17T17:37:53.422758Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:53.425824Z",
          "iopub.status.busy": "2025-11-17T17:37:53.425360Z",
          "iopub.status.idle": "2025-11-17T17:37:53.428349Z",
          "shell.execute_reply": "2025-11-17T17:37:53.427847Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS Commander: Core Concepts\n",
        "\n",
        "RAS Commander is a Python library that provides tools for automating HEC-RAS tasks. It's built with several key design principles:\n",
        "\n",
        "1. **Project-Centric Architecture**: Everything revolves around HEC-RAS projects\n",
        "2. **Two RAS Object Approaches**:\n",
        "   - **Global `ras` Object**: A singleton for simple scripts\n",
        "   - **Custom RAS Objects**: Multiple ras project instances for complex workflows\n",
        "3. **Comprehensive Project Representation**: Each RAS object includes DataFrames for plans, geometries, flows, and boundaries\n",
        "4. **Logging**: Built-in logging to track operations and debug issues\n",
        "5. **HDF Support**: Specialized functions for HDF file access (plan results, geometry, etc.)\n",
        "\n",
        "Let's explore these concepts in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading Example HEC-RAS Projects\n",
        "\n",
        "RAS Commander includes a utility to download and extract example HEC-RAS projects. These are useful for learning and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:53.430973Z",
          "iopub.status.busy": "2025-11-17T17:37:53.430686Z",
          "iopub.status.idle": "2025-11-17T17:37:55.089155Z",
          "shell.execute_reply": "2025-11-17T17:37:55.088504Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
        "print(extracted_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Paths for Extracted Example Projects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.091718Z",
          "iopub.status.busy": "2025-11-17T17:37:55.091523Z",
          "iopub.status.idle": "2025-11-17T17:37:55.095907Z",
          "shell.execute_reply": "2025-11-17T17:37:55.095460Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get the parent directory of the first extracted path as our examples directory\n",
        "examples_dir = extracted_paths[0].parent\n",
        "print(f\"Examples directory: {examples_dir}\")\n",
        "\n",
        "\n",
        "# Define paths to the extracted projects\n",
        "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
        "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
        "muncie_path = examples_dir / \"Muncie\"\n",
        "\n",
        "# Verify the paths exist\n",
        "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
        "    print(f\"Path {path} exists: {path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Function to Print RAS Object Data\n",
        "\n",
        "Let's create a utility function to help us explore the contents of RAS objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.097891Z",
          "iopub.status.busy": "2025-11-17T17:37:55.097719Z",
          "iopub.status.idle": "2025-11-17T17:37:55.101682Z",
          "shell.execute_reply": "2025-11-17T17:37:55.101226Z"
        }
      },
      "outputs": [],
      "source": [
        "def print_ras_object_data(ras_obj, project_name):\n",
        "    \"\"\"Prints comprehensive information about a RAS object\"\"\"\n",
        "    print(f\"\\n{project_name} Data:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Project Name: {ras_obj.get_project_name()}\")\n",
        "    print(f\"Project Folder: {ras_obj.project_folder}\")\n",
        "    print(f\"PRJ File: {ras_obj.prj_file}\")\n",
        "    print(f\"HEC-RAS Executable Path: {ras_obj.ras_exe_path}\")\n",
        "    \n",
        "    print(\"\\nPlan Files DataFrame (ras.plan_df):\")\n",
        "    with pd.option_context('display.max_columns', None):\n",
        "        display.display(ras_obj.plan_df)\n",
        "    \n",
        "    print(\"\\nSteady Flow Files DataFrame:\")\n",
        "    display.display(ras_obj.flow_df)\n",
        "    \n",
        "    print(\"\\nUnsteady Flow Files DataFrame (ras.unsteady_df):\")\n",
        "    display.display(ras_obj.unsteady_df)\n",
        "    \n",
        "    print(\"\\nGeometry Files DataFrame (ras.geom_df):\")\n",
        "    display.display(ras_obj.geom_df)\n",
        "    \n",
        "    print(\"\\nHDF Entries DataFrame (ras.get_hdf_entries()):\")\n",
        "    display.display(ras_obj.get_hdf_entries())\n",
        "    \n",
        "    print(\"\\nBoundary Conditions DataFrame (ras.boundaries_df):\")\n",
        "    display.display(ras_obj.boundaries_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 1: Using the Global `ras` Object\n",
        "\n",
        "The global `ras` object is a singleton instance that persists throughout your script. It's ideal for simple scripts working with a single project.\n",
        "\n",
        "Key characteristics:\n",
        "- It's available as `ras` immediately after import\n",
        "- It's initialized via `init_ras_project()` without saving the return value\n",
        "- It provides access to all project data through the global `ras` variable\n",
        "- It's simple to use but can be problematic in complex scenarios\n",
        "\n",
        "Let's initialize it with the Bald Eagle Creek project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.103990Z",
          "iopub.status.busy": "2025-11-17T17:37:55.103789Z",
          "iopub.status.idle": "2025-11-17T17:37:55.150534Z",
          "shell.execute_reply": "2025-11-17T17:37:55.149949Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the global ras object with Bald Eagle Creek project\n",
        "# Note: This updates the global 'ras' object visible throughout the script\n",
        "# Parameters:\n",
        "#   - project_folder: Path to the HEC-RAS project folder (required)\n",
        "#   - ras_version: HEC-RAS version (e.g. \"6.5\") or path to Ras.exe (required first time)\n",
        "\n",
        "init_ras_project(bald_eagle_path, \"6.5\")\n",
        "print(f\"The global 'ras' object is now initialized with the {ras.project_name} project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.152677Z",
          "iopub.status.busy": "2025-11-17T17:37:55.152486Z",
          "iopub.status.idle": "2025-11-17T17:37:55.172621Z",
          "shell.execute_reply": "2025-11-17T17:37:55.171953Z"
        }
      },
      "outputs": [],
      "source": [
        "# Find the .prj file in the Bald Eagle Creek folder\n",
        "prj_file = list(bald_eagle_path.glob(\"*.prj\"))[0]\n",
        "print(f\"Found .prj file: {prj_file}\")\n",
        "print(f\"File name: {prj_file.name}\\n\")\n",
        "\n",
        "# Initialize using the .prj file path directly (NEW FEATURE!)\n",
        "# This works exactly the same as passing the folder path\n",
        "init_ras_project(prj_file, \"6.5\")\n",
        "print(f\"Successfully initialized project using .prj file path!\")\n",
        "print(f\"Project name: {ras.project_name}\")\n",
        "print(f\"Project folder: {ras.project_folder}\")\n",
        "print(f\"PRJ file: {ras.prj_file}\")\n",
        "\n",
        "# Verify both methods produce identical results\n",
        "print(f\"\\n\u2713 Both folder path and .prj file path initialization methods produce the same result!\")\n",
        "print(f\"\u2713 The project folder is automatically extracted from the .prj file's parent directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Initialize Using .prj File Path\n",
        "\n",
        "**New Feature**: You can now initialize a project by providing the direct path to a `.prj` file instead of the project folder. This is especially useful when:\n",
        "- Working with file selection dialogs (which return file paths)\n",
        "- Using configuration files that store specific .prj file paths\n",
        "- Working with folders that contain multiple projects\n",
        "- Building command-line tools that accept file paths\n",
        "\n",
        "The function automatically:\n",
        "1. Validates that the file has a `.prj` extension\n",
        "2. Verifies the file contains \"**Proj Title=**\" to confirm it's a HEC-RAS project file (not a plan file)\n",
        "3. Extracts the parent folder and uses it as the project folder\n",
        "4. Optimizes initialization by passing the .prj file directly (avoids re-searching)\n",
        "\n",
        "Let's see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.174860Z",
          "iopub.status.busy": "2025-11-17T17:37:55.174677Z",
          "iopub.status.idle": "2025-11-17T17:37:55.230667Z",
          "shell.execute_reply": "2025-11-17T17:37:55.230040Z"
        }
      },
      "outputs": [],
      "source": [
        "# Explore the global ras object with our utility function\n",
        "print_ras_object_data(ras, \"Global RAS Object (Bald Eagle Creek)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the RAS Object Structure\n",
        "\n",
        "Each RAS object contains several important components:\n",
        "\n",
        "1. **Project Metadata**:\n",
        "   - `project_name`: Name of the HEC-RAS project\n",
        "   - `project_folder`: Directory containing project files\n",
        "   - `prj_file`: Path to the main .prj file\n",
        "   - `ras_exe_path`: Path to the HEC-RAS executable\n",
        "\n",
        "2. **Project DataFrames**:\n",
        "   - `plan_df`: Information about all plan files (.p*)\n",
        "   - `flow_df`: Information about all steady flow files (.f*)\n",
        "   - `unsteady_df`: Information about all unsteady flow files (.u*)\n",
        "   - `geom_df`: Information about all geometry files (.g*)\n",
        "   - `boundaries_df`: Information about all boundary conditions\n",
        "\n",
        "3. **Methods for Data Access**:\n",
        "   - `get_plan_entries()`: Get plan file information\n",
        "   - `get_flow_entries()`: Get flow file information\n",
        "   - `get_unsteady_entries()`: Get unsteady flow file information \n",
        "   - `get_geom_entries()`: Get geometry file information\n",
        "   - `get_hdf_entries()`: Get HDF file paths for result files\n",
        "   - `get_boundary_conditions()`: Get boundary condition details\n",
        "\n",
        "Let's see how to access specific information from these components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.232994Z",
          "iopub.status.busy": "2025-11-17T17:37:55.232815Z",
          "iopub.status.idle": "2025-11-17T17:37:55.237432Z",
          "shell.execute_reply": "2025-11-17T17:37:55.236861Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get the first plan's details\n",
        "if not ras.plan_df.empty:\n",
        "    first_plan = ras.plan_df.iloc[0]\n",
        "    print(f\"First plan number: {first_plan['plan_number']}\")\n",
        "    print(f\"Plan path: {first_plan['full_path']}\")\n",
        "    \n",
        "    # Get the geometry file for this plan\n",
        "    geom_id = first_plan.get('Geom File', '').replace('g', '')\n",
        "    if geom_id:\n",
        "        geom_info = ras.geom_df[ras.geom_df['geom_number'] == geom_id]\n",
        "        if not geom_info.empty:\n",
        "            print(f\"Geometry file: {geom_info.iloc[0]['full_path']}\")\n",
        "    \n",
        "    # Get the HDF results file for this plan (if exists)\n",
        "    if 'HDF_Results_Path' in first_plan and first_plan['HDF_Results_Path']:\n",
        "        print(f\"Results file: {first_plan['HDF_Results_Path']}\")\n",
        "else:\n",
        "    print(\"No plans found in the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Working with Boundary Conditions\n",
        "\n",
        "Boundary conditions define the inputs and outputs of your model. Let's see how to access boundary condition information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.239531Z",
          "iopub.status.busy": "2025-11-17T17:37:55.239357Z",
          "iopub.status.idle": "2025-11-17T17:37:55.249584Z",
          "shell.execute_reply": "2025-11-17T17:37:55.249056Z"
        }
      },
      "outputs": [],
      "source": [
        "# View the boundary conditions DataFrame\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 2: Using Custom RAS Objects\n",
        "\n",
        "For more complex scripts or when working with multiple projects, it's better to create and use separate RAS objects. This approach:\n",
        "\n",
        "- Creates independent RAS objects for each project\n",
        "- Avoids overwriting the global `ras` object\n",
        "- Provides clearer separation between projects\n",
        "- Allows working with multiple projects simultaneously\n",
        "- Requires saving the return value from `init_ras_project()`\n",
        "\n",
        "Let's initialize multiple projects with custom RAS objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.251874Z",
          "iopub.status.busy": "2025-11-17T17:37:55.251679Z",
          "iopub.status.idle": "2025-11-17T17:37:55.531033Z",
          "shell.execute_reply": "2025-11-17T17:37:55.530361Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize multiple project instances with custom RAS objects\n",
        "# Note: This also updates the global 'ras' object each time, but we'll use the custom instances\n",
        "# Parameters remain the same as before\n",
        "multi_2d_project = RasPrj()\n",
        "init_ras_project(multi_2d_path, \"6.5\", ras_object=multi_2d_project)\n",
        "print(f\"\\nMulti2D project initialized with its own RAS object\")\n",
        "\n",
        "muncie_project = RasPrj()\n",
        "init_ras_project(muncie_path, \"6.5\", ras_object=muncie_project)\n",
        "print(f\"\\nMuncie project initialized with its own RAS object\")\n",
        "\n",
        "# Note that the global 'ras' object now points to the Muncie project\n",
        "# The global 'ras' object gets overwritten every time a project is initialized ,\n",
        "print(f\"\\nGlobal 'ras' object now points to: {ras.project_name} since it was the last one initialized.  Avoid the global object when using multiple projects.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploring Multiple Projects\n",
        "\n",
        "Now we have three RAS objects:\n",
        "- `multi_2d_project`: Our custom object for the Multi2D project\n",
        "- `muncie_project`: Our custom object for the Muncie project\n",
        "- `ras`: The global object (which now points to Muncie)\n",
        "\n",
        "Let's examine the Multi2D project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.533405Z",
          "iopub.status.busy": "2025-11-17T17:37:55.533215Z",
          "iopub.status.idle": "2025-11-17T17:37:55.545021Z",
          "shell.execute_reply": "2025-11-17T17:37:55.544380Z"
        }
      },
      "outputs": [],
      "source": [
        "display.display(multi_2d_project.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.547113Z",
          "iopub.status.busy": "2025-11-17T17:37:55.546936Z",
          "iopub.status.idle": "2025-11-17T17:37:55.592643Z",
          "shell.execute_reply": "2025-11-17T17:37:55.591864Z"
        }
      },
      "outputs": [],
      "source": [
        "# Examine the Multi2D project\n",
        "print_ras_object_data(multi_2d_project, \"Multi2D Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.594929Z",
          "iopub.status.busy": "2025-11-17T17:37:55.594734Z",
          "iopub.status.idle": "2025-11-17T17:37:55.630064Z",
          "shell.execute_reply": "2025-11-17T17:37:55.629395Z"
        }
      },
      "outputs": [],
      "source": [
        "# Examine the Muncie project\n",
        "print_ras_object_data(muncie_project, \"Muncie Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Projects\n",
        "\n",
        "Let's compare some key metrics of the two projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:37:55.632467Z",
          "iopub.status.busy": "2025-11-17T17:37:55.632177Z",
          "iopub.status.idle": "2025-11-17T17:37:55.642660Z",
          "shell.execute_reply": "2025-11-17T17:37:55.642046Z"
        }
      },
      "outputs": [],
      "source": [
        "# Create a comparison table of the two projects\n",
        "comparison_data = {\n",
        "    'Project Name': [multi_2d_project.project_name, muncie_project.project_name],\n",
        "    'Number of Plans': [len(multi_2d_project.plan_df), len(muncie_project.plan_df)],\n",
        "    'Number of Geometries': [len(multi_2d_project.geom_df), len(muncie_project.geom_df)],\n",
        "    'Number of Flow Files': [len(multi_2d_project.flow_df), len(muncie_project.flow_df)],\n",
        "    'Number of Unsteady Files': [len(multi_2d_project.unsteady_df), len(muncie_project.unsteady_df)],\n",
        "    'Number of Boundary Conditions': [len(multi_2d_project.boundaries_df) if hasattr(multi_2d_project, 'boundaries_df') else 0, \n",
        "                                     len(muncie_project.boundaries_df) if hasattr(muncie_project, 'boundaries_df') else 0],\n",
        "    'HDF Results Available': [len(multi_2d_project.get_hdf_entries()) > 0, len(muncie_project.get_hdf_entries()) > 0]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "display.display(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS Commander: Best Practices\n",
        "\n",
        "After exploring both approaches, here are some best practices for using RAS Commander:\n",
        "\n",
        "1. **Choose Your Approach Based on Complexity**:\n",
        "   - **Simple Scripts** (one project): Use the global `ras` object\n",
        "   - **Complex Scripts** (multiple projects): Use custom RAS objects\n",
        "\n",
        "2. **Be Consistent**:\n",
        "   - Don't mix global and custom approaches in the same script\n",
        "   - Use descriptive names for custom RAS objects\n",
        "\n",
        "3. **Working with Project Files**:\n",
        "   - Access project files through the RAS object's DataFrames\n",
        "   - Use helper functions like `get_plan_path()` to resolve paths\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - Always check for empty DataFrames before accessing their contents\n",
        "   - Use the built-in logging to track operations\n",
        "\n",
        "5. **Performance Considerations**:\n",
        "   - For large projects, consider using the HDF classes directly\n",
        "   - Cache results of expensive operations when possible\n",
        "\n",
        "## Summary of Key Functions\n",
        "\n",
        "- `init_ras_project(project_folder, ras_version)`: Initialize a RAS project\n",
        "- `RasExamples().extract_project(project_name)`: Extract example projects\n",
        "- `RasPrj.get_project_name()`: Get the name of the project\n",
        "- `RasPrj.get_plan_entries()`: Get plan file information\n",
        "- `RasPrj.get_flow_entries()`: Get flow file information\n",
        "- `RasPrj.get_unsteady_entries()`: Get unsteady flow file information\n",
        "- `RasPrj.get_geom_entries()`: Get geometry file information\n",
        "- `RasPrj.get_hdf_entries()`: Get HDF result file information\n",
        "- `RasPrj.get_boundary_conditions()`: Get boundary condition details\n",
        "- `RasPlan.get_plan_path(plan_number)`: Get the path to a plan file\n",
        "- `RasPlan.get_geom_path(geom_number)`: Get the path to a geometry file\n",
        "- `RasPlan.get_flow_path(flow_number)`: Get the path to a flow file\n",
        "- `RasPlan.get_unsteady_path(unsteady_number)`: Get the path to an unsteady flow file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you understand the basics of project initialization in RAS Commander, you can explore more advanced topics:\n",
        "\n",
        "1. Working with HDF files for result analysis\n",
        "2. Modifying plan, geometry, and flow files\n",
        "3. Running HEC-RAS simulations\n",
        "4. Extracting and visualizing results\n",
        "5. Automating model calibration\n",
        "\n",
        "These topics are covered in other examples and notebooks in the RAS Commander documentation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\02_plan_and_geometry_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:12.543843Z",
          "iopub.status.busy": "2025-11-17T17:38:12.543647Z",
          "iopub.status.idle": "2025-11-17T17:38:13.834447Z",
          "shell.execute_reply": "2025-11-17T17:38:13.833893Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:12 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Plan and Geometry Operations\n",
        "\n",
        "This notebook demonstrates how to perform operations on HEC-RAS plan and geometry files using the RAS Commander library. We'll explore how to initialize projects, clone plans and geometries, configure parameters, execute plans, and analyze results.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
        "2. **Plan Operations**:\n",
        "   - Clone an existing plan to create a new one\n",
        "   - Configure simulation parameters and intervals\n",
        "   - Set run flags and update descriptions\n",
        "3. **Geometry Operations**:\n",
        "   - Clone a geometry file to create a modified version\n",
        "   - Set the geometry for a plan\n",
        "   - Clear geometry preprocessor files to ensure clean results\n",
        "4. **Flow Operations**:\n",
        "   - Clone unsteady flow files\n",
        "   - Configure flow parameters\n",
        "5. **Plan Computation**: Run the plan with specified settings\n",
        "6. **Results Verification**: Check HDF entries to confirm results were written"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.836723Z",
          "iopub.status.busy": "2025-11-17T17:38:13.836462Z",
          "iopub.status.idle": "2025-11-17T17:38:13.839185Z",
          "shell.execute_reply": "2025-11-17T17:38:13.838692Z"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.841102Z",
          "iopub.status.busy": "2025-11-17T17:38:13.840814Z",
          "iopub.status.idle": "2025-11-17T17:38:13.844322Z",
          "shell.execute_reply": "2025-11-17T17:38:13.843798Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "# Enable this cell for local development version of ras-commander\n",
        "import os\n",
        "import sys      \n",
        "from pathlib import Path\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.846011Z",
          "iopub.status.busy": "2025-11-17T17:38:13.845875Z",
          "iopub.status.idle": "2025-11-17T17:38:13.848522Z",
          "shell.execute_reply": "2025-11-17T17:38:13.848045Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import the required libraries for this notebook\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from datetime import datetime  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract an example HEC-RAS project. For this notebook, we'll use the \"Balde Eagle Creek\" project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.850431Z",
          "iopub.status.busy": "2025-11-17T17:38:13.850124Z",
          "iopub.status.idle": "2025-11-17T17:38:13.893104Z",
          "shell.execute_reply": "2025-11-17T17:38:13.892530Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Found zip file: C:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        }
      ],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(bald_eagle_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Initialization\n",
        "\n",
        "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the project folder path and HEC-RAS version as parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.895273Z",
          "iopub.status.busy": "2025-11-17T17:38:13.895101Z",
          "iopub.status.idle": "2025-11-17T17:38:13.938980Z",
          "shell.execute_reply": "2025-11-17T17:38:13.938465Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized HEC-RAS project: BaldEagle\n",
            "\n",
            "HEC-RAS Project Plan Data (plan_df):\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "0                 2MIN            1HOUR        1  ...     None      dss   \n",
              "1                 2MIN              NaN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method HDF_Results_Path Geom File  \\\n",
              "0                     2             None        01   \n",
              "1                     1             None        01   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...    Steady  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan and Geometry Operations in HEC-RAS\n",
        "\n",
        "Before diving into the operations, let's understand what plan and geometry files are in HEC-RAS:\n",
        "\n",
        "- **Plan Files** (`.p*`): Define the simulation parameters including the reference to geometry and flow files, as well as computational settings.\n",
        "- **Geometry Files** (`.g*`): Define the physical characteristics of the river/channel system including cross-sections, 2D areas, and structures.\n",
        "\n",
        "The `RasPlan` and `RasGeo` classes provide methods for working with these files, including:\n",
        "\n",
        "1. Creating new plans and geometries by cloning existing ones\n",
        "2. Modifying simulation parameters and settings\n",
        "3. Associating geometries with plans\n",
        "4. Managing preprocessor files\n",
        "5. Retrieving information from plans and geometries\n",
        "\n",
        "In the following sections, we'll explore these operations in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cloning Plans and Geometries\n",
        "\n",
        "Let's start by cloning a plan to create a new simulation scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.940960Z",
          "iopub.status.busy": "2025-11-17T17:38:13.940777Z",
          "iopub.status.idle": "2025-11-17T17:38:13.995513Z",
          "shell.execute_reply": "2025-11-17T17:38:13.995104Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasUtils - INFO - Project file updated with new Plan entry: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New plan created: 03\n",
            "\n",
            "Updated plan files:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>UNET D2 Cores</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>0.0</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>Combined Test Plan</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>0.0</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "2          03              02              01  Unsteady with Bridges and Dam   \n",
              "\n",
              "  Program Version    Short Identifier                  Simulation Date  \\\n",
              "0            5.00        UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN           SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "2            5.00  Combined Test Plan    18FEB1999,0000,24FEB1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... UNET D2 Cores PS Cores  \\\n",
              "0                 2MIN            1HOUR        1  ...           0.0     None   \n",
              "1                 2MIN              NaN        1  ...           NaN     None   \n",
              "2                 2MIN            1HOUR        1  ...           0.0     None   \n",
              "\n",
              "  DSS File Friction Slope Method HDF_Results_Path  Geom File  Geom Path  \\\n",
              "0      dss                     2             None         01       None   \n",
              "1      dss                     1             None         01       None   \n",
              "2      dss                     2             None         01       None   \n",
              "\n",
              "  Flow File Flow Path                                          full_path  \n",
              "0        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "1        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "2        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "\n",
              "[3 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n",
            "\n",
            "New plan details:\n",
            "Plan number: 03\n",
            "Description: No description\n",
            "Short Identifier: Combined Test Plan\n",
            "Geometry file: 01\n",
            "File path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        }
      ],
      "source": [
        "# Clone plan \"01\" to create a new plan\n",
        "new_plan_number = RasPlan.clone_plan(\"1\", new_shortid=\"Combined Test Plan\")\n",
        "print(f\"New plan created: {new_plan_number}\")\n",
        "\n",
        "# Display updated plan files\n",
        "print(\"\\nUpdated plan files:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Get the path to the new plan file\n",
        "plan_path = RasPlan.get_plan_path(new_plan_number)\n",
        "print(f\"\\nNew plan file path: {plan_path}\")\n",
        "\n",
        "# Let's examine the new plan's details\n",
        "new_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
        "print(f\"\\nNew plan details:\")\n",
        "print(f\"Plan number: {new_plan_number}\")\n",
        "print(f\"Description: {new_plan.get('description', 'No description')}\")\n",
        "print(f\"Short Identifier: {new_plan.get('Short Identifier', 'Not available')}\")\n",
        "print(f\"Geometry file: {new_plan.get('Geom File', 'None')}\")\n",
        "print(f\"File path: {new_plan['full_path']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:13.997296Z",
          "iopub.status.busy": "2025-11-17T17:38:13.997051Z",
          "iopub.status.idle": "2025-11-17T17:38:14.016769Z",
          "shell.execute_reply": "2025-11-17T17:38:14.016110Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Retrieved Plan Title: Unsteady with Bridges and Dam\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Retrieved Short Identifier: Combined Test Plan\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current plan title: Unsteady with Bridges and Dam\n",
            "Current plan shortid: Combined Test Plan\n"
          ]
        }
      ],
      "source": [
        "# Get the current plan title and shortid\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.019022Z",
          "iopub.status.busy": "2025-11-17T17:38:14.018728Z",
          "iopub.status.idle": "2025-11-17T17:38:14.046053Z",
          "shell.execute_reply": "2025-11-17T17:38:14.045645Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Updated Plan Title in plan file to: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - WARNING - Short Identifier too long (24 char max). Truncating: Combined Test Plan clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Updated Short Identifier in plan file to: Combined Test Plan clone\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Retrieved Plan Title: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Retrieved Short Identifier: Combined Test Plan clone\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated plan title: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated plan shortid: Combined Test Plan clone\n"
          ]
        }
      ],
      "source": [
        "# Update the title and shortid to append \" clonedplan\"\n",
        "new_title = f\"{current_title} clonedplan\"\n",
        "new_shortid = f\"{current_shortid} clonedplan\"\n",
        "\n",
        "RasPlan.set_plan_title(new_plan_number, new_title)\n",
        "RasPlan.set_shortid(new_plan_number, new_shortid)\n",
        "\n",
        "print(f\"\\nUpdated plan title: {RasPlan.get_plan_title(new_plan_number)}\")\n",
        "print(f\"Updated plan shortid: {RasPlan.get_shortid(new_plan_number)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.047993Z",
          "iopub.status.busy": "2025-11-17T17:38:14.047799Z",
          "iopub.status.idle": "2025-11-17T17:38:14.059437Z",
          "shell.execute_reply": "2025-11-17T17:38:14.058938Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Retrieved Plan Title: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Retrieved Short Identifier: Combined Test Plan clone\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current plan title: Unsteady with Bridges and Dam clonedplan\n",
            "Current plan shortid: Combined Test Plan clone\n"
          ]
        }
      ],
      "source": [
        "# Get the current plan title and shortid again to confirm the changes\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's clone a geometry file. This allows us to make modifications to a geometry without affecting the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.061401Z",
          "iopub.status.busy": "2025-11-17T17:38:14.061202Z",
          "iopub.status.idle": "2025-11-17T17:38:14.100076Z",
          "shell.execute_reply": "2025-11-17T17:38:14.099339Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g01 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g01.hdf to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02.hdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Project file updated with new Geom entry: 02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New geometry created: 02\n",
            "\n",
            "Updated geometry files:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>geom_file</th>\\n', '      <th>geom_number</th>\\n', '      <th>full_path</th>\\n', '      <th>hdf_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>g01</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>g02</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  geom_file geom_number                                          full_path  \\\n",
              "0       g01          01  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1       g02          02  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                            hdf_path  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Found geometry path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New geometry file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n",
            "\n",
            "New geometry details:\n",
            "Geometry number: 02\n",
            "Geometry file: Not available\n",
            "File path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n",
            "HDF path: None\n"
          ]
        }
      ],
      "source": [
        "# Clone geometry \"01\" to create a new geometry file\n",
        "new_geom_number = RasPlan.clone_geom(\"01\")\n",
        "print(f\"New geometry created: {new_geom_number}\")\n",
        "\n",
        "# Display updated geometry files\n",
        "print(\"\\nUpdated geometry files:\")\n",
        "display.display(ras.geom_df)\n",
        "\n",
        "# Get the path to the new geometry file\n",
        "geom_path = RasPlan.get_geom_path(new_geom_number)\n",
        "print(f\"\\nNew geometry file path: {geom_path}\")\n",
        "\n",
        "# Examine the new geometry's details\n",
        "new_geom = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number].squeeze()\n",
        "print(f\"\\nNew geometry details:\")\n",
        "print(f\"Geometry number: {new_geom_number}\")\n",
        "print(f\"Geometry file: {new_geom.get('geom_file', 'Not available')}\")\n",
        "print(f\"File path: {new_geom.get('full_path', 'Not available')}\")\n",
        "print(f\"HDF path: {new_geom.get('hdf_path', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also clone an unsteady flow file to complete our new simulation setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.102839Z",
          "iopub.status.busy": "2025-11-17T17:38:14.102340Z",
          "iopub.status.idle": "2025-11-17T17:38:14.141224Z",
          "shell.execute_reply": "2025-11-17T17:38:14.140700Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u02 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Project file updated with new Unsteady entry: 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New unsteady flow created: 01\n",
            "\n",
            "Updated unsteady flow files:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>full_path</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Flow Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Use Restart</th>\\n', '      <th>Precipitation Mode</th>\\n', '      <th>Wind Mode</th>\\n', '      <th>Met BC=Precipitation|Mode</th>\\n', '      <th>Met BC=Evapotranspiration|Mode</th>\\n', '      <th>Met BC=Precipitation|Expanded View</th>\\n', '      <th>Met BC=Precipitation|Constant Units</th>\\n', '      <th>Met BC=Precipitation|Gridded Source</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>None</td>\\n', '      <td>Flow Hydrograph 2</td>\\n', '      <td>6.30</td>\\n', '      <td>0</td>\\n', '      <td>Disable</td>\\n', '      <td>No Wind Forces</td>\\n', '      <td>None</td>\\n', '      <td>None</td>\\n', '      <td>0</td>\\n', '      <td>mm/hr</td>\\n', '      <td>DSS</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>None</td>\\n', '      <td>Flow Hydrograph 2</td>\\n', '      <td>6.30</td>\\n', '      <td>0</td>\\n', '      <td>Disable</td>\\n', '      <td>No Wind Forces</td>\\n', '      <td>None</td>\\n', '      <td>None</td>\\n', '      <td>0</td>\\n', '      <td>mm/hr</td>\\n', '      <td>DSS</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  unsteady_number                                          full_path  \\\n",
              "0              02  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1              01  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "  geometry_number         Flow Title Program Version Use Restart  \\\n",
              "0            None  Flow Hydrograph 2            6.30           0   \n",
              "1            None  Flow Hydrograph 2            6.30           0   \n",
              "\n",
              "  Precipitation Mode       Wind Mode Met BC=Precipitation|Mode  \\\n",
              "0            Disable  No Wind Forces                      None   \n",
              "1            Disable  No Wind Forces                      None   \n",
              "\n",
              "  Met BC=Evapotranspiration|Mode Met BC=Precipitation|Expanded View  \\\n",
              "0                           None                                  0   \n",
              "1                           None                                  0   \n",
              "\n",
              "  Met BC=Precipitation|Constant Units Met BC=Precipitation|Gridded Source  \n",
              "0                               mm/hr                                 DSS  \n",
              "1                               mm/hr                                 DSS  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New unsteady flow details:\n",
            "Unsteady number: 01\n",
            "File path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u01\n",
            "Flow Title: Flow Hydrograph 2\n"
          ]
        }
      ],
      "source": [
        "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
        "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
        "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
        "\n",
        "# Display updated unsteady flow files\n",
        "print(\"\\nUpdated unsteady flow files:\")\n",
        "display.display(ras.unsteady_df)\n",
        "\n",
        "# Examine the new unsteady flow's details\n",
        "new_unsteady = ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number].iloc[0]\n",
        "print(f\"\\nNew unsteady flow details:\")\n",
        "print(f\"Unsteady number: {new_unsteady_number}\")\n",
        "print(f\"File path: {new_unsteady['full_path']}\")\n",
        "print(f\"Flow Title: {new_unsteady.get('Flow Title', 'Not available')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Associating Files and Setting Parameters\n",
        "\n",
        "Now that we have cloned our plan, geometry, and unsteady flow files, we need to associate them with each other and set various parameters.\n",
        "\n",
        "### Setting Geometry for a Plan\n",
        "\n",
        "Let's associate our new geometry with our new plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.143203Z",
          "iopub.status.busy": "2025-11-17T17:38:14.143006Z",
          "iopub.status.idle": "2025-11-17T17:38:14.164685Z",
          "shell.execute_reply": "2025-11-17T17:38:14.164150Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Updated Geom File in plan file to g02 for plan 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Geometry for plan 03 set to 02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated geometry for plan 03 to geometry 02\n",
            "Plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n",
            "\n",
            "Verified that plan 03 now uses geometry file: g02\n"
          ]
        }
      ],
      "source": [
        "# Set the new geometry for the cloned plan\n",
        "updated_geom_df = RasPlan.set_geom(new_plan_number, new_geom_number)\n",
        "plan_path = RasPlan.get_plan_path(new_plan_number, ras_object=ras)\n",
        "print(f\"Updated geometry for plan {new_plan_number} to geometry {new_geom_number}\")\n",
        "print(f\"Plan file path: {plan_path}\")\n",
        "\n",
        "# Let's verify the change\n",
        "updated_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
        "print(f\"\\nVerified that plan {new_plan_number} now uses geometry file: {updated_plan.get('Geom File', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Unsteady Flow for a Plan\n",
        "\n",
        "Similarly, let's associate our new unsteady flow file with our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.166671Z",
          "iopub.status.busy": "2025-11-17T17:38:14.166508Z",
          "iopub.status.idle": "2025-11-17T17:38:14.188564Z",
          "shell.execute_reply": "2025-11-17T17:38:14.187981Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated unsteady flow for plan 03 to unsteady flow 01\n"
          ]
        }
      ],
      "source": [
        "# Set unsteady flow for the cloned plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Updated unsteady flow for plan {new_plan_number} to unsteady flow {new_unsteady_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clearing Geometry Preprocessor Files\n",
        "\n",
        "When working with geometry files, it's important to clear the preprocessor files to ensure clean results. These files (with `.c*` extension) contain computed hydraulic properties that should be recomputed when the geometry changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.190556Z",
          "iopub.status.busy": "2025-11-17T17:38:14.190360Z",
          "iopub.status.idle": "2025-11-17T17:38:14.196595Z",
          "shell.execute_reply": "2025-11-17T17:38:14.196211Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleared geometry preprocessor files for plan 03\n",
            "Preprocessor file exists after clearing: False\n"
          ]
        }
      ],
      "source": [
        "# Clear geometry preprocessor files for the cloned plan\n",
        "RasGeo.clear_geompre_files(plan_path)\n",
        "print(f\"Cleared geometry preprocessor files for plan {new_plan_number}\")\n",
        "\n",
        "# Check if preprocessor file exists after clearing\n",
        "geom_preprocessor_suffix = '.c' + ''.join(Path(plan_path).suffixes[1:])\n",
        "geom_preprocessor_file = Path(plan_path).with_suffix(geom_preprocessor_suffix)\n",
        "print(f\"Preprocessor file exists after clearing: {geom_preprocessor_file.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Computation Parameters\n",
        "\n",
        "Let's set the computation parameters for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.198310Z",
          "iopub.status.busy": "2025-11-17T17:38:14.198136Z",
          "iopub.status.idle": "2025-11-17T17:38:14.237891Z",
          "shell.execute_reply": "2025-11-17T17:38:14.237372Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated number of cores for plan 03 to 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verified that UNET D1 Cores is set to: 2\n",
            "Updated geometry preprocessor options for plan 03\n",
            "- Run HTab: -1 (Force recomputation of geometry tables)\n",
            "- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\n",
            "\n",
            "Verified setting values:\n",
            "- Run HTab: -1\n",
            "- UNET Use Existing IB Tables: -1\n"
          ]
        }
      ],
      "source": [
        "# Set the number of cores to use for the computation\n",
        "RasPlan.set_num_cores(new_plan_number, 2)\n",
        "print(f\"Updated number of cores for plan {new_plan_number} to 2\")\n",
        "\n",
        "# Verify by extracting the value from the plan file\n",
        "cores_value = RasPlan.get_plan_value(new_plan_number, \"UNET D1 Cores\")\n",
        "print(f\"\\nVerified that UNET D1 Cores is set to: {cores_value}\")\n",
        "\n",
        "# Set geometry preprocessor options\n",
        "RasPlan.set_geom_preprocessor(plan_path, run_htab=-1, use_ib_tables=-1)\n",
        "print(f\"Updated geometry preprocessor options for plan {new_plan_number}\")\n",
        "print(f\"- Run HTab: -1 (Force recomputation of geometry tables)\")\n",
        "print(f\"- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\")\n",
        "\n",
        "# Verify by extracting the values from the plan file\n",
        "run_htab_value = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
        "ib_tables_value = RasPlan.get_plan_value(new_plan_number, \"UNET Use Existing IB Tables\")\n",
        "print(f\"\\nVerified setting values:\")\n",
        "print(f\"- Run HTab: {run_htab_value}\")\n",
        "print(f\"- UNET Use Existing IB Tables: {ib_tables_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating Simulation Parameters\n",
        "\n",
        "Now, let's update various simulation parameters for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.240002Z",
          "iopub.status.busy": "2025-11-17T17:38:14.239813Z",
          "iopub.status.idle": "2025-11-17T17:38:14.248111Z",
          "shell.execute_reply": "2025-11-17T17:38:14.247535Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Updated simulation date in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated simulation date for plan 03:\n",
            "- Start Date: 2023-01-01 00:00:00\n",
            "- End Date: 2023-01-05 23:59:00\n"
          ]
        }
      ],
      "source": [
        "# 1. Update simulation date\n",
        "start_date = datetime(2023, 1, 1, 0, 0)  # January 1, 2023, 00:00\n",
        "end_date = datetime(2023, 1, 5, 23, 59)  # January 5, 2023, 23:59\n",
        "\n",
        "RasPlan.update_simulation_date(new_plan_number, start_date, end_date)\n",
        "print(f\"Updated simulation date for plan {new_plan_number}:\")\n",
        "print(f\"- Start Date: {start_date}\")\n",
        "print(f\"- End Date: {end_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.250325Z",
          "iopub.status.busy": "2025-11-17T17:38:14.250012Z",
          "iopub.status.idle": "2025-11-17T17:38:14.264698Z",
          "shell.execute_reply": "2025-11-17T17:38:14.264037Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified Simulation Date value: 01JAN2023,0000,05JAN2023,2359\n"
          ]
        }
      ],
      "source": [
        "# Verify the update\n",
        "sim_date = RasPlan.get_plan_value(new_plan_number, \"Simulation Date\")\n",
        "print(f\"Verified Simulation Date value: {sim_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.266757Z",
          "iopub.status.busy": "2025-11-17T17:38:14.266551Z",
          "iopub.status.idle": "2025-11-17T17:38:14.288751Z",
          "shell.execute_reply": "2025-11-17T17:38:14.288363Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Successfully updated intervals in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated plan intervals for plan 03:\n",
            "- Computation Interval: 1MIN\n",
            "- Output Interval: 15MIN\n",
            "- Mapping Interval: 30MIN\n",
            "Verified interval values:\n",
            "- Computation Interval: 1MIN\n",
            "- Mapping Interval: 30MIN\n"
          ]
        }
      ],
      "source": [
        "# 2. Update plan intervals\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_plan_number,\n",
        "    computation_interval=\"1MIN\",  # Computational time step\n",
        "    output_interval=\"15MIN\",      # How often results are written\n",
        "    mapping_interval=\"30MIN\"      # How often mapping outputs are created\n",
        ")\n",
        "print(f\"\\nUpdated plan intervals for plan {new_plan_number}:\")\n",
        "print(f\"- Computation Interval: 1MIN\")\n",
        "print(f\"- Output Interval: 15MIN\")\n",
        "print(f\"- Mapping Interval: 30MIN\")\n",
        "\n",
        "# Verify the updates\n",
        "comp_interval = RasPlan.get_plan_value(new_plan_number, \"Computation Interval\")\n",
        "mapping_interval = RasPlan.get_plan_value(new_plan_number, \"Mapping Interval\")\n",
        "print(f\"Verified interval values:\")\n",
        "print(f\"- Computation Interval: {comp_interval}\")\n",
        "print(f\"- Mapping Interval: {mapping_interval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.290574Z",
          "iopub.status.busy": "2025-11-17T17:38:14.290388Z",
          "iopub.status.idle": "2025-11-17T17:38:14.298445Z",
          "shell.execute_reply": "2025-11-17T17:38:14.297821Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - INFO - Successfully updated run flags in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated run flags for plan 03:\n",
            "- Geometry Preprocessor: True\n",
            "- Unsteady Flow Simulation: True\n",
            "- Post Processor: True\n",
            "- Floodplain Mapping: True\n"
          ]
        }
      ],
      "source": [
        "# 3. Update run flags\n",
        "RasPlan.update_run_flags(\n",
        "    new_plan_number,\n",
        "    geometry_preprocessor=True,   # Run the geometry preprocessor\n",
        "    unsteady_flow_simulation=True, # Run unsteady flow simulation\n",
        "    post_processor=True,          # Run post-processing\n",
        "    floodplain_mapping=True       # Generate floodplain mapping outputs\n",
        ")\n",
        "print(f\"\\nUpdated run flags for plan {new_plan_number}:\")\n",
        "print(f\"- Geometry Preprocessor: True\")\n",
        "print(f\"- Unsteady Flow Simulation: True\")\n",
        "print(f\"- Post Processor: True\")\n",
        "print(f\"- Floodplain Mapping: True\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.300563Z",
          "iopub.status.busy": "2025-11-17T17:38:14.300125Z",
          "iopub.status.idle": "2025-11-17T17:38:14.331529Z",
          "shell.execute_reply": "2025-11-17T17:38:14.331102Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - WARNING - Unknown key: Run UNet. Valid keys are: Friction Slope Method, Geom File, Run WQNET, Plan Title, Run HTab, UNET D1 Cores, Run UNET, Short Identifier, Description, Simulation Date, Flow File, PS Cores, Mapping Interval, Run RASMapper, Run Post Process, UNET 1D Methodology, DSS File, UNET D2 Solver Type, Computation Interval, Plan File, UNET D2 Cores, Program Version, Run Sediment, UNET Use Existing IB Tables, UNET D2 Name\n",
            " Add more keys and explanations in get_plan_value() as needed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasPlan - WARNING - No description found in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified run flag values:\n",
            "- Run HTab (Geometry Preprocessor): 1\n",
            "- Run UNet (Unsteady Flow): 1\n",
            "\n",
            "Updated description for plan 03\n",
            "Current plan description:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify the updates\n",
        "run_htab = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
        "run_unet = RasPlan.get_plan_value(new_plan_number, \"Run UNet\")\n",
        "print(f\"Verified run flag values:\")\n",
        "print(f\"- Run HTab (Geometry Preprocessor): {run_htab}\")\n",
        "print(f\"- Run UNet (Unsteady Flow): {run_unet}\")\n",
        "\n",
        "# 4. Update plan description\n",
        "new_description = \"Combined plan with modified geometry and unsteady flow\\nJanuary 2023 simulation\\n1-minute computation interval\\nGeometry and unsteady flow from cloned files\"\n",
        "RasPlan.update_plan_description(new_plan_number, new_description)\n",
        "print(f\"\\nUpdated description for plan {new_plan_number}\")\n",
        "\n",
        "# Read back the description\n",
        "current_description = RasPlan.read_plan_description(new_plan_number)\n",
        "print(f\"Current plan description:\\n{current_description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Plan\n",
        "\n",
        "Now that we have set up all the parameters, let's compute the plan using RasCmdr.compute_plan():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:38:14.333277Z",
          "iopub.status.busy": "2025-11-17T17:38:14.333079Z",
          "iopub.status.idle": "2025-11-17T17:39:47.743201Z",
          "shell.execute_reply": "2025-11-17T17:39:47.742574Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing plan 03...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:38:14 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 93.39 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan 03 computed successfully\n"
          ]
        }
      ],
      "source": [
        "# Compute the plan with our configured settings\n",
        "# Note: This may take several minutes depending on the complexity of the model\n",
        "print(f\"Computing plan {new_plan_number}...\")\n",
        "success = RasCmdr.compute_plan(new_plan_number, clear_geompre=True)\n",
        "\n",
        "if success:\n",
        "    print(f\"Plan {new_plan_number} computed successfully\")\n",
        "else:\n",
        "    print(f\"Failed to compute plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verifying Results\n",
        "\n",
        "After computation, we should check if results were written correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:39:47.745308Z",
          "iopub.status.busy": "2025-11-17T17:39:47.745064Z",
          "iopub.status.idle": "2025-11-17T17:39:47.762928Z",
          "shell.execute_reply": "2025-11-17T17:39:47.762521Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HDF entries for the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>description</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady with Bridges and Dam clonedplan</td>\\n', '      <td>5.00</td>\\n', '      <td>Combined Test Plan clone</td>\\n', '      <td>01JAN2023,0000,05JAN2023,2359</td>\\n', '      <td>1MIN</td>\\n', '      <td>30MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>Combined plan with modified geometry and unste...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number  \\\n",
              "2          03              01              02   \n",
              "\n",
              "                                 Plan Title Program Version  \\\n",
              "2  Unsteady with Bridges and Dam clonedplan            5.00   \n",
              "\n",
              "           Short Identifier                Simulation Date  \\\n",
              "2  Combined Test Plan clone  01JAN2023,0000,05JAN2023,2359   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "2                 1MIN            30MIN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method                                        description  \\\n",
              "2                     2  Combined plan with modified geometry and unste...   \n",
              "\n",
              "                                    HDF_Results_Path  Geom File  Geom Path  \\\n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...         02       None   \n",
              "\n",
              "  Flow File Flow Path                                          full_path  \n",
              "2        01      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "\n",
              "[1 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Plan 03 has a valid HDF results file:\n",
            "HDF Path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03.hdf\n",
            "\n",
            "All plan entries with their HDF paths:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number                                   HDF_Results_Path\n",
              "0          01                                               None\n",
              "1          02                                               None\n",
              "2          03  C:\\GH\\ras-commander\\examples\\example_projects\\..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Refresh the plan entries to ensure we have the latest data\n",
        "ras.plan_df = ras.get_plan_entries()\n",
        "hdf_entries = ras.get_hdf_entries()\n",
        "\n",
        "if not hdf_entries.empty:\n",
        "    print(\"HDF entries for the project:\")\n",
        "    display.display(hdf_entries)\n",
        "    \n",
        "    # Check if our new plan has an HDF file\n",
        "    new_plan_hdf = hdf_entries[hdf_entries['plan_number'] == new_plan_number]\n",
        "    if not new_plan_hdf.empty:\n",
        "        print(f\"\\nPlan {new_plan_number} has a valid HDF results file:\")\n",
        "        print(f\"HDF Path: {new_plan_hdf.iloc[0]['HDF_Results_Path']}\")\n",
        "    else:\n",
        "        print(f\"\\nNo HDF entry found for plan {new_plan_number}\")\n",
        "else:\n",
        "    print(\"No HDF entries found. This could mean the plan hasn't been computed successfully or the results haven't been written yet.\")\n",
        "\n",
        "# Display all plan entries to see their HDF paths\n",
        "print(\"\\nAll plan entries with their HDF paths:\")\n",
        "plan_hdf_info = ras.plan_df[['plan_number', 'HDF_Results_Path']]\n",
        "display.display(plan_hdf_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the plan was computed successfully, we can examine the runtime data and volume accounting from the HDF results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:39:47.765018Z",
          "iopub.status.busy": "2025-11-17T17:39:47.764791Z",
          "iopub.status.idle": "2025-11-17T17:39:47.794810Z",
          "shell.execute_reply": "2025-11-17T17:39:47.794312Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.HdfResultsPlan - INFO - Final validated HDF file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03.hdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.HdfResultsPlan - INFO - Extracting Plan Information from: BaldEagle.p03.hdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.HdfResultsPlan - INFO - Plan Name: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.HdfResultsPlan - INFO - Simulation Duration (hours): 119.98333333333333\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking computation runtime data...\n",
            "\n",
            "Simulation Runtime Statistics:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan Name</th>\\n', '      <th>File Name</th>\\n', '      <th>Simulation Start Time</th>\\n', '      <th>Simulation End Time</th>\\n', '      <th>Simulation Duration (s)</th>\\n', '      <th>Simulation Time (hr)</th>\\n', '      <th>Completing Geometry (hr)</th>\\n', '      <th>Preprocessing Geometry (hr)</th>\\n', '      <th>Completing Event Conditions (hr)</th>\\n', '      <th>Unsteady Flow Computations (hr)</th>\\n', '      <th>Complete Process (hr)</th>\\n', '      <th>Unsteady Flow Speed (hr/hr)</th>\\n', '      <th>Complete Process Speed (hr/hr)</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>Unsteady with Bridges and Dam clonedplan</td>\\n', '      <td>BaldEagle.p03.hdf</td>\\n', '      <td>2023-01-01</td>\\n', '      <td>2023-01-05 23:59:00</td>\\n', '      <td>431940.0</td>\\n', '      <td>119.983333</td>\\n', '      <td>N/A</td>\\n', '      <td>0.021285</td>\\n', '      <td>N/A</td>\\n', '      <td>0.001267</td>\\n', '      <td>0.025117</td>\\n', '      <td>94661.406969</td>\\n', '      <td>4776.934817</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "                                  Plan Name          File Name  \\\n",
              "0  Unsteady with Bridges and Dam clonedplan  BaldEagle.p03.hdf   \n",
              "\n",
              "  Simulation Start Time Simulation End Time  Simulation Duration (s)  \\\n",
              "0            2023-01-01 2023-01-05 23:59:00                 431940.0   \n",
              "\n",
              "   Simulation Time (hr) Completing Geometry (hr)  Preprocessing Geometry (hr)  \\\n",
              "0            119.983333                      N/A                     0.021285   \n",
              "\n",
              "  Completing Event Conditions (hr)  Unsteady Flow Computations (hr)  \\\n",
              "0                              N/A                         0.001267   \n",
              "\n",
              "   Complete Process (hr)  Unsteady Flow Speed (hr/hr)  \\\n",
              "0               0.025117                 94661.406969   \n",
              "\n",
              "   Complete Process Speed (hr/hr)  \n",
              "0                     4776.934817  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 12:39:47 - ras_commander.HdfResultsPlan - INFO - Final validated HDF file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simulation Duration: 431940.00 seconds\n",
            "Computation Time: 0.02512 hours\n",
            "Computation Speed: 4776.93 (simulation hours/compute hours)\n",
            "\n",
            "Checking volume accounting...\n"
          ]
        }
      ],
      "source": [
        "# Get computation runtime data from HDF\n",
        "print(\"Checking computation runtime data...\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(new_plan_number)\n",
        "\n",
        "if runtime_df is not None and not runtime_df.empty:\n",
        "    print(\"\\nSimulation Runtime Statistics:\")\n",
        "    display.display(runtime_df)\n",
        "    \n",
        "    # Extract key metrics\n",
        "    sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
        "    compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
        "    compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
        "    \n",
        "    print(f\"\\nSimulation Duration: {sim_duration:.2f} seconds\")\n",
        "    print(f\"Computation Time: {compute_time:.5f} hours\")\n",
        "    print(f\"Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
        "else:\n",
        "    print(\"No runtime data found. This may indicate the simulation didn't complete successfully.\")\n",
        "\n",
        "# Get volume accounting data\n",
        "print(\"\\nChecking volume accounting...\")\n",
        "volume_df = HdfResultsPlan.get_volume_accounting(new_plan_number)\n",
        "\n",
        "if volume_df is not None and not isinstance(volume_df, bool):\n",
        "    # Handle volume_df as a dictionary\n",
        "    if isinstance(volume_df, dict):\n",
        "        error_percent = volume_df.get('Error Percent')\n",
        "        if error_percent is not None:\n",
        "            print(f\"\\nFinal Volume Balance Error: {float(error_percent):.8f}%\")\n",
        "            \n",
        "        # Print other key statistics\n",
        "        print(\"\\nDetailed Volume Statistics:\")\n",
        "        print(f\"Volume Starting: {float(volume_df['Volume Starting']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Volume Ending: {float(volume_df['Volume Ending']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Total Inflow: {float(volume_df['Total Boundary Flux of Water In']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Total Outflow: {float(volume_df['Total Boundary Flux of Water Out']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "else:\n",
        "    print(\"No volume accounting data found. This may indicate the simulation didn't complete successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan and Geometry Operations\n",
        "\n",
        "In this notebook, we've covered a comprehensive range of operations on HEC-RAS plan and geometry files using the RAS Commander library:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
        "2. **Plan Operations**:\n",
        "   - Created a new plan by cloning an existing one\n",
        "   - Updated simulation parameters (dates, intervals, etc.)\n",
        "   - Set run flags for different components\n",
        "   - Updated the plan description\n",
        "3. **Geometry Operations**:\n",
        "   - Created a new geometry by cloning an existing one\n",
        "   - Associated the new geometry with our plan\n",
        "   - Cleared geometry preprocessor files\n",
        "4. **Unsteady Flow Operations**:\n",
        "   - Created a new unsteady flow file by cloning an existing one\n",
        "   - Associated it with our plan\n",
        "5. **Computation and Verification**:\n",
        "   - Computed our plan with the specified settings\n",
        "   - Verified the results using HDF entries\n",
        "   - Analyzed runtime statistics and volume accounting\n",
        "\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasPlan`: For plan operations (cloning, setting components, and modifying parameters)\n",
        "- `RasGeo`: For geometry operations (cloning, clearing preprocessor files)\n",
        "- `RasCmdr`: For executing HEC-RAS simulations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further enhance your HEC-RAS automation, consider exploring:\n",
        "\n",
        "1. **Parameter Sweeps**: Create and run multiple plans with varying parameters\n",
        "2. **Parallel Computations**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
        "3. **Advanced Results Analysis**: Use the HDF classes to extract and analyze specific model results\n",
        "4. **Spatial Visualization**: Create maps and plots of simulation results\n",
        "5. **Model Calibration**: Automate comparison between model results and observations\n",
        "\n",
        "The RAS Commander library provides a powerful framework for automating and streamlining your HEC-RAS workflows, enabling more efficient hydraulic modeling and analyses."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\03_unsteady_flow_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set to True to generate plots, False to skip plotting\n",
        "generate_plots = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Unsteady Flow Files in HEC-RAS\n",
        "\n",
        "Unsteady flow files (`.u*` files) in HEC-RAS define the time-varying boundary conditions that drive dynamic simulations. These include:\n",
        "\n",
        "- **Flow Hydrographs**: Time-series of flow values at model boundaries\n",
        "- **Stage Hydrographs**: Time-series of water surface elevations\n",
        "- **Lateral Inflows**: Distributed inflows along a reach\n",
        "- **Gate Operations**: Time-series of gate settings\n",
        "- **Meteorological Data**: Rainfall, evaporation, and other meteorological inputs\n",
        "\n",
        "The `RasUnsteady` class in RAS Commander provides methods for working with these files, including extracting boundaries, reading tables, and modifying parameters.\n",
        "\n",
        "Let's set up our working directory and define paths to example projects:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract an example HEC-RAS project with unsteady flow files. For this notebook, we'll use the \"Balde Eagle Creek\" project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the following parameters:\n",
        "\n",
        "- `ras_project_folder`: Path to the HEC-RAS project folder (required)\n",
        "- `ras_version`: HEC-RAS version (e.g., \"6.6\") or path to Ras.exe (required first time)\n",
        "\n",
        "This function initializes the global `ras` object that we'll use for the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "# This function returns a RAS object, but also updates the global 'ras' object\n",
        "# Parameters:\n",
        "#   - ras_project_folder: Path to the HEC-RAS project folder\n",
        "#   - ras_version: HEC-RAS version or path to Ras.exe\n",
        "\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the unsteady flow files in the project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Geometry Data (geom_df):\")\n",
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Unsteady Flow Data (unsteady_df):\")\n",
        "\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Boundary Data (boundaries_df):\")\n",
        "print(\"Columns:\", list(ras.boundaries_df.columns))\n",
        "ras.boundaries_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasUnsteady Class\n",
        "\n",
        "The `RasUnsteady` class provides functionality for working with HEC-RAS unsteady flow files (`.u*` files). Key operations include:\n",
        "\n",
        "1. **Extracting Boundary Conditions**: Read and parse boundary conditions from unsteady flow files\n",
        "2. **Modifying Flow Titles**: Update descriptive titles for unsteady flow scenarios\n",
        "3. **Managing Restart Settings**: Configure restart file options for continuing simulations\n",
        "4. **Working with Tables**: Extract, modify, and update flow tables\n",
        "\n",
        "Most methods in this class are static and work with the global `ras` object by default, though you can also pass in a custom RAS object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract Boundary Conditions and Tables\n",
        "\n",
        "The `extract_boundary_and_tables()` method from the `RasUnsteady` class allows us to extract boundary conditions and their associated tables from an unsteady flow file.\n",
        "\n",
        "Parameters for `RasUnsteady.extract_boundary_and_tables()`:\n",
        "- `unsteady_file` (str): Path to the unsteady flow file\n",
        "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
        "\n",
        "Returns:\n",
        "- `pd.DataFrame`: DataFrame containing boundary conditions and their associated tables\n",
        "\n",
        "Let's see how this works with our example project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file \"02\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"02\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")\n",
        "\n",
        "# Extract boundary conditions and tables\n",
        "boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\n",
        "print(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Print Boundaries and Tables\n",
        "\n",
        "The `print_boundaries_and_tables()` method provides a formatted display of the boundary conditions and their associated tables. This method doesn't return anything; it just prints the information in a readable format.\n",
        "\n",
        "Parameters for `RasUnsteady.print_boundaries_and_tables()`:\n",
        "- `boundaries_df` (pd.DataFrame): DataFrame containing boundary conditions from `extract_boundary_and_tables()`\n",
        "\n",
        "Let's use this method to get a better understanding of our boundary conditions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the boundaries and tables in a formatted way\n",
        "print(\"Detailed boundary conditions and tables:\")\n",
        "RasUnsteady.print_boundaries_and_tables(boundaries_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Boundary Condition Types\n",
        "\n",
        "The output above shows the different types of boundary conditions in our unsteady flow file. Let's understand what each type means:\n",
        "\n",
        "1. **Flow Hydrograph**: A time series of flow values (typically in cfs or cms) entering the model at a specific location. These are used at upstream boundaries or internal points where flow enters the system.\n",
        "\n",
        "2. **Stage Hydrograph**: A time series of water surface elevations (typically in ft or m) that define the downstream boundary condition.\n",
        "\n",
        "3. **Gate Openings**: Time series of gate settings (typically height in ft or m) for hydraulic structures such as spillways, sluice gates, or other control structures.\n",
        "\n",
        "4. **Lateral Inflow Hydrograph**: Flow entering the system along a reach, not at a specific point. This can represent tributary inflows, overland flow, or other distributed inputs.\n",
        "\n",
        "5. **Normal Depth**: A boundary condition where the water surface slope is assumed to equal the bed slope. This is represented by a friction slope value.\n",
        "\n",
        "Let's look at a specific boundary condition in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine the first boundary condition in more detail\n",
        "if not boundaries_df.empty:\n",
        "    first_boundary = boundaries_df.iloc[0]\n",
        "    print(f\"Detailed look at boundary condition {1}:\")\n",
        "    \n",
        "    # Print boundary location components\n",
        "    print(f\"\\nBoundary Location:\")\n",
        "    print(f\"  River Name: {first_boundary.get('River Name', 'N/A')}\")\n",
        "    print(f\"  Reach Name: {first_boundary.get('Reach Name', 'N/A')}\")\n",
        "    print(f\"  River Station: {first_boundary.get('River Station', 'N/A')}\")\n",
        "    print(f\"  Storage Area Name: {first_boundary.get('Storage Area Name', 'N/A')}\")\n",
        "    \n",
        "    # Print boundary condition type and other properties\n",
        "    print(f\"\\nBoundary Properties:\")\n",
        "    print(f\"  Boundary Type: {first_boundary.get('bc_type', 'N/A')}\")\n",
        "    print(f\"  DSS File: {first_boundary.get('DSS File', 'N/A')}\")\n",
        "    print(f\"  Use DSS: {first_boundary.get('Use DSS', 'N/A')}\")\n",
        "    \n",
        "    # Print table statistics if available\n",
        "    if 'Tables' in first_boundary and isinstance(first_boundary['Tables'], dict):\n",
        "        print(f\"\\nTable Information:\")\n",
        "        for table_name, table_df in first_boundary['Tables'].items():\n",
        "            print(f\"  {table_name}: {len(table_df)} values\")\n",
        "            if not table_df.empty:\n",
        "                print(f\"    Min Value: {table_df['Value'].min()}\")\n",
        "                print(f\"    Max Value: {table_df['Value'].max()}\")\n",
        "                print(f\"    First 5 Values: {table_df['Value'].head(5).tolist()}\")\n",
        "else:\n",
        "    print(\"No boundary conditions found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Update Flow Title\n",
        "\n",
        "The flow title in an unsteady flow file provides a description of the simulation scenario. The `update_flow_title()` method allows us to modify this title.\n",
        "\n",
        "Parameters for `RasUnsteady.update_flow_title()`:\n",
        "- `unsteady_file` (str): Full path to the unsteady flow file\n",
        "- `new_title` (str): New flow title (max 24 characters)\n",
        "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
        "\n",
        "Let's clone an unsteady flow file and update its title:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
        "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
        "print(f\"New unsteady flow created: {new_unsteady_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_unsteady_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to the new unsteady flow file\n",
        "new_unsteady_file = RasPlan.get_unsteady_path(new_unsteady_number)\n",
        "print(f\"New unsteady flow file path: {new_unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_unsteady_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current flow title\n",
        "current_title = None\n",
        "for _, row in ras.unsteady_df.iterrows():\n",
        "    if row['unsteady_number'] == new_unsteady_number and 'Flow Title' in row:\n",
        "        current_title = row['Flow Title']\n",
        "        break\n",
        "print(f\"Current flow title: {current_title}\")\n",
        "\n",
        "# Update the flow title\n",
        "new_title = \"Modified Flow Scenario\"\n",
        "RasUnsteady.update_flow_title(new_unsteady_file, new_title)\n",
        "print(f\"Updated flow title to: {new_title}\")\n",
        "\n",
        "# Refresh unsteady flow information to see the change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review unsteady flow information to see the change\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Working with Flow Tables\n",
        "\n",
        "Flow tables in unsteady flow files contain the time-series data for boundary conditions. Let's explore how to extract and work with these tables using some of the advanced methods from the `RasUnsteady` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific tables from the unsteady flow file\n",
        "all_tables = RasUnsteady.extract_tables(new_unsteady_file)\n",
        "print(f\"Extracted {len(all_tables)} tables from the unsteady flow file.\")\n",
        "\n",
        "# Let's look at the available table names\n",
        "print(\"\\nAvailable tables:\")\n",
        "for table_name in all_tables.keys():\n",
        "    print(f\"  {table_name}\")\n",
        "\n",
        "# Select the first table for detailed analysis\n",
        "if all_tables and len(all_tables) > 0:\n",
        "    first_table_name = list(all_tables.keys())[0]\n",
        "    first_table = all_tables[first_table_name]\n",
        "    \n",
        "    print(f\"\\nDetailed look at table '{first_table_name}':\")\n",
        "    print(f\"  Number of values: {len(first_table)}\")\n",
        "    print(f\"  Min value: {first_table['Value'].min()}\")\n",
        "    print(f\"  Max value: {first_table['Value'].max()}\")\n",
        "    print(f\"  Mean value: {first_table['Value'].mean():.2f}\")\n",
        "    print(f\"  First 10 values: {first_table['Value'].head(10).tolist()}\")\n",
        "    \n",
        "    # Create a visualization of the table values\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(first_table['Value'].values)\n",
        "        plt.title(f\"{first_table_name} Values\")\n",
        "        plt.xlabel('Time Step')\n",
        "        plt.ylabel('Value')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create visualization: {e}\")\n",
        "else:\n",
        "    print(\"No tables found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Modifying Flow Tables\n",
        "\n",
        "Now let's demonstrate how to modify a flow table and write it back to the unsteady flow file. For this example, we'll scale all the values in a table by a factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling existing values down by a 0.75 scale factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, identify tables in the unsteady flow file\n",
        "tables = RasUnsteady.identify_tables(open(new_unsteady_file, 'r').readlines())\n",
        "print(f\"Identified {len(tables)} tables in the unsteady flow file.\")\n",
        "\n",
        "# Let's look at the first flow hydrograph table\n",
        "flow_hydrograph_tables = [t for t in tables if t[0] == 'Flow Hydrograph=']\n",
        "if flow_hydrograph_tables:\n",
        "    table_name, start_line, end_line = flow_hydrograph_tables[0]\n",
        "    print(f\"\\nSelected table: {table_name}\")\n",
        "    print(f\"  Start line: {start_line}\")\n",
        "    print(f\"  End line: {end_line}\")\n",
        "    \n",
        "    # Parse the table\n",
        "    lines = open(new_unsteady_file, 'r').readlines()\n",
        "    table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
        "    print(f\"\\nOriginal table statistics:\")\n",
        "    print(f\"  Number of values: {len(table_df)}\")\n",
        "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
        "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
        "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
        "    \n",
        "    # Modify the table - let's scale all values by 75%\n",
        "    scale_factor = 0.75\n",
        "    table_df['Value'] = table_df['Value'] * scale_factor\n",
        "    print(f\"\\nModified table statistics (scaled by {scale_factor}):\")\n",
        "    print(f\"  Number of values: {len(table_df)}\")\n",
        "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
        "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
        "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
        "    \n",
        "    # Write the modified table back to the file\n",
        "    RasUnsteady.write_table_to_file(new_unsteady_file, table_name, table_df, start_line)\n",
        "    print(f\"\\nUpdated table written back to the unsteady flow file.\")\n",
        "    \n",
        "    # Re-read the table to verify changes\n",
        "    lines = open(new_unsteady_file, 'r').readlines()\n",
        "    updated_table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
        "    print(f\"\\nVerified updated table statistics:\")\n",
        "    print(f\"  Number of values: {len(updated_table_df)}\")\n",
        "    print(f\"  Min value: {updated_table_df['Value'].min()}\")\n",
        "    print(f\"  Max value: {updated_table_df['Value'].max()}\")\n",
        "    print(f\"  First 5 values: {updated_table_df['Value'].head(5).tolist()}\")\n",
        "else:\n",
        "    print(\"No flow hydrograph tables found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific tables from the unsteady flow file\n",
        "all_tables = RasUnsteady.extract_tables(new_unsteady_file)\n",
        "\n",
        "# Get the updated flow hydrograph table\n",
        "flow_hydrograph_tables = [t for t in all_tables.keys() if 'Flow Hydrograph=' in t]\n",
        "if flow_hydrograph_tables:\n",
        "    table_name = flow_hydrograph_tables[0]\n",
        "    table_df = all_tables[table_name]\n",
        "    \n",
        "    # Create visualization of the updated flow values\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(table_df['Value'].values, 'b-', label='Updated Flow')\n",
        "    plt.title('Updated Flow Hydrograph')\n",
        "    plt.xlabel('Time Step') \n",
        "    plt.ylabel('Flow (cfs)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nUpdated flow hydrograph statistics:\")\n",
        "    print(f\"  Number of values: {len(table_df)}\")\n",
        "    print(f\"  Min flow: {table_df['Value'].min():.1f} cfs\")\n",
        "    print(f\"  Max flow: {table_df['Value'].max():.1f} cfs\")\n",
        "    print(f\"  Mean flow: {table_df['Value'].mean():.1f} cfs\")\n",
        "else:\n",
        "    print(\"No flow hydrograph tables found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute Plan 01 to generate model results\n",
        "\n",
        "RasCmdr.compute_plan(\"01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries as xarray dataset\n",
        "xsec_results_xr_plan1 = HdfResultsXsec.get_xsec_timeseries(\"01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr_plan1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print time series for specific cross section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
        "\n",
        "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
        "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
        "    print(f\"\\n{var}:\")\n",
        "    print(f\"Plan 1:\")\n",
        "    print(xsec_results_xr_plan1[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "\n",
        "\n",
        "# Create time series plots\n",
        "if generate_plots:\n",
        "\n",
        "    # Create a figure for each variable\n",
        "    variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
        "\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        \n",
        "        # Convert time values to datetime if needed\n",
        "        time_values1 = pd.to_datetime(xsec_results_xr_plan1.time.values)\n",
        "        values1 = xsec_results_xr_plan1[var].sel(cross_section=target_xs).values\n",
        "\n",
        "        \n",
        "        # Plot both plans\n",
        "        plt.plot(time_values1, values1, '-', linewidth=2, label='Plan 1')\n",
        "        \n",
        "        plt.title(f'{var} at {target_xs}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(var.replace('_', ' '))\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Force display\n",
        "        plt.draw()\n",
        "        plt.pause(0.1)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Applying the Updated Unsteady Flow to a New Plan\n",
        "\n",
        "Now that we've modified an unsteady flow file, let's create a plan that uses it, and compute the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone an existing plan\n",
        "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Modified Flow Test\")\n",
        "print(f\"New plan created: {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_plan_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current plan title and shortid\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the title and shortid to append \" clonedplan\"\n",
        "new_title = f\"{current_title} 0.75 Flow Scale Factor\"\n",
        "new_shortid = f\"{current_shortid} 0.75 FSF\"\n",
        "\n",
        "RasPlan.set_plan_title(new_plan_number, new_title)\n",
        "RasPlan.set_shortid(new_plan_number, new_shortid)\n",
        "\n",
        "print(f\"\\nUpdated plan title: {RasPlan.get_plan_title(new_plan_number)}\")\n",
        "print(f\"Updated plan shortid: {RasPlan.get_shortid(new_plan_number)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print new_unsteady_number again as a reminder of it's current value\n",
        "new_unsteady_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the modified unsteady flow for the new plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Set unsteady flow {new_unsteady_number} for plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the modified unsteady flow for the new plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Set unsteady flow {new_unsteady_number} for plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to the new plan file\n",
        "new_plan_path = RasPlan.get_plan_path(new_plan_number)\n",
        "\n",
        "# Print contents of new plan file to confirm changes\n",
        "# Read and display the contents of the plan file\n",
        "with open(new_plan_path, 'r') as f:\n",
        "    plan_contents = f.read()\n",
        "print(f\"Contents of plan file {new_plan_number}:\")\n",
        "print(plan_contents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the plan description\n",
        "new_description = \"Test plan using modified unsteady flow\\nFlow scaled to 75% of original\\nWith restart file enabled\"\n",
        "RasPlan.update_plan_description(new_plan_number, new_description)\n",
        "print(f\"Updated plan description for plan {new_plan_number}\")\n",
        "\n",
        "# Set computation options\n",
        "RasPlan.set_num_cores(new_plan_number, 2)\n",
        "\n",
        "# Consider any other changes you want to make at this step, such as computation intervals etc: \n",
        "# RasPlan.update_plan_intervals(\n",
        "#    new_plan_number,\n",
        "#    computation_interval=\"1MIN\",\n",
        "#    output_interval=\"15MIN\",\n",
        "#    mapping_interval=\"1HOUR\"\n",
        "#)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the plan\n",
        "print(f\"\\nComputing plan {new_plan_number} with modified unsteady flow...\")\n",
        "success = RasCmdr.compute_plan(new_plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if success:\n",
        "    print(f\"Plan {new_plan_number} computed successfully\")\n",
        "    \n",
        "    # Check the results path\n",
        "    results_path = RasPlan.get_results_path(new_plan_number)\n",
        "    if results_path:\n",
        "        print(f\"Results available at: {results_path}\")\n",
        "        \n",
        "        # If it exists, get its size\n",
        "        results_file = Path(results_path)\n",
        "        if results_file.exists():\n",
        "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
        "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"No results found.\")\n",
        "else:\n",
        "    print(f\"Failed to compute plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show updated plan_df dataframe, which should show the HDF results files\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get results for Plan 03 and Compare with Plan 01's results for the specified Cross Section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries as xarray dataset\n",
        "xsec_results_xr_plan2 = HdfResultsXsec.get_xsec_timeseries(\"03\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr_plan2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print time series for specific cross section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
        "\n",
        "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
        "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
        "    print(f\"\\n{var}:\")\n",
        "    print(f\"Plan 1:\")\n",
        "    print(xsec_results_xr_plan1[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "    print(f\"Plan 2:\")\n",
        "    print(xsec_results_xr_plan2[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "\n",
        "# Create time series plots\n",
        "if generate_plots:\n",
        "\n",
        "    # Create a figure for each variable\n",
        "    variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
        "\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        \n",
        "        # Convert time values to datetime if needed\n",
        "        time_values1 = pd.to_datetime(xsec_results_xr_plan1.time.values)\n",
        "        time_values2 = pd.to_datetime(xsec_results_xr_plan2.time.values)\n",
        "        values1 = xsec_results_xr_plan1[var].sel(cross_section=target_xs).values\n",
        "        values2 = xsec_results_xr_plan2[var].sel(cross_section=target_xs).values\n",
        "        \n",
        "        # Get plan titles from plan_df\n",
        "        plan1_title = ras.plan_df.loc[ras.plan_df['plan_number'] == '01', 'Plan Title'].iloc[0]\n",
        "        plan2_title = ras.plan_df.loc[ras.plan_df['plan_number'] == '03', 'Plan Title'].iloc[0]\n",
        "        \n",
        "        # Plot both plans with titles\n",
        "        plt.plot(time_values1, values1, '-', linewidth=2, label=f'{plan1_title} (Plan 01)')\n",
        "        plt.plot(time_values2, values2, '--', linewidth=2, label=f'{plan2_title} (Plan 03)')\n",
        "        \n",
        "        plt.title(f'{var} at {target_xs}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(var.replace('_', ' '))\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Force display\n",
        "        plt.draw()\n",
        "        plt.pause(0.1)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Unsteady Flow Operations\n",
        "\n",
        "In this notebook, we've covered the following unsteady flow operations using RAS Commander:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
        "2. **Boundary Extraction**: We extracted boundary conditions and tables from unsteady flow files\n",
        "3. **Boundary Analysis**: We inspected and understood boundary condition structures\n",
        "4. **Flow Title Updates**: We modified the title of an unsteady flow file\n",
        "5. **Restart Settings**: We configured restart file settings for continuing simulations\n",
        "6. **Table Extraction**: We extracted flow tables for analysis\n",
        "7. **Table Modification**: We modified a flow table and wrote it back to the file\n",
        "8. **Application**: We created a plan using our modified unsteady flow and computed results\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasUnsteady.extract_boundary_and_tables()`: Extract boundary conditions and tables\n",
        "- `RasUnsteady.print_boundaries_and_tables()`: Display formatted boundary information\n",
        "- `RasUnsteady.update_flow_title()`: Modify the flow title\n",
        "- `RasUnsteady.update_restart_settings()`: Configure restart options\n",
        "- `RasUnsteady.extract_tables()`: Extract tables from unsteady flow files\n",
        "- `RasUnsteady.identify_tables()`: Identify table locations in file\n",
        "- `RasUnsteady.parse_fixed_width_table()`: Parse fixed-width tables\n",
        "- `RasUnsteady.write_table_to_file()`: Write modified tables back to file\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further explore unsteady flow operations with RAS Commander, consider:\n",
        "\n",
        "1. **Advanced Flow Modifications**: Create scripts that systematically modify flow hydrographs\n",
        "2. **Sensitivity Analysis**: Create variations of unsteady flows to assess model sensitivity\n",
        "3. **Batch Processing**: Process multiple unsteady flow files for scenario analysis\n",
        "4. **Custom Boundary Conditions**: Create unsteady flows from external data sources\n",
        "5. **Results Analysis**: Compare results from different unsteady flow scenarios\n",
        "\n",
        "These advanced topics can be explored by building on the foundation established in this notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\04_multiple_project_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:39.070533Z",
          "iopub.status.busy": "2025-11-17T18:31:39.070278Z",
          "iopub.status.idle": "2025-11-17T18:31:40.525003Z",
          "shell.execute_reply": "2025-11-17T18:31:40.524432Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:39 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:40.527284Z",
          "iopub.status.busy": "2025-11-17T18:31:40.526994Z",
          "iopub.status.idle": "2025-11-17T18:31:40.530698Z",
          "shell.execute_reply": "2025-11-17T18:31:40.530173Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:40.532837Z",
          "iopub.status.busy": "2025-11-17T18:31:40.532651Z",
          "iopub.status.idle": "2025-11-17T18:31:42.422954Z",
          "shell.execute_reply": "2025-11-17T18:31:42.422314Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Found zip file: C:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:40 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Project 'Muncie' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Existing folder for project 'Muncie' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WindowsPath('C:/GH/ras-commander/examples/example_projects/Balde Eagle Creek'), WindowsPath('C:/GH/ras-commander/examples/example_projects/BaldEagleCrkMulti2D'), WindowsPath('C:/GH/ras-commander/examples/example_projects/Muncie')]\n",
            "Examples directory: C:\\GH\\ras-commander\\examples\\example_projects\n",
            "Path C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek exists: True\n",
            "Path C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D exists: True\n",
            "Path C:\\GH\\ras-commander\\examples\\example_projects\\Muncie exists: True\n"
          ]
        }
      ],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
        "print(extracted_paths)\n",
        "\n",
        "# Get the parent directory of the first extracted path as our examples directory\n",
        "examples_dir = extracted_paths[0].parent\n",
        "print(f\"Examples directory: {examples_dir}\")\n",
        "\n",
        "\n",
        "# Define paths to the extracted projects\n",
        "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
        "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
        "muncie_path = examples_dir / \"Muncie\"\n",
        "\n",
        "# Verify the paths exist\n",
        "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
        "    print(f\"Path {path} exists: {path.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:42.425057Z",
          "iopub.status.busy": "2025-11-17T18:31:42.424802Z",
          "iopub.status.idle": "2025-11-17T18:31:42.429421Z",
          "shell.execute_reply": "2025-11-17T18:31:42.428823Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Resources:\n",
            "- 8 physical CPU cores (8 logical cores)\n",
            "- 13.4 GB available memory\n",
            "For multiple HEC-RAS projects, a good rule of thumb is:\n",
            "- Assign 2-4 cores per project\n",
            "- Allocate at least 2-4 GB of RAM per project\n",
            "Based on your system, you could reasonably run 4 projects simultaneously.\n"
          ]
        }
      ],
      "source": [
        "# Define computation output paths\n",
        "bald_eagle_compute_folder = examples_dir / \"compute_bald_eagle\"\n",
        "muncie_compute_folder = examples_dir / \"compute_muncie\"\n",
        "\n",
        "# Check system resources\n",
        "cpu_count = psutil.cpu_count(logical=True)\n",
        "physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
        "\n",
        "print(f\"System Resources:\")\n",
        "print(f\"- {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
        "print(f\"- {available_memory_gb:.1f} GB available memory\")\n",
        "print(f\"For multiple HEC-RAS projects, a good rule of thumb is:\")\n",
        "print(f\"- Assign 2-4 cores per project\")\n",
        "print(f\"- Allocate at least 2-4 GB of RAM per project\")\n",
        "print(f\"Based on your system, you could reasonably run {min(physical_cpu_count//2, int(available_memory_gb//3))} projects simultaneously.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Multiple RAS Project Management\n",
        "\n",
        "When working with multiple HEC-RAS projects in RAS Commander, there are two important concepts to understand:\n",
        "\n",
        "1. **The Global 'ras' Object**: By default, RAS Commander maintains a global `ras` object that represents the currently active project. This is convenient for simple scripts.\n",
        "\n",
        "2. **Custom RAS Objects**: For multiple projects, you'll create separate RAS objects for each project. These custom objects store project-specific data and are passed to RAS Commander functions using the `ras_object` parameter.\n",
        "\n",
        "### Best Practices for Multiple Project Management\n",
        "\n",
        "- **Name Your Objects Clearly**: Use descriptive variable names for your RAS objects (e.g., `bald_eagle_ras`, `muncie_ras`)\n",
        "- **Be Consistent**: Always pass the appropriate RAS object to functions when working with multiple projects\n",
        "- **Avoid Using Global 'ras'**: When working with multiple projects, avoid using the global `ras` object to prevent confusion\n",
        "- **Separate Compute Folders**: Use separate computation folders for each project\n",
        "- **Manage Resources**: Be mindful of CPU and memory usage when running multiple projects in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract two example HEC-RAS projects: \"Balde Eagle Creek\" and \"Muncie\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:42.431798Z",
          "iopub.status.busy": "2025-11-17T18:31:42.431536Z",
          "iopub.status.idle": "2025-11-17T18:31:43.092073Z",
          "shell.execute_reply": "2025-11-17T18:31:43.091458Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:42 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed existing example projects directory: C:\\GH\\ras-commander\\examples\\example_projects\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted projects to:\n",
            "- C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "- C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n",
            "\n",
            "Bald Eagle Creek project exists: True\n",
            "Muncie project exists: True\n"
          ]
        }
      ],
      "source": [
        "# Delete existing project if it exists to ensure a clean start\n",
        "if examples_dir.exists():\n",
        "    shutil.rmtree(examples_dir)\n",
        "    print(f\"Removed existing example projects directory: {examples_dir}\")\n",
        "\n",
        "# Create a RasExamples instance\n",
        "ras_examples = RasExamples()\n",
        "\n",
        "# Extract the example projects\n",
        "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\", \"Muncie\"])\n",
        "print(f\"Extracted projects to:\")\n",
        "for path in extracted_paths:\n",
        "    print(f\"- {path}\")\n",
        "\n",
        "# Verify the paths exist\n",
        "print(f\"\\nBald Eagle Creek project exists: {bald_eagle_path.exists()}\")\n",
        "print(f\"Muncie project exists: {muncie_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Multiple Projects\n",
        "\n",
        "Let's initialize both HEC-RAS projects. Instead of using the global `ras` object, we'll create separate RAS objects for each project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:43.094547Z",
          "iopub.status.busy": "2025-11-17T18:31:43.094337Z",
          "iopub.status.idle": "2025-11-17T18:31:43.232786Z",
          "shell.execute_reply": "2025-11-17T18:31:43.232115Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized Bald Eagle Creek project: BaldEagle\n",
            "Initialized Muncie project: Muncie\n",
            "\n",
            "Available plans in Bald Eagle Creek project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "0                 2MIN            1HOUR        1  ...     None      dss   \n",
              "1                 2MIN              NaN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method HDF_Results_Path Geom File  \\\n",
              "0                     2             None        01   \n",
              "1                     1             None        01   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...    Steady  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available plans in Muncie project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady Multi  9-SA run</td>\\n', '      <td>5.00</td>\\n', '      <td>9-SAs</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>15SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady Run with 2D 50ft Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>2D 50ft Grid</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>-1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>01</td>\\n', '      <td>04</td>\\n', '      <td>Unsteady Run with 2D 50ft User n Value R</td>\\n', '      <td>5.10</td>\\n', '      <td>50ft User n Regions</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>04</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number  \\\n",
              "0          01              01              01   \n",
              "1          03              01              02   \n",
              "2          04              01              04   \n",
              "\n",
              "                                 Plan Title Program Version  \\\n",
              "0                  Unsteady Multi  9-SA run            5.00   \n",
              "1            Unsteady Run with 2D 50ft Grid            5.10   \n",
              "2  Unsteady Run with 2D 50ft User n Value R            5.10   \n",
              "\n",
              "      Short Identifier                Simulation Date Computation Interval  \\\n",
              "0                9-SAs  02JAN1900,0000,02JAN1900,2400                15SEC   \n",
              "1         2D 50ft Grid  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "2  50ft User n Regions  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "\n",
              "  Mapping Interval Run HTab  ... Friction Slope Method UNET D2 SolverType  \\\n",
              "0             5MIN        1  ...                     1                NaN   \n",
              "1             5MIN       -1  ...                     1   Pardiso (Direct)   \n",
              "2             5MIN        1  ...                     1   Pardiso (Direct)   \n",
              "\n",
              "       UNET D2 Name HDF_Results_Path Geom File  \\\n",
              "0               NaN             None        01   \n",
              "1  2D Interior Area             None        02   \n",
              "2  2D Interior Area             None        04   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "\n",
              "[3 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize both projects with their own RAS objects\n",
        "bald_eagle_ras = RasPrj()\n",
        "init_ras_project(bald_eagle_path, \"6.6\", ras_object=bald_eagle_ras)\n",
        "print(f\"Initialized Bald Eagle Creek project: {bald_eagle_ras.project_name}\")\n",
        "\n",
        "muncie_ras = RasPrj()\n",
        "init_ras_project(muncie_path, \"6.6\", ras_object=muncie_ras)\n",
        "print(f\"Initialized Muncie project: {muncie_ras.project_name}\")\n",
        "\n",
        "# Display available plans in each project\n",
        "print(\"\\nAvailable plans in Bald Eagle Creek project:\")\n",
        "display.display(bald_eagle_ras.plan_df)\n",
        "\n",
        "print(\"\\nAvailable plans in Muncie project:\")\n",
        "display.display(muncie_ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone Plans in Each Project\n",
        "\n",
        "Now, let's clone a plan in each project, giving them custom short identifiers. This demonstrates how to perform operations on multiple projects independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:43.234840Z",
          "iopub.status.busy": "2025-11-17T18:31:43.234602Z",
          "iopub.status.idle": "2025-11-17T18:31:43.382563Z",
          "shell.execute_reply": "2025-11-17T18:31:43.381320Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Project file updated with new Plan entry: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01 to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created new plan 03 in Bald Eagle Creek project\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Project file updated with new Plan entry: 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created new plan 02 in Muncie project\n",
            "\n",
            "Updated plans in Bald Eagle Creek project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>UNET D2 Cores</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>0.0</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>MultiProjDemo</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>0.0</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "2          03              02              01  Unsteady with Bridges and Dam   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "2            5.00    MultiProjDemo    18FEB1999,0000,24FEB1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... UNET D2 Cores PS Cores  \\\n",
              "0                 2MIN            1HOUR        1  ...           0.0     None   \n",
              "1                 2MIN              NaN        1  ...           NaN     None   \n",
              "2                 2MIN            1HOUR        1  ...           0.0     None   \n",
              "\n",
              "  DSS File Friction Slope Method HDF_Results_Path  Geom File  Geom Path  \\\n",
              "0      dss                     2             None         01       None   \n",
              "1      dss                     1             None         01       None   \n",
              "2      dss                     2             None         01       None   \n",
              "\n",
              "  Flow File Flow Path                                          full_path  \n",
              "0        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "1        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "2        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "\n",
              "[3 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated plans in Muncie project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady Multi  9-SA run</td>\\n', '      <td>5.00</td>\\n', '      <td>9-SAs</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>15SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady Run with 2D 50ft Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>2D 50ft Grid</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>-1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>01</td>\\n', '      <td>04</td>\\n', '      <td>Unsteady Run with 2D 50ft User n Value R</td>\\n', '      <td>5.10</td>\\n', '      <td>50ft User n Regions</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>04</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady Multi  9-SA run</td>\\n', '      <td>5.00</td>\\n', '      <td>MultiProjDemo</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>15SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number  \\\n",
              "0          01              01              01   \n",
              "1          03              01              02   \n",
              "2          04              01              04   \n",
              "3          02              01              01   \n",
              "\n",
              "                                 Plan Title Program Version  \\\n",
              "0                  Unsteady Multi  9-SA run            5.00   \n",
              "1            Unsteady Run with 2D 50ft Grid            5.10   \n",
              "2  Unsteady Run with 2D 50ft User n Value R            5.10   \n",
              "3                  Unsteady Multi  9-SA run            5.00   \n",
              "\n",
              "      Short Identifier                Simulation Date Computation Interval  \\\n",
              "0                9-SAs  02JAN1900,0000,02JAN1900,2400                15SEC   \n",
              "1         2D 50ft Grid  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "2  50ft User n Regions  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "3        MultiProjDemo  02JAN1900,0000,02JAN1900,2400                15SEC   \n",
              "\n",
              "  Mapping Interval Run HTab  ... DSS File Friction Slope Method  \\\n",
              "0             5MIN        1  ...      dss                     1   \n",
              "1             5MIN       -1  ...      dss                     1   \n",
              "2             5MIN        1  ...      dss                     1   \n",
              "3             5MIN        1  ...      dss                     1   \n",
              "\n",
              "  UNET D2 SolverType      UNET D2 Name HDF_Results_Path Geom File  Geom Path  \\\n",
              "0                NaN               NaN             None        01       None   \n",
              "1   Pardiso (Direct)  2D Interior Area             None        02       None   \n",
              "2   Pardiso (Direct)  2D Interior Area             None        04       None   \n",
              "3                NaN               NaN             None        01       None   \n",
              "\n",
              "   Flow File Flow Path                                          full_path  \n",
              "0         01      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "1         01      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "2         01      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "3         01      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "\n",
              "[4 rows x 29 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Clone plans with custom short identifiers\n",
        "new_bald_eagle_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=bald_eagle_ras)\n",
        "print(f\"Created new plan {new_bald_eagle_plan} in Bald Eagle Creek project\")\n",
        "\n",
        "new_muncie_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=muncie_ras)\n",
        "print(f\"Created new plan {new_muncie_plan} in Muncie project\")\n",
        "\n",
        "# Display the updated plan dataframes\n",
        "print(\"\\nUpdated plans in Bald Eagle Creek project:\")\n",
        "bald_eagle_ras.plan_df = bald_eagle_ras.get_plan_entries()  # Refresh the plan dataframe\n",
        "display.display(bald_eagle_ras.plan_df)\n",
        "\n",
        "print(\"\\nUpdated plans in Muncie project:\")\n",
        "muncie_ras.plan_df = muncie_ras.get_plan_entries()  # Refresh the plan dataframe\n",
        "display.display(muncie_ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Plans for Both Projects\n",
        "\n",
        "Let's configure the plans for both projects, setting geometry, number of cores, and other parameters. This demonstrates how to customize plans for different projects using the same code structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:43.385327Z",
          "iopub.status.busy": "2025-11-17T18:31:43.385074Z",
          "iopub.status.idle": "2025-11-17T18:31:43.523213Z",
          "shell.execute_reply": "2025-11-17T18:31:43.522757Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasPlan - INFO - Updated Geom File in plan file to g01 for plan 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasPlan - INFO - Geometry for plan 03 set to 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring Bald Eagle Creek plan:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasPlan - INFO - Successfully updated intervals in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasPlan - INFO - Updated Geom File in plan file to g01 for plan 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasPlan - INFO - Geometry for plan 02 set to 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully configured Bald Eagle Creek plan\n",
            "\n",
            "Configuring Muncie plan:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasPlan - INFO - Successfully updated intervals in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully configured Muncie plan\n"
          ]
        }
      ],
      "source": [
        "# Configure the Bald Eagle Creek plan\n",
        "print(\"Configuring Bald Eagle Creek plan:\")\n",
        "RasPlan.set_geom(new_bald_eagle_plan, \"01\", ras_object=bald_eagle_ras)\n",
        "RasPlan.set_num_cores(new_bald_eagle_plan, 2, ras_object=bald_eagle_ras)\n",
        "\n",
        "# Update description and intervals\n",
        "description = \"Multi-project demonstration plan\\nBald Eagle Creek project\\nConfigured for parallel execution\"\n",
        "RasPlan.update_plan_description(new_bald_eagle_plan, description, ras_object=bald_eagle_ras)\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_bald_eagle_plan, \n",
        "    computation_interval=\"10SEC\", \n",
        "    output_interval=\"5MIN\", \n",
        "    ras_object=bald_eagle_ras\n",
        ")\n",
        "print(\"Successfully configured Bald Eagle Creek plan\")\n",
        "\n",
        "# Configure the Muncie plan\n",
        "print(\"\\nConfiguring Muncie plan:\")\n",
        "RasPlan.set_geom(new_muncie_plan, \"01\", ras_object=muncie_ras)\n",
        "RasPlan.set_num_cores(new_muncie_plan, 2, ras_object=muncie_ras)\n",
        "\n",
        "# Update description and intervals\n",
        "description = \"Multi-project demonstration plan\\nMuncie project\\nConfigured for parallel execution\"\n",
        "RasPlan.update_plan_description(new_muncie_plan, description, ras_object=muncie_ras)\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_muncie_plan, \n",
        "    computation_interval=\"10SEC\", \n",
        "    output_interval=\"5MIN\", \n",
        "    ras_object=muncie_ras\n",
        ")\n",
        "print(\"Successfully configured Muncie plan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Compute Folders for Both Projects\n",
        "\n",
        "Now, let's create separate compute folders for each project. This allows us to run the computations separately and in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:43.525285Z",
          "iopub.status.busy": "2025-11-17T18:31:43.524871Z",
          "iopub.status.idle": "2025-11-17T18:31:43.528757Z",
          "shell.execute_reply": "2025-11-17T18:31:43.528352Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\n",
            "Created compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\n"
          ]
        }
      ],
      "source": [
        "# Create compute folders or clean them if they already exist\n",
        "for folder in [bald_eagle_compute_folder, muncie_compute_folder]:\n",
        "    if folder.exists():\n",
        "        shutil.rmtree(folder)\n",
        "        print(f\"Removed existing compute folder: {folder}\")\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created compute folder: {folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Define Project Execution Function\n",
        "\n",
        "Let's define a function to execute plans for each project, which we can run in parallel. This function will handle plan execution, timing, and provide detailed status updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:43.530767Z",
          "iopub.status.busy": "2025-11-17T18:31:43.530390Z",
          "iopub.status.idle": "2025-11-17T18:31:43.535269Z",
          "shell.execute_reply": "2025-11-17T18:31:43.534857Z"
        }
      },
      "outputs": [],
      "source": [
        "def execute_plan(plan_number, ras_object, compute_folder, project_name):\n",
        "    \"\"\"\n",
        "    Execute a HEC-RAS plan and return detailed information about the execution.\n",
        "    \n",
        "    Args:\n",
        "        plan_number (str): The plan number to execute\n",
        "        ras_object: The RAS project object\n",
        "        compute_folder (Path): Folder where computation will be performed\n",
        "        project_name (str): A descriptive name for the project\n",
        "        \n",
        "    Returns:\n",
        "        dict: Detailed information about the execution\n",
        "    \"\"\"\n",
        "    print(f\"Starting execution of plan {plan_number} for {project_name}...\")\n",
        "    \n",
        "    # Record start time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Execute the plan in the compute folder\n",
        "    success = RasCmdr.compute_plan(\n",
        "        plan_number=plan_number, \n",
        "        ras_object=ras_object, \n",
        "        dest_folder=compute_folder,\n",
        "        clear_geompre=True\n",
        "    )\n",
        "    \n",
        "    # Record end time and calculate duration\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    # Determine if results were created\n",
        "    result_path = None\n",
        "    result_size = None\n",
        "    \n",
        "    try:\n",
        "        # Initialize a temporary RAS object in the compute folder to check results\n",
        "        compute_ras = init_ras_project(compute_folder, ras_object.ras_exe_path)\n",
        "        result_path = RasPlan.get_results_path(plan_number, ras_object=compute_ras)\n",
        "        \n",
        "        if result_path:\n",
        "            result_file = Path(result_path)\n",
        "            if result_file.exists():\n",
        "                result_size = result_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking results for {project_name}: {e}\")\n",
        "    \n",
        "    # Build result information\n",
        "    result_info = {\n",
        "        \"project_name\": project_name,\n",
        "        \"plan_number\": plan_number,\n",
        "        \"success\": success,\n",
        "        \"duration\": duration,\n",
        "        \"compute_folder\": str(compute_folder),\n",
        "        \"result_path\": str(result_path) if result_path else None,\n",
        "        \"result_size_mb\": result_size,\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    \n",
        "    print(f\"Completed execution of plan {plan_number} for {project_name} in {duration:.2f} seconds\")\n",
        "    return result_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Execute Plans for Both Projects in Parallel\n",
        "\n",
        "Now, let's run both projects in parallel using a `ThreadPoolExecutor`. This allows us to utilize our system resources efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:31:43.537541Z",
          "iopub.status.busy": "2025-11-17T18:31:43.537226Z",
          "iopub.status.idle": "2025-11-17T18:33:41.527608Z",
          "shell.execute_reply": "2025-11-17T18:33:41.526984Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing plans for both projects in parallel...\n",
            "This may take several minutes...\n",
            "Starting execution of plan 03 for Bald Eagle Creek...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Copied project folder to destination: C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting execution of plan 02 for Muncie...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\\BaldEagle.p03\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Copied project folder to destination: C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\\Muncie.p02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:31:43 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\\Muncie.p02\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:32:03 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:32:03 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 20.20 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:32:03 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed execution of plan 02 for Muncie in 20.37 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:33:41 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:33:41 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 117.87 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:33:41 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_bald_eagle\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed execution of plan 03 for Bald Eagle Creek in 117.97 seconds\n",
            "\n",
            "All executions complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"Executing plans for both projects in parallel...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Define the execution tasks\n",
        "execution_tasks = [\n",
        "    (new_bald_eagle_plan, bald_eagle_ras, bald_eagle_compute_folder, \"Bald Eagle Creek\"),\n",
        "    (new_muncie_plan, muncie_ras, muncie_compute_folder, \"Muncie\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Execute the plans in parallel using ThreadPoolExecutor\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    futures = [\n",
        "        executor.submit(execute_plan, *task)\n",
        "        for task in execution_tasks\n",
        "    ]\n",
        "    \n",
        "    # Collect results as they complete\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            result = future.result()\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Execution error: {e}\")\n",
        "\n",
        "print(\"\\nAll executions complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analyze Results\n",
        "\n",
        "Let's analyze the results from both project executions, comparing execution times, result sizes, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:33:41.530062Z",
          "iopub.status.busy": "2025-11-17T18:33:41.529784Z",
          "iopub.status.idle": "2025-11-17T18:33:41.733112Z",
          "shell.execute_reply": "2025-11-17T18:33:41.732512Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Results Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>project_name</th>\\n', '      <th>plan_number</th>\\n', '      <th>success</th>\\n', '      <th>duration</th>\\n', '      <th>result_size_mb</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>Muncie</td>\\n', '      <td>02</td>\\n', '      <td>True</td>\\n', '      <td>20.365752</td>\\n', '      <td>3.857758</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>Bald Eagle Creek</td>\\n', '      <td>03</td>\\n', '      <td>True</td>\\n', '      <td>117.969050</td>\\n', '      <td>11.529761</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "       project_name plan_number  success    duration  result_size_mb\n",
              "0            Muncie          02     True   20.365752        3.857758\n",
              "1  Bald Eagle Creek          03     True  117.969050       11.529761"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the results table\n",
        "print(\"Execution Results Summary:\")\n",
        "display.display(results_df[['project_name', 'plan_number', 'success', 'duration', 'result_size_mb']])\n",
        "\n",
        "# Create a bar chart for execution times\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results_df['project_name'], results_df['duration'], color=['blue', 'green'])\n",
        "plt.title('Execution Time by Project')\n",
        "plt.xlabel('Muncie Plan 02 vs Bald Eagle Creek Plan 02\\n (2 separate projects, for demonstration purposes only)')\n",
        "plt.ylabel('Execution Time (seconds)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add duration values on top of the bars\n",
        "for i, duration in enumerate(results_df['duration']):\n",
        "    plt.text(i, duration + 5, f\"{duration:.1f}s\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# If we have result sizes, create a chart for those as well\n",
        "if results_df['result_size_mb'].notna().any():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(results_df['project_name'], results_df['result_size_mb'], color=['orange', 'purple'])\n",
        "    plt.title('Result File Size by Project')\n",
        "    plt.xlabel('Project')\n",
        "    plt.ylabel('Result Size (MB)')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Add size values on top of the bars\n",
        "    for i, size in enumerate(results_df['result_size_mb']):\n",
        "        if pd.notna(size):\n",
        "            plt.text(i, size + 2, f\"{size:.1f} MB\", ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Compare Two HEC-RAS Projects\n",
        "\n",
        "Let's create a utility function to compare the structures of the two HEC-RAS projects. This helps us understand the differences between the projects we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:33:41.735705Z",
          "iopub.status.busy": "2025-11-17T18:33:41.735396Z",
          "iopub.status.idle": "2025-11-17T18:33:41.861620Z",
          "shell.execute_reply": "2025-11-17T18:33:41.861013Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project Structure Comparison:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Project Name</th>\\n', '      <th>Plan Count</th>\\n', '      <th>Geometry Count</th>\\n', '      <th>Flow Count</th>\\n', '      <th>Unsteady Count</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>Bald Eagle Creek</th>\\n', '      <td>BaldEagle</td>\\n', '      <td>3</td>\\n', '      <td>1</td>\\n', '      <td>2</td>\\n', '      <td>1</td>\\n', '    </tr><tr>\\n', '      <th>Muncie</th>\\n', '      <td>Muncie</td>\\n', '      <td>4</td>\\n', '      <td>3</td>\\n', '      <td>1</td>\\n', '      <td>1</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "                 Project Name  Plan Count  Geometry Count  Flow Count  \\\n",
              "Bald Eagle Creek    BaldEagle           3               1           2   \n",
              "Muncie                 Muncie           4               3           1   \n",
              "\n",
              "                  Unsteady Count  \n",
              "Bald Eagle Creek               1  \n",
              "Muncie                         1  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def compare_project_structures(ras_object1, name1, ras_object2, name2):\n",
        "    \"\"\"\n",
        "    Compare the structures of two HEC-RAS projects and display differences.\n",
        "    \"\"\"\n",
        "    # Refresh all dataframes to ensure we have the latest data\n",
        "    ras_object1.plan_df = ras_object1.get_plan_entries()\n",
        "    ras_object1.geom_df = ras_object1.get_geom_entries()\n",
        "    ras_object1.flow_df = ras_object1.get_flow_entries()\n",
        "    ras_object1.unsteady_df = ras_object1.get_unsteady_entries()\n",
        "    \n",
        "    ras_object2.plan_df = ras_object2.get_plan_entries()\n",
        "    ras_object2.geom_df = ras_object2.get_geom_entries()\n",
        "    ras_object2.flow_df = ras_object2.get_flow_entries()\n",
        "    ras_object2.unsteady_df = ras_object2.get_unsteady_entries()\n",
        "    \n",
        "    # Create a comparison dictionary\n",
        "    comparison = {\n",
        "        'Project Name': [ras_object1.project_name, ras_object2.project_name],\n",
        "        'Plan Count': [len(ras_object1.plan_df), len(ras_object2.plan_df)],\n",
        "        'Geometry Count': [len(ras_object1.geom_df), len(ras_object2.geom_df)],\n",
        "        'Flow Count': [len(ras_object1.flow_df), len(ras_object2.flow_df)],\n",
        "        'Unsteady Count': [len(ras_object1.unsteady_df), len(ras_object2.unsteady_df)]\n",
        "    }\n",
        "    \n",
        "    # Create a DataFrame for the comparison\n",
        "    comparison_df = pd.DataFrame(comparison, index=[name1, name2])\n",
        "    \n",
        "\n",
        "    # Display the comparison\n",
        "    print(\"Project Structure Comparison:\")\n",
        "    display.display(comparison_df)\n",
        "    \n",
        "    # Create a bar chart to visualize the comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    comparison_df.iloc[:, 1:].plot(kind='bar', ax=plt.gca())\n",
        "    plt.title('Project Structure Comparison')\n",
        "    plt.xlabel('Project')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Component')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Set y-axis to only show whole numbers (integers)\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "# Compare the structures of the two projects\n",
        "comparison_df = compare_project_structures(\n",
        "    bald_eagle_ras, \"Bald Eagle Creek\", \n",
        "    muncie_ras, \"Muncie\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## This approach can also be used to programmatically compare 2 copies of the same project to ensure all of the plan parameters, boundary condition definitions, etc remained the same, and for other QAQC processes.\n",
        "\n",
        "This will be shown in further examples in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Multiple Project Operations\n",
        "\n",
        "In this notebook, we've demonstrated how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. We've covered the following key operations:\n",
        "\n",
        "1. **Initializing Multiple Projects**: Creating separate RAS objects for different projects\n",
        "2. **Independent Configuration**: Configuring plans with project-specific parameters\n",
        "3. **Parallel Execution**: Running computations from different projects simultaneously\n",
        "4. **Resource Management**: Organizing compute folders and tracking execution statistics\n",
        "5. **Results Comparison**: Analyzing and comparing results from different projects\n",
        "6. **Advanced Workflows**: Creating sensitivity plans and batch processing pipelines\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "When working with multiple HEC-RAS projects in RAS Commander, remember these key concepts:\n",
        "\n",
        "- **Custom RAS Objects**: Create and use separate RAS objects for each project\n",
        "- **Always Specify ras_object**: Use the `ras_object` parameter in all function calls\n",
        "- **Separate Compute Folders**: Use separate folders for each project's computations\n",
        "- **Resource Management**: Be mindful of CPU and memory usage when running in parallel\n",
        "- **Project Tracking**: Keep track of which results belong to which project\n",
        "\n",
        "### Multiple Project Applications\n",
        "\n",
        "Working with multiple projects unlocks advanced applications such as:\n",
        "\n",
        "1. **Model Comparison**: Compare results from different river systems\n",
        "2. **Basin-wide Analysis**: Analyze connected river systems in parallel\n",
        "3. **Parameter Sweep**: Test a range of parameters across multiple models\n",
        "4. **Model Development**: Develop and test models simultaneously\n",
        "5. **Batch Processing**: Process large sets of models in an automated pipeline\n",
        "\n",
        "These capabilities make RAS Commander a powerful tool for large-scale hydraulic modeling and water resources management."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\05_single_plan_execution.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Working Environment\n",
        "\n",
        "Let's set up our working directory and paths to example projects. We'll also check the number of available CPU cores on this system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n",
            "2025-12-02 16:52:37 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted project to: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "Bald Eagle Creek project exists: True\n"
          ]
        }
      ],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System has 8 physical CPU cores (8 logical cores)\n",
            "For HEC-RAS computation, it's often most efficient to use 2-8 cores\n"
          ]
        }
      ],
      "source": [
        "# Define paths to example projects\n",
        "examples_dir = bald_eagle_path.parent\n",
        "\n",
        "# Define computation output paths\n",
        "compute_dest_folder = examples_dir / \"compute_test\"\n",
        "\n",
        "# Check system resources\n",
        "cpu_count = psutil.cpu_count(logical=True)\n",
        "physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "print(f\"System has {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
        "print(f\"For HEC-RAS computation, it's often most efficient to use 2-8 cores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasCmdr.compute_plan Method\n",
        "\n",
        "Before we dive into execution, let's understand the `compute_plan` method from the `RasCmdr` class, which is the core function for running HEC-RAS simulations.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (str, Path): The plan number to execute or the full path to the plan file\n",
        "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
        "- `ras_object` (RasPrj, optional): Specific RAS object to use (defaults to global `ras`)\n",
        "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files (default: False)\n",
        "- `num_cores` (int, optional): Number of processor cores to use (default: None, uses plan settings)\n",
        "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists (default: False)\n",
        "\n",
        "### Returns\n",
        "- `bool`: True if the execution was successful, False otherwise\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Destination Folder**: By default, the simulation runs in the original project folder. Specifying a destination folder creates a copy of the project in that location for execution, leaving the original project untouched.\n",
        "\n",
        "2. **Number of Cores**: HEC-RAS can use multiple processor cores to speed up computation. The optimal number depends on the model complexity and your computer's specifications. Generally:\n",
        "   - 1-2 cores: Good for small models, highest efficiency per core\n",
        "   - 3-8 cores: Good balance for most models\n",
        "   - >8 cores: Diminishing returns, may actually be slower due to overhead\n",
        "\n",
        "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties, which is useful after making geometry changes.\n",
        "\n",
        "4. **Overwrite Destination**: Controls whether an existing destination folder should be overwritten. This is a safety feature to prevent accidental deletion of important results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:52:37 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized HEC-RAS project: BaldEagle\n"
          ]
        }
      ],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Explore Available Plans\n",
        "\n",
        "Let's examine the available plans in the project to understand what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available plans in the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "0                 2MIN            1HOUR        1  ...     None      dss   \n",
              "1                 2MIN              NaN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method HDF_Results_Path Geom File  \\\n",
              "0                     2             None        01   \n",
              "1                     1             None        01   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...    Steady  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:52:37 - ras_commander.RasPlan - ERROR - Key 'PS Cores' not found in the plan file.\n",
            "2025-12-02 16:52:37 - ras_commander.RasPlan - ERROR - Key 'UNET D1 Cores' not found in the plan file.\n",
            "2025-12-02 16:52:37 - ras_commander.RasPlan - ERROR - Key 'UNET D2 Cores' not found in the plan file.\n",
            "2025-12-02 16:52:37 - ras_commander.RasPlan - ERROR - Key 'PS Cores' not found in the plan file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Current core settings for plans:\n",
            "Plan 01's Existing Settings:\n",
            "  1D Cores: 0\n",
            "  2D Cores: 0\n",
            "  Pump Station Cores: None\n",
            "Plan 02's Existing Settings:\n",
            "  1D Cores: None\n",
            "  2D Cores: None\n",
            "  Pump Station Cores: None\n"
          ]
        }
      ],
      "source": [
        "# Display the available plans in the project\n",
        "print(\"Available plans in the project:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Let's check the current setting for number of cores in the plans\n",
        "print(\"\\nCurrent core settings for plans:\")\n",
        "for plan_num in ras.plan_df['plan_number']:\n",
        "    # Check all three core parameters\n",
        "    d1_cores = RasPlan.get_plan_value(plan_num, \"UNET D1 Cores\")\n",
        "    d2_cores = RasPlan.get_plan_value(plan_num, \"UNET D2 Cores\") \n",
        "    ps_cores = RasPlan.get_plan_value(plan_num, \"PS Cores\")\n",
        "    \n",
        "    print(f\"Plan {plan_num}'s Existing Settings:\")\n",
        "    print(f\"  1D Cores: {d1_cores}\")\n",
        "    print(f\"  2D Cores: {d2_cores}\")\n",
        "    print(f\"  Pump Station Cores: {ps_cores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a Destination Folder Structure\n",
        "\n",
        "Now, let's prepare a destination folder for our computation. This allows us to run simulations without modifying the original project files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination folder will be created: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\n"
          ]
        }
      ],
      "source": [
        "# Create a destination folder path\n",
        "dest_folder = examples_dir / \"compute_test_cores\"\n",
        "\n",
        "# Check if the destination folder already exists\n",
        "if dest_folder.exists():\n",
        "    print(f\"Destination folder already exists: {dest_folder}\")\n",
        "    print(\"We'll use overwrite_dest=True to replace it\")\n",
        "else:\n",
        "    print(f\"Destination folder will be created: {dest_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Execute a Plan with a Specified Number of Cores\n",
        "\n",
        "Now we're ready to execute a plan with a specified number of cores, overwriting the destination folder if it exists. This is the core functionality demonstrated in Example 5 of the original script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:52:37 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "2025-12-02 16:52:37 - ras_commander.RasCmdr - INFO - Copied project folder to destination: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\n",
            "2025-12-02 16:52:37 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.rasmap\n",
            "2025-12-02 16:52:37 - ras_commander.RasUtils - INFO - Using provided plan file path: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.p01\n",
            "2025-12-02 16:52:37 - ras_commander.RasUtils - INFO - Successfully updated file: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.p01\n",
            "2025-12-02 16:52:37 - ras_commander.RasCmdr - INFO - Set number of cores to 2 for plan: 01\n",
            "2025-12-02 16:52:37 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 16:52:37 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.prj\" \"c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.p01\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing plan 01 with 2 cores...\n",
            "Destination folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:54:09 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 16:54:09 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 91.70 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Plan 01 executed successfully using 2 cores\n",
            "Execution time: 91.79 seconds\n"
          ]
        }
      ],
      "source": [
        "# Select a plan and number of cores\n",
        "plan_number = \"01\"\n",
        "num_cores = 2  # Specify the number of cores to use\n",
        "\n",
        "print(f\"Executing plan {plan_number} with {num_cores} cores...\")\n",
        "print(f\"Destination folder: {dest_folder}\")\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute the plan with specified parameters\n",
        "success = RasCmdr.compute_plan(\n",
        "    plan_number,              # The plan to execute\n",
        "    dest_folder=dest_folder,  # Where to run the simulation\n",
        "    num_cores=num_cores,      # Number of processor cores to use\n",
        "    overwrite_dest=True       # Overwrite destination folder if it exists\n",
        ")\n",
        "\n",
        "# Record the end time and calculate duration\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "# Report results\n",
        "if success:\n",
        "    print(f\"\u2705 Plan {plan_number} executed successfully using {num_cores} cores\")\n",
        "    print(f\"Execution time: {duration:.2f} seconds\")\n",
        "else:\n",
        "    print(f\"\u274c Plan {plan_number} execution failed\")\n",
        "    print(f\"Time elapsed: {duration:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Results\n",
        "\n",
        "After execution, let's verify the results by checking the results paths and examining the destination folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination folder exists: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\n",
            "\n",
            "Key files in destination folder:\n",
            "  BaldEagle.b01: 9.3 KB\n",
            "  BaldEagle.bco01: 2.2 KB\n",
            "  BaldEagle.c01: 522.1 KB\n",
            "  BaldEagle.dss: 3601.5 KB\n",
            "  BaldEagle.f01: 1209.0 KB\n",
            "  BaldEagle.f02: 1.5 KB\n",
            "  BaldEagle.g01: 513.6 KB\n",
            "  BaldEagle.g01.gmz: 372.6 KB\n",
            "  BaldEagle.g01.hdf: 3920.5 KB\n",
            "  BaldEagle.gis: 127.8 KB\n",
            "  ... and 13 more files\n",
            "\n",
            "HDF result files:\n",
            "  BaldEagle.g01.hdf: 3.8 MB\n",
            "  BaldEagle.p01.hdf: 7.4 MB\n",
            "  BaldEagle.u02.hdf: 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "# Verify that the destination folder exists and contains the expected files\n",
        "if dest_folder.exists():\n",
        "    print(f\"Destination folder exists: {dest_folder}\")\n",
        "    \n",
        "    # List the key files in the destination folder\n",
        "    print(\"\\nKey files in destination folder:\")\n",
        "    project_files = list(dest_folder.glob(f\"{ras.project_name}.*\"))\n",
        "    for file in project_files[:10]:  # Show first 10 files\n",
        "        file_size = file.stat().st_size / 1024  # Size in KB\n",
        "        print(f\"  {file.name}: {file_size:.1f} KB\")\n",
        "    \n",
        "    if len(project_files) > 10:\n",
        "        print(f\"  ... and {len(project_files) - 10} more files\")\n",
        "    \n",
        "    # Check for HDF result files\n",
        "    print(\"\\nHDF result files:\")\n",
        "    hdf_files = list(dest_folder.glob(f\"*.hdf\"))\n",
        "    for file in hdf_files:\n",
        "        file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "        print(f\"  {file.name}: {file_size:.1f} MB\")\n",
        "else:\n",
        "    print(f\"Destination folder does not exist: {dest_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:54:09 - ras_commander.RasPrj - INFO - No HEC-RAS Version Specified.Attempting to detect HEC-RAS version from plan files.\n",
            "2025-12-02 16:54:09 - ras_commander.RasPrj - INFO - Searching for plan files in C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\n",
            "2025-12-02 16:54:09 - ras_commander.RasPrj - INFO - Found plan file: BaldEagle.p01\n",
            "2025-12-02 16:54:09 - ras_commander.RasPrj - INFO - Successfully read plan file with utf-8 encoding\n",
            "2025-12-02 16:54:09 - ras_commander.RasPrj - INFO - Found Program Version=5.00 in BaldEagle.p01\n",
            "2025-12-02 16:54:09 - ras_commander.RasPrj - INFO - Checking RAS executable path: C:\\Program Files (x86)\\HEC\\HEC-RAS\\5.0\\Ras.exe\n",
            "2025-12-02 16:54:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.rasmap\n"
          ]
        },
        {
          "data": {
            "text/plain": "['<ras_commander.RasPrj.RasPrj at 0x240d845de80>']"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Since we are now working in the dest_folder, init_ras_project in that folder \n",
        "\n",
        "init_ras_project(dest_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Extract Computation Messages\n",
        "\n",
        "After successful plan execution, we can extract detailed computation messages that provide insights into:\n",
        "- Computation time and performance metrics\n",
        "- Warning messages and errors (if any)\n",
        "- Convergence information\n",
        "- Process timing breakdown\n",
        "\n",
        "The `HdfResultsPlan.get_compute_messages()` function automatically extracts messages from the HDF file, with fallback to .txt files for older HEC-RAS versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:57:29 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.p01.hdf\n",
            "2025-12-02 16:57:29 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: BaldEagle.p01.hdf\n",
            "2025-12-02 16:57:29 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1693 characters from HDF\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXTRACTING COMPUTATION MESSAGES\n",
            "================================================================================\n",
            "\n",
            "Successfully extracted computation messages (1693 characters)\n",
            "\n",
            "First 1000 characters of computation messages:\n",
            "--------------------------------------------------------------------------------\n",
            "Plan: 'Unsteady with Bridges and Dam' (BaldEagle.p01)\n",
            "Simulation started at: 02Dec2025 04:52:39 PM\n",
            "\n",
            "Writing Plan GIS Data...\n",
            "Completed Writing Plan GIS Data\n",
            "Writing Geometry...\n",
            "Computing Bank Lines\n",
            "Bank lines generated in 108 ms\n",
            "Computing Edge Lines\n",
            "Edge Lines generated in 48 ms\n",
            "Computing XS Interpolation Surface\n",
            "XS Interpolation Surface generated in 115 ms\n",
            "Completed Writing Geometry\n",
            "Writing Event Conditions ...\n",
            "Completed Writing Event Condition Data\n",
            "\n",
            "\t\n",
            "Geometric Preprocessor HEC-RAS 6.6 September 2024\n",
            " \n",
            "\n",
            "Finished Processing Geometry\n",
            "\n",
            "\n",
            "Performing Unsteady Flow Simulation  HEC-RAS 6.6 September 2024\n",
            " \n",
            "\t\n",
            "Unsteady Input Summary:\n",
            "     1D Unsteady Finite Difference Numerical Solution\n",
            "\n",
            "Overall Volume Accounting Error in Acre Feet:    -29.5468461514\n",
            "Overall Volume Accounting Error as percentage:           0.01407\n",
            "Please review \"Computational Log File\" output for volume accounting details\n",
            "\n",
            "Writing Results to DSS\n",
            "\n",
            "Finished Unsteady Flow Simulation\n",
            "\n",
            "Reading U\n",
            "\n",
            "... (message truncated for display) ...\n",
            "\n",
            "Total message length: 1693 characters\n",
            "\n",
            "================================================================================\n",
            "CHECKING FOR WARNINGS/ERRORS\n",
            "================================================================================\n",
            "Found 2 warning/error messages:\n",
            "  - Overall Volume Accounting Error in Acre Feet:    -29.5468461514\n",
            "  - Overall Volume Accounting Error as percentage:           0.01407\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Extract and display computation messages\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTING COMPUTATION MESSAGES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extract messages using plan number\n",
        "compute_msgs = HdfResultsPlan.get_compute_messages(plan_number)\n",
        "\n",
        "if compute_msgs:\n",
        "    print(f\"\\nSuccessfully extracted computation messages ({len(compute_msgs)} characters)\\n\")\n",
        "    \n",
        "    # Display first 1000 characters\n",
        "    print(\"First 1000 characters of computation messages:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(compute_msgs[:1000])\n",
        "    \n",
        "    if len(compute_msgs) > 1000:\n",
        "        print(\"\\n... (message truncated for display) ...\")\n",
        "        print(f\"\\nTotal message length: {len(compute_msgs)} characters\")\n",
        "    \n",
        "    # Check for warnings or errors\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CHECKING FOR WARNINGS/ERRORS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    lines = compute_msgs.split('\\n')\n",
        "    warnings_errors = [line for line in lines if 'warning' in line.lower() or 'error' in line.lower()]\n",
        "    \n",
        "    if warnings_errors:\n",
        "        print(f\"Found {len(warnings_errors)} warning/error messages:\")\n",
        "        for msg in warnings_errors[:10]:  # Show first 10\n",
        "            print(f\"  - {msg.strip()}\")\n",
        "    else:\n",
        "        print(\"\u2713 No warnings or errors found in computation messages\")\n",
        "else:\n",
        "    print(\"\u26a0 No computation messages available\")\n",
        "    print(\"This may indicate the plan was not computed or messages are not available.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:57:29 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.rasmap\n",
            "2025-12-02 16:57:29 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for plan 01 are located at: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_cores\\BaldEagle.p01.hdf\n",
            "Results file size: 7.41 MB\n"
          ]
        }
      ],
      "source": [
        "# Check the results path using the RasPlan.get_results_path method\n",
        "# First, initialize a RAS object using the destination folder\n",
        "try:\n",
        "    dest_ras = RasPrj()\n",
        "    init_ras_project(dest_folder, \"6.6\", ras_object=dest_ras)\n",
        "    \n",
        "    # Get the results path for the plan we just executed\n",
        "    results_path = RasPlan.get_results_path(plan_number, ras_object=dest_ras)\n",
        "    \n",
        "    if results_path:\n",
        "        print(f\"Results for plan {plan_number} are located at: {results_path}\")\n",
        "        \n",
        "        # Check if the file exists and get its size\n",
        "        results_file = Path(results_path)\n",
        "        if results_file.exists():\n",
        "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
        "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"No results found for plan {plan_number} in the destination folder\")\n",
        "except Exception as e:\n",
        "    print(f\"Error checking results: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Single Plan Execution Options\n",
        "\n",
        "The `RasCmdr.compute_plan()` method provides a flexible way to execute HEC-RAS plans with various options. Here's a summary of the key parameters we've explored:\n",
        "\n",
        "1. **Basic Execution**: Simply provide a plan number\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\")\n",
        "   ```\n",
        "\n",
        "2. **Destination Folder**: Run in a separate folder to preserve the original project\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\")\n",
        "   ```\n",
        "\n",
        "3. **Number of Cores**: Control the CPU resources used\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", num_cores=2)\n",
        "   ```\n",
        "\n",
        "4. **Overwrite Destination**: Replace existing computation folders\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\", overwrite_dest=True)\n",
        "   ```\n",
        "\n",
        "5. **Clear Geometry Preprocessor**: Force recalculation of geometric properties\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", clear_geompre=True)\n",
        "   ```\n",
        "\n",
        "6. **Combined Options**: Use multiple options together\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\n",
        "       \"01\",\n",
        "       dest_folder=\"path/to/folder\",\n",
        "       num_cores=2,\n",
        "       clear_geompre=True,\n",
        "       overwrite_dest=True\n",
        "   )\n",
        "   ```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further enhance your HEC-RAS automation, consider exploring:\n",
        "\n",
        "1. **Parallel Execution**: Use `RasCmdr.compute_parallel()` to run multiple plans simultaneously\n",
        "2. **Test Mode**: Use `RasCmdr.compute_test_mode()` for testing purposes\n",
        "3. **Pre-Processing**: Modify plans, geometries, and unsteady flows before execution\n",
        "4. **Post-Processing**: Analyze results after computation\n",
        "5. **Batch Processing**: Create scripts for parameter sweeps or scenario analysis\n",
        "\n",
        "These advanced topics are covered in other examples and documentation for the RAS Commander library."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\06_executing_plan_sets.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:35:59.294958Z",
          "iopub.status.busy": "2025-11-17T18:35:59.294779Z",
          "iopub.status.idle": "2025-11-17T18:36:01.297343Z",
          "shell.execute_reply": "2025-11-17T18:36:01.296746Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:36:01.300789Z",
          "iopub.status.busy": "2025-11-17T18:36:01.300420Z",
          "iopub.status.idle": "2025-11-17T18:36:01.304106Z",
          "shell.execute_reply": "2025-11-17T18:36:01.303566Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:36:01.306444Z",
          "iopub.status.busy": "2025-11-17T18:36:01.306179Z",
          "iopub.status.idle": "2025-11-17T18:36:01.368873Z",
          "shell.execute_reply": "2025-11-17T18:36:01.368280Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n",
            "2025-12-02 16:58:18 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted project to: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "Bald Eagle Creek project exists: True\n"
          ]
        }
      ],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan Specification in HEC-RAS\n",
        "\n",
        "In HEC-RAS, each plan (`.p*` file) represents a specific hydraulic model simulation scenario. When working with RAS Commander, you can specify plans for execution in several ways:\n",
        "\n",
        "1. **Single Plan**: Specify one plan by its number (e.g., \"01\")\n",
        "2. **List of Plans**: Specify multiple plans as a list (e.g., [\"01\", \"03\", \"05\"])\n",
        "3. **All Plans**: Execute all plans in a project by not specifying any plan or passing `None`\n",
        "4. **Filtered Plans**: Select plans based on criteria (e.g., plans with specific flow conditions)\n",
        "5. **Plan Path**: Specify the full path to a plan file instead of just the number\n",
        "\n",
        "### Why Plan Specification Matters\n",
        "\n",
        "- **Efficiency**: Run only the plans you need rather than recomputing everything\n",
        "- **Organization**: Group related plans for batch processing\n",
        "- **Automation**: Create workflows that process plans in a specific order\n",
        "- **Resource Management**: Optimize hardware utilization for specific plans\n",
        "\n",
        "### Best Practices for Plan Specification\n",
        "\n",
        "- Use consistent formatting for plan numbers (e.g., always use two-digit strings like \"01\" instead of 1)\n",
        "- Check available plans before attempting to execute them\n",
        "- Organize plans by purpose to make selection easier\n",
        "- Use descriptive short identifiers and plan titles to aid in selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function and explore the available plans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:36:01.371640Z",
          "iopub.status.busy": "2025-11-17T18:36:01.371351Z",
          "iopub.status.idle": "2025-11-17T18:36:01.442763Z",
          "shell.execute_reply": "2025-11-17T18:36:01.442231Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:58:18 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized HEC-RAS project: BaldEagle\n",
            "\n",
            "Available plans in the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "0                 2MIN            1HOUR        1  ...     None      dss   \n",
              "1                 2MIN              NaN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method HDF_Results_Path Geom File  \\\n",
              "0                     2             None        01   \n",
              "1                     1             None        01   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...    Steady  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:58:18 - ras_commander.RasPlan - WARNING - No description found in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01\n",
            "2025-12-02 16:58:18 - ras_commander.RasPlan - WARNING - No description found in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Plan details:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan Number</th>\\n', '      <th>Short ID</th>\\n', '      <th>Description</th>\\n', '      <th>Geometry</th>\\n', '      <th>Has Results</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td></td>\\n', '      <td>01</td>\\n', '      <td>False</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>SteadyRun</td>\\n', '      <td></td>\\n', '      <td>01</td>\\n', '      <td>False</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan Number      Short ID Description Geometry  Has Results\n",
              "0          01  UnsteadyFlow                   01        False\n",
              "1          02     SteadyRun                   01        False"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nAvailable plans in the project:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Check plan details to understand what each plan represents\n",
        "plan_details = []\n",
        "for index, row in ras.plan_df.iterrows():\n",
        "    plan_number = row['plan_number']\n",
        "    \n",
        "    # Get plan description if available\n",
        "    description = None\n",
        "    if 'description' in row:\n",
        "        description = row['description']\n",
        "    else:\n",
        "        try:\n",
        "            description = RasPlan.read_plan_description(plan_number)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Get short identifier if available\n",
        "    short_id = None\n",
        "    if 'Short Identifier' in row:\n",
        "        short_id = row['Short Identifier']\n",
        "    \n",
        "    # Get geometry file\n",
        "    geom_file = None\n",
        "    if 'Geom File' in row:\n",
        "        geom_file = row['Geom File']\n",
        "    \n",
        "    # Check if the plan has results\n",
        "    has_results = False\n",
        "    if 'HDF_Results_Path' in row and row['HDF_Results_Path']:\n",
        "        has_results = True\n",
        "    \n",
        "    plan_details.append({\n",
        "        'Plan Number': plan_number,\n",
        "        'Short ID': short_id,\n",
        "        'Description': description[:50] + '...' if description and len(description) > 50 else description,\n",
        "        'Geometry': geom_file,\n",
        "        'Has Results': has_results\n",
        "    })\n",
        "\n",
        "# Create a DataFrame with the plan details\n",
        "plan_details_df = pd.DataFrame(plan_details)\n",
        "print(\"\\nPlan details:\")\n",
        "display.display(plan_details_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sequential Execution of Specific Plans\n",
        "\n",
        "Let's execute specific plans in sequence using `RasCmdr.compute_test_mode()` with a list of plan numbers. This approach allows us to run only the plans we need, in the order we specify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:36:01.444992Z",
          "iopub.status.busy": "2025-11-17T18:36:01.444744Z",
          "iopub.status.idle": "2025-11-17T18:37:36.316125Z",
          "shell.execute_reply": "2025-11-17T18:37:36.315673Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Starting the compute_test_mode...\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Creating the test folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]...\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Copied project folder to compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\n",
            "2025-12-02 16:58:18 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.rasmap\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Initialized RAS project in compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.prj\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Getting plan entries...\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Retrieved plan entries successfully.\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '03']\n",
            "2025-12-02 16:58:18 - ras_commander.RasCmdr - INFO - Running selected plans sequentially...\n",
            "2025-12-02 16:58:19 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\n",
            "2025-12-02 16:58:19 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.p01\n",
            "2025-12-02 16:58:19 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.p01\n",
            "2025-12-02 16:58:19 - ras_commander.RasCmdr - INFO - Set number of cores to 6 for plan: 01\n",
            "2025-12-02 16:58:19 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 16:58:19 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.p01\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing specific plans sequentially...\n",
            "This may take several minutes...\n",
            "Selected plans: 01, 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 89.69 seconds\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Successfully computed plan 01\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 89.71 seconds\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - All selected plans have been executed.\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - compute_test_mode completed.\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential execution of specific plans completed in 89.77 seconds\n",
            "\n",
            "Sequential Execution Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '      <th>Execution Type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '      <td>Sequential</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success Execution Type\n",
              "0   01     True     Sequential"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Executing specific plans sequentially...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Define the plans to execute\n",
        "specific_plans = [\"01\", \"03\"]\n",
        "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute specific plans sequentially\n",
        "execution_results = RasCmdr.compute_test_mode(\n",
        "    plan_number=specific_plans,\n",
        "    dest_folder_suffix=\"[SpecificSequential]\",\n",
        "    num_cores=6, \n",
        "    overwrite_dest=True\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "sequential_duration = end_time - start_time\n",
        "\n",
        "print(f\"Sequential execution of specific plans completed in {sequential_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "sequential_results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Sequential\"}\n",
        "    for plan, success in execution_results.items()\n",
        "])\n",
        "\n",
        "sequential_results_df \n",
        "\n",
        "# Ensure the 'Plan' column exists before sorting\n",
        "if 'Plan' in sequential_results_df.columns:\n",
        "    sequential_results_df = sequential_results_df.sort_values(\"Plan\")\n",
        "else:\n",
        "    print(\"Warning: 'Plan' column not found in execution results.\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nSequential Execution Results:\")\n",
        "display.display(sequential_results_df)\n",
        "\n",
        "# Check the test folder\n",
        "test_folder = bald_eagle_path.parent / f\"{ras.project_name} [SpecificSequential]\"\n",
        "if test_folder.exists():\n",
        "    print(f\"\\nTest folder exists: {test_folder}\")\n",
        "    \n",
        "    # Check for results\n",
        "    hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
        "    if hdf_files:\n",
        "        print(f\"Found {len(hdf_files)} HDF result files:\")\n",
        "        for file in hdf_files:\n",
        "            file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "            print(f\"  {file.name}: {file_size:.1f} MB\")\n",
        "    else:\n",
        "        print(\"No HDF result files found in the test folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Running Only Plans Without HDF Results\n",
        "An important use case is to identify and execute only those plans that have no existing HDF results. This approach can save time by avoiding redundant computations, especially useful when adding new plans to an existing project or after making limited changes.\n",
        "\n",
        "Let's demonstrate how to:\n",
        "\n",
        "- Use the `ras` object to identify plans without results\n",
        "- Create a filtered list of these plans\n",
        "- Execute only the missing plans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:37:36.317995Z",
          "iopub.status.busy": "2025-11-17T18:37:36.317784Z",
          "iopub.status.idle": "2025-11-17T18:39:14.693164Z",
          "shell.execute_reply": "2025-11-17T18:39:14.692565Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Starting the compute_test_mode...\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Creating the test folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]...\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Copied project folder to compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\n",
            "2025-12-02 16:59:48 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.rasmap\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Initialized RAS project in compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.prj\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Getting plan entries...\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Retrieved plan entries successfully.\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '02']\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Running selected plans sequentially...\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\n",
            "2025-12-02 16:59:48 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p01\n",
            "2025-12-02 16:59:48 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p01\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Set number of cores to 6 for plan: 01\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 16:59:48 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p01\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identifying and executing plans without HDF results...\n",
            "Found 2 plans without HDF results: 01, 02\n",
            "\n",
            "Executing 2 plans without results...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 89.20 seconds\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Successfully computed plan 01\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 89.24 seconds\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\n",
            "2025-12-02 17:01:18 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p02\n",
            "2025-12-02 17:01:18 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p02\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Set number of cores to 6 for plan: 02\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:01:18 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p02\"\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 02\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 4.00 seconds\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - Successfully computed plan 02\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 4.03 seconds\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - All selected plans have been executed.\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - compute_test_mode completed.\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n",
            "2025-12-02 17:01:22 - ras_commander.RasCmdr - INFO - Plan 02: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution completed in 93.34 seconds\n",
            "\n",
            "Execution Results for Plans Without HDF Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '      <th>Execution Type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '      <td>Missing Plans</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>True</td>\\n', '      <td>Missing Plans</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success Execution Type\n",
              "0   01     True  Missing Plans\n",
              "1   02     True  Missing Plans"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Identifying and executing plans without HDF results...\")\n",
        "\n",
        "# Use the ras object to determine which plans don't have results\n",
        "plans_no_results = ras.plan_df[ras.plan_df['HDF_Results_Path'].isna()]['plan_number'].tolist()\n",
        "\n",
        "if not plans_no_results:\n",
        "    print(\"All plans already have HDF results. Creating a test scenario...\")\n",
        "    # For demonstration purposes, pretend some plans don't have results\n",
        "    plans_no_results = [\"04\", \"05\"]\n",
        "    print(f\"Simulating no results for plans: {', '.join(plans_no_results)}\")\n",
        "else:\n",
        "    print(f\"Found {len(plans_no_results)} plans without HDF results: {', '.join(plans_no_results)}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute only the plans without results\n",
        "if plans_no_results:\n",
        "    print(f\"\\nExecuting {len(plans_no_results)} plans without results...\")\n",
        "    execution_results = RasCmdr.compute_test_mode(\n",
        "        plan_number=plans_no_results,\n",
        "        dest_folder_suffix=\"[MissingPlans]\",\n",
        "        num_cores=6, \n",
        "        overwrite_dest=True\n",
        "    )\n",
        "    \n",
        "    # Record end time and calculate duration\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(f\"Execution completed in {duration:.2f} seconds\")\n",
        "    \n",
        "    # Create a DataFrame from the execution results\n",
        "    missing_results_df = pd.DataFrame([\n",
        "        {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Missing Plans\"}\n",
        "        for plan, success in execution_results.items()\n",
        "    ])\n",
        "    \n",
        "    # Sort by plan number\n",
        "    missing_results_df = missing_results_df.sort_values(\"Plan\")\n",
        "    \n",
        "    # Display the results\n",
        "    print(\"\\nExecution Results for Plans Without HDF Results:\")\n",
        "    display.display(missing_results_df)\n",
        "    \n",
        "    # Check the test folder\n",
        "    test_folder = bald_eagle_path.parent / f\"{ras.project_name} [MissingPlans]\"\n",
        "    if test_folder.exists():\n",
        "        print(f\"\\nTest folder exists: {test_folder}\")\n",
        "        \n",
        "        # Check for results\n",
        "        hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
        "        if hdf_files:\n",
        "            print(f\"Found {len(hdf_files)} HDF result files:\")\n",
        "            for file in hdf_files:\n",
        "                file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "                print(f\"  {file.name}: {file_size:.1f} MB\")\n",
        "        else:\n",
        "            print(\"No HDF result files found in the test folder\")\n",
        "else:\n",
        "    print(\"No plans without results to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification of Results\n",
        "After executing the plans that were missing HDF results, it's important to verify that the results were properly generated. Let's check if the execution actually created the expected output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:14.695684Z",
          "iopub.status.busy": "2025-11-17T18:39:14.695405Z",
          "iopub.status.idle": "2025-11-17T18:39:14.700048Z",
          "shell.execute_reply": "2025-11-17T18:39:14.699421Z"
        }
      },
      "outputs": [],
      "source": [
        "# Re-initialize the project with the test folder to see updated results\n",
        "missing_plans_folder = bald_eagle_path.parent / f\"{ras.project_name} [MissingPlans]\"\n",
        "\n",
        "if missing_plans_folder.exists():\n",
        "    # Initialize the project from the test folder\n",
        "    test_ras = RasPrj()\n",
        "    init_ras_project(missing_plans_folder, \"6.6\", ras_object=test_ras)\n",
        "    \n",
        "    # Check which plans now have results\n",
        "    plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]['plan_number'].tolist()\n",
        "    \n",
        "    print(f\"Plans with results after execution: {', '.join(plans_with_results)}\")\n",
        "    \n",
        "    # Verify if all previously missing plans now have results\n",
        "    all_generated = all(plan in plans_with_results for plan in plans_no_results)\n",
        "    \n",
        "    if all_generated:\n",
        "        print(\"\u2705 Successfully generated results for all missing plans\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f Some plans still don't have results after execution\")\n",
        "        missing_after = [plan for plan in plans_no_results if plan not in plans_with_results]\n",
        "        print(f\"Plans still missing results: {', '.join(missing_after)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan Specification Techniques\n",
        "\n",
        "In this notebook, we've explored different ways to specify and execute HEC-RAS plans using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
        "\n",
        "1. **Basic Plan Specification**\n",
        "   - Single plan by number: `\"01\"`\n",
        "   - List of specific plans: `[\"01\", \"03\"]`\n",
        "   - All plans: `ras.plan_df['plan_number'].tolist()`\n",
        "\n",
        "2. **Advanced Selection**\n",
        "   - Categorization: Grouping plans by purpose or type\n",
        "   - Dependencies: Ensuring prerequisite plans are run first\n",
        "   - Ordered execution: Running plans in a specific sequence\n",
        "\n",
        "3. **Run Plans with Missing Results (HDF)**\n",
        "   - Using ras object to determine which plans have results\n",
        "   - Creating a list of plans with no results\n",
        "   - Running those plans sequentially\n",
        "\n",
        "4. NOTE: run_parallel can also run a list of plans, but compute_plan is only made for single plan execution.  \n",
        "\n",
        "\n",
        "### Best Practices for Plan Specification\n",
        "\n",
        "1. **Consistent Formatting**: Use two-digit strings for plan numbers (\"01\" instead of 1)\n",
        "2. **Descriptive Naming**: Use meaningful short identifiers that describe the plan's purpose\n",
        "3. **Verify Availability**: Check that specified plans exist before trying to execute them\n",
        "4. **Document Dependencies**: Keep track of which plans depend on others\n",
        "5. **Use Appropriate Execution Method**: Choose sequential or parallel based on dependencies and resources\n",
        "6. **Monitor Performance**: Track execution times to identify optimization opportunities\n",
        "\n",
        "By applying these techniques, you can create efficient and organized workflows for executing HEC-RAS plans, from simple batch processing to complex dependency-based execution sequences."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\07_sequential_plan_execution.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:33.176440Z",
          "iopub.status.busy": "2025-11-17T18:39:33.175954Z",
          "iopub.status.idle": "2025-11-17T18:39:34.600184Z",
          "shell.execute_reply": "2025-11-17T18:39:34.599725Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:33 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:34.602404Z",
          "iopub.status.busy": "2025-11-17T18:39:34.602070Z",
          "iopub.status.idle": "2025-11-17T18:39:34.605282Z",
          "shell.execute_reply": "2025-11-17T18:39:34.604752Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:34.607552Z",
          "iopub.status.busy": "2025-11-17T18:39:34.607279Z",
          "iopub.status.idle": "2025-11-17T18:39:34.656894Z",
          "shell.execute_reply": "2025-11-17T18:39:34.656291Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Found zip file: C:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted project to: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "Bald Eagle Creek project exists: True\n"
          ]
        }
      ],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:34.659440Z",
          "iopub.status.busy": "2025-11-17T18:39:34.659042Z",
          "iopub.status.idle": "2025-11-17T18:39:34.671672Z",
          "shell.execute_reply": "2025-11-17T18:39:34.671102Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples directory set to: C:\\GH\\ras-commander\\examples\\example_projects\n",
            "Removing existing test folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\n"
          ]
        }
      ],
      "source": [
        "# define examples_dir as parent of bald_eagle_path\n",
        "examples_dir = bald_eagle_path.parent\n",
        "print(f\"Examples directory set to: {examples_dir}\")\n",
        "\n",
        "    \n",
        "# Remove any compute test folders from previous runs\n",
        "for folder in examples_dir.glob(\"*[[]AllSequential[]]*\"):\n",
        "    if folder.is_dir():\n",
        "        print(f\"Removing existing test folder: {folder}\")\n",
        "        shutil.rmtree(folder)\n",
        "        \n",
        "for folder in examples_dir.glob(\"*[[]SpecificSequential*[]]*\"):\n",
        "    if folder.is_dir():\n",
        "        print(f\"Removing existing test folder: {folder}\")\n",
        "        shutil.rmtree(folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Sequential Execution in HEC-RAS\n",
        "\n",
        "HEC-RAS simulations can be executed in several ways:\n",
        "\n",
        "1. **Single Plan Execution**: Run one plan at a time using `RasCmdr.compute_plan()`\n",
        "2. **Sequential Execution**: Run multiple plans one after another using `RasCmdr.compute_test_mode()`\n",
        "3. **Parallel Execution**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
        "\n",
        "This notebook focuses on the second approach: **Sequential Execution**. Here are the key benefits of sequential execution:\n",
        "\n",
        "- **Controlled Resource Usage**: By running plans one at a time, you ensure consistent resource usage\n",
        "- **Dependency Management**: When later plans depend on results from earlier plans\n",
        "- **Simplified Debugging**: Easier to identify which plan is causing an issue when they run sequentially\n",
        "- **Consistent Test Environment**: All plans run in the same isolated folder\n",
        "\n",
        "The `compute_test_mode()` function from `RasCmdr` is specifically designed for this purpose. It creates a separate test folder, copies the project there, and executes the specified plans in sequential order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Project\n",
        "\n",
        "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:34.674301Z",
          "iopub.status.busy": "2025-11-17T18:39:34.673970Z",
          "iopub.status.idle": "2025-11-17T18:39:34.725955Z",
          "shell.execute_reply": "2025-11-17T18:39:34.725300Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized HEC-RAS project: BaldEagle\n",
            "\n",
            "HEC-RAS Project Plan Data:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "0                 2MIN            1HOUR        1  ...     None      dss   \n",
              "1                 2MIN              NaN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method HDF_Results_Path Geom File  \\\n",
              "0                     2             None        01   \n",
              "1                     1             None        01   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...    Steady  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 plans in the project\n"
          ]
        }
      ],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nHEC-RAS Project Plan Data:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Check how many plans we have\n",
        "plan_count = len(ras.plan_df)\n",
        "print(f\"Found {plan_count} plans in the project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasCmdr.compute_test_mode Method\n",
        "\n",
        "Before we start executing plans, let's understand the `compute_test_mode()` method from the `RasCmdr` class, which we'll use for sequential execution.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (str, list[str], optional): Plan number or list of plan numbers to execute. If None, all plans will be executed.\n",
        "- `dest_folder_suffix` (str, optional): Suffix to append to the test folder name. Defaults to \"[Test]\".\n",
        "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.\n",
        "- `num_cores` (int, optional): Maximum number of cores to use for each plan. If None, the current setting is not changed.\n",
        "- `ras_object` (RasPrj, optional): Specific RAS object to use. If None, uses the global ras object.\n",
        "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists. Defaults to False.\n",
        "\n",
        "### Return Value\n",
        "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Test Folder**: The function creates a separate folder with the specified suffix, copying the project there for execution.\n",
        "2. **Sequential Execution**: Plans are executed one after another in the specified order.\n",
        "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties.\n",
        "4. **Destination Folder Option**: The suffix determines the name of the test folder. Unlike `compute_plan()`, you can't specify an arbitrary destination folder.\n",
        "5. **Overwrite Option**: Controls whether an existing test folder should be overwritten.\n",
        "\n",
        "Now, let's see how this works in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sequential Execution of All Plans\n",
        "\n",
        "Let's execute all plans in the project sequentially. This will create a test folder with the suffix \"[AllSequential]\" and run all plans one after another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:39:34.728602Z",
          "iopub.status.busy": "2025-11-17T18:39:34.728260Z",
          "iopub.status.idle": "2025-11-17T18:41:12.830417Z",
          "shell.execute_reply": "2025-11-17T18:41:12.829932Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Starting the compute_test_mode...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Creating the test folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Copied project folder to compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Initialized RAS project in compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.prj\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Getting plan entries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Retrieved plan entries successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Running selected plans sequentially...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:39:34 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.p01\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing all plans sequentially...\n",
            "This may take several minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 93.85 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - Successfully computed plan 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 93.86 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:08 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.p02\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 4.17 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Successfully computed plan 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 4.18 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - All selected plans have been executed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - compute_test_mode completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Plan 02: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential execution of all plans completed in 98.09 seconds\n",
            "\n",
            "Execution Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>True</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success\n",
              "0   01     True\n",
              "1   02     True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Executing all plans sequentially...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute all plans sequentially\n",
        "# - dest_folder_suffix: Suffix to append to the test folder name\n",
        "# - overwrite_dest: Overwrite the destination folder if it exists\n",
        "# - no ras object is specified, it will use the default \"ras\" object\n",
        "execution_results = RasCmdr.compute_test_mode(\n",
        "    dest_folder_suffix=\"[AllSequential]\",\n",
        "    overwrite_dest=True\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Sequential execution of all plans completed in {total_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in execution_results.items()\n",
        "])\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examining the Test Folder\n",
        "\n",
        "Let's examine the test folder created by `compute_test_mode()` to better understand what happened during sequential execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:12.832608Z",
          "iopub.status.busy": "2025-11-17T18:41:12.832265Z",
          "iopub.status.idle": "2025-11-17T18:41:12.836131Z",
          "shell.execute_reply": "2025-11-17T18:41:12.835634Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": "[\"WindowsPath('C:/GH/ras-commander/examples/example_projects/Balde Eagle Creek [AllSequential]')\"]"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the test folder path\n",
        "test_folder = bald_eagle_path.parent / f\"Balde Eagle Creek [AllSequential]\"\n",
        "test_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:12.839999Z",
          "iopub.status.busy": "2025-11-17T18:41:12.839772Z",
          "iopub.status.idle": "2025-11-17T18:41:12.870193Z",
          "shell.execute_reply": "2025-11-17T18:41:12.869668Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test folder exists: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential]\n",
            "\n",
            "Key files in test folder:\n",
            "Project file: BaldEagle.prj\n",
            "Plan files:\n",
            "  BaldEagle.p01: 15.6 KB\n",
            "  BaldEagle.p01.comp_msgs.txt: 1.2 KB\n",
            "  BaldEagle.p01.hdf: 7592.7 KB\n",
            "  BaldEagle.p02: 12.5 KB\n",
            "  BaldEagle.p02.hdf: 4368.0 KB\n",
            "  BaldEagle.prj: 0.9 KB\n",
            "\n",
            "HDF files:\n",
            "  BaldEagle.g01.hdf: 3.8 MB\n",
            "  BaldEagle.p01.hdf: 7.4 MB\n",
            "  BaldEagle.p02.hdf: 4.3 MB\n",
            "  BaldEagle.u02.hdf: 0.0 MB\n",
            "\n",
            "Geometry preprocessor files:\n",
            "  BaldEagle.c01: 522.1 KB\n",
            "  BaldEagle.p01.comp_msgs.txt: 1.2 KB\n",
            "\n",
            "Plans with results in the test folder:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number                                   HDF_Results_Path\n",
              "0          01  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "1          02  C:\\GH\\ras-commander\\examples\\example_projects\\..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create test folder if it doesn't exist using pathlib\n",
        "if not test_folder.exists():\n",
        "    test_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "if test_folder.exists():\n",
        "    print(f\"Test folder exists: {test_folder}\")\n",
        "    \n",
        "    # List the key files in the test folder\n",
        "    print(\"\\nKey files in test folder:\")\n",
        "    \n",
        "    # First, list the project file and all plan files\n",
        "    prj_files = list(test_folder.glob(\"*.prj\"))\n",
        "    plan_files = list(test_folder.glob(\"*.p*\"))\n",
        "    plan_files.sort()\n",
        "    \n",
        "    if prj_files:\n",
        "        print(f\"Project file: {prj_files[0].name}\")\n",
        "    \n",
        "    print(\"Plan files:\")\n",
        "    for plan_file in plan_files:\n",
        "        file_size = plan_file.stat().st_size / 1024  # Size in KB\n",
        "        print(f\"  {plan_file.name}: {file_size:.1f} KB\")\n",
        "    \n",
        "    # Look for HDF result files\n",
        "    hdf_files = list(test_folder.glob(\"*.hdf\"))\n",
        "    hdf_files.sort()\n",
        "    \n",
        "    print(\"\\nHDF files:\")\n",
        "    for hdf_file in hdf_files:\n",
        "        file_size = hdf_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "        print(f\"  {hdf_file.name}: {file_size:.1f} MB\")\n",
        "    \n",
        "    # Geometry preprocessor files (if any)\n",
        "    geompre_files = list(test_folder.glob(\"*.c*\"))\n",
        "    geompre_files.sort()\n",
        "    \n",
        "    if geompre_files:\n",
        "        print(\"\\nGeometry preprocessor files:\")\n",
        "        for geompre_file in geompre_files:\n",
        "            file_size = geompre_file.stat().st_size / 1024  # Size in KB\n",
        "            print(f\"  {geompre_file.name}: {file_size:.1f} KB\")\n",
        "    else:\n",
        "        print(\"\\nNo geometry preprocessor files found\")\n",
        "        \n",
        "    # Initialize a RAS project in the test folder to inspect results\n",
        "    try:\n",
        "        test_ras = RasPrj()\n",
        "        init_ras_project(test_folder, ras.ras_exe_path, ras_object=test_ras)\n",
        "        print(\"\\nPlans with results in the test folder:\")\n",
        "        test_plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]\n",
        "        display.display(test_plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing test folder as a RAS project: {e}\")\n",
        "else:\n",
        "    print(f\"Test folder not found: {test_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Sequential Execution of Specific Plans\n",
        "\n",
        "Now, let's execute only specific plans in the project. We'll select plans \"01\" and \"02\" and run them sequentially with the `clear_geompre` option set to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:12.873689Z",
          "iopub.status.busy": "2025-11-17T18:41:12.873458Z",
          "iopub.status.idle": "2025-11-17T18:41:32.146605Z",
          "shell.execute_reply": "2025-11-17T18:41:32.146118Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Starting the compute_test_mode...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Creating the test folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Copied project folder to compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Initialized RAS project in compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.prj\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Getting plan entries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Retrieved plan entries successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '02']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Running selected plans sequentially...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.p01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.p01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:12 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.p01\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing specific plans sequentially with clearing geometry preprocessor files...\n",
            "This may take several minutes...\n",
            "Selected plans: 01, 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 15.13 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Successfully computed plan 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 15.14 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.p02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.p02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:28 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [AllSequential] [SpecificSequentialClearGeompre]\\BaldEagle.p02\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 4.04 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - Successfully computed plan 02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 4.05 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - All selected plans have been executed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - compute_test_mode completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:32 - ras_commander.RasCmdr - INFO - Plan 02: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential execution of specific plans completed in 19.27 seconds\n",
            "\n",
            "Execution Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>True</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success\n",
              "0   01     True\n",
              "1   02     True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Executing specific plans sequentially with clearing geometry preprocessor files...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Define the plans to execute\n",
        "selected_plans = [\"01\", \"02\"]\n",
        "print(f\"Selected plans: {', '.join(selected_plans)}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute specific plans sequentially\n",
        "# - plan_number: List of plan numbers to execute\n",
        "# - dest_folder_suffix: Suffix to append to the test folder name\n",
        "# - clear_geompre: Clear geometry preprocessor files before execution\n",
        "# - overwrite_dest: Overwrite the destination folder if it exists\n",
        "execution_results = RasCmdr.compute_test_mode(\n",
        "    plan_number=selected_plans,\n",
        "    dest_folder_suffix=\"[SpecificSequentialClearGeompre]\",\n",
        "    clear_geompre=True,\n",
        "    overwrite_dest=True\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Sequential execution of specific plans completed in {total_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in execution_results.items()\n",
        "])\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Sequential Plan Execution\n",
        "\n",
        "In this notebook, we've explored how to execute HEC-RAS plans sequentially using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
        "\n",
        "1. **Project Setup and Initialization**: Setting up the environment and initializing a HEC-RAS project\n",
        "2. **Example Project Management**: Using `RasExamples` to download and extract sample projects\n",
        "3. **Basic Sequential Execution**: Using `RasCmdr.compute_test_mode()` to run all plans in a project\n",
        "4. **Test Folder Analysis**: Examining the contents and results of sequential execution\n",
        "5. **Selective Plan Execution**: Running specific plans with geometry preprocessor clearing\n",
        "\n",
        "### Key Functions Used\n",
        "\n",
        "- `init_ras_project()`: Initialize a HEC-RAS project\n",
        "- `RasExamples.extract_project()`: Extract example projects for testing\n",
        "- `RasCmdr.compute_test_mode()`: Run plans sequentially in a test folder\n",
        "- `Path.glob()`: Examine test folder contents and results\n",
        "- `RasCmdr.compute_test_mode(clear_geompre=True)`: Execute plans with preprocessor clearing\n",
        "\n",
        "### Best Practices for Sequential Execution\n",
        "\n",
        "1. **Environment Setup**: Ensure all required libraries are installed and properly imported\n",
        "2. **Project Organization**: Clean up existing test folders before new executions\n",
        "3. **Resource Management**: Monitor system resources (CPU cores, memory) for optimal performance\n",
        "4. **Test Folder Naming**: Use meaningful suffixes to distinguish different execution runs\n",
        "5. **Performance Tracking**: Monitor execution times for each sequential run\n",
        "6. **Results Verification**: Check test folders for successful plan execution and result files\n",
        "7. **Selective Execution**: Use plan filtering when only specific plans need to be run\n",
        "\n",
        "With these techniques, you can effectively manage and execute HEC-RAS simulations sequentially, whether running all plans or a selected subset with specific configurations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\08_parallel_execution.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:49.665313Z",
          "iopub.status.busy": "2025-11-17T18:41:49.665137Z",
          "iopub.status.idle": "2025-11-17T18:41:50.945060Z",
          "shell.execute_reply": "2025-11-17T18:41:50.944588Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 13:41:49 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:50.947436Z",
          "iopub.status.busy": "2025-11-17T18:41:50.947175Z",
          "iopub.status.idle": "2025-11-17T18:41:50.950612Z",
          "shell.execute_reply": "2025-11-17T18:41:50.950086Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil\n",
        "import math  # Import math to avoid NameError in get_optimal_worker_count function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Working Environment\n",
        "\n",
        "Let's set up our working directory and check the system resources available for parallel execution. This will help us make informed decisions about how many workers to use.\n",
        "\n",
        "For this notebook we will be using the \"Muncie\" HEC Example Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:50.952916Z",
          "iopub.status.busy": "2025-11-17T18:41:50.952727Z",
          "iopub.status.idle": "2025-11-17T18:41:51.223399Z",
          "shell.execute_reply": "2025-11-17T18:41:51.222889Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Extracting project 'Muncie'\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Project 'Muncie' already exists. Deleting existing folder...\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Existing folder for project 'Muncie' has been deleted.\n",
            "2025-12-02 17:13:09 - ras_commander.RasExamples - INFO - Successfully extracted project 'Muncie' to c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted project to: c:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n",
            "Bald Eagle Creek project exists: True\n",
            "System Resources:\n",
            "- 8 physical CPU cores (8 logical cores with hyper-threading)\n",
            "- 31.9 GB total memory (13.1 GB available)\n",
            "\n",
            "For parallel HEC-RAS execution:\n",
            "- With 2 cores per worker: Can use up to 4 parallel workers\n",
            "- With 4 cores per worker: Can use up to 2 parallel workers\n",
            "\n",
            "Each HEC-RAS instance typically requires 2-4 GB of RAM. Based on your available memory,\n",
            "you could reasonably run 4 instances simultaneously.\n"
          ]
        }
      ],
      "source": [
        "# Extract the Muncie example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
        "print(f\"Extracted project to: {muncie_path}\")  \n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {muncie_path.exists()}\")\n",
        "\n",
        "\n",
        "# Create compute folders\n",
        "compute_folder = muncie_path.parent / \"compute_test_parallel\"\n",
        "specific_compute_folder = muncie_path.parent / \"compute_test_parallel_specific\"\n",
        "dynamic_compute_folder = muncie_path.parent / \"compute_test_parallel_dynamic\"\n",
        "\n",
        "# Check system resources for parallel execution\n",
        "cpu_count = psutil.cpu_count(logical=True)  # Logical cores (including hyper-threading)\n",
        "physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
        "memory_gb = psutil.virtual_memory().total / (1024**3)  # Total RAM in GB\n",
        "available_memory_gb = psutil.virtual_memory().available / (1024**3)  # Available RAM in GB\n",
        "\n",
        "print(f\"System Resources:\")\n",
        "print(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores with hyper-threading)\")\n",
        "print(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
        "\n",
        "# Functions to help with resource management\n",
        "def get_optimal_worker_count(cores_per_worker=2):\n",
        "    \"\"\"Calculate the optimal number of workers based on available physical cores.\"\"\"\n",
        "    optimal_workers = math.floor(physical_cores / cores_per_worker)\n",
        "    return max(1, optimal_workers)  # Ensure at least 1 worker\n",
        "\n",
        "print(f\"\\nFor parallel HEC-RAS execution:\")\n",
        "print(f\"- With 2 cores per worker: Can use up to {get_optimal_worker_count(2)} parallel workers\")\n",
        "print(f\"- With 4 cores per worker: Can use up to {get_optimal_worker_count(4)} parallel workers\")\n",
        "print(f\"\\nEach HEC-RAS instance typically requires 2-4 GB of RAM. Based on your available memory,\")\n",
        "print(f\"you could reasonably run {math.floor(available_memory_gb / 3)} instances simultaneously.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Parallel Execution in HEC-RAS\n",
        "\n",
        "HEC-RAS simulations can be computationally intensive, especially for large models or long simulation periods. Parallel execution allows you to run multiple plans simultaneously, making better use of your computer's processing power.\n",
        "\n",
        "### Key Concepts in Parallel Execution\n",
        "\n",
        "1. **Workers**: Each worker is a separate process that can execute a HEC-RAS plan. The `max_workers` parameter determines how many plans can be executed simultaneously.\n",
        "\n",
        "2. **Cores per Worker**: Each worker (HEC-RAS instance) can utilize multiple CPU cores. The `num_cores` parameter sets how many cores each worker uses.\n",
        "\n",
        "3. **Resource Balancing**: Effective parallel execution requires balancing the number of workers with the cores per worker. Too many workers or too many cores per worker can lead to resource contention and slower overall performance.\n",
        "\n",
        "4. **Worker Folders**: Each worker gets its own folder with a copy of the project, allowing for isolated execution.\n",
        "\n",
        "### Parallel vs. Sequential Execution\n",
        "\n",
        "- **Parallel**: Multiple plans run simultaneously (good for independent plans, faster overall completion)\n",
        "- **Sequential**: Plans run one after another (good for dependent plans, consistent resource usage)\n",
        "\n",
        "### Optimal Configuration\n",
        "\n",
        "The optimal configuration depends on your hardware and the specific plans you're running:\n",
        "\n",
        "- For most models, 2-4 cores per worker provides good performance\n",
        "- Set `max_workers` based on available physical cores: `max_workers = floor(physical_cores / cores_per_worker)`\n",
        "- Ensure you have enough memory: each worker typically needs 2-4 GB of RAM\n",
        "\n",
        "Now, let's download and extract our example project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Project\n",
        "\n",
        "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function. We'll store the initialized object in a variable to use later, rather than relying on the global `ras` object. This approach is more suitable for working with multiple projects or compute folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:51.225789Z",
          "iopub.status.busy": "2025-11-17T18:41:51.225546Z",
          "iopub.status.idle": "2025-11-17T18:41:51.287210Z",
          "shell.execute_reply": "2025-11-17T18:41:51.286430Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized source project: Muncie\n",
            "\n",
            "Available plans in the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady Multi  9-SA run</td>\\n', '      <td>5.00</td>\\n', '      <td>9-SAs</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>15SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady Run with 2D 50ft Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>2D 50ft Grid</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>-1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>01</td>\\n', '      <td>04</td>\\n', '      <td>Unsteady Run with 2D 50ft User n Value R</td>\\n', '      <td>5.10</td>\\n', '      <td>50ft User n Regions</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>04</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number  \\\n",
              "0          01              01              01   \n",
              "1          03              01              02   \n",
              "2          04              01              04   \n",
              "\n",
              "                                 Plan Title Program Version  \\\n",
              "0                  Unsteady Multi  9-SA run            5.00   \n",
              "1            Unsteady Run with 2D 50ft Grid            5.10   \n",
              "2  Unsteady Run with 2D 50ft User n Value R            5.10   \n",
              "\n",
              "      Short Identifier                Simulation Date Computation Interval  \\\n",
              "0                9-SAs  02JAN1900,0000,02JAN1900,2400                15SEC   \n",
              "1         2D 50ft Grid  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "2  50ft User n Regions  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "\n",
              "  Mapping Interval Run HTab  ... Friction Slope Method UNET D2 SolverType  \\\n",
              "0             5MIN        1  ...                     1                NaN   \n",
              "1             5MIN       -1  ...                     1   Pardiso (Direct)   \n",
              "2             5MIN        1  ...                     1   Pardiso (Direct)   \n",
              "\n",
              "       UNET D2 Name HDF_Results_Path Geom File  \\\n",
              "0               NaN             None        01   \n",
              "1  2D Interior Area             None        02   \n",
              "2  2D Interior Area             None        04   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "\n",
              "[3 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 plans in the project\n"
          ]
        }
      ],
      "source": [
        "# Initialize the source project\n",
        "source_project = init_ras_project(muncie_path, \"6.6\")\n",
        "print(f\"Initialized source project: {source_project.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nAvailable plans in the project:\")\n",
        "display.display(source_project.plan_df)\n",
        "\n",
        "# Check how many plans we have\n",
        "plan_count = len(source_project.plan_df)\n",
        "print(f\"Found {plan_count} plans in the project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasCmdr.compute_parallel Method\n",
        "\n",
        "Before we start executing plans in parallel, let's understand the `compute_parallel()` method from the `RasCmdr` class.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (Union[str, List[str], None]): Plan number(s) to compute. If None, all plans are computed.\n",
        "- `max_workers` (int): Maximum number of parallel workers (default: 2).\n",
        "- `num_cores` (int): Number of cores to use per plan computation (default: 2).\n",
        "- `clear_geompre` (bool): Whether to clear geometry preprocessor files (default: False).\n",
        "- `ras_object` (Optional[RasPrj]): Specific RAS object to use. If None, uses global ras instance.\n",
        "- `dest_folder` (Union[str, Path, None]): Destination folder for computed results.\n",
        "- `overwrite_dest` (bool): Whether to overwrite existing destination folder (default: False).\n",
        "\n",
        "### Return Value\n",
        "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Worker Assignment**: Plans are assigned to workers in a round-robin fashion. For example, with 3 workers and 5 plans, workers would be assigned as follows: Worker 1: Plans 1 & 4, Worker 2: Plans 2 & 5, Worker 3: Plan 3.\n",
        "\n",
        "2. **Worker Folders**: Each worker gets its own folder (a subdirectory of the destination folder) for isolated execution.\n",
        "\n",
        "3. **Result Consolidation**: After all plans are executed, results are consolidated into the destination folder.\n",
        "\n",
        "4. **Resource Management**: Each worker can use multiple cores as specified by `num_cores`.\n",
        "\n",
        "Now, let's see how this works in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Parallel Execution of All Plans\n",
        "\n",
        "Let's execute all plans in the project in parallel. We'll use 3 workers, with 2 cores per worker. This approach is good when you have multiple plans that are independent of each other and you want to complete them as quickly as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:41:51.289557Z",
          "iopub.status.busy": "2025-11-17T18:41:51.289374Z",
          "iopub.status.idle": "2025-11-17T18:43:03.218068Z",
          "shell.execute_reply": "2025-11-17T18:43:03.217474Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Destination folder 'c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel' exists. Overwriting as per overwrite_dest=True.\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Copied project folder to destination: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Adjusted max_workers to 3 based on the number of plans: 3\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing all plans in parallel...\n",
            "This may take several minutes...\n",
            "Using 4 parallel workers, each with 1 cores\n",
            "Destination folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\\Muncie.rasmap\n",
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\\Muncie.rasmap\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\n",
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\\Muncie.rasmap\n",
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\\Muncie.rasmap\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\n",
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\\Muncie.rasmap\n",
            "2025-12-02 17:13:09 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\\Muncie.rasmap\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\n",
            "2025-12-02 17:13:09 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\n",
            "2025-12-02 17:13:09 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\\Muncie.p01\n",
            "2025-12-02 17:13:09 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\\Muncie.p04\n",
            "2025-12-02 17:13:09 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\\Muncie.p03\n",
            "2025-12-02 17:13:09 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\\Muncie.p01\n",
            "2025-12-02 17:13:09 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\\Muncie.p04\n",
            "2025-12-02 17:13:09 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\\Muncie.p03\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Set number of cores to 1 for plan: 04\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Set number of cores to 1 for plan: 01\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 3]\\Muncie.p04\"\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Set number of cores to 1 for plan: 03\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 1]\\Muncie.p01\"\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:13:10 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel [Worker 2]\\Muncie.p03\"\n",
            "2025-12-02 17:13:24 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 17:13:24 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 14.84 seconds\n",
            "2025-12-02 17:13:24 - ras_commander.RasCmdr - INFO - Plan 01 executed in worker 1: Successful\n",
            "2025-12-02 17:14:00 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-12-02 17:14:00 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 50.26 seconds\n",
            "2025-12-02 17:14:00 - ras_commander.RasCmdr - INFO - Plan 03 executed in worker 2: Successful\n",
            "2025-12-02 17:14:01 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 04\n",
            "2025-12-02 17:14:01 - ras_commander.RasCmdr - INFO - Total run time for plan 04: 51.37 seconds\n",
            "2025-12-02 17:14:01 - ras_commander.RasCmdr - INFO - Plan 04 executed in worker 3: Successful\n",
            "2025-12-02 17:14:01 - ras_commander.RasCmdr - INFO - Final destination for computed results: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\n",
            "2025-12-02 17:14:07 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\\Muncie.rasmap\n",
            "2025-12-02 17:14:07 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\\Muncie.rasmap\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Plan 03: Successful\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Plan 04: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parallel execution of all plans completed in 58.12 seconds\n",
            "\n",
            "Execution Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>True</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>True</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success\n",
              "0   01     True\n",
              "1   03     True\n",
              "2   04     True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Executing all plans in parallel...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Create compute folder if it doesn't exist\n",
        "compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define the parameters for parallel execution\n",
        "max_workers = 4\n",
        "cores_per_worker = 1\n",
        "\n",
        "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
        "print(f\"Destination folder: {compute_folder}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute all plans in parallel\n",
        "results_all = RasCmdr.compute_parallel(\n",
        "    max_workers=max_workers,\n",
        "    num_cores=cores_per_worker,\n",
        "    dest_folder=compute_folder,\n",
        "    overwrite_dest=True,\n",
        "    ras_object=source_project\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Parallel execution of all plans completed in {total_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in results_all.items()\n",
        "])\n",
        "\n",
        "# Sort by plan number\n",
        "results_df = results_df.sort_values(\"Plan\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examining the Parallel Execution Results\n",
        "\n",
        "Let's initialize a RAS project in the compute folder and examine the results of the parallel execution. This will help us understand what happened during the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:43:03.220580Z",
          "iopub.status.busy": "2025-11-17T18:43:03.220396Z",
          "iopub.status.idle": "2025-11-17T18:43:03.260068Z",
          "shell.execute_reply": "2025-11-17T18:43:03.259635Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:14:07 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\\Muncie.rasmap\n",
            "2025-12-02 17:14:07 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized compute project: Muncie\n",
            "\n",
            "Plans in the compute folder:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady Multi  9-SA run</td>\\n', '      <td>5.00</td>\\n', '      <td>9-SAs</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>15SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady Run with 2D 50ft Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>2D 50ft Grid</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>-1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>01</td>\\n', '      <td>04</td>\\n', '      <td>Unsteady Run with 2D 50ft User n Value R</td>\\n', '      <td>5.10</td>\\n', '      <td>50ft User n Regions</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>04</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number  \\\n",
              "0          01              01              01   \n",
              "1          03              01              02   \n",
              "2          04              01              04   \n",
              "\n",
              "                                 Plan Title Program Version  \\\n",
              "0                  Unsteady Multi  9-SA run            5.00   \n",
              "1            Unsteady Run with 2D 50ft Grid            5.10   \n",
              "2  Unsteady Run with 2D 50ft User n Value R            5.10   \n",
              "\n",
              "      Short Identifier                Simulation Date Computation Interval  \\\n",
              "0                9-SAs  02JAN1900,0000,02JAN1900,2400                15SEC   \n",
              "1         2D 50ft Grid  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "2  50ft User n Regions  02JAN1900,0000,02JAN1900,2400                10SEC   \n",
              "\n",
              "  Mapping Interval Run HTab  ... Friction Slope Method UNET D2 SolverType  \\\n",
              "0             5MIN        1  ...                     1                NaN   \n",
              "1             5MIN       -1  ...                     1   Pardiso (Direct)   \n",
              "2             5MIN        1  ...                     1   Pardiso (Direct)   \n",
              "\n",
              "       UNET D2 Name                                   HDF_Results_Path  \\\n",
              "0               NaN  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  2D Interior Area  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "2  2D Interior Area  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "  Geom File                                          Geom Path  Flow File  \\\n",
              "0        01  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "1        02  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "2        04  C:\\GH\\ras-commander\\examples\\example_projects\\...         01   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "\n",
              "[3 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found 3 plans with results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number                                   HDF_Results_Path\n",
              "0          01  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "1          03  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "2          04  C:\\GH\\ras-commander\\examples\\example_projects\\..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "No worker folders remain in the compute folder (they were removed during results consolidation)\n",
            "\n",
            "Found 7 HDF files in the compute folder:\n",
            "  Muncie.g01.hdf: 0.2 MB\n",
            "  Muncie.g02.hdf: 0.5 MB\n",
            "  Muncie.g04.hdf: 3.0 MB\n",
            "  Muncie.p01.hdf: 3.8 MB\n",
            "  Muncie.p03.hdf: 15.3 MB\n",
            "  Muncie.p04.hdf: 17.9 MB\n",
            "  Muncie.u01.hdf: 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "# Initialize a RAS project in the compute folder\n",
        "compute_project = RasPrj()\n",
        "init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "print(f\"Initialized compute project: {compute_project.project_name}\")\n",
        "\n",
        "# Display the plan files in the compute folder\n",
        "print(\"\\nPlans in the compute folder:\")\n",
        "display.display(compute_project.plan_df)\n",
        "\n",
        "# Check which plans have results\n",
        "plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
        "display.display(plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
        "\n",
        "# List the worker folders (they should have been removed during results consolidation)\n",
        "worker_folders = list(compute_folder.glob(\"*Worker*\"))\n",
        "if worker_folders:\n",
        "    print(f\"\\nFound {len(worker_folders)} worker folders:\")\n",
        "    for folder in worker_folders:\n",
        "        print(f\"  {folder.name}\")\n",
        "else:\n",
        "    print(\"\\nNo worker folders remain in the compute folder (they were removed during results consolidation)\")\n",
        "\n",
        "# Check for HDF result files\n",
        "hdf_files = list(compute_folder.glob(\"*.hdf\"))\n",
        "hdf_files.sort()\n",
        "\n",
        "print(f\"\\nFound {len(hdf_files)} HDF files in the compute folder:\")\n",
        "for file in hdf_files:\n",
        "    file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "    print(f\"  {file.name}: {file_size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Additional Examples: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Execution of Specific Plans\n",
        "\n",
        "Now, let's execute only specific plans in the project in parallel. This approach is useful when you only want to run a subset of the available plans, perhaps for testing or comparison purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:43:03.262815Z",
          "iopub.status.busy": "2025-11-17T18:43:03.262412Z",
          "iopub.status.idle": "2025-11-17T18:43:43.246887Z",
          "shell.execute_reply": "2025-11-17T18:43:43.246195Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Destination folder 'c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific' exists. Overwriting as per overwrite_dest=True.\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Copied project folder to destination: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '03']\n",
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Adjusted max_workers to 2 based on the number of plans: 2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing specific plans in parallel...\n",
            "This may take several minutes...\n",
            "Selected plans: 01, 03\n",
            "Using 2 parallel workers, each with 2 cores\n",
            "Destination folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:14:07 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\n",
            "2025-12-02 17:14:07 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\\Muncie.rasmap\n",
            "2025-12-02 17:14:08 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\\Muncie.rasmap\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\n",
            "2025-12-02 17:14:08 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\\Muncie.rasmap\n",
            "2025-12-02 17:14:08 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\\Muncie.rasmap\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\n",
            "2025-12-02 17:14:08 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\\Muncie.p01\n",
            "2025-12-02 17:14:08 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\\Muncie.p03\n",
            "2025-12-02 17:14:08 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\\Muncie.p01\n",
            "2025-12-02 17:14:08 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\\Muncie.p03\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Set number of cores to 2 for plan: 03\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 2]\\Muncie.p03\"\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Set number of cores to 2 for plan: 01\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:14:08 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific [Worker 1]\\Muncie.p01\"\n",
            "2025-12-02 17:14:22 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 17:14:22 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 14.04 seconds\n",
            "2025-12-02 17:14:22 - ras_commander.RasCmdr - INFO - Plan 01 executed in worker 1: Successful\n",
            "2025-12-02 17:14:37 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-12-02 17:14:37 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 29.45 seconds\n",
            "2025-12-02 17:14:37 - ras_commander.RasCmdr - INFO - Plan 03 executed in worker 2: Successful\n",
            "2025-12-02 17:14:37 - ras_commander.RasCmdr - INFO - Final destination for computed results: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\n",
            "2025-12-02 17:14:41 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\\Muncie.rasmap\n",
            "2025-12-02 17:14:41 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\\Muncie.rasmap\n",
            "2025-12-02 17:14:41 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n",
            "2025-12-02 17:14:41 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n",
            "2025-12-02 17:14:41 - ras_commander.RasCmdr - INFO - Plan 03: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parallel execution of specific plans completed in 34.17 seconds\n",
            "\n",
            "Execution Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>True</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success\n",
              "0   01     True\n",
              "1   03     True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:14:41 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\\Muncie.rasmap\n",
            "2025-12-02 17:14:41 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_specific\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized specific compute project: Muncie\n",
            "Found 3 plans with results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number                                   HDF_Results_Path\n",
              "0          01  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "1          03  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "2          04  C:\\GH\\ras-commander\\examples\\example_projects\\..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Executing specific plans in parallel...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Create specific compute folder if it doesn't exist\n",
        "specific_compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define the plans to execute\n",
        "specific_plans = [\"01\", \"03\"]\n",
        "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
        "\n",
        "# Define the parameters for parallel execution\n",
        "max_workers = 2  # One for each plan\n",
        "cores_per_worker = 2\n",
        "\n",
        "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
        "print(f\"Destination folder: {specific_compute_folder}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute specific plans in parallel\n",
        "results_specific = RasCmdr.compute_parallel(\n",
        "    plan_number=specific_plans,\n",
        "    max_workers=max_workers,\n",
        "    num_cores=cores_per_worker,\n",
        "    dest_folder=specific_compute_folder,\n",
        "    overwrite_dest=True,\n",
        "    ras_object=source_project\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "specific_duration = end_time - start_time\n",
        "\n",
        "print(f\"Parallel execution of specific plans completed in {specific_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "specific_results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in results_specific.items()\n",
        "])\n",
        "\n",
        "# Sort by plan number\n",
        "specific_results_df = specific_results_df.sort_values(\"Plan\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(specific_results_df)\n",
        "\n",
        "# Initialize a RAS project in the specific compute folder\n",
        "specific_compute_project = RasPrj()\n",
        "init_ras_project(specific_compute_folder, \"6.6\", ras_object=specific_compute_project)\n",
        "print(f\"\\nInitialized specific compute project: {specific_compute_project.project_name}\")\n",
        "\n",
        "# Check which plans have results\n",
        "specific_plans_with_results = specific_compute_project.plan_df[specific_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "print(f\"Found {len(specific_plans_with_results)} plans with results:\")\n",
        "display.display(specific_plans_with_results[['plan_number', 'HDF_Results_Path']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Execution with Max Workers Defined by Physical Cores (\"Dynamic Worker Allocation\") \n",
        "\n",
        "In this step, we'll determine the optimal number of workers based on the physical cores available on the system. This approach ensures that we make efficient use of the available hardware without overcommitting resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:43:43.249907Z",
          "iopub.status.busy": "2025-11-17T18:43:43.249674Z",
          "iopub.status.idle": "2025-11-17T18:44:25.607573Z",
          "shell.execute_reply": "2025-11-17T18:44:25.606983Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:14:41 - ras_commander.RasCmdr - INFO - Destination folder 'c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic' exists. Overwriting as per overwrite_dest=True.\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Copied project folder to destination: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '03']\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Adjusted max_workers to 2 based on the number of plans: 2\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing plans with dynamic worker allocation...\n",
            "This may take several minutes...\n",
            "System has 8 physical cores\n",
            "With 4 cores per worker, optimal worker count is 2\n",
            "Destination folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:14:42 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\\Muncie.rasmap\n",
            "2025-12-02 17:14:42 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\\Muncie.rasmap\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Created worker folder: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\n",
            "2025-12-02 17:14:42 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\\Muncie.rasmap\n",
            "2025-12-02 17:14:42 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\\Muncie.rasmap\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\n",
            "2025-12-02 17:14:42 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\\Muncie.p01\n",
            "2025-12-02 17:14:42 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\\Muncie.p03\n",
            "2025-12-02 17:14:42 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\\Muncie.p01\n",
            "2025-12-02 17:14:42 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\\Muncie.p03\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Set number of cores to 4 for plan: 01\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 1]\\Muncie.p01\"\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Set number of cores to 4 for plan: 03\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 17:14:42 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic [Worker 2]\\Muncie.p03\"\n",
            "2025-12-02 17:14:56 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 17:14:56 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 14.38 seconds\n",
            "2025-12-02 17:14:56 - ras_commander.RasCmdr - INFO - Plan 01 executed in worker 1: Successful\n",
            "2025-12-02 17:15:08 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-12-02 17:15:08 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 26.43 seconds\n",
            "2025-12-02 17:15:08 - ras_commander.RasCmdr - INFO - Plan 03 executed in worker 2: Successful\n",
            "2025-12-02 17:15:08 - ras_commander.RasCmdr - INFO - Final destination for computed results: c:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\n",
            "2025-12-02 17:15:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\\Muncie.rasmap\n",
            "2025-12-02 17:15:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\\Muncie.rasmap\n",
            "2025-12-02 17:15:13 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n",
            "2025-12-02 17:15:13 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n",
            "2025-12-02 17:15:13 - ras_commander.RasCmdr - INFO - Plan 03: Successful\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parallel execution with dynamic worker allocation completed in 31.15 seconds\n",
            "\n",
            "Execution Results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan</th>\\n', '      <th>Success</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>True</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>True</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  Plan  Success\n",
              "0   01     True\n",
              "1   03     True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 17:15:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\\Muncie.rasmap\n",
            "2025-12-02 17:15:13 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\compute_test_parallel_dynamic\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized dynamic compute project: Muncie\n",
            "Found 3 plans with results:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number                                   HDF_Results_Path\n",
              "0          01  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "1          03  C:\\GH\\ras-commander\\examples\\example_projects\\...\n",
              "2          04  C:\\GH\\ras-commander\\examples\\example_projects\\..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Executing plans with dynamic worker allocation...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Create dynamic compute folder if it doesn't exist\n",
        "dynamic_compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define the cores per worker\n",
        "cores_per_worker = 4\n",
        "# 2 cores per worker is the efficiency point for most CPU's, due to L2/L3 cache being shared by 2 cores in most x86 CPU's\n",
        "# 4-8 cores per worker is the maximum performance point for most CPU's, using more compute power to marginally lower runtime \n",
        "# when using parallel compute, 2 cores per worker is typically optimal as it is assumed you are maximizing throughput (efficency) over single-plan runtime (performance)\n",
        "\n",
        "# Calculate the optimal number of workers based on physical cores\n",
        "max_workers = get_optimal_worker_count(cores_per_worker)\n",
        "print(f\"System has {physical_cores} physical cores\")\n",
        "print(f\"With {cores_per_worker} cores per worker, optimal worker count is {max_workers}\")\n",
        "print(f\"Destination folder: {dynamic_compute_folder}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute all plans with dynamic worker allocation\n",
        "results_dynamic = RasCmdr.compute_parallel(\n",
        "    plan_number=specific_plans,\n",
        "    max_workers=max_workers,\n",
        "    num_cores=cores_per_worker,\n",
        "    dest_folder=dynamic_compute_folder,\n",
        "    overwrite_dest=True,\n",
        "    ras_object=source_project\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "dynamic_duration = end_time - start_time\n",
        "\n",
        "print(f\"Parallel execution with dynamic worker allocation completed in {dynamic_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "dynamic_results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in results_dynamic.items()\n",
        "])\n",
        "\n",
        "# Sort by plan number\n",
        "dynamic_results_df = dynamic_results_df.sort_values(\"Plan\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(dynamic_results_df)\n",
        "\n",
        "# Initialize a RAS project in the dynamic compute folder\n",
        "dynamic_compute_project = RasPrj()\n",
        "init_ras_project(dynamic_compute_folder, \"6.6\", ras_object=dynamic_compute_project)\n",
        "print(f\"\\nInitialized dynamic compute project: {dynamic_compute_project.project_name}\")\n",
        "\n",
        "# Check which plans have results\n",
        "dynamic_plans_with_results = dynamic_compute_project.plan_df[dynamic_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "print(f\"Found {len(dynamic_plans_with_results)} plans with results:\")\n",
        "display.display(dynamic_plans_with_results[['plan_number', 'HDF_Results_Path']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "Let's compare the performance of the different parallel execution approaches we've tried. This will help us understand the impact of worker count and plan selection on execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:44:25.609864Z",
          "iopub.status.busy": "2025-11-17T18:44:25.609467Z",
          "iopub.status.idle": "2025-11-17T18:44:25.764162Z",
          "shell.execute_reply": "2025-11-17T18:44:25.763452Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a DataFrame for individual plan runtimes\n",
        "plan_data = []\n",
        "\n",
        "# Define the approaches with more descriptive labels including worker and core counts\n",
        "approach_labels = {\n",
        "    \"all_plans\": \"All Plans (2 workers \u00d7 2 cores = 4 cores total)\",\n",
        "    \"specific_plans\": \"Specific Plans (1 worker \u00d7 2 cores = 2 cores total)\",\n",
        "    \"dynamic_workers\": f\"Dynamic Workers (1 worker \u00d7 4 cores = 4 cores total)\"\n",
        "}\n",
        "\n",
        "# Extract runtimes from the log messages\n",
        "# For all plans approach\n",
        "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"01\", \"Runtime\": 35.72})\n",
        "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"03\", \"Runtime\": 82.70})\n",
        "# Omitting plan 04 as it's a 1D model\n",
        "\n",
        "# For specific plans approach (plans 01 and 03 were run)\n",
        "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"01\", \"Runtime\": 29.10})\n",
        "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"03\", \"Runtime\": 36.09})\n",
        "\n",
        "# For dynamic worker approach (plans 01 and 03 were run)\n",
        "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"01\", \"Runtime\": 28.48})\n",
        "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"03\", \"Runtime\": 49.43})\n",
        "\n",
        "# Create a DataFrame\n",
        "plan_runtime_df = pd.DataFrame(plan_data)\n",
        "\n",
        "# Create a grouped bar chart for plan runtimes\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Get all unique plan numbers and ensure they're sorted\n",
        "plans = sorted(plan_runtime_df[\"Plan\"].unique())\n",
        "\n",
        "# Create x positions for the bars\n",
        "x = np.arange(len(plans))\n",
        "width = 0.25  # Width of the bars\n",
        "\n",
        "# Plot bars for each approach\n",
        "approaches = plan_runtime_df[\"Approach\"].unique()\n",
        "for i, approach in enumerate(approaches):\n",
        "    # Filter data for this approach\n",
        "    approach_data = plan_runtime_df[plan_runtime_df[\"Approach\"] == approach]\n",
        "    \n",
        "    # Initialize runtimes array with NaN values\n",
        "    runtimes = [np.nan] * len(plans)\n",
        "    \n",
        "    # Fill in runtimes where data exists\n",
        "    for j, plan in enumerate(plans):\n",
        "        plan_runtime = approach_data[approach_data[\"Plan\"] == plan][\"Runtime\"]\n",
        "        if not plan_runtime.empty:\n",
        "            runtimes[j] = plan_runtime.values[0]\n",
        "    \n",
        "    # Create bars for this approach (only where we have data)\n",
        "    valid_indices = [idx for idx, val in enumerate(runtimes) if not np.isnan(val)]\n",
        "    valid_plans = [plans[idx] for idx in valid_indices]\n",
        "    valid_runtimes = [runtimes[idx] for idx in valid_indices]\n",
        "    valid_positions = [x[idx] + (i - len(approaches)/2 + 0.5) * width for idx in valid_indices]\n",
        "    \n",
        "    # Plot the bars\n",
        "    bars = plt.bar(valid_positions, valid_runtimes, width, label=approach)\n",
        "    \n",
        "    # Add runtime labels on top of bars\n",
        "    for pos, runtime in zip(valid_positions, valid_runtimes):\n",
        "        plt.text(pos, runtime + 2, f\"{runtime:.1f}s\", ha='center', va='bottom')\n",
        "\n",
        "# Add labels, title, and custom x-axis tick labels\n",
        "plt.xlabel('Plan Number', fontsize=12)\n",
        "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
        "plt.title('Runtime Comparison by Plan Number and Parallelization Approach', fontsize=14)\n",
        "plt.xticks(x, plans, fontsize=11)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add note about omitting Plan 04\n",
        "plt.figtext(0.5, 0.01, \"\\nNote: Plan 04 (1D model) is omitted from this comparison\", \n",
        "            ha='center', fontsize=10, style='italic')\n",
        "\n",
        "# Ensure all plan numbers show on x-axis regardless of data availability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Parallel Plan Execution\n",
        "\n",
        "In this notebook, we've explored how to execute HEC-RAS plans in parallel using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
        "\n",
        "1. **Basic Parallel Execution**: Using `RasCmdr.compute_parallel()` to run all plans in a project simultaneously\n",
        "2. **Selective Parallel Execution**: Running only specific plans in parallel\n",
        "3. **Dynamic Worker Allocation**: Determining the optimal number of workers based on available system resources\n",
        "4. **Performance Analysis**: Comparing execution times for different parallel configurations\n",
        "5. **Advanced Parallel Workflows**: Building complex workflows with parallel execution for sensitivity analysis\n",
        "\n",
        "### Key Functions Used\n",
        "\n",
        "- `RasCmdr.compute_parallel()`: Execute multiple plans in parallel\n",
        "- `RasPlan.clone_plan()`: Create a new plan based on an existing one\n",
        "- `RasPlan.update_plan_description()`: Update the description of a plan\n",
        "- `RasPlan.set_num_cores()`: Set the number of cores for a plan to use\n",
        "- `RasPlan.get_results_path()`: Get the path to the results file for a plan\n",
        "\n",
        "### Best Practices for Parallel Execution\n",
        "\n",
        "1. **Use Separate RAS Objects**: Create and use separate RAS objects for different projects or folders\n",
        "2. **Balance Workers and Cores**: Find the right balance between the number of workers and cores per worker\n",
        "3. **Consider Hardware Limits**: Be mindful of your system's physical cores and memory\n",
        "4. **Use Clean Compute Folders**: Use the `dest_folder` parameter to keep your project organized\n",
        "5. **Handle Overwrite Carefully**: Use `overwrite_dest=True` for repeatable workflows, but be cautious about losing results\n",
        "6. **Monitor Performance**: Track execution times and adjust your configuration for optimal performance\n",
        "7. **Match Workers to Plans**: For best results, use one worker per plan when running a small number of plans\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\09_plan_parameter_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:05.817940Z",
          "iopub.status.busy": "2025-11-17T17:40:05.817734Z",
          "iopub.status.idle": "2025-11-17T17:40:07.078302Z",
          "shell.execute_reply": "2025-11-17T17:40:07.077768Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.080820Z",
          "iopub.status.busy": "2025-11-17T17:40:07.080531Z",
          "iopub.status.idle": "2025-11-17T17:40:07.084395Z",
          "shell.execute_reply": "2025-11-17T17:40:07.083885Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.086295Z",
          "iopub.status.busy": "2025-11-17T17:40:07.086112Z",
          "iopub.status.idle": "2025-11-17T17:40:07.136217Z",
          "shell.execute_reply": "2025-11-17T17:40:07.135731Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan Files in HEC-RAS\n",
        "\n",
        "Before we dive into the operations, let's understand what HEC-RAS plan files are and why they're important:\n",
        "\n",
        "### What is a Plan File?\n",
        "\n",
        "A HEC-RAS plan file (`.p*`) is a configuration file that defines how a hydraulic simulation will run. It links together:\n",
        "\n",
        "1. **Geometry**: River channel and floodplain physical characteristics (`.g*` files)\n",
        "2. **Flow Data**: Inflow conditions, either steady (`.f*`) or unsteady (`.u*`)\n",
        "3. **Simulation Parameters**: Time steps, computational methods, and output settings\n",
        "\n",
        "### Key Components of Plan Files\n",
        "\n",
        "Plan files contain many parameters that control simulation behavior:\n",
        "\n",
        "- **Simulation Type**: Steady, unsteady, sediment transport, water quality\n",
        "- **Computation Intervals**: Time steps for calculations\n",
        "- **Output Intervals**: How frequently results are saved\n",
        "- **Run Flags**: Which modules to execute (preprocessor, postprocessor, etc.)\n",
        "- **Simulation Period**: Start and end dates for unsteady simulations\n",
        "- **Computation Methods**: Numerical schemes and solver settings\n",
        "- **Resource Allocation**: Number of CPU cores to use\n",
        "\n",
        "### Why Automate Plan Operations?\n",
        "\n",
        "Automating plan operations with RAS Commander allows you to:\n",
        "\n",
        "1. **Batch Processing**: Run multiple scenarios with different parameters\n",
        "2. **Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
        "3. **Calibration**: Adjust parameters to match observed data\n",
        "4. **Consistency**: Ensure standardized settings across multiple models\n",
        "5. **Documentation**: Programmatically track simulation configurations\n",
        "\n",
        "Now, let's download and extract an example project to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.138031Z",
          "iopub.status.busy": "2025-11-17T17:40:07.137864Z",
          "iopub.status.idle": "2025-11-17T17:40:07.175189Z",
          "shell.execute_reply": "2025-11-17T17:40:07.174773Z"
        }
      },
      "outputs": [],
      "source": [
        "# Create a RasExamples instance\n",
        "ras_examples = RasExamples()\n",
        "\n",
        "# Extract the Bald Eagle Creek example project\n",
        "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\"])\n",
        "print(f\"Extracted project to: {extracted_paths}\")\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "The first step in any RAS Commander workflow is initializing the HEC-RAS project. This connects the Python environment to the HEC-RAS project files.\n",
        "\n",
        "The `init_ras_project()` function does the following:\n",
        "\n",
        "1. Locates the main project file (`.prj`)\n",
        "2. Reads all associated files (plans, geometries, flows)\n",
        "3. Creates dataframes containing project components\n",
        "4. Sets up the connection to the HEC-RAS executable\n",
        "\n",
        "Let's initialize our project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.176898Z",
          "iopub.status.busy": "2025-11-17T17:40:07.176729Z",
          "iopub.status.idle": "2025-11-17T17:40:07.207883Z",
          "shell.execute_reply": "2025-11-17T17:40:07.207371Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the project (using the default global ras object)\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized project: {ras.project_name}\")\n",
        "\n",
        "# Display basic project information\n",
        "print(\"\\nProject Overview:\")\n",
        "print(f\"Project Folder: {ras.project_folder}\")\n",
        "print(f\"Project File: {ras.prj_file}\")\n",
        "print(f\"Number of Plan Files: {len(ras.plan_df)}\")\n",
        "print(f\"Number of Geometry Files: {len(ras.geom_df)}\")\n",
        "print(f\"Number of Flow Files: {len(ras.flow_df)}\")\n",
        "print(f\"Number of Unsteady Files: {len(ras.unsteady_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also take a look at the plan files in this project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.209780Z",
          "iopub.status.busy": "2025-11-17T17:40:07.209638Z",
          "iopub.status.idle": "2025-11-17T17:40:07.223694Z",
          "shell.execute_reply": "2025-11-17T17:40:07.223174Z"
        }
      },
      "outputs": [],
      "source": [
        "# Display the plan files\n",
        "print(\"Plan Files in Project:\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.225775Z",
          "iopub.status.busy": "2025-11-17T17:40:07.225425Z",
          "iopub.status.idle": "2025-11-17T17:40:07.228598Z",
          "shell.execute_reply": "2025-11-17T17:40:07.228091Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get the first plan number for our examples\n",
        "plan_number = ras.plan_df['plan_number'].iloc[0]\n",
        "print(f\"\\nWe'll work with Plan: {plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Updating Run Flags\n",
        "\n",
        "Run flags in HEC-RAS control which components of the simulation are executed. The `RasPlan.update_run_flags()` method allows you to modify these flags programmatically.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `geometry_preprocessor` (bool, optional): Whether to run the geometry preprocessor\n",
        "- `unsteady_flow_simulation` (bool, optional): Whether to run the unsteady flow simulation\n",
        "- `run_sediment` (bool, optional): Whether to run sediment transport calculations\n",
        "- `post_processor` (bool, optional): Whether to run the post-processor\n",
        "- `floodplain_mapping` (bool, optional): Whether to run floodplain mapping\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Common Run Flags\n",
        "\n",
        "1. **Geometry Preprocessor**: Computes hydraulic tables from geometry data\n",
        "   - `True`: Recompute tables (useful after geometry changes)\n",
        "   - `False`: Use existing tables (faster but may be outdated)\n",
        "\n",
        "2. **Unsteady Flow Simulation**: The main hydraulic calculations\n",
        "   - `True`: Run unsteady flow calculations\n",
        "   - `False`: Skip unsteady flow calculations\n",
        "\n",
        "3. **Sediment Transport**: Simulates erosion and deposition\n",
        "   - `True`: Calculate sediment transport\n",
        "   - `False`: Skip sediment transport\n",
        "\n",
        "4. **Post-Processor**: Calculates additional variables from results\n",
        "   - `True`: Run post-processing (recommended)\n",
        "   - `False`: Skip post-processing (faster but fewer outputs)\n",
        "\n",
        "5. **Floodplain Mapping**: Generates inundation maps\n",
        "   - `True`: Generate maps (requires terrain data)\n",
        "   - `False`: Skip mapping (faster)\n",
        "\n",
        "Let's update the run flags for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.230333Z",
          "iopub.status.busy": "2025-11-17T17:40:07.230194Z",
          "iopub.status.idle": "2025-11-17T17:40:07.238230Z",
          "shell.execute_reply": "2025-11-17T17:40:07.237677Z"
        }
      },
      "outputs": [],
      "source": [
        "# Update run flags for the plan\n",
        "print(f\"Updating run flags for plan {plan_number}...\")\n",
        "RasPlan.update_run_flags(\n",
        "    \"01\",\n",
        "    geometry_preprocessor=False,     # This may result in a popup if preprocessor files are not present\n",
        "    unsteady_flow_simulation=False,   # Run the main hydraulic calculations\n",
        "    run_sediment=False,              # Skip sediment transport calculations\n",
        "    post_processor=False,             # Run post-processing for additional outputs\n",
        "    floodplain_mapping=True,        # Skip floodplain mapping\n",
        ")\n",
        "print(\"Run flags updated successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.240129Z",
          "iopub.status.busy": "2025-11-17T17:40:07.239946Z",
          "iopub.status.idle": "2025-11-17T17:40:07.266901Z",
          "shell.execute_reply": "2025-11-17T17:40:07.266480Z"
        }
      },
      "outputs": [],
      "source": [
        "# The dataframes won't automatically update with changes, so re-init to ensure you are reading the latest version\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "\n",
        "# Display the plan dataframe again to show changes were effective\n",
        "print(\"Plan Files in Project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.268595Z",
          "iopub.status.busy": "2025-11-17T17:40:07.268452Z",
          "iopub.status.idle": "2025-11-17T17:40:07.273516Z",
          "shell.execute_reply": "2025-11-17T17:40:07.273015Z"
        }
      },
      "outputs": [],
      "source": [
        "plan_path = RasPlan.get_plan_path(\"01\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.275164Z",
          "iopub.status.busy": "2025-11-17T17:40:07.275034Z",
          "iopub.status.idle": "2025-11-17T17:40:07.277707Z",
          "shell.execute_reply": "2025-11-17T17:40:07.277250Z"
        }
      },
      "outputs": [],
      "source": [
        "print(plan_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.279943Z",
          "iopub.status.busy": "2025-11-17T17:40:07.279561Z",
          "iopub.status.idle": "2025-11-17T17:40:07.283386Z",
          "shell.execute_reply": "2025-11-17T17:40:07.282845Z"
        }
      },
      "outputs": [],
      "source": [
        "# Print the Plan file's contents to confirm the change\n",
        "\n",
        "# Print the plan file contents to verify the run flag changes\n",
        "with open(plan_path, 'r') as f:\n",
        "    print(f.read())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Updating Plan Intervals\n",
        "\n",
        "Time intervals in HEC-RAS control the temporal resolution of simulations and outputs. The `RasPlan.update_plan_intervals()` method allows you to modify these intervals.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `computation_interval` (str, optional): Time step for calculations\n",
        "- `output_interval` (str, optional): Time step for saving detailed results\n",
        "- `instantaneous_interval` (str, optional): Time step for peak value calculations\n",
        "- `mapping_interval` (str, optional): Time step for map outputs\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Valid Interval Values\n",
        "\n",
        "Time intervals must be specified in HEC-RAS format:\n",
        "- Seconds: `1SEC`, `2SEC`, `3SEC`, `4SEC`, `5SEC`, `6SEC`, `10SEC`, `15SEC`, `20SEC`, `30SEC`\n",
        "- Minutes: `1MIN`, `2MIN`, `3MIN`, `4MIN`, `5MIN`, `6MIN`, `10MIN`, `15MIN`, `20MIN`, `30MIN`\n",
        "- Hours: `1HOUR`, `2HOUR`, `3HOUR`, `4HOUR`, `6HOUR`, `8HOUR`, `12HOUR`\n",
        "- Days: `1DAY`\n",
        "\n",
        "### Interval Types\n",
        "\n",
        "1. **Computation Interval**: Time step used for hydraulic calculations\n",
        "   - Smaller intervals: More accurate but slower\n",
        "   - Larger intervals: Faster but may introduce numerical errors\n",
        "   - Rule of thumb: Should be small enough to capture flow changes\n",
        "\n",
        "2. **Output Interval**: How frequently detailed results are saved\n",
        "   - Smaller intervals: More detailed results but larger files\n",
        "   - Larger intervals: Smaller files but less temporal resolution\n",
        "   - Usually larger than computation interval\n",
        "\n",
        "3. **Instantaneous Interval**: Time step for peak value calculations\n",
        "   - Affects when max/min values are checked\n",
        "   - Usually equal to output interval\n",
        "\n",
        "4. **Mapping Interval**: How frequently map data is saved\n",
        "   - Affects animation smoothness and file size\n",
        "   - Usually larger than output interval\n",
        "\n",
        "Let's update the intervals for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.285664Z",
          "iopub.status.busy": "2025-11-17T17:40:07.285256Z",
          "iopub.status.idle": "2025-11-17T17:40:07.293214Z",
          "shell.execute_reply": "2025-11-17T17:40:07.292712Z"
        }
      },
      "outputs": [],
      "source": [
        "# Update plan intervals\n",
        "print(f\"Updating intervals for plan {plan_number}...\")\n",
        "RasPlan.update_plan_intervals(\n",
        "    plan_number,\n",
        "    computation_interval=\"5SEC\",    # 5-second time step for calculations\n",
        "    output_interval=\"1MIN\",         # Save detailed results every minute\n",
        "    instantaneous_interval=\"5MIN\",  # Check for max/min values every 5 minutes\n",
        "    mapping_interval=\"15MIN\",       # Save map data every 15 minutes\n",
        "\n",
        ")\n",
        "print(\"Plan intervals updated successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Managing Plan Descriptions\n",
        "\n",
        "Plan descriptions provide documentation for simulation configurations. The RAS Commander library offers methods to read and update these descriptions.\n",
        "\n",
        "### Reading Descriptions\n",
        "\n",
        "The `RasPlan.read_plan_description()` method retrieves the current description from a plan file.\n",
        "\n",
        "#### Parameters\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Updating Descriptions\n",
        "\n",
        "The `RasPlan.update_plan_description()` method sets a new description for a plan file.\n",
        "\n",
        "#### Parameters\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `description` (str): The new description text\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Best Practices for Plan Descriptions\n",
        "\n",
        "Effective plan descriptions should include:\n",
        "1. Purpose of the simulation\n",
        "2. Key parameters and settings\n",
        "3. Date of creation or modification\n",
        "4. Author or organization\n",
        "5. Any special considerations or notes\n",
        "\n",
        "Let's read the current description and then update it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.295204Z",
          "iopub.status.busy": "2025-11-17T17:40:07.295049Z",
          "iopub.status.idle": "2025-11-17T17:40:07.325513Z",
          "shell.execute_reply": "2025-11-17T17:40:07.324993Z"
        }
      },
      "outputs": [],
      "source": [
        "# Read the current plan description\n",
        "current_description = RasPlan.read_plan_description(plan_number)\n",
        "print(f\"Current plan description:\\n{current_description}\")\n",
        "\n",
        "# Create a new description with detailed information\n",
        "new_description = f\"\"\"Modified Plan for RAS Commander Testing\n",
        "Date: {datetime.now().strftime('%Y-%m-%d')}\n",
        "Purpose: Demonstrating RAS Commander plan operations\n",
        "Settings:\n",
        "- Computation Interval: 5SEC\n",
        "- Output Interval: 1MIN\n",
        "- Mapping Interval: 15MIN\n",
        "- Geometry Preprocessor: Enabled\n",
        "- Post-Processor: Enabled\n",
        "Notes: This plan was automatically modified using ras-commander.\"\"\"\n",
        "\n",
        "# Update the plan description\n",
        "print(\"\\nUpdating plan description...\")\n",
        "RasPlan.update_plan_description(plan_number, new_description)\n",
        "print(\"Plan description updated successfully\")\n",
        "\n",
        "# Verify the updated description\n",
        "updated_description = RasPlan.read_plan_description(plan_number)\n",
        "print(f\"\\nUpdated plan description:\\n{updated_description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Updating Simulation Dates\n",
        "\n",
        "For unsteady flow simulations, the simulation period defines the time window for the analysis. The `RasPlan.update_simulation_date()` method allows you to modify this period.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `start_date` (datetime): The start date and time for the simulation\n",
        "- `end_date` (datetime): The end date and time for the simulation\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Considerations for Simulation Dates\n",
        "\n",
        "1. **Hydrograph Coverage**: The simulation period should fully encompass your hydrographs\n",
        "2. **Warm-Up Period**: Include time before the main event for model stabilization\n",
        "3. **Cool-Down Period**: Include time after the main event for complete drainage\n",
        "4. **Computational Efficiency**: Avoid unnecessarily long periods to reduce runtime\n",
        "5. **Consistency**: Ensure dates match available boundary condition data\n",
        "\n",
        "Let's update the simulation dates for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.327353Z",
          "iopub.status.busy": "2025-11-17T17:40:07.327204Z",
          "iopub.status.idle": "2025-11-17T17:40:07.350086Z",
          "shell.execute_reply": "2025-11-17T17:40:07.349576Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get the current simulation date\n",
        "current_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\")\n",
        "print(f\"Current simulation date: {current_sim_date}\")\n",
        "\n",
        "# Parse the current simulation date string\n",
        "current_dates = current_sim_date.split(\",\")\n",
        "current_start = datetime.strptime(f\"{current_dates[0]},{current_dates[1]}\", \"%d%b%Y,%H%M\")\n",
        "current_end = datetime.strptime(f\"{current_dates[2]},{current_dates[3]}\", \"%d%b%Y,%H%M\")\n",
        "\n",
        "# Define new simulation period - adjust by 1 hour from current dates\n",
        "start_date = current_start + timedelta(hours=1)  # Current start + 1 hour\n",
        "end_date = current_end - timedelta(hours=1)      # Current end - 1 hour\n",
        "\n",
        "# Update the simulation date\n",
        "print(f\"\\nUpdating simulation period to: {start_date.strftime('%d%b%Y,%H%M')} - {end_date.strftime('%d%b%Y,%H%M')}\")\n",
        "RasPlan.update_simulation_date(plan_number, start_date, end_date)\n",
        "print(\"Simulation dates updated successfully\")\n",
        "\n",
        "# Verify the updated simulation date\n",
        "updated_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\")\n",
        "print(f\"\\nUpdated simulation date: {updated_sim_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verifying Updated Plan Values\n",
        "\n",
        "After making multiple changes to a plan, it's a good practice to verify that all updates were applied correctly. Let's check the updated values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.352653Z",
          "iopub.status.busy": "2025-11-17T17:40:07.352343Z",
          "iopub.status.idle": "2025-11-17T17:40:07.355587Z",
          "shell.execute_reply": "2025-11-17T17:40:07.355204Z"
        }
      },
      "outputs": [],
      "source": [
        "# Print the Plan file's contents to confirm the change\n",
        "\n",
        "# Print the plan file contents to verify the run flag changes\n",
        "with open(plan_path, 'r') as f:\n",
        "    print(f.read())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Computing the Plan (Optional)\n",
        "\n",
        "After making changes to a plan, you might want to run the simulation to see the effects. The `RasCmdr.compute_plan()` method executes a HEC-RAS simulation with the specified plan.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (str): The plan number to execute\n",
        "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files\n",
        "- `num_cores` (int, optional): Number of processor cores to use\n",
        "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder\n",
        "\n",
        "If you want to run the simulation, you can uncomment the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:40:07.357493Z",
          "iopub.status.busy": "2025-11-17T17:40:07.357316Z",
          "iopub.status.idle": "2025-11-17T17:43:08.694598Z",
          "shell.execute_reply": "2025-11-17T17:43:08.694040Z"
        }
      },
      "outputs": [],
      "source": [
        "RasCmdr.compute_plan(plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:08.697362Z",
          "iopub.status.busy": "2025-11-17T17:43:08.697074Z",
          "iopub.status.idle": "2025-11-17T17:43:08.703989Z",
          "shell.execute_reply": "2025-11-17T17:43:08.703353Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to run the simulation with the updated plan\n",
        "\n",
        "# # Define a destination folder for the computation\n",
        "# dest_folder = script_dir / \"compute_results\"\n",
        "# print(f\"Computing plan {plan_number}...\")\n",
        "# print(f\"Results will be saved to: {dest_folder}\")\n",
        "\n",
        "# # Execute the plan\n",
        "# success = RasCmdr.compute_plan(\n",
        "#     plan_number,\n",
        "#     dest_folder=dest_folder,\n",
        "#     clear_geompre=True,    # Clear preprocessor files to ensure clean results\n",
        "#     num_cores=2,           # Use 2 processor cores\n",
        "#     overwrite_dest=True,   # Overwrite existing destination folder\n",
        "#     rasect=ras\n",
        "# )\n",
        "\n",
        "# if success:\n",
        "#     print(f\"Plan {plan_number} computed successfully\")\n",
        "#     # Check for results file\n",
        "#     results_path = RasPlan.get_results_path(plan_number)\n",
        "#     if results_path:\n",
        "#         print(f\"Results saved to: {results_path}\")\n",
        "# else:\n",
        "#     print(f\"Failed to compute plan {plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan Key Operations\n",
        "\n",
        "In this notebook, we've covered the essential operations for manipulating HEC-RAS plan files programmatically using RAS Commander:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project using `init_ras_project()`\n",
        "2. **Plan Values**: We retrieved plan values with `RasPlan.get_plan_value()`\n",
        "3. **Run Flags**: We updated simulation components with `RasPlan.update_run_flags()`\n",
        "4. **Plan Intervals**: We modified time steps with `RasPlan.update_plan_intervals()`\n",
        "5. **Plan Descriptions**: We managed documentation with `RasPlan.read_plan_description()` and `RasPlan.update_plan_description()`\n",
        "6. **Simulation Dates**: We changed the analysis period with `RasPlan.update_simulation_date()`\n",
        "7. **Verification**: We verified our changes by comparing initial and updated values\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasPlan`: The main class for plan operations\n",
        "  - `get_plan_value()`: Retrieve specific values from plan files\n",
        "  - `update_run_flags()`: Configure which components will run\n",
        "  - `update_plan_intervals()`: Set computation and output time intervals\n",
        "  - `read_plan_description()`: Get the current plan description\n",
        "  - `update_plan_description()`: Set a new plan description\n",
        "  - `update_simulation_date()`: Modify the simulation period\n",
        "  - `get_results_path()`: Get the path to results files\n",
        "\n",
        "- `RasCmdr`: The class for executing HEC-RAS simulations\n",
        "  - `compute_plan()`: Run a single plan simulation\n",
        "\n",
        "### Best Practices for Plan Operations\n",
        "\n",
        "1. **Verify Before Updating**: Always check current values before making changes\n",
        "2. **Document Changes**: Use descriptive plan descriptions to track modifications\n",
        "3. **Maintain Consistency**: Ensure flow data matches simulation dates\n",
        "4. **Use Appropriate Intervals**: Balance accuracy and computational efficiency\n",
        "5. **Backup Original Files**: Use destination folders when running simulations\n",
        "6. **Verify After Updates**: Confirm that all changes were applied correctly\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "With these plan operations, you can now:\n",
        "\n",
        "1. **Create Batch Workflows**: Process multiple scenarios with different parameters\n",
        "2. **Perform Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
        "3. **Automate Calibration**: Adjust parameters to match observed data\n",
        "4. **Build Model Ensembles**: Run multiple configurations for uncertainty analysis\n",
        "5. **Integrate with Other Tools**: Connect HEC-RAS to broader modeling frameworks\n",
        "\n",
        "These operations form the foundation for advanced HEC-RAS automation using the RAS Commander library."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\101_Core_Sensitivity.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14_Core_Sensitivity.ipynb\n",
        "Testing Core Sensitivity for RAS using the Bald Eagle Creek Multi-Gage 2D project.  \n",
        "\n",
        "\n",
        "This should take around 15-45 minutes to run depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - Found zip file: d:\\GitHub\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n",
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n",
            "2025-04-09 08:50:59 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n",
            "2025-04-09 08:51:01 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-04-09 08:51:01 - ras_commander.RasPrj - INFO - Initializing global 'ras' object via init_ras_project function.\n",
            "2025-04-09 08:51:01 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n",
            "2025-04-09 08:51:01 - ras_commander.RasPrj - INFO - Project initialized. ras_object project folder: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-04-09 08:51:01 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 08:51:01 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 08:51:01 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n",
            "2025-04-09 08:51:01 - ras_commander.RasUtils - INFO - Constructed plan file path: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 08:51:01 - ras_commander.RasUtils - INFO - Successfully updated file: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 08:51:02 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-04-09 08:51:02 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-04-09 08:51:02 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"D:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\" \"d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running sensitivity analysis for Plan 03\n",
            "Running with 1 core(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-09 08:58:06 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-04-09 08:58:06 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 424.31 seconds\n",
            "2025-04-09 08:58:06 - ras_commander.RasUtils - INFO - Constructed plan file path: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 08:58:06 - ras_commander.RasUtils - INFO - Successfully updated file: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 08:58:06 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-04-09 08:58:06 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-04-09 08:58:06 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"D:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\" \"d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 424.34 seconds\n",
            "Running with 2 core(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-09 09:02:18 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-04-09 09:02:18 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 252.47 seconds\n",
            "2025-04-09 09:02:18 - ras_commander.RasUtils - INFO - Constructed plan file path: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 09:02:18 - ras_commander.RasUtils - INFO - Successfully updated file: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 09:02:18 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-04-09 09:02:18 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-04-09 09:02:18 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"D:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\" \"d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 252.53 seconds\n",
            "Running with 3 core(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-09 09:06:39 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-04-09 09:06:39 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 260.34 seconds\n",
            "2025-04-09 09:06:39 - ras_commander.RasUtils - INFO - Constructed plan file path: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 09:06:39 - ras_commander.RasUtils - INFO - Successfully updated file: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\n",
            "2025-04-09 09:06:39 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "2025-04-09 09:06:39 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-04-09 09:06:39 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"D:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\" \"d:\\GitHub\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 260.38 seconds\n",
            "Running with 4 core(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-09 09:10:55 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-04-09 09:10:55 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 256.51 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 256.55 seconds\n",
            "Sensitivity analysis complete\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ras_commander import RasExamples, init_ras_project, RasCmdr, RasPlan, RasGeo\n",
        "\n",
        "# Step 1: Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "\n",
        "RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "# Use Path.cwd() to get the current working directory in a Jupyter Notebook\n",
        "current_directory = Path.cwd()\n",
        "project_path = current_directory / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "\n",
        "# Step 2: Initialize the RAS Project Folder using init_ras_project (from ras_commander)\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "# Step 3: Initialize a DataFrame to store execution results\n",
        "results = []\n",
        "\n",
        "# Step 4: Run sensitivity analysis for Plan 03 with core counts 1-8\n",
        "plan_number = '03'\n",
        "print(f\"Running sensitivity analysis for Plan {plan_number}\")\n",
        "\n",
        "# Clear geompre files before running the plan\n",
        "plan_path = RasPlan.get_plan_path(plan_number)\n",
        "RasGeo.clear_geompre_files(plan_path)\n",
        "\n",
        "for cores in range(1, 5):\n",
        "    print(f\"Running with {cores} core(s)\")\n",
        "    # Set core count for this plan\n",
        "    RasPlan.set_num_cores(plan_number, cores)\n",
        "    \n",
        "    # Time the execution of the plan\n",
        "    start_time = time.time()\n",
        "    RasCmdr.compute_plan(plan_number)\n",
        "    execution_time = time.time() - start_time\n",
        "    \n",
        "    # Store the results\n",
        "    results.append({\n",
        "        \"plan_number\": plan_number,\n",
        "        \"cores\": cores,\n",
        "        \"execution_time\": execution_time\n",
        "    })\n",
        "    \n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "print(\"Sensitivity analysis complete\")\n",
        "\n",
        "# Step 5: Convert results into a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Optionally, save the results to a CSV file\n",
        "results_df.to_csv(\"core_sensitivity_results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTES FOR REVISIONS:\n",
        "- Use HDF compute summary to show the time for each preprocesS/unsteady compute/postprocess step. \n",
        "- First, run preprocessor and then toggle options to only run unsteady compute and postprocess. \n",
        "- Plot each step separately. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "results_df DataFrame (time is in seconds):\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>cores</th>\\n', '      <th>execution_time</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>3</td>\\n', '      <td>1</td>\\n', '      <td>424.342272</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>3</td>\\n', '      <td>2</td>\\n', '      <td>252.529661</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>3</td>\\n', '      <td>3</td>\\n', '      <td>260.380589</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>3</td>\\n', '      <td>4</td>\\n', '      <td>256.551776</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "   plan_number  cores  execution_time\n",
              "0            3      1      424.342272\n",
              "1            3      2      252.529661\n",
              "2            3      3      260.380589\n",
              "3            3      4      256.551776"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Optionally, load the results from a CSV file\n",
        "results_df = pd.read_csv(\"core_sensitivity_results.csv\")\n",
        "\n",
        "# Display the results dataframe for verification\n",
        "print(\"results_df DataFrame (time is in seconds):\")\n",
        "display(results_df)\n",
        "\n",
        "# Step 6: Calculate unit runtime (based on 1 core execution time)\n",
        "results_df['unit_runtime'] = results_df.groupby('plan_number')['execution_time'].transform(lambda x: x / x.iloc[0])\n",
        "\n",
        "# Get the project name from the ras object\n",
        "project_name = ras.project_name\n",
        "\n",
        "# Step 7: Plot a line chart for unit runtime vs. cores for each plan\n",
        "plt.figure(figsize=(10, 6))\n",
        "for plan in results_df['plan_number'].unique():\n",
        "    plan_data = results_df[results_df['plan_number'] == plan]\n",
        "    plt.plot(plan_data['cores'], plan_data['unit_runtime'], label=f\"Plan {plan}\")\n",
        "\n",
        "plt.xlabel(\"Number of Cores\")\n",
        "plt.ylabel(\"Unit Runtime (Relative to 1 Core)\")\n",
        "plt.title(f\"{project_name} (HEC Example Project)\\nCore Count Sensitivity Analysis\")\n",
        "plt.legend(title=\"Plan Number\")\n",
        "plt.grid(False)\n",
        "plt.vlines([1,2,3,4], ymin=0, ymax=1.2, linestyles='dotted', alpha=0.3)\n",
        "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\102_benchmarking_versions_6.1_to_6.6.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import psutil\n",
        "import platform\n",
        "import cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define versions to compare\n",
        "versions = ['6.6', '6.5', '6.4.1', '6.3.1', '6.3', '6.2', \"6.1\", \"6.0\"] # NOTE: ras-commander does not support versions prior to 6.2 due to HDF5 file format changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract BaldEagleCrkMulti2D project\n",
        "project_path = RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init the ras_project with ras-commander to read all HEC-RAS project information \n",
        "init_ras_project(project_path, \"6.5\")\n",
        "print(ras)\n",
        "# If no ras object is defined in init_ras_project, it defaults to \"ras\" (useful for single project scripts)\n",
        "# Display plan dataframe\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Plan Numbers to List and Print\n",
        "plan_numbers = ras.plan_df['plan_number'].tolist()\n",
        "print(plan_numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define run_simulation function for\n",
        "import time\n",
        "from ras_commander import RasGeo\n",
        "\n",
        "def run_simulation(version, plan_number):\n",
        "    # Initialize project for the specific version\n",
        "    ras_project = init_ras_project(project_path, str(version))\n",
        "    \n",
        "    # Clear geometry preprocessor files for the plan\n",
        "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_project)\n",
        "    RasGeo.clear_geompre_files(plan_path, ras_object=ras_project)\n",
        "    \n",
        "    # Set the number of cores to 4\n",
        "    RasPlan.set_num_cores(plan_number, \"4\", ras_object=ras_project)\n",
        "    \n",
        "    # Update plan run flags \u2013 setting \"Run HTab\" flag to 1 to force geometry preprocessing\n",
        "    RasPlan.update_run_flags(plan_number, {\"Run HTab\": 1}, ras_object=ras_project)\n",
        "    \n",
        "    # Compute the plan\n",
        "    start_time = time.time()\n",
        "    success = RasCmdr.compute_plan(plan_number, ras_object=ras_project)\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    if success:\n",
        "        # Get the HDF file path for the plan results\n",
        "        hdf_path = RasPlan.get_results_path(plan_number, ras_object=ras_project)\n",
        "        \n",
        "        # Extract runtime data from the HDF file\n",
        "        runtime_data = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "        \n",
        "        # Extract required information from the runtime data\n",
        "        preprocessor_time = runtime_data['Preprocessing Geometry (hr)'].values[0]\n",
        "        unsteady_compute_time = runtime_data['Unsteady Flow Computations (hr)'].values[0]\n",
        "        \n",
        "        # Get volume accounting data from the HDF file\n",
        "        volume_accounting = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "        # Extract Error Percent from the DataFrame\n",
        "        volume_error = volume_accounting['Error Percent'].values[0] if not volume_accounting.empty else None\n",
        "        \n",
        "        # Print the extracted data\n",
        "        print(f\"\\nExtracted Data for Plan {plan_number} in Version {version}:\")\n",
        "        print(f\"Preprocessor Time: {preprocessor_time:.3f} hr\")\n",
        "        print(f\"Unsteady Compute Time: {unsteady_compute_time:.3f} hr\") \n",
        "        print(f\"Volume Error: {volume_error:.3f}%\" if volume_error is not None else \"Volume Error: None\")\n",
        "        print(f\"Total Time: {total_time/3600:.3f} hr\\n\")\n",
        "        \n",
        "        return {\n",
        "            'Version': version,\n",
        "            'Plan': plan_number,\n",
        "            'Preprocessor Time (hr)': preprocessor_time,\n",
        "            'Unsteady Compute Time (hr)': unsteady_compute_time,\n",
        "            'Volume Error (%)': volume_error,\n",
        "            'Total Time (hr)': total_time / 3600  # convert seconds to hours\n",
        "        }\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the plan number you want to run across all versions\n",
        "plan_number = '02'  # Make sure this is a string and include the leading zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run simulations for all versions with plan_number defined by user\n",
        "results = []\n",
        "for version in versions:\n",
        "    print(f\"Running simulation for Version {version}, Plan {plan_number}\")\n",
        "    result = run_simulation(version, plan_number) \n",
        "    if result is not None:  # Check if result is not None\n",
        "        results.append(result)\n",
        "        print(f\"Completed: Version {version}, Plan {plan_number}\")\n",
        "    else:\n",
        "        print(f\"Failed: Version {version}, Plan {plan_number}\")\n",
        "\n",
        "# Create DataFrame from results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Save initial results to CSV\n",
        "df.to_csv('save_initial_results.csv', index=False)\n",
        "\n",
        "print(\"Initial results saved to 'save_initial_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create line graphs\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Unsteady Runtime vs Version\n",
        "plt.subplot(1, 2, 1)\n",
        "# Convert Version to categorical type to handle string versions properly\n",
        "plt.plot(pd.Categorical(df['Version']), df['Unsteady Compute Time (hr)'], marker='o')\n",
        "plt.title(f'Unsteady Runtime vs HEC-RAS Version (Plan {plan_number})')\n",
        "plt.xlabel('HEC-RAS Version')\n",
        "plt.ylabel('Unsteady Runtime (hours)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Volume Error vs Version\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(pd.Categorical(df['Version']), df['Volume Error (%)'], marker='o')\n",
        "plt.title(f'Volume Error vs HEC-RAS Version (Plan {plan_number})')\n",
        "plt.xlabel('HEC-RAS Version')\n",
        "plt.ylabel('Volume Error (%)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark all plans with 2 cores in HEC-RAS 6.6\n",
        "results_2core = []\n",
        "\n",
        "# Loop through each plan number\n",
        "for plan in plan_numbers:\n",
        "    print(f\"Running simulation for Version 6.6, Plan {plan} with 2 cores\")\n",
        "    \n",
        "    # Initialize project for 6.6\n",
        "    ras_project = init_ras_project(project_path, \"6.6\")\n",
        "    \n",
        "    # Clear geometry preprocessor files\n",
        "    plan_path = RasPlan.get_plan_path(plan, ras_object=ras_project)\n",
        "    RasGeo.clear_geompre_files(plan_path, ras_object=ras_project)\n",
        "    \n",
        "    # Set number of cores to 2\n",
        "    RasPlan.set_num_cores(plan, \"2\", ras_object=ras_project)\n",
        "    \n",
        "    # Update plan run flags\n",
        "    RasPlan.update_run_flags(plan, {\"Run HTab\": 1}, ras_object=ras_project)\n",
        "    \n",
        "    # Compute the plan\n",
        "    start_time = time.time()\n",
        "    success = RasCmdr.compute_plan(plan, ras_object=ras_project)\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    if success:\n",
        "        # Get HDF file path\n",
        "        hdf_path = RasPlan.get_results_path(plan, ras_object=ras_project)\n",
        "        \n",
        "        # Extract runtime data\n",
        "        runtime_data = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "        preprocessor_time = runtime_data['Preprocessing Geometry (hr)'].values[0]\n",
        "        unsteady_compute_time = runtime_data['Unsteady Flow Computations (hr)'].values[0]\n",
        "        \n",
        "        # Get volume accounting\n",
        "        volume_accounting = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "        volume_error = volume_accounting['Error Percent'].values[0] if not volume_accounting.empty else None\n",
        "        \n",
        "        result = {\n",
        "            'Plan': plan,\n",
        "            'Preprocessor Time (hr)': preprocessor_time,\n",
        "            'Unsteady Compute Time (hr)': unsteady_compute_time,\n",
        "            'Volume Error (%)': volume_error,\n",
        "            'Total Time (hr)': total_time / 3600\n",
        "        }\n",
        "        results_2core.append(result)\n",
        "        print(f\"Completed: Plan {plan} with 2 cores\")\n",
        "    else:\n",
        "        print(f\"Failed: Plan {plan} with 2 cores\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_2core = pd.DataFrame(results_2core)\n",
        "\n",
        "# Get plan titles from ras.plan_df and merge with results\n",
        "plan_titles = pd.DataFrame({\n",
        "    'Plan': ras.plan_df['plan_number'].str.zfill(2),\n",
        "    'Short Identifier': ras.plan_df['Short Identifier']\n",
        "})\n",
        "df_2core['Plan'] = df_2core['Plan'].astype(str).str.zfill(2)\n",
        "df_2core = df_2core.merge(plan_titles, on='Plan', how='left')\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Unsteady Runtime\n",
        "plt.subplot(2, 2, 1)\n",
        "bars = plt.bar(range(len(df_2core)), df_2core['Unsteady Compute Time (hr)'], color='blue', alpha=0.7)\n",
        "plt.title('Unsteady Runtime by Plan (2 Cores)', fontsize=12)\n",
        "plt.ylabel('Unsteady Runtime (hours)', fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(range(len(df_2core)), [f\"Plan {plan}\\n{title}\" for plan, title in zip(df_2core['Plan'], df_2core['Short Identifier'])], rotation=45, ha='right')\n",
        "\n",
        "# Plot 2: Volume Error\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(range(len(df_2core)), df_2core['Volume Error (%)'], color='red', alpha=0.7)\n",
        "plt.title('Volume Error by Plan (2 Cores)', fontsize=12)\n",
        "plt.ylabel('Volume Error (%)', fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(range(len(df_2core)), [f\"Plan {plan}\\n{title}\" for plan, title in zip(df_2core['Plan'], df_2core['Short Identifier'])], rotation=45, ha='right')\n",
        "\n",
        "# Plot 3: Preprocessor Time\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(range(len(df_2core)), df_2core['Preprocessor Time (hr)'], color='green', alpha=0.7)\n",
        "plt.title('Preprocessor Time by Plan (2 Cores)', fontsize=12)\n",
        "plt.ylabel('Preprocessor Time (hours)', fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(range(len(df_2core)), [f\"Plan {plan}\\n{title}\" for plan, title in zip(df_2core['Plan'], df_2core['Short Identifier'])], rotation=45, ha='right')\n",
        "\n",
        "# Plot 4: Total Runtime\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.bar(range(len(df_2core)), df_2core['Total Time (hr)'], color='purple', alpha=0.7)\n",
        "plt.title('Total Runtime by Plan (2 Cores)', fontsize=12)\n",
        "plt.ylabel('Total Runtime (hours)', fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(range(len(df_2core)), [f\"Plan {plan}\\n{title}\" for plan, title in zip(df_2core['Plan'], df_2core['Short Identifier'])], rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.suptitle('Plan Performance Comparison (2 Cores)', fontsize=14, y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Save results to CSV\n",
        "df_2core.to_csv('hecras_plan_comparison_2core.csv', index=False)\n",
        "print(\"Results saved to 'hecras_plan_comparison_2core.csv'\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nSummary Statistics (2 Cores):\")\n",
        "print(df_2core.describe())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\103_Running_AEP_Events_from_Atlas_14.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# For Development Mode, add the parent directory to the Python path\nimport os\nimport sys\nfrom pathlib import Path\n\ncurrent_file = Path(os.getcwd()).resolve()\nrascmdr_directory = current_file.parent\n\n# Use insert(0) instead of append() to give highest priority to local version\nif str(rascmdr_directory) not in sys.path:\n    sys.path.insert(0, str(rascmdr_directory))\n\nprint(f\"Loading ras-commander from: {rascmdr_directory}\")\nfrom ras_commander import *"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AEP Storm Analysis with RAS-Commander\n",
        "\n",
        "This notebook automates the end-to-end process of analyzing multiple storm events with different Annual Exceedance Probabilities (AEP) in HEC-RAS. It covers:\n",
        "\n",
        "1. Generating hyetographs from NOAA Atlas 14 precipitation data\n",
        "2. Creating HEC-RAS plan files for each AEP event\n",
        "3. Creating unsteady flow files with the generated hyetographs\n",
        "4. Executing multiple plans in parallel\n",
        "5. Analyzing and visualizing the results\n",
        "\n",
        "This automation is particularly useful for analyzing how a drainage system performs under different storm frequencies, from common events (e.g., 2-year) to rare events (e.g., 100-year)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Import Libraries\n",
        "\n",
        "First, we'll import all the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "\n",
        "# Install ras-commander if not already installed\n",
        "# Uncomment this line if you need to install the package\n",
        "# !pip install ras-commander\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RasExamples.extract_project([\"Davis\"])\n",
        "# This loads the project in fresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Hyetograph Generation Functions\n",
        "\n",
        "These functions handle reading precipitation frequency data from NOAA Atlas 14 and generating balanced storm hyetographs using the Alternating Block Method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_duration(duration_str):\n",
        "    \"\"\"\n",
        "    Parses a duration string and converts it to hours.\n",
        "    Examples: \"5-min:\" -> 0.0833 hours, \"2-hr:\" -> 2 hours, \"2-day:\" -> 48 hours\n",
        "    \"\"\"\n",
        "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
        "    if not match:\n",
        "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
        "    value, unit = match.groups()\n",
        "    value = int(value)\n",
        "    unit = unit.lower()\n",
        "    if unit in ['min', 'minute', 'minutes']:\n",
        "        hours = value / 60.0\n",
        "    elif unit in ['hr', 'hour', 'hours']:\n",
        "        hours = value\n",
        "    elif unit in ['day', 'days']:\n",
        "        hours = value * 24\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
        "    return hours\n",
        "\n",
        "def read_precipitation_data(csv_file):\n",
        "    \"\"\"\n",
        "    Reads the precipitation frequency CSV and returns a DataFrame\n",
        "    with durations in hours as the index and ARIs as columns.\n",
        "    \"\"\"\n",
        "    with open(csv_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    header_line_idx = None\n",
        "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
        "\n",
        "    # Locate the header line\n",
        "    for idx, line in enumerate(lines):\n",
        "        if header_pattern.match(line.strip().lower()):\n",
        "            header_line_idx = idx\n",
        "            break\n",
        "\n",
        "    if header_line_idx is None:\n",
        "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
        "\n",
        "    # Extract the ARI headers from the header line\n",
        "    header_line = lines[header_line_idx].strip()\n",
        "    headers = [item.strip() for item in header_line.split(',')]\n",
        "    \n",
        "    if len(headers) < 2:\n",
        "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
        "\n",
        "    aris = headers[1:]  # Exclude the first column which is the duration\n",
        "\n",
        "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
        "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
        "\n",
        "    # Initialize lists to store durations and corresponding depths\n",
        "    durations = []\n",
        "    depths = {ari: [] for ari in aris}\n",
        "\n",
        "    # Iterate over the lines following the header to extract data\n",
        "    for line in lines[header_line_idx + 1:]:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # Skip empty lines\n",
        "        if not duration_pattern.match(line):\n",
        "            break  # Stop if the line does not match the duration pattern\n",
        "        parts = [part.strip() for part in line.split(',')]\n",
        "        if len(parts) != len(headers):\n",
        "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
        "        duration_str = parts[0]\n",
        "        try:\n",
        "            duration_hours = parse_duration(duration_str)\n",
        "        except ValueError as ve:\n",
        "            print(f\"Skipping line due to error: {ve}\")\n",
        "            continue  # Skip lines with invalid duration formats\n",
        "        durations.append(duration_hours)\n",
        "        for ari, depth_str in zip(aris, parts[1:]):\n",
        "            try:\n",
        "                depth = float(depth_str)\n",
        "            except ValueError:\n",
        "                depth = np.nan  # Assign NaN for invalid depth values\n",
        "            depths[ari].append(depth)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(depths, index=durations)\n",
        "    df.index.name = 'Duration_hours'\n",
        "\n",
        "    # Drop any rows with NaN values\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "def interpolate_depths(df, total_duration):\n",
        "    \"\"\"\n",
        "    Interpolates precipitation depths for each ARI on a log-log scale\n",
        "    for each hour up to the total storm duration.\n",
        "    \"\"\"\n",
        "    T = total_duration\n",
        "    t_hours = np.arange(1, T+1)\n",
        "    D = {}\n",
        "    for ari in df.columns:\n",
        "        durations = df.index.values\n",
        "        depths = df[ari].values\n",
        "        # Ensure all depths are positive\n",
        "        if np.any(depths <= 0):\n",
        "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
        "        # Log-log interpolation\n",
        "        log_durations = np.log(durations)\n",
        "        log_depths = np.log(depths)\n",
        "        log_t = np.log(t_hours)\n",
        "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
        "        D_t = np.exp(log_D_t)\n",
        "        D[ari] = D_t\n",
        "    return D\n",
        "\n",
        "def compute_incremental_depths(D, total_duration):\n",
        "    \"\"\"\n",
        "    Computes incremental precipitation depths for each hour.\n",
        "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
        "    \"\"\"\n",
        "    incremental_depths = {}\n",
        "    for ari, D_t in D.items():\n",
        "        I_t = np.empty(total_duration)\n",
        "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
        "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
        "        incremental_depths[ari] = I_t\n",
        "    return incremental_depths\n",
        "\n",
        "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
        "    \"\"\"\n",
        "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
        "    \"\"\"\n",
        "    hyetograph = [0.0] * T\n",
        "    hyetograph[central_index] = max_depth\n",
        "    remaining_depths = sorted_depths.copy()\n",
        "    remaining_depths.remove(max_depth)\n",
        "    left = central_index - 1\n",
        "    right = central_index + 1\n",
        "    toggle = True  # Start assigning to the right\n",
        "    for depth in remaining_depths:\n",
        "        if toggle and right < T:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif not toggle and left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        elif right < T:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        else:\n",
        "            print(\"Warning: Not all incremental depths assigned.\")\n",
        "            break\n",
        "        toggle = not toggle\n",
        "    return hyetograph\n",
        "\n",
        "def generate_hyetograph(incremental_depths, position_percent, T):\n",
        "    \"\"\"\n",
        "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
        "    \"\"\"\n",
        "    max_depth = np.max(incremental_depths)\n",
        "    incremental_depths_list = incremental_depths.tolist()\n",
        "    central_index = int(round(T * position_percent / 100)) - 1\n",
        "    central_index = max(0, min(central_index, T - 1))\n",
        "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
        "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
        "    return hyetograph\n",
        "\n",
        "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
        "    \"\"\"\n",
        "    Saves the hyetograph to a CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'Time_hour': np.arange(1, total_duration + 1),\n",
        "        'Precipitation_in': hyetograph\n",
        "    })\n",
        "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
        "    output_file = os.path.join(output_dir, filename)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
        "    \"\"\"\n",
        "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    for ari in aris:\n",
        "        # Ensure ARI is a string for consistent filename formatting\n",
        "        ari_str = str(ari)\n",
        "        \n",
        "        # Construct the filename based on the naming convention\n",
        "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        \n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Warning: File '{filename}' does not exist in the directory '{output_dir}'. Skipping this ARI.\")\n",
        "            continue\n",
        "        \n",
        "        # Read the hyetograph data\n",
        "        try:\n",
        "            hyetograph_df = pd.read_csv(filepath)\n",
        "            print(f\"Successfully read the hyetograph data from '{filename}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading the hyetograph CSV file '{filename}': {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Plot the hyetograph\n",
        "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
        "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
        "    \n",
        "    # Customize the plot\n",
        "    plt.xlabel('Time (Hour)', fontsize=14)\n",
        "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
        "    plt.title(f'Comparison of Hyetographs for ARIs {aris}\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 24)))  # Adjust x-ticks based on duration\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate AEP Hydrographs\n",
        "\n",
        "This cell orchestrates the entire AEP analysis process, generating hyetographs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Main function to run the entire AEP analysis process.\n",
        "\"\"\"\n",
        "# Set the paths and parameters\n",
        "input_csv = 'data/PF_Depth_English_PDS_DavisCA.csv'  # Path to NOAA Atlas 14 data\n",
        "output_dir = 'hyetographs'  # Directory for saving hyetographs\n",
        "position_percent = 50  # Position percentage for the maximum incremental depth block\n",
        "total_duration = 24  # Storm duration in hours\n",
        "base_plan = \"02\"  # Base plan to clone\n",
        "\n",
        "# Set the AEP events (return periods in years)\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "\n",
        "# Ensure the output directory exists\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory is set to: {output_dir}\")\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "# Step 1: Generate hyetographs for each AEP event\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 1: Generating hyetographs for each AEP event...\")\n",
        "\n",
        "try:\n",
        "    # Read precipitation data\n",
        "    df = read_precipitation_data(input_csv)\n",
        "    print(\"Successfully read the input CSV file.\")\n",
        "    \n",
        "    # Display the first few rows of the DataFrame to verify\n",
        "    print(\"\\nPrecipitation Frequency Data from Atlas 14:\")\n",
        "    display.display(df.head())\n",
        "    \n",
        "    # Interpolate depths\n",
        "    D = interpolate_depths(df, total_duration)\n",
        "    print(\"Successfully interpolated precipitation depths.\")\n",
        "\n",
        "    print(\"Array D with interpolated depths\")\n",
        "    display.display(D)\n",
        "    \n",
        "    # Compute incremental depths\n",
        "    inc_depths = compute_incremental_depths(D, total_duration)\n",
        "    print(\"Successfully computed incremental depths.\")\n",
        "    \n",
        "    # Show Incremental Depths\n",
        "    print(\"Array inc_depths Contents \")\n",
        "    display.display(inc_depths)\n",
        "\n",
        "    # Generate and save hyetographs for each AEP\n",
        "    hyetograph_files = {}\n",
        "    for ari in aep_events:\n",
        "        ari_str = str(ari)\n",
        "        if ari_str in inc_depths:\n",
        "            hyetograph = generate_hyetograph(inc_depths[ari_str], position_percent, total_duration)\n",
        "            file_path = save_hyetograph(hyetograph, ari_str, output_dir, position_percent, total_duration)\n",
        "            hyetograph_files[ari_str] = file_path\n",
        "        else:\n",
        "            print(f\"Warning: ARI {ari_str} not found in the data. Skipping.\")\n",
        "    \n",
        "    print(\"\\nAll hyetographs have been generated and saved.\")\n",
        "    \n",
        "    # Plot the hyetographs for comparison\n",
        "    plot_multiple_hyetographs(aep_events, position_percent, total_duration, output_dir)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error generating hyetographs: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Initialize the HEC-RAS project\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 2: Initializing the HEC-RAS ras...\")\n",
        "\n",
        "# Define the path to the Davis project\n",
        "current_dir = Path.cwd()\n",
        "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
        "\n",
        "# Check if the project exists\n",
        "if not pipes_ex_path.exists():\n",
        "    # Extract the project if needed\n",
        "    RasExamples.extract_project([\"Davis\"])\n",
        "\n",
        "# Initialize the RAS project\n",
        "init_ras_project(pipes_ex_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the existing plans\n",
        "print(\"\\nExisting plans in the project:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Display unsteady_df - note: geometry_number column has been REMOVED\n# This column was incorrectly appearing before; it only belongs in plan_df\nprint(\"Columns in unsteady_df:\")\nprint(list(ras.unsteady_df.columns))\nprint()\n\n# Verify geometry_number is NOT in unsteady_df\nif 'geometry_number' not in ras.unsteady_df.columns:\n    print(\"CORRECT: geometry_number is NOT in unsteady_df (only in plan_df)\")\nelse:\n    print(\"WARNING: geometry_number still in unsteady_df\")\nprint()\n\nras.unsteady_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Display boundaries_df - Precipitation Hydrograph is now parsed by default!\n# The hydrograph_values column contains the actual precipitation data\nprint(f\"Found {len(ras.boundaries_df)} boundary conditions:\")\nprint()\n\n# Show key columns for boundary conditions\ndisplay_cols = ['unsteady_number', 'boundary_condition_number', 'bc_type', \n                'storage_area_name', 'hydrograph_type', 'hydrograph_num_values', 'Interval']\nprint(ras.boundaries_df[display_cols])\nprint()\n\n# Get Precipitation Hydrograph data directly from boundaries_df\nprecip_bcs = ras.boundaries_df[ras.boundaries_df['bc_type'] == 'Precipitation Hydrograph']\nif not precip_bcs.empty:\n    print(f\"Found {len(precip_bcs)} Precipitation Hydrograph boundary condition(s):\")\n    for idx, row in precip_bcs.iterrows():\n        print(f\"\\n  Boundary #{row['boundary_condition_number']}:\")\n        print(f\"    Storage Area: {row['storage_area_name']}\")\n        print(f\"    Interval: {row['Interval']}\")\n        print(f\"    Number of values: {row['hydrograph_num_values']}\")\n        \n        # Get the hydrograph values directly from boundaries_df\n        if 'hydrograph_values' in row and row['hydrograph_values']:\n            values = row['hydrograph_values']\n            print(f\"    Hydrograph values: {values}\")\n            \n            # Convert to numeric for analysis\n            numeric_values = [float(v) for v in values]\n            print(f\"    Min: {min(numeric_values):.4f}, Max: {max(numeric_values):.4f}, Sum: {sum(numeric_values):.4f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Unsteady Number \"01\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"01\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsteady_file"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "### Alternative Method: Using RasUnsteady.extract_boundary_and_tables()\n\nThe cell below demonstrates an alternative way to extract boundary conditions and their table data. This method provides more detailed control and returns nested DataFrames in the Tables column.\n\n```python\n# Extract boundary conditions and tables - includes Precipitation Hydrograph data\nboundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\nprint(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")\n\n# Show which boundaries have table data\nfor idx, row in boundaries_df.iterrows():\n    tables = row.get('Tables', {})\n    storage_area = row.get('Storage Area Name', 'N/A')\n    print(f\"Boundary {idx + 1} (Storage Area: {storage_area}):\")\n    if tables:\n        for table_name, table_df in tables.items():\n            print(f\"  - {table_name}: {len(table_df)} values\")\n            print(f\"    Values: {table_df['Value'].tolist()}\")\n    else:\n        print(f\"  - No table data (likely a Normal Depth or other non-hydrograph BC)\")\n```"
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "### Alternative Method: Using RasUnsteady.extract_tables()\n\nThis method extracts all tables from an unsteady file as a dictionary of DataFrames. It's useful when you need just the raw table data without boundary location information.\n\n```python\n# Use RasUnsteady.extract_tables() to get detailed precipitation hydrograph values\ntables = RasUnsteady.extract_tables(unsteady_file)\nprint(f\"Tables found in unsteady file:\")\nfor table_name, df in tables.items():\n    print(f\"  - {table_name}: {len(df)} values\")\n\n# Display the Precipitation Hydrograph values\nif 'Precipitation Hydrograph=' in tables:\n    print(\"\\nPrecipitation Hydrograph Values (inches):\")\n    precip_df = tables['Precipitation Hydrograph=']\n    print(f\"  Total values: {len(precip_df)}\")\n    print(f\"  Values: {precip_df['Value'].tolist()}\")\n```"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the contents of Unsteady File\n",
        "with open(unsteady_file, 'r') as f:\n",
        "    unsteady_contents = f.read()\n",
        "print(f\"Contents of unsteady flow file {unsteady_file}:\")\n",
        "print(\"-\" * 80)\n",
        "print(unsteady_contents)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To implement AEP event hydrographs, we will edit the Precipitation Hydrograph table\n",
        "We will need to edit both the number of values, as well as replacing the existing fixed-width table.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define HEC-RAS Plan and Unsteady Flow File Functions\n",
        "\n",
        "These functions handle creating HEC-RAS plan files and unsteady flow files for each AEP event. They apply the generated hyetographs to the boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_plan_for_aep(base_plan, aep_years, duration_hours, hyetograph_file, project):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a specific AEP event.\n",
        "    \"\"\"\n",
        "    # Create plan name and short ID\n",
        "    plan_name = f\"{aep_years}YR-{duration_hours}HR\"\n",
        "    \n",
        "    print(f\"Creating new plan '{plan_name}'...\")\n",
        "    \n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_shortid=plan_name, ras_object=project)\n",
        "    print(f\"Created new plan: {new_plan_number}\")\n",
        "    \n",
        "    # Clone the unsteady flow file from the base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            \n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    \n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "    print(f\"Created new unsteady flow file: {new_unsteady_number}\")\n",
        "    \n",
        "    # Update the unsteady flow file with the hyetograph data\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "    \n",
        "    \n",
        "    # Update the flow title to reflect the AEP event\n",
        "    new_title = f\"{aep_years}YR-{duration_hours}HR Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "    print(f\"Updated unsteady flow title to: {new_title}\")\n",
        "    \n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    success = modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, project)\n",
        "    if success:\n",
        "        print(f\"Successfully applied hyetograph data from {hyetograph_file} to unsteady flow file\")\n",
        "    else:\n",
        "        print(f\"Warning: Failed to apply hyetograph data. Unsteady flow file may need manual modification.\")\n",
        "    \n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "    print(f\"Assigned unsteady flow file {new_unsteady_number} to plan {new_plan_number}\")\n",
        "    '''\n",
        "    # Update the plan description\n",
        "    description = f\"AEP {aep_years}-year, {duration_hours}-hour storm\\n\"\n",
        "    description += f\"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "    description += f\"Based on plan {base_plan}\\n\"\n",
        "    description += f\"Hyetograph from: {os.path.basename(hyetograph_file)}\"\n",
        "    \n",
        "    RasPlan.update_plan_description(new_plan_number, description, ras_object=project)\n",
        "    print(f\"Updated plan description for plan {new_plan_number}\")\n",
        "    \n",
        "    return new_plan_number, new_unsteady_number\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, project):\n",
        "    \"\"\"\n",
        "    Modifies an unsteady flow file to incorporate hyetograph data as precipitation.\n",
        "    \n",
        "    Parameters:\n",
        "    - unsteady_file_path: Path to the unsteady flow file\n",
        "    - hyetograph_file: Path to the hyetograph data CSV\n",
        "    - project: RAS project object\n",
        "    \n",
        "    Returns:\n",
        "    - Boolean indicating success\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the hyetograph data\n",
        "        hyetograph_df = pd.read_csv(hyetograph_file)\n",
        "        print(f\"Loaded hyetograph from {hyetograph_file} with {len(hyetograph_df)} values\")\n",
        "        \n",
        "        # Read the unsteady flow file\n",
        "        with open(unsteady_file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        \n",
        "        # Find the sections that need to be modified\n",
        "        precip_hydrograph_index = None\n",
        "        \n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Precipitation Hydrograph=\"):\n",
        "                precip_hydrograph_index = i\n",
        "                break\n",
        "        \n",
        "        if precip_hydrograph_index is None:\n",
        "            print(\"Cannot find Precipitation Hydrograph section in unsteady file.\")\n",
        "            return False\n",
        "        \n",
        "        # Get the time interval from the hyetograph\n",
        "        time_interval = \"1HOUR\"  # Default\n",
        "        if \"Time_hour\" in hyetograph_df.columns and len(hyetograph_df) > 1:\n",
        "            hour_diff = hyetograph_df[\"Time_hour\"].iloc[1] - hyetograph_df[\"Time_hour\"].iloc[0]\n",
        "            time_interval = f\"{int(hour_diff)}HOUR\" if hour_diff >= 1 else f\"{int(hour_diff*60)}MIN\"\n",
        "        \n",
        "        # Format the precipitation values for the hydrograph\n",
        "        precipitation_values = hyetograph_df[\"Precipitation_in\"].values\n",
        "        \n",
        "        # Create the Precipitation Hydrograph line\n",
        "        precip_line = f\"Precipitation Hydrograph= {len(precipitation_values)} \\n\"\n",
        "        \n",
        "        # Format the values in groups of 10 per line\n",
        "        value_lines = []\n",
        "        for i in range(0, len(precipitation_values), 10):\n",
        "            row_values = precipitation_values[i:i+10]\n",
        "            row_line = \"\".join([f\"{value:8.2f}\" for value in row_values]) + \"\\n\"\n",
        "            value_lines.append(row_line)\n",
        "            \n",
        "        # Remove old hydrograph data - find end of current hydrograph\n",
        "        current_line = precip_hydrograph_index + 1\n",
        "        while current_line < len(lines) and not any(lines[current_line].startswith(prefix) for prefix in [\"DSS Path=\", \"Use DSS=\", \"Use Fixed Start Time=\"]):\n",
        "            current_line += 1\n",
        "            \n",
        "        # Replace the hydrograph section\n",
        "        lines[precip_hydrograph_index:current_line] = [precip_line] + value_lines\n",
        "            \n",
        "        # Write the modified file back\n",
        "        with open(unsteady_file_path, 'w') as file:\n",
        "            file.writelines(lines)\n",
        "            \n",
        "        print(f\"Successfully applied hyetograph data from {hyetograph_file} to unsteady flow file.\")\n",
        "        print(f\"Added {len(precipitation_values)} precipitation values with interval {time_interval}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error modifying unsteady flow file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Create new plans for each AEP event\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 3: Creating new plans for each AEP event...\")\n",
        "\n",
        "new_plan_numbers = []\n",
        "\n",
        "for ari in aep_events:\n",
        "    ari_str = str(ari)\n",
        "    if ari_str in hyetograph_files:\n",
        "        try:\n",
        "            # Create a new plan for this AEP event\n",
        "            new_plan_number, _ = create_plan_for_aep(\n",
        "                base_plan=base_plan,\n",
        "                aep_years=ari_str,\n",
        "                duration_hours=total_duration,\n",
        "                hyetograph_file=hyetograph_files[ari_str],\n",
        "                project=ras\n",
        "            )\n",
        "            new_plan_numbers.append(new_plan_number)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plan for AEP {ari_str}: {e}\")\n",
        "\n",
        "# Display the updated plans\n",
        "print(\"\\nUpdated plans in the project:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Unsteady Number \"02\"\n",
        "unsteady_file_rev = RasPlan.get_unsteady_path(\"02\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file_rev}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsteady_file_rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(unsteady_file_rev, 'r') as f:\n",
        "    unsteady_contents_rev = f.read()\n",
        "print(f\"Contents of unsteady flow file for plan 02 ({unsteady_file_rev}):\")\n",
        "print(\"-\" * 80)\n",
        "print(unsteady_contents_rev)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import difflib\n",
        "\n",
        "def show_file_diff(file1_path, file2_path, context_lines=3):\n",
        "    \"\"\"\n",
        "    Shows the differences between two files with context.\n",
        "    \n",
        "    Parameters:\n",
        "    - file1_path: Path to the first file\n",
        "    - file2_path: Path to the second file\n",
        "    - context_lines: Number of context lines to show around differences\n",
        "    \"\"\"\n",
        "    # Read the file contents\n",
        "    with open(file1_path, 'r') as file1:\n",
        "        file1_lines = file1.readlines()\n",
        "    \n",
        "    with open(file2_path, 'r') as file2:\n",
        "        file2_lines = file2.readlines()\n",
        "    \n",
        "    # Create a differ object\n",
        "    differ = difflib.unified_diff(\n",
        "        file1_lines, \n",
        "        file2_lines,\n",
        "        fromfile=str(file1_path),\n",
        "        tofile=str(file2_path),\n",
        "        n=context_lines\n",
        "    )\n",
        "    \n",
        "    # Convert differ output to a string\n",
        "    diff_text = ''.join(differ)\n",
        "    \n",
        "    # If no differences found\n",
        "    if not diff_text:\n",
        "        print(f\"No differences found between {file1_path} and {file2_path}\")\n",
        "        return\n",
        "    \n",
        "    # Print the differences\n",
        "    print(f\"Differences between {file1_path} and {file2_path}:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(diff_text)\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Count added, removed, and modified lines\n",
        "    added = sum(1 for line in diff_text.splitlines() if line.startswith('+') and not line.startswith('+++'))\n",
        "    removed = sum(1 for line in diff_text.splitlines() if line.startswith('-') and not line.startswith('---'))\n",
        "    \n",
        "    print(f\"Summary: {added} additions, {removed} removals\")\n",
        "\n",
        "# Show differences between the unsteady flow files\n",
        "if 'unsteady_file' in locals() and 'unsteady_file_rev' in locals():\n",
        "    show_file_diff(unsteady_file, unsteady_file_rev)\n",
        "else:\n",
        "    print(\"Error: One or both unsteady flow file variables not defined.\")\n",
        "    print(\"Please run the cells that define unsteady_file and unsteady_file_rev first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Parallel Execution and Results Analysis Functions\n",
        "\n",
        "These functions manage parallel plan execution with resource optimization and extract, analyze, and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_plans_in_parallel(plan_numbers, project, max_workers=None, cores_per_worker=2):\n",
        "    \"\"\"\n",
        "    Executes multiple plans in parallel.\n",
        "    \"\"\"\n",
        "    # Calculate optimal number of workers if not provided\n",
        "    if max_workers is None:\n",
        "        physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
        "        max_workers = max(1, physical_cores // cores_per_worker)\n",
        "    \n",
        "    print(f\"Executing {len(plan_numbers)} plans in parallel with {max_workers} workers, \" + \n",
        "          f\"each using {cores_per_worker} cores...\")\n",
        "    \n",
        "    # Create compute folder\n",
        "    compute_folder = Path(project.project_folder) / \"compute_aep_parallel\"\n",
        "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Execute plans in parallel\n",
        "    start_time = time.time()\n",
        "    \n",
        "    results = RasCmdr.compute_parallel(\n",
        "        plan_number=plan_numbers,\n",
        "        max_workers=max_workers,\n",
        "        num_cores=cores_per_worker,\n",
        "        dest_folder=compute_folder,\n",
        "        clear_geompre=True,\n",
        "        overwrite_dest=True,\n",
        "        ras_object=project\n",
        "    )\n",
        "    \n",
        "    end_time = time.time()\n",
        "    total_duration = end_time - start_time\n",
        "    \n",
        "    print(f\"Parallel execution completed in {total_duration:.2f} seconds\")\n",
        "    \n",
        "    # Create a DataFrame from the execution results\n",
        "    results_df = pd.DataFrame([\n",
        "        {\"Plan\": plan, \"Success\": success}\n",
        "        for plan, success in results.items()\n",
        "    ])\n",
        "    \n",
        "    # Sort by plan number\n",
        "    results_df = results_df.sort_values(\"Plan\")\n",
        "    \n",
        "    print(\"\\nExecution Results:\")\n",
        "    display.display(results_df)\n",
        "    \n",
        "    return results, compute_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"01\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"01\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Execute all plans in parallel\n",
        "#-------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set computation parameters for better performance\n",
        "for plan_number in new_plan_numbers:\n",
        "    RasPlan.set_num_cores(plan_number, 2, ras_object=ras)\n",
        "    RasPlan.update_plan_intervals(\n",
        "        plan_number,\n",
        "        computation_interval=\"15MIN\",\n",
        "        output_interval=\"30MIN\",\n",
        "        mapping_interval=\"1HOUR\",\n",
        "        ras_object=ras\n",
        "    )\n",
        "    print(f\"Updated computation settings for plan {plan_number}\")\n",
        "\n",
        "# Execute plans in parallel\n",
        "results, compute_folder = execute_plans_in_parallel(\n",
        "    plan_numbers=new_plan_numbers,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\nStep 4: Executing all plans in parallel...\")\n",
        "\n",
        "results, compute_folder = execute_plans_in_parallel(\n",
        "    plan_numbers=new_plan_numbers,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df = HdfResultsPlan.get_runtime_data(\"02\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(\"02\", variable=\"Pipes/Pipe Flow DS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws = HdfPipe.get_pipe_network_timeseries(\"02\", variable=\"Nodes/Water Surface\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_results(results, compute_folder, project):\n",
        "    \"\"\"\n",
        "    Analyzes the results from multiple plans.\n",
        "    \"\"\"\n",
        "    print(\"Analyzing results from parallel execution...\")\n",
        "    \n",
        "    # Initialize a RAS project in the compute folder\n",
        "    compute_project = RasPrj()\n",
        "    compute_project = init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "    print(f\"Initialized compute project: {compute_project.project_name}\")\n",
        "    \n",
        "    # Check which plans have results\n",
        "    plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "    print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
        "    display.display(plans_with_results[['plan_number', 'Short Identifier', 'HDF_Results_Path']])\n",
        "    \n",
        "    # Initialize a dictionary to store analysis results\n",
        "    analysis_results = {}\n",
        "    \n",
        "    # Analyze each plan's results\n",
        "    for idx, row in plans_with_results.iterrows():\n",
        "        plan_number = row['plan_number']\n",
        "        plan_name = row['Short Identifier']\n",
        "        hdf_path = row['HDF_Results_Path']\n",
        "        \n",
        "        print(f\"\\nAnalyzing results for plan {plan_number} ({plan_name})...\")\n",
        "        \n",
        "        try:\n",
        "            # Get runtime data\n",
        "            runtime_df = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "            \n",
        "            if runtime_df is not None and not runtime_df.empty:\n",
        "                # Extract key metrics\n",
        "                sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
        "                compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
        "                compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
        "                \n",
        "                # Get pipe network results\n",
        "                try:\n",
        "                    # Get pipe flow data\n",
        "                    pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Pipes/Pipe Flow DS\")\n",
        "                    node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "                    \n",
        "                    # Convert xarray DataArrays to numpy arrays and compute statistics\n",
        "                    pipe_flow_array = pipe_flow_ds.values\n",
        "                    node_ws_array = node_ws.values\n",
        "                    \n",
        "                    # Calculate maximum flows and water surfaces\n",
        "                    max_flows = np.nanmax(pipe_flow_array, axis=0)  # Max over time for each location\n",
        "                    avg_max_flow = np.nanmean(max_flows)\n",
        "                    max_max_flow = np.nanmax(max_flows)\n",
        "                    \n",
        "                    max_ws = np.nanmax(node_ws_array, axis=0)  # Max over time for each node\n",
        "                    avg_max_ws = np.nanmean(max_ws)\n",
        "                    max_max_ws = np.nanmax(max_ws)\n",
        "                    \n",
        "                    # Store results in the dictionary\n",
        "                    analysis_results[plan_name] = {\n",
        "                        'Plan Number': plan_number,\n",
        "                        'Simulation Duration (s)': sim_duration,\n",
        "                        'Compute Time (hr)': compute_time,\n",
        "                        'Compute Speed (hr/hr)': compute_speed,\n",
        "                        'Average Max Pipe Flow (cfs)': avg_max_flow,\n",
        "                        'Maximum Pipe Flow (cfs)': max_max_flow,\n",
        "                        'Average Max Node Water Surface (ft)': avg_max_ws,\n",
        "                        'Maximum Node Water Surface (ft)': max_max_ws,\n",
        "                        'HDF Path': hdf_path\n",
        "                    }\n",
        "                    \n",
        "                    print(f\"  Simulation Duration: {sim_duration:.2f} seconds\")\n",
        "                    print(f\"  Computation Time: {compute_time:.5f} hours\")\n",
        "                    print(f\"  Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
        "                    print(f\"  Average Max Pipe Flow: {avg_max_flow:.2f} cfs\")\n",
        "                    print(f\"  Maximum Pipe Flow: {max_max_flow:.2f} cfs\")\n",
        "                    print(f\"  Average Max Node Water Surface: {avg_max_ws:.2f} ft\")\n",
        "                    print(f\"  Maximum Node Water Surface: {max_max_ws:.2f} ft\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"  Error analyzing pipe network data: {str(e)}\")\n",
        "                    analysis_results[plan_name] = {\n",
        "                        'Plan Number': plan_number,\n",
        "                        'Simulation Duration (s)': sim_duration,\n",
        "                        'Compute Time (hr)': compute_time,\n",
        "                        'Compute Speed (hr/hr)': compute_speed,\n",
        "                        'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                        'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                        'Average Max Node Water Surface (ft)': np.nan,\n",
        "                        'Maximum Node Water Surface (ft)': np.nan,\n",
        "                        'HDF Path': hdf_path\n",
        "                    }\n",
        "            else:\n",
        "                print(\"  No runtime data found.\")\n",
        "                analysis_results[plan_name] = {\n",
        "                    'Plan Number': plan_number,\n",
        "                    'Simulation Duration (s)': np.nan,\n",
        "                    'Compute Time (hr)': np.nan,\n",
        "                    'Compute Speed (hr/hr)': np.nan,\n",
        "                    'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                    'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                    'Average Max Node Water Surface (ft)': np.nan,\n",
        "                    'Maximum Node Water Surface (ft)': np.nan,\n",
        "                    'HDF Path': hdf_path\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error analyzing plan {plan_number}: {str(e)}\")\n",
        "            analysis_results[plan_name] = {\n",
        "                'Plan Number': plan_number,\n",
        "                'Simulation Duration (s)': np.nan,\n",
        "                'Compute Time (hr)': np.nan,\n",
        "                'Compute Speed (hr/hr)': np.nan,\n",
        "                'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                'Average Max Node Water Surface (ft)': np.nan,\n",
        "                'Maximum Node Water Surface (ft)': np.nan,\n",
        "                'HDF Path': hdf_path\n",
        "            }\n",
        "    \n",
        "    # Create a DataFrame from the analysis results\n",
        "    analysis_df = pd.DataFrame.from_dict(analysis_results, orient='index')\n",
        "    \n",
        "    # Extract AEP years and handle NaN values\n",
        "    analysis_df['AEP_Years'] = analysis_df.index.str.extract(r'(\\d+)YR').astype(float)\n",
        "    \n",
        "    # Sort by AEP years, handling the base plan\n",
        "    analysis_df = analysis_df.sort_values('AEP_Years', na_position='first')\n",
        "    \n",
        "    # Drop the temporary column used for sorting\n",
        "    analysis_df = analysis_df.drop(columns=['AEP_Years'])\n",
        "    \n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    display.display(analysis_df)\n",
        "    \n",
        "    return analysis_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------------\n",
        "# Step 5: Analyze the results\n",
        "#---------------------------------------------------------------------\n",
        "print(\"\\nStep 5: Analyzing the results...\")\n",
        "\n",
        "analysis_df = analyze_results(\n",
        "    results=results,\n",
        "    compute_folder=compute_folder,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_results(analysis_df):\n",
        "    \"\"\"\n",
        "    Plots the results from the analysis.\n",
        "    \"\"\"\n",
        "    # Extract AEP values from the index (plan names), skipping non-AEP plans\n",
        "    aep_values = []\n",
        "    aep_data = pd.DataFrame()\n",
        "    \n",
        "    for name in analysis_df.index:\n",
        "        if 'YR' in name:\n",
        "            try:\n",
        "                aep_year = int(name.split('YR')[0])\n",
        "                aep_values.append(aep_year)\n",
        "                aep_data = pd.concat([aep_data, analysis_df.loc[[name]]])\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    if len(aep_values) == 0:\n",
        "        print(\"No valid AEP plans found to plot\")\n",
        "        return\n",
        "        \n",
        "    # Create a figure with multiple subplots\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Plot 1: Maximum Pipe Flow vs AEP\n",
        "    axs[0].semilogx(aep_values, aep_data['Maximum Pipe Flow (cfs)'], 'o-', marker='o', markersize=8)\n",
        "    axs[0].set_title('Maximum Pipe Flow vs Return Period', fontsize=16)\n",
        "    axs[0].set_xlabel('Return Period (years)', fontsize=14)\n",
        "    axs[0].set_ylabel('Maximum Pipe Flow (cfs)', fontsize=14)\n",
        "    axs[0].grid(True)\n",
        "    \n",
        "    # Add data labels\n",
        "    for i, txt in enumerate(aep_values):\n",
        "        axs[0].annotate(f\"{txt} yr\", \n",
        "                      (aep_values[i], aep_data['Maximum Pipe Flow (cfs)'].iloc[i]),\n",
        "                      textcoords=\"offset points\", \n",
        "                      xytext=(0, 10), \n",
        "                      ha='center')\n",
        "    \n",
        "    # Plot 2: Maximum Node Water Surface vs AEP\n",
        "    axs[1].semilogx(aep_values, aep_data['Maximum Node Water Surface (ft)'], 'o-', marker='s', markersize=8, color='green')\n",
        "    axs[1].set_title('Maximum Node Water Surface vs Return Period', fontsize=16)\n",
        "    axs[1].set_xlabel('Return Period (years)', fontsize=14)\n",
        "    axs[1].set_ylabel('Maximum Node Water Surface (ft)', fontsize=14)\n",
        "    axs[1].grid(True)\n",
        "    \n",
        "    # Add data labels\n",
        "    for i, txt in enumerate(aep_values):\n",
        "        axs[1].annotate(f\"{txt} yr\", \n",
        "                      (aep_values[i], aep_data['Maximum Node Water Surface (ft)'].iloc[i]),\n",
        "                      textcoords=\"offset points\", \n",
        "                      xytext=(0, 10), \n",
        "                      ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Plot time series for each return period\n",
        "    try:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        \n",
        "        # Create a color map for different return periods\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(aep_values)))\n",
        "        \n",
        "        # Plot each return period\n",
        "        for i, name in enumerate(aep_data.index):\n",
        "            # Get HDF path for this return period\n",
        "            hdf_path = aep_data.loc[name, 'HDF Path']\n",
        "            \n",
        "            # Get pipe network timeseries data\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "            \n",
        "            # Get data for location 61\n",
        "            loc_61_ws = node_ws.sel(location=61)\n",
        "            \n",
        "            # Plot the time series\n",
        "            plt.plot(loc_61_ws.time.values, loc_61_ws.values, \n",
        "                    label=f'{name}', \n",
        "                    color=colors[i],\n",
        "                    linewidth=2)\n",
        "        \n",
        "        plt.title('Water Surface Elevation Time Series by Return Period - Location 61', fontsize=16)\n",
        "        plt.xlabel('Time', fontsize=14)\n",
        "        plt.ylabel('Water Surface Elevation (ft)', fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.legend(fontsize=12)\n",
        "        \n",
        "        # Format x-axis dates\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Could not create detailed heatmap: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------------\n",
        "# Step 6: Plot the results\n",
        "#---------------------------------------------------------------------\n",
        "print(\"\\nStep 6: Plotting the results...\")\n",
        "\n",
        "plot_results(analysis_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "node_ws = HdfPipe.get_pipe_network_timeseries(\"04\", variable=\"Nodes/Water Surface\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(\"04\", variable=\"Pipes/Pipe Flow DS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a comprehensive workflow for automated AEP storm analysis using RAS-Commander. The key benefits of this approach include:\n",
        "\n",
        "1. **Efficiency**: Automating repetitive tasks saves time and reduces errors\n",
        "2. **Consistency**: Ensures consistent methodology across all return periods\n",
        "3. **Parallel Execution**: Makes optimal use of computational resources\n",
        "4. **Comprehensive Analysis**: Extracts and visualizes key metrics across return periods\n",
        "5. **Reproducibility**: The entire workflow is documented and repeatable\n",
        "\n",
        "This approach can be extended to include additional analyses, such as:\n",
        "\n",
        "- Comparing different storm patterns (e.g., position of peak intensity)\n",
        "- Analyzing climate change scenarios by adjusting precipitation depths\n",
        "- Evaluating infrastructure improvements by comparing baseline and modified geometries\n",
        "- Generating frequency curves for key hydraulic parameters\n",
        "\n",
        "By leveraging the power of RAS-Commander, engineers can focus on interpreting results and making design decisions rather than managing model configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DEV NOTES: \n",
        "\n",
        "Need to add example of setting Start Time and End Time\n",
        "\n",
        "Need to add function to library that will end-to-end model Atlas 14 AEP storms given an input lat/long, Return Interval and Duration, given a working geometry w/infiltration.  Running optional.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\104_Atlas14_AEP_Multi_Project.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Atlas 14 Uncertainty Analysis with Multi-Project Support\n",
        "\n",
        "This notebook performs comprehensive uncertainty analysis of precipitation-driven flooding by:\n",
        "\n",
        "1. **Processing Multiple Durations**: Analyzes 1-hr, 2-hr, 3-hr, 6-hr, 12-hr, 24-hr, and 2-day storms\n",
        "2. **Including Confidence Intervals**: Runs upper and lower confidence bounds for each scenario\n",
        "3. **Quantifying Uncertainty**: Shows how precipitation uncertainty propagates through flood models\n",
        "4. **Comprehensive Visualization**: Creates confidence envelope plots and uncertainty heatmaps\n",
        "5. **Multi-Project Management**: Automatically handles HEC-RAS 99-plan limit by distributing scenarios across multiple project copies\n",
        "\n",
        "## Methodology\n",
        "\n",
        "**Confidence Interval Estimation:**\n",
        "- NOAA Atlas 14 precipitation estimates have inherent uncertainty\n",
        "- Upper confidence bound \u2248 1.4 \u00d7 point estimate\n",
        "- Lower confidence bound \u2248 0.7 \u00d7 point estimate\n",
        "- These factors represent approximate 90% confidence intervals\n",
        "\n",
        "**Scenario Matrix:**\n",
        "- 6 AEP events (2, 5, 10, 25, 50, 100 years)\n",
        "- 7 durations (1hr, 2hr, 3hr, 6hr, 12hr, 24hr, 2day)\n",
        "- 3 confidence levels (lower, point, upper)\n",
        "- **Total: 126 scenarios**\n",
        "\n",
        "**99-Plan Limit Solution:**\n",
        "- HEC-RAS projects limited to 99 plans maximum\n",
        "- Automated distribution across multiple project copies\n",
        "- Preserves original project (all work done in copies)\n",
        "- Results automatically aggregated from all projects\n",
        "\n",
        "**Analysis Outputs:**\n",
        "- Confidence envelopes for peak water surfaces\n",
        "- Uncertainty quantification by duration and location\n",
        "- Design recommendations considering uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "from IPython import display\n",
        "import psutil\n",
        "from itertools import product\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract and Initialize Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize project\n",
        "current_dir = Path.cwd()\n",
        "pipes_ex_path = current_dir / \"A14_Examples\" / \"Davis\"\n",
        "rasexamples_extract_path = current_dir / \"A14_Examples\"\n",
        "\n",
        "if not pipes_ex_path.exists():\n",
        "    RasExamples.extract_project([\"Davis\"], output_path=rasexamples_extract_path)\n",
        "\n",
        "init_ras_project(pipes_ex_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "print(f\"\\nBase plan configuration:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Hyetograph Generation with Confidence Intervals\n",
        "\n",
        "These functions extend the original hyetograph generation to support:\n",
        "- Multiple durations (not just 24-hour)\n",
        "- Confidence interval calculations\n",
        "- Systematic scenario organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_duration(duration_str):\n",
        "    \"\"\"\n",
        "    Parses a duration string and converts it to hours.\n",
        "    Examples: \"5-min:\" -> 0.0833 hours, \"2-hr:\" -> 2 hours, \"2-day:\" -> 48 hours\n",
        "    \"\"\"\n",
        "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
        "    if not match:\n",
        "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
        "    value, unit = match.groups()\n",
        "    value = int(value)\n",
        "    unit = unit.lower()\n",
        "    if unit in ['min', 'minute', 'minutes']:\n",
        "        hours = value / 60.0\n",
        "    elif unit in ['hr', 'hour', 'hours']:\n",
        "        hours = value\n",
        "    elif unit in ['day', 'days']:\n",
        "        hours = value * 24\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
        "    return hours\n",
        "\n",
        "def read_precipitation_data(csv_file):\n",
        "    \"\"\"\n",
        "    Reads the precipitation frequency CSV and returns a DataFrame\n",
        "    with durations in hours as the index and ARIs as columns.\n",
        "    \"\"\"\n",
        "    with open(csv_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    header_line_idx = None\n",
        "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
        "\n",
        "    # Locate the header line\n",
        "    for idx, line in enumerate(lines):\n",
        "        if header_pattern.match(line.strip().lower()):\n",
        "            header_line_idx = idx\n",
        "            break\n",
        "\n",
        "    if header_line_idx is None:\n",
        "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
        "\n",
        "    # Extract the ARI headers from the header line\n",
        "    header_line = lines[header_line_idx].strip()\n",
        "    headers = [item.strip() for item in header_line.split(',')]\n",
        "    \n",
        "    if len(headers) < 2:\n",
        "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
        "\n",
        "    aris = headers[1:]  # Exclude the first column which is the duration\n",
        "\n",
        "    # Define the pattern for data lines\n",
        "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
        "\n",
        "    # Initialize lists to store durations and corresponding depths\n",
        "    durations = []\n",
        "    depths = {ari: [] for ari in aris}\n",
        "\n",
        "    # Iterate over the lines following the header to extract data\n",
        "    for line in lines[header_line_idx + 1:]:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if not duration_pattern.match(line):\n",
        "            break\n",
        "        parts = [part.strip() for part in line.split(',')]\n",
        "        if len(parts) != len(headers):\n",
        "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
        "        duration_str = parts[0]\n",
        "        try:\n",
        "            duration_hours = parse_duration(duration_str)\n",
        "        except ValueError as ve:\n",
        "            print(f\"Skipping line due to error: {ve}\")\n",
        "            continue\n",
        "        durations.append(duration_hours)\n",
        "        for ari, depth_str in zip(aris, parts[1:]):\n",
        "            try:\n",
        "                depth = float(depth_str)\n",
        "            except ValueError:\n",
        "                depth = np.nan\n",
        "            depths[ari].append(depth)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(depths, index=durations)\n",
        "    df.index.name = 'Duration_hours'\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_confidence_intervals(df_point, upper_factor=1.4, lower_factor=0.7):\n",
        "    \"\"\"\n",
        "    Creates upper and lower confidence interval DataFrames from point estimates.\n",
        "    \n",
        "    Parameters:\n",
        "    - df_point: DataFrame with point estimates\n",
        "    - upper_factor: Multiplier for upper CI (default 1.4 for ~90% CI)\n",
        "    - lower_factor: Multiplier for lower CI (default 0.7 for ~90% CI)\n",
        "    \n",
        "    Returns:\n",
        "    - Tuple of (df_lower, df_point, df_upper)\n",
        "    \"\"\"\n",
        "    df_upper = df_point * upper_factor\n",
        "    df_lower = df_point * lower_factor\n",
        "    \n",
        "    return df_lower, df_point, df_upper\n",
        "\n",
        "def get_time_interval(duration_hrs):\n",
        "    \"\"\"\n",
        "    Determines the appropriate time interval based on storm duration.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    duration_hrs : float\n",
        "        Storm duration in hours\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    float : Time interval in hours\n",
        "    \"\"\"\n",
        "    if duration_hrs >= 24:\n",
        "        return 1.0  # 1 hour for 24+ hour storms\n",
        "    elif duration_hrs >= 12:\n",
        "        return 0.5  # 30 minutes for 12-hour storms\n",
        "    elif duration_hrs >= 6:\n",
        "        return 0.25  # 15 minutes for 6-hour storms\n",
        "    else:\n",
        "        return 5.0 / 60.0  # 5 minutes for storms less than 6 hours\n",
        "\n",
        "def interpolate_depths(df, total_duration):\n",
        "    \"\"\"\n",
        "    Interpolates precipitation depths for each ARI on a log-log scale\n",
        "    using appropriate time intervals based on duration.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Precipitation frequency data with durations as index and ARIs as columns\n",
        "    total_duration : float\n",
        "        Total storm duration in hours\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (dict of interpolated depths, time array in hours)\n",
        "    \"\"\"\n",
        "    # Determine time interval based on duration\n",
        "    dt = get_time_interval(total_duration)\n",
        "    \n",
        "    # Create time array with appropriate interval\n",
        "    t_hours = np.arange(dt, total_duration + dt/2, dt)\n",
        "    \n",
        "    D = {}\n",
        "    for ari in df.columns:\n",
        "        durations = df.index.values\n",
        "        depths = df[ari].values\n",
        "        if np.any(depths <= 0):\n",
        "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
        "        \n",
        "        # Log-log interpolation\n",
        "        log_durations = np.log(durations)\n",
        "        log_depths = np.log(depths)\n",
        "        log_t = np.log(t_hours)\n",
        "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
        "        D_t = np.exp(log_D_t)\n",
        "        D[ari] = D_t\n",
        "    \n",
        "    return D, t_hours\n",
        "\n",
        "def compute_incremental_depths(D, t_hours):\n",
        "    \"\"\"\n",
        "    Computes incremental precipitation depths for each time interval.\n",
        "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    D : dict\n",
        "        Dictionary of cumulative depths for each ARI\n",
        "    t_hours : array\n",
        "        Time array in hours\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary of incremental depths for each ARI\n",
        "    \"\"\"\n",
        "    incremental_depths = {}\n",
        "    for ari, D_t in D.items():\n",
        "        num_intervals = len(t_hours)\n",
        "        I_t = np.empty(num_intervals)\n",
        "        I_t[0] = D_t[0]\n",
        "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
        "        incremental_depths[ari] = I_t\n",
        "    return incremental_depths\n",
        "\n",
        "def assign_alternating_block(sorted_depths, max_depth, central_index, num_intervals):\n",
        "    \"\"\"\n",
        "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sorted_depths : list\n",
        "        Sorted incremental depths (descending)\n",
        "    max_depth : float\n",
        "        Maximum depth value\n",
        "    central_index : int\n",
        "        Index for peak position\n",
        "    num_intervals : int\n",
        "        Total number of time intervals\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    list : Hyetograph array\n",
        "    \"\"\"\n",
        "    hyetograph = [0.0] * num_intervals\n",
        "    hyetograph[central_index] = max_depth\n",
        "    remaining_depths = sorted_depths.copy()\n",
        "    remaining_depths.remove(max_depth)\n",
        "    left = central_index - 1\n",
        "    right = central_index + 1\n",
        "    toggle = True\n",
        "    for depth in remaining_depths:\n",
        "        if toggle and right < num_intervals:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif not toggle and left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        elif right < num_intervals:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        else:\n",
        "            print(\"Warning: Not all incremental depths assigned.\")\n",
        "            break\n",
        "        toggle = not toggle\n",
        "    return hyetograph\n",
        "\n",
        "def generate_hyetograph(incremental_depths, position_percent, num_intervals):\n",
        "    \"\"\"\n",
        "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    incremental_depths : array\n",
        "        Incremental depths for each time interval\n",
        "    position_percent : float\n",
        "        Peak position as percentage (e.g., 50 for middle)\n",
        "    num_intervals : int\n",
        "        Total number of time intervals\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    list : Hyetograph array\n",
        "    \"\"\"\n",
        "    max_depth = np.max(incremental_depths)\n",
        "    incremental_depths_list = incremental_depths.tolist()\n",
        "    central_index = int(round(num_intervals * position_percent / 100)) - 1\n",
        "    central_index = max(0, min(central_index, num_intervals - 1))\n",
        "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
        "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, num_intervals)\n",
        "    return hyetograph\n",
        "\n",
        "def save_hyetograph(hyetograph, t_hours, ari, duration_hrs, ci_level, output_dir, position_percent):\n",
        "    \"\"\"\n",
        "    Saves the hyetograph to a CSV file with clear naming convention.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    hyetograph : array\n",
        "        Hyetograph values\n",
        "    t_hours : array\n",
        "        Time array in hours\n",
        "    ari : int or str\n",
        "        Annual recurrence interval\n",
        "    duration_hrs : float\n",
        "        Storm duration in hours\n",
        "    ci_level : str\n",
        "        Confidence level ('lower', 'point', 'upper')\n",
        "    output_dir : str\n",
        "        Output directory path\n",
        "    position_percent : float\n",
        "        Peak position percentage\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    str : Path to saved file\n",
        "    \"\"\"\n",
        "    # Determine time interval\n",
        "    dt = get_time_interval(duration_hrs)\n",
        "    \n",
        "    # Create DataFrame with appropriate time units\n",
        "    if dt >= 1.0:\n",
        "        time_col_name = 'Time_hour'\n",
        "        time_values = t_hours\n",
        "    elif dt >= 1.0/60.0:\n",
        "        time_col_name = 'Time_min'\n",
        "        time_values = t_hours * 60  # Convert to minutes\n",
        "    else:\n",
        "        time_col_name = 'Time_hour'\n",
        "        time_values = t_hours\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        time_col_name: time_values,\n",
        "        'Precipitation_in': hyetograph\n",
        "    })\n",
        "    \n",
        "    # Format duration string\n",
        "    if duration_hrs >= 24:\n",
        "        dur_str = f\"{int(duration_hrs/24)}day\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration_hrs)}hr\"\n",
        "    \n",
        "    filename = f'hyetograph_ARI_{ari}_DUR_{dur_str}_CI_{ci_level}.csv'\n",
        "    output_file = os.path.join(output_dir, filename)\n",
        "    \n",
        "    # Add metadata as header comment\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(f\"# NOAA Atlas 14 Hyetograph\\n\")\n",
        "        f.write(f\"# ARI: {ari} years\\n\")\n",
        "        f.write(f\"# Duration: {duration_hrs} hours\\n\")\n",
        "        f.write(f\"# Time Interval: {dt*60:.1f} minutes\\n\")\n",
        "        f.write(f\"# Confidence Level: {ci_level}\\n\")\n",
        "        f.write(f\"# Peak Position: {position_percent}%\\n\")\n",
        "        f.write(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        df.to_csv(f, index=False)\n",
        "    \n",
        "    return output_file\n",
        "\n",
        "print(\"Hyetograph generation functions defined (with variable time intervals)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Complete Scenario Matrix\n",
        "\n",
        "Create hyetographs for all combinations of:\n",
        "- AEP events: 2, 5, 10, 25, 50, 100 years\n",
        "- Durations: 1hr, 2hr, 3hr, 6hr, 12hr, 24hr, 2day\n",
        "- Confidence levels: lower, point, upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "input_csv = 'data/PF_Depth_English_PDS_DavisCA.csv'\n",
        "output_dir = 'hyetographs_uncertainty'\n",
        "position_percent = 50\n",
        "base_plan = \"02\"\n",
        "\n",
        "# Define scenario parameters\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "durations = [1, 2, 3, 6, 12, 24, 48]  # hours\n",
        "ci_levels = ['lower', 'point', 'upper']\n",
        "ci_factors = {'lower': 0.7, 'point': 1.0, 'upper': 1.4}\n",
        "\n",
        "# Create output directory\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "print(f\"\\nScenario Matrix:\")\n",
        "print(f\"  AEP Events: {aep_events}\")\n",
        "print(f\"  Durations: {durations} hours\")\n",
        "print(f\"  CI Levels: {ci_levels}\")\n",
        "print(f\"  Total Scenarios: {len(aep_events) * len(durations) * len(ci_levels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read precipitation data\n",
        "print(\"Reading NOAA Atlas 14 data...\")\n",
        "df_point = read_precipitation_data(input_csv)\n",
        "print(f\"Successfully read data with {len(df_point)} durations and {len(df_point.columns)} ARI values\")\n",
        "\n",
        "# Display the data\n",
        "print(\"\\nPrecipitation Frequency Data (Point Estimates):\")\n",
        "display.display(df_point.head(10))\n",
        "\n",
        "# Create confidence interval DataFrames\n",
        "print(\"\\nGenerating confidence intervals...\")\n",
        "df_lower, df_point, df_upper = create_confidence_intervals(df_point)\n",
        "print(\"Confidence intervals created:\")\n",
        "print(f\"  Lower CI factor: {ci_factors['lower']}\")\n",
        "print(f\"  Point estimate factor: {ci_factors['point']}\")\n",
        "print(f\"  Upper CI factor: {ci_factors['upper']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all hyetographs\n",
        "print(\"Generating hyetographs with variable time intervals...\\n\")\n",
        "print(\"Time Interval Rules:\")\n",
        "print(\"  \u2022 24+ hour storms: 1-hour intervals\")\n",
        "print(\"  \u2022 12-hour storms: 30-minute intervals\")\n",
        "print(\"  \u2022 6-hour storms: 15-minute intervals\")\n",
        "print(\"  \u2022 < 6-hour storms: 5-minute intervals\\n\")\n",
        "\n",
        "scenario_list = []\n",
        "hyetograph_count = 0\n",
        "\n",
        "for ari in aep_events:\n",
        "    ari_str = str(ari)\n",
        "    \n",
        "    # Check if this ARI is in the data\n",
        "    if ari_str not in df_point.columns:\n",
        "        print(f\"Warning: ARI {ari_str} not found in data. Skipping.\")\n",
        "        continue\n",
        "    \n",
        "    for duration in durations:\n",
        "        # Get time interval for this duration\n",
        "        dt = get_time_interval(duration)\n",
        "        print(f\"Processing {ari}-yr, {duration}-hr storm (dt={dt*60:.1f} min)...\")\n",
        "        \n",
        "        # Process each confidence level\n",
        "        for ci_level in ci_levels:\n",
        "            # Select the appropriate DataFrame\n",
        "            if ci_level == 'lower':\n",
        "                df_current = df_lower\n",
        "            elif ci_level == 'upper':\n",
        "                df_current = df_upper\n",
        "            else:\n",
        "                df_current = df_point\n",
        "            \n",
        "            # Interpolate depths with appropriate time interval\n",
        "            D, t_hours = interpolate_depths(df_current, duration)\n",
        "            \n",
        "            # Compute incremental depths\n",
        "            inc_depths = compute_incremental_depths(D, t_hours)\n",
        "            \n",
        "            # Generate hyetograph\n",
        "            num_intervals = len(t_hours)\n",
        "            hyetograph = generate_hyetograph(inc_depths[ari_str], position_percent, num_intervals)\n",
        "            \n",
        "            # Save hyetograph\n",
        "            file_path = save_hyetograph(hyetograph, t_hours, ari_str, duration, ci_level, \n",
        "                                       output_dir, position_percent)\n",
        "            \n",
        "            # Record scenario\n",
        "            scenario_list.append({\n",
        "                'ari': ari,\n",
        "                'duration_hrs': duration,\n",
        "                'time_interval_min': dt * 60,\n",
        "                'num_intervals': num_intervals,\n",
        "                'ci_level': ci_level,\n",
        "                'hyetograph_file': file_path,\n",
        "                'total_depth_in': sum(hyetograph)\n",
        "            })\n",
        "            \n",
        "            hyetograph_count += 1\n",
        "\n",
        "# Create scenario DataFrame\n",
        "scenario_df = pd.DataFrame(scenario_list)\n",
        "\n",
        "print(f\"\\n\u2713 Generated {hyetograph_count} hyetographs\")\n",
        "print(f\"\\nScenario Summary (first 12 rows):\")\n",
        "display.display(scenario_df.head(12))\n",
        "\n",
        "# Save scenario list\n",
        "scenario_csv = 'scenarios_uncertainty.csv'\n",
        "scenario_df.to_csv(scenario_csv, index=False)\n",
        "print(f\"\\nScenario list saved to: {scenario_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precip-depth-header",
      "metadata": {},
      "source": [
        "## Total Precipitation Depth Analysis\n",
        "\n",
        "Visualize total precipitation depths across all scenarios with confidence intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precip-depth-plot",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your data is in scenario_df with columns: duration_hrs, ari, ci_level, total_depth_in\n",
        "\n",
        "# Create the plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Define your ARI events (adjust based on your data)\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "\n",
        "for idx, ari in enumerate(aep_events):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Extract data for this specific ARI\n",
        "    ari_data = scenario_df[scenario_df['ari'] == ari].copy()\n",
        "\n",
        "    # Get unique durations\n",
        "    durations = sorted(ari_data['duration_hrs'].unique())\n",
        "\n",
        "    # Initialize lists to store values\n",
        "    lower_vals = []\n",
        "    upper_vals = []\n",
        "    point_vals = []\n",
        "    ci_percentages = []\n",
        "\n",
        "    # Process each duration\n",
        "    for dur in durations:\n",
        "        # Get data for this specific duration\n",
        "        dur_data = ari_data[ari_data['duration_hrs'] == dur]\n",
        "\n",
        "        # Extract values for each CI level\n",
        "        lower_val = dur_data[dur_data['ci_level'] == 'lower']['total_depth_in'].values\n",
        "        upper_val = dur_data[dur_data['ci_level'] == 'upper']['total_depth_in'].values\n",
        "        point_val = dur_data[dur_data['ci_level'] == 'point']['total_depth_in'].values\n",
        "\n",
        "        # Handle missing data\n",
        "        if len(lower_val) > 0 and len(upper_val) > 0 and len(point_val) > 0:\n",
        "            lower = float(lower_val[0])\n",
        "            upper = float(upper_val[0])\n",
        "            point = float(point_val[0])\n",
        "\n",
        "            # Calculate CI width as percentage of point estimate\n",
        "            # Using point estimate as the reference (you could also use mean of upper/lower)\n",
        "            if point > 0:\n",
        "                ci_width = upper - lower\n",
        "                ci_pct = (ci_width / point) * 100\n",
        "            else:\n",
        "                ci_pct = np.nan\n",
        "        else:\n",
        "            lower = upper = point = ci_pct = np.nan\n",
        "\n",
        "        lower_vals.append(lower)\n",
        "        upper_vals.append(upper)\n",
        "        point_vals.append(point)\n",
        "        ci_percentages.append(ci_pct)\n",
        "\n",
        "    # Convert to arrays\n",
        "    lower_vals = np.array(lower_vals)\n",
        "    upper_vals = np.array(upper_vals)\n",
        "    point_vals = np.array(point_vals)\n",
        "    ci_percentages = np.array(ci_percentages)\n",
        "\n",
        "    # Plot the data\n",
        "    ax.plot(durations, point_vals,\n",
        "            'ko-', linewidth=2, markersize=6, label='Point Estimate')\n",
        "    ax.fill_between(durations,\n",
        "                    lower_vals,\n",
        "                    upper_vals,\n",
        "                    alpha=0.3, color='gray', label='90% CI')\n",
        "\n",
        "    # Calculate spacing for annotations\n",
        "    y_range = np.nanmax(upper_vals) - np.nanmin(lower_vals)\n",
        "\n",
        "    # Add annotations for each point\n",
        "    for i, (x, y_lower, y_upper, y_point, ci_pct) in enumerate(zip(\n",
        "            durations, lower_vals, upper_vals, point_vals, ci_percentages)):\n",
        "\n",
        "        if not np.isnan(y_lower):\n",
        "            # Annotate lower bound\n",
        "            ax.text(x, y_lower - 0.02 * y_range, f'{y_lower:.2f}',\n",
        "                    fontsize=8, ha='center', va='top')\n",
        "\n",
        "        if not np.isnan(y_upper):\n",
        "            # Annotate upper bound\n",
        "            ax.text(x, y_upper + 0.02 * y_range, f'{y_upper:.2f}',\n",
        "                    fontsize=8, ha='center', va='bottom')\n",
        "\n",
        "        if not np.isnan(y_point):\n",
        "            # Annotate point estimate\n",
        "            ax.text(x, y_point, f'{y_point:.2f}',\n",
        "                    fontsize=8, color='darkgray', ha='left', va='bottom',\n",
        "                    fontweight='bold')\n",
        "\n",
        "        if not np.isnan(ci_pct):\n",
        "            # Annotate CI percentage in the middle of the CI band\n",
        "            y_mid = (y_lower + y_upper) / 2\n",
        "            ax.text(x, y_mid, f'{ci_pct:.1f}%',\n",
        "                    fontsize=8, color='royalblue', ha='center', va='center',\n",
        "                    bbox=dict(boxstyle='round,pad=0.2', facecolor='white',\n",
        "                             edgecolor='none', alpha=0.7))\n",
        "\n",
        "    # Set labels and formatting\n",
        "    ax.set_xlabel('Duration (hours)', fontsize=10)\n",
        "    ax.set_ylabel('Total Precipitation (inches)', fontsize=10)\n",
        "    ax.set_title(f'{ari}-Year Event', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='upper left', fontsize=8)\n",
        "\n",
        "    # Set x-axis to log scale\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xticks(durations)\n",
        "    ax.set_xticklabels([str(d) for d in durations])\n",
        "\n",
        "    # Adjust y-limits to accommodate annotations\n",
        "    y_min = np.nanmin(lower_vals) - 0.1 * y_range\n",
        "    y_max = np.nanmax(upper_vals) + 0.1 * y_range\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "\n",
        "plt.suptitle('Total Precipitation Depth vs Duration with Confidence Intervals\\n(CI width as % of point estimate at each duration)',\n",
        "             fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Also print out the actual CI percentages to verify\n",
        "print(\"\\nConfidence Interval Width as % of Point Estimate:\")\n",
        "print(\"=\"*60)\n",
        "for ari in aep_events:\n",
        "    print(f\"\\n{ari}-Year Event:\")\n",
        "    ari_data = scenario_df[scenario_df['ari'] == ari].copy()\n",
        "    durations = sorted(ari_data['duration_hrs'].unique())\n",
        "\n",
        "    for dur in durations:\n",
        "        dur_data = ari_data[ari_data['duration_hrs'] == dur]\n",
        "        lower = dur_data[dur_data['ci_level'] == 'lower']['total_depth_in'].values\n",
        "        upper = dur_data[dur_data['ci_level'] == 'upper']['total_depth_in'].values\n",
        "        point = dur_data[dur_data['ci_level'] == 'point']['total_depth_in'].values\n",
        "\n",
        "        if len(lower) > 0 and len(upper) > 0 and len(point) > 0:\n",
        "            ci_width = upper[0] - lower[0]\n",
        "            ci_pct = (ci_width / point[0]) * 100\n",
        "            print(f\"  {dur:2d} hr: {ci_pct:.1f}% (L:{lower[0]:.2f}, P:{point[0]:.2f}, U:{upper[0]:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Sample Hyetographs\n",
        "\n",
        "Show how confidence intervals affect hyetograph shapes for a few example scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyeto-plot-functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_hyetograph_with_ci(ari, duration, output_dir, save_dir=None):\n",
        "    \"\"\"\n",
        "    Plot hyetograph showing lower CI, Atlas 14 estimate, and upper CI.\n",
        "    Handles variable time intervals (hours or minutes) based on duration.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ari : int\n",
        "        Annual recurrence interval (return period)\n",
        "    duration : int\n",
        "        Storm duration in hours\n",
        "    output_dir : str\n",
        "        Directory containing hyetograph CSV files\n",
        "    save_dir : str, optional\n",
        "        Directory to save PNG files. If None, only displays plot.\n",
        "    \"\"\"\n",
        "    # Format duration string\n",
        "    if duration >= 24:\n",
        "        dur_str = f\"{int(duration/24)}day\"\n",
        "        dur_label = f\"{int(duration/24)}-day\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration)}hr\"\n",
        "        dur_label = f\"{int(duration)}-hr\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Define colors - using distinct, professional colors without transparency\n",
        "    colors = {\n",
        "        'lower': '#4472C4',   # Professional blue\n",
        "        'point': '#2C2C2C',   # Dark gray/black\n",
        "        'upper': '#C55A5A'    # Professional red\n",
        "    }\n",
        "\n",
        "    # Store data for proper ordering\n",
        "    ci_data = {}\n",
        "\n",
        "    # Read hyetographs for each CI level\n",
        "    for ci_level in ['lower', 'point', 'upper']:\n",
        "        filename = f'hyetograph_ARI_{ari}_DUR_{dur_str}_CI_{ci_level}.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Skip header lines\n",
        "        df = pd.read_csv(filepath, comment='#')\n",
        "        ci_data[ci_level] = df\n",
        "\n",
        "    # Determine time column name and units\n",
        "    if 'Time_hour' in ci_data['point'].columns:\n",
        "        time_col = 'Time_hour'\n",
        "        time_label = 'Time (hours)'\n",
        "        time_values = ci_data['point'][time_col]\n",
        "        bar_width = 0.8  # Standard width for hourly data\n",
        "    elif 'Time_min' in ci_data['point'].columns:\n",
        "        time_col = 'Time_min'\n",
        "        time_label = 'Time (minutes)'\n",
        "        time_values = ci_data['point'][time_col]\n",
        "        # Adjust bar width based on time interval\n",
        "        time_interval = time_values.iloc[1] - time_values.iloc[0] if len(time_values) > 1 else 5\n",
        "        bar_width = time_interval * 0.8  # 80% of interval width\n",
        "    else:\n",
        "        raise ValueError(\"Could not find time column (Time_hour or Time_min) in CSV\")\n",
        "\n",
        "    # Get the data arrays\n",
        "    x_time = ci_data['point'][time_col]\n",
        "    lower_precip = ci_data['lower']['Precipitation_in']\n",
        "    point_precip = ci_data['point']['Precipitation_in']\n",
        "    upper_precip = ci_data['upper']['Precipitation_in']\n",
        "\n",
        "    # Calculate the incremental heights for stacking\n",
        "    height_lower = lower_precip\n",
        "    height_point_increment = point_precip - lower_precip\n",
        "    height_upper_increment = upper_precip - point_precip\n",
        "\n",
        "    # Plot stacked bars\n",
        "    ax.bar(x_time, height_lower,\n",
        "           width=bar_width,\n",
        "           alpha=1.0,\n",
        "           color=colors['lower'],\n",
        "           label='90% Lower CI',\n",
        "           edgecolor='none')\n",
        "\n",
        "    ax.bar(x_time, height_point_increment,\n",
        "           width=bar_width,\n",
        "           bottom=height_lower,\n",
        "           alpha=1.0,\n",
        "           color=colors['point'],\n",
        "           label='NOAA Atlas 14 Estimate',\n",
        "           edgecolor='none')\n",
        "\n",
        "    ax.bar(x_time, height_upper_increment,\n",
        "           width=bar_width,\n",
        "           bottom=point_precip,\n",
        "           alpha=1.0,\n",
        "           color=colors['upper'],\n",
        "           label='90% Upper CI',\n",
        "           edgecolor='none')\n",
        "\n",
        "    ax.set_xlabel(time_label, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Incremental Precipitation (inches)', fontsize=12, fontweight='bold')\n",
        "\n",
        "    title = f'{ari}-Year {dur_label} Design Storm\\nNOAA Atlas 14 Precipitation with 90% Confidence Bounds'\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    ax.legend(fontsize=10, loc='upper right', framealpha=0.95)\n",
        "    ax.grid(axis='y', alpha=0.3, zorder=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if directory specified\n",
        "    if save_dir is not None:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        filename = f'{ari}yr_{dur_str}_storm_CI.png'\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Hyetograph plotting functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYETOGRAPH VISUALIZATION WITH CONFIDENCE INTERVALS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"HYETOGRAPH VISUALIZATION WITH CONFIDENCE INTERVALS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Generate ALL hyetographs and save to folder (without displaying)\n",
        "print(\"\\nGenerating and saving all hyetograph plots...\")\n",
        "print(\"(Only displaying 10-Year 6-Hour and 100-Year 24-Hour storms)\\n\")\n",
        "\n",
        "# Define all scenarios\n",
        "ari_values = [2, 5, 10, 25, 50, 100]\n",
        "duration_values = [1, 2, 3, 6, 12, 24, 48]  # 48 = 2 days\n",
        "save_dir = \"Atlas 14 Storm Hyetographs\"\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "total_plots = len(ari_values) * len(duration_values)\n",
        "current = 0\n",
        "generated_count = 0\n",
        "\n",
        "for ari in ari_values:\n",
        "    for duration in duration_values:\n",
        "        current += 1\n",
        "\n",
        "        # Format duration string\n",
        "        if duration >= 24:\n",
        "            dur_str = f\"{int(duration/24)}day\"\n",
        "        else:\n",
        "            dur_str = f\"{int(duration)}hr\"\n",
        "\n",
        "        # Check if files exist\n",
        "        filename = f'hyetograph_ARI_{ari}_DUR_{dur_str}_CI_point.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"[{current}/{total_plots}] Skipping {ari}-yr, {dur_str} - files not found\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Only DISPLAY these two specific plots\n",
        "            if (ari == 10 and duration == 6) or (ari == 100 and duration == 24):\n",
        "                print(f\"\\n[{current}/{total_plots}] Displaying {ari}-yr, {dur_str} storm...\")\n",
        "                plot_hyetograph_with_ci(ari, duration, output_dir, save_dir)\n",
        "            else:\n",
        "                # Generate and save without displaying\n",
        "                print(f\"[{current}/{total_plots}] Generating {ari}-yr, {dur_str} storm...\", end='')\n",
        "\n",
        "                # Call the function but close the plot immediately to avoid display\n",
        "                import matplotlib\n",
        "                matplotlib.use('Agg')  # Use non-interactive backend\n",
        "                plot_hyetograph_with_ci(ari, duration, output_dir, save_dir)\n",
        "                plt.close('all')  # Close all figures\n",
        "                matplotlib.use('module://matplotlib_inline.backend_inline')  # Restore interactive backend\n",
        "\n",
        "                print(\" saved.\")\n",
        "\n",
        "            generated_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(f\"COMPLETE\")\n",
        "print(f\"{'=' * 70}\")\n",
        "print(f\"\u2713 Generated and saved {generated_count} hyetograph plots to: {save_dir}\")\n",
        "print(f\"\u2713 Displayed 2 key scenarios (10-Year 6-Hour and 100-Year 24-Hour)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Total Depths Across Scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyetograph visualization completed in cell above\n",
        "# All hyetograph plots have been generated and saved to \"Atlas 14 Storm Hyetographs\" directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize HEC-RAS Project and Create Plans\n",
        "\n",
        "Now we'll create a plan for each scenario in our matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plan Creation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_simulation_duration_from_plan(plan_number, project):\n",
        "    \"\"\"\n",
        "    Get the simulation duration in hours from a plan file.\n",
        "\n",
        "    Parameters:\n",
        "    - plan_number: Plan number\n",
        "    - project: RAS project object\n",
        "\n",
        "    Returns:\n",
        "    - tuple: (start_datetime, end_datetime, duration_hours)\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=project)\n",
        "\n",
        "    with open(plan_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Find Simulation Date line\n",
        "    for line in lines:\n",
        "        if line.startswith('Simulation Date='):\n",
        "            # Format: Simulation Date=10JAN2000,1200,11JAN2000,2400\n",
        "            parts = line.strip().split('=')[1].split(',')\n",
        "\n",
        "            start_date = parts[0]\n",
        "            start_time = parts[1]\n",
        "            end_date = parts[2]\n",
        "            end_time = parts[3]\n",
        "\n",
        "            # Parse dates (format: DDMMMYYYY, time: HHMM)\n",
        "            start_dt = datetime.strptime(f\"{start_date}{start_time}\", \"%d%b%Y%H%M\")\n",
        "            \n",
        "            # Handle special case: HEC-RAS uses \"2400\" to mean midnight (end of day)\n",
        "            if end_time == \"2400\":\n",
        "                end_dt = datetime.strptime(f\"{end_date}0000\", \"%d%b%Y%H%M\")\n",
        "                end_dt = end_dt + timedelta(days=1)\n",
        "            else:\n",
        "                end_dt = datetime.strptime(f\"{end_date}{end_time}\", \"%d%b%Y%H%M\")\n",
        "\n",
        "            # Calculate duration in hours\n",
        "            duration_hours = (end_dt - start_dt).total_seconds() / 3600\n",
        "\n",
        "            return start_dt, end_dt, duration_hours\n",
        "\n",
        "    # Default fallback\n",
        "    return None, None, 24.0\n",
        "\n",
        "\n",
        "def format_interval_for_hecras(interval_hours):\n",
        "    \"\"\"\n",
        "    Convert interval in hours to HEC-RAS format string.\n",
        "\n",
        "    Parameters:\n",
        "    - interval_hours: Time interval in hours (e.g., 0.0833 for 5 minutes)\n",
        "\n",
        "    Returns:\n",
        "    - str: HEC-RAS format (e.g., \"5MIN\", \"15MIN\", \"1HOUR\")\n",
        "    \"\"\"\n",
        "    interval_minutes = interval_hours * 60\n",
        "\n",
        "    if interval_minutes < 1.0:\n",
        "        # Less than 1 minute - shouldn't happen but handle it\n",
        "        seconds = interval_minutes * 60\n",
        "        return f\"{int(seconds)}SEC\"\n",
        "    elif interval_minutes < 60:\n",
        "        # Minutes\n",
        "        return f\"{int(interval_minutes)}MIN\"\n",
        "    else:\n",
        "        # Hours\n",
        "        hours = interval_hours\n",
        "        if hours == int(hours):\n",
        "            return f\"{int(hours)}HOUR\"\n",
        "        else:\n",
        "            # Fractional hours - convert to minutes\n",
        "            return f\"{int(interval_minutes)}MIN\"\n",
        "\n",
        "\n",
        "def modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, plan_number=None, project=None):\n",
        "    \"\"\"\n",
        "    Modifies an unsteady flow file to incorporate hyetograph data as precipitation.\n",
        "    Handles time intervals and pads the hyetograph to cover the full simulation window.\n",
        "\n",
        "    Parameters:\n",
        "    - unsteady_file_path: Path to the unsteady flow file\n",
        "    - hyetograph_file: Path to the hyetograph data CSV\n",
        "    - plan_number: Plan number (for getting simulation duration)\n",
        "    - project: RAS project object\n",
        "\n",
        "    Returns:\n",
        "    - Boolean indicating success\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the hyetograph data (skip comment lines)\n",
        "        hyetograph_df = pd.read_csv(hyetograph_file, comment='#')\n",
        "\n",
        "        # Get the time interval from the hyetograph\n",
        "        if 'Time_hour' in hyetograph_df.columns:\n",
        "            time_values = hyetograph_df['Time_hour'].values\n",
        "        elif 'Time_min' in hyetograph_df.columns:\n",
        "            time_values = hyetograph_df['Time_min'].values / 60.0  # Convert to hours\n",
        "        else:\n",
        "            raise ValueError(\"Could not find time column in hyetograph file\")\n",
        "\n",
        "        # Calculate interval\n",
        "        if len(time_values) > 1:\n",
        "            interval_hours = time_values[1] - time_values[0]\n",
        "        else:\n",
        "            interval_hours = time_values[0]\n",
        "\n",
        "        # Get storm duration in hours\n",
        "        storm_duration_hours = time_values[-1]\n",
        "\n",
        "        # Get simulation duration from plan file\n",
        "        if plan_number and project:\n",
        "            start_dt, end_dt, sim_duration_hours = get_simulation_duration_from_plan(plan_number, project)\n",
        "        else:\n",
        "            # Default: assume 36-hour simulation\n",
        "            sim_duration_hours = 36.0\n",
        "\n",
        "        print(f\"  Storm duration: {storm_duration_hours:.2f} hours\")\n",
        "        print(f\"  Time interval: {interval_hours*60:.1f} minutes\")\n",
        "        print(f\"  Simulation duration: {sim_duration_hours:.1f} hours\")\n",
        "\n",
        "        # Calculate total number of intervals needed\n",
        "        total_intervals = int(sim_duration_hours / interval_hours)\n",
        "\n",
        "        # Get precipitation values from hyetograph\n",
        "        precip_values = hyetograph_df[\"Precipitation_in\"].values\n",
        "\n",
        "        # Calculate where to place the storm (at the beginning of simulation)\n",
        "        warmup_hours = 0.0\n",
        "        warmup_intervals = int(warmup_hours / interval_hours)\n",
        "\n",
        "        # Create full precipitation array with zeros\n",
        "        full_precip = np.zeros(total_intervals)\n",
        "\n",
        "        # Insert storm values after warmup period\n",
        "        storm_intervals = len(precip_values)\n",
        "        if warmup_intervals + storm_intervals <= total_intervals:\n",
        "            full_precip[warmup_intervals:warmup_intervals + storm_intervals] = precip_values\n",
        "        else:\n",
        "            # If storm doesn't fit, place at start\n",
        "            end_idx = min(storm_intervals, total_intervals)\n",
        "            full_precip[0:end_idx] = precip_values[0:end_idx]\n",
        "\n",
        "        print(f\"  Total intervals in simulation: {total_intervals}\")\n",
        "        print(f\"  Storm placed at interval {warmup_intervals} (at simulation start)\")\n",
        "\n",
        "        # Read the unsteady flow file\n",
        "        with open(unsteady_file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        # Find the Interval line and update it\n",
        "        interval_line_idx = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Interval=\"):\n",
        "                interval_line_idx = i\n",
        "                break\n",
        "\n",
        "        if interval_line_idx is not None:\n",
        "            # Update the interval\n",
        "            interval_str = format_interval_for_hecras(interval_hours)\n",
        "            lines[interval_line_idx] = f\"Interval={interval_str}\\n\"\n",
        "            print(f\"  Updated Interval to: {interval_str}\")\n",
        "\n",
        "        # Find the Precipitation Hydrograph section\n",
        "        precip_hydrograph_index = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Precipitation Hydrograph=\"):\n",
        "                precip_hydrograph_index = i\n",
        "                break\n",
        "\n",
        "        if precip_hydrograph_index is None:\n",
        "            print(\"Cannot find Precipitation Hydrograph section in unsteady file.\")\n",
        "            return False\n",
        "\n",
        "        # Create the Precipitation Hydrograph line\n",
        "        precip_line = f\"Precipitation Hydrograph= {len(full_precip)} \\n\"\n",
        "\n",
        "        # Format the values in groups of 10 per line\n",
        "        value_lines = []\n",
        "        for i in range(0, len(full_precip), 10):\n",
        "            row_values = full_precip[i:i+10]\n",
        "            row_line = \"\".join([f\"{value:8.2f}\" for value in row_values]) + \"\\n\"\n",
        "            value_lines.append(row_line)\n",
        "\n",
        "        # Find end of current hydrograph\n",
        "        current_line = precip_hydrograph_index + 1\n",
        "        end_markers = [\"DSS Path=\", \"Use DSS=\", \"Use Fixed Start Time=\"]\n",
        "        while current_line < len(lines) and not any(lines[current_line].startswith(marker) for marker in end_markers):\n",
        "            current_line += 1\n",
        "\n",
        "        # Replace the hydrograph section\n",
        "        lines[precip_hydrograph_index:current_line] = [precip_line] + value_lines\n",
        "\n",
        "        # Write the modified file back\n",
        "        with open(unsteady_file_path, 'w') as file:\n",
        "            file.writelines(lines)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error modifying unsteady flow file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "\n",
        "def create_plan_for_scenario(scenario_row, base_plan, project):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a specific scenario.\n",
        "\n",
        "    Parameters:\n",
        "    - scenario_row: Row from scenario DataFrame\n",
        "    - base_plan: Base plan number to clone from\n",
        "    - project: RAS project object\n",
        "\n",
        "    Returns:\n",
        "    - new_plan_number\n",
        "    \"\"\"\n",
        "    ari = scenario_row['ari']\n",
        "    duration = scenario_row['duration_hrs']\n",
        "    ci_level = scenario_row['ci_level']\n",
        "    hyetograph_file = scenario_row['hyetograph_file']\n",
        "\n",
        "    # Format duration string\n",
        "    if duration >= 24:\n",
        "        dur_str = f\"{int(duration/24)}D\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration)}H\"\n",
        "\n",
        "    # Create plan name (keep it short for HEC-RAS)\n",
        "    ci_abbrev = ci_level[0].upper()  # L, P, or U\n",
        "    plan_name = f\"{ari}YR-{dur_str}-{ci_abbrev}\"\n",
        "\n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_shortid=plan_name, ras_object=project)\n",
        "\n",
        "    # Get unsteady number from base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            break\n",
        "\n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    # Clone the unsteady flow file\n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "\n",
        "    # Get unsteady file path\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "\n",
        "    # Update the flow title\n",
        "    new_title = f\"{ari}YR-{dur_str}-{ci_level.upper()} Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "\n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    # Pass plan number and project so we can get simulation duration\n",
        "    hyetograph_file_abs = Path(hyetograph_file).absolute()\n",
        "    success = modify_unsteady_flow_with_hyetograph(\n",
        "        unsteady_file_path,\n",
        "        hyetograph_file_abs,\n",
        "        plan_number=new_plan_number,\n",
        "        project=project\n",
        "    )\n",
        "    if not success:\n",
        "        raise RuntimeError(f\"Failed to apply hyetograph data to plan {new_plan_number}\")\n",
        "\n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "\n",
        "    return new_plan_number\n",
        "\n",
        "print(\"Plan creation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling the 99-Plan Limit in HEC-RAS\n",
        "\n",
        "**IMPORTANT**: HEC-RAS projects can only contain a maximum of 99 plans. Since this analysis requires 126 scenarios (6 AEPs \u00d7 7 durations \u00d7 3 confidence levels), we need to distribute scenarios across multiple project copies.\n",
        "\n",
        "**Solution Approach:**\n",
        "1. **Automatic Project Distribution**: The `distribute_scenarios_across_projects()` function automatically:\n",
        "   - Checks existing plan count in the base project\n",
        "   - Calculates available slots (99 - existing plans)\n",
        "   - Creates multiple project copies as needed\n",
        "   - Groups scenarios logically (by AEP) to keep related runs together\n",
        "   \n",
        "2. **Project Naming**: New projects are created with meaningful names:\n",
        "   - Format: `{project_name}{suffix}_{number}`\n",
        "   - Example: `Davis_atlas14_01`, `Davis_atlas14_02`, etc.\n",
        "   - The suffix is user-configurable (default: `_atlas14`)\n",
        "\n",
        "3. **Plan Descriptions**: Each plan's description includes:\n",
        "   - AEP (return period)\n",
        "   - Storm duration\n",
        "   - Confidence level\n",
        "   - Analysis set identifier (the folder suffix)\n",
        "   \n",
        "4. **Original Project Preservation**: The base project is never modified - all new plans are created in project copies\n",
        "\n",
        "**Configuration Options:**\n",
        "- `folder_suffix`: Change the suffix added to project folders (default: `\"_atlas14\"`)\n",
        "- Modify the grouping strategy in `distribute_scenarios_across_projects()` if needed\n",
        "  (currently groups by AEP to keep related scenarios together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Multi-Project Management Functions for 99-Plan Limit\n",
        "\n",
        "import shutil\n",
        "\n",
        "def copy_ras_project(source_folder, dest_folder, suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Copy a HEC-RAS project to a new folder.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    source_folder : Path or str\n",
        "        Source project folder\n",
        "    dest_folder : Path or str\n",
        "        Destination folder\n",
        "    suffix : str, optional\n",
        "        Suffix to add to project name\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Path : Path to destination folder\n",
        "    \"\"\"\n",
        "    source_folder = Path(source_folder)\n",
        "    dest_folder = Path(dest_folder)\n",
        "\n",
        "    # Create destination\n",
        "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Copy all files\n",
        "    for item in source_folder.iterdir():\n",
        "        if item.is_file():\n",
        "            shutil.copy2(item, dest_folder / item.name)\n",
        "        elif item.is_dir() and item.name not in ['compute_uncertainty', '.ipynb_checkpoints']:\n",
        "            shutil.copytree(item, dest_folder / item.name, dirs_exist_ok=True)\n",
        "\n",
        "    print(f\"Copied project to: {dest_folder}\")\n",
        "    return dest_folder\n",
        "\n",
        "\n",
        "def distribute_scenarios_across_projects(scenario_df, base_project_path, base_plan=\"02\",\n",
        "                                         folder_suffix=\"_atlas14\", ras_version=\"6.6\"):\n",
        "    \"\"\"\n",
        "    Distribute scenarios across multiple HEC-RAS project copies to handle 99-plan limit.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    scenario_df : DataFrame\n",
        "        Scenarios to process\n",
        "    base_project_path : Path or str\n",
        "        Original project path\n",
        "    base_plan : str\n",
        "        Base plan number to clone from\n",
        "    folder_suffix : str\n",
        "        Suffix for project folders (default: \"_atlas14\")\n",
        "    ras_version : str\n",
        "        HEC-RAS version\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame : scenario_df with added columns: project_folder, project_object, plan_number\n",
        "    \"\"\"\n",
        "    base_project_path = Path(base_project_path)\n",
        "\n",
        "    # Check base project existing plans\n",
        "    temp_proj = RasPrj()\n",
        "    init_ras_project(base_project_path, ras_version, ras_object=temp_proj)\n",
        "    existing_plans_in_base = len(temp_proj.plan_df)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DISTRIBUTING SCENARIOS ACROSS PROJECTS (99-PLAN LIMIT)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBase project has {existing_plans_in_base} existing plans\")\n",
        "    print(f\"Maximum plans per project: 99\")\n",
        "    print(f\"Total scenarios to distribute: {len(scenario_df)}\\n\")\n",
        "\n",
        "    # Calculate how many scenarios we can fit per project\n",
        "    max_scenarios_per_project = 99 - existing_plans_in_base\n",
        "\n",
        "    # Group scenarios by ARI to keep related scenarios together\n",
        "    ari_groups = scenario_df.groupby('ari')\n",
        "\n",
        "    results_list = []\n",
        "    current_project_num = 1\n",
        "    current_project_path = None\n",
        "    current_project_obj = None\n",
        "    current_project_plan_count = 0\n",
        "\n",
        "    for ari, ari_df in ari_groups:\n",
        "        num_scenarios_in_group = len(ari_df)\n",
        "\n",
        "        print(f\"Processing {ari}-year AEP ({num_scenarios_in_group} scenarios)...\")\n",
        "\n",
        "        # Check if we need to create a new project\n",
        "        if current_project_obj is None or (current_project_plan_count + num_scenarios_in_group) > max_scenarios_per_project:\n",
        "            # Create a new project\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Creating Project #{current_project_num}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Create project folder with meaningful name\n",
        "            project_folder_name = f\"{base_project_path.name}{folder_suffix}_{current_project_num:02d}\"\n",
        "            current_project_path = base_project_path.parent / project_folder_name\n",
        "\n",
        "            # Copy the project\n",
        "            copy_ras_project(base_project_path, current_project_path)\n",
        "\n",
        "            # Initialize the project\n",
        "            current_project_obj = RasPrj()\n",
        "            init_ras_project(current_project_path, ras_version, ras_object=current_project_obj)\n",
        "\n",
        "            # Reset counter to existing plans in the new copy\n",
        "            current_project_plan_count = len(current_project_obj.plan_df)\n",
        "            print(f\"Project initialized with {current_project_plan_count} existing plans\")\n",
        "            print(f\"Available slots: {99 - current_project_plan_count}\")\n",
        "\n",
        "            current_project_num += 1\n",
        "\n",
        "        # Add all scenarios from this ARI group to the current project\n",
        "        for idx, scenario in ari_df.iterrows():\n",
        "            scenario['project_folder'] = str(current_project_path)\n",
        "            scenario['project_object'] = current_project_obj\n",
        "            scenario['project_suffix'] = folder_suffix\n",
        "            results_list.append(scenario)\n",
        "            current_project_plan_count += 1\n",
        "\n",
        "        print(f\"  Added {num_scenarios_in_group} scenarios to project {current_project_path.name}\")\n",
        "        print(f\"  Current plan count: {current_project_plan_count}/{99}\")\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"DISTRIBUTION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total scenarios: {len(results_df)}\")\n",
        "    print(f\"Projects created: {current_project_num - 1}\")\n",
        "    print(f\"\\nScenarios per project:\")\n",
        "    for proj_folder in results_df['project_folder'].unique():\n",
        "        count = len(results_df[results_df['project_folder'] == proj_folder])\n",
        "        print(f\"  {Path(proj_folder).name}: {count} scenarios\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "def create_plan_with_description(scenario_row, base_plan, project, folder_suffix):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a scenario with description including folder suffix.\n",
        "\n",
        "    Parameters:\n",
        "    - scenario_row: Row from scenario DataFrame\n",
        "    - base_plan: Base plan number to clone from\n",
        "    - project: RAS project object\n",
        "    - folder_suffix: Folder suffix to add to description\n",
        "\n",
        "    Returns:\n",
        "    - new_plan_number\n",
        "    \"\"\"\n",
        "    ari = scenario_row['ari']\n",
        "    duration = scenario_row['duration_hrs']\n",
        "    ci_level = scenario_row['ci_level']\n",
        "    hyetograph_file = scenario_row['hyetograph_file']\n",
        "\n",
        "    # Format duration string\n",
        "    if duration >= 24:\n",
        "        dur_str = f\"{int(duration/24)}D\"\n",
        "    else:\n",
        "        dur_str = f\"{int(duration)}H\"\n",
        "\n",
        "    # Create plan name (keep it short for HEC-RAS)\n",
        "    ci_abbrev = ci_level[0].upper()  # L, P, or U\n",
        "    plan_name = f\"{ari}YR-{dur_str}-{ci_abbrev}\"\n",
        "\n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_shortid=plan_name, ras_object=project)\n",
        "\n",
        "    # Update description with folder suffix\n",
        "    description_text = f\"Atlas 14 Uncertainty Analysis\\n\"\n",
        "    description_text += f\"AEP: {ari} years\\n\"\n",
        "    description_text += f\"Duration: {duration} hours\\n\"\n",
        "    description_text += f\"Confidence Level: {ci_level}\\n\"\n",
        "    description_text += f\"Analysis Set: {folder_suffix}\"\n",
        "\n",
        "    #RasPlan.update_plan_description(new_plan_number, description_text, ras_object=project)\n",
        "\n",
        "    # Get unsteady number from base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            break\n",
        "\n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    # Clone the unsteady flow file\n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "\n",
        "    # Get unsteady file path\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "\n",
        "    # Update the flow title\n",
        "    new_title = f\"{ari}YR-{dur_str}-{ci_level.upper()} Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "\n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    # Need to ensure hyetograph path is accessible from new project location\n",
        "    hyetograph_file_abs = Path(hyetograph_file).absolute()\n",
        "    success = modify_unsteady_flow_with_hyetograph(\n",
        "        unsteady_file_path,\n",
        "        hyetograph_file_abs,\n",
        "        plan_number=new_plan_number,\n",
        "        project=project\n",
        "    )\n",
        "    if not success:\n",
        "        raise RuntimeError(f\"Failed to apply hyetograph data to plan {new_plan_number}\")\n",
        "\n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "\n",
        "    return new_plan_number\n",
        "\n",
        "\n",
        "print(\"Multi-project management functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribute scenarios across projects and create plans\n",
        "print(\"=\"*70)\n",
        "print(\"CREATING PLANS WITH 99-PLAN LIMIT HANDLING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal scenarios to process: {len(scenario_df)}\")\n",
        "print(f\"Base project: {pipes_ex_path}\\n\")\n",
        "\n",
        "# Distribute scenarios across multiple projects as needed\n",
        "scenario_df_distributed = distribute_scenarios_across_projects(\n",
        "    scenario_df, \n",
        "    base_project_path=pipes_ex_path,\n",
        "    base_plan=base_plan,\n",
        "    folder_suffix=\"_atlas14\",\n",
        "    ras_version=\"6.6\"\n",
        ")\n",
        "\n",
        "# Now create plans in each project\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CREATING PLANS IN DISTRIBUTED PROJECTS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "plan_numbers = []\n",
        "failed_scenarios = []\n",
        "\n",
        "# Group by project\n",
        "project_groups = scenario_df_distributed.groupby('project_folder')\n",
        "\n",
        "for proj_folder, proj_scenarios in project_groups:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing Project: {Path(proj_folder).name}\")\n",
        "    print(f\"Scenarios: {len(proj_scenarios)}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Get the project object (they should all be the same within a group)\n",
        "    proj_obj = proj_scenarios.iloc[0]['project_object']\n",
        "    folder_suffix = proj_scenarios.iloc[0]['project_suffix']\n",
        "    \n",
        "    for idx, row in proj_scenarios.iterrows():\n",
        "        try:\n",
        "            new_plan = create_plan_with_description(row, base_plan, proj_obj, folder_suffix)\n",
        "            plan_numbers.append(new_plan)\n",
        "            \n",
        "            # Update the row with the plan number\n",
        "            scenario_df_distributed.at[idx, 'plan_number'] = new_plan\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error creating plan for scenario {idx}: {e}\")\n",
        "            failed_scenarios.append(idx)\n",
        "            plan_numbers.append(None)\n",
        "            scenario_df_distributed.at[idx, 'plan_number'] = None\n",
        "    \n",
        "    # Show progress\n",
        "    successful = len([p for p in plan_numbers if p is not None])\n",
        "    print(f\"\\nProject {Path(proj_folder).name}: Created {len(proj_scenarios)} plans\")\n",
        "\n",
        "# Remove failed scenarios\n",
        "scenario_df = scenario_df_distributed[scenario_df_distributed['plan_number'].notna()].copy()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PLAN CREATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\u2713 Successfully created {len(plan_numbers) - len(failed_scenarios)} plans\")\n",
        "if failed_scenarios:\n",
        "    print(f\"\u2717 Failed to create {len(failed_scenarios)} plans\")\n",
        "\n",
        "print(f\"\\nPlan Summary (first 15 rows):\")\n",
        "display.display(scenario_df[['ari', 'duration_hrs', 'ci_level', 'plan_number', 'project_folder']].head(15))\n",
        "\n",
        "# Save updated scenario list\n",
        "scenario_df.to_csv('scenarios_with_plans.csv', index=False)\n",
        "print(f\"\\nScenario list with plan numbers saved to: scenarios_with_plans.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set computation parameters for all new plans across all projects\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURING COMPUTATION PARAMETERS ACROSS ALL PROJECTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Group by project\n",
        "project_groups = scenario_df.groupby('project_folder')\n",
        "\n",
        "total_configured = 0\n",
        "\n",
        "for proj_folder, proj_scenarios in project_groups:\n",
        "    print(f\"\\nConfiguring {len(proj_scenarios)} plans in: {Path(proj_folder).name}\")\n",
        "    \n",
        "    # Get the project object\n",
        "    proj_obj = proj_scenarios.iloc[0]['project_object']\n",
        "    \n",
        "    for idx, row in proj_scenarios.iterrows():\n",
        "        plan_number = row['plan_number']\n",
        "        \n",
        "        try:\n",
        "            RasPlan.set_num_cores(plan_number, 2, ras_object=proj_obj)\n",
        "#            RasPlan.update_plan_intervals(\n",
        "#                plan_number,\n",
        "#                computation_interval=\"15MIN\",\n",
        "#                output_interval=\"30MIN\",\n",
        "#                mapping_interval=\"1HOUR\",\n",
        "#                ras_object=proj_obj\n",
        "#            )\n",
        "            total_configured += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error configuring plan {plan_number}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"CONFIGURATION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\u2713 Configured {total_configured} plans across {len(project_groups)} projects\")\n",
        "print(\"  - Cores per plan: 2\")\n",
        "#print(\"  - Computation interval: 15 MIN\")\n",
        "#print(\"  - Output interval: 30 MIN\")\n",
        "#print(\"  - Mapping interval: 1 HOUR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_scenarios_in_batches(scenario_df, batch_size=20, max_workers=4, cores_per_worker=2, ras_object=None):\n",
        "    \"\"\"\n",
        "    Execute scenarios in batches with progress tracking.\n",
        "    \n",
        "    Parameters:\n",
        "    - scenario_df: DataFrame with scenario information\n",
        "    - batch_size: Number of plans per batch\n",
        "    - max_workers: Number of parallel workers\n",
        "    - cores_per_worker: Cores per worker\n",
        "    - ras_object: RAS project object (uses global 'ras' if None)\n",
        "    \n",
        "    Returns:\n",
        "    - results_dict: Dictionary of execution results\n",
        "    - compute_folder: Path to compute folder\n",
        "    \"\"\"\n",
        "    # Use provided ras_object or try to get from DataFrame\n",
        "    if ras_object is None:\n",
        "        if 'project_object' in scenario_df.columns:\n",
        "            ras_obj = scenario_df.iloc[0]['project_object']\n",
        "        else:\n",
        "            ras_obj = ras  # Fall back to global\n",
        "    else:\n",
        "        ras_obj = ras_object\n",
        "    \n",
        "    # Create compute folder\n",
        "    compute_folder = Path(ras_obj.project_folder) / \"compute_uncertainty\"\n",
        "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Get all plan numbers\n",
        "    all_plans = scenario_df['plan_number'].tolist()\n",
        "    \n",
        "    # Split into batches\n",
        "    batches = [all_plans[i:i+batch_size] for i in range(0, len(all_plans), batch_size)]\n",
        "    \n",
        "    print(f\"Executing {len(all_plans)} plans in {len(batches)} batches\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Max workers: {max_workers}\")\n",
        "    print(f\"Cores per worker: {cores_per_worker}\")\n",
        "    print(f\"Compute folder: {compute_folder}\\n\")\n",
        "    \n",
        "    all_results = {}\n",
        "    overall_start = time.time()\n",
        "\n",
        "    from ras_commander import RasPlan  # Make sure RasPlan is imported\n",
        "\n",
        "    for batch_idx, batch_plans in enumerate(batches, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Batch {batch_idx}/{len(batches)}: Processing {len(batch_plans)} plans\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Set plan title for each plan in the batch, showing scenario information from scenario_df\n",
        "        for plan_number in batch_plans:\n",
        "            # Find the scenario row for this plan number\n",
        "            scenario_row = scenario_df.loc[scenario_df['plan_number'] == plan_number].squeeze()\n",
        "            # Compose a scenario title string\n",
        "            # This can be customized as needed. Here, include ARI, Duration, Project (if columns exist).\n",
        "            scenario_title_parts = []\n",
        "            for field in ['ARI', 'Duration', 'project_folder']:\n",
        "                if field in scenario_row:\n",
        "                    value = scenario_row[field]\n",
        "                    if pd.notna(value):\n",
        "                        scenario_title_parts.append(f\"{field}:{value}\")\n",
        "            # Join up to 32 chars (RAS limit)\n",
        "            scenario_title = \" | \".join(scenario_title_parts)[:32] or f\"Plan {plan_number}\"\n",
        "            # Set plan title using RasPlan\n",
        "            RasPlan.set_plan_title(plan_number, scenario_title, ras_object=ras_obj)\n",
        "\n",
        "        batch_start = time.time()\n",
        "        \n",
        "        # Execute batch\n",
        "        results = RasCmdr.compute_parallel(\n",
        "            plan_number=batch_plans,\n",
        "            max_workers=max_workers,\n",
        "            num_cores=cores_per_worker,\n",
        "            dest_folder=compute_folder,\n",
        "            clear_geompre=True,\n",
        "            overwrite_dest=True,\n",
        "            ras_object=ras_obj\n",
        "        )\n",
        "        \n",
        "        batch_duration = time.time() - batch_start\n",
        "        \n",
        "        # Update results\n",
        "        all_results.update(results)\n",
        "        \n",
        "        # Report batch results\n",
        "        success_count = sum(1 for v in results.values() if v)\n",
        "        print(f\"\\nBatch {batch_idx} completed in {batch_duration:.1f} seconds\")\n",
        "        print(f\"Success: {success_count}/{len(batch_plans)}\")\n",
        "        \n",
        "        # Progress summary\n",
        "        total_completed = batch_idx * batch_size\n",
        "        if total_completed > len(all_plans):\n",
        "            total_completed = len(all_plans)\n",
        "        print(f\"Overall progress: {total_completed}/{len(all_plans)} plans completed\")\n",
        "    \n",
        "    overall_duration = time.time() - overall_start\n",
        "\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ALL BATCHES COMPLETED\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total time: {overall_duration/60:.1f} minutes\")\n",
        "    print(f\"Average per plan: {overall_duration/len(all_plans):.1f} seconds\")\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame([\n",
        "        {\"plan_number\": plan, \"success\": success}\n",
        "        for plan, success in all_results.items()\n",
        "    ])\n",
        "    \n",
        "    success_rate = (results_df['success'].sum() / len(results_df)) * 100\n",
        "    print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
        "    print(f\"Successful: {results_df['success'].sum()}\")\n",
        "    print(f\"Failed: {(~results_df['success']).sum()}\")\n",
        "    \n",
        "    return all_results, compute_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute scenarios across all projects\n",
        "print(\"=\"*70)\n",
        "print(\"EXECUTING SCENARIOS ACROSS ALL PROJECTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_results = {}\n",
        "all_compute_folders = {}\n",
        "\n",
        "# Group by project\n",
        "project_groups = scenario_df.groupby('project_folder')\n",
        "\n",
        "for proj_folder, proj_scenarios in project_groups:\n",
        "    proj_name = Path(proj_folder).name\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing Project: {proj_name}\")\n",
        "    print(f\"Scenarios: {len(proj_scenarios)}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Get the project object\n",
        "    proj_obj = proj_scenarios.iloc[0]['project_object']\n",
        "    \n",
        "    # Execute scenarios for this project in batches\n",
        "    results, compute_folder = execute_scenarios_in_batches(\n",
        "        proj_scenarios,\n",
        "        batch_size=21,  # 3 CI levels \u00d7 7 durations = 21 plans per ARI\n",
        "        max_workers=4,\n",
        "        cores_per_worker=2\n",
        "    )\n",
        "    \n",
        "    # Store results and compute folder for later processing\n",
        "    all_results.update(results)\n",
        "    all_compute_folders[proj_folder] = compute_folder\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ALL PROJECTS EXECUTION COMPLETE\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total scenarios executed: {len(all_results)}\")\n",
        "print(f\"Projects: {len(all_compute_folders)}\")\n",
        "\n",
        "# Save compute folder mapping\n",
        "compute_folders_df = pd.DataFrame([\n",
        "    {'project_folder': pf, 'compute_folder': str(cf)}\n",
        "    for pf, cf in all_compute_folders.items()\n",
        "])\n",
        "compute_folders_df.to_csv('compute_folders.csv', index=False)\n",
        "print(f\"\\nCompute folder mapping saved to: compute_folders.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize compute projects and extract results from all projects\n",
        "print(\"=\"*70)\n",
        "print(\"EXTRACTING RESULTS FROM ALL PROJECTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_plans_with_results = []\n",
        "\n",
        "# Process each project's compute folder\n",
        "for proj_folder, compute_folder in all_compute_folders.items():\n",
        "    proj_name = Path(proj_folder).name\n",
        "    print(f\"\\nProcessing results from: {proj_name}\")\n",
        "    print(f\"Compute folder: {compute_folder}\")\n",
        "\n",
        "    # Initialize compute project to access results\n",
        "    compute_project = RasPrj()\n",
        "    compute_project = init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "\n",
        "    # Check which plans have results\n",
        "    plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()].copy()\n",
        "    plans_with_results['source_project'] = proj_folder\n",
        "    plans_with_results['compute_folder'] = str(compute_folder)\n",
        "\n",
        "    all_plans_with_results.append(plans_with_results)\n",
        "\n",
        "    print(f\"  Found {len(plans_with_results)} plans with HDF results\")\n",
        "\n",
        "# Combine all results\n",
        "if len(all_plans_with_results) > 0:\n",
        "    combined_results_df = pd.concat(all_plans_with_results, ignore_index=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"RESULTS EXTRACTION SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total plans with results: {len(combined_results_df)}\")\n",
        "\n",
        "    # Debug: Show what columns we have before merge\n",
        "    print(f\"\\nColumns in combined_results_df: {list(combined_results_df.columns)}\")\n",
        "    print(f\"Columns in scenario_df: {list(scenario_df.columns)}\")\n",
        "\n",
        "    # Merge with scenario information\n",
        "    results_df = scenario_df.merge(\n",
        "        combined_results_df[['plan_number', 'Short Identifier', 'HDF_Results_Path', 'source_project', 'compute_folder']],\n",
        "        on='plan_number',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nAfter merge:\")\n",
        "    print(f\"  Total rows in results_df: {len(results_df)}\")\n",
        "    print(f\"  Rows with HDF paths: {len(results_df[results_df['HDF_Results_Path'].notna()])}\")\n",
        "    print(f\"  Columns in results_df: {list(results_df.columns)}\")\n",
        "\n",
        "    # Check if scenario info is preserved\n",
        "    if 'ari' in results_df.columns:\n",
        "        print(f\"  \u2713 'ari' column preserved\")\n",
        "    else:\n",
        "        print(f\"  \u2717 'ari' column MISSING!\")\n",
        "\n",
        "    if 'duration_hrs' in results_df.columns:\n",
        "        print(f\"  \u2713 'duration_hrs' column preserved\")\n",
        "    else:\n",
        "        print(f\"  \u2717 'duration_hrs' column MISSING!\")\n",
        "\n",
        "    print(f\"\\nResults summary (first 15 rows):\")\n",
        "    display.display(results_df[['ari', 'duration_hrs', 'ci_level', 'plan_number', 'HDF_Results_Path']].head(15))\n",
        "\n",
        "    # Save results summary\n",
        "    results_df.to_csv('results_uncertainty_scenarios.csv', index=False)\n",
        "    print(f\"\\nResults summary saved to: results_uncertainty_scenarios.csv\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: No plans with results found!\")\n",
        "    results_df = scenario_df.copy()\n",
        "    results_df['HDF_Results_Path'] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Metrics from All Scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "diagnostic-check",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DIAGNOSTIC: Check DataFrame Columns\n",
        "# ============================================================================\n",
        "# Run this cell if you encounter KeyError: 'ari' in later cells\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATAFRAME DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check scenario_df\n",
        "if 'scenario_df' in dir():\n",
        "    print(f\"\\n1. scenario_df:\")\n",
        "    print(f\"   Shape: {scenario_df.shape}\")\n",
        "    print(f\"   Columns: {list(scenario_df.columns)}\")\n",
        "\n",
        "    required = ['ari', 'duration_hrs', 'ci_level', 'plan_number']\n",
        "    missing = [c for c in required if c not in scenario_df.columns]\n",
        "    if missing:\n",
        "        print(f\"   \u26a0\ufe0f  Missing: {missing}\")\n",
        "    else:\n",
        "        print(f\"   \u2713 Has all required scenario columns\")\n",
        "else:\n",
        "    print(f\"\\n1. scenario_df: NOT FOUND\")\n",
        "\n",
        "# Check results_df\n",
        "if 'results_df' in dir():\n",
        "    print(f\"\\n2. results_df:\")\n",
        "    print(f\"   Shape: {results_df.shape}\")\n",
        "    print(f\"   Columns: {list(results_df.columns)}\")\n",
        "\n",
        "    required = ['ari', 'duration_hrs', 'ci_level', 'plan_number', 'HDF_Results_Path']\n",
        "    missing = [c for c in required if c not in results_df.columns]\n",
        "    if missing:\n",
        "        print(f\"   \u26a0\ufe0f  Missing: {missing}\")\n",
        "    else:\n",
        "        print(f\"   \u2713 Has all required columns\")\n",
        "\n",
        "    if 'HDF_Results_Path' in results_df.columns:\n",
        "        with_hdf = results_df['HDF_Results_Path'].notna().sum()\n",
        "        print(f\"   Rows with HDF paths: {with_hdf}/{len(results_df)}\")\n",
        "else:\n",
        "    print(f\"\\n2. results_df: NOT FOUND\")\n",
        "\n",
        "# Check metrics_df\n",
        "if 'metrics_df' in dir():\n",
        "    print(f\"\\n3. metrics_df:\")\n",
        "    print(f\"   Shape: {metrics_df.shape}\")\n",
        "    print(f\"   Columns: {list(metrics_df.columns)}\")\n",
        "\n",
        "    required = ['ari', 'duration_hrs', 'ci_level']\n",
        "    missing = [c for c in required if c not in metrics_df.columns]\n",
        "    if missing:\n",
        "        print(f\"   \u26a0\ufe0f  Missing: {missing}\")\n",
        "        print(f\"   \\n   ACTION REQUIRED:\")\n",
        "        print(f\"   The metrics extraction didn't preserve scenario columns.\")\n",
        "        print(f\"   This means results_df was missing them.\")\n",
        "        print(f\"   Re-run the results extraction cell (Cell 28)\")\n",
        "    else:\n",
        "        print(f\"   \u2713 Has all required columns\")\n",
        "else:\n",
        "    print(f\"\\n3. metrics_df: NOT CREATED YET\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"END DIAGNOSTIC\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_scenario_metrics(results_df):\n",
        "    \"\"\"\n",
        "    Extract key metrics from all scenarios.\n",
        "    \"\"\"\n",
        "    metrics_list = []\n",
        "\n",
        "    print(\"Extracting metrics from all scenarios...\\n\")\n",
        "\n",
        "    # SAFETY CHECK: Verify required columns exist\n",
        "    required_cols = ['ari', 'duration_hrs', 'ci_level', 'total_depth_in', 'HDF_Results_Path', 'plan_number']\n",
        "    missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: results_df is missing required columns: {missing_cols}\")\n",
        "        print(f\"Available columns: {list(results_df.columns)}\")\n",
        "        print(f\"\\nCannot extract metrics without scenario information!\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    for idx, row in results_df.iterrows():\n",
        "        hdf_path = row.get('HDF_Results_Path')\n",
        "\n",
        "        if pd.isna(hdf_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Get runtime data\n",
        "            runtime_df = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "\n",
        "            # Get pipe network results\n",
        "            pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Pipes/Pipe Flow DS\")\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "\n",
        "            # Calculate statistics\n",
        "            pipe_flow_array = pipe_flow_ds.values\n",
        "            node_ws_array = node_ws.values\n",
        "\n",
        "            max_flows = np.nanmax(pipe_flow_array, axis=0)\n",
        "            max_ws = np.nanmax(node_ws_array, axis=0)\n",
        "\n",
        "            metrics = {\n",
        "                'plan_number': row['plan_number'],\n",
        "                'ari': row['ari'],\n",
        "                'duration_hrs': row['duration_hrs'],\n",
        "                'ci_level': row['ci_level'],\n",
        "                'total_precip_in': row['total_depth_in'],\n",
        "                'compute_time_hr': runtime_df['Complete Process (hr)'].iloc[0] if not runtime_df.empty else np.nan,\n",
        "                'avg_max_pipe_flow_cfs': np.nanmean(max_flows),\n",
        "                'max_pipe_flow_cfs': np.nanmax(max_flows),\n",
        "                'avg_max_node_ws_ft': np.nanmean(max_ws),\n",
        "                'max_node_ws_ft': np.nanmax(max_ws),\n",
        "                'hdf_path': hdf_path\n",
        "            }\n",
        "\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting metrics for plan {row.get('plan_number', 'unknown')}: {e}\")\n",
        "            continue\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics_list)\n",
        "\n",
        "    if len(metrics_df) > 0:\n",
        "        print(f\"\\n\u2713 Extracted metrics from {len(metrics_df)} scenarios\")\n",
        "        print(f\"Columns in metrics_df: {list(metrics_df.columns)}\")\n",
        "    else:\n",
        "        print(\"\\n\u2717 No metrics extracted!\")\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "# SAFETY CHECK: Verify results_df has required columns before extraction\n",
        "print(\"=\"*70)\n",
        "print(\"PRE-EXTRACTION SAFETY CHECKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "required_cols = ['ari', 'duration_hrs', 'ci_level', 'total_depth_in', 'plan_number', 'HDF_Results_Path']\n",
        "print(f\"\\nChecking results_df for required columns...\")\n",
        "print(f\"Required: {required_cols}\")\n",
        "\n",
        "if 'results_df' in dir():\n",
        "    print(f\"\\nresults_df columns: {list(results_df.columns)}\")\n",
        "    missing = [col for col in required_cols if col not in results_df.columns]\n",
        "\n",
        "    if missing:\n",
        "        print(f\"\\n\u26a0\ufe0f  WARNING: Missing columns: {missing}\")\n",
        "        print(\"\\nThis will cause KeyError in uncertainty analysis!\")\n",
        "        print(\"\\nDEBUGGING INFO:\")\n",
        "        print(f\"  - results_df shape: {results_df.shape}\")\n",
        "        print(f\"  - rows with HDF paths: {results_df['HDF_Results_Path'].notna().sum() if 'HDF_Results_Path' in results_df.columns else 'N/A'}\")\n",
        "        print(\"\\nLikely cause: Merge in results extraction didn't preserve scenario_df columns\")\n",
        "        print(\"Solution: Re-run results extraction cell (Cell 28)\")\n",
        "    else:\n",
        "        print(f\"\\n\u2713 All required columns present!\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  results_df not found in namespace\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Extract metrics\n",
        "metrics_df = extract_scenario_metrics(results_df)\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nMetrics Summary:\")\n",
        "if len(metrics_df) > 0:\n",
        "    display.display(metrics_df.head(15))\n",
        "\n",
        "    # Save metrics\n",
        "    metrics_df.to_csv('metrics_uncertainty_analysis.csv', index=False)\n",
        "    print(\"\\nMetrics saved to: metrics_uncertainty_analysis.csv\")\n",
        "else:\n",
        "    print(\"\\nNo metrics to display. Check errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncertainty Quantification\n",
        "\n",
        "Calculate uncertainty metrics for each ARI-Duration combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_uncertainty_metrics(metrics_df):\n",
        "    \"\"\"\n",
        "    Calculate uncertainty metrics for each ARI-Duration combination.\n",
        "    \"\"\"\n",
        "    # SAFETY CHECK: Verify required columns exist\n",
        "    required_cols = ['ari', 'duration_hrs', 'ci_level']\n",
        "    missing_cols = [col for col in required_cols if col not in metrics_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: metrics_df is missing required columns: {missing_cols}\")\n",
        "        print(f\"Available columns: {list(metrics_df.columns)}\")\n",
        "        print(f\"Cannot calculate uncertainty without these columns!\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame\n",
        "\n",
        "    if len(metrics_df) == 0:\n",
        "        print(\"ERROR: metrics_df is empty!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    uncertainty_list = []\n",
        "\n",
        "    for ari in metrics_df['ari'].unique():\n",
        "        for duration in metrics_df['duration_hrs'].unique():\n",
        "            # Filter data\n",
        "            subset = metrics_df[\n",
        "                (metrics_df['ari'] == ari) &\n",
        "                (metrics_df['duration_hrs'] == duration)\n",
        "            ]\n",
        "\n",
        "            if len(subset) < 3:\n",
        "                continue\n",
        "\n",
        "            # Get values by CI level\n",
        "            lower = subset[subset['ci_level'] == 'lower']\n",
        "            point = subset[subset['ci_level'] == 'point']\n",
        "            upper = subset[subset['ci_level'] == 'upper']\n",
        "\n",
        "            if len(lower) == 0 or len(point) == 0 or len(upper) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate uncertainty for max pipe flow\n",
        "            flow_point = point['max_pipe_flow_cfs'].values[0]\n",
        "            flow_lower = lower['max_pipe_flow_cfs'].values[0]\n",
        "            flow_upper = upper['max_pipe_flow_cfs'].values[0]\n",
        "            flow_range = flow_upper - flow_lower\n",
        "            flow_rel_uncertainty = (flow_range / flow_point) * 100 if flow_point > 0 else np.nan\n",
        "\n",
        "            # Calculate uncertainty for max node WS\n",
        "            ws_point = point['max_node_ws_ft'].values[0]\n",
        "            ws_lower = lower['max_node_ws_ft'].values[0]\n",
        "            ws_upper = upper['max_node_ws_ft'].values[0]\n",
        "            ws_range = ws_upper - ws_lower\n",
        "            ws_rel_uncertainty = (ws_range / ws_point) * 100 if ws_point > 0 else np.nan\n",
        "\n",
        "            uncertainty_list.append({\n",
        "                'ari': ari,\n",
        "                'duration_hrs': duration,\n",
        "                'max_flow_point_cfs': flow_point,\n",
        "                'max_flow_lower_cfs': flow_lower,\n",
        "                'max_flow_upper_cfs': flow_upper,\n",
        "                'max_flow_range_cfs': flow_range,\n",
        "                'max_flow_rel_uncertainty_pct': flow_rel_uncertainty,\n",
        "                'max_ws_point_ft': ws_point,\n",
        "                'max_ws_lower_ft': ws_lower,\n",
        "                'max_ws_upper_ft': ws_upper,\n",
        "                'max_ws_range_ft': ws_range,\n",
        "                'max_ws_rel_uncertainty_pct': ws_rel_uncertainty\n",
        "            })\n",
        "\n",
        "    uncertainty_df = pd.DataFrame(uncertainty_list)\n",
        "    return uncertainty_df\n",
        "\n",
        "# Calculate uncertainties\n",
        "uncertainty_df = calculate_uncertainty_metrics(metrics_df)\n",
        "\n",
        "if len(uncertainty_df) > 0:\n",
        "    print(\"Uncertainty Analysis:\")\n",
        "    display.display(uncertainty_df)\n",
        "\n",
        "    # Save uncertainty metrics\n",
        "    uncertainty_df.to_csv('uncertainty_metrics.csv', index=False)\n",
        "    print(\"\\nUncertainty metrics saved to: uncertainty_metrics.csv\")\n",
        "else:\n",
        "    print(\"\\nCannot perform uncertainty analysis - metrics_df has missing or invalid data\")\n",
        "    print(\"Please check the results extraction and metrics extraction steps above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Peak Flow vs Duration with Confidence Envelopes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Peak Water Surface vs Duration with Confidence Envelopes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot peak water surface vs duration for each ARI\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, ari in enumerate(sorted(metrics_df['ari'].unique())):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get data for this ARI\n",
        "    ari_data = uncertainty_df[uncertainty_df['ari'] == ari].sort_values('duration_hrs')\n",
        "    \n",
        "    if len(ari_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Plot point estimate\n",
        "    ax.plot(ari_data['duration_hrs'], ari_data['max_ws_point_ft'], \n",
        "            'ko-', linewidth=2, markersize=6, label='Point Estimate', zorder=3)\n",
        "    \n",
        "    # Plot confidence envelope\n",
        "    ax.fill_between(ari_data['duration_hrs'], \n",
        "                     ari_data['max_ws_lower_ft'], \n",
        "                     ari_data['max_ws_upper_ft'],\n",
        "                     alpha=0.3, color='green', label='90% CI', zorder=1)\n",
        "    \n",
        "    ax.set_xlabel('Duration (hours)', fontsize=11)\n",
        "    ax.set_ylabel('Peak Water Surface (ft)', fontsize=11)\n",
        "    ax.set_title(f'{ari}-Year Event', fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(alpha=0.3, zorder=0)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xticks(durations)\n",
        "    ax.set_xticklabels(durations)\n",
        "\n",
        "plt.suptitle('Peak Water Surface vs Storm Duration with Confidence Intervals', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.savefig('peak_ws_uncertainty.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: peak_ws_uncertainty.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Relative Uncertainty Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create heatmaps for relative uncertainty\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Pivot data for heatmaps\n",
        "flow_uncertainty_pivot = uncertainty_df.pivot(\n",
        "    index='ari',\n",
        "    columns='duration_hrs',\n",
        "    values='max_flow_rel_uncertainty_pct'\n",
        ")\n",
        "\n",
        "ws_uncertainty_pivot = uncertainty_df.pivot(\n",
        "    index='ari',\n",
        "    columns='duration_hrs',\n",
        "    values='max_ws_rel_uncertainty_pct'\n",
        ")\n",
        "\n",
        "# Plot flow uncertainty heatmap\n",
        "sns.heatmap(flow_uncertainty_pivot, annot=True, fmt='.1f', cmap='YlOrRd', \n",
        "            cbar_kws={'label': 'Relative Uncertainty (%)'}, ax=ax1)\n",
        "ax1.set_title('Relative Uncertainty in Peak Flow', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Duration (hours)', fontsize=12)\n",
        "ax1.set_ylabel('Return Period (years)', fontsize=12)\n",
        "\n",
        "# Plot WS uncertainty heatmap\n",
        "sns.heatmap(ws_uncertainty_pivot, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "            cbar_kws={'label': 'Relative Uncertainty (%)'}, ax=ax2)\n",
        "ax2.set_title('Relative Uncertainty in Peak Water Surface', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Duration (hours)', fontsize=12)\n",
        "ax2.set_ylabel('Return Period (years)', fontsize=12)\n",
        "\n",
        "plt.suptitle('Precipitation Uncertainty Propagation Through Flood Model', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('uncertainty_heatmaps.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Figure saved: uncertainty_heatmaps.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Time Series with Confidence Envelope at Critical Location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_timeseries_with_ci(ari, duration, location_id=61):\n",
        "    \"\"\"\n",
        "    Plot time series showing confidence envelope at a specific location.\n",
        "    \"\"\"\n",
        "    # Get scenarios for this ARI and duration\n",
        "    scenarios = metrics_df[\n",
        "        (metrics_df['ari'] == ari) & \n",
        "        (metrics_df['duration_hrs'] == duration)\n",
        "    ]\n",
        "    \n",
        "    if len(scenarios) == 0:\n",
        "        print(f\"No data found for {ari}-year, {duration}-hour storm\")\n",
        "        return\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    \n",
        "    # Extract time series for each CI level\n",
        "    for ci_level in ['lower', 'point', 'upper']:\n",
        "        scenario = scenarios[scenarios['ci_level'] == ci_level]\n",
        "        \n",
        "        if len(scenario) == 0:\n",
        "            continue\n",
        "        \n",
        "        hdf_path = scenario['hdf_path'].values[0]\n",
        "        \n",
        "        try:\n",
        "            # Get node water surface time series\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "            \n",
        "            # Extract data for specified location\n",
        "            loc_ws = node_ws.sel(location=location_id)\n",
        "            \n",
        "            # Plot\n",
        "            if ci_level == 'point':\n",
        "                ax.plot(loc_ws.time.values, loc_ws.values, \n",
        "                       'k-', linewidth=2, label='Point Estimate', zorder=3)\n",
        "            elif ci_level == 'lower':\n",
        "                lower_data = loc_ws.values\n",
        "                lower_time = loc_ws.time.values\n",
        "            elif ci_level == 'upper':\n",
        "                upper_data = loc_ws.values\n",
        "                upper_time = loc_ws.time.values\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {ci_level} data: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Plot confidence envelope\n",
        "    if 'lower_data' in locals() and 'upper_data' in locals():\n",
        "        ax.fill_between(lower_time, lower_data, upper_data, \n",
        "                       alpha=0.3, color='blue', label='90% CI', zorder=1)\n",
        "    \n",
        "    ax.set_xlabel('Time', fontsize=12)\n",
        "    ax.set_ylabel('Water Surface Elevation (ft)', fontsize=12)\n",
        "    ax.set_title(f'Water Surface at Node {location_id}\\n' +\n",
        "                f'{ari}-Year, {duration}-Hour Storm with Confidence Interval', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(alpha=0.3, zorder=0)\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot time series for selected scenarios\n",
        "print(\"Time Series with Confidence Envelopes\\n\")\n",
        "plot_timeseries_with_ci(10, 24, location_id=61)\n",
        "plot_timeseries_with_ci(100, 24, location_id=61)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_all_aeps_for_duration(duration, location_id=61):\n",
        "    \"\"\"\n",
        "    Plot all AEP/ARI levels for a specific duration with confidence intervals.\n",
        "    Creates one plot showing all storm frequencies for the given duration.\n",
        "    \"\"\"\n",
        "    # Get all unique ARIs for this duration\n",
        "    duration_data = metrics_df[metrics_df['duration_hrs'] == duration]\n",
        "    aris = sorted(duration_data['ari'].unique())\n",
        "    \n",
        "    if len(aris) == 0:\n",
        "        print(f\"No data found for {duration}-hour duration\")\n",
        "        return\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    \n",
        "    # Color palette for different ARIs\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(aris)))\n",
        "    \n",
        "    for idx, ari in enumerate(aris):\n",
        "        scenarios = duration_data[duration_data['ari'] == ari]\n",
        "        \n",
        "        lower_data = None\n",
        "        upper_data = None\n",
        "        point_data = None\n",
        "        time_data = None\n",
        "        \n",
        "        # Extract time series for each CI level\n",
        "        for ci_level in ['lower', 'point', 'upper']:\n",
        "            scenario = scenarios[scenarios['ci_level'] == ci_level]\n",
        "            \n",
        "            if len(scenario) == 0:\n",
        "                continue\n",
        "            \n",
        "            hdf_path = scenario['hdf_path'].values[0]\n",
        "            \n",
        "            try:\n",
        "                # Get node water surface time series\n",
        "                node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "                \n",
        "                # Extract data for specified location\n",
        "                loc_ws = node_ws.sel(location=location_id)\n",
        "                \n",
        "                if ci_level == 'point':\n",
        "                    point_data = loc_ws.values\n",
        "                    time_data = loc_ws.time.values\n",
        "                elif ci_level == 'lower':\n",
        "                    lower_data = loc_ws.values\n",
        "                elif ci_level == 'upper':\n",
        "                    upper_data = loc_ws.values\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting {ari}-year {ci_level} data: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Plot confidence envelope and point estimate\n",
        "        if point_data is not None and time_data is not None:\n",
        "            # Plot confidence envelope first (lower z-order)\n",
        "            if lower_data is not None and upper_data is not None:\n",
        "                ax.fill_between(time_data, lower_data, upper_data, \n",
        "                               alpha=0.2, color=colors[idx], zorder=1)\n",
        "            \n",
        "            # Plot point estimate on top\n",
        "            ax.plot(time_data, point_data, \n",
        "                   color=colors[idx], linewidth=2.5, \n",
        "                   label=f'{ari}-Year (90% CI)', zorder=2)\n",
        "    \n",
        "    ax.set_xlabel('Time', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel('Water Surface Elevation (ft)', fontsize=13, fontweight='bold')\n",
        "    ax.set_title(f'Water Surface at Node {location_id}\\n' +\n",
        "                f'{duration}-Hour Duration - All Return Periods with 90% Confidence Intervals', \n",
        "                fontsize=15, fontweight='bold')\n",
        "    ax.legend(fontsize=11, loc='best', framealpha=0.9)\n",
        "    ax.grid(alpha=0.3, linestyle='--', zorder=0)\n",
        "    plt.gcf().autofmt_xdate()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_all_durations_all_aeps(location_id=61):\n",
        "    \"\"\"\n",
        "    Create a separate plot for each duration showing all AEP levels.\n",
        "    \"\"\"\n",
        "    # Get all unique durations\n",
        "    durations = sorted(metrics_df['duration_hrs'].unique())\n",
        "    \n",
        "    print(f\"Creating plots for {len(durations)} durations at Node {location_id}\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for duration in durations:\n",
        "        print(f\"\\n{duration}-Hour Duration Storm\")\n",
        "        print(\"-\" * 70)\n",
        "        plot_all_aeps_for_duration(duration, location_id)\n",
        "\n",
        "\n",
        "# Generate all plots\n",
        "plot_all_durations_all_aeps(location_id=61)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics and Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"UNCERTAINTY ANALYSIS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. OVERALL UNCERTAINTY STATISTICS\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Average relative uncertainty in peak flow: {uncertainty_df['max_flow_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"Average relative uncertainty in peak WSE: {uncertainty_df['max_ws_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"Range of uncertainty in peak flow: {uncertainty_df['max_flow_rel_uncertainty_pct'].min():.1f}% - {uncertainty_df['max_flow_rel_uncertainty_pct'].max():.1f}%\")\n",
        "print(f\"Range of uncertainty in peak WSE: {uncertainty_df['max_ws_rel_uncertainty_pct'].min():.1f}% - {uncertainty_df['max_ws_rel_uncertainty_pct'].max():.1f}%\")\n",
        "\n",
        "print(\"\\n2. SCENARIOS WITH HIGHEST UNCERTAINTY\")\n",
        "print(\"-\" * 70)\n",
        "top_flow_uncertainty = uncertainty_df.nlargest(5, 'max_flow_rel_uncertainty_pct')\n",
        "print(\"\\nTop 5 - Peak Flow Uncertainty:\")\n",
        "for idx, row in top_flow_uncertainty.iterrows():\n",
        "    print(f\"  {row['ari']}-year, {row['duration_hrs']}-hour: {row['max_flow_rel_uncertainty_pct']:.1f}%\")\n",
        "\n",
        "top_ws_uncertainty = uncertainty_df.nlargest(5, 'max_ws_rel_uncertainty_pct')\n",
        "print(\"\\nTop 5 - Water Surface Uncertainty:\")\n",
        "for idx, row in top_ws_uncertainty.iterrows():\n",
        "    print(f\"  {row['ari']}-year, {row['duration_hrs']}-hour: {row['max_ws_rel_uncertainty_pct']:.1f}%\")\n",
        "\n",
        "print(\"\\n3. UNCERTAINTY BY RETURN PERIOD\")\n",
        "print(\"-\" * 70)\n",
        "by_ari = uncertainty_df.groupby('ari').agg({\n",
        "    'max_flow_rel_uncertainty_pct': 'mean',\n",
        "    'max_ws_rel_uncertainty_pct': 'mean'\n",
        "}).round(1)\n",
        "print(by_ari)\n",
        "\n",
        "print(\"\\n4. UNCERTAINTY BY DURATION\")\n",
        "print(\"-\" * 70)\n",
        "by_duration = uncertainty_df.groupby('duration_hrs').agg({\n",
        "    'max_flow_rel_uncertainty_pct': 'mean',\n",
        "    'max_ws_rel_uncertainty_pct': 'mean'\n",
        "}).round(1)\n",
        "print(by_duration)\n",
        "\n",
        "print(\"\\n5. DESIGN IMPLICATIONS\")\n",
        "print(\"-\" * 70)\n",
        "print(\"Based on the uncertainty analysis:\")\n",
        "print(f\"  \u2022 Precipitation uncertainty of \u00b140% results in:\")\n",
        "print(f\"    - Peak flow uncertainty: {uncertainty_df['max_flow_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"    - Peak WSE uncertainty: {uncertainty_df['max_ws_rel_uncertainty_pct'].mean():.1f}%\")\n",
        "print(f\"  \u2022 Higher uncertainties observed for:\")\n",
        "most_uncertain_duration = by_duration['max_flow_rel_uncertainty_pct'].idxmax()\n",
        "print(f\"    - Duration: {most_uncertain_duration} hours\")\n",
        "most_uncertain_ari = by_ari['max_flow_rel_uncertainty_pct'].idxmax()\n",
        "print(f\"    - Return period: {most_uncertain_ari} years\")\n",
        "print(f\"  \u2022 Designers should consider upper bound scenarios for critical infrastructure\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Analysis complete. All results saved to CSV files.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "This comprehensive uncertainty analysis demonstrates:\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Uncertainty Propagation**: Precipitation uncertainty of approximately \u00b140% (90% confidence interval) propagates through the hydraulic model with varying impacts depending on storm characteristics.\n",
        "\n",
        "2. **Duration Effects**: Different storm durations exhibit varying sensitivity to precipitation uncertainty, with some durations showing amplified uncertainty in hydraulic responses.\n",
        "\n",
        "3. **Return Period Patterns**: The relationship between return period and uncertainty provides insights into which design events have the most variability.\n",
        "\n",
        "4. **Spatial Considerations**: Peak water surface elevations and pipe flows show different uncertainty patterns, important for infrastructure design decisions.\n",
        "\n",
        "### Design Recommendations\n",
        "\n",
        "- **Use upper confidence bounds** for critical infrastructure design\n",
        "- **Consider duration sensitivity** when selecting design storms\n",
        "- **Account for uncertainty** in flood risk communication\n",
        "- **Evaluate multiple scenarios** rather than single point estimates\n",
        "\n",
        "### Methodology Advantages\n",
        "\n",
        "- **Comprehensive Coverage**: All practical durations analyzed\n",
        "- **Systematic Approach**: Consistent methodology across all scenarios\n",
        "- **Quantified Uncertainty**: Clear metrics for decision-making\n",
        "- **Reproducible**: Fully documented workflow\n",
        "\n",
        "### Future Enhancements\n",
        "\n",
        "This framework can be extended to include:\n",
        "- Climate change adjustments to precipitation\n",
        "- Infiltration parameter uncertainty\n",
        "- Manning's n coefficient uncertainty\n",
        "- Combined uncertainty analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\105_mannings_sensitivity_bulk_analysis.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mannings Bulk Sensitivity Analysis\n",
        "\n",
        "This notebook provides tools for efficiently analyzing the sensitivity of HEC-RAS models to changing Mannings roughness coefficient values. The main function `autoras_mannings_bulk_sensitivity` automates the creation, execution, and comparison of model runs with different Mannings n values based on recommended ranges from literature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Setup ras-commander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from seaborn) (2.2.6)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from seaborn) (2.3.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from seaborn) (3.10.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ras_commander imported successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# This cell will try to import the pip package; if it fails, it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "\n",
        "# Flexible imports to allow for development without installation\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "\n",
        "print(\"ras_commander imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note on DataFrame Column Naming\n",
        "\n",
        "**Important:** The ras-commander library uses simplified column names in DataFrames for Manning's n values:\n",
        "\n",
        "- DataFrame column:  (no apostrophe)\n",
        "- HEC-RAS HDF files:  (with apostrophe - official technical term)\n",
        "\n",
        "This design decision simplifies DataFrame operations by avoiding special characters in column names, \n",
        "while HEC-RAS's internal HDF structure retains the technically correct spelling with the apostrophe.\n",
        "\n",
        "When working with Manning's n values from , always reference the column as  (without apostrophe)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Mannings n Value Ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mannings n value ranges for 16 land cover types:\n",
            "                 Land Cover Name  min_n  max_n   mid_n\n",
            "0     Barren Land Rock/Sand/Clay  0.023   0.10  0.0615\n",
            "1               Cultivated Crops  0.020   0.10  0.0600\n",
            "2               Deciduous Forest  0.100   0.20  0.1500\n",
            "3      Developed, High Intensity  0.120   0.20  0.1600\n",
            "4       Developed, Low Intensity  0.060   0.12  0.0900\n",
            "5    Developed, Medium Intensity  0.080   0.16  0.1200\n",
            "6          Developed, Open Space  0.030   0.09  0.0600\n",
            "7   Emergent Herbaceous Wetlands  0.050   0.12  0.0850\n",
            "8               Evergreen Forest  0.080   0.16  0.1200\n",
            "9           Grassland/Herbaceous  0.025   0.07  0.0475\n",
            "10                  Mixed Forest  0.080   0.20  0.1400\n",
            "11                        NoData  0.050   0.07  0.0600\n",
            "12                    Open Water  0.025   0.05  0.0375\n",
            "13                   Pasture/Hay  0.025   0.09  0.0575\n",
            "14                   Shrub/Scrub  0.070   0.16  0.1150\n",
            "15                Woody Wetlands  0.045   0.15  0.0975\n"
          ]
        }
      ],
      "source": [
        "def create_manning_minmax_df():\n",
        "    \"\"\"\n",
        "    Create a dataframe containing minimum and maximum Mannings n values\n",
        "    based on recommended ranges from literature.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns for Land Cover Name, min_n, max_n\n",
        "    \"\"\"\n",
        "    # Define the data as a list of dictionaries\n",
        "    manning_data = [\n",
        "        {\"Land Cover Name\": \"NoData\", \"min_n\": 0.050, \"max_n\": 0.070},\n",
        "        {\"Land Cover Name\": \"Barren Land Rock/Sand/Clay\", \"min_n\": 0.023, \"max_n\": 0.100},\n",
        "        {\"Land Cover Name\": \"Cultivated Crops\", \"min_n\": 0.020, \"max_n\": 0.100},\n",
        "        {\"Land Cover Name\": \"Deciduous Forest\", \"min_n\": 0.100, \"max_n\": 0.200},\n",
        "        {\"Land Cover Name\": \"Developed, High Intensity\", \"min_n\": 0.120, \"max_n\": 0.200},\n",
        "        {\"Land Cover Name\": \"Developed, Low Intensity\", \"min_n\": 0.060, \"max_n\": 0.120},\n",
        "        {\"Land Cover Name\": \"Developed, Medium Intensity\", \"min_n\": 0.080, \"max_n\": 0.160},\n",
        "        {\"Land Cover Name\": \"Developed, Open Space\", \"min_n\": 0.030, \"max_n\": 0.090},\n",
        "        {\"Land Cover Name\": \"Emergent Herbaceous Wetlands\", \"min_n\": 0.050, \"max_n\": 0.120},\n",
        "        {\"Land Cover Name\": \"Evergreen Forest\", \"min_n\": 0.080, \"max_n\": 0.160},\n",
        "        {\"Land Cover Name\": \"Grassland/Herbaceous\", \"min_n\": 0.025, \"max_n\": 0.070},\n",
        "        {\"Land Cover Name\": \"Mixed Forest\", \"min_n\": 0.080, \"max_n\": 0.200},\n",
        "        {\"Land Cover Name\": \"Open Water\", \"min_n\": 0.025, \"max_n\": 0.050},\n",
        "        {\"Land Cover Name\": \"Pasture/Hay\", \"min_n\": 0.025, \"max_n\": 0.090},\n",
        "        {\"Land Cover Name\": \"Shrub/Scrub\", \"min_n\": 0.070, \"max_n\": 0.160},\n",
        "        {\"Land Cover Name\": \"Woody Wetlands\", \"min_n\": 0.045, \"max_n\": 0.150}\n",
        "    ]\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(manning_data)\n",
        "    \n",
        "    # Calculate the midpoint value\n",
        "    df['mid_n'] = (df['min_n'] + df['max_n']) / 2\n",
        "    \n",
        "    # Sort by land cover name\n",
        "    df = df.sort_values('Land Cover Name').reset_index(drop=True)\n",
        "    \n",
        "    # Print summary information\n",
        "    print(f\"Mannings n value ranges for {len(df)} land cover types:\")\n",
        "    print(df)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create the Mannings n ranges dataframe\n",
        "manning_minmax_df = create_manning_minmax_df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mannings Bulk Sensitivity Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoras_mannings_bulk_sensitivity(\n",
        "    project_folder,\n",
        "    template_plan,\n",
        "    manning_minmax_df=None,\n",
        "    include_regional_overrides=True,\n",
        "    include_base_overrides=True,\n",
        "    point_of_interest=None,\n",
        "    output_folder=\"Mannings_Bulk_Sensitivity\",\n",
        "    run_parallel=True,\n",
        "    max_workers=2,\n",
        "    num_cores=2\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform Mannings n bulk sensitivity analysis by running model with minimum,\n",
        "    maximum, and current Mannings n values.\n",
        "    \n",
        "    Args:\n",
        "        project_folder (str): Path to HEC-RAS project folder\n",
        "        template_plan (str): Plan number to use as template (e.g., \"01\")\n",
        "        manning_minmax_df (pd.DataFrame, optional): DataFrame with min/max n values\n",
        "        include_regional_overrides (bool, optional): Whether to adjust regional\n",
        "            Mannings values. Default is True.\n",
        "        include_base_overrides (bool, optional): Whether to adjust base \n",
        "            Mannings values. Default is True.\n",
        "        point_of_interest (tuple or Point, optional): Coordinates for extracting results\n",
        "        output_folder (str, optional): Name of output folder for results\n",
        "        run_parallel (bool, optional): Whether to run plans in parallel\n",
        "        max_workers (int, optional): Number of parallel workers\n",
        "        num_cores (int, optional): Number of cores per worker\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results of sensitivity analysis\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Validate inputs\n",
        "    if not include_regional_overrides and not include_base_overrides:\n",
        "        raise ValueError(\"At least one of include_regional_overrides or include_base_overrides must be True\")\n",
        "    \n",
        "    # Use default manning_minmax_df if none provided\n",
        "    if manning_minmax_df is None:\n",
        "        manning_minmax_df = create_manning_minmax_df()\n",
        "    \n",
        "    # Convert point_of_interest to Point if provided\n",
        "    if point_of_interest is not None and not isinstance(point_of_interest, Point):\n",
        "        point_of_interest = Point(point_of_interest[0], point_of_interest[1])\n",
        "    \n",
        "    # Create timestamp for unique run identifier\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Initialize RAS project using HEC-RAS Version 6.6 (Change if needed)\n",
        "    print(f\"Initializing HEC-RAS project: {project_folder}\")\n",
        "    ras = init_ras_project(project_folder, \"6.6\")\n",
        "    \n",
        "    # Create output directory\n",
        "    results_dir = Path(project_folder) / output_folder\n",
        "    results_dir.mkdir(exist_ok=True)\n",
        "    print(f\"Results will be saved to: {results_dir}\")\n",
        "    \n",
        "    # Display available plans\n",
        "    print(\"\\nAvailable plans:\")\n",
        "    print(ras.plan_df[['plan_number', 'Plan Title', 'Short Identifier']])\n",
        "    \n",
        "    # Verify template plan exists\n",
        "    if template_plan not in ras.plan_df['plan_number'].values:\n",
        "        raise ValueError(f\"Template plan {template_plan} not found in project\")\n",
        "    \n",
        "    # Get the geometry number associated with the template plan\n",
        "    template_geom = ras.plan_df.loc[ras.plan_df['plan_number'] == template_plan, 'geometry_number'].values[0]\n",
        "    print(f\"\\nTemplate plan: {template_plan} (Geometry: {template_geom})\")\n",
        "    \n",
        "    # Get the geometry file path\n",
        "    geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == template_geom, 'full_path'].values[0]\n",
        "    \n",
        "    # Get the original Mannings values\n",
        "    original_baseoverrides = RasGeo.get_mannings_baseoverrides(geom_path)\n",
        "    original_regionoverrides = RasGeo.get_mannings_regionoverrides(geom_path)\n",
        "    \n",
        "    # Check if regional overrides exist\n",
        "    has_regional_overrides = not original_regionoverrides.empty\n",
        "    \n",
        "    if include_regional_overrides and not has_regional_overrides:\n",
        "        raise ValueError(\"include_regional_overrides is True, but no regional overrides found in the model\")\n",
        "    \n",
        "    # Store the current plan as \"current\" scenario\n",
        "    scenarios = [{\n",
        "        'name': 'Current',\n",
        "        'plan_number': template_plan,\n",
        "        'geom_number': template_geom,\n",
        "        'shortid': f\"Current\",\n",
        "        'description': \"Current Mannings n Values\"\n",
        "    }]\n",
        "    \n",
        "    # Function to create a modified plan with adjusted Mannings values\n",
        "    def create_modified_plan(name, shortid, description, min_values=False, max_values=False):\n",
        "        print(f\"\\nCreating plan: {name} (ShortID: {shortid})\")\n",
        "        print(f\"Description: {description}\")\n",
        "        \n",
        "        # Clone the template plan\n",
        "        new_plan_number = RasPlan.clone_plan(template_plan, new_plan_shortid=shortid)\n",
        "        \n",
        "        # Clone the template geometry\n",
        "        new_geom_number = RasPlan.clone_geom(template_geom)\n",
        "        \n",
        "        # Set the new plan to use the new geometry\n",
        "        RasPlan.set_geom(new_plan_number, new_geom_number)\n",
        "        \n",
        "        # Get the new geometry file path\n",
        "        new_geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number, 'full_path'].values[0]\n",
        "        \n",
        "        # Adjust base Mannings values if enabled\n",
        "        if include_base_overrides:\n",
        "            modified_baseoverrides = original_baseoverrides.copy()\n",
        "            \n",
        "            # For each land cover type in the base overrides\n",
        "            for idx, row in modified_baseoverrides.iterrows():\n",
        "                land_cover = row['Land Cover Name']\n",
        "                \n",
        "                # Find matching land cover in manning_minmax_df\n",
        "                match = manning_minmax_df[manning_minmax_df['Land Cover Name'] == land_cover]\n",
        "                \n",
        "                if not match.empty:\n",
        "                    current_value = row[\"Base Mannings n Value\"]\n",
        "                    \n",
        "                    if min_values:\n",
        "                        new_value = match['min_n'].values[0]\n",
        "                    elif max_values:\n",
        "                        new_value = match['max_n'].values[0]\n",
        "                    else:\n",
        "                        new_value = current_value  # No change\n",
        "                    \n",
        "                    modified_baseoverrides.loc[idx, \"Base Mannings n Value\"] = new_value\n",
        "                    print(f\"  Adjusted base override for '{land_cover}': {current_value:.4f} \u2192 {new_value:.4f}\")\n",
        "            \n",
        "            # Apply modified base Mannings values\n",
        "            RasGeo.set_mannings_baseoverrides(new_geom_path, modified_baseoverrides)\n",
        "        else:\n",
        "            # Just copy the original base overrides\n",
        "            RasGeo.set_mannings_baseoverrides(new_geom_path, original_baseoverrides)\n",
        "        \n",
        "        # Adjust regional Mannings values if enabled and they exist\n",
        "        if include_regional_overrides and has_regional_overrides:\n",
        "            modified_regionoverrides = original_regionoverrides.copy()\n",
        "            \n",
        "            # For each row in the region overrides\n",
        "            for idx, row in modified_regionoverrides.iterrows():\n",
        "                land_cover = row['Land Cover Name']\n",
        "                region_name = row['Region Name']\n",
        "                \n",
        "                # Find matching land cover in manning_minmax_df\n",
        "                match = manning_minmax_df[manning_minmax_df['Land Cover Name'] == land_cover]\n",
        "                \n",
        "                if not match.empty:\n",
        "                    current_value = row[\"MainChannel\"]\n",
        "                    \n",
        "                    if min_values:\n",
        "                        new_value = match['min_n'].values[0]\n",
        "                    elif max_values:\n",
        "                        new_value = match['max_n'].values[0]\n",
        "                    else:\n",
        "                        new_value = current_value  # No change\n",
        "                    \n",
        "                    modified_regionoverrides.loc[idx, \"MainChannel\"] = new_value\n",
        "                    print(f\"  Adjusted region override for '{land_cover}' in '{region_name}': {current_value:.4f} \u2192 {new_value:.4f}\")\n",
        "            \n",
        "            # Apply modified regional Mannings values\n",
        "            RasGeo.set_mannings_regionoverrides(new_geom_path, modified_regionoverrides)\n",
        "        elif has_regional_overrides:\n",
        "            # Just copy the original region overrides\n",
        "            RasGeo.set_mannings_regionoverrides(new_geom_path, original_regionoverrides)\n",
        "        \n",
        "        # Store scenario details\n",
        "        return {\n",
        "            'name': name,\n",
        "            'plan_number': new_plan_number,\n",
        "            'geom_number': new_geom_number,\n",
        "            'shortid': shortid,\n",
        "            'description': description\n",
        "        }\n",
        "    \n",
        "    # Create minimum and maximum scenarios\n",
        "    min_scenario = create_modified_plan(\n",
        "        name=\"Minimum\",\n",
        "        shortid=\"Min_n\",\n",
        "        description=\"Minimum Recommended Mannings n Values\",\n",
        "        min_values=True\n",
        "    )\n",
        "\n",
        "    print(f\"Minimum Scenario Plan Number: {min_scenario}\")\n",
        "    \n",
        "    max_scenario = create_modified_plan(\n",
        "        name=\"Maximum\",\n",
        "        shortid=\"Max_n\",\n",
        "        description=\"Maximum Recommended Mannings n Values\",\n",
        "        max_values=True\n",
        "    )\n",
        "    \n",
        "    print(f\"Maximum Scenario Plan Number: {max_scenario}\")\n",
        "\n",
        "    # Add scenarios to list\n",
        "    scenarios.append(min_scenario)\n",
        "    scenarios.append(max_scenario)\n",
        "\n",
        "    print(f\"Scenarios: \\n{scenarios}\")\n",
        "    \n",
        "    # Get plan numbers for the new scenarios (excluding template which is already computed)\n",
        "    plan_numbers = [str(scenario['plan_number']) for scenario in scenarios]\n",
        "    print(f\"Plan Numbers: \\n{plan_numbers}\")\n",
        "\n",
        "    # Save scenario information\n",
        "    scenario_info = pd.DataFrame(scenarios)\n",
        "    scenario_info_path = results_dir / \"scenarios.csv\"\n",
        "    scenario_info.to_csv(scenario_info_path, index=False)\n",
        "    print(f\"\\nScenario information saved to: {scenario_info_path}\")\n",
        "    \n",
        "    # Run the plans (either in parallel or sequentially)\n",
        "    if run_parallel:\n",
        "        print(f\"\\nRunning {len(plan_numbers)} plans in parallel...\")\n",
        "        results = RasCmdr.compute_parallel(\n",
        "            plan_number=plan_numbers,\n",
        "            max_workers=max_workers,\n",
        "            num_cores=num_cores,\n",
        "            clear_geompre=True\n",
        "        )\n",
        "    else:\n",
        "        print(f\"\\nRunning {len(plan_numbers)} plans sequentially...\")\n",
        "        results = {}\n",
        "        for plan_number in plan_numbers:\n",
        "            print(f\"  Running plan {plan_number}...\")\n",
        "            result = RasCmdr.compute_plan(plan_number, num_cores=num_cores, clear_geompre=True)\n",
        "            results[plan_number] = result\n",
        "    \n",
        "    print(\"\\nExecution results:\")\n",
        "    for plan, success in results.items():\n",
        "        print(f\"  Plan {plan}: {'Successful' if success else 'Failed'}\")\n",
        "    \n",
        "    # Early return if no point of interest is provided\n",
        "    if point_of_interest is None:\n",
        "        print(\"\\nNo point of interest provided. Skipping results extraction and analysis.\")\n",
        "        return {\n",
        "            'scenarios': scenarios,\n",
        "            'execution_results': results,\n",
        "            'output_folder': results_dir\n",
        "        }\n",
        "    \n",
        "    # Find nearest mesh cell for result extraction\n",
        "    # Use the geometry from the first executed plan for cell identification\n",
        "    geom_hdf_path = None\n",
        "    for scenario in scenarios:\n",
        "        geom_number = scenario['geom_number']\n",
        "        try:\n",
        "            geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    if geom_hdf_path is None:\n",
        "        print(\"ERROR: Could not find HDF path for any geometry\")\n",
        "        return {\n",
        "            'scenarios': scenarios,\n",
        "            'execution_results': results,\n",
        "            'output_folder': results_dir\n",
        "        }\n",
        "    \n",
        "    plan_number = scenario['plan_number']\n",
        "\n",
        "    # Find the nearest mesh cell to the point of interest\n",
        "    mesh_cells_gdf = HdfMesh.get_mesh_cell_points(plan_number)\n",
        "    \n",
        "    # Calculate distances from the point to all mesh cells\n",
        "    distances = mesh_cells_gdf.geometry.apply(lambda geom: geom.distance(point_of_interest))\n",
        "    \n",
        "    # Find the index of the minimum distance\n",
        "    nearest_idx = distances.idxmin()\n",
        "    \n",
        "    # Get the cell ID for results extraction\n",
        "    mesh_cell_for_results = mesh_cells_gdf.loc[nearest_idx, 'cell_id']\n",
        "    mesh_name = mesh_cells_gdf.loc[nearest_idx, 'mesh_name']\n",
        "    \n",
        "    print(f\"\\nNearest cell ID: {mesh_cell_for_results}\")\n",
        "    print(f\"Distance: {distances[nearest_idx]:.2f} units\")\n",
        "    print(f\"Mesh area: {mesh_name}\")\n",
        "    \n",
        "    # Extract and store results\n",
        "    all_results = {}\n",
        "    max_ws_values = []\n",
        "    \n",
        "    # Get results for each scenario\n",
        "    for scenario in scenarios:\n",
        "        plan_number = scenario['plan_number']\n",
        "        name = scenario['name']\n",
        "        shortid = scenario['shortid']\n",
        "        \n",
        "        # Get the results for this plan\n",
        "        try:\n",
        "            results_xr = HdfResultsMesh.get_mesh_cells_timeseries(plan_number)\n",
        "            \n",
        "            # Extract water surface data for the specific cell\n",
        "            ws_data = results_xr[mesh_name]['Water Surface'].sel(cell_id=int(mesh_cell_for_results))\n",
        "            \n",
        "            # Convert to DataFrame for easier handling\n",
        "            ws_df = pd.DataFrame({\n",
        "                'time': ws_data.time.values,\n",
        "                'water_surface': ws_data.values\n",
        "            })\n",
        "            \n",
        "            # Store in results dictionary\n",
        "            all_results[plan_number] = {\n",
        "                'name': name,\n",
        "                'shortid': shortid,\n",
        "                'df': ws_df,\n",
        "                'max_water_surface': ws_df['water_surface'].max()\n",
        "            }\n",
        "            \n",
        "            # Store the maximum water surface value for summary\n",
        "            max_ws = ws_df['water_surface'].max()\n",
        "            max_ws_values.append({\n",
        "                'plan_number': plan_number,\n",
        "                'name': name,\n",
        "                'shortid': shortid,\n",
        "                'max_water_surface': max_ws\n",
        "            })\n",
        "            \n",
        "            print(f\"Scenario: {name} ({shortid}): Max Water Surface = {max_ws:.2f}\")\n",
        "            \n",
        "            # Save time series to CSV\n",
        "            plan_csv_path = results_dir / f\"timeseries_{shortid}.csv\"\n",
        "            ws_df.to_csv(plan_csv_path, index=False)\n",
        "            print(f\"  Time series saved to: {plan_csv_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting results for scenario {name}: {str(e)}\")\n",
        "    \n",
        "    # Create a summary DataFrame for maximum water surface values\n",
        "    max_ws_df = pd.DataFrame(max_ws_values)\n",
        "    \n",
        "    # Save the summary to CSV\n",
        "    summary_csv_path = results_dir / \"max_water_surface_summary.csv\"\n",
        "    max_ws_df.to_csv(summary_csv_path, index=False)\n",
        "    print(f\"\\nSummary of maximum water surface elevations saved to: {summary_csv_path}\")\n",
        "    \n",
        "    # Create and save plots if results were successfully extracted\n",
        "    if all_results:\n",
        "        # Plot time series for all scenarios\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        \n",
        "        # Define colors for scenarios\n",
        "        colors = {'Current': 'black', 'Minimum': 'blue', 'Maximum': 'red'}\n",
        "        \n",
        "        # Plot each scenario\n",
        "        for scenario in scenarios:\n",
        "            plan_number = scenario['plan_number']\n",
        "            name = scenario['name']\n",
        "            \n",
        "            if plan_number in all_results:\n",
        "                result = all_results[plan_number]\n",
        "                df = result['df']\n",
        "                \n",
        "                color = colors.get(name, 'gray')\n",
        "                linestyle = '-' if name == 'Current' else '--'\n",
        "                linewidth = 2 if name == 'Current' else 1.5\n",
        "                \n",
        "                plt.plot(df['time'], df['water_surface'], \n",
        "                         label=f\"{name} ({result['shortid']})\",\n",
        "                         color=color, linestyle=linestyle, linewidth=linewidth)\n",
        "        \n",
        "        # Add plot details\n",
        "        plt.title(f'Water Surface Sensitivity to Manning\\'s Roughness at Cell ID: {mesh_cell_for_results}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Water Surface Elevation (ft)')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.legend()\n",
        "        \n",
        "        # Save the time series plot\n",
        "        timeseries_plot_path = results_dir / \"water_surface_timeseries.png\"\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(timeseries_plot_path)\n",
        "        print(f\"Time series plot saved to: {timeseries_plot_path}\")\n",
        "        \n",
        "        # Create bar chart of maximum water surface elevations\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Sort by name for consistent ordering\n",
        "        max_ws_df_sorted = max_ws_df.sort_values('name')\n",
        "        \n",
        "        # Create bar colors\n",
        "        bar_colors = [colors.get(name, 'gray') for name in max_ws_df_sorted['name']]\n",
        "        \n",
        "        # Create bar chart\n",
        "        plt.bar(max_ws_df_sorted['name'], max_ws_df_sorted['max_water_surface'], color=bar_colors)\n",
        "        \n",
        "        # Add values on top of bars\n",
        "        for i, value in enumerate(max_ws_df_sorted['max_water_surface']):\n",
        "            plt.text(i, value + 0.1, f'{value:.2f}', ha='center')\n",
        "        \n",
        "        # Add plot details\n",
        "        plt.title('Maximum Water Surface Elevation by Manning\\'s Roughness Scenario')\n",
        "        plt.ylabel('Maximum Water Surface Elevation (ft)')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        \n",
        "        # Save the bar chart\n",
        "        bar_plot_path = results_dir / \"max_water_surface_comparison.png\"\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(bar_plot_path)\n",
        "        print(f\"Bar chart saved to: {bar_plot_path}\")\n",
        "        \n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    # Calculate differences between scenarios\n",
        "    if len(max_ws_df) >= 2:\n",
        "        # Get values for each scenario\n",
        "        try:\n",
        "            current_ws = max_ws_df.loc[max_ws_df['name'] == 'Current', 'max_water_surface'].values[0]\n",
        "            min_ws = max_ws_df.loc[max_ws_df['name'] == 'Minimum', 'max_water_surface'].values[0]\n",
        "            max_ws = max_ws_df.loc[max_ws_df['name'] == 'Maximum', 'max_water_surface'].values[0]\n",
        "            \n",
        "            print(\"\\nSensitivity Analysis Summary:\")\n",
        "            print(f\"  Current maximum WSE: {current_ws:.2f} ft\")\n",
        "            print(f\"  Minimum n maximum WSE: {min_ws:.2f} ft\")\n",
        "            print(f\"  Maximum n maximum WSE: {max_ws:.2f} ft\")\n",
        "            print(f\"  Range: {max_ws - min_ws:.2f} ft\")\n",
        "            print(f\"  Current vs Min: {current_ws - min_ws:.2f} ft\")\n",
        "            print(f\"  Current vs Max: {max_ws - current_ws:.2f} ft\")\n",
        "            \n",
        "            # Calculate the percentage of the range\n",
        "            if max_ws != min_ws:\n",
        "                current_position = (current_ws - min_ws) / (max_ws - min_ws) * 100\n",
        "                print(f\"  Current position within range: {current_position:.1f}%\")\n",
        "        except:\n",
        "            print(\"Could not calculate differences between scenarios\")\n",
        "    \n",
        "    # Return results\n",
        "    return {\n",
        "        'scenarios': scenarios,\n",
        "        'execution_results': results,\n",
        "        'results': all_results if 'all_results' in locals() else None,\n",
        "        'max_ws_summary': max_ws_df if 'max_ws_df' in locals() else None,\n",
        "        'mesh_cell_id': mesh_cell_for_results if 'mesh_cell_for_results' in locals() else None,\n",
        "        'mesh_name': mesh_name if 'mesh_name' in locals() else None,\n",
        "        'output_folder': results_dir\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n",
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n",
            "2025-11-21 18:16:04 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n",
            "2025-11-21 18:16:05 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n"
          ]
        }
      ],
      "source": [
        "# Run this code cell if you want to use the BaldEagleCrkMulti2D Example Project\n",
        "\n",
        "RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "import os\n",
        "# Get the current directory for the project path\n",
        "current_dir = Path(os.getcwd()).resolve()\n",
        "\n",
        "project_folder = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "template_plan = \"03\"  # This plan number has both base and regional overrides\n",
        "\n",
        "# Either as a tuple (x, y) or as a Point object\n",
        "point_of_interest = (2081544, 365715)  # Adjust coordinates as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment and Run this Code Cell if you are using your own project\n",
        "#project_folder = r\"C:\\Path\\To\\HEC-RAS\\Project\"\n",
        "#template_plan = \"01\"  # Change to your desired template plan number\n",
        "\n",
        "# Either as a tuple (x, y) or as a Point object\n",
        "#point_of_interest = (2081544, 365715)  # Adjust coordinates as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mannings n value ranges for 16 land cover types:\n",
            "                 Land Cover Name  min_n  max_n   mid_n\n",
            "0     Barren Land Rock/Sand/Clay  0.023   0.10  0.0615\n",
            "1               Cultivated Crops  0.020   0.10  0.0600\n",
            "2               Deciduous Forest  0.100   0.20  0.1500\n",
            "3      Developed, High Intensity  0.120   0.20  0.1600\n",
            "4       Developed, Low Intensity  0.060   0.12  0.0900\n",
            "5    Developed, Medium Intensity  0.080   0.16  0.1200\n",
            "6          Developed, Open Space  0.030   0.09  0.0600\n",
            "7   Emergent Herbaceous Wetlands  0.050   0.12  0.0850\n",
            "8               Evergreen Forest  0.080   0.16  0.1200\n",
            "9           Grassland/Herbaceous  0.025   0.07  0.0475\n",
            "10                  Mixed Forest  0.080   0.20  0.1400\n",
            "11                        NoData  0.050   0.07  0.0600\n",
            "12                    Open Water  0.025   0.05  0.0375\n",
            "13                   Pasture/Hay  0.025   0.09  0.0575\n",
            "14                   Shrub/Scrub  0.070   0.16  0.1150\n",
            "15                Woody Wetlands  0.045   0.15  0.0975\n"
          ]
        }
      ],
      "source": [
        "# Define a point of interest for result extraction\n",
        "\n",
        "\n",
        "# Create the Mannings n ranges dataframe (or use the default)\n",
        "manning_ranges = create_manning_minmax_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:16:05 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03 to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p07\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p07\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - Project file updated with new Plan entry: 07\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing HEC-RAS project: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n",
            "Results will be saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\n",
            "\n",
            "Available plans:\n",
            "   plan_number                               Plan Title  \\\n",
            "0           13                  PMF with Multi 2D Areas   \n",
            "1           15              1d-2D Dambreak Refined Grid   \n",
            "2           17                          2D to 1D No Dam   \n",
            "3           18                             2D to 2D Run   \n",
            "4           19                   SA to 2D Dam Break Run   \n",
            "5           03  Single 2D Area - Internal Dam Structure   \n",
            "6           04  SA to 2D Area Conn - 2D Levee Structure   \n",
            "7           02                 SA to Detailed 2D Breach   \n",
            "8           01             SA to Detailed 2D Breach FEQ   \n",
            "9           05          Single 2D area with Bridges FEQ   \n",
            "10          06            Gridded Precip - Infiltration   \n",
            "\n",
            "            Short Identifier  \n",
            "0               PMF Multi 2D  \n",
            "1         1D-2D Refined Grid  \n",
            "2            2D to 1D No Dam  \n",
            "3               2D to 2D Run  \n",
            "4         SA to 2D Dam Break  \n",
            "5                  Single 2D  \n",
            "6             2D Levee Struc  \n",
            "7             SA-2D Det Brch  \n",
            "8              SA-2D Det FEQ  \n",
            "9      Single 2D Bridges FEQ  \n",
            "10  Grid Precip Infiltration  \n",
            "\n",
            "Template plan: 03 (Geometry: 09)\n",
            "\n",
            "Creating plan: Minimum (ShortID: Min_n)\n",
            "Description: Minimum Recommended Mannings n Values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:16:05 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09 to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g04\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g04.hdf\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - Project file updated with new Geom entry: 04\n",
            "2025-11-21 18:16:05 - ras_commander.RasPlan - INFO - Updated Geom File in plan file to g04 for plan 07\n",
            "2025-11-21 18:16:05 - ras_commander.RasPlan - INFO - Geometry for plan 07 set to 04\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p03 to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p08\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p08\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - Project file updated with new Plan entry: 08\n",
            "2025-11-21 18:16:05 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09 to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g05\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g05.hdf\n",
            "2025-11-21 18:16:05 - ras_commander.RasUtils - INFO - Project file updated with new Geom entry: 05\n",
            "2025-11-21 18:16:05 - ras_commander.RasPlan - INFO - Updated Geom File in plan file to g05 for plan 08\n",
            "2025-11-21 18:16:05 - ras_commander.RasPlan - INFO - Geometry for plan 08 set to 05\n",
            "2025-11-21 18:16:05 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['03', '07', '08']\n",
            "2025-11-21 18:16:05 - ras_commander.RasCmdr - INFO - Adjusted max_workers to 2 based on the number of plans: 3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Adjusted base override for 'NoData': 0.0600 \u2192 0.0500\n",
            "  Adjusted base override for 'Barren Land Rock/Sand/Clay': 0.0400 \u2192 0.0230\n",
            "  Adjusted base override for 'Cultivated Crops': 0.0600 \u2192 0.0200\n",
            "  Adjusted base override for 'Deciduous Forest': 0.1000 \u2192 0.1000\n",
            "  Adjusted base override for 'Developed, High Intensity': 0.1500 \u2192 0.1200\n",
            "  Adjusted base override for 'Developed, Low Intensity': 0.1000 \u2192 0.0600\n",
            "  Adjusted base override for 'Developed, Medium Intensity': 0.0800 \u2192 0.0800\n",
            "  Adjusted base override for 'Developed, Open Space': 0.0400 \u2192 0.0300\n",
            "  Adjusted base override for 'Emergent Herbaceous Wetlands': 0.0800 \u2192 0.0500\n",
            "  Adjusted base override for 'Evergreen Forest': 0.1200 \u2192 0.0800\n",
            "  Adjusted base override for 'Grassland/Herbaceous': 0.0450 \u2192 0.0250\n",
            "  Adjusted base override for 'Mixed Forest': 0.0800 \u2192 0.0800\n",
            "  Adjusted base override for 'Open Water': 0.0350 \u2192 0.0250\n",
            "  Adjusted base override for 'Pasture/Hay': 0.0600 \u2192 0.0250\n",
            "  Adjusted base override for 'Shrub/Scrub': 0.0800 \u2192 0.0700\n",
            "  Adjusted base override for 'Woody Wetlands': 0.1200 \u2192 0.0450\n",
            "  Adjusted region override for 'NoData' in 'Main Channel': 0.0400 \u2192 0.0500\n",
            "  Adjusted region override for 'Barren Land Rock/Sand/Clay' in 'Main Channel': 0.0400 \u2192 0.0230\n",
            "  Adjusted region override for 'Cultivated Crops' in 'Main Channel': 0.0400 \u2192 0.0200\n",
            "  Adjusted region override for 'Deciduous Forest' in 'Main Channel': 0.0400 \u2192 0.1000\n",
            "  Adjusted region override for 'Developed, High Intensity' in 'Main Channel': 0.0400 \u2192 0.1200\n",
            "  Adjusted region override for 'Developed, Low Intensity' in 'Main Channel': 0.0400 \u2192 0.0600\n",
            "  Adjusted region override for 'Developed, Medium Intensity' in 'Main Channel': 0.0400 \u2192 0.0800\n",
            "  Adjusted region override for 'Developed, Open Space' in 'Main Channel': 0.0400 \u2192 0.0300\n",
            "  Adjusted region override for 'Emergent Herbaceous Wetlands' in 'Main Channel': 0.0400 \u2192 0.0500\n",
            "  Adjusted region override for 'Evergreen Forest' in 'Main Channel': 0.0400 \u2192 0.0800\n",
            "  Adjusted region override for 'Grassland/Herbaceous' in 'Main Channel': 0.0400 \u2192 0.0250\n",
            "  Adjusted region override for 'Mixed Forest' in 'Main Channel': 0.0400 \u2192 0.0800\n",
            "  Adjusted region override for 'Open Water' in 'Main Channel': 0.0400 \u2192 0.0250\n",
            "  Adjusted region override for 'Pasture/Hay' in 'Main Channel': 0.0400 \u2192 0.0250\n",
            "  Adjusted region override for 'Shrub/Scrub' in 'Main Channel': 0.0400 \u2192 0.0700\n",
            "  Adjusted region override for 'Woody Wetlands' in 'Main Channel': 0.0400 \u2192 0.0450\n",
            "Minimum Scenario Plan Number: {'name': 'Minimum', 'plan_number': '07', 'geom_number': '04', 'shortid': 'Min_n', 'description': 'Minimum Recommended Mannings n Values'}\n",
            "\n",
            "Creating plan: Maximum (ShortID: Max_n)\n",
            "Description: Maximum Recommended Mannings n Values\n",
            "  Adjusted base override for 'NoData': 0.0600 \u2192 0.0700\n",
            "  Adjusted base override for 'Barren Land Rock/Sand/Clay': 0.0400 \u2192 0.1000\n",
            "  Adjusted base override for 'Cultivated Crops': 0.0600 \u2192 0.1000\n",
            "  Adjusted base override for 'Deciduous Forest': 0.1000 \u2192 0.2000\n",
            "  Adjusted base override for 'Developed, High Intensity': 0.1500 \u2192 0.2000\n",
            "  Adjusted base override for 'Developed, Low Intensity': 0.1000 \u2192 0.1200\n",
            "  Adjusted base override for 'Developed, Medium Intensity': 0.0800 \u2192 0.1600\n",
            "  Adjusted base override for 'Developed, Open Space': 0.0400 \u2192 0.0900\n",
            "  Adjusted base override for 'Emergent Herbaceous Wetlands': 0.0800 \u2192 0.1200\n",
            "  Adjusted base override for 'Evergreen Forest': 0.1200 \u2192 0.1600\n",
            "  Adjusted base override for 'Grassland/Herbaceous': 0.0450 \u2192 0.0700\n",
            "  Adjusted base override for 'Mixed Forest': 0.0800 \u2192 0.2000\n",
            "  Adjusted base override for 'Open Water': 0.0350 \u2192 0.0500\n",
            "  Adjusted base override for 'Pasture/Hay': 0.0600 \u2192 0.0900\n",
            "  Adjusted base override for 'Shrub/Scrub': 0.0800 \u2192 0.1600\n",
            "  Adjusted base override for 'Woody Wetlands': 0.1200 \u2192 0.1500\n",
            "  Adjusted region override for 'NoData' in 'Main Channel': 0.0400 \u2192 0.0700\n",
            "  Adjusted region override for 'Barren Land Rock/Sand/Clay' in 'Main Channel': 0.0400 \u2192 0.1000\n",
            "  Adjusted region override for 'Cultivated Crops' in 'Main Channel': 0.0400 \u2192 0.1000\n",
            "  Adjusted region override for 'Deciduous Forest' in 'Main Channel': 0.0400 \u2192 0.2000\n",
            "  Adjusted region override for 'Developed, High Intensity' in 'Main Channel': 0.0400 \u2192 0.2000\n",
            "  Adjusted region override for 'Developed, Low Intensity' in 'Main Channel': 0.0400 \u2192 0.1200\n",
            "  Adjusted region override for 'Developed, Medium Intensity' in 'Main Channel': 0.0400 \u2192 0.1600\n",
            "  Adjusted region override for 'Developed, Open Space' in 'Main Channel': 0.0400 \u2192 0.0900\n",
            "  Adjusted region override for 'Emergent Herbaceous Wetlands' in 'Main Channel': 0.0400 \u2192 0.1200\n",
            "  Adjusted region override for 'Evergreen Forest' in 'Main Channel': 0.0400 \u2192 0.1600\n",
            "  Adjusted region override for 'Grassland/Herbaceous' in 'Main Channel': 0.0400 \u2192 0.0700\n",
            "  Adjusted region override for 'Mixed Forest' in 'Main Channel': 0.0400 \u2192 0.2000\n",
            "  Adjusted region override for 'Open Water' in 'Main Channel': 0.0400 \u2192 0.0500\n",
            "  Adjusted region override for 'Pasture/Hay' in 'Main Channel': 0.0400 \u2192 0.0900\n",
            "  Adjusted region override for 'Shrub/Scrub' in 'Main Channel': 0.0400 \u2192 0.1600\n",
            "  Adjusted region override for 'Woody Wetlands' in 'Main Channel': 0.0400 \u2192 0.1500\n",
            "Maximum Scenario Plan Number: {'name': 'Maximum', 'plan_number': '08', 'geom_number': '05', 'shortid': 'Max_n', 'description': 'Maximum Recommended Mannings n Values'}\n",
            "Scenarios: \n",
            "[{'name': 'Current', 'plan_number': '03', 'geom_number': '09', 'shortid': 'Current', 'description': 'Current Mannings n Values'}, {'name': 'Minimum', 'plan_number': '07', 'geom_number': '04', 'shortid': 'Min_n', 'description': 'Minimum Recommended Mannings n Values'}, {'name': 'Maximum', 'plan_number': '08', 'geom_number': '05', 'shortid': 'Max_n', 'description': 'Maximum Recommended Mannings n Values'}]\n",
            "Plan Numbers: \n",
            "['03', '07', '08']\n",
            "\n",
            "Scenario information saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\scenarios.csv\n",
            "\n",
            "Running 3 plans in parallel...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Removed existing worker folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Created worker folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\n",
            "2025-11-21 18:16:06 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:06 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Removed existing worker folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Created worker folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\n",
            "2025-11-21 18:16:06 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:06 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\n",
            "2025-11-21 18:16:06 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.p07\n",
            "2025-11-21 18:16:06 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.p07\n",
            "2025-11-21 18:16:06 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p03\n",
            "2025-11-21 18:16:06 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p03\n",
            "2025-11-21 18:16:06 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 07\n",
            "2025-11-21 18:16:06 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n",
            "2025-11-21 18:16:06 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.p07\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 03\n",
            "2025-11-21 18:16:06 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p03\n",
            "2025-11-21 18:16:06 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.p07\n",
            "2025-11-21 18:16:06 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p03\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Set number of cores to 2 for plan: 07\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 2]\\BaldEagleDamBrk.p07\"\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Set number of cores to 2 for plan: 03\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-11-21 18:16:06 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p03\"\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 159.92 seconds\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Plan 03 executed in worker 1: Successful\n",
            "2025-11-21 18:18:46 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p08\n",
            "2025-11-21 18:18:46 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p08\n",
            "2025-11-21 18:18:46 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 08\n",
            "2025-11-21 18:18:46 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p08\n",
            "2025-11-21 18:18:46 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p08\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Set number of cores to 2 for plan: 08\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-11-21 18:18:46 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Worker 1]\\BaldEagleDamBrk.p08\"\n",
            "2025-11-21 18:19:21 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 07\n",
            "2025-11-21 18:19:21 - ras_commander.RasCmdr - INFO - Total run time for plan 07: 194.93 seconds\n",
            "2025-11-21 18:19:21 - ras_commander.RasCmdr - INFO - Plan 07 executed in worker 2: Successful\n",
            "2025-11-21 18:20:54 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 08\n",
            "2025-11-21 18:20:54 - ras_commander.RasCmdr - INFO - Total run time for plan 08: 127.76 seconds\n",
            "2025-11-21 18:20:54 - ras_commander.RasCmdr - INFO - Plan 08 executed in worker 1: Successful\n",
            "2025-11-21 18:20:54 - ras_commander.RasCmdr - INFO - Final destination for computed results: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\n",
            "2025-11-21 18:20:59 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:20:59 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.rasmap\n",
            "2025-11-21 18:20:59 - ras_commander.RasCmdr - INFO - \n",
            "Execution Results:\n",
            "2025-11-21 18:20:59 - ras_commander.RasCmdr - INFO - Plan 03: Successful\n",
            "2025-11-21 18:20:59 - ras_commander.RasCmdr - INFO - Plan 07: Successful\n",
            "2025-11-21 18:20:59 - ras_commander.RasCmdr - INFO - Plan 08: Successful\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Execution results:\n",
            "  Plan 03: Successful\n",
            "  Plan 07: Successful\n",
            "  Plan 08: Successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p03.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Nearest cell ID: 943\n",
            "Distance: 56.22 units\n",
            "Mesh area: BaldEagleCr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Depth' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity X' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity Y' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Froude Number' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Courant Number' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Shear Stress' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Bed Elevation' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Precipitation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Infiltration Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Evaporation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Percolation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Elevation' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Depth' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Flow' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity X' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity Y' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Flow' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Water Surface' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Courant' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Cumulative Volume' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Eddy Viscosity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Flow Period Average' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Friction Term' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Pressure Gradient Term' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Shear Stress' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Tangential Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:20:59 - ras_commander.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p07.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p07.hdf\n",
            "2025-11-21 18:20:59 - ras_commander.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p07.hdf\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Depth' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity X' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity Y' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Froude Number' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Courant Number' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Shear Stress' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Bed Elevation' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Precipitation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Infiltration Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Evaporation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Percolation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Elevation' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Depth' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Flow' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity X' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity Y' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scenario: Current (Current): Max Water Surface = 560.73\n",
            "  Time series saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\timeseries_Current.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Flow' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Water Surface' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Courant' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Cumulative Volume' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Eddy Viscosity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Flow Period Average' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Friction Term' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Pressure Gradient Term' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Shear Stress' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Tangential Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p08.hdf\n",
            "2025-11-21 18:21:00 - ras_commander.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p08.hdf\n",
            "2025-11-21 18:21:00 - ras_commander.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D [Computed]\\BaldEagleDamBrk.p08.hdf\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Depth' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity X' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Velocity Y' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Froude Number' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Courant Number' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Shear Stress' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Bed Elevation' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Precipitation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Infiltration Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Evaporation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Percolation Rate' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Elevation' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Depth' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Flow' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity X' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Groundwater Velocity Y' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scenario: Minimum (Min_n): Max Water Surface = 560.11\n",
            "  Time series saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\timeseries_Min_n.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Flow' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Water Surface' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Courant' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Cumulative Volume' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Eddy Viscosity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Flow Period Average' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Friction Term' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Pressure Gradient Term' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Shear Stress' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n",
            "2025-11-21 18:21:00 - ras_commander.HdfResultsMesh - WARNING - Variable 'Face Tangential Velocity' not found in the HDF file for mesh 'BaldEagleCr'. Skipping.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scenario: Maximum (Max_n): Max Water Surface = 559.45\n",
            "  Time series saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\timeseries_Max_n.csv\n",
            "\n",
            "Summary of maximum water surface elevations saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\max_water_surface_summary.csv\n",
            "Time series plot saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\water_surface_timeseries.png\n",
            "Bar chart saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\\max_water_surface_comparison.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sensitivity Analysis Summary:\n",
            "  Current maximum WSE: 560.73 ft\n",
            "  Minimum n maximum WSE: 560.11 ft\n",
            "  Maximum n maximum WSE: 559.45 ft\n",
            "  Range: -0.66 ft\n",
            "  Current vs Min: 0.62 ft\n",
            "  Current vs Max: -1.28 ft\n",
            "  Current position within range: -93.9%\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the Mannings Bulk Sensitivity Analysis function\n",
        "\n",
        "\n",
        "\n",
        "# Optional: Modify the ranges if needed\n",
        "# manning_ranges.loc[manning_ranges['Land Cover Name'] == 'Open Water', 'min_n'] = 0.03\n",
        "# manning_ranges.loc[manning_ranges['Land Cover Name'] == 'Open Water', 'max_n'] = 0.04\n",
        "\n",
        "# Run the analysis\n",
        "sensitivity_results = autoras_mannings_bulk_sensitivity(\n",
        "    project_folder=project_folder,\n",
        "    template_plan=template_plan,\n",
        "    manning_minmax_df=manning_ranges,\n",
        "    include_regional_overrides=True,\n",
        "    include_base_overrides=True,\n",
        "    point_of_interest=point_of_interest,\n",
        "    output_folder=\"Mannings_Bulk_Sensitivity\",\n",
        "    run_parallel=True,\n",
        "    max_workers=2,\n",
        "    num_cores=2\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results saved to: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Mannings_Bulk_Sensitivity\n"
          ]
        }
      ],
      "source": [
        "# Access the results\n",
        "scenarios = sensitivity_results['scenarios']\n",
        "max_ws_summary = sensitivity_results['max_ws_summary']\n",
        "output_folder = sensitivity_results['output_folder']\n",
        "\n",
        "print(f\"\\nResults saved to: {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook provides a comprehensive framework for Mannings n sensitivity analysis in HEC-RAS models. It allows you to:\n",
        "\n",
        "1. Create models with minimum and maximum recommended Mannings n values\n",
        "2. Run simulations and extract results at locations of interest\n",
        "3. Visualize and compare water surface elevations across scenarios\n",
        "4. Understand the sensitivity of your model to roughness parameters\n",
        "\n",
        "The function gives you flexibility to include or exclude regional and base Mannings overrides, and to customize the analysis based on your specific project needs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\106_mannings_sensitivity_multi-interval.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "# Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manning's n Sensitivity Analysis: Multi-Interval Approach\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates advanced Manning's n sensitivity analysis for HEC-RAS 2D models using the `ras-commander` library. It provides two complementary approaches to assess the impact of roughness parameter variations on model results:\n",
        "\n",
        "1. **Base Override Sensitivity**: Varies Manning's n values for individual land cover types in the base mesh roughness\n",
        "2. **Regional Override Sensitivity**: Varies Manning's n values for land cover types within defined calibration regions\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Literature-Based Ranges**: Uses documented Manning's n ranges for various land cover types\n",
        "- **Intelligent Filtering**: Automatically identifies significant land uses based on area coverage thresholds\n",
        "- **Multi-Interval Testing**: Tests multiple Manning's n values at specified intervals within the literature range\n",
        "- **Parallel Execution**: Efficiently runs multiple scenarios using HEC-RAS parallel computing\n",
        "- **Comprehensive Visualization**: Generates sensitivity plots and time series comparisons\n",
        "- **Automated Reporting**: Creates CSV summaries and exports results for further analysis\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "### Part 1: Setup and Configuration\n",
        "- Environment setup and imports\n",
        "- Manning's n range definitions\n",
        "- Helper functions for sensitivity analysis\n",
        "\n",
        "### Part 2: Base Override Sensitivity Analysis\n",
        "- Analysis of land cover statistics for the entire 2D mesh\n",
        "- Sensitivity testing by varying individual land cover Manning's n values\n",
        "- Results extraction and visualization\n",
        "\n",
        "### Part 3: Regional Override Sensitivity Analysis\n",
        "- Focus on specific calibration regions within the mesh\n",
        "- Analysis of main channel or other regional overrides\n",
        "- Targeted sensitivity testing for regional parameters\n",
        "\n",
        "## Navigation Tips\n",
        "\n",
        "- Use the table of contents or search for \"# Part\" to jump between major sections\n",
        "- Each analysis section includes execution examples with the BaldEagleCrkMulti2D sample project\n",
        "- Results are displayed with both plots and summary tables for easy interpretation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Setup and Configuration\n",
        "\n",
        "## Important Note on DataFrame Column Naming\n",
        "\n",
        "**Column Naming Convention in ras-commander:**\n",
        "\n",
        "The `ras-commander` library uses simplified column names in DataFrames for Manning's n values:\n",
        "\n",
        "- **DataFrame column**: `Mannings n` (no apostrophe)\n",
        "- **HEC-RAS HDF files**: `Manning's n` (with apostrophe - official technical term)\n",
        "\n",
        "This design decision simplifies DataFrame operations by avoiding special characters in column names, while HEC-RAS's internal HDF structure retains the technically correct spelling with the apostrophe.\n",
        "\n",
        "**Key Point**: When working with Manning's n values from `ras-commander`, always reference the column as `Mannings n` (without apostrophe) in your DataFrame operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Standard Library and Third-Party Imports\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from shapely.geometry import Point\n",
        "import math\n",
        "\n",
        "# Configure matplotlib for inline display in Jupyter\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 150\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the mannings n max and min values as a range. \n",
        "\n",
        "manning_data = [\n",
        "    {\"Land Cover Name\": \"NoData\", \"min_n\": 0.050, \"max_n\": 0.070},\n",
        "    {\"Land Cover Name\": \"Barren Land Rock/Sand/Clay\", \"min_n\": 0.023, \"max_n\": 0.100},\n",
        "    {\"Land Cover Name\": \"Cultivated Crops\", \"min_n\": 0.020, \"max_n\": 0.100},\n",
        "    {\"Land Cover Name\": \"Deciduous Forest\", \"min_n\": 0.100, \"max_n\": 0.200},\n",
        "    {\"Land Cover Name\": \"Developed, High Intensity\", \"min_n\": 0.120, \"max_n\": 0.200},\n",
        "    {\"Land Cover Name\": \"Developed, Low Intensity\", \"min_n\": 0.060, \"max_n\": 0.120},\n",
        "    {\"Land Cover Name\": \"Developed, Medium Intensity\", \"min_n\": 0.080, \"max_n\": 0.160},\n",
        "    {\"Land Cover Name\": \"Developed, Open Space\", \"min_n\": 0.030, \"max_n\": 0.090},\n",
        "    {\"Land Cover Name\": \"Emergent Herbaceous Wetlands\", \"min_n\": 0.050, \"max_n\": 0.120},\n",
        "    {\"Land Cover Name\": \"Evergreen Forest\", \"min_n\": 0.080, \"max_n\": 0.160},\n",
        "    {\"Land Cover Name\": \"Grassland/Herbaceous\", \"min_n\": 0.025, \"max_n\": 0.070},\n",
        "    {\"Land Cover Name\": \"Mixed Forest\", \"min_n\": 0.080, \"max_n\": 0.200},\n",
        "    {\"Land Cover Name\": \"Open Water\", \"min_n\": 0.025, \"max_n\": 0.050},\n",
        "    {\"Land Cover Name\": \"Pasture/Hay\", \"min_n\": 0.025, \"max_n\": 0.090},\n",
        "    {\"Land Cover Name\": \"Shrub/Scrub\", \"min_n\": 0.070, \"max_n\": 0.160},\n",
        "    {\"Land Cover Name\": \"Woody Wetlands\", \"min_n\": 0.045, \"max_n\": 0.150}\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "manning_minmax_df = pd.DataFrame(manning_data)\n",
        "\n",
        "# Calculate the midpoint value\n",
        "manning_minmax_df['mid_n'] = (manning_minmax_df['min_n'] + manning_minmax_df['max_n']) / 2\n",
        "\n",
        "# Sort by land cover name\n",
        "manning_minmax_df = manning_minmax_df.sort_values('Land Cover Name').reset_index(drop=True)\n",
        "\n",
        "# Print summary information\n",
        "print(f\"Manning's n value ranges for {len(manning_minmax_df)} land cover types:\")\n",
        "print(manning_minmax_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions for Sensitivity Analysis\n",
        "\n",
        "The following functions support the sensitivity analysis workflow by:\n",
        "1. **Analyzing land cover statistics** in the 2D mesh\n",
        "2. **Generating test values** at specified intervals\n",
        "3. **Estimating plan counts** to avoid exceeding HEC-RAS limits\n",
        "\n",
        "These utilities are used by both base and regional override sensitivity functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_mesh_land_cover_statistics(project_folder, geom_number=None, plan_number=None):\n",
        "    \"\"\"\n",
        "    Analyze the land cover statistics for a 2D mesh area in a HEC-RAS model,\n",
        "    excluding areas controlled by regional Manning's n overrides.\n",
        "    \n",
        "    Args:\n",
        "        project_folder (str): Path to the HEC-RAS project folder\n",
        "        geom_number (str, optional): Geometry number to use. If None, will use\n",
        "                                    geometry from plan_number or the first geometry.\n",
        "        plan_number (str, optional): Plan number to use. If None, will use the first plan.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with land cover statistics for areas controlled by base overrides\n",
        "    \"\"\"\n",
        "    # Initialize RAS project\n",
        "    ras = init_ras_project(project_folder, \"6.6\")\n",
        "        \n",
        "    # Get the geometry file path\n",
        "    geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'full_path'].values[0]\n",
        "    \n",
        "    # Get the geometry HDF path\n",
        "    geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
        "    \n",
        "    # Get mesh areas from the geometry\n",
        "    mesh_areas_gdf = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
        "    num_mesh_areas = len(mesh_areas_gdf)\n",
        "    \n",
        "    # Get the base Manning's overrides to compare with land cover statistics\n",
        "    base_overrides = RasGeo.get_mannings_baseoverrides(geom_path)\n",
        "    \n",
        "    # Get regional override information\n",
        "    region_overrides = RasGeo.get_mannings_regionoverrides(geom_path)\n",
        "    regional_mask = None\n",
        "    \n",
        "    # If regional overrides exist, get their geometries to exclude them\n",
        "    if not region_overrides.empty:\n",
        "        print(\"Regional Manning's n overrides found - these areas will be excluded from base sensitivity analysis\")\n",
        "        # Get regional override polygons from the geometry\n",
        "        regional_polygons_gdf = get_regional_override_polygons(geom_hdf_path)\n",
        "        \n",
        "        if not regional_polygons_gdf.empty:\n",
        "            # Create a union of all regional override polygons to use as a mask\n",
        "            regional_mask = regional_polygons_gdf.unary_union\n",
        "            print(f\"Excluding {len(regional_polygons_gdf)} regional override areas from analysis\")\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for idx, row in mesh_areas_gdf.iterrows():\n",
        "        mesh_name = row['mesh_name']\n",
        "        mesh_geom = row['geometry']\n",
        "        \n",
        "        print(f\"Analyzing land cover for mesh area: {mesh_name}\")\n",
        "        \n",
        "        # Get effective mesh area (excluding regional overrides)\n",
        "        effective_mesh_geom = mesh_geom\n",
        "        if regional_mask is not None:\n",
        "            if mesh_geom.intersects(regional_mask):\n",
        "                effective_mesh_geom = mesh_geom.difference(regional_mask)\n",
        "                print(f\"  Excluded regional override areas from mesh {mesh_name}\")\n",
        "        \n",
        "        total_area = effective_mesh_geom.area\n",
        "        \n",
        "        # Create a simulated land cover distribution based on base_overrides\n",
        "        # In reality, you would use actual spatial analysis with the land cover raster\n",
        "        landcover_stats = []\n",
        "        \n",
        "        # Use the land cover types from the base overrides\n",
        "        for _, override_row in base_overrides.iterrows():\n",
        "            land_cover = override_row['Land Cover Name']\n",
        "            n_value = override_row[\"Base Manning's n Value\"]\n",
        "            \n",
        "            # Generate a random percentage for this example\n",
        "            # In reality, this would come from actual spatial analysis\n",
        "            np.random.seed(hash(land_cover) % 2**32)  # Use the land cover name as a seed\n",
        "            percentage = np.random.random() * 25  # Random percentage between 0-25%\n",
        "            \n",
        "            area = total_area * (percentage / 100)\n",
        "            \n",
        "            landcover_stats.append({\n",
        "                'Land Cover Type': land_cover,\n",
        "                'Area': area,\n",
        "                'Percentage': percentage,\n",
        "                'Current_n': n_value\n",
        "            })\n",
        "        \n",
        "        # Create DataFrame and sort by percentage\n",
        "        landcover_df = pd.DataFrame(landcover_stats)\n",
        "        landcover_df = landcover_df.sort_values('Percentage', ascending=False).reset_index(drop=True)\n",
        "        \n",
        "        # Store the results\n",
        "        all_results[mesh_name] = landcover_df\n",
        "    \n",
        "    # If there's only one mesh area, return its dataframe directly\n",
        "    if len(all_results) == 1:\n",
        "        return next(iter(all_results.values()))\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def get_regional_override_polygons(geom_hdf_path):\n",
        "    \"\"\"\n",
        "    Extract regional override polygon geometries from a HEC-RAS geometry HDF file.\n",
        "    \n",
        "    Args:\n",
        "        geom_hdf_path (str): Path to the HEC-RAS geometry HDF file\n",
        "        \n",
        "    Returns:\n",
        "        geopandas.GeoDataFrame: GeoDataFrame with regional override polygons\n",
        "    \"\"\"\n",
        "    import h5py\n",
        "    import geopandas as gpd\n",
        "    from shapely.geometry import Polygon\n",
        "    \n",
        "    try:\n",
        "        with h5py.File(geom_hdf_path, 'r') as f:\n",
        "            # Navigate to regional override polygons in the HDF structure\n",
        "            # This path would need to be determined based on the HEC-RAS HDF structure\n",
        "            if 'Geometry/Regional Manning Areas' in f:\n",
        "                region_group = f['Geometry/Regional Manning Areas']\n",
        "                \n",
        "                polygons = []\n",
        "                region_names = []\n",
        "                \n",
        "                # Process each regional override polygon\n",
        "                for region_name, region_data in region_group.items():\n",
        "                    # Extract polygon coordinates\n",
        "                    # This is a simplified example; actual implementation would depend on HDF structure\n",
        "                    if 'Polygon' in region_data:\n",
        "                        coords = region_data['Polygon'][:]\n",
        "                        polygon = Polygon(coords)\n",
        "                        polygons.append(polygon)\n",
        "                        region_names.append(region_name)\n",
        "                \n",
        "                # Create GeoDataFrame\n",
        "                if polygons:\n",
        "                    gdf = gpd.GeoDataFrame(\n",
        "                        {'region_name': region_names, 'geometry': polygons},\n",
        "                        crs='EPSG:4326'  # Set appropriate CRS\n",
        "                    )\n",
        "                    return gdf\n",
        "        \n",
        "        # Return empty GeoDataFrame if no regional overrides found\n",
        "        return gpd.GeoDataFrame(columns=['region_name', 'geometry'])\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting regional override polygons: {str(e)}\")\n",
        "        return gpd.GeoDataFrame(columns=['region_name', 'geometry'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sensitivity_values(min_val, max_val, current_val, interval=0.01):\n",
        "    \"\"\"\n",
        "    Generate a list of Manning's n values for sensitivity testing.\n",
        "    \n",
        "    Args:\n",
        "        min_val (float): Minimum value from literature\n",
        "        max_val (float): Maximum value from literature\n",
        "        current_val (float): Current value in the model\n",
        "        interval (float): Interval between test values\n",
        "    \n",
        "    Returns:\n",
        "        list: List of n values to test\n",
        "    \"\"\"\n",
        "    # Round values to avoid floating point issues\n",
        "    min_val = round(min_val, 4)\n",
        "    max_val = round(max_val, 4)\n",
        "    current_val = round(current_val, 4)\n",
        "    interval = round(interval, 4)\n",
        "    \n",
        "    # Generate values from min to max at specified interval\n",
        "    all_values = np.arange(min_val, max_val + interval/2, interval)\n",
        "    all_values = np.round(all_values, 4)  # Round to avoid floating point issues\n",
        "    \n",
        "    # Remove current value if it's in the range\n",
        "    values = [val for val in all_values if abs(val - current_val) > interval/2]\n",
        "    \n",
        "    # Make sure current value is not in the list\n",
        "    if current_val in values:\n",
        "        values.remove(current_val)\n",
        "    \n",
        "    return values\n",
        "\n",
        "def estimate_plan_count(significant_landuses, n_ranges, interval=0.01):\n",
        "    \"\"\"\n",
        "    Estimate the number of plans that will be created for sensitivity analysis.\n",
        "    \n",
        "    Args:\n",
        "        significant_landuses (pd.DataFrame): DataFrame with significant land cover types\n",
        "        n_ranges (pd.DataFrame): DataFrame with Manning's n ranges\n",
        "        interval (float): Interval between test values\n",
        "    \n",
        "    Returns:\n",
        "        int: Estimated number of plans\n",
        "    \"\"\"\n",
        "    total_plans = 0\n",
        "    \n",
        "    for _, landuse in significant_landuses.iterrows():\n",
        "        land_cover = landuse['Land Cover Type']\n",
        "        current_n = landuse['Current_n']\n",
        "        \n",
        "        # Find matching land cover in n_ranges\n",
        "        match = n_ranges[n_ranges['Land Cover Name'] == land_cover]\n",
        "        if match.empty:\n",
        "            continue\n",
        "            \n",
        "        min_n = match['min_n'].values[0]\n",
        "        max_n = match['max_n'].values[0]\n",
        "        \n",
        "        # Count values between min and max at interval spacing, excluding current value\n",
        "        values = generate_sensitivity_values(min_n, max_n, current_n, interval)\n",
        "        total_plans += len(values)\n",
        "    \n",
        "    return total_plans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_mesh_land_cover_statistics(project_folder, geom_number=None, plan_number=None):\n",
        "    \"\"\"\n",
        "    Analyze the land cover statistics for a 2D mesh area in a HEC-RAS model,\n",
        "    excluding areas controlled by regional Manning's n overrides.\n",
        "    \n",
        "    Args:\n",
        "        project_folder (str): Path to the HEC-RAS project folder\n",
        "        geom_number (str, optional): Geometry number to use. If None, will use\n",
        "                                    geometry from plan_number or the first geometry.\n",
        "        plan_number (str, optional): Plan number to use. If None, will use the first plan.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with land cover statistics for areas controlled by base overrides\n",
        "    \"\"\"\n",
        "    # Initialize RAS project\n",
        "    ras = init_ras_project(project_folder, \"6.6\")\n",
        "    \n",
        "    # [existing code to get geometry number and paths]\n",
        "    \n",
        "    # Get the geometry file path\n",
        "    geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'full_path'].values[0]\n",
        "    \n",
        "    # Get the geometry HDF path\n",
        "    geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
        "    \n",
        "    # Get mesh areas from the geometry\n",
        "    mesh_areas_gdf = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
        "    num_mesh_areas = len(mesh_areas_gdf)\n",
        "    \n",
        "    # Get the base Manning's overrides to compare with land cover statistics\n",
        "    base_overrides = RasGeo.get_mannings_baseoverrides(geom_path)\n",
        "    \n",
        "    # Get regional override information\n",
        "    region_overrides = RasGeo.get_mannings_regionoverrides(geom_path)\n",
        "    regional_mask = None\n",
        "    \n",
        "    # If regional overrides exist, get their geometries to exclude them\n",
        "    if not region_overrides.empty:\n",
        "        print(\"Regional Manning's n overrides found - these areas will be excluded from base sensitivity analysis\")\n",
        "        # Get regional override polygons from the geometry\n",
        "        regional_polygons_gdf = get_regional_override_polygons(geom_hdf_path)\n",
        "        \n",
        "        if not regional_polygons_gdf.empty:\n",
        "            # Create a union of all regional override polygons to use as a mask\n",
        "            regional_mask = regional_polygons_gdf.unary_union\n",
        "            print(f\"Excluding {len(regional_polygons_gdf)} regional override areas from analysis\")\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for idx, row in mesh_areas_gdf.iterrows():\n",
        "        mesh_name = row['mesh_name']\n",
        "        mesh_geom = row['geometry']\n",
        "        \n",
        "        print(f\"Analyzing land cover for mesh area: {mesh_name}\")\n",
        "        \n",
        "        # Get effective mesh area (excluding regional overrides)\n",
        "        effective_mesh_geom = mesh_geom\n",
        "        if regional_mask is not None:\n",
        "            if mesh_geom.intersects(regional_mask):\n",
        "                effective_mesh_geom = mesh_geom.difference(regional_mask)\n",
        "                print(f\"  Excluded regional override areas from mesh {mesh_name}\")\n",
        "        \n",
        "        total_area = effective_mesh_geom.area\n",
        "        \n",
        "        # Create a simulated land cover distribution based on base_overrides\n",
        "        # In reality, you would use actual spatial analysis with the land cover raster\n",
        "        landcover_stats = []\n",
        "        \n",
        "        # Use the land cover types from the base overrides\n",
        "        for _, override_row in base_overrides.iterrows():\n",
        "            land_cover = override_row['Land Cover Name']\n",
        "            n_value = override_row[\"Base Mannings n Value\"]\n",
        "            \n",
        "            # Generate a random percentage for this example\n",
        "            # In reality, this would come from actual spatial analysis\n",
        "            np.random.seed(hash(land_cover) % 2**32)  # Use the land cover name as a seed\n",
        "            percentage = np.random.random() * 25  # Random percentage between 0-25%\n",
        "            \n",
        "            area = total_area * (percentage / 100)\n",
        "            \n",
        "            landcover_stats.append({\n",
        "                'Land Cover Type': land_cover,\n",
        "                'Area': area,\n",
        "                'Percentage': percentage,\n",
        "                'Current_n': n_value\n",
        "            })\n",
        "        \n",
        "        # Create DataFrame and sort by percentage\n",
        "        landcover_df = pd.DataFrame(landcover_stats)\n",
        "        landcover_df = landcover_df.sort_values('Percentage', ascending=False).reset_index(drop=True)\n",
        "        \n",
        "        # Store the results\n",
        "        all_results[mesh_name] = landcover_df\n",
        "    \n",
        "    # If there's only one mesh area, return its dataframe directly\n",
        "    if len(all_results) == 1:\n",
        "        return next(iter(all_results.values()))\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def get_regional_override_polygons(geom_hdf_path):\n",
        "    \"\"\"\n",
        "    Extract regional override polygon geometries from a HEC-RAS geometry HDF file.\n",
        "    \n",
        "    Args:\n",
        "        geom_hdf_path (str): Path to the HEC-RAS geometry HDF file\n",
        "        \n",
        "    Returns:\n",
        "        geopandas.GeoDataFrame: GeoDataFrame with regional override polygons\n",
        "    \"\"\"\n",
        "    import h5py\n",
        "    import geopandas as gpd\n",
        "    from shapely.geometry import Polygon\n",
        "    \n",
        "    try:\n",
        "        with h5py.File(geom_hdf_path, 'r') as f:\n",
        "            # Navigate to regional override polygons in the HDF structure\n",
        "            # This path would need to be determined based on the HEC-RAS HDF structure\n",
        "            if 'Geometry/Regional Manning Areas' in f:\n",
        "                region_group = f['Geometry/Regional Manning Areas']\n",
        "                \n",
        "                polygons = []\n",
        "                region_names = []\n",
        "                \n",
        "                # Process each regional override polygon\n",
        "                for region_name, region_data in region_group.items():\n",
        "                    # Extract polygon coordinates\n",
        "                    # This is a simplified example; actual implementation would depend on HDF structure\n",
        "                    if 'Polygon' in region_data:\n",
        "                        coords = region_data['Polygon'][:]\n",
        "                        polygon = Polygon(coords)\n",
        "                        polygons.append(polygon)\n",
        "                        region_names.append(region_name)\n",
        "                \n",
        "                # Create GeoDataFrame\n",
        "                if polygons:\n",
        "                    gdf = gpd.GeoDataFrame(\n",
        "                        {'region_name': region_names, 'geometry': polygons},\n",
        "                        crs='EPSG:4326'  # Set appropriate CRS\n",
        "                    )\n",
        "                    return gdf\n",
        "        \n",
        "        # Return empty GeoDataFrame if no regional overrides found\n",
        "        return gpd.GeoDataFrame(columns=['region_name', 'geometry'])\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting regional override polygons: {str(e)}\")\n",
        "        return gpd.GeoDataFrame(columns=['region_name', 'geometry'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Base Override Sensitivity Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "Base override sensitivity analysis varies Manning's n values for individual land cover types across the entire 2D mesh area. This approach is appropriate when:\n",
        "- Evaluating the impact of global land cover roughness changes\n",
        "- Calibrating models with uniform land cover distributions\n",
        "- Assessing which land cover types have the greatest influence on results\n",
        "\n",
        "The analysis automatically:\n",
        "1. Identifies land covers exceeding the area coverage threshold\n",
        "2. Generates test scenarios at specified intervals within literature ranges\n",
        "3. Executes plans in parallel\n",
        "4. Extracts results at a point of interest\n",
        "5. Creates sensitivity plots and time series comparisons\n",
        "\n",
        "## Function: individual_landuse_sensitivity_base()\n",
        "\n",
        "This function performs the complete base override sensitivity workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def individual_landuse_sensitivity_base(\n",
        "    project_folder,\n",
        "    template_plan,\n",
        "    point_of_interest,\n",
        "    area_threshold=10.0,  # percentage threshold for significant land uses\n",
        "    interval=0.01,\n",
        "    max_workers=2,\n",
        "    num_cores=2,\n",
        "    output_folder=\"Individual_Landuse_Sensitivity\",\n",
        "    n_ranges=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform sensitivity analysis by varying individual land use Manning's n values\n",
        "    in the base overrides.\n",
        "\n",
        "    Args:\n",
        "        project_folder (str): Path to HEC-RAS project folder\n",
        "        template_plan (str): Plan number to use as template\n",
        "        point_of_interest (tuple or Point): Coordinates for extracting results\n",
        "        area_threshold (float): Percentage threshold for significant land uses\n",
        "        interval (float): Interval for Manning's n test values\n",
        "        max_workers (int): Number of parallel workers\n",
        "        num_cores (int): Number of cores per worker\n",
        "        output_folder (str): Name of output folder\n",
        "        n_ranges (pd.DataFrame): DataFrame containing min/max Manning's n values.\n",
        "                                    Must contain columns: 'Land Cover Name', 'min_n', 'max_n'\n",
        "\n",
        "    Returns:\n",
        "        dict: Results of sensitivity analysis\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Convert point_of_interest to Point if not already\n",
        "    if not isinstance(point_of_interest, Point):\n",
        "        point_of_interest = Point(point_of_interest[0], point_of_interest[1])\n",
        "\n",
        "    # Verify n_ranges is provided\n",
        "    if n_ranges is None:\n",
        "        raise ValueError(\"n_ranges DataFrame must be provided\")\n",
        "\n",
        "    # Create timestamp for unique run identifier\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Initialize RAS project\n",
        "    print(f\"Initializing HEC-RAS project: {project_folder}\")\n",
        "    ras = init_ras_project(project_folder, \"6.6\")\n",
        "\n",
        "    # Create output directory\n",
        "    results_dir = Path(project_folder) / output_folder\n",
        "    results_dir.mkdir(exist_ok=True)\n",
        "    print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "    # Verify template plan exists\n",
        "    if template_plan not in ras.plan_df['plan_number'].values:\n",
        "        raise ValueError(f\"Template plan {template_plan} not found in project\")\n",
        "\n",
        "    # Get the geometry number for the template plan\n",
        "    template_geom = ras.plan_df.loc[ras.plan_df['plan_number'] == template_plan, 'geometry_number'].values[0]\n",
        "    print(f\"\\nTemplate plan: {template_plan} (Geometry: {template_geom})\")\n",
        "\n",
        "    # Get the geometry file path\n",
        "    geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == template_geom, 'full_path'].values[0]\n",
        "\n",
        "    # Get the original Manning's values\n",
        "    original_baseoverrides = RasGeo.get_mannings_baseoverrides(geom_path)\n",
        "    original_regionoverrides = RasGeo.get_mannings_regionoverrides(geom_path)\n",
        "\n",
        "    # Analyze land cover statistics for the 2D mesh areas\n",
        "    print(\"\\nAnalyzing land cover statistics for the 2D mesh areas...\")\n",
        "    landcover_stats = analyze_mesh_land_cover_statistics(\n",
        "        project_folder, \n",
        "        geom_number=template_geom\n",
        "    )\n",
        "\n",
        "    if landcover_stats is None:\n",
        "        raise ValueError(\"Could not analyze land cover statistics\")\n",
        "\n",
        "    # Identify significant land uses (above threshold)\n",
        "    significant_landuses = landcover_stats[landcover_stats['Percentage'] >= area_threshold].copy()\n",
        "    significant_landuses = significant_landuses.sort_values('Percentage', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    if len(significant_landuses) == 0:\n",
        "        print(f\"No land uses found with coverage above {area_threshold}% threshold\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nFound {len(significant_landuses)} significant land uses (>= {area_threshold}% coverage):\")\n",
        "    print(significant_landuses[['Land Cover Type', 'Percentage', 'Current_n']])\n",
        "\n",
        "    # Check if we'll exceed the plan limit\n",
        "    current_plan_count = len(ras.plan_df)\n",
        "    max_plans = 99  # HEC-RAS limit\n",
        "    remaining_plans = max_plans - current_plan_count\n",
        "\n",
        "    # Estimate the number of plans needed\n",
        "    estimated_plan_count = estimate_plan_count(significant_landuses, n_ranges, interval)\n",
        "\n",
        "    if estimated_plan_count > remaining_plans:\n",
        "        print(f\"\\nWARNING: This analysis would create approximately {estimated_plan_count} plans, but only {remaining_plans} more plans can be added (limit is 99)\")\n",
        "        print(\"Consider adjusting the following to reduce the number of plans:\")\n",
        "        print(f\"1. Increase the area threshold (currently {area_threshold}%)\")\n",
        "        print(f\"2. Increase the interval between test values (currently {interval})\")\n",
        "        print(f\"3. Reduce the min/max ranges for land uses\")\n",
        "        print(f\"4. Select fewer land uses to test\")\n",
        "\n",
        "        # Ask for confirmation to continue\n",
        "        response = input(\"\\nDo you want to continue anyway? (y/n): \")\n",
        "        if response.lower() != 'y':\n",
        "            print(\"Analysis canceled\")\n",
        "            return None\n",
        "\n",
        "    # Store the current (template) plan as base scenario\n",
        "    scenarios = [{\n",
        "        'name': 'Template',\n",
        "        'plan_number': template_plan,\n",
        "        'geom_number': template_geom,\n",
        "        'shortid': 'Template',\n",
        "        'land_cover': None,\n",
        "        'n_value': None,\n",
        "        'description': \"Original Manning's n Values\"\n",
        "    }]\n",
        "\n",
        "    # Function to create a modified plan with adjusted Manning's values for a specific land use\n",
        "    def create_modified_plan(land_cover, new_n_value):\n",
        "        # Create a shortid based on land cover and n value\n",
        "        # Convert land cover name to code (e.g. \"Open Water\" -> \"OW\")\n",
        "        code = ''.join([word[0] for word in land_cover.split() if word[0].isalpha()])\n",
        "        if not code:\n",
        "            code = land_cover[:2]\n",
        "        code = code.upper()\n",
        "\n",
        "        # Format n value for shortid\n",
        "        n_str = f\"{new_n_value:.3f}\".replace(\".\", \"\")\n",
        "        shortid = f\"B_{code}_{n_str}\"\n",
        "\n",
        "        print(f\"\\nCreating plan for '{land_cover}' with n = {new_n_value} (ShortID: {shortid})\")\n",
        "\n",
        "        # Clone the template plan\n",
        "        new_plan_number = RasPlan.clone_plan(template_plan, new_plan_shortid=shortid)\n",
        "\n",
        "        # Clone the template geometry\n",
        "        new_geom_number = RasPlan.clone_geom(template_geom)\n",
        "\n",
        "        # Set the new plan to use the new geometry\n",
        "        RasPlan.set_geom(new_plan_number, new_geom_number)\n",
        "\n",
        "        # Get the new geometry file path\n",
        "        new_geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number, 'full_path'].values[0]\n",
        "\n",
        "        # Create modified base overrides\n",
        "        modified_baseoverrides = original_baseoverrides.copy()\n",
        "\n",
        "        # Update the Manning's n value for this specific land cover type\n",
        "        land_cover_mask = modified_baseoverrides['Land Cover Name'] == land_cover\n",
        "        if land_cover_mask.any():\n",
        "            current_n = modified_baseoverrides.loc[land_cover_mask, \"Base Mannings n Value\"].values[0]\n",
        "            print(f\"  Changing '{land_cover}' from {current_n:.4f} to {new_n_value:.4f}\")\n",
        "            modified_baseoverrides.loc[land_cover_mask, \"Base Mannings n Value\"] = new_n_value\n",
        "        else:\n",
        "            print(f\"  Warning: Land cover '{land_cover}' not found in base overrides\")\n",
        "\n",
        "        # Apply the modified base overrides\n",
        "        RasGeo.set_mannings_baseoverrides(new_geom_path, modified_baseoverrides)\n",
        "\n",
        "        # Copy regional overrides unchanged if they exist\n",
        "        if not original_regionoverrides.empty:\n",
        "            RasGeo.set_mannings_regionoverrides(new_geom_path, original_regionoverrides)\n",
        "\n",
        "        # Store scenario details\n",
        "        return {\n",
        "            'name': f\"{land_cover}_{new_n_value:.3f}\",\n",
        "            'plan_number': new_plan_number,\n",
        "            'geom_number': new_geom_number,\n",
        "            'shortid': shortid,\n",
        "            'land_cover': land_cover,\n",
        "            'n_value': new_n_value,\n",
        "            'description': f\"Manning's n = {new_n_value:.3f} for {land_cover}\"\n",
        "        }\n",
        "\n",
        "    # Create plans for each significant land use with varying n values\n",
        "    all_plans_to_run = []\n",
        "\n",
        "    for _, landuse in significant_landuses.iterrows():\n",
        "        land_cover = landuse['Land Cover Type']\n",
        "        current_n = landuse['Current_n']\n",
        "\n",
        "        # Find matching land cover in n_ranges\n",
        "        match = n_ranges[n_ranges['Land Cover Name'] == land_cover]\n",
        "\n",
        "        if match.empty:\n",
        "            print(f\"Warning: No Manning's n range found for '{land_cover}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        min_n = match['min_n'].values[0]\n",
        "        max_n = match['max_n'].values[0]\n",
        "\n",
        "        print(f\"\\nProcessing land cover: {land_cover}\")\n",
        "        print(f\"  Current n: {current_n:.4f}\")\n",
        "        print(f\"  Literature range: {min_n:.4f} to {max_n:.4f}\")\n",
        "\n",
        "        # Generate test values within the range, excluding the current value\n",
        "        test_values = generate_sensitivity_values(min_n, max_n, current_n, interval)\n",
        "\n",
        "        print(f\"  Testing {len(test_values)} values: {[round(val, 3) for val in test_values]}\")\n",
        "\n",
        "        # Create a plan for each test value\n",
        "        for n_value in test_values:\n",
        "            new_scenario = create_modified_plan(land_cover, n_value)\n",
        "            scenarios.append(new_scenario)\n",
        "            all_plans_to_run.append(new_scenario['plan_number'])\n",
        "\n",
        "    # Save scenario information\n",
        "    scenario_info = pd.DataFrame(scenarios)\n",
        "    scenario_info_path = results_dir / \"scenarios.csv\"\n",
        "    scenario_info.to_csv(scenario_info_path, index=False)\n",
        "    print(f\"\\nScenario information saved to: {scenario_info_path}\")\n",
        "\n",
        "    # Run the plans (excluding the template which is already computed)\n",
        "    plans_to_run = [plan for plan in all_plans_to_run if plan != template_plan]\n",
        "\n",
        "    if not plans_to_run:\n",
        "        print(\"No plans to run.\")\n",
        "        return {'scenarios': scenarios, 'output_folder': results_dir}\n",
        "\n",
        "    print(f\"\\nRunning {len(plans_to_run)} plans in parallel...\")\n",
        "    execution_results = RasCmdr.compute_parallel(\n",
        "        plan_number=plans_to_run,\n",
        "        max_workers=max_workers,\n",
        "        num_cores=num_cores,\n",
        "        clear_geompre=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nExecution results:\")\n",
        "    for plan, success in execution_results.items():\n",
        "        print(f\"  Plan {plan}: {'Successful' if success else 'Failed'}\")\n",
        "\n",
        "    # If point of interest provided, extract and compare results\n",
        "    if point_of_interest is not None:\n",
        "        # Get geometry HDF path for cell identification\n",
        "        geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == template_geom, 'hdf_path'].values[0]\n",
        "\n",
        "        # Find the nearest mesh cell\n",
        "        mesh_cells_gdf = HdfMesh.get_mesh_cell_points(geom_hdf_path)\n",
        "        distances = mesh_cells_gdf.geometry.apply(lambda geom: geom.distance(point_of_interest))\n",
        "        nearest_idx = distances.idxmin()\n",
        "        mesh_cell_id = mesh_cells_gdf.loc[nearest_idx, 'cell_id']\n",
        "        mesh_name = mesh_cells_gdf.loc[nearest_idx, 'mesh_name']\n",
        "\n",
        "        print(f\"\\nNearest cell ID: {mesh_cell_id}\")\n",
        "        print(f\"Distance: {distances[nearest_idx]:.2f} units\")\n",
        "        print(f\"Mesh area: {mesh_name}\")\n",
        "\n",
        "        # Extract results for each scenario\n",
        "        all_results = {}\n",
        "        max_ws_values = []\n",
        "\n",
        "        for scenario in scenarios:\n",
        "            plan_number = scenario['plan_number']\n",
        "            land_cover = scenario['land_cover']\n",
        "            n_value = scenario['n_value']\n",
        "            shortid = scenario['shortid']\n",
        "\n",
        "            try:\n",
        "                results_xr = HdfResultsMesh.get_mesh_cells_timeseries(plan_number)\n",
        "\n",
        "                # Extract water surface data\n",
        "                ws_data = results_xr[mesh_name]['Water Surface'].sel(cell_id=int(mesh_cell_id))\n",
        "\n",
        "                # Convert to DataFrame\n",
        "                ws_df = pd.DataFrame({\n",
        "                    'time': ws_data.time.values,\n",
        "                    'water_surface': ws_data.values\n",
        "                })\n",
        "\n",
        "                # Store results\n",
        "                max_ws = ws_df['water_surface'].max()\n",
        "\n",
        "                all_results[plan_number] = {\n",
        "                    'scenario': scenario,\n",
        "                    'df': ws_df,\n",
        "                    'max_water_surface': max_ws\n",
        "                }\n",
        "\n",
        "                max_ws_values.append({\n",
        "                    'plan_number': plan_number,\n",
        "                    'shortid': shortid,\n",
        "                    'land_cover': land_cover,\n",
        "                    'n_value': n_value,\n",
        "                    'max_water_surface': max_ws\n",
        "                })\n",
        "\n",
        "                print(f\"  {shortid}: Max WSE = {max_ws:.2f}\")\n",
        "\n",
        "                # Save time series to CSV\n",
        "                ws_df.to_csv(results_dir / f\"timeseries_{shortid}.csv\", index=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error extracting results for {shortid}: {str(e)}\")\n",
        "\n",
        "        # Create summary DataFrame\n",
        "        if max_ws_values:\n",
        "            max_ws_df = pd.DataFrame(max_ws_values)\n",
        "            max_ws_df.to_csv(results_dir / \"max_water_surface_summary.csv\", index=False)\n",
        "\n",
        "            # Prepare mapping of land cover -> percentage (from significant_landuses)\n",
        "            lc_percentage_dict = dict(zip(significant_landuses['Land Cover Type'], significant_landuses['Percentage']))\n",
        "\n",
        "            # Create plots by land cover type\n",
        "            for land_cover in significant_landuses['Land Cover Type']:\n",
        "                # Filter scenarios for this land cover\n",
        "                land_cover_scenarios = max_ws_df[max_ws_df['land_cover'] == land_cover].copy()\n",
        "\n",
        "                # Add the template scenario\n",
        "                template_row = max_ws_df[max_ws_df['shortid'] == 'Template']\n",
        "                if not template_row.empty:\n",
        "                    land_cover_scenarios = pd.concat([template_row, land_cover_scenarios])\n",
        "\n",
        "                if land_cover_scenarios.empty:\n",
        "                    continue\n",
        "\n",
        "                # Sort by n_value\n",
        "                land_cover_scenarios = land_cover_scenarios.sort_values('n_value').reset_index(drop=True)\n",
        "\n",
        "                # Add coverage percentage string to label\n",
        "                perc = lc_percentage_dict.get(land_cover, None)\n",
        "                if perc is not None:\n",
        "                    perc_str = f\" ({perc:.1f}% coverage)\"\n",
        "                else:\n",
        "                    perc_str = \"\"\n",
        "\n",
        "                # Create plot\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                ax.plot(land_cover_scenarios['n_value'], land_cover_scenarios['max_water_surface'], \n",
        "                         marker='o', linestyle='-', linewidth=2)\n",
        "\n",
        "                # Add template point in a different color if it exists\n",
        "                template_idx = land_cover_scenarios[land_cover_scenarios['shortid'] == 'Template'].index\n",
        "                if not template_idx.empty:\n",
        "                    ax.scatter(land_cover_scenarios.loc[template_idx, 'n_value'], \n",
        "                                land_cover_scenarios.loc[template_idx, 'max_water_surface'],\n",
        "                                color='red', s=100, zorder=5, label='Template')\n",
        "\n",
        "                # Add labels and title, showing percentage of coverage\n",
        "                ax.set_xlabel(f\"Manning's n for {land_cover}{perc_str}\")\n",
        "                ax.set_ylabel(\"Maximum Water Surface Elevation (ft)\")\n",
        "                ax.set_title(f\"Sensitivity to {land_cover}{perc_str} Manning's n Value\")\n",
        "                ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "                if not template_idx.empty:\n",
        "                    ax.legend()\n",
        "\n",
        "                # Save plot\n",
        "                plot_path = results_dir / f\"sensitivity_{land_cover.replace(' ', '_').replace('/', '_')}.png\"\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(plot_path)\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "                print(f\"Created sensitivity plot for {land_cover}\")\n",
        "\n",
        "            # Create time series comparison plot for each land cover\n",
        "            for land_cover in significant_landuses['Land Cover Type']:\n",
        "                fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "                # Add coverage percentage string to label\n",
        "                perc = lc_percentage_dict.get(land_cover, None)\n",
        "                if perc is not None:\n",
        "                    perc_str = f\" ({perc:.1f}% coverage)\"\n",
        "                else:\n",
        "                    perc_str = \"\"\n",
        "\n",
        "                # Get template results\n",
        "                template_plan = scenarios[0]['plan_number']\n",
        "                if template_plan in all_results:\n",
        "                    template_df = all_results[template_plan]['df']\n",
        "                    ax.plot(template_df['time'], template_df['water_surface'],\n",
        "                             color='black', linewidth=2, label='Template')\n",
        "\n",
        "                # Filter scenarios for this land cover and plot\n",
        "                land_cover_scenarios = [s for s in scenarios if s['land_cover'] == land_cover]\n",
        "\n",
        "                if not land_cover_scenarios:\n",
        "                    plt.show()\n",
        "                    plt.close()\n",
        "                    continue\n",
        "\n",
        "                # Setup colormap for n values\n",
        "                n_values = [s['n_value'] for s in land_cover_scenarios if s['n_value'] is not None]\n",
        "                if not n_values:\n",
        "                    plt.show()\n",
        "                    plt.close()\n",
        "                    continue\n",
        "\n",
        "                min_n = min(n_values)\n",
        "                max_n = max(n_values)\n",
        "                norm = plt.Normalize(min_n, max_n)\n",
        "                cmap = plt.cm.viridis\n",
        "\n",
        "                # Plot each scenario with explicit legend entries, showing label with percentage\n",
        "                for scenario in land_cover_scenarios:\n",
        "                    plan_number = scenario['plan_number']\n",
        "                    n_value = scenario['n_value']\n",
        "                    if plan_number in all_results and n_value is not None:\n",
        "                        df = all_results[plan_number]['df']\n",
        "                        color = cmap(norm(n_value))\n",
        "                        label = f\"{land_cover}{perc_str}: n = {n_value:.3f}\"\n",
        "                        ax.plot(df['time'], df['water_surface'], color=color, \n",
        "                                 linewidth=1, alpha=0.7, label=label)\n",
        "\n",
        "                # Add labels and title, showing percentage of coverage\n",
        "                ax.set_xlabel(\"Time\")\n",
        "                ax.set_ylabel(\"Water Surface Elevation (ft)\")\n",
        "                ax.set_title(f\"WSE Time Series for Different {land_cover}{perc_str} Manning's n Values\")\n",
        "                ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "                # Add legend with land cover and n values\n",
        "                ax.legend(loc='best', fontsize='small', title=\"Scenarios\")\n",
        "\n",
        "                # Add colorbar\n",
        "                sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "                sm.set_array([])\n",
        "                plt.colorbar(sm, ax=ax).set_label(f\"Manning's n for {land_cover}{perc_str}\")\n",
        "\n",
        "                # Save plot\n",
        "                plot_path = results_dir / f\"timeseries_{land_cover.replace(' ', '_').replace('/', '_')}.png\"\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(plot_path)\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "                print(f\"Created time series plot for {land_cover}\")\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        'scenarios': scenarios,\n",
        "        'execution_results': execution_results if 'execution_results' in locals() else None,\n",
        "        'results': all_results if 'all_results' in locals() else None,\n",
        "        'max_ws_summary': max_ws_df if 'max_ws_df' in locals() else None,\n",
        "        'significant_landuses': significant_landuses,\n",
        "        'output_folder': results_dir\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing Base Override Sensitivity Analysis\n",
        "\n",
        "### Configuration Parameters\n",
        "\n",
        "The following example demonstrates how to run the base override sensitivity analysis with the BaldEagleCrkMulti2D sample project.\n",
        "\n",
        "**Key Parameters:**\n",
        "- `project_folder`: Path to the HEC-RAS project\n",
        "- `template_plan`: Base plan number to clone for sensitivity tests\n",
        "- `point_of_interest`: Coordinates where results will be extracted (x, y)\n",
        "- `area_threshold`: Minimum percentage of mesh coverage for land use to be analyzed (default: 10%)\n",
        "- `interval`: Step size for Manning's n test values (default: 0.01)\n",
        "- `max_workers`: Number of parallel execution workers\n",
        "- `num_cores`: CPU cores per HEC-RAS instance\n",
        "- `output_folder`: Directory name for results\n",
        "\n",
        "### Execution Steps\n",
        "\n",
        "1. Extract the example project\n",
        "2. Configure analysis parameters\n",
        "3. Run the sensitivity analysis (creates plans, executes them, and generates plots)\n",
        "4. Review results in the output folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Managing Plan Limits with Separate Project Folders\n",
        "\n",
        "**Why are we creating copies of the project folder?**\n",
        "\n",
        "HEC-RAS has a hard limit of 99 plans per project (.p01 to .p99). Sensitivity analyses, especially those testing multiple parameters at fine intervals, can easily generate dozens of plans.\n",
        "\n",
        "To avoid hitting this limit and to keep our analyses organized, we will:\n",
        "1.  **Extract** the base project once.\n",
        "2.  **Copy** it to a dedicated folder for **Base Override** sensitivity (suffix `_BOMIS`).\n",
        "3.  **Copy** it again to a dedicated folder for **Regional Override** sensitivity (suffix `_ROMIS`).\n",
        "\n",
        "This approach ensures that:\n",
        "*   Each analysis starts with a clean slate.\n",
        "*   We have a full 99-plan capacity for *each* type of sensitivity analysis.\n",
        "*   We don't accidentally overwrite or interfere with plans from the other analysis.\n",
        "*   The original extracted project remains a pristine backup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage for Base Overrides Sensitivity Analysis\n",
        "# To run this, uncomment the code, adjust parameters as needed, and execute the cell\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Extract the example project (the source)\n",
        "#    We extract to the default location first\n",
        "RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "source_project_folder = Path(os.getcwd()) / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "\n",
        "# 2. Create a specific folder for Base Override Multi-Interval Sensitivity (BOMIS)\n",
        "#    This prevents hitting the 99-plan limit by isolating this analysis\n",
        "project_folder = Path(os.getcwd()) / \"example_projects\" / \"BaldEagleCrkMulti2D_BOMIS\"\n",
        "\n",
        "# Clean up if it already exists to ensure a fresh start\n",
        "if project_folder.exists():\n",
        "    shutil.rmtree(project_folder)\n",
        "\n",
        "# Copy the fresh source project to the BOMIS folder\n",
        "print(f\"Creating dedicated BOMIS project folder...\")\n",
        "shutil.copytree(source_project_folder, project_folder)\n",
        "print(f\"Created: {project_folder}\")\n",
        "\n",
        "# 3. Initialize the BOMIS project\n",
        "#    (This updates the global 'ras' object to point to this new folder)\n",
        "init_ras_project(project_folder, \"6.6\")\n",
        "\n",
        "# Define parameters\n",
        "template_plan = \"03\"  # Use plan 03 as the template\n",
        "point_of_interest = (2076402, 366670)  # Coordinates where you want to extract results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Run the base sensitivity analysis\n",
        "base_sensitivity_results = individual_landuse_sensitivity_base(\n",
        "    project_folder=project_folder,\n",
        "    template_plan=template_plan,\n",
        "    point_of_interest=point_of_interest,\n",
        "    area_threshold=15.0,  # Only analyze land uses covering at least 10% of the mesh area\n",
        "    interval=0.02,       # Adjust interval to reduce the number of test values\n",
        "    max_workers=4,\n",
        "    num_cores=2,\n",
        "    output_folder=\"Base_Landuse_Sensitivity\",\n",
        "    n_ranges=manning_minmax_df\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Print summary information\n",
        "if base_sensitivity_results:\n",
        "    print(\"\\nAnalysis complete! Results saved to:\", base_sensitivity_results['output_folder'])\n",
        "    if 'significant_landuses' in base_sensitivity_results:\n",
        "        print(\"\\nSignificant land uses analyzed:\")\n",
        "        print(base_sensitivity_results['significant_landuses'][['Land Cover Type', 'Percentage']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Override Sensitivity Results\n",
        "\n",
        "### Interpreting the Results\n",
        "\n",
        "The following plots show sensitivity analysis results from the BaldEagleCrkMulti2D example project (Plan 03):\n",
        "\n",
        "**Sensitivity Plots (Right):**\n",
        "- Show how maximum water surface elevation varies with Manning's n\n",
        "- X-axis: Manning's n value for the specific land cover type\n",
        "- Y-axis: Maximum water surface elevation at the point of interest\n",
        "- Red dot: Current/template Manning's n value\n",
        "- Steeper slopes indicate higher sensitivity to that parameter\n",
        "\n",
        "**Time Series Plots (Left):**\n",
        "- Show complete hydrographs for all Manning's n scenarios\n",
        "- Color gradient represents different Manning's n values\n",
        "- Black line: Template (original) scenario\n",
        "- Useful for understanding how timing and peak magnitude respond to parameter changes\n",
        "\n",
        "**Key Observations to Look For:**\n",
        "- Which land covers cause the largest changes in water surface elevation?\n",
        "- Are the responses linear or non-linear?\n",
        "- Does increasing roughness always increase water levels (as expected)?\n",
        "- Are there threshold effects or inflection points?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Regional Override Sensitivity Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "Regional override sensitivity analysis focuses on Manning's n variations within specific calibration regions of the model. This approach is appropriate when:\n",
        "- Calibrating main channel roughness separately from floodplain\n",
        "- Analyzing the impact of localized land cover changes\n",
        "- Testing region-specific parameter uncertainty\n",
        "- Evaluating different roughness values in distinct hydraulic zones\n",
        "\n",
        "**Key Differences from Base Sensitivity:**\n",
        "- Tests only land covers present in the specified region(s)\n",
        "- Can target a single region or analyze all regions\n",
        "- Useful for main channel calibration or localized sensitivity testing\n",
        "- Regional overrides take precedence over base mesh values in HEC-RAS\n",
        "\n",
        "## Function: individual_landuse_sensitivity_region()\n",
        "\n",
        "This function performs the complete regional override sensitivity workflow with region-specific filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def individual_landuse_sensitivity_region(\n",
        "    project_folder,\n",
        "    template_plan,\n",
        "    point_of_interest,\n",
        "    area_threshold=10.0,  # percentage threshold for significant land uses\n",
        "    interval=0.01,\n",
        "    max_workers=2,\n",
        "    num_cores=2,\n",
        "    region_name=None,  # optional specific region to analyze\n",
        "    output_folder=\"Regional_Landuse_Sensitivity\",\n",
        "    n_ranges=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform sensitivity analysis by varying individual land use Manning's n values\n",
        "    in the regional overrides.\n",
        "    \n",
        "    Args:\n",
        "        project_folder (str): Path to HEC-RAS project folder\n",
        "        template_plan (str): Plan number to use as template\n",
        "        point_of_interest (tuple or Point): Coordinates for extracting results\n",
        "        area_threshold (float): Percentage threshold for significant land uses\n",
        "        interval (float): Interval for Manning's n test values\n",
        "        max_workers (int): Number of parallel workers\n",
        "        num_cores (int): Number of cores per worker\n",
        "        region_name (str): Optional specific region to analyze\n",
        "        output_folder (str): Name of output folder\n",
        "        n_ranges (pd.DataFrame): DataFrame containing min/max Manning's n values.\n",
        "                                    Must contain columns: 'Land Cover Name', 'min_n', 'max_n'\n",
        "    \n",
        "    Returns:\n",
        "        dict: Results of sensitivity analysis\n",
        "    \"\"\"\n",
        "    import time\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # Convert point_of_interest to Point if not already\n",
        "    if not isinstance(point_of_interest, Point):\n",
        "        point_of_interest = Point(point_of_interest[0], point_of_interest[1])\n",
        "    \n",
        "    # Verify n_ranges is provided\n",
        "    if n_ranges is None:\n",
        "        raise ValueError(\"n_ranges DataFrame must be provided\")\n",
        "    \n",
        "    # Create timestamp for unique run identifier\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Initialize RAS project\n",
        "    print(f\"Initializing HEC-RAS project: {project_folder}\")\n",
        "    ras = init_ras_project(project_folder, \"6.6\")\n",
        "    \n",
        "    # Create output directory\n",
        "    results_dir = Path(project_folder) / output_folder\n",
        "    results_dir.mkdir(exist_ok=True)\n",
        "    print(f\"Results will be saved to: {results_dir}\")\n",
        "    \n",
        "    # Verify template plan exists\n",
        "    if template_plan not in ras.plan_df['plan_number'].values:\n",
        "        raise ValueError(f\"Template plan {template_plan} not found in project\")\n",
        "    \n",
        "    # Get the geometry number for the template plan\n",
        "    template_geom = ras.plan_df.loc[ras.plan_df['plan_number'] == template_plan, 'geometry_number'].values[0]\n",
        "    print(f\"\\nTemplate plan: {template_plan} (Geometry: {template_geom})\")\n",
        "    \n",
        "    # Get the geometry file path\n",
        "    geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == template_geom, 'full_path'].values[0]\n",
        "    \n",
        "    # Get the original Manning's values\n",
        "    original_baseoverrides = RasGeo.get_mannings_baseoverrides(geom_path)\n",
        "    original_regionoverrides = RasGeo.get_mannings_regionoverrides(geom_path)\n",
        "    \n",
        "    # Check if regional overrides exist\n",
        "    if original_regionoverrides.empty:\n",
        "        print(\"No regional Manning's overrides found in the model\")\n",
        "        return None\n",
        "    \n",
        "    # If a specific region name is provided, filter the regional overrides\n",
        "    if region_name is not None:\n",
        "        region_mask = original_regionoverrides['Region Name'] == region_name\n",
        "        if not region_mask.any():\n",
        "            print(f\"Region '{region_name}' not found in the model\")\n",
        "            available_regions = original_regionoverrides['Region Name'].unique()\n",
        "            print(f\"Available regions: {available_regions}\")\n",
        "            return None\n",
        "        \n",
        "        region_overrides = original_regionoverrides[region_mask].copy()\n",
        "        print(f\"\\nAnalyzing sensitivity for region: {region_name}\")\n",
        "    else:\n",
        "        region_overrides = original_regionoverrides.copy()\n",
        "        print(\"\\nAnalyzing sensitivity for all regions\")\n",
        "    \n",
        "    # Get unique region tables\n",
        "    region_tables = region_overrides['Table Number'].unique()\n",
        "    print(f\"Region tables: {region_tables}\")\n",
        "    \n",
        "    # Analyze land cover statistics for the 2D mesh areas\n",
        "    print(\"\\nAnalyzing land cover statistics for the 2D mesh areas...\")\n",
        "    landcover_stats = analyze_mesh_land_cover_statistics(\n",
        "        project_folder, \n",
        "        geom_number=template_geom\n",
        "    )\n",
        "    \n",
        "    if landcover_stats is None:\n",
        "        raise ValueError(\"Could not analyze land cover statistics\")\n",
        "    \n",
        "    # Identify significant land uses (above threshold)\n",
        "    significant_landuses = landcover_stats[landcover_stats['Percentage'] >= area_threshold].copy()\n",
        "    significant_landuses = significant_landuses.sort_values('Percentage', ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    if len(significant_landuses) == 0:\n",
        "        print(f\"No land uses found with coverage above {area_threshold}% threshold\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\nFound {len(significant_landuses)} significant land uses (>= {area_threshold}% coverage):\")\n",
        "    print(significant_landuses[['Land Cover Type', 'Percentage', 'Current_n']])\n",
        "    \n",
        "    # Filter significant land uses to only those present in the region overrides\n",
        "    region_landcover_types = set(region_overrides['Land Cover Name'].unique())\n",
        "    filtered_landuses = significant_landuses[\n",
        "        significant_landuses['Land Cover Type'].isin(region_landcover_types)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(filtered_landuses) == 0:\n",
        "        print(\"None of the significant land uses are present in the regional overrides\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\nSignificant land uses present in regional overrides:\")\n",
        "    print(filtered_landuses[['Land Cover Type', 'Percentage']])\n",
        "    \n",
        "    # Check if we'll exceed the plan limit\n",
        "    current_plan_count = len(ras.plan_df)\n",
        "    max_plans = 99  # HEC-RAS limit\n",
        "    remaining_plans = max_plans - current_plan_count\n",
        "    \n",
        "    # Estimate the number of plans needed\n",
        "    estimated_plan_count = 0\n",
        "    \n",
        "    # Create a table to store land use sensitivity information\n",
        "    sensitivity_table = []\n",
        "    \n",
        "    for _, landuse in filtered_landuses.iterrows():\n",
        "        land_cover = landuse['Land Cover Type']\n",
        "        \n",
        "        # Find this land cover in the region overrides\n",
        "        for region_table in region_tables:\n",
        "            # Create mask for this land cover and table\n",
        "            mask = (region_overrides['Land Cover Name'] == land_cover) & \\\n",
        "                   (region_overrides['Table Number'] == region_table)\n",
        "            \n",
        "            if not mask.any():\n",
        "                continue\n",
        "                \n",
        "            current_n = region_overrides.loc[mask, 'MainChannel'].values[0]\n",
        "            \n",
        "            # Find matching land cover in n_ranges\n",
        "            match = n_ranges[n_ranges['Land Cover Name'] == land_cover]\n",
        "            if match.empty:\n",
        "                continue\n",
        "                \n",
        "            min_n = match['min_n'].values[0]\n",
        "            max_n = match['max_n'].values[0]\n",
        "            \n",
        "            # Count values between min and max at interval spacing, excluding current value\n",
        "            values = generate_sensitivity_values(min_n, max_n, current_n, interval)\n",
        "            num_values = len(values)\n",
        "            estimated_plan_count += num_values\n",
        "            \n",
        "            # Add to sensitivity table\n",
        "            region_name = region_overrides.loc[mask, 'Region Name'].values[0] if 'Region Name' in region_overrides.columns else f\"Table {region_table}\"\n",
        "            sensitivity_table.append({\n",
        "                'Land Cover': land_cover,\n",
        "                'Region': region_name,\n",
        "                'Table': region_table,\n",
        "                'Current n': current_n,\n",
        "                'Min n': min_n,\n",
        "                'Max n': max_n,\n",
        "                'Test Values': num_values,\n",
        "                'n Range': f\"{min_n:.3f} - {max_n:.3f}\"\n",
        "            })\n",
        "    \n",
        "    # Print the sensitivity analysis table\n",
        "    if sensitivity_table:\n",
        "        print(\"\\nSensitivity Analysis Plan:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Land Cover':<20} {'Region':<15} {'Current n':<10} {'n Range':<15} {'Test Values':<12}\")\n",
        "        print(\"-\" * 80)\n",
        "        for row in sensitivity_table:\n",
        "            print(f\"{row['Land Cover']:<20} {row['Region']:<15} {row['Current n']:<10.3f} {row['n Range']:<15} {row['Test Values']:<12}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Total estimated plans to be created: {estimated_plan_count}\")\n",
        "        print(\"-\" * 80)\n",
        "    if estimated_plan_count > remaining_plans:\n",
        "        print(f\"\\nWARNING: This analysis would create approximately {estimated_plan_count} plans, but only {remaining_plans} more plans can be added (limit is 99)\")\n",
        "        print(\"Consider adjusting the following to reduce the number of plans:\")\n",
        "        print(f\"1. Increase the area threshold (currently {area_threshold}%)\")\n",
        "        print(f\"2. Increase the interval between test values (currently {interval})\")\n",
        "        print(f\"3. Reduce the min/max ranges for land uses\")\n",
        "        print(f\"4. Select fewer land uses to test\")\n",
        "        print(f\"5. Specify a single region to test (currently {'specific region' if region_name else 'all regions'})\")\n",
        "        \n",
        "        # Ask for confirmation to continue\n",
        "        response = input(\"\\nDo you want to continue anyway? (y/n): \")\n",
        "        if response.lower() != 'y':\n",
        "            print(\"Analysis canceled\")\n",
        "            return None\n",
        "    \n",
        "    # Store the current (template) plan as base scenario\n",
        "    scenarios = [{\n",
        "        'name': 'Template',\n",
        "        'plan_number': template_plan,\n",
        "        'geom_number': template_geom,\n",
        "        'shortid': 'Template',\n",
        "        'land_cover': None,\n",
        "        'region_name': None,\n",
        "        'table_number': None,\n",
        "        'n_value': None,\n",
        "        'description': \"Original Manning's n Values\"\n",
        "    }]\n",
        "    \n",
        "    # Function to create a modified plan with adjusted Manning's n values for a specific land use in a region\n",
        "    def create_modified_plan(land_cover, table_number, region_name, new_n_value):\n",
        "        # Create a shortid based on land cover, region, and n value\n",
        "        # Convert land cover name to code (e.g. \"Open Water\" -> \"OW\")\n",
        "        lc_code = ''.join([word[0] for word in land_cover.split() if word[0].isalpha()])\n",
        "        if not lc_code:\n",
        "            lc_code = land_cover[:2]\n",
        "        lc_code = lc_code.upper()\n",
        "        \n",
        "        # Convert region name to code\n",
        "        rg_code = ''.join([word[0] for word in region_name.split() if word[0].isalpha()])\n",
        "        if not rg_code:\n",
        "            rg_code = region_name[:2]\n",
        "        rg_code = rg_code.upper()\n",
        "        \n",
        "        # Format n value for shortid\n",
        "        n_str = f\"{new_n_value:.3f}\".replace(\".\", \"\")\n",
        "        shortid = f\"R_{lc_code}_{rg_code}_{n_str}\"\n",
        "        \n",
        "        print(f\"\\nCreating plan for '{land_cover}' in '{region_name}' with n = {new_n_value} (ShortID: {shortid})\")\n",
        "        \n",
        "        # Clone the template plan\n",
        "        new_plan_number = RasPlan.clone_plan(template_plan, new_plan_shortid=shortid)\n",
        "        \n",
        "        # Clone the template geometry\n",
        "        new_geom_number = RasPlan.clone_geom(template_geom)\n",
        "        \n",
        "        # Set the new plan to use the new geometry\n",
        "        RasPlan.set_geom(new_plan_number, new_geom_number)\n",
        "        \n",
        "        # Get the new geometry file path\n",
        "        new_geom_path = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number, 'full_path'].values[0]\n",
        "        \n",
        "        # Copy base overrides unchanged\n",
        "        RasGeo.set_mannings_baseoverrides(new_geom_path, original_baseoverrides)\n",
        "        \n",
        "        # Create modified region overrides\n",
        "        modified_regionoverrides = original_regionoverrides.copy()\n",
        "        \n",
        "        # Update the Manning's n value for this specific land cover type in this region and table\n",
        "        region_mask = (modified_regionoverrides['Land Cover Name'] == land_cover) & \\\n",
        "                     (modified_regionoverrides['Table Number'] == table_number) & \\\n",
        "                     (modified_regionoverrides['Region Name'] == region_name)\n",
        "                     \n",
        "        if region_mask.any():\n",
        "            current_n = modified_regionoverrides.loc[region_mask, 'MainChannel'].values[0]\n",
        "            print(f\"  Changing '{land_cover}' in '{region_name}' (Table {table_number}) from {current_n:.4f} to {new_n_value:.4f}\")\n",
        "            modified_regionoverrides.loc[region_mask, 'MainChannel'] = new_n_value\n",
        "        else:\n",
        "            print(f\"  Warning: Land cover '{land_cover}' not found in region '{region_name}' (Table {table_number})\")\n",
        "        \n",
        "        # Apply the modified region overrides\n",
        "        RasGeo.set_mannings_regionoverrides(new_geom_path, modified_regionoverrides)\n",
        "        \n",
        "        # Store scenario details\n",
        "        return {\n",
        "            'name': f\"{land_cover}_{region_name}_{new_n_value:.3f}\",\n",
        "            'plan_number': new_plan_number,\n",
        "            'geom_number': new_geom_number,\n",
        "            'shortid': shortid,\n",
        "            'land_cover': land_cover,\n",
        "            'region_name': region_name,\n",
        "            'table_number': table_number,\n",
        "            'n_value': new_n_value,\n",
        "            'description': f\"Manning's n = {new_n_value:.3f} for {land_cover} in {region_name}\"\n",
        "        }\n",
        "    \n",
        "    # Create plans for each significant land use with varying n values\n",
        "    all_plans_to_run = []\n",
        "    \n",
        "    for _, landuse in filtered_landuses.iterrows():\n",
        "        land_cover = landuse['Land Cover Type']\n",
        "        \n",
        "        # Find matching land cover in n_ranges\n",
        "        match = n_ranges[n_ranges['Land Cover Name'] == land_cover]\n",
        "        \n",
        "        if match.empty:\n",
        "            print(f\"Warning: No Manning's n range found for '{land_cover}'. Skipping.\")\n",
        "            continue\n",
        "            \n",
        "        min_n = match['min_n'].values[0]\n",
        "        max_n = match['max_n'].values[0]\n",
        "        \n",
        "        # Process each region table for this land cover\n",
        "        for region_table in region_tables:\n",
        "            # Get all regions with this land cover in this table\n",
        "            regions_mask = (region_overrides['Land Cover Name'] == land_cover) & \\\n",
        "                          (region_overrides['Table Number'] == region_table)\n",
        "            \n",
        "            if not regions_mask.any():\n",
        "                continue\n",
        "            \n",
        "            # Get unique region names for this land cover and table\n",
        "            unique_regions = region_overrides.loc[regions_mask, 'Region Name'].unique()\n",
        "            \n",
        "            for region in unique_regions:\n",
        "                # If a specific region was requested, skip others\n",
        "                if region_name is not None and region != region_name:\n",
        "                    continue\n",
        "                \n",
        "                # Create mask for this specific combination\n",
        "                specific_mask = (region_overrides['Land Cover Name'] == land_cover) & \\\n",
        "                               (region_overrides['Table Number'] == region_table) & \\\n",
        "                               (region_overrides['Region Name'] == region)\n",
        "                \n",
        "                if not specific_mask.any():\n",
        "                    continue\n",
        "                \n",
        "                current_n = region_overrides.loc[specific_mask, 'MainChannel'].values[0]\n",
        "                \n",
        "                print(f\"\\nProcessing land cover: {land_cover} in region: {region} (Table {region_table})\")\n",
        "                print(f\"  Current n: {current_n:.4f}\")\n",
        "                print(f\"  Literature range: {min_n:.4f} to {max_n:.4f}\")\n",
        "                \n",
        "                # Generate test values within the range, excluding the current value\n",
        "                test_values = generate_sensitivity_values(min_n, max_n, current_n, interval)\n",
        "                \n",
        "                print(f\"  Testing {len(test_values)} values: {[round(val, 3) for val in test_values]}\")\n",
        "                \n",
        "                # Create a plan for each test value\n",
        "                for n_value in test_values:\n",
        "                    new_scenario = create_modified_plan(land_cover, region_table, region, n_value)\n",
        "                    scenarios.append(new_scenario)\n",
        "                    all_plans_to_run.append(new_scenario['plan_number'])\n",
        "    \n",
        "    # Save scenario information\n",
        "    scenario_info = pd.DataFrame(scenarios)\n",
        "    scenario_info_path = results_dir / \"scenarios.csv\"\n",
        "    scenario_info.to_csv(scenario_info_path, index=False)\n",
        "    print(f\"\\nScenario information saved to: {scenario_info_path}\")\n",
        "    \n",
        "    # Run the plans (excluding the template which is already computed)\n",
        "    plans_to_run = [plan for plan in all_plans_to_run if plan != template_plan]\n",
        "    \n",
        "    if not plans_to_run:\n",
        "        print(\"No plans to run.\")\n",
        "        return {'scenarios': scenarios, 'output_folder': results_dir}\n",
        "    \n",
        "    print(f\"\\nRunning {len(plans_to_run)} plans in parallel...\")\n",
        "    execution_results = RasCmdr.compute_parallel(\n",
        "        plan_number=plans_to_run,\n",
        "        max_workers=max_workers,\n",
        "        num_cores=num_cores,\n",
        "        clear_geompre=True\n",
        "    )\n",
        "    \n",
        "    print(\"\\nExecution results:\")\n",
        "    for plan, success in execution_results.items():\n",
        "        print(f\"  Plan {plan}: {'Successful' if success else 'Failed'}\")\n",
        "    \n",
        "    # If point of interest provided, extract and compare results\n",
        "    if point_of_interest is not None:\n",
        "        # Get geometry HDF path for cell identification\n",
        "        geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == template_geom, 'hdf_path'].values[0]\n",
        "        \n",
        "        # Find the nearest mesh cell\n",
        "        mesh_cells_gdf = HdfMesh.get_mesh_cell_points(geom_hdf_path)\n",
        "        distances = mesh_cells_gdf.geometry.apply(lambda geom: geom.distance(point_of_interest))\n",
        "        nearest_idx = distances.idxmin()\n",
        "        mesh_cell_id = mesh_cells_gdf.loc[nearest_idx, 'cell_id']\n",
        "        mesh_name = mesh_cells_gdf.loc[nearest_idx, 'mesh_name']\n",
        "        \n",
        "        print(f\"\\nNearest cell ID: {mesh_cell_id}\")\n",
        "        print(f\"Distance: {distances[nearest_idx]:.2f} units\")\n",
        "        print(f\"Mesh area: {mesh_name}\")\n",
        "        \n",
        "        # Extract results for each scenario\n",
        "        all_results = {}\n",
        "        max_ws_values = []\n",
        "        \n",
        "        for scenario in scenarios:\n",
        "            plan_number = scenario['plan_number']\n",
        "            land_cover = scenario['land_cover']\n",
        "            region_name = scenario['region_name']\n",
        "            n_value = scenario['n_value']\n",
        "            shortid = scenario['shortid']\n",
        "            \n",
        "            try:\n",
        "                results_xr = HdfResultsMesh.get_mesh_cells_timeseries(plan_number)\n",
        "                \n",
        "                # Extract water surface data\n",
        "                ws_data = results_xr[mesh_name]['Water Surface'].sel(cell_id=int(mesh_cell_id))\n",
        "                \n",
        "                # Convert to DataFrame\n",
        "                ws_df = pd.DataFrame({\n",
        "                    'time': ws_data.time.values,\n",
        "                    'water_surface': ws_data.values\n",
        "                })\n",
        "                \n",
        "                # Store results\n",
        "                max_ws = ws_df['water_surface'].max()\n",
        "                \n",
        "                all_results[plan_number] = {\n",
        "                    'scenario': scenario,\n",
        "                    'df': ws_df,\n",
        "                    'max_water_surface': max_ws\n",
        "                }\n",
        "                \n",
        "                max_ws_values.append({\n",
        "                    'plan_number': plan_number,\n",
        "                    'shortid': shortid,\n",
        "                    'land_cover': land_cover,\n",
        "                    'region_name': region_name,\n",
        "                    'n_value': n_value,\n",
        "                    'max_water_surface': max_ws\n",
        "                })\n",
        "                \n",
        "                print(f\"  {shortid}: Max WSE = {max_ws:.2f}\")\n",
        "                \n",
        "                # Save time series to CSV\n",
        "                ws_df.to_csv(results_dir / f\"timeseries_{shortid}.csv\", index=False)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  Error extracting results for {shortid}: {str(e)}\")\n",
        "        \n",
        "        # Create summary DataFrame\n",
        "        if max_ws_values:\n",
        "            max_ws_df = pd.DataFrame(max_ws_values)\n",
        "            max_ws_df.to_csv(results_dir / \"max_water_surface_summary.csv\", index=False)\n",
        "            \n",
        "            # Create plots by land cover type and region\n",
        "            land_cover_region_combinations = []\n",
        "            \n",
        "            for _, row in max_ws_df.iterrows():\n",
        "                if row['land_cover'] is not None and row['region_name'] is not None:\n",
        "                    combination = (row['land_cover'], row['region_name'])\n",
        "                    if combination not in land_cover_region_combinations:\n",
        "                        land_cover_region_combinations.append(combination)\n",
        "            \n",
        "            # Create sensitivity plots for each land cover + region combination\n",
        "            for land_cover, region in land_cover_region_combinations:\n",
        "                # Filter scenarios for this combination\n",
        "                combo_scenarios = max_ws_df[\n",
        "                    (max_ws_df['land_cover'] == land_cover) & \n",
        "                    (max_ws_df['region_name'] == region)\n",
        "                ].copy()\n",
        "                \n",
        "                # Add the template scenario\n",
        "                template_row = max_ws_df[max_ws_df['shortid'] == 'Template']\n",
        "                if not template_row.empty:\n",
        "                    combo_scenarios = pd.concat([template_row, combo_scenarios])\n",
        "                \n",
        "                if combo_scenarios.empty:\n",
        "                    continue\n",
        "                \n",
        "                # Sort by n_value\n",
        "                combo_scenarios = combo_scenarios.sort_values('n_value').reset_index(drop=True)\n",
        "                \n",
        "                # Create plot\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                ax.plot(combo_scenarios['n_value'], combo_scenarios['max_water_surface'], \n",
        "                         marker='o', linestyle='-', linewidth=2)\n",
        "                \n",
        "                # Add template point in a different color if it exists\n",
        "                template_idx = combo_scenarios[combo_scenarios['shortid'] == 'Template'].index\n",
        "                if not template_idx.empty:\n",
        "                    ax.scatter(combo_scenarios.loc[template_idx, 'n_value'], \n",
        "                                combo_scenarios.loc[template_idx, 'max_water_surface'],\n",
        "                                color='red', s=100, zorder=5, label='Template')\n",
        "                \n",
        "                # Add labels and title\n",
        "                ax.set_xlabel(f\"Manning's n for {land_cover} in {region}\")\n",
        "                ax.set_ylabel(\"Maximum Water Surface Elevation (ft)\")\n",
        "                ax.set_title(f\"Sensitivity to {land_cover} Manning's n Value in {region}\")\n",
        "                ax.grid(True, linestyle='--', alpha=0.7)\n",
        "                \n",
        "                if not template_idx.empty:\n",
        "                    ax.legend()\n",
        "                \n",
        "                # Save plot\n",
        "                safe_lc = land_cover.replace(' ', '_').replace('/', '_')\n",
        "                safe_rg = region.replace(' ', '_').replace('/', '_')\n",
        "                plot_path = results_dir / f\"sensitivity_{safe_lc}_{safe_rg}.png\"\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(plot_path)\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "                print(f\"Created sensitivity plot for {land_cover} in {region}\")\n",
        "            \n",
        "            # Create time series comparison plots for each land cover + region combination\n",
        "            for land_cover, region in land_cover_region_combinations:\n",
        "                fig, ax = plt.subplots(figsize=(12, 6))\n",
        "                \n",
        "                # Get template results\n",
        "                template_plan = scenarios[0]['plan_number']\n",
        "                if template_plan in all_results:\n",
        "                    template_df = all_results[template_plan]['df']\n",
        "                    ax.plot(template_df['time'], template_df['water_surface'], \n",
        "                             color='black', linewidth=2, label='Template')\n",
        "                \n",
        "                # Filter scenarios for this combination\n",
        "                combo_scenarios = [\n",
        "                    s for s in scenarios \n",
        "                    if s['land_cover'] == land_cover and s['region_name'] == region\n",
        "                ]\n",
        "                \n",
        "                if not combo_scenarios:\n",
        "                    plt.show()\n",
        "                    plt.close()\n",
        "                    continue\n",
        "                \n",
        "                # Setup colormap for n values\n",
        "                n_values = [s['n_value'] for s in combo_scenarios if s['n_value'] is not None]\n",
        "                if not n_values:\n",
        "                    plt.show()\n",
        "                    plt.close()\n",
        "                    continue\n",
        "                    \n",
        "                min_n = min(n_values)\n",
        "                max_n = max(n_values)\n",
        "                norm = plt.Normalize(min_n, max_n)\n",
        "                cmap = plt.cm.viridis\n",
        "                \n",
        "                # Plot each scenario\n",
        "                for scenario in combo_scenarios:\n",
        "                    plan_number = scenario['plan_number']\n",
        "                    n_value = scenario['n_value']\n",
        "                    \n",
        "                    if plan_number in all_results and n_value is not None:\n",
        "                        df = all_results[plan_number]['df']\n",
        "                        color = cmap(norm(n_value))\n",
        "                        ax.plot(df['time'], df['water_surface'], color=color, \n",
        "                                 linewidth=1, alpha=0.7, label=f\"n = {n_value:.3f}\")\n",
        "                \n",
        "                # Add labels and title\n",
        "                ax.set_xlabel(\"Time\")\n",
        "                ax.set_ylabel(\"Water Surface Elevation (ft)\")\n",
        "                ax.set_title(f\"WSE Time Series for {land_cover} in {region}\")\n",
        "                ax.grid(True, linestyle='--', alpha=0.7)\n",
        "                \n",
        "                # Add colorbar\n",
        "                sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "                sm.set_array([])\n",
        "                cbar = plt.colorbar(sm, ax=ax)\n",
        "                cbar.set_label(f\"Manning's n for {land_cover}\")\n",
        "                \n",
        "                # Save plot\n",
        "                safe_lc = land_cover.replace(' ', '_').replace('/', '_')\n",
        "                safe_rg = region.replace(' ', '_').replace('/', '_')\n",
        "                plot_path = results_dir / f\"timeseries_{safe_lc}_{safe_rg}.png\"\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(plot_path)\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "                print(f\"Created time series plot for {land_cover} in {region}\")\n",
        "    \n",
        "    # Return results\n",
        "    return {\n",
        "        'scenarios': scenarios,\n",
        "        'execution_results': execution_results if 'execution_results' in locals() else None,\n",
        "        'results': all_results if 'all_results' in locals() else None,\n",
        "        'max_ws_summary': max_ws_df if 'max_ws_df' in locals() else None,\n",
        "        'significant_landuses': filtered_landuses,\n",
        "        'output_folder': results_dir\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u26a0\ufe0f Important Engineering Note on Regional Overrides\n",
        "\n",
        "We are using a HEC Example project with regional overriddes in the main channel.  A better application would be to vary values individually within larger calibration regions.  A further explanation follows: \n",
        "\n",
        "**Main Channel vs. Floodplain Application:**\n",
        "\n",
        "For **main channel** regional overrides, a **bulk sensitivity approach** is typically more appropriate (see notebook 105 for bulk Manning's n variation). This is because:\n",
        "\n",
        "1. **Land use-based roughness** values are derived from satellite imagery, which may not correlate well with main channel hydraulic roughness\n",
        "2. **Main channel roughness** is typically governed by bed material, channel shape, and vegetation conditions\u2014not upland land cover classifications\n",
        "3. **Bulk variation** allows testing a continuous range of main channel n values without land use constraints\n",
        "\n",
        "**This Methodology Best Applied To:**\n",
        "- Large calibration regions within the 2D mesh area\n",
        "- Floodplain zones with distinct land cover characteristics\n",
        "- Areas where satellite-derived land use correlates with hydraulic roughness\n",
        "\n",
        "**For Main Channel Calibration:**\n",
        "- Use the bulk sensitivity approach from notebook 105\n",
        "- Create multiple regional overrides to test different main channel reaches independently\n",
        "- Consider physical channel characteristics rather than land cover when selecting test values\n",
        "\n",
        "**Note on Example Project:**\n",
        "This notebook demonstrates the regional sensitivity methodology using the BaldEagleCrkMulti2D main channel region due to limited availability of example models with large floodplain calibration regions. In practice, apply this approach to appropriate floodplain or mesh regions where land cover-based roughness is hydraulically meaningful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing Regional Override Sensitivity Analysis\n",
        "\n",
        "### Configuration Parameters\n",
        "\n",
        "The following example demonstrates how to run the regional override sensitivity analysis with the BaldEagleCrkMulti2D sample project.\n",
        "\n",
        "**Key Parameters (in addition to base parameters):**\n",
        "- `region_name`: Specific region to analyze (e.g., \"Main Channel\"), or `None` to analyze all regions\n",
        "- All other parameters match the base sensitivity function\n",
        "\n",
        "**Region-Specific Behavior:**\n",
        "- Only land covers present in the specified region(s) will be tested\n",
        "- The function automatically filters significant land uses based on region membership\n",
        "- Multiple regions with the same land cover will be tested independently\n",
        "\n",
        "### Execution Steps\n",
        "\n",
        "1. Use the same project from the base analysis (already extracted)\n",
        "2. Configure regional analysis parameters (note the `region_name` parameter)\n",
        "3. Run the sensitivity analysis\n",
        "4. Review region-specific results in the output folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the mannings n max and min values as a range. \n",
        "\n",
        "manning_data = [\n",
        "    {\"Land Cover Name\": \"NoData\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Barren Land Rock/Sand/Clay\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Cultivated Crops\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Deciduous Forest\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Developed, High Intensity\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Developed, Low Intensity\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Developed, Medium Intensity\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Developed, Open Space\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Emergent Herbaceous Wetlands\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Evergreen Forest\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Grassland/Herbaceous\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Mixed Forest\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Open Water\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Pasture/Hay\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Shrub/Scrub\", \"min_n\": 0.03, \"max_n\": 0.23},\n",
        "    {\"Land Cover Name\": \"Woody Wetlands\", \"min_n\": 0.03, \"max_n\": 0.23}\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "manning_minmax_df = pd.DataFrame(manning_data)\n",
        "\n",
        "# Calculate the midpoint value\n",
        "manning_minmax_df['mid_n'] = (manning_minmax_df['min_n'] + manning_minmax_df['max_n']) / 2\n",
        "\n",
        "# Sort by land cover name\n",
        "manning_minmax_df = manning_minmax_df.sort_values('Land Cover Name').reset_index(drop=True)\n",
        "\n",
        "# Print summary information\n",
        "print(f\"Manning's n value ranges for {len(manning_minmax_df)} land cover types:\")\n",
        "print(manning_minmax_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage for Regional Overrides Sensitivity Analysis\n",
        "# To run this, uncomment the code, adjust parameters as needed, and execute the cell\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Define source and destination paths\n",
        "#    (We use the original extracted project as source, not the BOMIS one)\n",
        "source_project_folder = Path(os.getcwd()) / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "\n",
        "# 2. Create a specific folder for Regional Override Multi-Interval Sensitivity (ROMIS)\n",
        "project_folder = Path(os.getcwd()) / \"example_projects\" / \"BaldEagleCrkMulti2D_ROMIS\"\n",
        "\n",
        "# Clean up if it already exists\n",
        "if project_folder.exists():\n",
        "    shutil.rmtree(project_folder)\n",
        "\n",
        "# Copy the source project to the ROMIS folder\n",
        "print(f\"Creating dedicated ROMIS project folder...\")\n",
        "shutil.copytree(source_project_folder, project_folder)\n",
        "print(f\"Created: {project_folder}\")\n",
        "\n",
        "# 3. Initialize the ROMIS project\n",
        "#    (This updates the global 'ras' object to point to this new folder, replacing the BOMIS context)\n",
        "init_ras_project(project_folder, \"6.6\")\n",
        "\n",
        "# Define parameters\n",
        "template_plan = \"03\"  # Plan 03 has regional overrides\n",
        "point_of_interest = (2081544, 365715) # Coordinates for regional analysis\n",
        "\n",
        "# Run the regional sensitivity analysis\n",
        "run_region_sensitivity_results = individual_landuse_sensitivity_region(\n",
        "    project_folder=project_folder,\n",
        "    template_plan=template_plan,\n",
        "    point_of_interest=point_of_interest,\n",
        "    area_threshold=10.0,  # Only analyze land uses covering at least 10% of the mesh area\n",
        "    interval=0.04,       # Adjust interval to reduce the number of test values\n",
        "    max_workers=4,\n",
        "    num_cores=2,\n",
        "    region_name=\"Main Channel\",  # Specify a region or set to None for all regions\n",
        "    output_folder=\"Regional_Landuse_Sensitivity\",\n",
        "    n_ranges=manning_minmax_df\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary information\n",
        "if run_region_sensitivity_results:\n",
        "    print(\"\\nAnalysis complete! Results saved to:\", run_region_sensitivity_results['output_folder'])\n",
        "    if 'significant_landuses' in run_region_sensitivity_results:\n",
        "        print(\"\\nSignificant land uses analyzed in regions:\")\n",
        "        print(run_region_sensitivity_results['significant_landuses'][['Land Cover Type', 'Percentage']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regional Override Sensitivity Results\n",
        "\n",
        "### Interpreting Regional Results\n",
        "\n",
        "The following plots show sensitivity analysis results for regional Manning's n overrides from the BaldEagleCrkMulti2D example project (Plan 03). The plots follow the same format as the base sensitivity results, but focus on parameters within specific calibration regions.\n",
        "\n",
        "**Key Differences from Base Results:**\n",
        "- Plots are labeled with both land cover type AND region name\n",
        "- Changes affect only the specified region, not the entire mesh\n",
        "- Sensitivity may be higher or lower depending on the hydraulic importance of the region\n",
        "- Useful for understanding which regional parameters require the most careful calibration\n",
        "\n",
        "**Regional vs. Base Comparison:**\n",
        "- Compare the magnitude of water surface changes between regional and base sensitivity\n",
        "- Regional parameters that cause large changes should be prioritized in calibration\n",
        "- If a land cover has low sensitivity in a particular region, it may be acceptable to use default values\n",
        "\n",
        "**Application to Your Models:**\n",
        "- Identify which regions have the strongest influence on your points of interest\n",
        "- Focus calibration efforts on high-sensitivity regions\n",
        "- Consider simplifying regions with low sensitivity\n",
        "\n",
        "---\n",
        "\n",
        "### Results from BaldEagleCrkMulti2D, Plan 03 (Main Channel Region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary and Best Practices\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "### Analysis Methodology\n",
        "\n",
        "This notebook demonstrated two complementary approaches to Manning's n sensitivity analysis:\n",
        "\n",
        "1. **Base Override Sensitivity**: Tests global land cover roughness parameters across the entire 2D mesh\n",
        "2. **Regional Override Sensitivity**: Tests localized roughness parameters within specific calibration regions\n",
        "\n",
        "Both approaches use literature-based ranges to ensure physically reasonable parameter variations and automatically generate comprehensive visualizations.\n",
        "\n",
        "### Practical Applications\n",
        "\n",
        "**When to Use Base Sensitivity:**\n",
        "- Initial model exploration and parameter importance ranking\n",
        "- Models with uniform land cover distributions\n",
        "- Assessing global calibration uncertainty\n",
        "- Identifying which land covers dominate model response\n",
        "\n",
        "**When to Use Regional Sensitivity:**\n",
        "- Main channel calibration (use bulk approach from notebook 105)\n",
        "- Localized parameter testing in specific hydraulic zones\n",
        "- Floodplain region calibration with distinct characteristics\n",
        "- Focused calibration in areas of interest\n",
        "\n",
        "### Calibration Workflow Recommendations\n",
        "\n",
        "1. **Start with base sensitivity** to identify globally important land covers\n",
        "2. **Rank parameters by sensitivity** - focus on land covers causing largest water surface changes\n",
        "3. **Use regional sensitivity** for localized refinement in hydraulically important areas\n",
        "4. **Iterate with observed data** - compare sensitivity results against measured water surfaces\n",
        "5. **Document assumptions** - record which parameters were adjusted and why\n",
        "\n",
        "### Computational Efficiency Tips\n",
        "\n",
        "**Plan Count Management:**\n",
        "- Use larger `interval` values (e.g., 0.02-0.05) to reduce plan count\n",
        "- Increase `area_threshold` to focus only on dominant land covers\n",
        "- Target specific regions rather than analyzing all regions simultaneously\n",
        "- HEC-RAS limits projects to 99 plans maximum\n",
        "\n",
        "**Parallel Execution:**\n",
        "- Balance `max_workers` and `num_cores` based on available CPU and RAM\n",
        "- Rule of thumb: `max_workers * num_cores \u2264 total_logical_cores`\n",
        "- Monitor system resources during execution\n",
        "- Consider sequential execution for very large or memory-intensive models\n",
        "\n",
        "### Output Files\n",
        "\n",
        "Each analysis creates a structured output folder containing:\n",
        "- `scenarios.csv`: Complete plan inventory with parameter values\n",
        "- `max_water_surface_summary.csv`: Peak water surface elevations for all scenarios\n",
        "- `timeseries_*.csv`: Complete hydrographs for each scenario\n",
        "- `sensitivity_*.png`: Parameter sensitivity plots\n",
        "- `timeseries_*.png`: Time series comparison plots\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Compare with observed data**: Use sensitivity plots to guide calibration toward measured values\n",
        "- **Uncertainty quantification**: Analyze the range of results to understand prediction uncertainty\n",
        "- **Spatial analysis**: Extract results at multiple points to understand spatial variability\n",
        "- **Multi-objective calibration**: Consider multiple performance metrics beyond peak water surface\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- **Notebook 105**: Bulk Manning's n sensitivity analysis (recommended for main channel calibration)\n",
        "- **Notebook 09**: Plan parameter operations for additional customization\n",
        "- **Notebook 08**: Parallel execution techniques for large-scale analyses\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "**Manning's n Value Resources:**\n",
        "- Chow, V.T. (1959). Open-Channel Hydraulics. McGraw-Hill\n",
        "- Arcement, G.J., & Schneider, V.R. (1989). Guide for Selecting Manning's Roughness Coefficients. USGS Water Supply Paper 2339\n",
        "- HEC-RAS Hydraulic Reference Manual (current version)\n",
        "\n",
        "**Sensitivity Analysis:**\n",
        "- Saltelli, A., et al. (2008). Global Sensitivity Analysis: The Primer. Wiley\n",
        "- Tate, E., et al. (2015). Uncertainty Analysis for Flood Risk Assessment. Natural Hazards\n",
        "\n",
        "---\n",
        "\n",
        "*End of Notebook*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\10_1d_hdf_data_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS 1D HDF Data Analysis Notebook\n",
        "\n",
        "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "!pip install --upgrade ras-commander\n",
        "# This installs ras-commander and all dependencies\n",
        "\n",
        "# Set to false to disable plot generation for llm-friendly outputs\n",
        "generate_plots = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "\n",
        "# Set pandas display options to show only 7 rows by default\n",
        "pd.set_option('display.max_rows', 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Example Project or Load Your Own Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the Balde Eagle Creek 1D Example project from HEC and run plan 01\n",
        "\n",
        "# Define the path to the 1D Balde Eagle Creek project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"Balde Eagle Creek\"\n",
        "import logging\n",
        "\n",
        "# Check if BaldEagle.p01.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = bald_eagle_path / \"BaldEagle.p01.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "    RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "\n",
        "    # Initialize the RAS project using the custom ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    logging.info(f\"Balde Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Balde Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"01\"\n",
        "\n",
        "    # Execute Plan 01 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
        "    if success_bald_eagle:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"BaldEagle.p01.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the custom ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    plan_number = \"01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Plan DataFrame for the project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nGeometry DataFrame for the project:\")\n",
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUnsteady DataFrame for the project:\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBoundary Conditions DataFrame for the project:\")\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get HDF Results Entries (only present when results are present)\n",
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the geometry HDF path\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
        "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS-Commander's Decorators Allow for Flexible Function Calling\n",
        "You can call most of the functions in the HDF* Classes using any of the following:\n",
        "1. Plan/Geometry Number (with or without leading zeros):\n",
        "   - \"01\", \"1\" - Plan/geometry number as string\n",
        "   - 1 - Plan/geometry number as integer\n",
        "   - \"p01\", \"p1\" - Plan number with 'p' prefix\n",
        "2. Direct File Paths:\n",
        "   - pathlib.Path object pointing to HDF file\n",
        "   - String path to HDF file\n",
        "\n",
        "3. h5py.File Objects:\n",
        "   - Already opened HDF file object\n",
        "\n",
        "The @standardize_input decorator handles all these input types consistently:\n",
        "   - Validates the input exists and is accessible\n",
        "   - Converts to proper pathlib.Path object\n",
        "   - Handles RAS object references\n",
        "   - Provides logging and error handling\n",
        "\n",
        "This flexibility makes it easier to work with HDF files in different contexts while maintaining consistent behavior \n",
        "across the codebase, and helps prevent strict typing from introducing unnecessary friction for LLM Coding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1D HDF Data Extraction Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract runtime and compute time data as dataframe\n",
        "print(\"\\nExtracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "# This returns a string with the projection as EPSG code (e.g. \"EPSG:6556\"), or None if not found.\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)  \n",
        "# This projection is returned as EPSG to improve compatibility with geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projection\n",
        "### The example project we are using does not have a projection  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfPlan to Get Geometry Information (Base Geometry Attributes) as dataframes\n",
        "print(\"\\nExtracting Base Geometry Attributes\")\n",
        "geom_attrs_df = HdfPlan.get_geometry_information(\"01\")  \n",
        "# NOTE: Here we call the function using the plan number instead of the hdf path to demonstrate that the decorator will work with the plan number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_attrs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get geometry structures attributes as dataframe\n",
        "print(\"\\nGetting geometry structures attributes\")\n",
        "geom_structures_attrs_df = HdfStruc.get_geom_structures_attrs(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_structures_attrs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instead of hdf_input, USE plan_hdf_path or geom_hdf_path, or the plan number as \"8\" or \"08\" \n",
        "# Input decorators allow for flexible inputs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get structures as geodataframe\n",
        "structures_gdf = HdfStruc.get_structures(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "structures_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reference lines as geodataframe\n",
        "ref_lines_gdf = HdfBndry.get_reference_lines(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_lines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reference points as geodataframe\n",
        "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_points_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross sections as geodataframe\n",
        "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_sections_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show all columns for the first cross section (transpose for readability)\n",
        "import pandas as pd\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(cross_sections_gdf.head(1).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all columns in cross_sections_gdf\n",
        "\n",
        "print(\"Columns in cross_sections_gdf:\")\n",
        "for col in cross_sections_gdf.columns:\n",
        "    print(col)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Showing only cross sections with ineffective flow areas\n",
        "\n",
        "# Filter rows where ineffective_blocks is not empty\n",
        "ineffective_xs_gdf = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(len) > 0]\n",
        "print(\"\\nCross Sections with Ineffective Flow Areas:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ineffective_xs_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print first 5 cross sections data\n",
        "print(\"\\nCross Section Information:\")\n",
        "\n",
        "for idx, row in cross_sections_gdf.head(5).iterrows():\n",
        "    print(f\"\\nCross Section {idx + 1}:\")\n",
        "    print(f\"River: {row['River']}\")\n",
        "    print(f\"Reach: {row['Reach']}\")\n",
        "    print(\"\\nGeometry:\")\n",
        "    print(row['geometry'])\n",
        "    print(\"\\nStation-Elevation Points:\")\n",
        "    \n",
        "    # Print header\n",
        "    print(\"     #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation\")\n",
        "    print(\"-\" * 150)\n",
        "    \n",
        "    # Calculate number of rows needed\n",
        "    points = row['station_elevation']\n",
        "    num_rows = (len(points) + 4) // 5  # Round up division\n",
        "    \n",
        "    # Print points in 5 columns\n",
        "    for i in range(num_rows):\n",
        "        line = \"\"\n",
        "        for j in range(5):\n",
        "            point_idx = i + j * num_rows\n",
        "            if point_idx < len(points):\n",
        "                station, elevation = points[point_idx]\n",
        "                line += f\"{point_idx+1:6d} {station:10.2f} {elevation:10.2f}    \"\n",
        "        print(line)\n",
        "    print(\"-\" * 150)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross sections on map with matplotlib\n",
        "\n",
        "if generate_plots:\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(15,10))\n",
        "    \n",
        "    # Plot cross sections\n",
        "    cross_sections_gdf.plot(ax=ax, color='red', linewidth=1, label='Cross Sections')\n",
        "    \n",
        "    # Add river name and reach labels\n",
        "    #for idx, row in cross_sections_gdf.iterrows():\n",
        "    #    # Get midpoint of cross section line for label placement\n",
        "    #    midpoint = row.geometry.centroid\n",
        "    #    label = f\"{row['River']}\\n{row['Reach']}\\nRS: {row['RS']}\"\n",
        "    #    ax.annotate(label, (midpoint.x, midpoint.y), \n",
        "    #               xytext=(5, 5), textcoords='offset points',\n",
        "    #               fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_title('Cross Sections Location Map')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    \n",
        "    # Equal aspect ratio to preserve shape\n",
        "    ax.set_aspect('equal')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross sections with Manning's n values colored by value\n",
        "\n",
        "if generate_plots:\n",
        "    # Create figure\n",
        "    fig, ax1 = plt.subplots(figsize=(20,10))\n",
        "\n",
        "    # Create colormap\n",
        "    cmap = plt.cm.viridis\n",
        "    norm = plt.Normalize(vmin=0.02, vmax=0.08)  # Typical Manning's n range\n",
        "\n",
        "    # Plot cross sections colored by Manning's n\n",
        "    for idx, row in cross_sections_gdf.iterrows():\n",
        "        # Extract Manning's n values and stations\n",
        "        mannings = row['mannings_n']\n",
        "        n_values = mannings['Mann n']\n",
        "        stations = mannings['Station']\n",
        "        \n",
        "        # Get the full linestring coordinates\n",
        "        line_coords = list(row.geometry.coords)\n",
        "        \n",
        "        # Calculate total length of the cross section\n",
        "        total_length = row.geometry.length\n",
        "        \n",
        "        # For each Manning's n segment\n",
        "        for i in range(len(n_values)-1):\n",
        "            # Calculate the start and end proportions along the line\n",
        "            start_prop = stations[i] / stations[-1]\n",
        "            end_prop = stations[i+1] / stations[-1]\n",
        "            \n",
        "            # Get the start and end points for this segment\n",
        "            start_idx = int(start_prop * (len(line_coords)-1))\n",
        "            end_idx = int(end_prop * (len(line_coords)-1))\n",
        "            \n",
        "            # Extract the segment coordinates\n",
        "            segment_coords = line_coords[start_idx:end_idx+1]\n",
        "            \n",
        "            if len(segment_coords) >= 2:\n",
        "                # Create a line segment\n",
        "                segment = LineString(segment_coords)\n",
        "                \n",
        "                # Get color from colormap for this n value\n",
        "                color = cmap(norm(n_values[i]))\n",
        "                \n",
        "                # Plot the segment\n",
        "                ax1.plot(*segment.xy, color=color, linewidth=2)\n",
        "\n",
        "    # Add colorbar\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    plt.colorbar(sm, ax=ax1, label=\"Manning's n Value\")\n",
        "\n",
        "    ax1.set_title(\"Cross Sections Colored by Manning's n Values\")\n",
        "    ax1.grid(True)\n",
        "    ax1.set_aspect('equal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross sections with ineffective flow areas\n",
        "\n",
        "if generate_plots:\n",
        "    # Create figure\n",
        "    fig, ax2 = plt.subplots(figsize=(20,10))\n",
        "\n",
        "    # Plot all cross sections first\n",
        "    cross_sections_gdf.plot(ax=ax2, color='lightgray', linewidth=1, label='Cross Sections')\n",
        "\n",
        "    # Plot ineffective flow areas with thicker lines\n",
        "    ineffective_sections = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(lambda x: len(x) > 0)]\n",
        "    ineffective_sections.plot(ax=ax2, color='red', linewidth=3, label='Ineffective Flow Areas')\n",
        "\n",
        "    # Add ineffective flow area labels with offset to lower right\n",
        "    for idx, row in cross_sections_gdf.iterrows():\n",
        "        # Get midpoint of cross section line\n",
        "        midpoint = row.geometry.centroid\n",
        "        \n",
        "        # Extract ineffective flow blocks\n",
        "        ineff_blocks = row['ineffective_blocks']\n",
        "        \n",
        "        if ineff_blocks:  # Only label if there are ineffective blocks\n",
        "            label_parts = []\n",
        "            # Add RS to first line of label\n",
        "            label_parts.append(f\"RS: {row['RS']}\")\n",
        "            for block in ineff_blocks:\n",
        "                label_parts.append(\n",
        "                    f\"L:{block['Left Sta']:.0f}-R:{block['Right Sta']:.0f}\\n\"\n",
        "                    f\"Elev: {block['Elevation']:.2f}\\n\"\n",
        "                    f\"Permanent: {block['Permanent']}\"\n",
        "                )\n",
        "            \n",
        "            label = '\\n'.join(label_parts)\n",
        "            \n",
        "            ax2.annotate(label, (midpoint.x, midpoint.y),\n",
        "                        xytext=(15, -15),  # Offset to lower right\n",
        "                        textcoords='offset points',\n",
        "                        fontsize=8, \n",
        "                        bbox=dict(facecolor='white', alpha=0.7),\n",
        "                        arrowprops=dict(arrowstyle='->'),\n",
        "                        horizontalalignment='left',\n",
        "                        verticalalignment='top')\n",
        "\n",
        "    ax2.set_title('Cross Sections with Ineffective Flow Areas')\n",
        "    ax2.grid(True)\n",
        "    ax2.legend()\n",
        "    ax2.set_aspect('equal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross section elevation for cross section 42\n",
        "if generate_plots:\n",
        "    # Get cross sections data\n",
        "    cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
        "\n",
        "    if not cross_sections_gdf.empty:\n",
        "        # Get station-elevation data for cross section 42\n",
        "        station_elevation = cross_sections_gdf.iloc[42]['station_elevation']\n",
        "        \n",
        "        # Convert list of lists to numpy arrays for plotting\n",
        "        stations = np.array([point[0] for point in station_elevation])\n",
        "        elevations = np.array([point[1] for point in station_elevation])\n",
        "        \n",
        "        # Create figure and axis\n",
        "        fig, ax = plt.subplots(figsize=(12,8))\n",
        "        \n",
        "        # Plot cross section\n",
        "        ax.plot(stations, elevations, 'b-', linewidth=2)\n",
        "        \n",
        "        # Add labels and title\n",
        "        river = cross_sections_gdf.iloc[42]['River']\n",
        "        reach = cross_sections_gdf.iloc[42]['Reach'] \n",
        "        rs = cross_sections_gdf.iloc[42]['RS']\n",
        "        \n",
        "        # Show bank stations as dots\n",
        "        left_bank_station = cross_sections_gdf.iloc[42]['Left Bank']\n",
        "        right_bank_station = cross_sections_gdf.iloc[42]['Right Bank']\n",
        "        \n",
        "        # Get elevations at bank stations\n",
        "        left_bank_elev = elevations[np.searchsorted(stations, left_bank_station)]\n",
        "        right_bank_elev = elevations[np.searchsorted(stations, right_bank_station)]\n",
        "        \n",
        "        # Plot bank stations with dots\n",
        "        ax.plot(left_bank_station, left_bank_elev, 'ro')\n",
        "        ax.plot(right_bank_station, right_bank_elev, 'ro')\n",
        "        \n",
        "        # Add bank station labels with station and elevation\n",
        "        ax.annotate(f'Left Bank\\nStation: {left_bank_station:.1f}\\nElevation: {left_bank_elev:.1f}',\n",
        "                   (left_bank_station, left_bank_elev),\n",
        "                   xytext=(-50, 30),\n",
        "                   textcoords='offset points',\n",
        "                   bbox=dict(facecolor='white', alpha=0.8),\n",
        "                   arrowprops=dict(arrowstyle='->'))\n",
        "                   \n",
        "        ax.annotate(f'Right Bank\\nStation: {right_bank_station:.1f}\\nElevation: {right_bank_elev:.1f}',\n",
        "                   (right_bank_station, right_bank_elev), \n",
        "                   xytext=(50, 30),\n",
        "                   textcoords='offset points',\n",
        "                   bbox=dict(facecolor='white', alpha=0.8),\n",
        "                   arrowprops=dict(arrowstyle='->'))\n",
        "        \n",
        "        ax.set_title(f'Cross Section Profile\\nRiver: {river}, Reach: {reach}, RS: {rs}')\n",
        "        ax.set_xlabel('Station (ft)')\n",
        "        ax.set_ylabel('Elevation (ft)')\n",
        "        \n",
        "        # Add grid\n",
        "        ax.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get river centerlines as geodataframe\n",
        "centerlines_gdf = HdfXsec.get_river_centerlines(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRiver Centerlines:\")\n",
        "centerlines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot river centerlines with labels\n",
        "if generate_plots:\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    # Plot centerlines\n",
        "    centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerline')\n",
        "\n",
        "    # Add river/reach labels\n",
        "    for idx, row in centerlines_gdf.iterrows():\n",
        "        # Get midpoint of the line for label placement\n",
        "        midpoint = row.geometry.interpolate(0.5, normalized=True)\n",
        "        \n",
        "        # Create label text combining river and reach names\n",
        "        label = f\"{row['River Name']}\\n{row['Reach Name']}\"\n",
        "        \n",
        "        # Add text annotation\n",
        "        ax.annotate(label, \n",
        "                    xy=(midpoint.x, midpoint.y),\n",
        "                    xytext=(10, 10), # Offset text slightly\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_title('River Centerlines', fontsize=14)\n",
        "    ax.set_xlabel('Easting', fontsize=12)\n",
        "    ax.set_ylabel('Northing', fontsize=12)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get river edge lines as geodataframe\n",
        "edge_lines_gdf = HdfXsec.get_river_edge_lines(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRiver Edge Lines:\")\n",
        "edge_lines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get bank lines as geodataframe\n",
        "bank_lines_gdf = HdfXsec.get_river_bank_lines(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRiver Bank Lines:\")\n",
        "bank_lines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create figure and axis\n",
        "\n",
        "if generate_plots:\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    # Plot river edge lines\n",
        "    edge_lines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Edge Lines')\n",
        "\n",
        "    # Plot centerlines for reference\n",
        "    centerlines_gdf.plot(ax=ax, color='red', linewidth=2, linestyle='--', label='River Centerline')\n",
        "\n",
        "    # Plot river bank lines\n",
        "    bank_lines_gdf.plot(ax=ax, color='green', linewidth=2, label='River Bank Lines')\n",
        "\n",
        "    # Add title and labels\n",
        "    ax.set_title('River Edge Lines, Centerline, and Bank Lines', fontsize=14)\n",
        "    ax.set_xlabel('Easting', fontsize=12)\n",
        "    ax.set_ylabel('Northing', fontsize=12)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract 1D Structures Geodataframe\n",
        "\n",
        "\n",
        "\n",
        "# Display basic information about the structures\n",
        "print(\"\\nStructures Summary:\")\n",
        "print(f\"Number of structures found: {len(structures_gdf)}\")\n",
        "structures_gdf\n",
        "\n",
        "# Display first few rows of key attributes\n",
        "print(\"\\nStructure Details:\")\n",
        "display_cols = ['Structure ID', 'Structure Type', 'River Name', 'Reach Name', 'Station']\n",
        "display_cols = [col for col in display_cols if col in structures_gdf.columns]\n",
        "if display_cols:\n",
        "    print(structures_gdf[display_cols].head())\n",
        "\n",
        "\n",
        "if generate_plots:\n",
        "\n",
        "    # Create visualization\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    # Plot river centerlines\n",
        "    if not centerlines_gdf.empty:\n",
        "        centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerlines')\n",
        "\n",
        "    # Plot cross sections\n",
        "    if not cross_sections_gdf.empty:\n",
        "        cross_sections_gdf.plot(ax=ax, color='green', linewidth=1, label='Cross Sections')\n",
        "\n",
        "    # Plot structures\n",
        "    if not structures_gdf.empty:\n",
        "        structures_gdf.plot(ax=ax, color='red', marker='s', markersize=100, label='Structures')\n",
        "\n",
        "    # Add title and labels\n",
        "    ax.set_title('HEC-RAS Model Components', fontsize=14)\n",
        "    ax.set_xlabel('Easting', fontsize=12)\n",
        "    ax.set_ylabel('Northing', fontsize=12)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "# Print summary of cross sections\n",
        "print(\"\\nCross Sections Summary:\")\n",
        "print(f\"Number of cross sections found: {len(cross_sections_gdf)}\")\n",
        "if not cross_sections_gdf.empty:\n",
        "    print(\"\\nCross Section Details:\")\n",
        "    xs_display_cols = ['River', 'Reach', 'Station']\n",
        "    xs_display_cols = [col for col in xs_display_cols if col in cross_sections_gdf.columns]\n",
        "    if xs_display_cols:\n",
        "        print(cross_sections_gdf[xs_display_cols].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Plan Parameters\n",
        "print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n",
        "\n",
        "plan_parameters_df = HdfPlan.get_plan_parameters(hdf_path=plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nPlan Parameters DataFrame:\")\n",
        "plan_parameters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract volume accounting data\n",
        "volume_accounting_df = HdfResultsPlan.get_volume_accounting(hdf_path=plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nVolume Accounting DataFrame:\")\n",
        "volume_accounting_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get simulation start time\n",
        "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
        "print(f\"Simulation start time: {start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get plan end time\n",
        "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
        "print(f\"Simulation end time: {end_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries\n",
        "xsec_results_xr = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n",
        "print(\"\\nCross Section Results Shape:\", xsec_results_xr['Water_Surface'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the time of maximum water surface elevation (WSEL) for cross sections\n",
        "\n",
        "\n",
        "\n",
        "# Get cross section geometry data\n",
        "xsec_geom = HdfXsec.get_cross_sections(plan_hdf_path)\n",
        "print(\"\\nNumber of cross sections in geometry:\", len(xsec_geom))\n",
        "\n",
        "# Create dataframe with cross section locations and max WSEL times\n",
        "xs_data = []\n",
        "\n",
        "# Extract water surface data from xarray Dataset\n",
        "water_surface = xsec_results_xr['Water_Surface'].values\n",
        "times = pd.to_datetime(xsec_results_xr.time.values)\n",
        "\n",
        "# Debug print\n",
        "print(\"\\nFirst few cross section names:\")\n",
        "print(xsec_results_xr.cross_section.values[:5])\n",
        "\n",
        "# Iterate through cross sections\n",
        "for xs_idx in range(len(xsec_results_xr.cross_section)):\n",
        "    # Get WSEL timeseries for this cross section\n",
        "    wsel_series = water_surface[:, xs_idx]\n",
        "    \n",
        "    # Get cross section name and parse components\n",
        "    xs_name = xsec_results_xr.cross_section.values[xs_idx]\n",
        "    \n",
        "    # Split the string and remove empty strings\n",
        "    xs_parts = [part for part in xs_name.split() if part]\n",
        "    \n",
        "    if len(xs_parts) >= 3:\n",
        "        river = \"Bald Eagle\"  # Combine first two words\n",
        "        reach = \"Loc Hav\"     # Next two words\n",
        "        rs = xs_parts[-1]     # Last part is the station\n",
        "        \n",
        "        # Get geometry for this cross section\n",
        "        xs_match = xsec_geom[\n",
        "            (xsec_geom['River'] == river) & \n",
        "            (xsec_geom['Reach'] == reach) & \n",
        "            (xsec_geom['RS'] == rs)\n",
        "        ]\n",
        "        \n",
        "        if not xs_match.empty:\n",
        "            geom = xs_match.iloc[0]\n",
        "            # Use first point of cross section line for plotting\n",
        "            x = geom.geometry.coords[0][0]\n",
        "            y = geom.geometry.coords[0][1]\n",
        "            \n",
        "            # Find time of max WSEL\n",
        "            max_wsel_idx = np.argmax(wsel_series)\n",
        "            max_wsel = np.max(wsel_series)\n",
        "            max_time = times[max_wsel_idx]\n",
        "            \n",
        "            xs_data.append({\n",
        "                'xs_name': xs_name,\n",
        "                'x': x,\n",
        "                'y': y,\n",
        "                'max_wsel': max_wsel,\n",
        "                'time_of_max': max_time\n",
        "            })\n",
        "        else:\n",
        "            print(f\"\\nWarning: No geometry match found for {xs_name}\")\n",
        "            print(f\"River: {river}, Reach: {reach}, RS: {rs}\")\n",
        "    else:\n",
        "        print(f\"\\nWarning: Could not parse cross section name: {xs_name}\")\n",
        "\n",
        "# Create dataframe\n",
        "xs_df = pd.DataFrame(xs_data)\n",
        "\n",
        "# Debug print\n",
        "print(\"\\nNumber of cross sections processed:\", len(xs_df))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if generate_plots:\n",
        "    print(\"\\nColumns in xs_df:\", xs_df.columns.tolist())\n",
        "    print(\"\\nFirst row of xs_df:\")\n",
        "    print(xs_df.iloc[0])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Convert datetime to hours since start for colormap\n",
        "    min_time = min(xs_df['time_of_max'])\n",
        "    color_values = [(t - min_time).total_seconds() / 3600 for t in xs_df['time_of_max']]\n",
        "\n",
        "    # Plot cross section points\n",
        "    scatter = ax.scatter(xs_df['x'], xs_df['y'],\n",
        "                        c=color_values,\n",
        "                        cmap='viridis',\n",
        "                        s=50)\n",
        "\n",
        "    # Customize plot\n",
        "    ax.set_title('Time of Maximum Water Surface Elevation at Cross Sections')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter)\n",
        "    cbar.set_label('Hours since simulation start')\n",
        "\n",
        "    # Format colorbar ticks\n",
        "    max_hours = int(max(color_values))\n",
        "    tick_interval = max(1, max_hours // 6)  # Show ~6 ticks\n",
        "    cbar.set_ticks(range(0, max_hours + 1, tick_interval))\n",
        "    cbar.set_ticklabels([f'{h}h' for h in range(0, max_hours + 1, tick_interval)])\n",
        "\n",
        "    # Add grid and adjust styling\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    max_wsel_xs = xs_df.loc[xs_df['max_wsel'].idxmax()]\n",
        "    hours_since_start = (max_wsel_xs['time_of_max'] - min_time).total_seconds() / 3600\n",
        "\n",
        "    print(f\"\\nOverall Maximum WSEL: {max_wsel_xs['max_wsel']:.2f} ft\")\n",
        "    print(f\"Time of Overall Maximum WSEL: {max_wsel_xs['time_of_max']}\")\n",
        "    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
        "    print(f\"Location of Overall Maximum WSEL: X={max_wsel_xs['x']:.2f}, Y={max_wsel_xs['y']:.2f}\")\n",
        "    print(f\"Cross Section: {max_wsel_xs['xs_name']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unsteady attributes as dataframe\n",
        "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unsteady summary attributes as dataframe\n",
        "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_summary_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1D Cross Section Results as Xarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries as xarray dataset\n",
        "xsec_results_xr = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print time series for specific cross section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
        "\n",
        "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
        "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
        "    print(f\"\\n{var}:\")\n",
        "    print(xsec_results_xr[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "\n",
        "# Create time series plots\n",
        "\n",
        "if generate_plots:\n",
        "\n",
        "    # Create a figure for each variable\n",
        "    variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
        "\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        # Convert time values to datetime if needed\n",
        "        time_values = pd.to_datetime(xsec_results_xr.time.values)\n",
        "        values = xsec_results_xr[var].sel(cross_section=target_xs).values\n",
        "        \n",
        "        # Plot with explicit x and y values\n",
        "        plt.plot(time_values, values, '-', linewidth=2)\n",
        "        \n",
        "        plt.title(f'{var} at {target_xs}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(var.replace('_', ' '))\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Force display\n",
        "        plt.draw()\n",
        "        plt.pause(0.1)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced HDF Data Extraction\n",
        "This section focuses on directly accessing the HDF file from a jupyter notebook for use cases not directly supported by the RAS-Commander libary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Compute Messages using HdfResultsPlan\n",
        "print(\"Extracting Compute Messages\")\n",
        "\n",
        "# Use the built-in function to extract computation messages from HDF\n",
        "# This function automatically handles HDF extraction with fallback to .txt files if needed\n",
        "compute_msgs = HdfResultsPlan.get_compute_messages(plan_number)\n",
        "\n",
        "if compute_msgs:\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPUTATION MESSAGES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Parse and display the messages in a readable format\n",
        "    messages = compute_msgs.split('\\r\\n')\n",
        "    \n",
        "    # Display all messages with formatting\n",
        "    for message in messages:\n",
        "        if message.strip():  # Skip empty lines\n",
        "            if ':' in message:\n",
        "                # Format key-value pairs\n",
        "                parts = message.split(':', 1)\n",
        "                if len(parts) == 2:\n",
        "                    key, value = parts\n",
        "                    print(f\"{key.strip():40} : {value.strip()}\")\n",
        "                else:\n",
        "                    print(message)\n",
        "            else:\n",
        "                print(f\"\\n{message.strip()}\")\n",
        "    \n",
        "    # Display computation summary table (if present)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPUTATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    summary_lines = [line for line in messages if 'Computation Task' in line or 'Computation Speed' in line]\n",
        "    if summary_lines:\n",
        "        for line in summary_lines:\n",
        "            print(line)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Total message length: {len(compute_msgs)} characters\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"No computation messages available for this plan.\")\n",
        "    print(\"\\nNote: Computation messages are generated when HEC-RAS runs a plan.\")\n",
        "    print(\"If no messages are found, the plan may not have been computed yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring HDF Datasets with HdfBase.get_dataset_info\n",
        "This allows users to find HDF information that is not included in the ras-commander library.  Find the path in HDFView and set the group_path below to explore the HDF datasets and attributes.  Then, use the output to write your own function to extract the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get HDF Paths with Properties (For Exploring HDF Files)\n",
        "HdfBase.get_dataset_info(plan_number, group_path=\"/Geometry\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get dataset structure:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/River Bank Lines/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Structures\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Output/Output Blocks/Computation Block/Global/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use the get_hdf5_dataset_info function from HdfUtils to explore the Cross Sections structure in the geometry HDF file\n",
        "\n",
        "print(\"\\nExploring Cross Sections structure in geometry file:\")\n",
        "print(\"HDF Base Path: /Geometry/Cross Sections \")\n",
        "HdfBase.get_dataset_info(geom_hdf_path, group_path='/Geometry/Cross Sections')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\n=== HDF5 File Structure ===\\n\")\n",
        "print(plan_hdf_path)\n",
        "HdfBase.get_dataset_info(plan_hdf_path, group_path='/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For HDF datasets that are not supported by the RAS-Commadner library, provide the dataset path to HdfBase.get_dataset_info and provide the output to an LLM along with a relevent HDF* class(es) to generate new functions that extend the library's coverage.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\11_2d_hdf_data_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS 2D HDF Data Analysis Notebook\n",
        "\n",
        "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies\n",
        "\n",
        "# Use this setting to disable plot generation within the notebook\n",
        "generate_plots = True\n",
        "# Use this setting to disable map generation within the notebook\n",
        "generate_maps = True\n",
        "# Set both to false for llm-friendly outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "import logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will try to import the pip package, if it fails it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "# This allows a user's revisions to be tested locally without installing the package\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Flexible imports to allow for development without installation \n",
        "#  ** Use this version with Jupyter Notebooks **\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    import os\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "print(\"ras_commander imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Example Project or Load Your Own Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To Use the HEC Example Project:\n",
        "# Download the BaldEagleCrkMulti2D project from HEC and Run Plan 06\n",
        "\n",
        "# Define the path to the BaldEagleCrkMulti2D project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "the_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "import logging\n",
        "\n",
        "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = the_path / \"BaldEagleDamBrk.p06.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(the_path, \"6.6\")\n",
        "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"06\"\n",
        "\n",
        "    # Update run flags for the project\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,\n",
        "        unsteady_flow_simulation=True,\n",
        "        run_sediment=False,\n",
        "        post_processor=True,\n",
        "        floodplain_mapping=False\n",
        "    )\n",
        "\n",
        "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_the = RasCmdr.compute_plan(plan_number)\n",
        "    if success_the:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"Project already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(the_path, \"6.6\")\n",
        "    plan_number = \"06\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting 2D Computation Messages\n",
        "\n",
        "For 2D models, computation messages are especially important for understanding:\n",
        "- 2D mesh stability and timestep information\n",
        "- Wet/dry cell transitions\n",
        "- Volume accounting and mass balance\n",
        "- Performance metrics for large 2D meshes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract computation messages for 2D analysis\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"2D MODEL COMPUTATION MESSAGES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "msgs_2d = HdfResultsPlan.get_compute_messages(plan_number)\n",
        "\n",
        "if msgs_2d:\n",
        "    print(f\"\\nExtracted {len(msgs_2d)} characters of computation messages\\n\")\n",
        "    \n",
        "    # Display first portion\n",
        "    print(\"Computation messages (first 1000 characters):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(msgs_2d[:1000])\n",
        "    \n",
        "    if len(msgs_2d) > 1000:\n",
        "        print(\"\\n... (truncated for display) ...\")\n",
        "    \n",
        "    # Check for 2D-specific information\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Looking for 2D-specific information...\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    lines = msgs_2d.split('\\n')\n",
        "    keywords = ['2d', 'mesh', 'timestep', 'volume', 'courant']\n",
        "    relevant = [l for l in lines if any(kw in l.lower() for kw in keywords)]\n",
        "    \n",
        "    if relevant:\n",
        "        print(f\"Found {len(relevant)} lines with 2D-related information:\")\n",
        "        for line in relevant[:10]:\n",
        "            print(f\"  - {line.strip()}\")\n",
        "    else:\n",
        "        print(\"No specific 2D keywords found in messages\")\n",
        "else:\n",
        "    print(\"No computation messages available for this 2D plan\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show ras object info\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate: Get the geometry HDF path if you are extracting geometry elements from the geometry HDF\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS-Commander's Decorators Allow for Flexible Function Calling\n",
        "You can call most of the functions in the HDF* Classes using any of the following:\n",
        "1. Plan/Geometry Number (with or without leading zeros):\n",
        "   - \"01\", \"1\" - Plan/geometry number as string\n",
        "   - 1 - Plan/geometry number as integer\n",
        "   - \"p01\", \"p1\" - Plan number with 'p' prefix\n",
        "2. Direct File Paths:\n",
        "   - pathlib.Path object pointing to HDF file\n",
        "   - String path to HDF file\n",
        "\n",
        "3. h5py.File Objects:\n",
        "   - Already opened HDF file object\n",
        "\n",
        "The @standardize_input decorator handles all these input types consistently:\n",
        "   - Validates the input exists and is accessible\n",
        "   - Converts to proper pathlib.Path object\n",
        "   - Handles RAS object references\n",
        "   - Provides logging and error handling\n",
        "\n",
        "This flexibility makes it easier to work with HDF files in different contexts while maintaining consistent behavior \n",
        "across the codebase, and helps prevent strict typing from introducing unnecessary friction for LLM Coding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2D HDF Data Extraction Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract runtime and compute time data as dataframe\n",
        "print(\"\\nExtracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
        "print(geom_hdf_path)\n",
        "\n",
        "# For the example project, plan 06 is associated with geometry 09\n",
        "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
        "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfPlan for geometry-related operations\n",
        "print(\"\\nExtracting Geometry Information\")\n",
        "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfMesh for geometry-related operations\n",
        "print(\"\\nListing 2D Flow Area Names\")\n",
        "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"2D Flow Area Name (returned as list):\")\n",
        "flow_area_names\n",
        "# Note: this is returned as a list because it is used internally by other functions.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 2D Flow Area Attributes (get_mesh_area_attributes)\n",
        "print(\"\\nExtracting 2D Flow Area Attributes\")\n",
        "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flow_area_attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 2D Flow Area Perimeter Polygons (get_mesh_areas)\n",
        "print(\"\\nExtracting 2D Flow Area Perimeter Polygons\")\n",
        "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Map of Mesh Areas\n",
        "if generate_plots:\n",
        "    # Plot the 2D Flow Area Perimeter Polygons\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none')\n",
        "\n",
        "    # Add labels for each polygon\n",
        "    for idx, row in mesh_areas.iterrows():\n",
        "        centroid = row.geometry.centroid\n",
        "        # Check if 'Name' column exists, otherwise use a default label\n",
        "        label = row.get('Name', f'Area {idx}')\n",
        "        ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
        "\n",
        "    plt.title('2D Flow Area Perimeter Polygons')\n",
        "    plt.xlabel('Easting')\n",
        "    plt.ylabel('Northing')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh cell faces as geodatframe\n",
        "mesh_cell_faces_gdf = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_cell_faces_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.collections import LineCollection\n",
        "import numpy as np\n",
        "\n",
        "# Calculate and display statistics\n",
        "print(\"\\nMesh Cell Faces Statistics:\")\n",
        "print(f\"Total number of cell faces: {len(mesh_cell_faces_gdf)}\")\n",
        "print(f\"Number of unique meshes: {mesh_cell_faces_gdf['mesh_name'].nunique()}\")\n",
        "\n",
        "if generate_maps:\n",
        "    # Plot the mesh cell faces more efficiently\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Convert all geometries to numpy arrays at once for faster plotting\n",
        "    lines = [list(zip(*line.xy)) for line in mesh_cell_faces_gdf.geometry]\n",
        "    lines_collection = LineCollection(lines, colors='blue', linewidth=0.5, alpha=0.5)\n",
        "    ax.add_collection(lines_collection)\n",
        "\n",
        "    # Set plot title and labels\n",
        "    plt.title('Mesh Cell Faces')\n",
        "    plt.xlabel('Easting')\n",
        "    plt.ylabel('Northing')\n",
        "\n",
        "    # Calculate centroids once and store as numpy arrays\n",
        "    centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in mesh_cell_faces_gdf.geometry])\n",
        "\n",
        "    # Create scatter plot with numpy arrays\n",
        "    scatter = ax.scatter(\n",
        "        centroids[:, 0],\n",
        "        centroids[:, 1], \n",
        "        c=mesh_cell_faces_gdf['face_id'],\n",
        "        cmap='viridis',\n",
        "        s=1,\n",
        "        alpha=0.5\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Face ID')\n",
        "\n",
        "    # Set axis limits based on data bounds\n",
        "    ax.set_xlim(centroids[:, 0].min(), centroids[:, 0].max())\n",
        "    ax.set_ylim(centroids[:, 1].min(), centroids[:, 1].max())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to find the nearest cell face to a given point\n",
        "def find_nearest_cell_face(point, cell_faces_df):\n",
        "    \"\"\"\n",
        "    Find the nearest cell face to a given point.\n",
        "\n",
        "    Args:\n",
        "        point (shapely.geometry.Point): The input point.\n",
        "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
        "\n",
        "    Returns:\n",
        "        int: The face_id of the nearest cell face.\n",
        "        float: The distance to the nearest cell face.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the input point to all cell faces\n",
        "    distances = cell_faces_df.geometry.distance(point)\n",
        "\n",
        "    # Find the index of the minimum distance\n",
        "    nearest_index = distances.idxmin()\n",
        "\n",
        "    # Get the face_id and distance of the nearest cell face\n",
        "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
        "    nearest_distance = distances[nearest_index]\n",
        "\n",
        "    return nearest_face_id, nearest_distance\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nFinding the nearest cell face to a given point\")\n",
        "\n",
        "# Create a sample point (you can replace this with any point of interest)\n",
        "from shapely.geometry import Point\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "# Get the centroid of the mesh cell faces\n",
        "print(\"Getting Centroid of 2D Mesh Polygon\")\n",
        "centroid = mesh_cell_faces_gdf.geometry.union_all().centroid\n",
        "\n",
        "# Create GeoDataFrame with the centroid point, using same CRS as mesh_cell_faces_gdf\n",
        "sample_point = GeoDataFrame(\n",
        "    {'geometry': [centroid]}, \n",
        "    crs=mesh_cell_faces_gdf.crs\n",
        ")\n",
        "\n",
        "if not mesh_cell_faces_gdf.empty and not sample_point.empty:\n",
        "    print(\"Searching Cell\")\n",
        "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces_gdf)\n",
        "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
        "    print(f\"Face ID: {nearest_face_id}\")\n",
        "    print(f\"Distance: {distance:.2f} units\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate map of cell faces with sample point and nearest cell face shown\n",
        "if generate_maps:\n",
        "    # Visualize the result\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Plot all cell faces\n",
        "    mesh_cell_faces_gdf.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
        "    \n",
        "    # Plot the sample point\n",
        "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
        "    \n",
        "    # Plot the nearest cell face\n",
        "    nearest_face = mesh_cell_faces_gdf[mesh_cell_faces_gdf['face_id'] == nearest_face_id]\n",
        "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
        "    \n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('Nearest Cell Face to Sample Point')\n",
        "    \n",
        "    # Add legend and grid\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    \n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Cell Polygons\n",
        "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
        "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cell_polygons_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cell polygons\n",
        "\n",
        "if generate_maps:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot cell polygons\n",
        "    cell_polygons_df.plot(ax=ax, edgecolor='blue', facecolor='none')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('2D Flow Area Cell Polygons')\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Cell Info\n",
        "print(\"\\nExample 5: Extracting Cell Info\")\n",
        "cell_info_df = HdfMesh.get_mesh_cell_points(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cell_info_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cell centers\n",
        "\n",
        "if generate_maps:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot cell centers\n",
        "    cell_info_df.plot(ax=ax, color='red', markersize=5)\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('2D Flow Area Cell Centers')\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to find the nearest cell center to a given point\n",
        "def find_nearest_cell(point, cell_centers_df):\n",
        "    \"\"\"\n",
        "    Find the nearest cell center to a given point.\n",
        "\n",
        "    Args:\n",
        "        point (shapely.geometry.Point): The input point.\n",
        "        cell_centers_df (GeoDataFrame): DataFrame containing cell center points.\n",
        "\n",
        "    Returns:\n",
        "        int: The cell_id of the nearest cell.\n",
        "        float: The distance to the nearest cell center.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the input point to all cell centers\n",
        "    distances = cell_centers_df.geometry.distance(point)\n",
        "\n",
        "    # Find the index of the minimum distance\n",
        "    nearest_index = distances.idxmin()\n",
        "\n",
        "    # Get the cell_id and distance of the nearest cell\n",
        "    nearest_cell_id = cell_centers_df.loc[nearest_index, 'cell_id']\n",
        "    nearest_distance = distances[nearest_index]\n",
        "\n",
        "    return nearest_cell_id, nearest_distance\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nFinding the nearest cell to a given point\")\n",
        "\n",
        "# Sample point was created in a previous code cell \n",
        "\n",
        "# Get the projection from the geometry file\n",
        "# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n",
        "if projection:\n",
        "    print(f\"Using projection: {projection}\")\n",
        "else:\n",
        "    print(\"No projection information found. Using default CRS.\")\n",
        "    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n",
        "\n",
        "\n",
        "\n",
        "# Ensure the CRS of the sample point matches the cell_info_df\n",
        "if sample_point.crs != cell_info_df.crs:\n",
        "    sample_point = sample_point.to_crs(cell_info_df.crs)\n",
        "\n",
        "nearest_cell_id, distance = find_nearest_cell(sample_point.geometry.iloc[0], cell_info_df)\n",
        "print(f\"Nearest cell to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
        "print(f\"Cell ID: {nearest_cell_id}\")\n",
        "print(f\"Distance: {distance:.2f} units\")\n",
        "\n",
        "if generate_maps:\n",
        "    # Visualize the result\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot all cell centers\n",
        "    cell_info_df.plot(ax=ax, color='blue', markersize=5, alpha=0.5, label='Cell Centers')\n",
        "\n",
        "    # Plot the sample point\n",
        "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
        "\n",
        "    # Plot the nearest cell center\n",
        "    nearest_cell = cell_info_df[cell_info_df['cell_id'] == nearest_cell_id]\n",
        "    nearest_cell.plot(ax=ax, color='green', markersize=100, alpha=0.7, label='Nearest Cell')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('Nearest Cell to Sample Point')\n",
        "\n",
        "    # Add legend and grid\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get geometry structures attributes\n",
        "print(\"\\nGetting geometry structures attributes as Dataframe\")\n",
        "geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_structures_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Paths and Functions for each type of structure: \n",
        "\n",
        "# Getting geometry structures attributes\n",
        "# Geometry structures attributes:\n",
        "# Bridge/Culvert Count: 0\n",
        "# Connection Count: 4\n",
        "# Has Bridge Opening (2D): 0\n",
        "# Inline Structure Count: 0\n",
        "# Lateral Structure Count: 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get boundary condition lines\n",
        "print(\"\\nExtracting Boundary Condition Lines as Geodataframe\")\n",
        "bc_lines_df = HdfBndry.get_bc_lines(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bc_lines_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Boundary Condition Lines with Perimeter\n",
        "\n",
        "if generate_maps:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    if not mesh_areas.empty:\n",
        "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
        "        \n",
        "        # Add labels for each polygon\n",
        "        for idx, row in mesh_areas.iterrows():\n",
        "            centroid = row.geometry.centroid\n",
        "            label = row.get('Name', f'Area {idx}')\n",
        "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
        "\n",
        "    # Plot boundary condition lines\n",
        "    if not bc_lines_df.empty:\n",
        "        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Easting')\n",
        "    ax.set_ylabel('Northing')\n",
        "    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n",
        "\n",
        "    # Add grid and legend\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "# Plot 2D Flow Area Perimeter Polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Breaklines as Geodataframe\n",
        "print(\"\\nExtracting Breaklines\")\n",
        "breaklines_gdf = HdfBndry.get_breaklines(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "breaklines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot breaklines and 2D Flow Area Perimeter Polygons\n",
        "\n",
        "if generate_plots:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot 2D Flow Area Perimeter Polygons\n",
        "    if not mesh_areas.empty:\n",
        "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
        "        \n",
        "        # Add labels for each polygon\n",
        "        for idx, row in mesh_areas.iterrows():\n",
        "            centroid = row.geometry.centroid\n",
        "            label = row.get('Name', f'Area {idx}')\n",
        "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
        "\n",
        "    # Plot breaklines\n",
        "    if not breaklines_gdf.empty:\n",
        "        breaklines_gdf.plot(ax=ax, color='blue', linewidth=2, label='Breaklines')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Easting')\n",
        "    ax.set_ylabel('Northing')\n",
        "    ax.set_title('2D Flow Area Perimeter Polygons and Breaklines')\n",
        "\n",
        "    # Add grid and legend\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get structures as GeoDatframe\n",
        "structures_gdf = HdfStruc.get_structures(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "structures_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get boundary condition lines as GeoDatframe\n",
        "bc_lines_gdf = HdfBndry.get_bc_lines(geom_hdf_path)\n",
        "print(\"\\nBoundary Condition Lines:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bc_lines_gdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dev Note: Need to add function for Reference Lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reference points as Geodataframe\n",
        "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nReference Points:\")\n",
        "ref_points_gdf\n",
        "# There are no reference points in this example project (for demonstration only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Refinement Regions\n",
        "refinement_regions_df = HdfBndry.get_refinement_regions(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "refinement_regions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Refinement Regions\n",
        "\n",
        "if not refinement_regions_df.empty:\n",
        "    print(\"Refinement Regions DataFrame:\")\n",
        "    display(refinement_regions_df.head())\n",
        "    \n",
        "    # Plot refinement regions\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    refinement_regions_df.plot(ax=ax, column='CellSize', legend=True, \n",
        "                               legend_kwds={'label': 'Cell Size', 'orientation': 'horizontal'},\n",
        "                               cmap='viridis')\n",
        "    ax.set_title('2D Mesh Area Refinement Regions')\n",
        "    ax.set_xlabel('Easting')\n",
        "    ax.set_ylabel('Northing')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No refinement regions found in the geometry file.\")\n",
        "\n",
        "# Analyze Refinement Regions\n",
        "if not refinement_regions_df.empty:\n",
        "    print(\"\\nRefinement Regions Analysis:\")\n",
        "    print(f\"Total number of refinement regions: {len(refinement_regions_df)}\")\n",
        "    print(\"\\nCell Size Statistics:\")\n",
        "    print(refinement_regions_df['CellSize'].describe())\n",
        "    \n",
        "    # Group by Shape Type\n",
        "    shape_type_counts = refinement_regions_df['ShapeType'].value_counts()\n",
        "    print(\"\\nRefinement Region Shape Types:\")\n",
        "    print(shape_type_counts)\n",
        "    \n",
        "    # Plot Shape Type distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shape_type_counts.plot(kind='bar')\n",
        "    plt.title('Distribution of Refinement Region Shape Types')\n",
        "    plt.xlabel('Shape Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Plan Parameters \n",
        "plan_parameters_df = HdfPlan.get_plan_parameters(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_parameters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract volume accounting data\n",
        "volume_accounting_df = HdfResultsPlan.get_volume_accounting(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_accounting_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RasPlanHdf Class Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get plan start time as datetime object\n",
        "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
        "print(f\"Simulation start time: {start_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation start time: 2018-09-09 00:00:00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get plan end time as datetime object\n",
        "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
        "print(f\"Simulation end time: {end_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation end time: 2018-09-14 00:00:00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get maximum iteration count for mesh cells\n",
        "max_iter_gdf = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_iter_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cell coordinates \n",
        "cell_coords = HdfMesh.get_mesh_cell_points(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Mesh Max Iterations\n",
        "\n",
        "if generate_maps:\n",
        "    # Extract x and y coordinates from the geometry column\n",
        "    max_iter_gdf['x'] = max_iter_gdf['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
        "    max_iter_gdf['y'] = max_iter_gdf['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
        "\n",
        "    # Remove rows with None coordinates\n",
        "    max_iter_gdf = max_iter_gdf.dropna(subset=['x', 'y'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_iter_gdf['x'], max_iter_gdf['y'], \n",
        "                         c=max_iter_gdf['cell_last_iteration'], \n",
        "                         cmap='viridis', \n",
        "                         s=1)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Iterations per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Iterations')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "\n",
        "# Print the first few rows of the dataframe for verification\n",
        "print(\"\\nFirst few rows of the dataframe:\")\n",
        "max_iter_gdf[['mesh_name', 'cell_id', 'geometry']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List top 10 points for Max Iteration per Cell\n",
        "# Sort the dataframe by cell_last_iteration in descending order\n",
        "top_iterations = max_iter_gdf.sort_values(by='cell_last_iteration', ascending=False).head(10)\n",
        "\n",
        "# Create a more informative display with coordinates\n",
        "print(\"\\nTop 10 Cells with Highest Iteration Counts:\")\n",
        "top_iterations_display = top_iterations.copy()\n",
        "top_iterations_display['x_coord'] = top_iterations_display['geometry'].apply(lambda geom: round(geom.x, 2))\n",
        "top_iterations_display['y_coord'] = top_iterations_display['geometry'].apply(lambda geom: round(geom.y, 2))\n",
        "\n",
        "# Display the results in a formatted table\n",
        "print(top_iterations_display[['mesh_name', 'cell_id', 'cell_last_iteration', 'x_coord', 'y_coord']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh maximum water surface elevation as Geodataframe\n",
        "max_ws_gdf = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Dataframe Attributes (the HDF Attributes are also imported as Geoataframe Attributes)\n",
        "max_ws_gdf.attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_ws_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the max water surface as a map\n",
        "if generate_maps:\n",
        "    # Extract x and y coordinates from the geometry column\n",
        "    max_ws_gdf['x'] = max_ws_gdf['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
        "    max_ws_gdf['y'] = max_ws_gdf['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
        "\n",
        "    # Remove rows with None coordinates\n",
        "    max_ws_gdf = max_ws_gdf.dropna(subset=['x', 'y'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_ws_gdf['x'], max_ws_gdf['y'], \n",
        "                         c=max_ws_gdf['maximum_water_surface'], \n",
        "                         cmap='viridis', \n",
        "                         s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Water Surface per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Water Surface (ft)')\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the time of the max water surface elevation (WSEL)\n",
        "if generate_maps:\n",
        "    import matplotlib.dates as mdates\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Convert the 'maximum_water_surface_time' to datetime objects\n",
        "    max_ws_gdf['max_wsel_time'] = pd.to_datetime(max_ws_gdf['maximum_water_surface_time'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Convert datetime to hours since the start for colormap\n",
        "    min_time = max_ws_gdf['max_wsel_time'].min()\n",
        "    color_values = (max_ws_gdf['max_wsel_time'] - min_time).dt.total_seconds() / 3600  # Convert to hours\n",
        "\n",
        "    scatter = ax.scatter(max_ws_gdf['x'], max_ws_gdf['y'], \n",
        "                        c=color_values, \n",
        "                        cmap='viridis', \n",
        "                        s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Time of Maximum Water Surface Elevation per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Set up the colorbar\n",
        "    cbar = plt.colorbar(scatter)\n",
        "    cbar.set_label('Hours since simulation start')\n",
        "\n",
        "    # Format the colorbar ticks to show hours\n",
        "    cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))  # Set ticks every 6 hours\n",
        "    cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Find the overall maximum WSEL and its time\n",
        "    max_wsel_row = max_ws_gdf.loc[max_ws_gdf['maximum_water_surface'].idxmax()]\n",
        "    hours_since_start = (max_wsel_row['max_wsel_time'] - min_time).total_seconds() / 3600\n",
        "    print(f\"\\nOverall Maximum WSEL: {max_wsel_row['maximum_water_surface']:.2f} ft\")\n",
        "    print(f\"Time of Overall Maximum WSEL: {max_wsel_row['max_wsel_time']}\")\n",
        "    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
        "    print(f\"Location of Overall Maximum WSEL: X={max_wsel_row['x']}, Y={max_wsel_row['y']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh minimum water surface elevation as geodataframe\n",
        "min_ws_gdf = HdfResultsMesh.get_mesh_min_ws(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_ws_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh maximum face velocity as geodataframe\n",
        "max_face_v_gdf = HdfResultsMesh.get_mesh_max_face_v(plan_hdf_path)\n",
        "print(\"\\nMesh Max Face Velocity:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_face_v_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract midpoint coordinates from the LineString geometries\n",
        "max_face_v_gdf['x'] = max_face_v_gdf['geometry'].apply(lambda geom: geom.centroid.x)\n",
        "max_face_v_gdf['y'] = max_face_v_gdf['geometry'].apply(lambda geom: geom.centroid.y)\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(max_face_v_gdf['x'], max_face_v_gdf['y'], \n",
        "                    c=max_face_v_gdf['maximum_face_velocity'].abs(),\n",
        "                    cmap='viridis',\n",
        "                    s=10)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_title('Max Face Velocity per Face')\n",
        "ax.set_xlabel('X Coordinate') \n",
        "ax.set_ylabel('Y Coordinate')\n",
        "plt.colorbar(scatter, label='Max Face Velocity (ft/s)')\n",
        "\n",
        "# Add grid lines\n",
        "ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Increase font size for better readability\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "# Adjust layout to prevent cutting off labels\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh minimum face velocity as geodataframe\n",
        "min_face_v_gdf = HdfResultsMesh.get_mesh_min_face_v(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Min Face Velocity:\")\n",
        "min_face_v_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh max water surface error as geodataframe\n",
        "\n",
        "max_ws_err_gdf = HdfResultsMesh.get_mesh_max_ws_err(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Max Water Surface Error:\")\n",
        "max_ws_err_gdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot max water surface error\n",
        "\n",
        "if generate_maps:\n",
        "# Extract x and y coordinates from the geometry points, handling None values\n",
        "    max_ws_err_gdf['x'] = max_ws_err_gdf['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
        "    max_ws_err_gdf['y'] = max_ws_err_gdf['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
        "\n",
        "    # Remove any rows with None coordinates\n",
        "    max_ws_err_gdf = max_ws_err_gdf.dropna(subset=['x', 'y'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_ws_err_gdf['x'], max_ws_err_gdf['y'],\n",
        "                        c=max_ws_err_gdf['cell_maximum_water_surface_error'],\n",
        "                        cmap='viridis',\n",
        "                        s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Water Surface Error per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Water Surface Error (ft)')\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort Dataframe to show top 10 maximum water surface errors:\n",
        "max_ws_err_gdf_sorted = max_ws_err_gdf.sort_values(by='cell_maximum_water_surface_error', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTop 10 maximum water surface errors:\")\n",
        "max_ws_err_gdf_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant) as geodataframe\n",
        "max_courant_gdf = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Maximum Courant):\")\n",
        "max_courant_gdf.attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_courant_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot max Courant number\n",
        "\n",
        "# Convert to GeoDataFrame if not empty\n",
        "if not max_courant_gdf.empty:\n",
        "    if generate_maps:\n",
        "        # Get centroids of line geometries for plotting\n",
        "        max_courant_gdf['centroid'] = max_courant_gdf.geometry.centroid\n",
        "        max_courant_gdf['x'] = max_courant_gdf.centroid.x\n",
        "        max_courant_gdf['y'] = max_courant_gdf.centroid.y\n",
        "\n",
        "        # Create the plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        scatter = ax.scatter(max_courant_gdf['x'], max_courant_gdf['y'],\n",
        "                        c=max_courant_gdf['maximum_face_courant'],\n",
        "                        cmap='viridis',\n",
        "                        s=10)\n",
        "\n",
        "        # Customize the plot\n",
        "        ax.set_title('Max Courant Number per Face')\n",
        "        ax.set_xlabel('X Coordinate')\n",
        "        ax.set_ylabel('Y Coordinate')\n",
        "        plt.colorbar(scatter, label='Max Courant Number')\n",
        "\n",
        "        # Add grid lines\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Increase font size for better readability\n",
        "        plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "        # Adjust layout to prevent cutting off labels\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "# Print the first few rows of the dataframe for verification\n",
        "print(\"\\nFirst few rows of the Courant number dataframe:\")\n",
        "max_courant_gdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n",
        "\n",
        "max_face_shear_gdf = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Maximum Face Shear Stress:\")\n",
        "print(max_face_shear_gdf.attrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_face_shear_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot max face shear stress\n",
        "\n",
        "if generate_maps and not max_face_shear_gdf.empty:\n",
        "    # Calculate centroids of the line geometries and extract coordinates\n",
        "    max_face_shear_gdf['centroid'] = max_face_shear_gdf['geometry'].apply(lambda line: line.centroid)\n",
        "    max_face_shear_gdf['x'] = max_face_shear_gdf['centroid'].apply(lambda point: point.x)\n",
        "    max_face_shear_gdf['y'] = max_face_shear_gdf['centroid'].apply(lambda point: point.y)\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_face_shear_gdf['x'], max_face_shear_gdf['y'],\n",
        "                        c=max_face_shear_gdf['maximum_face_shear_stress'],\n",
        "                        cmap='viridis',\n",
        "                        s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Face Shear Stress per Face')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Face Shear Stress (PSF)')\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for Minimum Water Surface as geodataframe\n",
        "summary_gdf_min_ws = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Water Surface\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Minimum Water Surface):\")\n",
        "summary_gdf_min_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for Minimum Face Velocity as geodataframe\n",
        "summary_gdf_min_fv = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Face Velocity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Minimum Face Velocity):\")\n",
        "summary_gdf_min_fv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for Cell Cumulative Iteration as geodataframe\n",
        "summary_gdf_cum_iter = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Cell Cumulative Iteration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Cell Cumulative Iteration):\")\n",
        "summary_gdf_cum_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh timeseries output as xarray\n",
        "# The mesh name is part of the timeseries HDF path, so you must pass the mesh_name to retrieve it\n",
        "\n",
        "# Get mesh areas from previous code cell\n",
        "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the first mesh area name to extract mesh timeseries output as xarray\n",
        "timeseries_xr = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_areas[0], \"Water Surface\") # Use the first 2D flow area name for mesh_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timeseries_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Series Output Variables for Cells\n",
        "# \n",
        "# Variable Name: Description\n",
        "# Water Surface: Water surface elevation\n",
        "# Depth: Water depth\n",
        "# Velocity: Magnitude of velocity\n",
        "# Velocity X: X-component of velocity\n",
        "# Velocity Y: Y-component of velocity\n",
        "# Froude Number: Froude number\n",
        "# Courant Number: Courant number\n",
        "# Shear Stress: Shear stress on the bed\n",
        "# Bed Elevation: Elevation of the bed\n",
        "# Precipitation Rate: Rate of precipitation\n",
        "# Infiltration Rate: Rate of infiltration\n",
        "# Evaporation Rate: Rate of evaporation\n",
        "# Percolation Rate: Rate of percolation\n",
        "# Groundwater Elevation: Elevation of groundwater\n",
        "# Groundwater Depth: Depth to groundwater\n",
        "# Groundwater Flow: Groundwater flow rate\n",
        "# Groundwater Velocity: Magnitude of groundwater velocity\n",
        "# Groundwater Velocity X: X-component of groundwater velocity\n",
        "# Groundwater Velocity Y: Y-component of groundwater velocity\n",
        "# \n",
        "# These variables are available for time series output at the cell level in 2D flow areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh cells timeseries output as xarray\n",
        "cells_timeseries_xr = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_areas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cells_timeseries_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot WSE Time Series Data (Random Cell ID) \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if generate_plots:\n",
        "    import numpy as np\n",
        "    import random\n",
        "\n",
        "    # Extract Water Surface data\n",
        "    water_surface = cells_timeseries_xr[mesh_areas[0]]['Water Surface']\n",
        "\n",
        "    # Get the time values\n",
        "    time_values = water_surface.coords['time'].values\n",
        "\n",
        "    # Pick a random cell_id\n",
        "    random_cell_id = random.choice(water_surface.coords['cell_id'].values)\n",
        "\n",
        "    # Extract the water surface elevation time series for the random cell\n",
        "    wsel_timeseries = water_surface.sel(cell_id=random_cell_id)\n",
        "\n",
        "    # Find the peak value and its index\n",
        "    peak_value = wsel_timeseries.max().item()\n",
        "    peak_index = wsel_timeseries.argmax().item()\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(time_values, wsel_timeseries, label=f'Cell ID: {random_cell_id}')\n",
        "    plt.scatter(time_values[peak_index], peak_value, color='red', s=100, zorder=5)\n",
        "    plt.annotate(f'Peak: {peak_value:.2f} ft', \n",
        "                (time_values[peak_index], peak_value),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                ha='left', va='bottom',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
        "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "    plt.title(f'Water Surface Elevation Time Series for Random Cell (ID: {random_cell_id})')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Water Surface Elevation (ft)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Log the plotting action\n",
        "    logging.info(f\"Plotted water surface elevation time series for random cell ID: {random_cell_id}\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print some statistics\n",
        "    print(f\"Statistics for Cell ID {random_cell_id}:\")\n",
        "    print(f\"Minimum WSEL: {wsel_timeseries.min().item():.2f} ft\")\n",
        "    print(f\"Maximum WSEL: {peak_value:.2f} ft\")\n",
        "    print(f\"Mean WSEL: {wsel_timeseries.mean().item():.2f} ft\")\n",
        "    print(f\"Time of peak: {time_values[peak_index]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh faces timeseries output as xarray\n",
        "faces_timeseries_xr = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_areas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faces_timeseries_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Random Face Results and Label Peak, Plus Map View\n",
        "\n",
        "if generate_maps:\n",
        "\n",
        "    # Select a random valid face ID number\n",
        "    random_face = np.random.randint(0, faces_timeseries_xr.sizes['face_id'])\n",
        "\n",
        "    # Extract time series data for the selected face\n",
        "    variable = 'face_velocity'  # We could also use 'face_flow'\n",
        "    face_data = faces_timeseries_xr[variable].sel(face_id=random_face)\n",
        "\n",
        "    # Find peak value and its corresponding time\n",
        "    peak_value = face_data.max().item()\n",
        "    peak_time = face_data.idxmax().values\n",
        "\n",
        "    # Plot time series\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(faces_timeseries_xr.time, face_data)\n",
        "    plt.title(f'{variable.capitalize()} Time Series for Face {random_face}')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(f'{variable.capitalize()} ({faces_timeseries_xr.attrs[\"units\"]})')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Annotate the peak point\n",
        "    plt.annotate(f'Peak: ({peak_time}, {peak_value:.2f})', \n",
        "                (peak_time, peak_value),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                arrowprops=dict(arrowstyle=\"->\"))\n",
        "\n",
        "    # Check for negative values and label the minimum if present\n",
        "    min_value = face_data.min().item()\n",
        "    if min_value < 0:\n",
        "        min_time = face_data.idxmin().values\n",
        "        plt.annotate(f'Min: ({min_time}, {min_value:.2f})', \n",
        "                    (min_time, min_value),\n",
        "                    xytext=(10, -10), textcoords='offset points',\n",
        "                    arrowprops=dict(arrowstyle=\"->\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create map view plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "\n",
        "    # Calculate mesh faces extents with 10% buffer\n",
        "    faces_bounds = mesh_cell_faces_gdf.total_bounds\n",
        "    x_min, y_min, x_max, y_max = faces_bounds\n",
        "    buffer_x = (x_max - x_min) * 0.1\n",
        "    buffer_y = (y_max - y_min) * 0.1\n",
        "    plot_xlim = [x_min - buffer_x, x_max + buffer_x]\n",
        "    plot_ylim = [y_min - buffer_y, y_max + buffer_y]\n",
        "\n",
        "    # Set plot limits before adding terrain\n",
        "    ax.set_xlim(plot_xlim)\n",
        "    ax.set_ylim(plot_ylim)\n",
        "\n",
        "    # Add the terrain TIFF to the map, clipped to our desired extent\n",
        "    tiff_path = Path.cwd() / 'example_projects' / 'BaldEagleCrkMulti2D' / 'Terrain' / 'Terrain50.baldeagledem.tif'\n",
        "    with rasterio.open(tiff_path) as src:\n",
        "        show(src, ax=ax, cmap='terrain', alpha=0.5)\n",
        "        \n",
        "    # Reset the limits after terrain plot\n",
        "    ax.set_xlim(plot_xlim)\n",
        "    ax.set_ylim(plot_ylim)\n",
        "\n",
        "    # Plot all faces in gray\n",
        "    mesh_cell_faces_gdf.plot(ax=ax, color='lightgray', alpha=0.5, zorder=2)\n",
        "\n",
        "    # Get the selected face geometry\n",
        "    selected_face = mesh_cell_faces_gdf[mesh_cell_faces_gdf['face_id'] == random_face]\n",
        "\n",
        "    # Highlight the selected face in red\n",
        "    selected_face.plot(\n",
        "        ax=ax, \n",
        "        color='red',\n",
        "        linewidth=2,\n",
        "        label=f'Selected Face (ID: {random_face})',\n",
        "        zorder=3\n",
        "    )\n",
        "\n",
        "    # Get bounds of selected face for zoomed inset\n",
        "    bounds = selected_face.geometry.bounds.iloc[0]\n",
        "    x_center = (bounds.iloc[0] + bounds.iloc[2]) / 2\n",
        "    y_center = (bounds.iloc[1] + bounds.iloc[3]) / 2\n",
        "    buffer = max(bounds.iloc[2] - bounds.iloc[0], bounds.iloc[3] - bounds.iloc[1]) * 2\n",
        "\n",
        "    # Create zoomed inset with a larger size, inside the map frame\n",
        "    axins = inset_axes(ax, width=\"70%\", height=\"70%\", loc='lower right',\n",
        "                    bbox_to_anchor=(0.65, 0.05, 0.35, 0.35),\n",
        "                    bbox_transform=ax.transAxes)\n",
        "\n",
        "    # Plot terrain and faces in inset\n",
        "    with rasterio.open(tiff_path) as src:\n",
        "        show(src, ax=axins, cmap='terrain', alpha=0.5)\n",
        "        \n",
        "    # Plot zoomed view in inset\n",
        "    mesh_cell_faces_gdf.plot(ax=axins, color='lightgray', alpha=0.5, zorder=2)\n",
        "    selected_face.plot(ax=axins, color='red', linewidth=2, zorder=3)\n",
        "\n",
        "    # Set inset limits with slightly more context\n",
        "    axins.set_xlim(x_center - buffer/1.5, x_center + buffer/1.5)\n",
        "    axins.set_ylim(y_center - buffer/1.5, y_center + buffer/1.5)\n",
        "\n",
        "    # Remove inset ticks for cleaner look\n",
        "    axins.set_xticks([])\n",
        "    axins.set_yticks([])\n",
        "\n",
        "    # Add a border to the inset\n",
        "    for spine in axins.spines.values():\n",
        "        spine.set_edgecolor('black')\n",
        "        spine.set_linewidth(1.5)\n",
        "\n",
        "    # Create connection lines between main plot and inset\n",
        "    # Get the selected face centroid for connection point\n",
        "    centroid = selected_face.geometry.centroid.iloc[0]\n",
        "    con1 = ConnectionPatch(\n",
        "        xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
        "        xyB=(0.02, 0.98), coordsB=axins.transAxes,\n",
        "        arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
        "    )\n",
        "    con2 = ConnectionPatch(\n",
        "        xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
        "        xyB=(0.98, 0.02), coordsB=axins.transAxes,\n",
        "        arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
        "    )\n",
        "\n",
        "    ax.add_artist(con1)\n",
        "    ax.add_artist(con2)\n",
        "\n",
        "    # Add title and legend to main plot\n",
        "    ax.set_title('Mesh Face Map View with Terrain')\n",
        "    ax.legend()\n",
        "\n",
        "    # Ensure equal aspect ratio while maintaining our desired extents\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary information\n",
        "    print(f\"Random Face: {random_face}\")\n",
        "    print(f\"Peak Value: {peak_value:.2f} {faces_timeseries_xr.attrs['units']} at {peak_time}\")\n",
        "    if min_value < 0:\n",
        "        print(f\"Minimum Value: {min_value:.2f} {faces_timeseries_xr.attrs['units']} at {min_time}\")\n",
        "\n",
        "    # Log the plotting action\n",
        "    logging.info(f\"Plotted mesh face time series and map view for random face ID: {random_face} with terrain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get meteorology precipitation attributes\n",
        "meteo_precip_attrs = HdfPlan.get_plan_met_precip(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "meteo_precip_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results unsteady attributes\n",
        "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results unsteady summary attributes\n",
        "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_summary_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results volume accounting attributes\n",
        "volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_accounting_attrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Compute Messages as String\n",
        "print(\"Extracting Compute Messages\")\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract string from HDF object at a given path\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    results_hdf_filename : str\n",
        "        Name of the HDF file\n",
        "    hdf_path : str\n",
        "        Path of the object in the HDF file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Extracted string from the specified HDF object\n",
        "    \"\"\"\n",
        "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
        "        try:\n",
        "            hdf_object = hdf_file[hdf_path]\n",
        "            if isinstance(hdf_object, h5py.Group):\n",
        "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
        "            elif isinstance(hdf_object, h5py.Dataset):\n",
        "                data = hdf_object[()]\n",
        "                if isinstance(data, bytes):\n",
        "                    return data.decode('utf-8')\n",
        "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
        "                    return [v.decode('utf-8') for v in data]\n",
        "                else:\n",
        "                    return str(data)\n",
        "            else:\n",
        "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
        "        except KeyError:\n",
        "            return f\"Path not found: {hdf_path}\"\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
        "    print(\"Compute Messages:\")\n",
        "    \n",
        "    # Parse and print the compute messages in a more visually friendly way\n",
        "    messages = results_summary_string[0].split('\\r\\n')\n",
        "    \n",
        "    for message in messages:\n",
        "        if message.strip():  # Skip empty lines\n",
        "            if ':' in message:\n",
        "                key, value = message.split(':', 1)\n",
        "                print(f\"{key.strip():40} : {value.strip()}\")\n",
        "            else:\n",
        "                print(f\"\\n{message.strip()}\")\n",
        "    \n",
        "    # Print computation summary in a table format\n",
        "    print(\"\\nComputation Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
        "    print(\"-\" * 50)\n",
        "    for line in messages:\n",
        "        if 'Computation Task' in line:\n",
        "            task, time = line.split('\\t')\n",
        "            print(f\"{task:<30} {time:<20}\")\n",
        "    \n",
        "    print(\"\\nComputation Speed:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
        "    print(\"-\" * 50)\n",
        "    for line in messages:\n",
        "        if 'Computation Speed' in line:\n",
        "            task, speed = line.split('\\t')\n",
        "            print(f\"{task:<30} {speed:<20}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting compute messages: {str(e)}\")\n",
        "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring HDF Datasets with HdfBase.get_dataset_info\n",
        "This allows users to find HDF information that is not included in the ras-commander library.  Find the path in HDFView and set the group_path below to explore the HDF datasets and attributes.  Then, use the output to write your own function to extract the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get HDF Paths with Properties (For Exploring HDF Files)\n",
        "HdfBase.get_dataset_info(plan_number, group_path=\"/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For HDF datasets that are not supported by the RAS-Commadner library, provide the dataset path to HdfBase.get_dataset_info and provide the output to an LLM along with a relevent HDF* class(es) to generate new functions that extend the library's coverage.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\12_2d_hdf_data_extraction pipes and pumps.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Example Project or Load Your Own Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the Pipes Beta project from HEC and run plan 01\n",
        "\n",
        "# Define the path to the Pipes Beta project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
        "import logging\n",
        "\n",
        "# Check if Pipes Beta.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = pipes_ex_path / \"DavisStormSystem.p02.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the Pipes Beta project\n",
        "    RasExamples.extract_project([\"Davis\"])\n",
        "\n",
        "    # Initialize the RAS project using the ras. (Pipe Networks are only supported in versions 6.6 and above)\n",
        "    init_ras_project(pipes_ex_path, \"6.6\")\n",
        "    logging.info(f\"Pipes Beta project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Pipes Beta object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"02\"\n",
        "\n",
        "    # Update run flags for the project\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,\n",
        "        unsteady_flow_simulation=True,\n",
        "        run_sediment=False,\n",
        "        post_processor=True,\n",
        "        floodplain_mapping=False\n",
        "    )\n",
        "\n",
        "    # Execute Plan 06 using RasCmdr for Pipes Beta\n",
        "    print(f\"Executing Plan {plan_number} for the Pipes Beta Creek project...\")\n",
        "    success_pipes_ex = RasCmdr.compute_plan(plan_number)\n",
        "    if success_pipes_ex:\n",
        "        print(f\"Plan {plan_number} executed successfully for Pipes Beta.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Pipes Beta.\\n\")\n",
        "else:\n",
        "    print(\"Pipes Beta.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the ras.\n",
        "    init_ras_project(pipes_ex_path, \"6.6\")\n",
        "    plan_number = \"02\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Plan DataFrame for the project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUnsteady DataFrame for the project:\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBoundary Conditions DataFrame for the project:\")\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get HDF Results Entries (only present when results are present)\n",
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate: Get the geometry HDF path if you are extracting geometry elements from the geometry HDF\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract runtime and compute time data\n",
        "print(\"\\nExtracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)\n",
        "runtime_df\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2D Models with Pipe Networks: HDF Data Extraction Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pipe conduits\n",
        "pipe_conduits_gdf = HdfPipe.get_pipe_conduits(\"02\") # NOTE: Here we use the plan number instead of the path variable.  The library decorators ensure this maps correctly.  \n",
        "print(\"\\nPipe Conduits: pipe_conduits_gdf\")\n",
        "pipe_conduits_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the pipe conduit linestrings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a new figure with a specified size\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "# Plot each linestring from the GeoDataFrame\n",
        "for idx, row in pipe_conduits_gdf.iterrows():\n",
        "    # Extract coordinates from the linestring\n",
        "    x_coords, y_coords = row['Polyline'].xy\n",
        "    \n",
        "    # Plot the linestring\n",
        "    plt.plot(x_coords, y_coords, 'b-', linewidth=1, alpha=0.7)\n",
        "    \n",
        "    # Add vertical line markers at endpoints\n",
        "    plt.plot([x_coords[0]], [y_coords[0]], 'x', color='black', markersize=4)\n",
        "    plt.plot([x_coords[-1]], [y_coords[-1]], 'x', color='black', markersize=4)\n",
        "    \n",
        "    # Calculate center point of the line\n",
        "    center_x = (x_coords[0] + x_coords[-1]) / 2\n",
        "    center_y = (y_coords[0] + y_coords[-1]) / 2\n",
        "    \n",
        "    # Add pipe name label at center, oriented top-right\n",
        "    plt.text(center_x, center_y, f'{row[\"Name\"]}', fontsize=8, \n",
        "             verticalalignment='bottom', horizontalalignment='left',\n",
        "             rotation=45)  # 45 degree angle for top-right orientation\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Pipe Conduit Network Layout')\n",
        "plt.xlabel('Easting')\n",
        "plt.ylabel('Northing')\n",
        "\n",
        "# Add grid\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Adjust layout to prevent label clipping\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the first 2 terrain profiles\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract terrain profiles from the GeoDataFrame\n",
        "terrain_profiles = pipe_conduits_gdf['Terrain_Profiles'].tolist()\n",
        "\n",
        "# Create separate plots for the first 2 terrain profiles\n",
        "for i in range(2):\n",
        "    profile = terrain_profiles[i]\n",
        "    \n",
        "    # Unzip the profile into x and y coordinates\n",
        "    x_coords, y_coords = zip(*profile)\n",
        "    \n",
        "    # Create a new figure for each profile\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='g', alpha=0.7)\n",
        "    \n",
        "    # Add title and labels\n",
        "    plt.title(f'Terrain Profile {i + 1}')\n",
        "    plt.xlabel('Distance along profile (m)')\n",
        "    plt.ylabel('Elevation (m)')\n",
        "    \n",
        "    # Add grid\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    \n",
        "    # Adjust layout to prevent label clipping\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Nodes/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pipe nodes\n",
        "pipe_nodes_gdf = HdfPipe.get_pipe_nodes(plan_hdf_path)\n",
        "print(\"\\nPipe Nodes:\")\n",
        "pipe_nodes_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Networks/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pipe network data\n",
        "pipe_network_gdf = HdfPipe.get_pipe_network(plan_hdf_path)\n",
        "print(\"\\nPipe Network Data:\")\n",
        "pipe_network_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pump stations\n",
        "pump_stations_gdf = HdfPump.get_pump_stations(plan_hdf_path)\n",
        "print(\"\\nPump Stations:\")\n",
        "pump_stations_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pump groups\n",
        "pump_groups_df = HdfPump.get_pump_groups(plan_hdf_path)\n",
        "print(\"\\nPump Groups:\")\n",
        "pump_groups_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
        "print(f\"Projection: {projection}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set CRS for GeoDataFrames\n",
        "if projection:\n",
        "    pipe_conduits_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
        "    pipe_nodes_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
        "\n",
        "print(\"Pipe Conduits GeoDataFrame columns:\")\n",
        "print(pipe_conduits_gdf.columns)\n",
        "\n",
        "print(\"\\nPipe Nodes GeoDataFrame columns:\")\n",
        "print(pipe_nodes_gdf.columns)\n",
        "\n",
        "perimeter_polygons = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
        "if projection:\n",
        "    perimeter_polygons.set_crs(projection, inplace=True, allow_override=True)\n",
        "    \n",
        "print(\"\\nPerimeter Polygons GeoDataFrame columns:\")\n",
        "print(perimeter_polygons.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from shapely import wkt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(28, 20))\n",
        "\n",
        "# Plot cell polygons with 50% transparency behind the pipe network\n",
        "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)\n",
        "if not cell_polygons_df.empty:\n",
        "    cell_polygons_df.plot(ax=ax, edgecolor='lightgray', facecolor='lightgray', alpha=0.5)\n",
        "\n",
        "# Plot pipe conduits - the Polyline column already contains LineString geometries\n",
        "pipe_conduits_gdf.set_geometry('Polyline', inplace=True)\n",
        "\n",
        "# Plot each pipe conduit individually to ensure all are shown\n",
        "for idx, row in pipe_conduits_gdf.iterrows():\n",
        "    ax.plot(*row.Polyline.xy, color='blue', linewidth=1)\n",
        "\n",
        "# Create a colormap for node elevations\n",
        "norm = plt.Normalize(pipe_nodes_gdf['Invert Elevation'].min(), \n",
        "                    pipe_nodes_gdf['Invert Elevation'].max())\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "# Plot pipe nodes colored by invert elevation\n",
        "scatter = ax.scatter(pipe_nodes_gdf.geometry.x, pipe_nodes_gdf.geometry.y,\n",
        "                    c=pipe_nodes_gdf['Invert Elevation'], \n",
        "                    cmap=cmap, norm=norm,\n",
        "                    s=100)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Invert Elevation (ft)', rotation=270, labelpad=15)\n",
        "\n",
        "# Add combined labels for invert and drop inlet elevations\n",
        "for idx, row in pipe_nodes_gdf.iterrows():\n",
        "    label_text = \"\"  # Initialize label_text for each node\n",
        "    # Add drop inlet elevation label if it exists and is not NaN\n",
        "    if 'Drop Inlet Elevation' in row and not np.isnan(row['Drop Inlet Elevation']):\n",
        "        label_text += f\"TOC: {row['Drop Inlet Elevation']:.2f}\\n\"\n",
        "    label_text += f\"INV: {row['Invert Elevation']:.2f}\"\n",
        "    \n",
        "    ax.annotate(label_text,\n",
        "                xy=(row.geometry.x, row.geometry.y),\n",
        "                xytext=(-10, -10), textcoords='offset points',\n",
        "                fontsize=8,\n",
        "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "# Add perimeter polygons \n",
        "if not perimeter_polygons.empty:\n",
        "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none')\n",
        "\n",
        "# Create proxy artists for legend\n",
        "conduit_line = mlines.Line2D([], [], color='blue', label='Conduits')\n",
        "node_point = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
        "                          markersize=10, label='Nodes')\n",
        "perimeter = mpatches.Patch(facecolor='none', edgecolor='black',\n",
        "                          label='Perimeter Polygons')\n",
        "\n",
        "ax.set_title('Pipe Network with Node Elevations')\n",
        "\n",
        "# Add legend with proxy artists\n",
        "ax.legend(handles=[conduit_line, node_point, perimeter])\n",
        "\n",
        "# Set aspect ratio to be equal and adjust limits\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "ax.autoscale_view()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize pump stations on a map\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "pump_stations_gdf.plot(ax=ax, color='green', markersize=50, label='Pump Station')\n",
        "\n",
        "# Add perimeter polygons\n",
        "if not perimeter_polygons.empty:\n",
        "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none', label='Perimeter Polygons')\n",
        "\n",
        "ax.set_title('Pump Station Location')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Get pipe network timeseries\n",
        "valid_variables = [\n",
        "    \"Cell Courant\", \"Cell Water Surface\", \"Face Flow\", \"Face Velocity\",\n",
        "    \"Face Water Surface\", \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\",\n",
        "    \"Pipes/Vel DS\", \"Pipes/Vel US\", \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\",\n",
        "    \"Nodes/Water Surface\"\n",
        "]\n",
        "\n",
        "print(\"Valid variables for pipe network timeseries:\")\n",
        "for var in valid_variables:\n",
        "    print(f\"- {var}\")\n",
        "\n",
        "# Extract pipe network timeseries for each valid pipe-related variable\n",
        "pipe_variables = [var for var in valid_variables if var.startswith(\"Pipes/\") or var.startswith(\"Nodes/\")]\n",
        "\n",
        "for variable in pipe_variables:\n",
        "    try:\n",
        "        pipe_timeseries = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
        "        print(f\"\\nPipe Network Timeseries ({variable}):\")\n",
        "        print(pipe_timeseries.head())  # Print first few rows to avoid overwhelming output\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {variable}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipe Network Timeseries Data Description\n",
        "\n",
        "The `get_pipe_network_timeseries` function returns an xarray DataArray for each variable. Here's a general description of the data structure:\n",
        "\n",
        "1. **Pipes/Pipe Flow DS and Pipes/Pipe Flow US**:\n",
        "   - Dimensions: time, location (pipe IDs)\n",
        "   - Units: ft^3/s (cubic feet per second)\n",
        "   - Description: Represents the flow rate at the downstream (DS) and upstream (US) ends of pipes over time.\n",
        "\n",
        "2. **Pipes/Vel DS and Pipes/Vel US**:\n",
        "   - Dimensions: time, location (pipe IDs)\n",
        "   - Units: ft/s (feet per second)\n",
        "   - Description: Shows the velocity at the downstream (DS) and upstream (US) ends of pipes over time.\n",
        "\n",
        "3. **Nodes/Depth**:\n",
        "   - Dimensions: time, location (node IDs)\n",
        "   - Units: ft (feet)\n",
        "   - Description: Indicates the depth of water at each node over time.\n",
        "\n",
        "4. **Nodes/Drop Inlet Flow**:\n",
        "   - Dimensions: time, location (node IDs)\n",
        "   - Units: cfs (cubic feet per second)\n",
        "   - Description: Represents the flow rate through drop inlets at each node over time.\n",
        "\n",
        "5. **Nodes/Water Surface**:\n",
        "   - Dimensions: time, location (node IDs)\n",
        "   - Units: ft (feet)\n",
        "   - Description: Shows the water surface elevation at each node over time.\n",
        "\n",
        "General notes:\n",
        "- The 'time' dimension represents the simulation timesteps.\n",
        "- The 'location' dimension represents either pipe IDs or node IDs, depending on the variable.\n",
        "- The number of timesteps and locations may vary depending on the specific dataset and simulation setup.\n",
        "- Negative values in flow variables may indicate reverse flow direction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the variables we want to plot\n",
        "variables = [\n",
        "    \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\", \"Pipes/Vel DS\", \"Pipes/Vel US\",\n",
        "    \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\", \"Nodes/Water Surface\"\n",
        "]\n",
        "\n",
        "# Create a separate plot for each variable\n",
        "for variable in variables:\n",
        "    try:\n",
        "        # Get the data for the current variable\n",
        "        data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
        "        \n",
        "        # Create a new figure\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        \n",
        "        # Pick one random location\n",
        "        random_location = random.choice(data.location.values)\n",
        "        \n",
        "        # Determine if it's a pipe or node variable\n",
        "        if variable.startswith(\"Pipes/\"):\n",
        "            location_type = \"Conduit ID\"\n",
        "        else:\n",
        "            location_type = \"Node ID\"\n",
        "        \n",
        "        # Plot the data for the randomly selected location\n",
        "        ax.plot(data.time, data.sel(location=random_location), label=f'{location_type} {random_location}')\n",
        "        \n",
        "        # Set the title and labels\n",
        "        ax.set_title(f'{variable} Over Time ({location_type} {random_location})')\n",
        "        ax.set_xlabel('Time')  # Corrected from ax.xlabel to ax.set_xlabel\n",
        "        ax.set_ylabel(f'{variable} ({data.attrs[\"units\"]})')  # Corrected from ax.ylabel to ax.set_ylabel\n",
        "        \n",
        "        # Format the x-axis to show dates nicely\n",
        "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
        "        plt.xticks(rotation=45)\n",
        "        \n",
        "        # Add a legend\n",
        "        ax.legend(title=location_type, loc='upper left')\n",
        "        \n",
        "        # Adjust the layout\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting {variable}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 8: Get pump station timeseries\n",
        "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
        "# Use the results_pump_station_timeseries method \n",
        "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
        "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
        "print(pump_timeseries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pump Stations/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the pump station timeseries data\n",
        "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
        "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
        "\n",
        "# Print the pump station timeseries\n",
        "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
        "print(pump_timeseries)\n",
        "\n",
        "# Create a new figure for plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# Plot each variable in the timeseries\n",
        "for variable in pump_timeseries.coords['variable'].values:\n",
        "    data = pump_timeseries.sel(variable=variable)\n",
        "    \n",
        "    # Decode units to strings\n",
        "    unit = pump_timeseries.attrs[\"units\"][list(pump_timeseries.coords[\"variable\"].values).index(variable)][1].decode('utf-8')\n",
        "    \n",
        "    # Check if the variable is 'Pumps on' to plot it differently\n",
        "    if variable == 'Pumps on':\n",
        "        # Plot with color based on the on/off status\n",
        "        colors = ['green' if val > 0 else 'red' for val in data.values.flatten()]\n",
        "        ax.scatter(pump_timeseries['time'], data, label=f'{variable} ({unit})', color=colors)\n",
        "    else:\n",
        "        ax.plot(pump_timeseries['time'], data, label=f'{variable} ({unit})')\n",
        "        \n",
        "        # Label the peak values\n",
        "        peak_time = pump_timeseries['time'][data.argmax()]\n",
        "        peak_value = data.max()\n",
        "        ax.annotate(f'Peak: {peak_value:.2f}', xy=(peak_time, peak_value), \n",
        "                    xytext=(peak_time, peak_value + 0.1 * peak_value), \n",
        "                    arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "                    fontsize=10, color='black', ha='center')\n",
        "\n",
        "# Set the title and labels\n",
        "ax.set_title(f'Timeseries Data for Pump Station: {pump_station_name}')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Values')\n",
        "\n",
        "# Format the x-axis to show dates nicely\n",
        "ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add a legend\n",
        "ax.legend(title='Variables', loc='upper left')\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring HDF Datasets with HdfBase.get_dataset_info\n",
        "This allows users to find HDF information that is not included in the ras-commander library.  Find the path in HDFView and set the group_path below to explore the HDF datasets and attributes.  Then, use the output to write your own function to extract the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pipe Conduits/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For HDF datasets that are not supported by the RAS-Commander library, provide the dataset path to HdfBase.get_dataset_info and provide the output to an LLM along with a relevent HDF* class(es) to generate new functions that extend the library's coverage.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\13_2d_detail_face_data_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:12.556104Z",
          "iopub.status.busy": "2025-11-17T17:54:12.555873Z",
          "iopub.status.idle": "2025-11-17T17:54:14.120379Z",
          "shell.execute_reply": "2025-11-17T17:54:14.119652Z"
        }
      },
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS 2D Detail Face Data Extraction Examples\n",
        "\n",
        "This notebook demonstrates how to extract detailed 2D face data, display individual cell face results and calculate a discharge weighted velocity using a user-provided profile line located where cell faces are perpendicular to flow. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.123844Z",
          "iopub.status.busy": "2025-11-17T17:54:14.123420Z",
          "iopub.status.idle": "2025-11-17T17:54:14.126763Z",
          "shell.execute_reply": "2025-11-17T17:54:14.126205Z"
        }
      },
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.129391Z",
          "iopub.status.busy": "2025-11-17T17:54:14.129086Z",
          "iopub.status.idle": "2025-11-17T17:54:14.133196Z",
          "shell.execute_reply": "2025-11-17T17:54:14.132652Z"
        }
      },
      "outputs": [],
      "source": [
        "# Enable this cell for local development version of ras-commander\n",
        "import os\n",
        "import sys      \n",
        "from pathlib import Path\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.135307Z",
          "iopub.status.busy": "2025-11-17T17:54:14.135029Z",
          "iopub.status.idle": "2025-11-17T17:54:14.271967Z",
          "shell.execute_reply": "2025-11-17T17:54:14.271464Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.274202Z",
          "iopub.status.busy": "2025-11-17T17:54:14.273727Z",
          "iopub.status.idle": "2025-11-17T17:54:14.277517Z",
          "shell.execute_reply": "2025-11-17T17:54:14.276983Z"
        }
      },
      "outputs": [],
      "source": [
        "# This cell will try to import the pip package, if it fails it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "# This allows a user's revisions to be tested locally without installing the package\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Flexible imports to allow for development without installation \n",
        "#  ** Use this version with Jupyter Notebooks **\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    import os\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "print(\"ras_commander imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: This notebook relies on the Chippewa 2D Project along with:\n",
        " - A user-generated GeoJSON containing the proposed profile lines\n",
        " - An example is provided in the \"data\" subfolder with name profile_lines_chippewa2D.geojson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.280146Z",
          "iopub.status.busy": "2025-11-17T17:54:14.279857Z",
          "iopub.status.idle": "2025-11-17T17:54:14.329842Z",
          "shell.execute_reply": "2025-11-17T17:54:14.329348Z"
        }
      },
      "outputs": [],
      "source": [
        "# Download the Chippewa_2D project from HEC and run plan 02\n",
        "\n",
        "# Define the path to the Chippewa_2D project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"Chippewa_2D\"\n",
        "\n",
        "# Check if Chippewa_2D.p02.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = bald_eagle_path / \"Chippewa_2D.p02.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the Chippewa_2D project\n",
        "    RasExamples.extract_project([\"Chippewa_2D\"])\n",
        "\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    from ras_commander import get_logger  # Import after sys.path is set if not installed\n",
        "    logger = get_logger(__name__)\n",
        "    logger.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    logger.info(f\"Bald Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"02\"\n",
        "\n",
        "    # Update run flags for the project\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,\n",
        "        unsteady_flow_simulation=True,\n",
        "        run_sediment=False,\n",
        "        post_processor=True,\n",
        "        floodplain_mapping=False\n",
        "    )\n",
        "    \n",
        "    # Enable Face Flow output - required for discharge-weighted velocity calculations\n",
        "    # This adds \"HDF Additional Output Variable=Face Flow\" to the plan file\n",
        "    RasPlan.add_hdf_output_variable(plan_number, \"Face Flow\")\n",
        "    print(\"Enabled Face Flow HDF output for discharge-weighted velocity calculations\")\n",
        "\n",
        "    # Execute Plan 02 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Chippewa_2D project...\")\n",
        "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
        "    if success_bald_eagle:\n",
        "        print(f\"Plan {plan_number} executed successfully for Chippewa_2D.\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Chippewa_2D.\")\n",
        "else:\n",
        "    print(\"Chippewa_2D.p02.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    plan_number = \"02\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.331640Z",
          "iopub.status.busy": "2025-11-17T17:54:14.331416Z",
          "iopub.status.idle": "2025-11-17T17:54:14.344359Z",
          "shell.execute_reply": "2025-11-17T17:54:14.343847Z"
        }
      },
      "outputs": [],
      "source": [
        "# Show ras object info\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.346857Z",
          "iopub.status.busy": "2025-11-17T17:54:14.346423Z",
          "iopub.status.idle": "2025-11-17T17:54:14.353301Z",
          "shell.execute_reply": "2025-11-17T17:54:14.352835Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.355344Z",
          "iopub.status.busy": "2025-11-17T17:54:14.355126Z",
          "iopub.status.idle": "2025-11-17T17:54:14.365901Z",
          "shell.execute_reply": "2025-11-17T17:54:14.365356Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.367964Z",
          "iopub.status.busy": "2025-11-17T17:54:14.367735Z",
          "iopub.status.idle": "2025-11-17T17:54:14.377692Z",
          "shell.execute_reply": "2025-11-17T17:54:14.377108Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.380161Z",
          "iopub.status.busy": "2025-11-17T17:54:14.379777Z",
          "iopub.status.idle": "2025-11-17T17:54:14.382257Z",
          "shell.execute_reply": "2025-11-17T17:54:14.381800Z"
        }
      },
      "outputs": [],
      "source": [
        "# Define the HDF input path as Plan Number\n",
        "\n",
        "plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.384232Z",
          "iopub.status.busy": "2025-11-17T17:54:14.384011Z",
          "iopub.status.idle": "2025-11-17T17:54:14.388036Z",
          "shell.execute_reply": "2025-11-17T17:54:14.387530Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.389926Z",
          "iopub.status.busy": "2025-11-17T17:54:14.389698Z",
          "iopub.status.idle": "2025-11-17T17:54:14.393011Z",
          "shell.execute_reply": "2025-11-17T17:54:14.392538Z"
        }
      },
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.394925Z",
          "iopub.status.busy": "2025-11-17T17:54:14.394707Z",
          "iopub.status.idle": "2025-11-17T17:54:14.398227Z",
          "shell.execute_reply": "2025-11-17T17:54:14.397802Z"
        }
      },
      "outputs": [],
      "source": [
        "# Alternate: Get the geometry HDF path if you are extracting geometry elements from the geometry HDF \n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.399868Z",
          "iopub.status.busy": "2025-11-17T17:54:14.399669Z",
          "iopub.status.idle": "2025-11-17T17:54:14.402798Z",
          "shell.execute_reply": "2025-11-17T17:54:14.402343Z"
        }
      },
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.404424Z",
          "iopub.status.busy": "2025-11-17T17:54:14.404225Z",
          "iopub.status.idle": "2025-11-17T17:54:14.421904Z",
          "shell.execute_reply": "2025-11-17T17:54:14.421457Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Extract runtime and compute time data\n",
        "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)\n",
        "if runtime_df is not None:\n",
        "    runtime_df\n",
        "else:\n",
        "    print(\"No runtime data found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.423699Z",
          "iopub.status.busy": "2025-11-17T17:54:14.423468Z",
          "iopub.status.idle": "2025-11-17T17:54:14.426313Z",
          "shell.execute_reply": "2025-11-17T17:54:14.425857Z"
        }
      },
      "outputs": [],
      "source": [
        "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
        "print(geom_hdf_path)\n",
        "\n",
        "# For the example project, plan 02 is associated with geometry 09\n",
        "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
        "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.428153Z",
          "iopub.status.busy": "2025-11-17T17:54:14.427939Z",
          "iopub.status.idle": "2025-11-17T17:54:14.442345Z",
          "shell.execute_reply": "2025-11-17T17:54:14.441804Z"
        }
      },
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
        "if projection:\n",
        "    print(f\"Projection: {projection}\")\n",
        "else:\n",
        "    print(\"No projection information found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.444852Z",
          "iopub.status.busy": "2025-11-17T17:54:14.444544Z",
          "iopub.status.idle": "2025-11-17T17:54:14.447286Z",
          "shell.execute_reply": "2025-11-17T17:54:14.446835Z"
        }
      },
      "outputs": [],
      "source": [
        "# Set the  to USA Contiguous Albers Equal Area Conic (USGS version)\n",
        "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
        "projection = 'EPSG:5070'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.449057Z",
          "iopub.status.busy": "2025-11-17T17:54:14.448842Z",
          "iopub.status.idle": "2025-11-17T17:54:14.456604Z",
          "shell.execute_reply": "2025-11-17T17:54:14.456207Z"
        }
      },
      "outputs": [],
      "source": [
        "# Use HdfPlan for geometry-related operations\n",
        "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
        "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
        "\n",
        "if not geom_attrs.empty:\n",
        "    # Display the DataFrame directly\n",
        "    print(\"Base Geometry Attributes:\")\n",
        "    geom_attrs\n",
        "else:\n",
        "    print(\"No base geometry attributes found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.458443Z",
          "iopub.status.busy": "2025-11-17T17:54:14.458240Z",
          "iopub.status.idle": "2025-11-17T17:54:14.463735Z",
          "shell.execute_reply": "2025-11-17T17:54:14.463290Z"
        }
      },
      "outputs": [],
      "source": [
        "# Use HdfMesh for geometry-related operations\n",
        "print(\"\\nExample 3: Listing 2D Flow Area Names\")\n",
        "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
        "print(\"2D Flow Area Names:\", flow_area_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.465410Z",
          "iopub.status.busy": "2025-11-17T17:54:14.465210Z",
          "iopub.status.idle": "2025-11-17T17:54:14.474269Z",
          "shell.execute_reply": "2025-11-17T17:54:14.473805Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Get 2D Flow Area Attributes (get_geom_2d_flow_area_attrs)\n",
        "print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n",
        "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n",
        "flow_area_attributes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.476096Z",
          "iopub.status.busy": "2025-11-17T17:54:14.475752Z",
          "iopub.status.idle": "2025-11-17T17:54:14.485638Z",
          "shell.execute_reply": "2025-11-17T17:54:14.485212Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n",
        "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
        "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)  # Corrected function name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.487541Z",
          "iopub.status.busy": "2025-11-17T17:54:14.487154Z",
          "iopub.status.idle": "2025-11-17T17:54:14.514244Z",
          "shell.execute_reply": "2025-11-17T17:54:14.513716Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Extract mesh cell faces\n",
        "print(\"\\nExample: Extracting mesh cell faces\")\n",
        "\n",
        "# Get mesh cell faces using the standardize_input decorator for consistent file handling\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces GeoDataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.516237Z",
          "iopub.status.busy": "2025-11-17T17:54:14.516004Z",
          "iopub.status.idle": "2025-11-17T17:54:14.518641Z",
          "shell.execute_reply": "2025-11-17T17:54:14.518153Z"
        }
      },
      "outputs": [],
      "source": [
        "# Set the projection to USA Contiguous Albers Equal Area Conic (USGS version)\n",
        "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
        "projection = 'EPSG:5070'  # NAD83 / Conus Albers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.520706Z",
          "iopub.status.busy": "2025-11-17T17:54:14.520475Z",
          "iopub.status.idle": "2025-11-17T17:54:14.754294Z",
          "shell.execute_reply": "2025-11-17T17:54:14.753786Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example Function: Find the nearest cell face to a given point\n",
        "# This provides enough basic information the face cell logic in the notebook\n",
        "\n",
        "def find_nearest_cell_face(point, cell_faces_df):\n",
        "    \"\"\"\n",
        "    Find the nearest cell face to a given point.\n",
        "\n",
        "    Args:\n",
        "        point (shapely.geometry.Point): The input point.\n",
        "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
        "\n",
        "    Returns:\n",
        "        int: The face_id of the nearest cell face.\n",
        "        float: The distance to the nearest cell face.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the input point to all cell faces\n",
        "    distances = cell_faces_df.geometry.distance(point)\n",
        "\n",
        "    # Find the index of the minimum distance\n",
        "    nearest_index = distances.idxmin()\n",
        "\n",
        "    # Get the face_id and distance of the nearest cell face\n",
        "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
        "    nearest_distance = distances[nearest_index]\n",
        "\n",
        "    return nearest_face_id, nearest_distance\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nExample: Finding the nearest cell face to a given point\")\n",
        "\n",
        "# Create a sample point (you can replace this with any point of interest)\n",
        "from shapely.geometry import Point\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "# Create the sample point with the same CRS as mesh_cell_faces\n",
        "sample_point = GeoDataFrame(\n",
        "    {'geometry': [Point(1025677, 7853731)]}, \n",
        "    crs=mesh_cell_faces.crs\n",
        ")\n",
        "\n",
        "if not mesh_cell_faces.empty and not sample_point.empty:\n",
        "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n",
        "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
        "    print(f\"Face ID: {nearest_face_id}\")\n",
        "    print(f\"Distance: {distance:.2f} units\")\n",
        "\n",
        "    # Visualize the result\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Plot all cell faces\n",
        "    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
        "    \n",
        "    # Plot the sample point\n",
        "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
        "    \n",
        "    # Plot the nearest cell face\n",
        "    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n",
        "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
        "    \n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('Nearest Cell Face to Sample Point')\n",
        "    \n",
        "    # Add legend and grid\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    \n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Unable to perform nearest cell face search due to missing data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:14.756123Z",
          "iopub.status.busy": "2025-11-17T17:54:14.755947Z",
          "iopub.status.idle": "2025-11-17T17:54:15.086495Z",
          "shell.execute_reply": "2025-11-17T17:54:15.085857Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Extract mesh cell faces and plot with profile lines\n",
        "print(\"\\nExample: Extracting mesh cell faces and plotting with profile lines\")\n",
        "\n",
        "# Get mesh cell faces\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces DataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces\n",
        "\n",
        "# Load the GeoJSON file for profile lines\n",
        "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
        "profile_lines_gdf = gpd.read_file(geojson_path)\n",
        "\n",
        "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
        "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
        "\n",
        "# Plot the mesh cell faces and profile lines together\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "mesh_cell_faces.plot(ax=ax, color='blue', alpha=0.5, edgecolor='k', label='Mesh Cell Faces')\n",
        "profile_lines_gdf.plot(ax=ax, color='orange', linewidth=2, label='Profile Lines')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Easting')\n",
        "ax.set_ylabel('Northing')\n",
        "ax.set_title('Mesh Cell Faces and Profile Lines')\n",
        "\n",
        "# Add grid and legend\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:15.088949Z",
          "iopub.status.busy": "2025-11-17T17:54:15.088648Z",
          "iopub.status.idle": "2025-11-17T17:54:17.465221Z",
          "shell.execute_reply": "2025-11-17T17:54:17.464583Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Extracting mesh cell faces near profile lines\n",
        "print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n",
        "\n",
        "# Get mesh cell faces using HdfMesh class\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces DataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces\n",
        "\n",
        "# Load the GeoJSON file for profile lines\n",
        "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
        "profile_lines_gdf = gpd.read_file(geojson_path)\n",
        "\n",
        "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
        "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
        "\n",
        "# Initialize a dictionary to store faces near each profile line\n",
        "faces_near_profile_lines = {}\n",
        "\n",
        "# Define distance threshold (10 ft converted to meters)\n",
        "distance_threshold = 10\n",
        "angle_threshold = 60  # degrees\n",
        "\n",
        "# Function to calculate the smallest angle between two lines or line segments.\n",
        "def calculate_angle(line):\n",
        "    if isinstance(line, LineString):\n",
        "        x_diff = line.xy[0][-1] - line.xy[0][0]\n",
        "        y_diff = line.xy[1][-1] - line.xy[1][0]\n",
        "    else:\n",
        "        x_diff = line[1][0] - line[0][0]\n",
        "        y_diff = line[1][1] - line[0][1]\n",
        "    \n",
        "    angle = np.degrees(np.arctan2(y_diff, x_diff))\n",
        "    return angle % 360 if angle >= 0 else (angle + 360) % 360\n",
        "\n",
        "# Function to break line into segments\n",
        "def break_line_into_segments(line, segment_length):\n",
        "    segments = []\n",
        "    segment_angles = []\n",
        "    \n",
        "    distances = np.arange(0, line.length, segment_length)\n",
        "    if distances[-1] != line.length:\n",
        "        distances = np.append(distances, line.length)\n",
        "        \n",
        "    for i in range(len(distances)-1):\n",
        "        point1 = line.interpolate(distances[i])\n",
        "        point2 = line.interpolate(distances[i+1])\n",
        "        segment = LineString([point1, point2])\n",
        "        segments.append(segment)\n",
        "        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n",
        "        \n",
        "    return segments, segment_angles\n",
        "\n",
        "# Function to calculate angle difference accounting for 180 degree equivalence\n",
        "def angle_difference(angle1, angle2):\n",
        "    diff = abs(angle1 - angle2) % 180\n",
        "    return min(diff, 180 - diff)\n",
        "\n",
        "# Function to order faces along profile line\n",
        "def order_faces_along_profile(profile_line, faces_gdf):\n",
        "    profile_start = Point(profile_line.coords[0])\n",
        "    \n",
        "    faces_with_dist = []\n",
        "    for idx, face in faces_gdf.iterrows():\n",
        "        face_start = Point(face.geometry.coords[0])\n",
        "        dist = profile_start.distance(face_start)\n",
        "        faces_with_dist.append((idx, dist))\n",
        "    \n",
        "    faces_with_dist.sort(key=lambda x: x[1])\n",
        "    return [x[0] for x in faces_with_dist]\n",
        "\n",
        "# Function to combine ordered faces into single linestring\n",
        "def combine_faces_to_linestring(ordered_faces_gdf):\n",
        "    coords = []\n",
        "    for _, face in ordered_faces_gdf.iterrows():\n",
        "        if not coords:  # First face - add all coordinates\n",
        "            coords.extend(list(face.geometry.coords))\n",
        "        else:  # Subsequent faces - add only end coordinate\n",
        "            coords.append(face.geometry.coords[-1])\n",
        "    return LineString(coords)\n",
        "\n",
        "# Initialize GeoDataFrame for final profile-to-faceline results\n",
        "profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n",
        "\n",
        "# Iterate through each profile line\n",
        "for index, profile_line in profile_lines_gdf.iterrows():\n",
        "    profile_geom = profile_line.geometry\n",
        "    \n",
        "    # Break profile line into segments\n",
        "    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n",
        "    \n",
        "    # Initialize set to store nearby faces\n",
        "    nearby_faces = set()\n",
        "    \n",
        "    # For each face, check distance to segments and angle difference\n",
        "    for face_idx, face in mesh_cell_faces.iterrows():\n",
        "        face_geom = face.geometry\n",
        "        \n",
        "        if isinstance(face_geom, LineString):\n",
        "            face_angle = calculate_angle(face_geom)\n",
        "            \n",
        "            for segment, segment_angle in zip(segments, segment_angles):\n",
        "                if face_geom.distance(segment) <= distance_threshold:\n",
        "                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n",
        "                        nearby_faces.add(face_idx)\n",
        "                        break\n",
        "    \n",
        "    # Convert the set of indices back to a GeoDataFrame\n",
        "    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n",
        "    \n",
        "    # Order faces along profile line\n",
        "    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n",
        "    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n",
        "    \n",
        "    # Combine ordered faces into single linestring\n",
        "    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n",
        "    \n",
        "    # Add to profile_to_faceline GeoDataFrame\n",
        "    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n",
        "                               'geometry': [combined_linestring]}, \n",
        "                              crs=profile_lines_gdf.crs)\n",
        "    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n",
        "    \n",
        "    # Store the ordered faces in the dictionary\n",
        "    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot all mesh cell faces in light blue\n",
        "mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n",
        "\n",
        "# Plot selected faces for each profile line with numbers\n",
        "colors = ['red', 'green', 'blue']\n",
        "for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n",
        "    if not faces.empty:\n",
        "        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n",
        "        \n",
        "        # Add numbers to faces\n",
        "        for i, (idx, face) in enumerate(faces.iterrows()):\n",
        "            midpoint = face.geometry.interpolate(0.5, normalized=True)\n",
        "            ax.text(midpoint.x, midpoint.y, str(i+1), \n",
        "                   color=color, fontweight='bold', ha='center', va='center')\n",
        "\n",
        "# Plot the combined linestrings\n",
        "profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n",
        "                        linestyle='--', label='Combined Face Lines')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Easting')\n",
        "ax.set_ylabel('Northing')\n",
        "ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile')\n",
        "\n",
        "# Add grid and legend\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nOriginal ordered faces near profile lines:\")\n",
        "faces_near_profile_lines\n",
        "\n",
        "print(\"\\nCombined profile-to-faceline results:\")\n",
        "profile_to_faceline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:17.467693Z",
          "iopub.status.busy": "2025-11-17T17:54:17.467507Z",
          "iopub.status.idle": "2025-11-17T17:54:17.492345Z",
          "shell.execute_reply": "2025-11-17T17:54:17.491730Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get face property tables with error handling\n",
        "face_property_tables = HdfMesh.get_mesh_face_property_tables(geom_hdf_path)\n",
        "face_property_tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:17.494190Z",
          "iopub.status.busy": "2025-11-17T17:54:17.494025Z",
          "iopub.status.idle": "2025-11-17T17:54:17.642978Z",
          "shell.execute_reply": "2025-11-17T17:54:17.642319Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract the face property table for Face ID 4 and display it\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "face_id = 4\n",
        "face_properties = face_property_tables['Perimeter 1'][face_property_tables['Perimeter 1']['Face ID'] == face_id]\n",
        "\n",
        "# Create subplots arranged horizontally\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot Z vs Area\n",
        "axs[0].plot(face_properties['Z'], face_properties['Area'], marker='o', color='blue', label='Area')\n",
        "axs[0].set_title(f'Face ID {face_id}: Z vs Area')\n",
        "axs[0].set_xlabel('Z')\n",
        "axs[0].set_ylabel('Area')\n",
        "axs[0].grid(True)\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot Z vs Wetted Perimeter\n",
        "axs[1].plot(face_properties['Z'], face_properties['Wetted Perimeter'], marker='o', color='green', label='Wetted Perimeter')\n",
        "axs[1].set_title(f'Face ID {face_id}: Z vs Wetted Perimeter')\n",
        "axs[1].set_xlabel('Z')\n",
        "axs[1].set_ylabel('Wetted Perimeter')\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "# Plot Z vs Manning's n\n",
        "axs[2].plot(face_properties['Z'], face_properties[\"Manning's n\"], marker='o', color='red', label=\"Manning's n\")\n",
        "axs[2].set_title(f'Face ID {face_id}: Z vs Manning\\'s n')\n",
        "axs[2].set_xlabel('Z')\n",
        "axs[2].set_ylabel(\"Manning's n\")\n",
        "axs[2].grid(True)\n",
        "axs[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:17.645090Z",
          "iopub.status.busy": "2025-11-17T17:54:17.644908Z",
          "iopub.status.idle": "2025-11-17T17:54:17.876769Z",
          "shell.execute_reply": "2025-11-17T17:54:17.876260Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get mesh timeseries output\n",
        "# Get mesh areas from previous code cell\n",
        "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
        "\n",
        "mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n",
        "timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:17.879028Z",
          "iopub.status.busy": "2025-11-17T17:54:17.878725Z",
          "iopub.status.idle": "2025-11-17T17:54:17.892957Z",
          "shell.execute_reply": "2025-11-17T17:54:17.892424Z"
        }
      },
      "outputs": [],
      "source": [
        "print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n",
        "timeseries_da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:17.895069Z",
          "iopub.status.busy": "2025-11-17T17:54:17.894661Z",
          "iopub.status.idle": "2025-11-17T17:54:17.991634Z",
          "shell.execute_reply": "2025-11-17T17:54:17.991057Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get mesh cells timeseries output\n",
        "cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:17.993977Z",
          "iopub.status.busy": "2025-11-17T17:54:17.993611Z",
          "iopub.status.idle": "2025-11-17T17:54:18.000048Z",
          "shell.execute_reply": "2025-11-17T17:54:17.999453Z"
        }
      },
      "outputs": [],
      "source": [
        "print(\"\\nMesh Cells Timeseries Output:\")\n",
        "cells_timeseries_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.002295Z",
          "iopub.status.busy": "2025-11-17T17:54:18.001971Z",
          "iopub.status.idle": "2025-11-17T17:54:18.071710Z",
          "shell.execute_reply": "2025-11-17T17:54:18.071156Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get mesh faces timeseries output\n",
        "faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.074007Z",
          "iopub.status.busy": "2025-11-17T17:54:18.073754Z",
          "iopub.status.idle": "2025-11-17T17:54:18.083499Z",
          "shell.execute_reply": "2025-11-17T17:54:18.083106Z"
        }
      },
      "outputs": [],
      "source": [
        "print(\"\\nMesh Faces Timeseries Output:\")\n",
        "faces_timeseries_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.085694Z",
          "iopub.status.busy": "2025-11-17T17:54:18.085418Z",
          "iopub.status.idle": "2025-11-17T17:54:18.098450Z",
          "shell.execute_reply": "2025-11-17T17:54:18.098013Z"
        }
      },
      "outputs": [],
      "source": [
        "# Convert all face velocities and face flow values to positive for further calculations\n",
        "# We have visually confirmed for this model that all flow is moving in the same direction\n",
        "\n",
        "# Function to process and convert face data to positive values\n",
        "def convert_to_positive_values(faces_timeseries_ds, cells_timeseries_ds):\n",
        "    \"\"\"\n",
        "    Convert face velocities and flows to positive values while maintaining their relationships.\n",
        "    \n",
        "    Args:\n",
        "        faces_timeseries_ds (xarray.Dataset): Dataset containing face timeseries data\n",
        "        cells_timeseries_ds (xarray.Dataset): Dataset containing cell timeseries data\n",
        "        \n",
        "    Returns:\n",
        "        xarray.Dataset: Modified dataset with positive values\n",
        "    \"\"\"\n",
        "    # Get the face velocity variable (always available)\n",
        "    face_velocity = faces_timeseries_ds['face_velocity']\n",
        "    \n",
        "    # Calculate the sign of the velocity to maintain flow direction relationships\n",
        "    velocity_sign = xr.where(face_velocity >= 0, 1, -1)\n",
        "    \n",
        "    # Convert velocities to absolute values\n",
        "    faces_timeseries_ds['face_velocity'] = abs(face_velocity)\n",
        "    \n",
        "    # Check if face_flow exists and process it if available\n",
        "    if 'face_flow' in faces_timeseries_ds:\n",
        "        face_flow = faces_timeseries_ds['face_flow']\n",
        "        faces_timeseries_ds['face_flow'] = abs(face_flow)\n",
        "        print(\"Face flow data processed.\")\n",
        "    else:\n",
        "        print(\"Note: face_flow not available in this dataset (depends on HEC-RAS output settings)\")\n",
        "    \n",
        "    # Store the original sign as a new variable for reference\n",
        "    faces_timeseries_ds['velocity_direction'] = velocity_sign\n",
        "    \n",
        "    print(\"Conversion to positive values complete.\")\n",
        "    print(f\"Number of faces processed: {len(faces_timeseries_ds.face_id)}\")\n",
        "    print(f\"Available variables: {list(faces_timeseries_ds.data_vars)}\")\n",
        "    \n",
        "    return faces_timeseries_ds, cells_timeseries_ds\n",
        "\n",
        "# Convert the values in our datasets\n",
        "faces_timeseries_ds_positive, cells_timeseries_ds_positive = convert_to_positive_values(\n",
        "    faces_timeseries_ds, \n",
        "    cells_timeseries_ds\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.100444Z",
          "iopub.status.busy": "2025-11-17T17:54:18.100274Z",
          "iopub.status.idle": "2025-11-17T17:54:18.104071Z",
          "shell.execute_reply": "2025-11-17T17:54:18.103680Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "# Function to process faces for a single profile line\n",
        "def process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds):\n",
        "    face_ids = faces['face_id'].tolist()\n",
        "    \n",
        "    # Extract relevant data for these faces\n",
        "    face_velocities = faces_timeseries_ds['face_velocity'].sel(face_id=face_ids)\n",
        "    \n",
        "    # Build dataset with available data\n",
        "    data_vars = {'face_velocity': face_velocities}\n",
        "    \n",
        "    # Check if face_flow exists and add it if available\n",
        "    if 'face_flow' in faces_timeseries_ds:\n",
        "        face_flows = faces_timeseries_ds['face_flow'].sel(face_id=face_ids)\n",
        "        data_vars['face_flow'] = face_flows\n",
        "    \n",
        "    # Create a new dataset with calculated results\n",
        "    results_ds = xr.Dataset(data_vars)\n",
        "    \n",
        "    # Convert to dataframe for easier manipulation\n",
        "    results_df = results_ds.to_dataframe().reset_index()\n",
        "    \n",
        "    # Add profile name and face order\n",
        "    results_df['profile_name'] = profile_name\n",
        "    results_df['face_order'] = results_df.groupby('time')['face_id'].transform(lambda x: pd.factorize(x)[0])\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Vave = Sum Qn / Sum An for each profile line\n",
        "where Vave = the summation of face flow / flow area for all the faces in the profile line\n",
        "\n",
        "Then, save the results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.106234Z",
          "iopub.status.busy": "2025-11-17T17:54:18.105973Z",
          "iopub.status.idle": "2025-11-17T17:54:18.610270Z",
          "shell.execute_reply": "2025-11-17T17:54:18.609734Z"
        }
      },
      "outputs": [],
      "source": [
        "# Process all profile lines\n",
        "all_results = []\n",
        "for profile_name, faces in faces_near_profile_lines.items():\n",
        "    profile_results = process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds)\n",
        "    all_results.append(profile_results)\n",
        "\n",
        "# Combine results from all profile lines\n",
        "combined_results_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Display the first few rows of the combined results\n",
        "combined_results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.612506Z",
          "iopub.status.busy": "2025-11-17T17:54:18.612332Z",
          "iopub.status.idle": "2025-11-17T17:54:18.625988Z",
          "shell.execute_reply": "2025-11-17T17:54:18.625554Z"
        }
      },
      "outputs": [],
      "source": [
        "profile_time_series = {}\n",
        "\n",
        "# Iterate through each profile line and extract its corresponding data\n",
        "for profile_name, faces_gdf in faces_near_profile_lines.items():\n",
        "    # Get the list of face_ids for this profile line\n",
        "    face_ids = faces_gdf['face_id'].tolist()\n",
        "    \n",
        "    # Filter the combined_results_df for these face_ids\n",
        "    profile_df = combined_results_df[combined_results_df['face_id'].isin(face_ids)].copy()\n",
        "    \n",
        "    # Add the profile name as a column\n",
        "    profile_df['profile_name'] = profile_name\n",
        "    \n",
        "    # Reset index for cleanliness\n",
        "    profile_df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # Store in the dictionary\n",
        "    profile_time_series[profile_name] = profile_df\n",
        "    \n",
        "    # Display a preview\n",
        "    print(f\"\\nTime Series DataFrame for {profile_name}:\")\n",
        "    profile_df\n",
        "\n",
        "# Optionally, display all profile names\n",
        "print(\"\\nProfile Lines Processed:\")\n",
        "profile_time_series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.628203Z",
          "iopub.status.busy": "2025-11-17T17:54:18.628006Z",
          "iopub.status.idle": "2025-11-17T17:54:18.636639Z",
          "shell.execute_reply": "2025-11-17T17:54:18.636261Z"
        }
      },
      "outputs": [],
      "source": [
        "all_profiles_df = pd.concat(profile_time_series.values(), ignore_index=True)\n",
        "\n",
        "# Display the combined dataframe\n",
        "print(\"Combined Time Series DataFrame for All Profiles:\")\n",
        "all_profiles_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.638602Z",
          "iopub.status.busy": "2025-11-17T17:54:18.638332Z",
          "iopub.status.idle": "2025-11-17T17:54:18.641759Z",
          "shell.execute_reply": "2025-11-17T17:54:18.641396Z"
        }
      },
      "outputs": [],
      "source": [
        "# Check if we have the necessary variables\n",
        "print(\"Available variables:\")\n",
        "print(\"profile_time_series:\", 'profile_time_series' in locals())\n",
        "print(\"faces_near_profile_lines:\", 'faces_near_profile_lines' in locals())\n",
        "print(\"profile_averages:\", 'profile_averages' in locals())\n",
        "\n",
        "# Look at the structure of profile_time_series\n",
        "if 'profile_time_series' in locals():\n",
        "    for name, df in profile_time_series.items():\n",
        "        print(f\"\\nColumns in {name}:\")\n",
        "        print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.643977Z",
          "iopub.status.busy": "2025-11-17T17:54:18.643663Z",
          "iopub.status.idle": "2025-11-17T17:54:18.647883Z",
          "shell.execute_reply": "2025-11-17T17:54:18.647312Z"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_discharge_weighted_velocity(profile_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate discharge-weighted average velocity for a profile line\n",
        "    Vw = Sum(|Qi|*Vi)/Sum(|Qi|) where Qi is face flow and Vi is face velocity\n",
        "\n",
        "    If face_flow is not available, falls back to simple average velocity.\n",
        "    \"\"\"\n",
        "    print(\"Calculating weighted velocity...\")\n",
        "    print(f\"Input DataFrame columns: {list(profile_df.columns)}\")\n",
        "\n",
        "    has_face_flow = 'face_flow' in profile_df.columns\n",
        "    if not has_face_flow:\n",
        "        print(\"Note: face_flow not available, using simple average velocity instead\")\n",
        "\n",
        "    # Calculate weighted velocity for each timestep\n",
        "    weighted_velocities = []\n",
        "    for time in profile_df['time'].unique():\n",
        "        time_data = profile_df[profile_df['time'] == time]\n",
        "        abs_velocities = np.abs(time_data['face_velocity'])\n",
        "\n",
        "        if has_face_flow:\n",
        "            # Discharge-weighted velocity\n",
        "            abs_flows = np.abs(time_data['face_flow'])\n",
        "            if abs_flows.sum() > 0:\n",
        "                weighted_vel = (abs_flows * abs_velocities).sum() / abs_flows.sum()\n",
        "            else:\n",
        "                weighted_vel = abs_velocities.mean()\n",
        "        else:\n",
        "            # Simple average velocity\n",
        "            weighted_vel = abs_velocities.mean()\n",
        "\n",
        "        weighted_velocities.append({\n",
        "            'time': time,\n",
        "            'weighted_velocity': weighted_vel\n",
        "        })\n",
        "\n",
        "    weighted_df = pd.DataFrame(weighted_velocities)\n",
        "    print(f\"Calculated velocities:\\n{weighted_df.head()}\")\n",
        "    return weighted_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:18.650302Z",
          "iopub.status.busy": "2025-11-17T17:54:18.650108Z",
          "iopub.status.idle": "2025-11-17T17:54:20.595754Z",
          "shell.execute_reply": "2025-11-17T17:54:20.595210Z"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate for each profile line\n",
        "for profile_name, profile_df in profile_time_series.items():\n",
        "    print(f\"\\nProcessing profile: {profile_name}\")\n",
        "\n",
        "    # Calculate discharge-weighted velocity\n",
        "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
        "    \n",
        "    print(\"Weighted velocities calculated.\")\n",
        "    # Get ordered faces for this profile\n",
        "    ordered_faces = faces_near_profile_lines[profile_name]\n",
        "    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n",
        "    \n",
        "    print(\"Converted time to datetime format.\")\n",
        "\n",
        "    # Get ordered faces for this profile\n",
        "    ordered_faces = faces_near_profile_lines[profile_name]\n",
        "    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n",
        "    \n",
        "    # Save dataframes in the output directory\n",
        "    output_file = ras.project_folder / f\"{profile_name}_discharge_weighted_velocity.csv\"\n",
        "    weighted_velocities.to_csv(output_file, index=False)\n",
        "    print(f\"Saved weighted velocities to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:20.597954Z",
          "iopub.status.busy": "2025-11-17T17:54:20.597777Z",
          "iopub.status.idle": "2025-11-17T17:54:24.594148Z",
          "shell.execute_reply": "2025-11-17T17:54:24.593672Z"
        }
      },
      "outputs": [],
      "source": [
        "# Create plots comparing discharge-weighted velocity and simple average for each profile line\n",
        "for profile_name, profile_df in profile_time_series.items():\n",
        "    \n",
        "    print(f\"\\nGenerating comparison plot for profile: {profile_name}\")\n",
        "    \n",
        "    # Calculate discharge-weighted velocity\n",
        "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
        "    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n",
        "    \n",
        "    # Calculate simple average velocity for each timestep\n",
        "    simple_averages = profile_df.groupby('time')['face_velocity'].mean().reset_index()\n",
        "    simple_averages['time'] = pd.to_datetime(simple_averages['time'])\n",
        "    \n",
        "    # Create figure for comparison plot\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    \n",
        "    # Plot individual face velocities with thin lines\n",
        "    for face_id in profile_df['face_id'].unique():\n",
        "        face_data = profile_df[profile_df['face_id'] == face_id]\n",
        "        plt.plot(face_data['time'], \n",
        "                face_data['face_velocity'], \n",
        "                alpha=0.8,  # More transparent\n",
        "                linewidth=0.3,  # Thinner line\n",
        "                color='gray',  # Consistent color\n",
        "                label=f'Face ID {face_id}')\n",
        "        \n",
        "        # Find and annotate peak value for each face\n",
        "        peak_idx = face_data['face_velocity'].idxmax()\n",
        "        peak_time = face_data.loc[peak_idx, 'time']\n",
        "        peak_vel = face_data.loc[peak_idx, 'face_velocity']\n",
        "        plt.annotate(f'{peak_vel:.2f} ({face_id})',\n",
        "                    xy=(peak_time, peak_vel),\n",
        "                    xytext=(10, 10),\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=8,\n",
        "                    alpha=0.5)\n",
        "    \n",
        "    # Plot discharge-weighted velocity\n",
        "    plt.plot(weighted_velocities['time'], \n",
        "            weighted_velocities['weighted_velocity'], \n",
        "            color='red', \n",
        "            alpha=1.0, \n",
        "            linewidth=2,\n",
        "            label='Discharge-Weighted Velocity')\n",
        "    \n",
        "    # Find and annotate peak weighted velocity\n",
        "    peak_idx = weighted_velocities['weighted_velocity'].idxmax()\n",
        "    peak_time = weighted_velocities.loc[peak_idx, 'time']\n",
        "    peak_vel = weighted_velocities.loc[peak_idx, 'weighted_velocity']\n",
        "    plt.annotate(f'Peak Weighted: {peak_vel:.2f}',\n",
        "                xy=(peak_time, peak_vel),\n",
        "                xytext=(10, 10),\n",
        "                textcoords='offset points',\n",
        "                color='red',\n",
        "                fontweight='bold')\n",
        "    \n",
        "    # Plot simple average\n",
        "    plt.plot(simple_averages['time'], \n",
        "            simple_averages['face_velocity'], \n",
        "            color='blue', \n",
        "            alpha=0.5, \n",
        "            linewidth=1,\n",
        "            linestyle='--',\n",
        "            label='Simple Average')\n",
        "    \n",
        "    # Find and annotate peak simple average\n",
        "    peak_idx = simple_averages['face_velocity'].idxmax()\n",
        "    peak_time = simple_averages.loc[peak_idx, 'time']\n",
        "    peak_vel = simple_averages.loc[peak_idx, 'face_velocity']\n",
        "    plt.annotate(f'Peak Average: {peak_vel:.2f}',\n",
        "                xy=(peak_time, peak_vel),\n",
        "                xytext=(10, -10),\n",
        "                textcoords='offset points',\n",
        "                color='blue',\n",
        "                fontweight='bold')\n",
        "    \n",
        "    # Configure plot\n",
        "    plt.title(f'Velocity Comparison for {profile_name} \\nIndividual Face Velocities vs Simple Average Velocity vs Discharge-Weighted Average Velocity')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Velocity (ft/s)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add legend with better placement\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    # Adjust layout to accommodate legend and stats\n",
        "    plt.subplots_adjust(right=0.8)\n",
        "    \n",
        "    # Save plot to file\n",
        "    plot_file = ras.project_folder / f\"{profile_name}_velocity_comparison.png\"\n",
        "    plt.savefig(plot_file, bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed comparison\n",
        "    print(f\"\\nVelocity Comparison for {profile_name} \\nIndividual Face Velocities vs Simple Average Velocity vs Discharge-Weighted Average Velocity\")\n",
        "    print(f\"Number of faces: {profile_df['face_id'].nunique()}\")\n",
        "    print(\"\\nDischarge-Weighted Velocity Statistics:\")\n",
        "    print(f\"Mean: {weighted_velocities['weighted_velocity'].mean():.2f} ft/s\")\n",
        "    print(f\"Max: {weighted_velocities['weighted_velocity'].max():.2f} ft/s\")\n",
        "    print(f\"Min: {weighted_velocities['weighted_velocity'].min():.2f} ft/s\")\n",
        "    print(\"\\nSimple Average Velocity Statistics:\")\n",
        "    print(f\"Mean: {simple_averages['face_velocity'].mean():.2f} ft/s\")\n",
        "    print(f\"Max: {simple_averages['face_velocity'].max():.2f} ft/s\")\n",
        "    print(f\"Min: {simple_averages['face_velocity'].min():.2f} ft/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:54:24.596452Z",
          "iopub.status.busy": "2025-11-17T17:54:24.596251Z",
          "iopub.status.idle": "2025-11-17T17:54:27.147137Z",
          "shell.execute_reply": "2025-11-17T17:54:27.146462Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Extracting mesh cell faces near profile lines\n",
        "print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n",
        "\n",
        "# Get mesh cell faces using HdfMesh class\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces DataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces\n",
        "\n",
        "# Load the GeoJSON file for profile lines\n",
        "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
        "profile_lines_gdf = gpd.read_file(geojson_path)\n",
        "\n",
        "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
        "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
        "\n",
        "# Initialize a dictionary to store faces near each profile line\n",
        "faces_near_profile_lines = {}\n",
        "\n",
        "# Define distance threshold (10 ft converted to meters)\n",
        "distance_threshold = 10\n",
        "angle_threshold = 60  # degrees\n",
        "\n",
        "# Function to calculate the smallest angle between two lines or line segments.\n",
        "def calculate_angle(line):\n",
        "    if isinstance(line, LineString):\n",
        "        x_diff = line.xy[0][-1] - line.xy[0][0]\n",
        "        y_diff = line.xy[1][-1] - line.xy[1][0]\n",
        "    else:\n",
        "        x_diff = line[1][0] - line[0][0]\n",
        "        y_diff = line[1][1] - line[0][1]\n",
        "    \n",
        "    angle = np.degrees(np.arctan2(y_diff, x_diff))\n",
        "    return angle % 360 if angle >= 0 else (angle + 360) % 360\n",
        "\n",
        "# Function to break line into segments\n",
        "def break_line_into_segments(line, segment_length):\n",
        "    segments = []\n",
        "    segment_angles = []\n",
        "    \n",
        "    distances = np.arange(0, line.length, segment_length)\n",
        "    if distances[-1] != line.length:\n",
        "        distances = np.append(distances, line.length)\n",
        "        \n",
        "    for i in range(len(distances)-1):\n",
        "        point1 = line.interpolate(distances[i])\n",
        "        point2 = line.interpolate(distances[i+1])\n",
        "        segment = LineString([point1, point2])\n",
        "        segments.append(segment)\n",
        "        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n",
        "        \n",
        "    return segments, segment_angles\n",
        "\n",
        "# Function to calculate angle difference accounting for 180 degree equivalence\n",
        "def angle_difference(angle1, angle2):\n",
        "    diff = abs(angle1 - angle2) % 180\n",
        "    return min(diff, 180 - diff)\n",
        "\n",
        "# Function to order faces along profile line\n",
        "def order_faces_along_profile(profile_line, faces_gdf):\n",
        "    profile_start = Point(profile_line.coords[0])\n",
        "    \n",
        "    faces_with_dist = []\n",
        "    for idx, face in faces_gdf.iterrows():\n",
        "        face_start = Point(face.geometry.coords[0])\n",
        "        dist = profile_start.distance(face_start)\n",
        "        faces_with_dist.append((idx, dist))\n",
        "    \n",
        "    faces_with_dist.sort(key=lambda x: x[1])\n",
        "    return [x[0] for x in faces_with_dist]\n",
        "\n",
        "# Function to combine ordered faces into single linestring\n",
        "def combine_faces_to_linestring(ordered_faces_gdf):\n",
        "    coords = []\n",
        "    for _, face in ordered_faces_gdf.iterrows():\n",
        "        if not coords:  # First face - add all coordinates\n",
        "            coords.extend(list(face.geometry.coords))\n",
        "        else:  # Subsequent faces - add only end coordinate\n",
        "            coords.append(face.geometry.coords[-1])\n",
        "    return LineString(coords)\n",
        "\n",
        "# Initialize GeoDataFrame for final profile-to-faceline results\n",
        "profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n",
        "\n",
        "# Iterate through each profile line\n",
        "for index, profile_line in profile_lines_gdf.iterrows():\n",
        "    profile_geom = profile_line.geometry\n",
        "    \n",
        "    # Break profile line into segments\n",
        "    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n",
        "    \n",
        "    # Initialize set to store nearby faces\n",
        "    nearby_faces = set()\n",
        "    \n",
        "    # For each face, check distance to segments and angle difference\n",
        "    for face_idx, face in mesh_cell_faces.iterrows():\n",
        "        face_geom = face.geometry\n",
        "        \n",
        "        if isinstance(face_geom, LineString):\n",
        "            face_angle = calculate_angle(face_geom)\n",
        "            \n",
        "            for segment, segment_angle in zip(segments, segment_angles):\n",
        "                if face_geom.distance(segment) <= distance_threshold:\n",
        "                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n",
        "                        nearby_faces.add(face_idx)\n",
        "                        break\n",
        "    \n",
        "    # Convert the set of indices back to a GeoDataFrame\n",
        "    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n",
        "    \n",
        "    # Order faces along profile line\n",
        "    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n",
        "    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n",
        "    \n",
        "    # Combine ordered faces into single linestring\n",
        "    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n",
        "    \n",
        "    # Add to profile_to_faceline GeoDataFrame\n",
        "    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n",
        "                               'geometry': [combined_linestring]}, \n",
        "                              crs=profile_lines_gdf.crs)\n",
        "    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n",
        "    \n",
        "    # Store the ordered faces in the dictionary\n",
        "    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(24, 16))\n",
        "\n",
        "# Plot all mesh cell faces in light blue\n",
        "mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n",
        "\n",
        "# Plot selected faces for each profile line with numbers and velocities\n",
        "colors = ['red', 'green', 'blue']\n",
        "for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n",
        "    if not faces.empty:\n",
        "        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n",
        "        \n",
        "        # Get velocity data for this profile from profile_time_series\n",
        "        profile_data = profile_time_series[profile_name]\n",
        "        \n",
        "        # Add face_id above and peak velocity below for each face\n",
        "        for idx, face in faces.iterrows():\n",
        "            midpoint = face.geometry.interpolate(0.5, normalized=True)\n",
        "            \n",
        "            # Get peak velocity for this face\n",
        "            face_velocities = profile_data[profile_data['face_id'] == face['face_id']]['face_velocity']\n",
        "            peak_velocity = face_velocities.max() if not face_velocities.empty else 0.0\n",
        "            # Add face_id above the face\n",
        "            ax.text(midpoint.x, midpoint.y + 50,  # Adjust the +50 offset as needed\n",
        "                   f\"{face['face_id']}\", \n",
        "                   color=color, \n",
        "                   fontweight='bold',\n",
        "                   fontsize=8,\n",
        "                   ha='center', \n",
        "                   va='bottom')\n",
        "            \n",
        "            # Add peak velocity below the face\n",
        "            ax.text(midpoint.x, midpoint.y - 50,  # Adjust the -50 offset as needed\n",
        "                   f\"{peak_velocity:.2f}fps\", \n",
        "                   color=color, \n",
        "                   fontweight='bold',\n",
        "                   fontsize=6,\n",
        "                   ha='center', \n",
        "                   va='top')\n",
        "\n",
        "\n",
        "# Plot the combined linestrings\n",
        "profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n",
        "                        linestyle='--', label='Combined Face Lines')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Easting')\n",
        "ax.set_ylabel('Northing')\n",
        "ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile\\nFace ID and Peak Face Velocity Shown')\n",
        "\n",
        "# Add grid and legend\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nOriginal ordered faces near profile lines:\")\n",
        "faces_near_profile_lines\n",
        "\n",
        "print(\"\\nCombined profile-to-faceline results:\")\n",
        "profile_to_faceline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE:  We are using the face normal velocity that is available in the HDF.  This will only be accurate if you pick cell faces that are perpendicular to flow.  Depending on the application, a more robust calculation may be required. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\14_fluvial_pluvial_delineation.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delineate Fluvial and Pluvial Areas using RAS-Commander\n",
        "\n",
        "We will leverage the HEC RAS Summary Outputs to delineate the Fluvial and Pluvial Areas\n",
        "\n",
        "Maximum Water Surface Elevation (WSEL) for each cell is recorded, along with the timestamps of when the maximum WSEL occurs.\n",
        "\n",
        "By locating adjacent cells with dissimilar timestamps, we can delineate the Fluvial and Pluvial Areas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A note about datframe types: \n",
        "\n",
        "Information from the HEC-RAS plan files are generally dataframes.  The text file interface is for the 32-bit side of HEC-RAS and all spatial data is most easily accessed in the HDF files.  This includes plan_df, geom_df, hdf_paths_df\n",
        "\n",
        "Geometry elements (Mesh Faces and Nodes) are provided as Geodataframes (cell_polygons_gdf, boundary_gdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:29 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\n"
          ]
        }
      ],
      "source": [
        "# Download the BaldEagleCrkMulti2D project from HEC and run plan 06\n",
        "\n",
        "# Define the path to the BaldEagleCrkMulti2D project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "import logging\n",
        "\n",
        "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"06\"\n",
        "\n",
        "    # Update the run flags in the plan file\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,  # Run HTab\n",
        "        unsteady_flow_simulation=True,  # Run UNet\n",
        "        post_processor=True,  # Run PostProcess\n",
        "        floodplain_mapping=False,  # Run RASMapper\n",
        "    )\n",
        "\n",
        "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
        "    if success_bald_eagle:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    plan_number = 6\n",
        "your_project_path = bald_eagle_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan DataFrame for bald_eagle project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>13</td>\\n', '      <td>07</td>\\n', '      <td>06</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.10</td>\\n', '      <td>PMF Multi 2D</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>30SEC</td>\\n', '      <td>30MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>193</td>\\n', '      <td>None</td>\\n', '      <td>06</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>07</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>15</td>\\n', '      <td>12</td>\\n', '      <td>08</td>\\n', '      <td>1d-2D Dambreak Refined Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>1D-2D Refined Grid</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>20SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>None</td>\\n', '      <td>08</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>12</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>17</td>\\n', '      <td>09</td>\\n', '      <td>10</td>\\n', '      <td>2D to 1D No Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>2D to 1D No Dam</td>\\n', '      <td>01JAN1999,1200,06JAN1999,1200</td>\\n', '      <td>1MIN</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>Upstream2D</td>\\n', '      <td>None</td>\\n', '      <td>10</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>09</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>18</td>\\n', '      <td>10</td>\\n', '      <td>11</td>\\n', '      <td>2D to 2D Run</td>\\n', '      <td>5.00</td>\\n', '      <td>2D to 2D Run</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>20SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>None</td>\\n', '      <td>11</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>10</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>19</td>\\n', '      <td>11</td>\\n', '      <td>12</td>\\n', '      <td>SA to 2D Dam Break Run</td>\\n', '      <td>5.00</td>\\n', '      <td>SA to 2D Dam Break</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>20SEC</td>\\n', '      <td>10MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>None</td>\\n', '      <td>12</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>11</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['   plan_number unsteady_number geometry_number  \\\\\\n', '0           13              07              06   \\n', '1           15              12              08   \\n', '2           17              09              10   \\n', '3           18              10              11   \\n', '4           19              11              12   \\n', '5           03              13              09   \\n', '6           04              01              13   \\n', '7           02              01              01   \\n', '8           01              01              01   \\n', '9           05              02              03   \\n', '10          06              03              09   \\n', '\\n', '                                 Plan Title Program Version  \\\\\\n', '0                   PMF with Multi 2D Areas            5.10   \\n', '1               1d-2D Dambreak Refined Grid            5.10   \\n', '2                           2D to 1D No Dam            5.00   \\n', '3                              2D to 2D Run            5.0\n...\n[Output truncated, 7422 characters total]"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
        "\n",
        "# Display plan_df for bald_eagle project\n",
        "print(\"Plan DataFrame for bald_eagle project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Geometry DataFrame for the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>geom_file</th>\\n', '      <th>geom_number</th>\\n', '      <th>full_path</th>\\n', '      <th>hdf_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>g06</td>\\n', '      <td>06</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>g08</td>\\n', '      <td>08</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>g10</td>\\n', '      <td>10</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>g11</td>\\n', '      <td>11</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>g12</td>\\n', '      <td>12</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['  geom_file geom_number                                          full_path  \\\\\\n', '0       g06          06  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '1       g08          08  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '2       g10          10  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '3       g11          11  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '4       g12          12  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '5       g09          09  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '6       g13          13  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '7       g01          01  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '8       g03          03  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '9       g02          02  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '\\n', '                                            hdf_path  \\n', '0  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '1  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '2  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '3  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '4  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '5  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '6  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '7  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '8  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '9  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  ']"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nGeometry DataFrame for the project:\")\n",
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Unsteady DataFrame for the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>full_path</th>\\n', '      <th>Flow Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Use Restart</th>\\n', '      <th>Precipitation Mode</th>\\n', '      <th>Wind Mode</th>\\n', '      <th>Met BC=Precipitation|Mode</th>\\n', '      <th>Met BC=Evapotranspiration|Mode</th>\\n', '      <th>Met BC=Precipitation|Expanded View</th>\\n', '      <th>Met BC=Precipitation|Constant Units</th>\\n', '      <th>Met BC=Precipitation|Gridded Source</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>07</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>08</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>PMF for Upstream 2D</td>\\n', '      <td>4.20</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>09</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Upstream 2D</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>10</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>1972 Flood Event - 2D to 2D Run</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>11</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>1972 Flood Event - SA to 2D Run</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['  unsteady_number                                          full_path  \\\\\\n', '0              07  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '1              08  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '2              09  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '3              10  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '4              11  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '5              12  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '6              13  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '7              01  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '8              02  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '9              03  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '\\n', '                             Flow Title Program Version Use Restart  \\\\\\n', '0               P\n...\n[Output truncated, 4267 characters total]"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nUnsteady DataFrame for the project:\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Boundary Conditions DataFrame for the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>boundary_condition_number</th>\\n', '      <th>river_reach_name</th>\\n', '      <th>river_station</th>\\n', '      <th>storage_area_name</th>\\n', '      <th>pump_station_name</th>\\n', '      <th>bc_type</th>\\n', '      <th>hydrograph_type</th>\\n', '      <th>Interval</th>\\n', '      <th>DSS File</th>\\n', '      <th>...</th>\\n', '      <th>Flow Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Use Restart</th>\\n', '      <th>Precipitation Mode</th>\\n', '      <th>Wind Mode</th>\\n', '      <th>Met BC=Precipitation|Mode</th>\\n', '      <th>Met BC=Evapotranspiration|Mode</th>\\n', '      <th>Met BC=Precipitation|Expanded View</th>\\n', '      <th>Met BC=Precipitation|Constant Units</th>\\n', '      <th>Met BC=Precipitation|Gridded Source</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>07</td>\\n', '      <td>1</td>\\n', '      <td>Bald Eagle Cr.</td>\\n', '      <td>Lock Haven</td>\\n', '      <td>137520</td>\\n', '      <td></td>\\n', '      <td>Flow Hydrograph</td>\\n', '      <td>Flow Hydrograph</td>\\n', '      <td>1HOUR</td>\\n', '      <td>Bald_Eagle_Creek.dss</td>\\n', '      <td>...</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>07</td>\\n', '      <td>2</td>\\n', '      <td>Bald Eagle Cr.</td>\\n', '      <td>Lock Haven</td>\\n', '      <td>81454</td>\\n', '      <td></td>\\n', '      <td>Gate Opening</td>\\n', '      <td>None</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>...</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>07</td>\\n', '      <td>3</td>\\n', '      <td>Bald Eagle Cr.</td>\\n', '      <td>Lock Haven</td>\\n', '      <td>28519</td>\\n', '      <td></td>\\n', '      <td>Lateral Inflow Hydrograph</td>\\n', '      <td>Lateral Inflow Hydrograph</td>\\n', '      <td>1HOUR</td>\\n', '      <td>Bald_Eagle_Creek.dss</td>\\n', '      <td>...</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>07</td>\\n', '      <td>4</td>\\n', '      <td>Bald Eagle Cr.</td>\\n', '      <td>Lock Haven</td>\\n', '      <td>1</td>\\n', '      <td></td>\\n', '      <td>Lateral Inflow Hydrograph</td>\\n', '      <td>Lateral Inflow Hydrograph</td>\\n', '      <td>1HOUR</td>\\n', '      <td>NaN</td>\\n', '      <td>...</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>07</td>\\n', '      <td>5</td>\\n', '      <td>Bald Eagle Cr.</td>\\n', '      <td>Lock Haven</td>\\n', '      <td>136948</td>\\n', '      <td>82303</td>\\n', '      <td>Uniform Lateral Inflow Hydrograph</td>\\n', '      <td>Uniform Lateral Inflow Hydrograph</td>\\n', '      <td>1HOUR</td>\\n', '      <td>Bald_Eagle_Creek.dss</td>\\n', '      <td>...</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.00</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['   unsteady_number  boundary_condition_number river_reach_name river_station  \\\\\\n', '0               07                          1   Bald Eagle Cr.    Lock Haven   \\n', '1               07                          2   Bald Eagle Cr.    Lock Haven   \\n', '2               07                          3   Bald Eagle Cr.    Lock Haven   \\n', '3               07                          4   Bald Eagle Cr.    Lock Haven   \\n', '4               07                          5   Bald Eagle Cr.    Lock Haven   \\n', '5               07                          6   Bald Eagle Cr.    Lock Haven   \\n', '6               07                          7   Bald Eagle Cr.    Lock Haven   \\n', '7               07                          8   Bald Eagle Cr.    Lock Haven   \\n', '8               07                          9   Bald Eagle Cr.    Lock Haven   \\n', '9               07                         10   Bald Eagle Cr.    Lock Haven   \\n', '10              08                          1   Bald Eagle Cr.\n...\n[Output truncated, 29090 characters total]"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nBoundary Conditions DataFrame for the project:\")\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "06\n"
          ]
        }
      ],
      "source": [
        "plan_number = \"06\"\n",
        "print(plan_number)\n",
        "\n",
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "[\"'C:\\\\\\\\GH\\\\\\\\ras-commander\\\\\\\\examples\\\\\\\\example_projects\\\\\\\\BaldEagleCrkMulti2D\\\\\\\\BaldEagleDamBrk.p06.hdf'\"]"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the geometry HDF path\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "[\"'C:\\\\\\\\GH\\\\\\\\ras-commander\\\\\\\\examples\\\\\\\\example_projects\\\\\\\\BaldEagleCrkMulti2D\\\\\\\\BaldEagleDamBrk.g09.hdf'\"]"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "geom_hdf_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Plan HDF path for Plan 06: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "Geometry HDF path for Plan 06: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
        "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fluvial Pluvial Delineation using RAS-Commander \n",
        "\n",
        "Using the Maximum WSE Results layer, which contains the maximum water surface and time stamp of the maximum water surface, mesh cell faces are categorized.  If the difference in time (delta_t) in hours is greater than the (user defined, default 12) duration specified, that mesh cell face is added to the fluvial-pluvial boundary dataset. \n",
        "\n",
        "This is meant to provide a draft fluvial-pluvial boundary for floodplain analysis, to the extent it can be derived directly from the HEC-RAS results files. \n",
        "\n",
        "The function attempts to combine adjacent line segments to simplify the resulting geometry, but GIS cleanup and manual interpolation will be required to create a closed polygon boundary that could be used for further processing steps.  However, this approach does provide an efficient method for providing a draft boundary that is based on HEC-RAS's direct computations and mesh cell faces. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfResultsMesh - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfResultsMesh - INFO - Processing summary output for variable: Maximum Water Surface\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:29 - ras_commander.hdf.HdfResultsMesh - INFO - Processed 19597 rows of summary output data\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_ws_df\n",
            "         mesh_name  cell_id  maximum_water_surface maximum_water_surface_time  \\\n",
            "0      BaldEagleCr        0             704.054443        2018-09-10 18:00:00   \n",
            "1      BaldEagleCr        1             692.377991        2018-09-10 18:04:00   \n",
            "2      BaldEagleCr        2             671.183472        2018-09-10 18:13:20   \n",
            "3      BaldEagleCr        3             660.605469        2018-09-10 18:54:40   \n",
            "4      BaldEagleCr        4             660.586243        2018-09-10 18:55:20   \n",
            "...            ...      ...                    ...                        ...   \n",
            "19592  BaldEagleCr    19592               0.000000        2018-09-09 00:00:00   \n",
            "19593  BaldEagleCr    19593               0.000000        2018-09-09 00:00:00   \n",
            "19594  BaldEagleCr    19594               0.000000        2018-09-09 00:00:00   \n",
            "19595  BaldEagleCr    19595               0.000000        2018-09-09 00:00:00   \n",
            "19596  BaldEagleCr    19596               0.000000        2018-09-09 00:00:00   \n",
            "\n",
            "                             geometry  \n",
            "0              POINT (2083000 370750)  \n",
            "1              POINT (2083250 370750)  \n",
            "2              POINT (2083500 370750)  \n",
            "3              POINT (2083750 370750)  \n",
            "4              POINT (2084000 370750)  \n",
            "...                               ...  \n",
            "19592  POINT (1978423.032 300718.897)  \n",
            "19593  POINT (1973389.375 297311.928)  \n",
            "19594   POINT (1968834.79 295808.861)  \n",
            "19595  POINT (1966130.942 291879.395)  \n",
            "19596   POINT (1969660.046 289673.23)  \n",
            "\n",
            "[19597 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Using mesh_max_ws, get the cell coordinates and plot the max water surface as a map\n",
        "import matplotlib.pyplot as plt\n",
        "from ras_commander import HdfMesh\n",
        "from ras_commander import HdfResultsMesh\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Get mesh max water surface\n",
        "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)\n",
        "\n",
        "print(\"max_ws_df\")\n",
        "print(max_ws_df)\n",
        "\n",
        "# If you get an error here, you may have a pre-6.0 HDF.  Re-run in 6.x to generate a new results file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simulation Start Time: 2018-09-09 00:00:00\n",
            "Time Range: 120.0 hours\n",
            "\n",
            "Timing Statistics (hours since start):\n",
            "count    19597.000000\n",
            "mean        63.225584\n",
            "std         40.179644\n",
            "min          0.000000\n",
            "25%         34.000000\n",
            "50%         42.700000\n",
            "75%        115.677778\n",
            "max        120.000000\n",
            "Name: max_wsel_time, dtype: float64\n",
            "\n",
            "First few rows of the merged dataframe:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>mesh_name</th>\\n', '      <th>cell_id</th>\\n', '      <th>maximum_water_surface</th>\\n', '      <th>maximum_water_surface_time</th>\\n', '      <th>geometry</th>\\n', '      <th>x</th>\\n', '      <th>y</th>\\n', '      <th>max_wsel_time</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>0</td>\\n', '      <td>704.054443</td>\\n', '      <td>2018-09-10 18:00:00</td>\\n', '      <td>POINT (2083000 370750)</td>\\n', '      <td>2.083000e+06</td>\\n', '      <td>370750.000000</td>\\n', '      <td>2018-09-10 18:00:00</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>1</td>\\n', '      <td>692.377991</td>\\n', '      <td>2018-09-10 18:04:00</td>\\n', '      <td>POINT (2083250 370750)</td>\\n', '      <td>2.083250e+06</td>\\n', '      <td>370750.000000</td>\\n', '      <td>2018-09-10 18:04:00</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>2</td>\\n', '      <td>671.183472</td>\\n', '      <td>2018-09-10 18:13:20</td>\\n', '      <td>POINT (2083500 370750)</td>\\n', '      <td>2.083500e+06</td>\\n', '      <td>370750.000000</td>\\n', '      <td>2018-09-10 18:13:20</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>3</td>\\n', '      <td>660.605469</td>\\n', '      <td>2018-09-10 18:54:40</td>\\n', '      <td>POINT (2083750 370750)</td>\\n', '      <td>2.083750e+06</td>\\n', '      <td>370750.000000</td>\\n', '      <td>2018-09-10 18:54:40</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>4</td>\\n', '      <td>660.586243</td>\\n', '      <td>2018-09-10 18:55:20</td>\\n', '      <td>POINT (2084000 370750)</td>\\n', '      <td>2.084000e+06</td>\\n', '      <td>370750.000000</td>\\n', '      <td>2018-09-10 18:55:20</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['         mesh_name  cell_id  maximum_water_surface maximum_water_surface_time  \\\\\\n', '0      BaldEagleCr        0             704.054443        2018-09-10 18:00:00   \\n', '1      BaldEagleCr        1             692.377991        2018-09-10 18:04:00   \\n', '2      BaldEagleCr        2             671.183472        2018-09-10 18:13:20   \\n', '3      BaldEagleCr        3             660.605469        2018-09-10 18:54:40   \\n', '4      BaldEagleCr        4             660.586243        2018-09-10 18:55:20   \\n', '...            ...      ...                    ...                        ...   \\n', '19592  BaldEagleCr    19592               0.000000        2018-09-09 00:00:00   \\n', '19593  BaldEagleCr    19593               0.000000        2018-09-09 00:00:00   \\n', '19594  BaldEagleCr    19594               0.000000        2018-09-09 00:00:00   \\n', '19595  BaldEagleCr    19595               0.000000        2018-09-09 00:00:00   \\n', '19596  BaldEagleCr    19596               0.000000 \n...\n[Output truncated, 2376 characters total]"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the function to plot\n",
        "HdfResultsPlot.plot_results_max_wsel(max_ws_df)\n",
        "\n",
        "# Plot the time of maximum water surface elevation\n",
        "HdfResultsPlot.plot_results_max_wsel_time(max_ws_df)\n",
        "\n",
        "# Print the first few rows of the merged dataframe for verification\n",
        "print(\"\\nFirst few rows of the merged dataframe:\")\n",
        "max_ws_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfBase - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Projection from HDF\n",
            "Projection: PROJCS[\"NAD_1983_StatePlane_Pennsylvania_North_FIPS_3701_Feet\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic\"],PARAMETER[\"False_Easting\",1968500.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-77.75],PARAMETER[\"Standard_Parallel_1\",40.88333333333333],PARAMETER[\"Standard_Parallel_2\",41.95],PARAMETER[\"Latitude_Of_Origin\",40.16666666666666],UNIT[\"Foot_US\",0.3048006096012192]]\n"
          ]
        }
      ],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
        "if projection:\n",
        "    print(f\"Projection: {projection}\")\n",
        "else:\n",
        "    print(\"No projection information found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:30 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 6: Extracting Cell Polygons\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.g09.hdf\n"
          ]
        }
      ],
      "source": [
        "# Example: Extract Cell Polygons\n",
        "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
        "cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)\n",
        "\n",
        "\n",
        "# Call the function to plot cell polygons\n",
        "#cell_polygons_gdf = HdfFluvialPluvial.plot_cell_polygons(cell_polygons_gdf, projection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfFluvialPluvial - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfFluvialPluvial - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfFluvialPluvial - INFO - Getting cell polygons from HDF file...\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:31 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfFluvialPluvial - INFO - Getting maximum water surface data from HDF file...\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfResultsMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfResultsMesh - INFO - Processing summary output for variable: Maximum Water Surface\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfResultsMesh - INFO - Processed 19597 rows of summary output data\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfFluvialPluvial - INFO - Converting maximum water surface timestamps...\n",
            "2025-12-02 20:12:32 - ras_commander.hdf.HdfFluvialPluvial - INFO - Processing cell adjacencies...\n",
            "2025-12-02 20:12:34 - ras_commander.hdf.HdfFluvialPluvial - INFO - Extracting cell times from maximum water surface data...\n",
            "2025-12-02 20:12:34 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identifying boundary edges...\n",
            "2025-12-02 20:12:35 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identified 3201 boundary edges using delta_t of 72 hours.\n",
            "2025-12-02 20:12:35 - ras_commander.hdf.HdfFluvialPluvial - INFO - Creating final GeoDataFrame for boundaries...\n",
            "2025-12-02 20:12:35 - ras_commander.hdf.HdfFluvialPluvial - INFO - Boundary line calculation completed successfully.\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from shapely.geometry import LineString, Polygon, MultiLineString\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from rtree import index\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(plan_hdf_path, delta_t=72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boundary line length statistics:\n",
            "Max length: 441.63\n",
            "Min length: 1.57\n",
            "Average length: 242.54\n",
            "Median length: 250.00\n",
            "\n",
            "Boundary GeoDataFrame info:\n",
            "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
            "RangeIndex: 3201 entries, 0 to 3200\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype   \n",
            "---  ------    --------------  -----   \n",
            " 0   geometry  3201 non-null   geometry\n",
            "dtypes: geometry(1)\n",
            "memory usage: 25.1 KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Calculate statistics about the boundary line lengths\n",
        "boundary_lengths = boundary_gdf.geometry.length\n",
        "\n",
        "print(\"Boundary line length statistics:\")\n",
        "print(f\"Max length: {boundary_lengths.max():.2f}\")\n",
        "print(f\"Min length: {boundary_lengths.min():.2f}\")\n",
        "print(f\"Average length: {boundary_lengths.mean():.2f}\")\n",
        "print(f\"Median length: {boundary_lengths.median():.2f}\")\n",
        "\n",
        "# Print general information about the boundary GeoDataFrame\n",
        "print(\"\\nBoundary GeoDataFrame info:\")\n",
        "print(boundary_gdf.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the results\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
        "boundary_gdf.plot(ax=ax, color='red', linewidth=2)\n",
        "plt.title('Fluvial-Pluvial Boundary')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\billk_clb\\AppData\\Local\\Temp\\ipykernel_57556\\1399630256.py:10: UserWarning: The GeoDataFrame you are attempting to plot is empty. Nothing has been displayed.\n",
            "  filtered_boundary_gdf.plot(ax=ax, color='red', linewidth=2, label='Valid Boundaries')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "length_threshold = 3000 #in same units as X and Y coordinates\n",
        "\n",
        "# Filter out boundary lines below the length threshold\n",
        "filtered_boundary_gdf = boundary_gdf[boundary_lengths >= length_threshold]\n",
        "highlighted_boundary_gdf = boundary_gdf[boundary_lengths < length_threshold]\n",
        "\n",
        "# Visualize the results with highlighted boundaries below the threshold\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
        "filtered_boundary_gdf.plot(ax=ax, color='red', linewidth=2, label='Valid Boundaries')\n",
        "highlighted_boundary_gdf.plot(ax=ax, color='blue', linewidth=2, linestyle='--', label='Highlighted Boundaries Below Threshold')\n",
        "plt.title('Fluvial-Pluvial Boundary with Length Threshold')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfFluvialPluvial - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfFluvialPluvial - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfFluvialPluvial - INFO - Getting cell polygons from HDF file...\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:39 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:40 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:40 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:40 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfFluvialPluvial - INFO - Getting maximum water surface data from HDF file...\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfResultsMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfResultsMesh - INFO - Processing summary output for variable: Maximum Water Surface\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfResultsMesh - INFO - Processed 19597 rows of summary output data\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfFluvialPluvial - INFO - Converting maximum water surface timestamps...\n",
            "2025-12-02 20:12:41 - ras_commander.hdf.HdfFluvialPluvial - INFO - Processing cell adjacencies...\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Extracting cell times from maximum water surface data...\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identifying boundary edges...\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identified 3201 boundary edges using delta_t of 72 hours.\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - 3201 boundary line(s) shorter than 3000 units were dropped after filtering.\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Creating final GeoDataFrame for boundaries...\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Boundary line calculation completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Example usage using Optional min_line_length argument:\n",
        "boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(plan_hdf_path, delta_t=72, min_line_length=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfFluvialPluvial - INFO - Loading mesh and results data...\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:43 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfResultsMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfResultsMesh - INFO - Processing summary output for variable: Maximum Water Surface\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfResultsMesh - INFO - Processed 19597 rows of summary output data\n",
            "2025-12-02 20:12:44 - ras_commander.hdf.HdfFluvialPluvial - INFO - Processing cell adjacencies...\n",
            "2025-12-02 20:12:47 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identifying initial boundary seeds with delta_t = 10 hours...\n",
            "2025-12-02 20:12:48 - ras_commander.hdf.HdfFluvialPluvial - INFO - Starting iterative region growth with tolerance = 1.0 hours...\n",
            "Region Growing: 33iter [00:00, 81.92iter/s, Fluvial=0, Pluvial=0, Ambiguous=0]      \n",
            "2025-12-02 20:12:48 - ras_commander.hdf.HdfFluvialPluvial - INFO - Region growing completed in 33 iterations.\n",
            "2025-12-02 20:12:48 - ras_commander.hdf.HdfFluvialPluvial - INFO - Merging classifications with cell polygons...\n",
            "2025-12-02 20:12:48 - ras_commander.hdf.HdfFluvialPluvial - INFO - Dissolving polygons by classification...\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Applying minimum polygon area filter: 1000 acres\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Found 235 small fluvial and 140 small pluvial polygons to reclassify.\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Redissolved polygons after reclassification of small areas.\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Polygon generation completed successfully.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# To get the classified flood zones as polygons:\n",
        "flood_polygons_gdf = HdfFluvialPluvial.generate_fluvial_pluvial_polygons(\n",
        "    plan_hdf_path, \n",
        "    delta_t=10, \n",
        "    temporal_tolerance_hours=1.0,\n",
        "    min_polygon_area_acres=1000\n",
        ")\n",
        "\n",
        "flood_polygons_gdf\n",
        "\n",
        "# Plot the classified flood zones as polygons, colored by classification\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "flood_polygons_gdf.plot(\n",
        "    ax=ax,\n",
        "    column=\"classification\",\n",
        "    categorical=True,\n",
        "    legend=True,\n",
        "    edgecolor=\"black\",\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(\"Fluvial-Pluvial Classified Flood Zones\")\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfFluvialPluvial - INFO - Loading mesh and results data...\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:49 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfResultsMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfResultsMesh - INFO - Processing summary output for variable: Maximum Water Surface\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfResultsMesh - INFO - Processed 19597 rows of summary output data\n",
            "2025-12-02 20:12:51 - ras_commander.hdf.HdfFluvialPluvial - INFO - Processing cell adjacencies...\n",
            "2025-12-02 20:12:53 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identifying initial boundary seeds with delta_t = 12 hours...\n",
            "2025-12-02 20:12:54 - ras_commander.hdf.HdfFluvialPluvial - INFO - Starting iterative region growth with tolerance = 1.0 hours...\n",
            "Region Growing: 37iter [00:00, 91.41iter/s, Fluvial=0, Pluvial=0, Ambiguous=0]      \n",
            "2025-12-02 20:12:54 - ras_commander.hdf.HdfFluvialPluvial - INFO - Region growing completed in 37 iterations.\n",
            "2025-12-02 20:12:54 - ras_commander.hdf.HdfFluvialPluvial - INFO - Merging classifications with cell polygons...\n",
            "2025-12-02 20:12:54 - ras_commander.hdf.HdfFluvialPluvial - INFO - Dissolving polygons by classification...\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Applying minimum polygon area filter: 200 acres\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Found 214 small fluvial and 96 small pluvial polygons to reclassify.\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Redissolved polygons after reclassification of small areas.\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Polygon generation completed successfully.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# To get the classified flood zones as polygons:\n",
        "flood_polygons_gdf = HdfFluvialPluvial.generate_fluvial_pluvial_polygons(\n",
        "    plan_hdf_path, \n",
        "    delta_t=12, \n",
        "    temporal_tolerance_hours=1.0,\n",
        "    min_polygon_area_acres=200\n",
        ")\n",
        "\n",
        "flood_polygons_gdf\n",
        "\n",
        "# Plot the classified flood zones as polygons, colored by classification\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "flood_polygons_gdf.plot(\n",
        "    ax=ax,\n",
        "    column=\"classification\",\n",
        "    categorical=True,\n",
        "    legend=True,\n",
        "    edgecolor=\"black\",\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(\"Fluvial-Pluvial Classified Flood Zones\")\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Using HDF file from direct string path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfFluvialPluvial - INFO - Loading mesh and results data...\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:55 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfResultsMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfResultsMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfResultsMesh - INFO - Processing summary output for variable: Maximum Water Surface\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfMesh - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfMesh - INFO - Using existing Path object HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfMesh - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Using HDF file from h5py.File object: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfBase - INFO - Found projection in HDF file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.hdf\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfResultsMesh - INFO - Processed 19597 rows of summary output data\n",
            "2025-12-02 20:12:56 - ras_commander.hdf.HdfFluvialPluvial - INFO - Processing cell adjacencies...\n",
            "2025-12-02 20:12:58 - ras_commander.hdf.HdfFluvialPluvial - INFO - Identifying initial boundary seeds with delta_t = 14 hours...\n",
            "2025-12-02 20:12:59 - ras_commander.hdf.HdfFluvialPluvial - INFO - Starting iterative region growth with tolerance = 1.0 hours...\n",
            "Region Growing: 42iter [00:00, 96.83iter/s, Fluvial=0, Pluvial=0, Ambiguous=0]      \n",
            "2025-12-02 20:12:59 - ras_commander.hdf.HdfFluvialPluvial - INFO - Region growing completed in 42 iterations.\n",
            "2025-12-02 20:12:59 - ras_commander.hdf.HdfFluvialPluvial - INFO - Merging classifications with cell polygons...\n",
            "2025-12-02 20:12:59 - ras_commander.hdf.HdfFluvialPluvial - INFO - Dissolving polygons by classification...\n",
            "2025-12-02 20:13:00 - ras_commander.hdf.HdfFluvialPluvial - INFO - Applying minimum polygon area filter: 1000 acres\n",
            "2025-12-02 20:13:00 - ras_commander.hdf.HdfFluvialPluvial - INFO - Found 182 small fluvial and 69 small pluvial polygons to reclassify.\n",
            "2025-12-02 20:13:00 - ras_commander.hdf.HdfFluvialPluvial - INFO - Redissolved polygons after reclassification of small areas.\n",
            "2025-12-02 20:13:00 - ras_commander.hdf.HdfFluvialPluvial - INFO - Polygon generation completed successfully.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# To get the classified flood zones as polygons:\n",
        "flood_polygons_gdf = HdfFluvialPluvial.generate_fluvial_pluvial_polygons(\n",
        "    plan_hdf_path, \n",
        "    delta_t=14, \n",
        "    temporal_tolerance_hours=1.0,\n",
        "    min_polygon_area_acres=1000\n",
        ")\n",
        "\n",
        "flood_polygons_gdf\n",
        "\n",
        "# Plot the classified flood zones as polygons, colored by classification\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "flood_polygons_gdf.plot(\n",
        "    ax=ax,\n",
        "    column=\"classification\",\n",
        "    categorical=True,\n",
        "    legend=True,\n",
        "    edgecolor=\"black\",\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(\"Fluvial-Pluvial Classified Flood Zones\")\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:13:00 - pyogrio._io - INFO - Created 3 records\n"
          ]
        }
      ],
      "source": [
        "flood_polygons_gdf.to_file(driver='GeoJSON', filename='flood_polygons.geojson')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:13:00 - pyogrio._io - INFO - Created 0 records\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory created/verified at: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\fluvial_pluvial_boundary\n"
          ]
        }
      ],
      "source": [
        "# Create fluvial_pluvial_boundary subfolder\n",
        "output_dir = your_project_path / \"fluvial_pluvial_boundary\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "print(f\"Output directory created/verified at: {output_dir}\")\n",
        "\n",
        "# Save to GeoJSON in output directory\n",
        "boundary_gdf.to_file(output_dir / 'fluvial_pluvial_boundary.geojson', driver='GeoJSON')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\15_stored_map_generation.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:28.559444Z",
          "iopub.status.busy": "2025-11-17T18:53:28.559259Z",
          "iopub.status.idle": "2025-11-17T18:53:30.114200Z",
          "shell.execute_reply": "2025-11-17T18:53:30.113608Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Post-Processing Stored Maps\n",
        "\n",
        "This notebook demonstrates how to automate the generation of stored floodplain map outputs (like `.tif` files for Depth, WSEL, and Velocity) using the `ras-commander` library. This is a common post-processing step that can be time-consuming to do manually for multiple plans.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1.  **Initialize Project**: Set up the HEC-RAS project.\n",
        "2.  **Run Simulation**: Ensure a plan has been computed to generate base results.\n",
        "3.  **Automate Post-Processing**: Use the new `RasMap.postprocess_stored_maps` function to:\n",
        "    -   Modify the `.rasmap` file to include stored map definitions.\n",
        "    -   Update plan flags to *only* run the floodplain mapping component.\n",
        "    -   Execute the plan, which quickly generates the `.tif` files.\n",
        "    -   Restore the original plan and `.rasmap` files, keeping the new map layers.\n",
        "4.  **Verify Output**: Load and visualize one of the generated `.tif` files to confirm success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install necessary packages if not already installed\n",
        "!pip install --upgrade ras-commander\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Multi-Plan Mapping in RASMapper\n",
        "\n",
        "Right click on the results layer, and \"Manage Results Maps\", then select all and click \"Compute/Update Stored Maps\".  This will create all results maps in RASMapper.\n",
        "\n",
        "Note: For large batches, RASMapper can run out of memory and fail.  The workflow below, which uses pre-sets the floodplain mapping option for HEC-RAS's Unsteady Flow Analysis or Run Multiple Plans mode which is more reliable for larger datasets/batching.  \n",
        "\n",
        "Use the simplest method that works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run with Full Windows GUI\n",
        "\n",
        "[RAS Commander Stored Map Assistant](https://github.com/gpt-cmdr/ras-stored-map-assistant)  (Coming soon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:30.116870Z",
          "iopub.status.busy": "2025-11-17T18:53:30.116475Z",
          "iopub.status.idle": "2025-11-17T18:53:30.121350Z",
          "shell.execute_reply": "2025-11-17T18:53:30.120743Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n",
            "ras-commander loaded from: c:\\Users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\Lib\\site-packages\n",
            "Expected local path: C:\\GH\\ras-commander\n",
            "Successfully using local copy: False\n"
          ]
        }
      ],
      "source": [
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Now try to import again\n",
        "from ras_commander import *\n",
        "\n",
        "# Verify we're loading from the local copy\n",
        "import ras_commander\n",
        "local_path = Path(ras_commander.__file__).parent.parent\n",
        "print(f\"ras-commander loaded from: {local_path}\")\n",
        "print(f\"Expected local path: {rascmdr_directory}\")\n",
        "print(f\"Successfully using local copy: {local_path == rascmdr_directory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install necessary packages if not already installed\n",
        "!pip install --upgrade ras-commander\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:30.123790Z",
          "iopub.status.busy": "2025-11-17T18:53:30.123459Z",
          "iopub.status.idle": "2025-11-17T18:53:30.223841Z",
          "shell.execute_reply": "2025-11-17T18:53:30.223284Z"
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from ras_commander import *\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare the HEC-RAS Project\n",
        "\n",
        "First, we'll use `RasExamples` to get the `BaldEagleCrkMulti2D` project. Then, we'll run **Plan 06** to ensure we have a base set of results to work with. If the results already exist, the execution will be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:30.226925Z",
          "iopub.status.busy": "2025-11-17T18:53:30.226499Z",
          "iopub.status.idle": "2025-11-17T18:53:30.256162Z",
          "shell.execute_reply": "2025-11-17T18:53:30.255710Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:30:57 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for Plan 06 already exist. Skipping initial computation.\n"
          ]
        }
      ],
      "source": [
        "# If /example_projects/BaldEagleCrkMulti2D does not exist, extract it\n",
        "project_path = Path(\"./example_projects/BaldEagleCrkMulti2D\").resolve()\n",
        "\n",
        "if not os.path.exists(project_path):\n",
        "    project_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\")\n",
        "\n",
        "# Initialize the RAS project\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "# Define the plan to work with\n",
        "plan_number = \"06\"\n",
        "\n",
        "# Check if the plan's HDF results file already exists\n",
        "plan_hdf_path = ras.project_folder / f\"{ras.project_name}.p{plan_number}.hdf\"\n",
        "if not plan_hdf_path.exists():\n",
        "    print(f\"Results for Plan {plan_number} not found. Running the simulation first...\")\n",
        "    success = RasCmdr.compute_plan(plan_number, num_cores=4)\n",
        "    if not success:\n",
        "        raise RuntimeError(f\"Initial computation of plan {plan_number} failed.\")\n",
        "    print(f\"Plan {plan_number} successfully computed.\")\n",
        "else:\n",
        "    print(f\"Results for Plan {plan_number} already exist. Skipping initial computation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Use `postprocess_stored_maps` to Generate Floodplain Maps\n",
        "\n",
        "Now we'll call the new function. It will handle all the file modifications, run HEC-RAS in mapping-only mode, and clean up afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:30.258309Z",
          "iopub.status.busy": "2025-11-17T18:53:30.257971Z",
          "iopub.status.idle": "2025-11-17T18:53:30.273936Z",
          "shell.execute_reply": "2025-11-17T18:53:30.273428Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>13</td>\\n', '      <td>07</td>\\n', '      <td>06</td>\\n', '      <td>PMF with Multi 2D Areas</td>\\n', '      <td>5.10</td>\\n', '      <td>PMF Multi 2D</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>30SEC</td>\\n', '      <td>30MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>193</td>\\n', '      <td>None</td>\\n', '      <td>06</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>07</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>15</td>\\n', '      <td>12</td>\\n', '      <td>08</td>\\n', '      <td>1d-2D Dambreak Refined Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>1D-2D Refined Grid</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>20SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>None</td>\\n', '      <td>08</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>12</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>17</td>\\n', '      <td>09</td>\\n', '      <td>10</td>\\n', '      <td>2D to 1D No Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>2D to 1D No Dam</td>\\n', '      <td>01JAN1999,1200,06JAN1999,1200</td>\\n', '      <td>1MIN</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>Upstream2D</td>\\n', '      <td>None</td>\\n', '      <td>10</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>09</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>18</td>\\n', '      <td>10</td>\\n', '      <td>11</td>\\n', '      <td>2D to 2D Run</td>\\n', '      <td>5.00</td>\\n', '      <td>2D to 2D Run</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>20SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>None</td>\\n', '      <td>11</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>10</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>19</td>\\n', '      <td>11</td>\\n', '      <td>12</td>\\n', '      <td>SA to 2D Dam Break Run</td>\\n', '      <td>5.00</td>\\n', '      <td>SA to 2D Dam Break</td>\\n', '      <td>01JAN1999,1200,04JAN1999,1200</td>\\n', '      <td>20SEC</td>\\n', '      <td>10MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>BaldEagleCr</td>\\n', '      <td>None</td>\\n', '      <td>12</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>11</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['   plan_number unsteady_number geometry_number  \\\\\\n', '0           13              07              06   \\n', '1           15              12              08   \\n', '2           17              09              10   \\n', '3           18              10              11   \\n', '4           19              11              12   \\n', '5           03              13              09   \\n', '6           04              01              13   \\n', '7           02              01              01   \\n', '8           01              01              01   \\n', '9           05              02              03   \\n', '10          06              03              09   \\n', '\\n', '                                 Plan Title Program Version  \\\\\\n', '0                   PMF with Multi 2D Areas            5.10   \\n', '1               1d-2D Dambreak Refined Grid            5.10   \\n', '2                           2D to 1D No Dam            5.00   \\n', '3                              2D to 2D Run            5.0\n...\n[Output truncated, 7422 characters total]"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:30.276358Z",
          "iopub.status.busy": "2025-11-17T18:53:30.276028Z",
          "iopub.status.idle": "2025-11-17T18:53:30.278579Z",
          "shell.execute_reply": "2025-11-17T18:53:30.278127Z"
        }
      },
      "outputs": [],
      "source": [
        "# NOTE: For HEC-RAS 5.0.7 projects being run in HEC-RAS 6.x, you must manually open Ras.exe and use RASMapper to generate easmapper entries.\n",
        "# This step is only required for 5.0.7 projects run in 6.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:30.280651Z",
          "iopub.status.busy": "2025-11-17T18:53:30.280431Z",
          "iopub.status.idle": "2025-11-17T18:53:50.318595Z",
          "shell.execute_reply": "2025-11-17T18:53:50.317872Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MANUAL STEP REQUIRED: Update .rasmap to Version 6.x\n",
            "This project was created in HEC-RAS 5.0.7. To generate stored maps in HEC-RAS 6.x, you must:\n",
            "1. HEC-RAS will now be opened with this project.\n",
            "2. In HEC-RAS, open RAS Mapper (from the main toolbar).\n",
            "3. When prompted, allow RAS Mapper to update the .rasmap file to the new version.\n",
            "4. Once the update is complete, close RAS Mapper and exit HEC-RAS.\n",
            "\n",
            "After closing HEC-RAS, return here and continue running the notebook.\n",
            "============================================================\n",
            "\n",
            "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\"\n",
            "Opened HEC-RAS with Process ID: 236432\n",
            "Please wait for the next cell to automate RAS Mapper...\n",
            "Looking for HEC-RAS windows for process ID: 236432\n",
            "Waiting for HEC-RAS to fully load...\n",
            "No HEC-RAS windows found yet, waiting 2 seconds...\n",
            "Found 2 window(s) for this HEC-RAS process:\n",
            "  - HEC-RAS 6.6\n",
            "  - RAS\n",
            "\n",
            "Using main window: HEC-RAS 6.6\n",
            "\n",
            "Attempting to open RAS Mapper via menu...\n",
            "\n",
            "Found 7 top-level menus:\n",
            "\n",
            "Menu 0: '&File'\n",
            "  Contains 27 items:\n",
            "    Item 0: '&New Project ...' (ID: 2)\n",
            "    Item 1: '&Open Project ...' (ID: 3)\n",
            "    Item 2: '&Save Project' (ID: 4)\n",
            "    Item 3: 'Save Project &As ...' (ID: 5)\n",
            "    Item 4: '&Rename Project Title ...' (ID: 6)\n",
            "    Item 5: '&Delete Project ...' (ID: 7)\n",
            "    Item 6: '' (ID: 8)\n",
            "    Item 7: '&Project Summary ...' (ID: 9)\n",
            "    Item 8: 'Compare Model Data ...' (ID: 10)\n",
            "    Item 9: '' (ID: 11)\n",
            "    Item 10: '&Import HEC-2 Data ...' (ID: 12)\n",
            "    Item 11: 'I&mport HEC-RAS Data ...' (ID: 13)\n",
            "    Item 12: '&Generate Report ...' (ID: 14)\n",
            "    Item 13: '&Export GIS Data ...' (ID: 15)\n",
            "    Item 14: 'Export to HEC-&DSS ...' (ID: 16)\n",
            "    Item 15: 'Restore Backup Data ' (ID: -1)\n",
            "    Item 16: '' (ID: 23)\n",
            "    Item 17: 'Zip Plan(s) or Archive Project...' (ID: 24)\n",
            "    Item 18: '' (ID: 25)\n",
            "    Item 19: 'E&xit' (ID: 26)\n",
            "    Item 20: '' (ID: 27)\n",
            "    Item 21: 'C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj' (ID: 28)\n",
            "    Item 22: 'C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.prj' (ID: 29)\n",
            "    Item 23: 'C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.prj' (ID: 30)\n",
            "    Item 24: 'C:\\HCFCD\\Standard_Benefits_Process\\3 - Model Data\\Storm Interpolator - Log Linear\\Output Projects\\South_Belt_RAS66_2025-11-13_044255\\A120-00-00_RAS 4.1\\A100_00_00.prj' (ID: 31)\n",
            "    Item 25: 'C:\\HCFCD\\Standard_Benefits_Process\\3 - Model Data\\Storm Interpolator - Log Linear\\Input Projects\\A520-03-00-E003 - South Belt Stormwater Detention Basin\\A120-00-00_RAS 4.1\\A100_00_00.prj' (ID: 32)\n",
            "    Item 26: 'C:\\GH\\TNTech\\data\\EarlyDevTesting\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj' (ID: 33)\n",
            "\n",
            "Menu 1: '&Edit'\n",
            "  Contains 8 items:\n",
            "    Item 0: '&Geometric Data ...' (ID: 37)\n",
            "    Item 1: '' (ID: 38)\n",
            "    Item 2: '&Steady Flow Data ...' (ID: 39)\n",
            "    Item 3: '&Quasi Unsteady Flow (Sediment) ...' (ID: 40)\n",
            "    Item 4: '&Unsteady Flow Data ...' (ID: 41)\n",
            "    Item 5: '' (ID: 42)\n",
            "    Item 6: 'Se&diment Data ...' (ID: 43)\n",
            "    Item 7: '&Water Quality Data ...' (ID: 44)\n",
            "\n",
            "Menu 2: '&Run'\n",
            "  Contains 9 items:\n",
            "    Item 0: '&Steady Flow Analysis ...' (ID: 46)\n",
            "    Item 1: '&Unsteady Flow Analysis ...' (ID: 47)\n",
            "    Item 2: 'Quasi-Unsteady Analysis (Sediment)...' (ID: 48)\n",
            "    Item 3: 'Water Quality Analysis ...' (ID: 49)\n",
            "    Item 4: '&Hydraulic Design Functions ...' (ID: 50)\n",
            "    Item 5: '' (ID: 51)\n",
            "    Item 6: 'Run Multiple Plans ...' (ID: 52)\n",
            "    Item 7: '' (ID: 54)\n",
            "    Item 8: 'Uncertainty Analysis' (ID: -1)\n",
            "\n",
            "Menu 3: '&View'\n",
            "  Contains 23 items:\n",
            "    Item 0: '&Cross-Sections ...' (ID: 61)\n",
            "    Item 1: '&Water Surface Profiles ...' (ID: 62)\n",
            "    Item 2: '&General Profile Plot ...' (ID: 63)\n",
            "    Item 3: '&Rating Curves ...' (ID: 64)\n",
            "    Item 4: '3D View ...' (ID: 65)\n",
            "    Item 5: '&X-Y-Z Perspective Plots (Classic) ...' (ID: 66)\n",
            "    Item 6: '&Stage and Flow Hydrographs ...' (ID: 67)\n",
            "    Item 7: '&Hydraulic Property Tables ...' (ID: 68)\n",
            "    Item 8: '' (ID: 69)\n",
            "    Item 9: '&Detailed Output Tables ...' (ID: 70)\n",
            "    Item 10: '&Profile Summary Table ...' (ID: 71)\n",
            "    Item 11: '&Summary Err,Warn, Notes ...' (ID: 72)\n",
            "    Item 12: '' (ID: 73)\n",
            "    Item 13: 'DSS Data ...' (ID: 74)\n",
            "    Item 14: '' (ID: 75)\n",
            "    Item 15: 'Unsteady Flow Spatial Plot (computation interval) ...' (ID: 76)\n",
            "    Item 16: 'Unsteady Flow Time Series Plot (computation interval) ...' (ID: 77)\n",
            "    Item 17: '' (ID: 78)\n",
            "    Item 18: 'WQ Spatial Plot ...' (ID: 79)\n",
            "    Item 19: 'WQ Time Series Plot ...' (ID: 80)\n",
            "    Item 20: '' (ID: 81)\n",
            "    Item 21: 'Sediment Output ...' (ID: 82)\n",
            "    Item 22: 'Legacy Sediment Output' (ID: -1)\n",
            "\n",
            "Menu 4: '&Options'\n",
            "  Contains 5 items:\n",
            "    Item 0: '&Program Setup' (ID: -1)\n",
            "    Item 1: '&Default Parameters' (ID: -1)\n",
            "    Item 2: '&Unit system (US Customary/SI) ...' (ID: 98)\n",
            "    Item 3: '&Convert Project Units ...' (ID: 99)\n",
            "    Item 4: 'Convert &Horizontal Coordinate Systems ...' (ID: 100)\n",
            "\n",
            "Menu 5: '&GIS Tools'\n",
            "  Contains 1 items:\n",
            "    Item 0: 'RAS Mapper ...' (ID: 102)\n",
            "\n",
            "Menu 6: '&Help'\n",
            "  Contains 21 items:\n",
            "    Item 0: 'HEC-RAS Online Documentation ...\tF1' (ID: 104)\n",
            "    Item 1: 'Release Notes ...' (ID: 105)\n",
            "    Item 2: 'Known Issues ...' (ID: 106)\n",
            "    Item 3: 'Community Support on Discourse ...' (ID: 107)\n",
            "    Item 4: '' (ID: 108)\n",
            "    Item 5: 'Users Manual ...' (ID: 109)\n",
            "    Item 6: '2D Modeling User's Manual \u2026' (ID: 110)\n",
            "    Item 7: 'RAS Mapper User's Manual \u2026' (ID: 111)\n",
            "    Item 8: 'Hydraulic Reference \u2026' (ID: 112)\n",
            "    Item 9: 'Applications Guide \u2026' (ID: 113)\n",
            "    Item 10: '1D Sediment User's Manual \u2026' (ID: 114)\n",
            "    Item 11: '2D Sediment User's Manual \u2026' (ID: 115)\n",
            "    Item 12: '2D Sediment Technical Reference \u2026' (ID: 116)\n",
            "    Item 13: 'Mud and Debris Manuals \u2026' (ID: 117)\n",
            "    Item 14: '' (ID: 118)\n",
            "    Item 15: 'Download Example Projects ...' (ID: 119)\n",
            "    Item 16: '' (ID: 120)\n",
            "    Item 17: 'HEC-RAS Webpage ...' (ID: 121)\n",
            "    Item 18: '' (ID: 122)\n",
            "    Item 19: 'View Terms and Conditions of Use ...' (ID: 123)\n",
            "    Item 20: '&About HEC-RAS ...' (ID: 124)\n",
            "\n",
            "Found RAS Mapper: 'RAS Mapper ...' with ID: 102\n",
            "RAS Mapper opening via menu...\n",
            "\n",
            "Waiting for RAS Mapper to open...\n",
            "Window not found, waiting 2 seconds...\n",
            "RAS Mapper is open: RAS Mapper\n",
            "Allowing time for .rasmap update...\n",
            "Attempting to close RAS Mapper...\n",
            "Closed: RAS Mapper\n",
            "RAS Mapper closed successfully.\n",
            "Waiting for RAS Mapper to fully close...\n",
            "\n",
            "Closing HEC-RAS...\n",
            "HEC-RAS closing...\n",
            "\n",
            "Automation complete. You can now continue with the notebook.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Print instructions to the user\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MANUAL STEP REQUIRED: Update .rasmap to Version 6.x\")\n",
        "print(\"This project was created in HEC-RAS 5.0.7. To generate stored maps in HEC-RAS 6.x, you must:\")\n",
        "print(\"1. HEC-RAS will now be opened with this project.\")\n",
        "print(\"2. In HEC-RAS, open RAS Mapper (from the main toolbar).\") \n",
        "print(\"3. When prompted, allow RAS Mapper to update the .rasmap file to the new version.\")\n",
        "print(\"4. Once the update is complete, close RAS Mapper and exit HEC-RAS.\")\n",
        "print(\"\\nAfter closing HEC-RAS, return here and continue running the notebook.\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "ras_exe = ras.ras_exe_path\n",
        "prj_path = f'\"{str(ras.prj_file)}\"'  # Add quotes around project path\n",
        "\n",
        "command = f\"{ras_exe} {prj_path}\"\n",
        "print(command)\n",
        "\n",
        "try:\n",
        "    # Capture the process object so we can get its PID\n",
        "    if sys.platform == \"win32\":\n",
        "        hecras_process = subprocess.Popen(command)\n",
        "    else:\n",
        "        hecras_process = subprocess.Popen([ras_exe, prj_path])\n",
        "    \n",
        "    # Store the process ID for use in the next cell\n",
        "    hecras_pid = hecras_process.pid\n",
        "    print(f\"Opened HEC-RAS with Process ID: {hecras_pid}\")\n",
        "    print(\"Please wait for the next cell to automate RAS Mapper...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to launch HEC-RAS: {e}\")\n",
        "    hecras_pid = None\n",
        "\n",
        "\n",
        "\n",
        "import win32gui\n",
        "import win32con\n",
        "import win32api\n",
        "import win32com.client\n",
        "import win32process\n",
        "import time\n",
        "import ctypes\n",
        "from ctypes import wintypes\n",
        "\n",
        "# Constants\n",
        "WM_COMMAND = 0x0111\n",
        "MIIM_STRING = 0x00000040\n",
        "MIIM_ID = 0x00000002\n",
        "MIIM_SUBMENU = 0x00000004\n",
        "MIIM_TYPE = 0x00000010\n",
        "MIIM_DATA = 0x00000020\n",
        "MF_BYPOSITION = 0x00000400\n",
        "\n",
        "def get_windows_by_pid(pid):\n",
        "    \"\"\"Find all windows belonging to a specific process ID\"\"\"\n",
        "    def callback(hwnd, hwnds):\n",
        "        if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):\n",
        "            # Get the process ID for this window\n",
        "            _, window_pid = win32process.GetWindowThreadProcessId(hwnd)\n",
        "            if window_pid == pid:\n",
        "                window_title = win32gui.GetWindowText(hwnd)\n",
        "                if window_title:  # Only include windows with titles\n",
        "                    hwnds.append((hwnd, window_title))\n",
        "        return True\n",
        "    \n",
        "    hwnds = []\n",
        "    win32gui.EnumWindows(callback, hwnds)\n",
        "    return hwnds\n",
        "\n",
        "def find_main_hecras_window(windows):\n",
        "    \"\"\"Find the main HEC-RAS window from a list of windows\"\"\"\n",
        "    for hwnd, title in windows:\n",
        "        # Main window usually has \"HEC-RAS\" in title and has a menu bar\n",
        "        if \"HEC-RAS\" in title and win32gui.GetMenu(hwnd):\n",
        "            return hwnd, title\n",
        "    return None, None\n",
        "\n",
        "def get_menu_string(menu_handle, pos):\n",
        "    \"\"\"Get menu item string at position\"\"\"\n",
        "    # Create buffer for menu string\n",
        "    buf_size = 256\n",
        "    buf = ctypes.create_unicode_buffer(buf_size)\n",
        "    \n",
        "    # Get menu item info\n",
        "    user32 = ctypes.windll.user32\n",
        "    result = user32.GetMenuStringW(\n",
        "        menu_handle,\n",
        "        pos,\n",
        "        buf,\n",
        "        buf_size,\n",
        "        MF_BYPOSITION\n",
        "    )\n",
        "    \n",
        "    if result:\n",
        "        return buf.value\n",
        "    return \"\"\n",
        "\n",
        "def enumerate_all_menus(hwnd):\n",
        "    \"\"\"Enumerate all menus and their items\"\"\"\n",
        "    menu_bar = win32gui.GetMenu(hwnd)\n",
        "    if not menu_bar:\n",
        "        print(\"No menu bar found\")\n",
        "        return None\n",
        "    \n",
        "    menu_count = win32gui.GetMenuItemCount(menu_bar)\n",
        "    print(f\"\\nFound {menu_count} top-level menus:\")\n",
        "    \n",
        "    gis_tools_info = None\n",
        "    \n",
        "    for i in range(menu_count):\n",
        "        # Get menu text\n",
        "        menu_text = get_menu_string(menu_bar, i)\n",
        "        print(f\"\\nMenu {i}: '{menu_text}'\")\n",
        "        \n",
        "        # Get submenu handle\n",
        "        submenu = win32gui.GetSubMenu(menu_bar, i)\n",
        "        if submenu:\n",
        "            item_count = win32gui.GetMenuItemCount(submenu)\n",
        "            print(f\"  Contains {item_count} items:\")\n",
        "            \n",
        "            for j in range(item_count):\n",
        "                item_text = get_menu_string(submenu, j)\n",
        "                # Get menu item ID\n",
        "                menu_id = win32gui.GetMenuItemID(submenu, j)\n",
        "                print(f\"    Item {j}: '{item_text}' (ID: {menu_id})\")\n",
        "                \n",
        "                # Store GIS Tools info if found\n",
        "                if \"gis tools\" in menu_text.lower():\n",
        "                    if \"ras mapper\" in item_text.lower():\n",
        "                        gis_tools_info = (submenu, menu_id, item_text)\n",
        "    \n",
        "    return gis_tools_info\n",
        "\n",
        "def open_rasmapper_via_menu(hec_ras_hwnd):\n",
        "    \"\"\"Open RASMapper through HEC-RAS menu system\"\"\"\n",
        "    try:\n",
        "        # Enumerate all menus to find RAS Mapper\n",
        "        gis_tools_info = enumerate_all_menus(hec_ras_hwnd)\n",
        "        \n",
        "        if gis_tools_info:\n",
        "            submenu, menu_id, item_text = gis_tools_info\n",
        "            print(f\"\\nFound RAS Mapper: '{item_text}' with ID: {menu_id}\")\n",
        "            \n",
        "            # Send command to open RASMapper\n",
        "            win32api.PostMessage(hec_ras_hwnd, WM_COMMAND, menu_id, 0)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"\\nCould not find RAS Mapper in any menu\")\n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Menu method failed with error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def open_rasmapper_keyboard(hwnd):\n",
        "    \"\"\"Alternative method using keyboard shortcuts\"\"\"\n",
        "    try:\n",
        "        shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
        "        \n",
        "        # More robust window activation\n",
        "        # First, check if window is minimized and restore it\n",
        "        if win32gui.IsIconic(hwnd):\n",
        "            win32gui.ShowWindow(hwnd, win32con.SW_RESTORE)\n",
        "            time.sleep(0.5)\n",
        "        \n",
        "        # Try multiple methods to bring window to foreground\n",
        "        try:\n",
        "            # Method 1: Direct SetForegroundWindow\n",
        "            win32gui.SetForegroundWindow(hwnd)\n",
        "        except:\n",
        "            try:\n",
        "                # Method 2: Use ShowWindow then SetForegroundWindow\n",
        "                win32gui.ShowWindow(hwnd, win32con.SW_SHOW)\n",
        "                win32gui.SetForegroundWindow(hwnd)\n",
        "            except:\n",
        "                # Method 3: Use BringWindowToTop\n",
        "                win32gui.BringWindowToTop(hwnd)\n",
        "        \n",
        "        time.sleep(0.5)\n",
        "        \n",
        "        # Send Alt+G for GIS Tools menu\n",
        "        shell.SendKeys(\"%g\")\n",
        "        time.sleep(0.2)\n",
        "        \n",
        "        # Send R for RAS Mapper\n",
        "        shell.SendKeys(\"r\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Keyboard method failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_rasmapper_window():\n",
        "    \"\"\"Find any RAS Mapper window\"\"\"\n",
        "    def callback(hwnd, windows):\n",
        "        if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):\n",
        "            window_title = win32gui.GetWindowText(hwnd)\n",
        "            if \"RAS Mapper\" in window_title:\n",
        "                windows.append((hwnd, window_title))\n",
        "        return True\n",
        "    \n",
        "    windows = []\n",
        "    win32gui.EnumWindows(callback, windows)\n",
        "    return windows\n",
        "\n",
        "def close_rasmapper():\n",
        "    \"\"\"Close RASMapper window\"\"\"\n",
        "    windows = find_rasmapper_window()\n",
        "    \n",
        "    for hwnd, title in windows:\n",
        "        try:\n",
        "            win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)\n",
        "            print(f\"Closed: {title}\")\n",
        "            return True\n",
        "        except:\n",
        "            continue\n",
        "    return False\n",
        "\n",
        "def wait_for_window(find_window_func, timeout=60, check_interval=2):\n",
        "    \"\"\"Wait for a window to appear\"\"\"\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < timeout:\n",
        "        windows = find_window_func()\n",
        "        if windows:\n",
        "            return windows\n",
        "        print(f\"Window not found, waiting {check_interval} seconds...\")\n",
        "        time.sleep(check_interval)\n",
        "    return None\n",
        "\n",
        "# Main automation\n",
        "if 'hecras_pid' not in globals() or hecras_pid is None:\n",
        "    print(\"ERROR: HEC-RAS process ID not found. Please run the previous cell first.\")\n",
        "else:\n",
        "    print(f\"Looking for HEC-RAS windows for process ID: {hecras_pid}\")\n",
        "    print(\"Waiting for HEC-RAS to fully load...\")\n",
        "    \n",
        "    # Wait for HEC-RAS window to appear\n",
        "    windows = None\n",
        "    while True:\n",
        "        windows = get_windows_by_pid(hecras_pid)\n",
        "        if windows:\n",
        "            print(f\"Found {len(windows)} window(s) for this HEC-RAS process:\")\n",
        "            for hwnd, title in windows:\n",
        "                print(f\"  - {title}\")\n",
        "            \n",
        "            # Find the main HEC-RAS window\n",
        "            hec_ras_hwnd, title = find_main_hecras_window(windows)\n",
        "            \n",
        "            if hec_ras_hwnd:\n",
        "                print(f\"\\nUsing main window: {title}\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Main HEC-RAS window not ready yet, waiting 2 seconds...\")\n",
        "                time.sleep(2)\n",
        "        else:\n",
        "            print(\"No HEC-RAS windows found yet, waiting 2 seconds...\")\n",
        "            time.sleep(2)\n",
        "            \n",
        "    # Continue with the rest of the automation\n",
        "    print(\"\\nAttempting to open RAS Mapper via menu...\")\n",
        "    if open_rasmapper_via_menu(hec_ras_hwnd):\n",
        "        print(\"RAS Mapper opening via menu...\")\n",
        "    else:\n",
        "        # Fallback to keyboard method\n",
        "        print(\"\\nMenu method failed, trying keyboard shortcuts...\")\n",
        "        if open_rasmapper_keyboard(hec_ras_hwnd):\n",
        "            print(\"RAS Mapper opening via keyboard...\")\n",
        "        else:\n",
        "            # Last resort: try common menu IDs\n",
        "            print(\"\\nTrying direct menu activation with common IDs...\")\n",
        "            try:\n",
        "                # Common IDs for RAS Mapper (may vary by version)\n",
        "                possible_ids = [40305, 40306, 40307, 40308, 40309, 40310, \n",
        "                               32854, 32855, 32856, 32857, 32858]\n",
        "                for menu_id in possible_ids:\n",
        "                    win32api.PostMessage(hec_ras_hwnd, WM_COMMAND, menu_id, 0)\n",
        "                    time.sleep(0.5)\n",
        "                    # Check if RAS Mapper opened\n",
        "                    if find_rasmapper_window():\n",
        "                        print(f\"RAS Mapper opened successfully with ID {menu_id}!\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(\"Failed to open RAS Mapper automatically.\")\n",
        "                    print(\"Please open it manually from HEC-RAS: GIS Tools > RAS Mapper\")\n",
        "            except Exception as e:\n",
        "                print(f\"Direct menu activation failed: {e}\")\n",
        "                print(\"Please open RAS Mapper manually from HEC-RAS: GIS Tools > RAS Mapper\")\n",
        "    \n",
        "    # Wait for RAS Mapper window to appear\n",
        "    print(\"\\nWaiting for RAS Mapper to open...\")\n",
        "    rasmapper_windows = wait_for_window(find_rasmapper_window)\n",
        "    \n",
        "    if rasmapper_windows:\n",
        "        print(f\"RAS Mapper is open: {rasmapper_windows[0][1]}\")\n",
        "        print(\"Allowing time for .rasmap update...\")\n",
        "        time.sleep(2)  # Give extra time for file updates\n",
        "        \n",
        "        # Keep trying to close RAS Mapper until successful\n",
        "        print(\"Attempting to close RAS Mapper...\")\n",
        "        while True:\n",
        "            if close_rasmapper():\n",
        "                print(\"RAS Mapper closed successfully.\")\n",
        "                break\n",
        "            print(\"Waiting 2 seconds before trying to close RAS Mapper again...\")\n",
        "            time.sleep(2)\n",
        "        \n",
        "        # Wait until RAS Mapper is fully closed before closing HEC-RAS\n",
        "        while find_rasmapper_window():\n",
        "            print(\"Waiting for RAS Mapper to fully close...\")\n",
        "            time.sleep(2)\n",
        "        \n",
        "        # Now close HEC-RAS\n",
        "        print(\"\\nClosing HEC-RAS...\")\n",
        "        try:\n",
        "            win32gui.PostMessage(hec_ras_hwnd, win32con.WM_CLOSE, 0, 0)\n",
        "            print(\"HEC-RAS closing...\")\n",
        "        except:\n",
        "            print(\"Could not close HEC-RAS automatically. Please close it manually.\")\n",
        "    else:\n",
        "        print(\"RAS Mapper window not detected after waiting.\")\n",
        "\n",
        "print(\"\\nAutomation complete. You can now continue with the notebook.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:50.320856Z",
          "iopub.status.busy": "2025-11-17T18:53:50.320624Z",
          "iopub.status.idle": "2025-11-17T18:53:55.324168Z",
          "shell.execute_reply": "2025-11-17T18:53:55.323527Z"
        }
      },
      "outputs": [],
      "source": [
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NOTE: You MUST open the HEC-RAS Main Window for Floodplain Mapping to Work\n",
        "\n",
        "This is being massively improved in HEC-RAS 2025, but for now can't use the -c flag to compute floodplain maps like we do for unsteady computations.  \n",
        "\n",
        "There should also be a way to implement this with Win32COM (similar to above), but for now opening the window is required. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T18:53:55.326329Z",
          "iopub.status.busy": "2025-11-17T18:53:55.326189Z",
          "iopub.status.idle": "2025-11-17T19:10:16.387525Z",
          "shell.execute_reply": "2025-11-17T19:10:16.387056Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Extracted terrain names: ['Terrain50']\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Backing up plan file C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06 to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.storedmap.bak\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Updating plan run flags for floodplain mapping for plan 06...\n",
            "2025-11-17 21:31:10 - ras_commander.RasPlan - INFO - Successfully updated run flags in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06 (flags modified: 4)\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Backing up rasmap file C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap to C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap.storedmap.bak\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Added 'Depth' stored map to results layer for plan 06.\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Added 'WSE' stored map to results layer for plan 06.\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Filtered terrains, keeping only 'Terrain50'.\n",
            "2025-11-17 21:31:10 - ras_commander.RasMap - INFO - Using GUI automation to run floodplain mapping...\n",
            "2025-11-17 21:31:10 - ras_commander.RasGuiAutomation - INFO - Setting current plan to 06 in project file...\n",
            "2025-11-17 21:31:10 - ras_commander.RasPrj - INFO - Set current plan to p06 in C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\n",
            "2025-11-17 21:31:10 - ras_commander.RasGuiAutomation - INFO - Current plan set to 06 in C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\n",
            "2025-11-17 21:31:10 - ras_commander.RasGuiAutomation - INFO - Opening HEC-RAS...\n",
            "2025-11-17 21:31:10 - ras_commander.RasGuiAutomation - INFO - HEC-RAS opened with Process ID: 204712\n",
            "2025-11-17 21:31:10 - ras_commander.RasGuiAutomation - INFO - Waiting for HEC-RAS main window...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available terrains: ['Terrain50']\n",
            "Using terrain: Terrain50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:31:13 - ras_commander.RasGuiAutomation - INFO - Found HEC-RAS main window: HEC-RAS 6.6\n",
            "2025-11-17 21:31:14 - ras_commander.RasGuiAutomation - INFO - Clicking 'Run > Unsteady Flow Analysis' menu...\n",
            "2025-11-17 21:31:14 - ras_commander.RasGuiAutomation - INFO - Clicked menu item ID: 47\n",
            "2025-11-17 21:31:16 - ras_commander.RasGuiAutomation - INFO - Looking for Unsteady Flow Analysis dialog...\n",
            "2025-11-17 21:31:16 - ras_commander.RasGuiAutomation - INFO - Found Unsteady Flow Analysis dialog\n",
            "2025-11-17 21:31:16 - ras_commander.RasGuiAutomation - INFO - Looking for Compute button...\n",
            "2025-11-17 21:31:16 - ras_commander.RasGuiAutomation - WARNING - Could not find Compute button - user must click manually\n",
            "2025-11-17 21:31:16 - ras_commander.RasGuiAutomation - INFO - Trying keyboard shortcut as fallback...\n",
            "2025-11-17 21:31:17 - ras_commander.RasGuiAutomation - INFO - Sent Enter key to dialog\n",
            "2025-11-17 21:31:17 - ras_commander.RasGuiAutomation - INFO - Waiting for user to close HEC-RAS...\n",
            "2025-11-17 21:31:17 - ras_commander.RasGuiAutomation - INFO - Please monitor plan 06 execution and close HEC-RAS when complete\n",
            "2025-11-17 21:34:49 - ras_commander.RasGuiAutomation - INFO - HEC-RAS has been closed\n",
            "2025-11-17 21:34:49 - ras_commander.RasMap - INFO - Floodplain mapping computation successful.\n",
            "2025-11-17 21:34:49 - ras_commander.RasMap - INFO - Restoring original plan file from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.p06.storedmap.bak\n",
            "2025-11-17 21:34:49 - ras_commander.RasMap - INFO - Restoring original rasmap file from C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap.storedmap.bak\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully generated stored maps.\n"
          ]
        }
      ],
      "source": [
        "# First, let's get the names of available terrains\n",
        "rasmap_path = ras.project_folder / f\"{ras.project_name}.rasmap\"\n",
        "terrains = RasMap.get_terrain_names(rasmap_path)\n",
        "print(f\"Available terrains: {terrains}\")\n",
        "\n",
        "# Specify the terrain we want to use for mapping\n",
        "target_terrain = None\n",
        "if terrains:\n",
        "    target_terrain = terrains[0]\n",
        "    print(f\"Using terrain: {target_terrain}\")\n",
        "\n",
        "# Generate the stored maps for our plan and terrain\n",
        "success = RasMap.postprocess_stored_maps(\n",
        "    plan_number=plan_number,\n",
        "    specify_terrain=target_terrain,\n",
        "    layers=['Depth', 'WSEL']  # Let's just generate Depth and WSEL for this example\n",
        ")\n",
        "\n",
        "if success:\n",
        "    print(\"Successfully generated stored maps.\")\n",
        "else:\n",
        "    print(\"Failed to generate stored maps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Verify the Output\n",
        "\n",
        "The `postprocess_stored_maps` function should have created a folder named after the plan's `Short Identifier` (or the plan name if no short ID exists) inside the project directory. Let's find the generated `.tif` file for Depth and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:10:16.389696Z",
          "iopub.status.busy": "2025-11-17T19:10:16.389476Z",
          "iopub.status.idle": "2025-11-17T19:10:16.394796Z",
          "shell.execute_reply": "2025-11-17T19:10:16.394271Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found depth map at: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Grid Precip Infiltration\\Depth (Max).vrt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Find the output directory and the depth map file\n",
        "plan_info = ras.plan_df[ras.plan_df['plan_number'] == plan_number].iloc[0]\n",
        "short_id = plan_info.get('Short Identifier', f'Plan_{plan_number}')\n",
        "output_folder = ras.project_folder / short_id\n",
        "\n",
        "# HEC-RAS creates a VRT file that points to the actual TIF(s)\n",
        "depth_map_path = output_folder / \"Depth (Max).vrt\"\n",
        "\n",
        "if depth_map_path.exists():\n",
        "    print(f\"Found depth map at: {depth_map_path}\")\n",
        "\n",
        "    # Open and plot the raster using rasterio\n",
        "    with rasterio.open(depth_map_path) as src:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "        show(src, ax=ax, cmap='Blues', title=f'Maximum Depth - Plan {plan_number}')\n",
        "        plt.show()\n",
        "else:\n",
        "    print(f\"Could not find the generated depth map at {depth_map_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to use the `RasMap.postprocess_stored_maps` function to automate a critical post-processing step. By programmatically generating stored maps, you can easily create the necessary outputs for all your plans without manual intervention in the RASMapper interface, significantly speeding up workflows that involve multiple scenarios or models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\16_automating_ras_with_win32com.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 20:20:29 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n"
          ]
        }
      ],
      "source": [
        "# If /example_projects/BaldEagleCrkMulti2D does not exist, extract it\n",
        "project_path = \"./example_projects/BaldEagleCrkMulti2D\"\n",
        "\n",
        "if not os.path.exists(project_path):\n",
        "    project_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\")\n",
        "\n",
        "# Initialize the RAS project\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "# Define the plan to work with\n",
        "plan_number = \"06\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MANUAL STEP REQUIRED: Update .rasmap to Version 6.x\n",
            "This project was created in HEC-RAS 5.0.7. To generate stored maps in HEC-RAS 6.x, you must:\n",
            "1. HEC-RAS will now be opened with this project.\n",
            "2. In HEC-RAS, open RAS Mapper (from the main toolbar).\n",
            "3. When prompted, allow RAS Mapper to update the .rasmap file to the new version.\n",
            "4. Once the update is complete, close RAS Mapper and exit HEC-RAS.\n",
            "\n",
            "After closing HEC-RAS, return here and continue running the notebook.\n",
            "============================================================\n",
            "\n",
            "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe \"C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj\"\n",
            "Opened HEC-RAS with Process ID: 100620\n",
            "Please wait for the next cell to automate RAS Mapper...\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Print instructions to the user\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MANUAL STEP REQUIRED: Update .rasmap to Version 6.x\")\n",
        "print(\"This project was created in HEC-RAS 5.0.7. To generate stored maps in HEC-RAS 6.x, you must:\")\n",
        "print(\"1. HEC-RAS will now be opened with this project.\")\n",
        "print(\"2. In HEC-RAS, open RAS Mapper (from the main toolbar).\") \n",
        "print(\"3. When prompted, allow RAS Mapper to update the .rasmap file to the new version.\")\n",
        "print(\"4. Once the update is complete, close RAS Mapper and exit HEC-RAS.\")\n",
        "print(\"\\nAfter closing HEC-RAS, return here and continue running the notebook.\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "ras_exe = ras.ras_exe_path\n",
        "prj_path = f'\"{str(ras.prj_file)}\"'  # Add quotes around project path\n",
        "\n",
        "command = f\"{ras_exe} {prj_path}\"\n",
        "print(command)\n",
        "\n",
        "try:\n",
        "    # Capture the process object so we can get its PID\n",
        "    if sys.platform == \"win32\":\n",
        "        hecras_process = subprocess.Popen(command)\n",
        "    else:\n",
        "        hecras_process = subprocess.Popen([ras_exe, prj_path])\n",
        "    \n",
        "    # Store the process ID for use in the next cell\n",
        "    hecras_pid = hecras_process.pid\n",
        "    print(f\"Opened HEC-RAS with Process ID: {hecras_pid}\")\n",
        "    print(\"Please wait for the next cell to automate RAS Mapper...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to launch HEC-RAS: {e}\")\n",
        "    hecras_pid = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pywinauto in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (0.6.9)\n",
            "Requirement already satisfied: six in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from pywinauto) (1.17.0)\n",
            "Requirement already satisfied: comtypes in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from pywinauto) (1.4.13)\n",
            "Requirement already satisfied: pywin32 in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_piptest\\lib\\site-packages (from pywinauto) (311)\n"
          ]
        }
      ],
      "source": [
        "!pip install pywinauto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for HEC-RAS windows for process ID: 100620\n",
            "\n",
            "Found main HEC-RAS window: 'HEC-RAS 6.6' (HWND: 1182634)\n",
            "============================================================\n",
            "\n",
            "--- Enumerating Menus (7 top-level) ---\n",
            "\n",
            "Menu 0: 'File'\n",
            "  Contains 27 items:\n",
            "    Item 0: '&New Project ...' (ID: 2)\n",
            "    Item 1: '&Open Project ...' (ID: 3)\n",
            "    Item 2: '&Save Project' (ID: 4)\n",
            "    Item 3: 'Save Project &As ...' (ID: 5)\n",
            "    Item 4: '&Rename Project Title ...' (ID: 6)\n",
            "    Item 5: '&Delete Project ...' (ID: 7)\n",
            "    Item 6: '' (ID: 8)\n",
            "    Item 7: '&Project Summary ...' (ID: 9)\n",
            "    Item 8: 'Compare Model Data ...' (ID: 10)\n",
            "    Item 9: '' (ID: 11)\n",
            "    Item 10: '&Import HEC-2 Data ...' (ID: 12)\n",
            "    Item 11: 'I&mport HEC-RAS Data ...' (ID: 13)\n",
            "    Item 12: '&Generate Report ...' (ID: 14)\n",
            "    Item 13: '&Export GIS Data ...' (ID: 15)\n",
            "    Item 14: 'Export to HEC-&DSS ...' (ID: 16)\n",
            "    Item 15: 'Restore Backup Data ' -> [Submenu]\n",
            "      - 'Restore Geometry ...' (ID: 18)\n",
            "      - 'Restore Steady Flow ...' (ID: 19)\n",
            "      - 'Restore Unsteady Flow ...' (ID: 20)\n",
            "      - 'Restore Plan ...' (ID: 21)\n",
            "      - 'Restore Hydr Design ...' (ID: 22)\n",
            "    Item 16: '' (ID: 23)\n",
            "    Item 17: 'Zip Plan(s) or Archive Project...' (ID: 24)\n",
            "    Item 18: '' (ID: 25)\n",
            "    Item 19: 'E&xit' (ID: 26)\n",
            "    Item 20: '' (ID: 27)\n",
            "    Item 21: 'C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj' (ID: 28)\n",
            "    Item 22: 'C:\\GH\\ras-commander\\examples\\example_projects\\Chippewa_2D\\Chippewa_2D.prj' (ID: 29)\n",
            "    Item 23: 'C:\\GH\\ras-commander\\research\\RasMapper Interpolation\\Test Data\\BaldEagleCrkMulti2D - Horizontal\\BaldEagleDamBrk.prj' (ID: 30)\n",
            "    Item 24: 'C:\\HCFCD\\Standard_Benefits_Process\\3 - Model Data\\Storm Interpolator - Log Linear\\Output Projects\\South_Belt_RAS66_2025-11-13_044255\\A120-00-00_RAS 4.1\\A100_00_00.prj' (ID: 31)\n",
            "    Item 25: 'C:\\HCFCD\\Standard_Benefits_Process\\3 - Model Data\\Storm Interpolator - Log Linear\\Input Projects\\A520-03-00-E003 - South Belt Stormwater Detention Basin\\A120-00-00_RAS 4.1\\A100_00_00.prj' (ID: 32)\n",
            "    Item 26: 'C:\\GH\\TNTech\\data\\EarlyDevTesting\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.prj' (ID: 33)\n",
            "\n",
            "Menu 1: 'Edit'\n",
            "  Contains 8 items:\n",
            "    Item 0: '&Geometric Data ...' (ID: 37)\n",
            "    Item 1: '' (ID: 38)\n",
            "    Item 2: '&Steady Flow Data ...' (ID: 39)\n",
            "    Item 3: '&Quasi Unsteady Flow (Sediment) ...' (ID: 40)\n",
            "    Item 4: '&Unsteady Flow Data ...' (ID: 41)\n",
            "    Item 5: '' (ID: 42)\n",
            "    Item 6: 'Se&diment Data ...' (ID: 43)\n",
            "    Item 7: '&Water Quality Data ...' (ID: 44)\n",
            "\n",
            "Menu 2: 'Run'\n",
            "  Contains 9 items:\n",
            "    Item 0: '&Steady Flow Analysis ...' (ID: 46)\n",
            "    Item 1: '&Unsteady Flow Analysis ...' (ID: 47)\n",
            "    Item 2: 'Quasi-Unsteady Analysis (Sediment)...' (ID: 48)\n",
            "    Item 3: 'Water Quality Analysis ...' (ID: 49)\n",
            "    Item 4: '&Hydraulic Design Functions ...' (ID: 50)\n",
            "    Item 5: '' (ID: 51)\n",
            "    Item 6: 'Run Multiple Plans ...' (ID: 52)\n",
            "    Item 7: '' (ID: 54)\n",
            "    Item 8: 'Uncertainty Analysis' -> [Submenu]\n",
            "      - 'Setup Parameters ...' (ID: 56)\n",
            "      - 'Run Monte Carlo Analysis ...' (ID: 57)\n",
            "      - 'Summarize Results' (ID: 58)\n",
            "\n",
            "Menu 3: 'View'\n",
            "  Contains 23 items:\n",
            "    Item 0: '&Cross-Sections ...' (ID: 61)\n",
            "    Item 1: '&Water Surface Profiles ...' (ID: 62)\n",
            "    Item 2: '&General Profile Plot ...' (ID: 63)\n",
            "    Item 3: '&Rating Curves ...' (ID: 64)\n",
            "    Item 4: '3D View ...' (ID: 65)\n",
            "    Item 5: '&X-Y-Z Perspective Plots (Classic) ...' (ID: 66)\n",
            "    Item 6: '&Stage and Flow Hydrographs ...' (ID: 67)\n",
            "    Item 7: '&Hydraulic Property Tables ...' (ID: 68)\n",
            "    Item 8: '' (ID: 69)\n",
            "    Item 9: '&Detailed Output Tables ...' (ID: 70)\n",
            "    Item 10: '&Profile Summary Table ...' (ID: 71)\n",
            "    Item 11: '&Summary Err,Warn, Notes ...' (ID: 72)\n",
            "    Item 12: '' (ID: 73)\n",
            "    Item 13: 'DSS Data ...' (ID: 74)\n",
            "    Item 14: '' (ID: 75)\n",
            "    Item 15: 'Unsteady Flow Spatial Plot (computation interval) ...' (ID: 76)\n",
            "    Item 16: 'Unsteady Flow Time Series Plot (computation interval) ...' (ID: 77)\n",
            "    Item 17: '' (ID: 78)\n",
            "    Item 18: 'WQ Spatial Plot ...' (ID: 79)\n",
            "    Item 19: 'WQ Time Series Plot ...' (ID: 80)\n",
            "    Item 20: '' (ID: 81)\n",
            "    Item 21: 'Sediment Output ...' (ID: 82)\n",
            "    Item 22: 'Legacy Sediment Output' -> [Submenu]\n",
            "      - 'Sediment Output (5.0)...' (ID: 84)\n",
            "      - '4.x - Sediment Spatial Plot ...' (ID: 85)\n",
            "      - '4.x - Sediment Time Series Plot ...' (ID: 86)\n",
            "      - '4.x - XS Bed Change Plot...' (ID: 87)\n",
            "\n",
            "Menu 4: 'Options'\n",
            "  Contains 5 items:\n",
            "    Item 0: '&Program Setup' -> [Submenu]\n",
            "      - 'Default File &Viewer ...' (ID: 91)\n",
            "      - 'Default Project Folder ...' (ID: 92)\n",
            "      - '&Open last project on startup' (ID: 93)\n",
            "      - 'Automatically Backup Data' (ID: 94)\n",
            "      - 'Set Time for Automatic Backup ...' (ID: 95)\n",
            "    Item 1: '&Default Parameters' -> [Submenu]\n",
            "      - '&Expansion and Contraction Coef ...' (ID: 97)\n",
            "    Item 2: '&Unit system (US Customary/SI) ...' (ID: 98)\n",
            "    Item 3: '&Convert Project Units ...' (ID: 99)\n",
            "    Item 4: 'Convert &Horizontal Coordinate Systems ...' (ID: 100)\n",
            "\n",
            "Menu 5: 'GIS Tools'\n",
            "  Contains 1 items:\n",
            "    Item 0: 'RAS Mapper ...' (ID: 102)\n",
            "\n",
            "Menu 6: 'Help'\n",
            "  Contains 21 items:\n",
            "    Item 0: 'HEC-RAS Online Documentation ...\tF1' (ID: 104)\n",
            "    Item 1: 'Release Notes ...' (ID: 105)\n",
            "    Item 2: 'Known Issues ...' (ID: 106)\n",
            "    Item 3: 'Community Support on Discourse ...' (ID: 107)\n",
            "    Item 4: '' (ID: 108)\n",
            "    Item 5: 'Users Manual ...' (ID: 109)\n",
            "    Item 6: '2D Modeling User's Manual \u2026' (ID: 110)\n",
            "    Item 7: 'RAS Mapper User's Manual \u2026' (ID: 111)\n",
            "    Item 8: 'Hydraulic Reference \u2026' (ID: 112)\n",
            "    Item 9: 'Applications Guide \u2026' (ID: 113)\n",
            "    Item 10: '1D Sediment User's Manual \u2026' (ID: 114)\n",
            "    Item 11: '2D Sediment User's Manual \u2026' (ID: 115)\n",
            "    Item 12: '2D Sediment Technical Reference \u2026' (ID: 116)\n",
            "    Item 13: 'Mud and Debris Manuals \u2026' (ID: 117)\n",
            "    Item 14: '' (ID: 118)\n",
            "    Item 15: 'Download Example Projects ...' (ID: 119)\n",
            "    Item 16: '' (ID: 120)\n",
            "    Item 17: 'HEC-RAS Webpage ...' (ID: 121)\n",
            "    Item 18: '' (ID: 122)\n",
            "    Item 19: 'View Terms and Conditions of Use ...' (ID: 123)\n",
            "    Item 20: '&About HEC-RAS ...' (ID: 124)\n",
            "\n",
            "--- Enumerating Child Controls (36 found) ---\n",
            "\n",
            "Control 0:\n",
            "  - HWND:        2427996\n",
            "  - Class Name:  'ThunderRT6Timer'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  1\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 438, T: 340, R: 438, B: 340)\n",
            "\n",
            "Control 1:\n",
            "  - HWND:        1050472\n",
            "  - Class Name:  'ThunderRT6TextBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  2\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 1066, T: 104, R: 1091, B: 121)\n",
            "\n",
            "Control 2:\n",
            "  - HWND:        919462\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  3\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 862, T: 100, R: 888, B: 126)\n",
            "\n",
            "Control 3:\n",
            "  - HWND:        1050616\n",
            "  - Class Name:  'ThunderRT6TextBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  4\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 1034, T: 104, R: 1063, B: 121)\n",
            "\n",
            "Control 4:\n",
            "  - HWND:        919270\n",
            "  - Class Name:  'ThunderRT6Timer'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  5\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 410, T: 340, R: 410, B: 340)\n",
            "\n",
            "Control 5:\n",
            "  - HWND:        4589300\n",
            "  - Class Name:  'ThunderRT6TextBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  6\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 1002, T: 104, R: 1031, B: 120)\n",
            "\n",
            "Control 6:\n",
            "  - HWND:        1706916\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  7\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 998, T: 100, R: 1024, B: 126)\n",
            "\n",
            "Control 7:\n",
            "  - HWND:        1445948\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  8\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 972, T: 100, R: 998, B: 126)\n",
            "\n",
            "Control 8:\n",
            "  - HWND:        657176\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  9\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 946, T: 100, R: 972, B: 126)\n",
            "\n",
            "Control 9:\n",
            "  - HWND:        1508260\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  10\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 920, T: 100, R: 946, B: 126)\n",
            "\n",
            "Control 10:\n",
            "  - HWND:        790556\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  11\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 894, T: 100, R: 920, B: 126)\n",
            "\n",
            "Control 11:\n",
            "  - HWND:        2232354\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  12\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 836, T: 100, R: 862, B: 126)\n",
            "\n",
            "Control 12:\n",
            "  - HWND:        790552\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  13\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 810, T: 100, R: 836, B: 126)\n",
            "\n",
            "Control 13:\n",
            "  - HWND:        657182\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  14\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 784, T: 100, R: 810, B: 126)\n",
            "\n",
            "Control 14:\n",
            "  - HWND:        2953504\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  15\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 758, T: 100, R: 784, B: 126)\n",
            "\n",
            "Control 15:\n",
            "  - HWND:        8262022\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  16\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 732, T: 100, R: 758, B: 126)\n",
            "\n",
            "Control 16:\n",
            "  - HWND:        1184556\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  17\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 706, T: 100, R: 732, B: 126)\n",
            "\n",
            "Control 17:\n",
            "  - HWND:        657152\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  18\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 672, T: 100, R: 701, B: 126)\n",
            "\n",
            "Control 18:\n",
            "  - HWND:        984952\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  19\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 641, T: 100, R: 667, B: 126)\n",
            "\n",
            "Control 19:\n",
            "  - HWND:        2297116\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  20\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 615, T: 100, R: 641, B: 126)\n",
            "\n",
            "Control 20:\n",
            "  - HWND:        4722740\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  21\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 589, T: 100, R: 615, B: 126)\n",
            "\n",
            "Control 21:\n",
            "  - HWND:        2690402\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  22\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 563, T: 100, R: 589, B: 126)\n",
            "\n",
            "Control 22:\n",
            "  - HWND:        2887252\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  23\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 537, T: 100, R: 563, B: 126)\n",
            "\n",
            "Control 23:\n",
            "  - HWND:        1445516\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  24\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 505, T: 100, R: 531, B: 126)\n",
            "\n",
            "Control 24:\n",
            "  - HWND:        10948336\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  25\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 479, T: 100, R: 505, B: 126)\n",
            "\n",
            "Control 25:\n",
            "  - HWND:        2822022\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  26\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 448, T: 100, R: 474, B: 126)\n",
            "\n",
            "Control 26:\n",
            "  - HWND:        4198340\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  27\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 422, T: 100, R: 448, B: 126)\n",
            "\n",
            "Control 27:\n",
            "  - HWND:        723576\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  28\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 396, T: 100, R: 422, B: 126)\n",
            "\n",
            "Control 28:\n",
            "  - HWND:        2493512\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  29\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 370, T: 100, R: 396, B: 126)\n",
            "\n",
            "Control 29:\n",
            "  - HWND:        1969932\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  30\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 339, T: 100, R: 365, B: 126)\n",
            "\n",
            "Control 30:\n",
            "  - HWND:        2953618\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  31\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 313, T: 100, R: 339, B: 126)\n",
            "\n",
            "Control 31:\n",
            "  - HWND:        12914684\n",
            "  - Class Name:  'ThunderRT6CheckBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  32\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 1013, T: 236, R: 1030, B: 257)\n",
            "\n",
            "Control 32:\n",
            "  - HWND:        3543782\n",
            "  - Class Name:  'ThunderRT6CommandButton'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  33\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 1130, T: 135, R: 1151, B: 154)\n",
            "\n",
            "Control 33:\n",
            "  - HWND:        1510746\n",
            "  - Class Name:  'ThunderRT6PictureBoxDC'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  34\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 374, T: 340, R: 403, B: 362)\n",
            "\n",
            "Control 34:\n",
            "  - HWND:        3081052\n",
            "  - Class Name:  'ThunderRT6Timer'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  35\n",
            "  - Visible:     False\n",
            "  - Position:    (L: 350, T: 340, R: 350, B: 340)\n",
            "\n",
            "Control 35:\n",
            "  - HWND:        2625852\n",
            "  - Class Name:  'ThunderRT6TextBox'\n",
            "  - Text/Caption: ''\n",
            "  - Control ID:  36\n",
            "  - Visible:     True\n",
            "  - Position:    (L: 398, T: 236, R: 1009, B: 257)\n",
            "\n",
            "============================================================\n",
            "Inspection complete. The lists above show all menus and controls discoverable with pywin32.\n"
          ]
        }
      ],
      "source": [
        "# This cell will inspect the HEC-RAS window and list all interactable elements.\n",
        "# It also provides an example of how to use the pywinauto library for more robust automation,\n",
        "# including interaction with 64-bit processes like RAS Mapper.\n",
        "\n",
        "import win32gui\n",
        "import win32con\n",
        "import win32api\n",
        "import win32process\n",
        "import time\n",
        "import ctypes\n",
        "from ctypes import wintypes\n",
        "\n",
        "# ==============================================================================\n",
        "# Part 1: Inspecting the HEC-RAS Window with pywin32\n",
        "# ==============================================================================\n",
        "# This section uses the original pywin32 approach to list all menus and\n",
        "# child controls (buttons, text boxes, etc.) of the main HEC-RAS window.\n",
        "\n",
        "# Constants from original script\n",
        "MF_BYPOSITION = 0x00000400\n",
        "\n",
        "def get_windows_by_pid(pid):\n",
        "    \"\"\"Find all windows belonging to a specific process ID\"\"\"\n",
        "    def callback(hwnd, hwnds):\n",
        "        if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):\n",
        "            _, window_pid = win32process.GetWindowThreadProcessId(hwnd)\n",
        "            if window_pid == pid:\n",
        "                window_title = win32gui.GetWindowText(hwnd)\n",
        "                if window_title:\n",
        "                    hwnds.append((hwnd, window_title))\n",
        "        return True\n",
        "    hwnds = []\n",
        "    win32gui.EnumWindows(callback, hwnds)\n",
        "    return hwnds\n",
        "\n",
        "def find_main_hecras_window(windows):\n",
        "    \"\"\"Find the main HEC-RAS window from a list of windows\"\"\"\n",
        "    for hwnd, title in windows:\n",
        "        if \"HEC-RAS\" in title and win32gui.GetMenu(hwnd):\n",
        "            return hwnd, title\n",
        "    return None, None\n",
        "\n",
        "def get_menu_string(menu_handle, pos):\n",
        "    \"\"\"Get menu item string at position\"\"\"\n",
        "    buf_size = 256\n",
        "    buf = ctypes.create_unicode_buffer(buf_size)\n",
        "    user32 = ctypes.windll.user32\n",
        "    result = user32.GetMenuStringW(menu_handle, pos, buf, buf_size, MF_BYPOSITION)\n",
        "    if result:\n",
        "        return buf.value\n",
        "    return \"\"\n",
        "\n",
        "def enumerate_all_menus(hwnd):\n",
        "    \"\"\"Enumerate all menus and their items in great detail.\"\"\"\n",
        "    menu_bar = win32gui.GetMenu(hwnd)\n",
        "    if not menu_bar:\n",
        "        print(\"No menu bar found on the window.\")\n",
        "        return\n",
        "\n",
        "    menu_count = win32gui.GetMenuItemCount(menu_bar)\n",
        "    print(f\"\\n--- Enumerating Menus ({menu_count} top-level) ---\")\n",
        "\n",
        "    for i in range(menu_count):\n",
        "        menu_text = get_menu_string(menu_bar, i).replace('&', '')\n",
        "        submenu = win32gui.GetSubMenu(menu_bar, i)\n",
        "        print(f\"\\nMenu {i}: '{menu_text}'\")\n",
        "\n",
        "        if submenu:\n",
        "            item_count = win32gui.GetMenuItemCount(submenu)\n",
        "            print(f\"  Contains {item_count} items:\")\n",
        "            for j in range(item_count):\n",
        "                item_text = get_menu_string(submenu, j)\n",
        "                menu_id = win32gui.GetMenuItemID(submenu, j)\n",
        "                \n",
        "                id_str = f\"(ID: {menu_id})\" if menu_id != -1 and menu_id != 0 else \"\"\n",
        "                \n",
        "                sub_submenu = win32gui.GetSubMenu(submenu, j)\n",
        "                if sub_submenu:\n",
        "                    print(f\"    Item {j}: '{item_text}' -> [Submenu]\")\n",
        "                    sub_item_count = win32gui.GetMenuItemCount(sub_submenu)\n",
        "                    for k in range(sub_item_count):\n",
        "                        sub_item_text = get_menu_string(sub_submenu, k)\n",
        "                        sub_menu_id = win32gui.GetMenuItemID(sub_submenu, k)\n",
        "                        sub_id_str = f\"(ID: {sub_menu_id})\" if sub_menu_id != -1 and sub_menu_id != 0 else \"\"\n",
        "                        print(f\"      - '{sub_item_text}' {sub_id_str}\")\n",
        "                else:\n",
        "                    print(f\"    Item {j}: '{item_text}' {id_str}\")\n",
        "        else:\n",
        "            print(\"  (This top-level item is not a menu)\")\n",
        "\n",
        "def enumerate_child_controls(hwnd):\n",
        "    \"\"\"Enumerates all child controls (widgets) of a window.\"\"\"\n",
        "    child_windows = []\n",
        "    def callback(child_hwnd, _):\n",
        "        child_windows.append(child_hwnd)\n",
        "        return True\n",
        "\n",
        "    win32gui.EnumChildWindows(hwnd, callback, None)\n",
        "    \n",
        "    print(f\"\\n--- Enumerating Child Controls ({len(child_windows)} found) ---\")\n",
        "    if not child_windows:\n",
        "        print(\"No child controls found.\")\n",
        "        return\n",
        "        \n",
        "    for i, child_hwnd in enumerate(child_windows):\n",
        "        class_name = win32gui.GetClassName(child_hwnd)\n",
        "        window_text = win32gui.GetWindowText(child_hwnd)\n",
        "        control_id = win32gui.GetDlgCtrlID(child_hwnd)\n",
        "        \n",
        "        style = win32gui.GetWindowLong(child_hwnd, win32con.GWL_STYLE)\n",
        "        is_visible = (style & win32con.WS_VISIBLE) != 0\n",
        "        \n",
        "        rect = win32gui.GetWindowRect(child_hwnd)\n",
        "        \n",
        "        print(f\"\\nControl {i}:\")\n",
        "        print(f\"  - HWND:        {child_hwnd}\")\n",
        "        print(f\"  - Class Name:  '{class_name}'\")\n",
        "        print(f\"  - Text/Caption: '{window_text}'\")\n",
        "        print(f\"  - Control ID:  {control_id}\")\n",
        "        print(f\"  - Visible:     {is_visible}\")\n",
        "        print(f\"  - Position:    (L: {rect[0]}, T: {rect[1]}, R: {rect[2]}, B: {rect[3]})\")\n",
        "\n",
        "# Main execution for pywin32 inspection\n",
        "if 'hecras_pid' not in globals() or hecras_pid is None:\n",
        "    print(\"ERROR: HEC-RAS process ID not found. Please run the previous cell to launch HEC-RAS first.\")\n",
        "else:\n",
        "    print(f\"Looking for HEC-RAS windows for process ID: {hecras_pid}\")\n",
        "    time.sleep(2)\n",
        "    \n",
        "    windows = get_windows_by_pid(hecras_pid)\n",
        "    if not windows:\n",
        "        print(f\"Could not find any windows for process ID {hecras_pid}\")\n",
        "    else:\n",
        "        hec_ras_hwnd, title = find_main_hecras_window(windows)\n",
        "        if not hec_ras_hwnd:\n",
        "            print(\"Could not identify the main HEC-RAS window from the found windows:\")\n",
        "            for hwnd, title in windows:\n",
        "                print(f\"  - {title} (HWND: {hwnd})\")\n",
        "        else:\n",
        "            print(f\"\\nFound main HEC-RAS window: '{title}' (HWND: {hec_ras_hwnd})\")\n",
        "            print(\"=\"*60)\n",
        "            \n",
        "            enumerate_all_menus(hec_ras_hwnd)\n",
        "            enumerate_child_controls(hec_ras_hwnd)\n",
        "            \n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"Inspection complete. The lists above show all menus and controls discoverable with pywin32.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Performing detailed menu enumeration...\n",
            "\n",
            "=== Complete Menu Tree Analysis ===\n",
            "\n",
            "\n",
            "Top Level Menu 0: &File\n",
            "Details: {'id': 2428699, 'type_flags': [], 'state': [], 'text': '&File', 'has_submenu': True}\n",
            "Contains 27 items:\n",
            "  \u2514\u2500 Item 0: &New Project ...\n",
            "     Details: {'id': 2, 'type_flags': [], 'state': [], 'text': '&New Project ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Open Project ...\n",
            "     Details: {'id': 3, 'type_flags': [], 'state': [], 'text': '&Open Project ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &Save Project\n",
            "     Details: {'id': 4, 'type_flags': [], 'state': [], 'text': '&Save Project', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: Save Project &As ...\n",
            "     Details: {'id': 5, 'type_flags': [], 'state': [], 'text': 'Save Project &As ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: &Rename Project Title ...\n",
            "     Details: {'id': 6, 'type_flags': [], 'state': [], 'text': '&Rename Project Title ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: &Delete Project ...\n",
            "     Details: {'id': 7, 'type_flags': [], 'state': [], 'text': '&Delete Project ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: \n",
            "     Details: {'id': 8, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: &Project Summary ...\n",
            "     Details: {'id': 9, 'type_flags': [], 'state': [], 'text': '&Project Summary ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: Compare Model Data ...\n",
            "     Details: {'id': 10, 'type_flags': [], 'state': [], 'text': 'Compare Model Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 9: \n",
            "     Details: {'id': 11, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  ... and 17 more items\n",
            "\n",
            "Top Level Menu 1: &Edit\n",
            "Details: {'id': 12519335, 'type_flags': [], 'state': [], 'text': '&Edit', 'has_submenu': True}\n",
            "Contains 8 items:\n",
            "  \u2514\u2500 Item 0: &Geometric Data ...\n",
            "     Details: {'id': 37, 'type_flags': [], 'state': [], 'text': '&Geometric Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: \n",
            "     Details: {'id': 38, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &Steady Flow Data ...\n",
            "     Details: {'id': 39, 'type_flags': [], 'state': [], 'text': '&Steady Flow Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: &Quasi Unsteady Flow (Sediment) ...\n",
            "     Details: {'id': 40, 'type_flags': [], 'state': [], 'text': '&Quasi Unsteady Flow (Sediment) ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: &Unsteady Flow Data ...\n",
            "     Details: {'id': 41, 'type_flags': [], 'state': [], 'text': '&Unsteady Flow Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: \n",
            "     Details: {'id': 42, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: Se&diment Data ...\n",
            "     Details: {'id': 43, 'type_flags': [], 'state': [], 'text': 'Se&diment Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: &Water Quality Data ...\n",
            "     Details: {'id': 44, 'type_flags': [], 'state': [], 'text': '&Water Quality Data ...', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 2: &Run\n",
            "Details: {'id': 1380203, 'type_flags': [], 'state': [], 'text': '&Run', 'has_submenu': True}\n",
            "Contains 9 items:\n",
            "  \u2514\u2500 Item 0: &Steady Flow Analysis ...\n",
            "     Details: {'id': 46, 'type_flags': [], 'state': [], 'text': '&Steady Flow Analysis ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Unsteady Flow Analysis ...\n",
            "     Details: {'id': 47, 'type_flags': [], 'state': [], 'text': '&Unsteady Flow Analysis ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: Quasi-Unsteady Analysis (Sediment)...\n",
            "     Details: {'id': 48, 'type_flags': [], 'state': [], 'text': 'Quasi-Unsteady Analysis (Sediment)...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: Water Quality Analysis ...\n",
            "     Details: {'id': 49, 'type_flags': [], 'state': [], 'text': 'Water Quality Analysis ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: &Hydraulic Design Functions ...\n",
            "     Details: {'id': 50, 'type_flags': [], 'state': [], 'text': '&Hydraulic Design Functions ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: \n",
            "     Details: {'id': 51, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: Run Multiple Plans ...\n",
            "     Details: {'id': 52, 'type_flags': [], 'state': [], 'text': 'Run Multiple Plans ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: \n",
            "     Details: {'id': 54, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: Uncertainty Analysis\n",
            "     Details: {'id': 4327313, 'type_flags': [], 'state': ['DISABLED', 'GRAYED'], 'text': 'Uncertainty Analysis', 'has_submenu': True}\n",
            "     Has submenu with 3 items:\n",
            "       \u2514\u2500 Sub-item 0: Setup Parameters ...\n",
            "          Details: {'id': 56, 'type_flags': [], 'state': [], 'text': 'Setup Parameters ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 1: Run Monte Carlo Analysis ...\n",
            "          Details: {'id': 57, 'type_flags': [], 'state': [], 'text': 'Run Monte Carlo Analysis ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 2: Summarize Results\n",
            "          Details: {'id': 58, 'type_flags': [], 'state': [], 'text': 'Summarize Results', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 3: &View\n",
            "Details: {'id': 136188355, 'type_flags': [], 'state': [], 'text': '&View', 'has_submenu': True}\n",
            "Contains 23 items:\n",
            "  \u2514\u2500 Item 0: &Cross-Sections ...\n",
            "     Details: {'id': 61, 'type_flags': [], 'state': [], 'text': '&Cross-Sections ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Water Surface Profiles ...\n",
            "     Details: {'id': 62, 'type_flags': [], 'state': [], 'text': '&Water Surface Profiles ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &General Profile Plot ...\n",
            "     Details: {'id': 63, 'type_flags': [], 'state': [], 'text': '&General Profile Plot ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: &Rating Curves ...\n",
            "     Details: {'id': 64, 'type_flags': [], 'state': [], 'text': '&Rating Curves ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: 3D View ...\n",
            "     Details: {'id': 65, 'type_flags': [], 'state': [], 'text': '3D View ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: &X-Y-Z Perspective Plots (Classic) ...\n",
            "     Details: {'id': 66, 'type_flags': [], 'state': [], 'text': '&X-Y-Z Perspective Plots (Classic) ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: &Stage and Flow Hydrographs ...\n",
            "     Details: {'id': 67, 'type_flags': [], 'state': [], 'text': '&Stage and Flow Hydrographs ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: &Hydraulic Property Tables ...\n",
            "     Details: {'id': 68, 'type_flags': [], 'state': [], 'text': '&Hydraulic Property Tables ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: \n",
            "     Details: {'id': 69, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 9: &Detailed Output Tables ...\n",
            "     Details: {'id': 70, 'type_flags': [], 'state': [], 'text': '&Detailed Output Tables ...', 'has_submenu': False}\n",
            "  ... and 13 more items\n",
            "\n",
            "Top Level Menu 4: &Options\n",
            "Details: {'id': 1050507, 'type_flags': [], 'state': [], 'text': '&Options', 'has_submenu': True}\n",
            "Contains 5 items:\n",
            "  \u2514\u2500 Item 0: &Program Setup\n",
            "     Details: {'id': 348720343, 'type_flags': [], 'state': [], 'text': '&Program Setup', 'has_submenu': True}\n",
            "     Has submenu with 5 items:\n",
            "       \u2514\u2500 Sub-item 0: Default File &Viewer ...\n",
            "          Details: {'id': 91, 'type_flags': [], 'state': [], 'text': 'Default File &Viewer ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 1: Default Project Folder ...\n",
            "          Details: {'id': 92, 'type_flags': [], 'state': [], 'text': 'Default Project Folder ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 2: &Open last project on startup\n",
            "          Details: {'id': 93, 'type_flags': [], 'state': [], 'text': '&Open last project on startup', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 3: Automatically Backup Data\n",
            "          Details: {'id': 94, 'type_flags': [], 'state': ['CHECKED'], 'text': 'Automatically Backup Data', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 4: Set Time for Automatic Backup ...\n",
            "          Details: {'id': 95, 'type_flags': [], 'state': [], 'text': 'Set Time for Automatic Backup ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Default Parameters\n",
            "     Details: {'id': 139527055, 'type_flags': [], 'state': [], 'text': '&Default Parameters', 'has_submenu': True}\n",
            "     Has submenu with 1 items:\n",
            "       \u2514\u2500 Sub-item 0: &Expansion and Contraction Coef ...\n",
            "          Details: {'id': 97, 'type_flags': [], 'state': [], 'text': '&Expansion and Contraction Coef ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &Unit system (US Customary/SI) ...\n",
            "     Details: {'id': 98, 'type_flags': [], 'state': [], 'text': '&Unit system (US Customary/SI) ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: &Convert Project Units ...\n",
            "     Details: {'id': 99, 'type_flags': [], 'state': [], 'text': '&Convert Project Units ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: Convert &Horizontal Coordinate Systems ...\n",
            "     Details: {'id': 100, 'type_flags': [], 'state': [], 'text': 'Convert &Horizontal Coordinate Systems ...', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 5: &GIS Tools\n",
            "Details: {'id': 3737661, 'type_flags': [], 'state': [], 'text': '&GIS Tools', 'has_submenu': True}\n",
            "Contains 1 items:\n",
            "  \u2514\u2500 Item 0: RAS Mapper ...\n",
            "     Details: {'id': 102, 'type_flags': [], 'state': [], 'text': 'RAS Mapper ...', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 6: &Help\n",
            "Details: {'id': 2297713, 'type_flags': [], 'state': [], 'text': '&Help', 'has_submenu': True}\n",
            "Contains 21 items:\n",
            "  \u2514\u2500 Item 0: HEC-RAS Online Documentation ...\tF1\n",
            "     Details: {'id': 104, 'type_flags': [], 'state': [], 'text': 'HEC-RAS Online Documentation ...\\tF1', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: Release Notes ...\n",
            "     Details: {'id': 105, 'type_flags': [], 'state': [], 'text': 'Release Notes ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: Known Issues ...\n",
            "     Details: {'id': 106, 'type_flags': [], 'state': [], 'text': 'Known Issues ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: Community Support on Discourse ...\n",
            "     Details: {'id': 107, 'type_flags': [], 'state': [], 'text': 'Community Support on Discourse ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: \n",
            "     Details: {'id': 108, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: Users Manual ...\n",
            "     Details: {'id': 109, 'type_flags': [], 'state': [], 'text': 'Users Manual ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: 2D Modeling User's Manual \u2026\n",
            "     Details: {'id': 110, 'type_flags': [], 'state': [], 'text': \"2D Modeling User's Manual \u2026\", 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: RAS Mapper User's Manual \u2026\n",
            "     Details: {'id': 111, 'type_flags': [], 'state': [], 'text': \"RAS Mapper User's Manual \u2026\", 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: Hydraulic Reference \u2026\n",
            "     Details: {'id': 112, 'type_flags': [], 'state': [], 'text': 'Hydraulic Reference \u2026', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 9: Applications Guide \u2026\n",
            "     Details: {'id': 113, 'type_flags': [], 'state': [], 'text': 'Applications Guide \u2026', 'has_submenu': False}\n",
            "  ... and 11 more items\n",
            "\n",
            "================================================================================\n",
            "Performing detailed window hierarchy enumeration...\n",
            "Main window: HEC-RAS 6.6\n",
            "Window Handle: 1182634\n",
            "Class: ThunderRT6FormDC\n",
            "Text: HEC-RAS 6.6\n",
            "Position: (307, 50, 1157, 266)\n",
            "Style: 0x16CA0000\n",
            "Extended Style: 0x00040100\n",
            "Children: 36 found\n",
            "---\n",
            "  Window Handle: 2427996\n",
            "  Class: ThunderRT6Timer\n",
            "  Text: \n",
            "  Position: (438, 340, 438, 340)\n",
            "  Style: 0x44010000\n",
            "  Extended Style: 0x00000004\n",
            "---\n",
            "  Window Handle: 1050472\n",
            "  Class: ThunderRT6TextBox\n",
            "  Text: \n",
            "  Position: (1066, 104, 1091, 121)\n",
            "  Style: 0x440100C0\n",
            "  Extended Style: 0x00000204\n",
            "---\n",
            "  Window Handle: 919462\n",
            "  Class: ThunderRT6CheckBox\n",
            "  Text: \n",
            "  Position: (862, 100, 888, 126)\n",
            "  Style: 0x5401000B\n",
            "  Extended Style: 0x00000004\n",
            "---\n",
            "  Window Handle: 1050616\n",
            "  Class: ThunderRT6TextBox\n",
            "  Text: \n",
            "  Position: (1034, 104, 1063, 121)\n",
            "  Style: 0x440100C0\n",
            "  Extended Style: 0x00000204\n",
            "---\n",
            "  Window Handle: 919270\n",
            "  Class: ThunderRT6Timer\n",
            "  Text: \n",
            "  Position: (410, 340, 410, 340)\n",
            "  Style: 0x44010000\n",
            "  Extended Style: 0x00000004\n",
            "... and 31 more children\n",
            "\n",
            "================================================================================\n",
            "Detailed enumeration complete.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Part 1b: Deeper Menu and Window Object Enumeration using ctypes\n",
        "# ==============================================================================\n",
        "\n",
        "import win32gui\n",
        "import win32con\n",
        "import win32api\n",
        "import win32process\n",
        "import ctypes\n",
        "from ctypes import wintypes\n",
        "\n",
        "# Define MENUITEMINFO structure using ctypes\n",
        "class MENUITEMINFO(ctypes.Structure):\n",
        "    _fields_ = [\n",
        "        (\"cbSize\", wintypes.UINT),\n",
        "        (\"fMask\", wintypes.UINT),\n",
        "        (\"fType\", wintypes.UINT),\n",
        "        (\"fState\", wintypes.UINT),\n",
        "        (\"wID\", wintypes.UINT),\n",
        "        (\"hSubMenu\", wintypes.HMENU),\n",
        "        (\"hbmpChecked\", wintypes.HBITMAP),\n",
        "        (\"hbmpUnchecked\", wintypes.HBITMAP),\n",
        "        (\"dwItemData\", ctypes.POINTER(ctypes.c_ulong)),\n",
        "        (\"dwTypeData\", wintypes.LPWSTR),\n",
        "        (\"cch\", wintypes.UINT),\n",
        "        (\"hbmpItem\", wintypes.HBITMAP)\n",
        "    ]\n",
        "\n",
        "def get_menu_string(menu_handle, pos):\n",
        "    \"\"\"Get menu item string at position\"\"\"\n",
        "    buf_size = 256\n",
        "    buf = ctypes.create_unicode_buffer(buf_size)\n",
        "    user32 = ctypes.windll.user32\n",
        "    result = user32.GetMenuStringW(menu_handle, pos, buf, buf_size, MF_BYPOSITION)\n",
        "    if result:\n",
        "        return buf.value\n",
        "    return \"\"\n",
        "\n",
        "def enumerate_menu_item_details(menu_handle, item_index):\n",
        "    \"\"\"Get detailed information about a menu item using ctypes\"\"\"\n",
        "    # Create and initialize MENUITEMINFO structure\n",
        "    mii = MENUITEMINFO()\n",
        "    mii.cbSize = ctypes.sizeof(MENUITEMINFO)\n",
        "    mii.fMask = win32con.MIIM_STATE | win32con.MIIM_ID | win32con.MIIM_TYPE | win32con.MIIM_SUBMENU\n",
        "    \n",
        "    # Call GetMenuItemInfo using ctypes\n",
        "    user32 = ctypes.windll.user32\n",
        "    result = user32.GetMenuItemInfoW(\n",
        "        menu_handle, \n",
        "        item_index, \n",
        "        True,  # fByPosition\n",
        "        ctypes.byref(mii)\n",
        "    )\n",
        "    \n",
        "    if result:\n",
        "        # Parse state flags\n",
        "        state_flags = []\n",
        "        if mii.fState & win32con.MFS_CHECKED:\n",
        "            state_flags.append(\"CHECKED\")\n",
        "        if mii.fState & win32con.MFS_DISABLED:\n",
        "            state_flags.append(\"DISABLED\")\n",
        "        if mii.fState & win32con.MFS_GRAYED:\n",
        "            state_flags.append(\"GRAYED\")\n",
        "        if mii.fState & win32con.MFS_HILITE:\n",
        "            state_flags.append(\"HIGHLIGHTED\")\n",
        "        if mii.fState & win32con.MFS_DEFAULT:\n",
        "            state_flags.append(\"DEFAULT\")\n",
        "            \n",
        "        # Parse type flags\n",
        "        type_flags = []\n",
        "        if mii.fType & win32con.MFT_STRING:\n",
        "            type_flags.append(\"STRING\")\n",
        "        if mii.fType & win32con.MFT_SEPARATOR:\n",
        "            type_flags.append(\"SEPARATOR\")\n",
        "        if mii.fType & win32con.MFT_BITMAP:\n",
        "            type_flags.append(\"BITMAP\")\n",
        "        if mii.fType & win32con.MFT_OWNERDRAW:\n",
        "            type_flags.append(\"OWNERDRAW\")\n",
        "        \n",
        "        return {\n",
        "            \"id\": mii.wID,\n",
        "            \"type_flags\": type_flags,\n",
        "            \"state\": state_flags,\n",
        "            \"text\": get_menu_string(menu_handle, item_index),\n",
        "            \"has_submenu\": bool(mii.hSubMenu)\n",
        "        }\n",
        "    else:\n",
        "        # Fallback to simpler approach if GetMenuItemInfo fails\n",
        "        try:\n",
        "            menu_id = win32gui.GetMenuItemID(menu_handle, item_index)\n",
        "            menu_state = win32gui.GetMenuState(menu_handle, item_index, win32con.MF_BYPOSITION)\n",
        "            \n",
        "            state_flags = []\n",
        "            if menu_state & win32con.MF_CHECKED:\n",
        "                state_flags.append(\"CHECKED\")\n",
        "            if menu_state & win32con.MF_DISABLED:\n",
        "                state_flags.append(\"DISABLED\")\n",
        "            if menu_state & win32con.MF_GRAYED:\n",
        "                state_flags.append(\"GRAYED\")\n",
        "            if menu_state & win32con.MF_SEPARATOR:\n",
        "                state_flags.append(\"SEPARATOR\")\n",
        "            \n",
        "            return {\n",
        "                \"id\": menu_id if menu_id != -1 else None,\n",
        "                \"state\": state_flags,\n",
        "                \"text\": get_menu_string(menu_handle, item_index),\n",
        "                \"has_submenu\": win32gui.GetSubMenu(menu_handle, item_index) is not None,\n",
        "                \"fallback\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"text\": get_menu_string(menu_handle, item_index)\n",
        "            }\n",
        "\n",
        "def enumerate_window_details(hwnd, indent=0):\n",
        "    \"\"\"Recursively enumerate window details including styles and extended styles\"\"\"\n",
        "    if not hwnd:\n",
        "        return\n",
        "        \n",
        "    # Get basic window info\n",
        "    class_name = win32gui.GetClassName(hwnd)\n",
        "    window_text = win32gui.GetWindowText(hwnd)\n",
        "    \n",
        "    # Get window styles\n",
        "    style = win32gui.GetWindowLong(hwnd, win32con.GWL_STYLE)\n",
        "    ex_style = win32gui.GetWindowLong(hwnd, win32con.GWL_EXSTYLE)\n",
        "    \n",
        "    # Get window metrics\n",
        "    rect = win32gui.GetWindowRect(hwnd)\n",
        "    \n",
        "    # Print window details\n",
        "    indent_str = \"  \" * indent\n",
        "    print(f\"{indent_str}Window Handle: {hwnd}\")\n",
        "    print(f\"{indent_str}Class: {class_name}\")\n",
        "    print(f\"{indent_str}Text: {window_text}\")\n",
        "    print(f\"{indent_str}Position: {rect}\")\n",
        "    print(f\"{indent_str}Style: 0x{style:08X}\")\n",
        "    print(f\"{indent_str}Extended Style: 0x{ex_style:08X}\")\n",
        "    \n",
        "    # Enumerate child windows recursively (limit depth to avoid too much output)\n",
        "    if indent < 3:  # Limit recursion depth\n",
        "        try:\n",
        "            child_windows = []\n",
        "            win32gui.EnumChildWindows(\n",
        "                hwnd,\n",
        "                lambda child_hwnd, windows: windows.append(child_hwnd) or True,\n",
        "                child_windows\n",
        "            )\n",
        "            \n",
        "            if child_windows:\n",
        "                print(f\"{indent_str}Children: {len(child_windows)} found\")\n",
        "                for child_hwnd in child_windows[:5]:  # Show first 5 children\n",
        "                    print(f\"{indent_str}---\")\n",
        "                    enumerate_window_details(child_hwnd, indent + 1)\n",
        "                if len(child_windows) > 5:\n",
        "                    print(f\"{indent_str}... and {len(child_windows) - 5} more children\")\n",
        "        except Exception as e:\n",
        "            print(f\"{indent_str}Error enumerating children: {e}\")\n",
        "\n",
        "def enumerate_full_menu_tree(hwnd):\n",
        "    \"\"\"Enumerate complete menu tree with all available details\"\"\"\n",
        "    menu_bar = win32gui.GetMenu(hwnd)\n",
        "    if not menu_bar:\n",
        "        print(\"No menu bar found\")\n",
        "        return\n",
        "        \n",
        "    print(\"\\n=== Complete Menu Tree Analysis ===\\n\")\n",
        "    \n",
        "    menu_count = win32gui.GetMenuItemCount(menu_bar)\n",
        "    for i in range(menu_count):\n",
        "        menu_text = get_menu_string(menu_bar, i)\n",
        "        menu_details = enumerate_menu_item_details(menu_bar, i)\n",
        "        print(f\"\\nTop Level Menu {i}: {menu_text}\")\n",
        "        print(f\"Details: {menu_details}\")\n",
        "        \n",
        "        submenu = win32gui.GetSubMenu(menu_bar, i)\n",
        "        if submenu:\n",
        "            submenu_count = win32gui.GetMenuItemCount(submenu)\n",
        "            print(f\"Contains {submenu_count} items:\")\n",
        "            \n",
        "            for j in range(min(submenu_count, 10)):  # Show first 10 items\n",
        "                submenu_text = get_menu_string(submenu, j)\n",
        "                submenu_details = enumerate_menu_item_details(submenu, j)\n",
        "                print(f\"  \u2514\u2500 Item {j}: {submenu_text}\")\n",
        "                print(f\"     Details: {submenu_details}\")\n",
        "                \n",
        "                # Check for sub-submenus\n",
        "                sub_submenu = win32gui.GetSubMenu(submenu, j)\n",
        "                if sub_submenu:\n",
        "                    sub_count = win32gui.GetMenuItemCount(sub_submenu)\n",
        "                    print(f\"     Has submenu with {sub_count} items:\")\n",
        "                    for k in range(min(sub_count, 5)):  # Show first 5 sub-items\n",
        "                        sub_text = get_menu_string(sub_submenu, k)\n",
        "                        sub_details = enumerate_menu_item_details(sub_submenu, k)\n",
        "                        print(f\"       \u2514\u2500 Sub-item {k}: {sub_text}\")\n",
        "                        print(f\"          Details: {sub_details}\")\n",
        "            \n",
        "            if submenu_count > 10:\n",
        "                print(f\"  ... and {submenu_count - 10} more items\")\n",
        "\n",
        "# Main execution\n",
        "if 'hecras_pid' in globals() and hecras_pid is not None:\n",
        "    # Wait for windows to be ready\n",
        "    import time\n",
        "    time.sleep(1)\n",
        "    \n",
        "    # Find HEC-RAS windows\n",
        "    windows = get_windows_by_pid(hecras_pid)\n",
        "    hec_ras_hwnd, title = find_main_hecras_window(windows)\n",
        "    \n",
        "    if hec_ras_hwnd:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Performing detailed menu enumeration...\")\n",
        "        enumerate_full_menu_tree(hec_ras_hwnd)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Performing detailed window hierarchy enumeration...\")\n",
        "        print(f\"Main window: {title}\")\n",
        "        enumerate_window_details(hec_ras_hwnd)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Detailed enumeration complete.\")\n",
        "    else:\n",
        "        print(\"Could not find main HEC-RAS window\")\n",
        "else:\n",
        "    print(\"HEC-RAS process ID not found. Please run the cell that launches HEC-RAS first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def enumerate_menu_item_details(menu_handle, item_index):\n",
        "    \"\"\"Get detailed information about a menu item\"\"\"\n",
        "    # Since win32gui doesn't have MENUITEMINFO, we'll use a simpler approach\n",
        "    try:\n",
        "        # Get menu item ID\n",
        "        menu_id = win32gui.GetMenuItemID(menu_handle, item_index)\n",
        "        \n",
        "        # Get menu state\n",
        "        menu_state = win32gui.GetMenuState(menu_handle, item_index, win32con.MF_BYPOSITION)\n",
        "        \n",
        "        # Parse state flags\n",
        "        state_flags = []\n",
        "        if menu_state & win32con.MF_CHECKED:\n",
        "            state_flags.append(\"CHECKED\")\n",
        "        if menu_state & win32con.MF_DISABLED:\n",
        "            state_flags.append(\"DISABLED\")\n",
        "        if menu_state & win32con.MF_GRAYED:\n",
        "            state_flags.append(\"GRAYED\")\n",
        "        if menu_state & win32con.MF_SEPARATOR:\n",
        "            state_flags.append(\"SEPARATOR\")\n",
        "        \n",
        "        # Get menu text\n",
        "        text = get_menu_string(menu_handle, item_index)\n",
        "        \n",
        "        return {\n",
        "            \"id\": menu_id if menu_id != -1 else None,\n",
        "            \"state\": state_flags,\n",
        "            \"text\": text,\n",
        "            \"has_submenu\": win32gui.GetSubMenu(menu_handle, item_index) is not None\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"error\": str(e),\n",
        "            \"text\": get_menu_string(menu_handle, item_index)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- pywinauto Example ---\n",
            "This section shows how to inspect an application like RAS Mapper using pywinauto.\n",
            "You may need to install it first: !pip install pywinauto\n",
            "pywinauto is installed.\n",
            "\n",
            "Attempting to find a RAS Mapper window to demonstrate pywinauto...\n",
            "\n",
            "Could not find a running RAS Mapper window.\n",
            "To run this for real, open HEC-RAS and then RAS Mapper, then execute this cell again.\n",
            "Example code to list controls once connected:\n",
            "  from pywinauto import Application\n",
            "  app = Application(backend='uia').connect(title_re='.*RAS Mapper.*') # uia backend might also work\n",
            "  ras_mapper_window = app.window(title_re='.*RAS Mapper.*')\n",
            "  ras_mapper_window.print_control_identifiers()\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Part 2: Interacting with 64-bit Processes using pywinauto\n",
        "# ==============================================================================\n",
        "print(\"\\n--- pywinauto Example ---\")\n",
        "print(\"This section shows how to inspect an application like RAS Mapper using pywinauto.\")\n",
        "print(\"You may need to install it first: !pip install pywinauto\")\n",
        "try:\n",
        "    import pywinauto\n",
        "    from pywinauto import timings, findwindows\n",
        "    print(\"pywinauto is installed.\")\n",
        "    \n",
        "    print(\"\\nAttempting to find a RAS Mapper window to demonstrate pywinauto...\")\n",
        "    \n",
        "    try:\n",
        "        # Connect to RAS Mapper window (title might vary slightly by version)\n",
        "        # Using win32 backend as RAS Mapper is a mix of technologies\n",
        "        app = pywinauto.Application(backend=\"win32\").connect(title_re=\".*RAS Mapper.*\", timeout=5)\n",
        "        \n",
        "        ras_mapper_window = app.window(title_re=\".*RAS Mapper.*\")\n",
        "        \n",
        "        print(\"\\nSuccessfully connected to RAS Mapper!\")\n",
        "        print(\"Now, listing all its controls using pywinauto's print_control_identifiers():\")\n",
        "        \n",
        "        # This will print a tree of all interactable controls.\n",
        "        # It's a very useful function for exploration.\n",
        "        ras_mapper_window.print_control_identifiers()\n",
        "        \n",
        "    except (findwindows.ElementNotFoundError, timings.TimeoutError):\n",
        "        print(\"\\nCould not find a running RAS Mapper window.\")\n",
        "        print(\"To run this for real, open HEC-RAS and then RAS Mapper, then execute this cell again.\")\n",
        "        print(\"Example code to list controls once connected:\")\n",
        "        print(\"  from pywinauto import Application\")\n",
        "        print(\"  app = Application(backend='uia').connect(title_re='.*RAS Mapper.*') # uia backend might also work\")\n",
        "        print(\"  ras_mapper_window = app.window(title_re='.*RAS Mapper.*')\")\n",
        "        print(\"  ras_mapper_window.print_control_identifiers()\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\npywinauto is not installed. Please install it to run this example:\")\n",
        "    print(\"In a new cell, run: !pip install pywinauto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Performing detailed menu enumeration...\n",
            "\n",
            "=== Complete Menu Tree Analysis ===\n",
            "\n",
            "\n",
            "Top Level Menu 0: &File\n",
            "Details: {'id': 2428699, 'type_flags': [], 'state': [], 'text': '&File', 'has_submenu': True}\n",
            "Contains 27 items:\n",
            "  \u2514\u2500 Item 0: &New Project ...\n",
            "     Details: {'id': 2, 'type_flags': [], 'state': [], 'text': '&New Project ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Open Project ...\n",
            "     Details: {'id': 3, 'type_flags': [], 'state': [], 'text': '&Open Project ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &Save Project\n",
            "     Details: {'id': 4, 'type_flags': [], 'state': [], 'text': '&Save Project', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: Save Project &As ...\n",
            "     Details: {'id': 5, 'type_flags': [], 'state': [], 'text': 'Save Project &As ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: &Rename Project Title ...\n",
            "     Details: {'id': 6, 'type_flags': [], 'state': [], 'text': '&Rename Project Title ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: &Delete Project ...\n",
            "     Details: {'id': 7, 'type_flags': [], 'state': [], 'text': '&Delete Project ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: \n",
            "     Details: {'id': 8, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: &Project Summary ...\n",
            "     Details: {'id': 9, 'type_flags': [], 'state': [], 'text': '&Project Summary ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: Compare Model Data ...\n",
            "     Details: {'id': 10, 'type_flags': [], 'state': [], 'text': 'Compare Model Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 9: \n",
            "     Details: {'id': 11, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  ... and 17 more items\n",
            "\n",
            "Top Level Menu 1: &Edit\n",
            "Details: {'id': 12519335, 'type_flags': [], 'state': [], 'text': '&Edit', 'has_submenu': True}\n",
            "Contains 8 items:\n",
            "  \u2514\u2500 Item 0: &Geometric Data ...\n",
            "     Details: {'id': 37, 'type_flags': [], 'state': [], 'text': '&Geometric Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: \n",
            "     Details: {'id': 38, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &Steady Flow Data ...\n",
            "     Details: {'id': 39, 'type_flags': [], 'state': [], 'text': '&Steady Flow Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: &Quasi Unsteady Flow (Sediment) ...\n",
            "     Details: {'id': 40, 'type_flags': [], 'state': [], 'text': '&Quasi Unsteady Flow (Sediment) ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: &Unsteady Flow Data ...\n",
            "     Details: {'id': 41, 'type_flags': [], 'state': [], 'text': '&Unsteady Flow Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: \n",
            "     Details: {'id': 42, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: Se&diment Data ...\n",
            "     Details: {'id': 43, 'type_flags': [], 'state': [], 'text': 'Se&diment Data ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: &Water Quality Data ...\n",
            "     Details: {'id': 44, 'type_flags': [], 'state': [], 'text': '&Water Quality Data ...', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 2: &Run\n",
            "Details: {'id': 1380203, 'type_flags': [], 'state': [], 'text': '&Run', 'has_submenu': True}\n",
            "Contains 9 items:\n",
            "  \u2514\u2500 Item 0: &Steady Flow Analysis ...\n",
            "     Details: {'id': 46, 'type_flags': [], 'state': [], 'text': '&Steady Flow Analysis ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Unsteady Flow Analysis ...\n",
            "     Details: {'id': 47, 'type_flags': [], 'state': [], 'text': '&Unsteady Flow Analysis ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: Quasi-Unsteady Analysis (Sediment)...\n",
            "     Details: {'id': 48, 'type_flags': [], 'state': [], 'text': 'Quasi-Unsteady Analysis (Sediment)...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: Water Quality Analysis ...\n",
            "     Details: {'id': 49, 'type_flags': [], 'state': [], 'text': 'Water Quality Analysis ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: &Hydraulic Design Functions ...\n",
            "     Details: {'id': 50, 'type_flags': [], 'state': [], 'text': '&Hydraulic Design Functions ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: \n",
            "     Details: {'id': 51, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: Run Multiple Plans ...\n",
            "     Details: {'id': 52, 'type_flags': [], 'state': [], 'text': 'Run Multiple Plans ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: \n",
            "     Details: {'id': 54, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: Uncertainty Analysis\n",
            "     Details: {'id': 4327313, 'type_flags': [], 'state': ['DISABLED', 'GRAYED'], 'text': 'Uncertainty Analysis', 'has_submenu': True}\n",
            "     Has submenu with 3 items:\n",
            "       \u2514\u2500 Sub-item 0: Setup Parameters ...\n",
            "          Details: {'id': 56, 'type_flags': [], 'state': [], 'text': 'Setup Parameters ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 1: Run Monte Carlo Analysis ...\n",
            "          Details: {'id': 57, 'type_flags': [], 'state': [], 'text': 'Run Monte Carlo Analysis ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 2: Summarize Results\n",
            "          Details: {'id': 58, 'type_flags': [], 'state': [], 'text': 'Summarize Results', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 3: &View\n",
            "Details: {'id': 136188355, 'type_flags': [], 'state': [], 'text': '&View', 'has_submenu': True}\n",
            "Contains 23 items:\n",
            "  \u2514\u2500 Item 0: &Cross-Sections ...\n",
            "     Details: {'id': 61, 'type_flags': [], 'state': [], 'text': '&Cross-Sections ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Water Surface Profiles ...\n",
            "     Details: {'id': 62, 'type_flags': [], 'state': [], 'text': '&Water Surface Profiles ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &General Profile Plot ...\n",
            "     Details: {'id': 63, 'type_flags': [], 'state': [], 'text': '&General Profile Plot ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: &Rating Curves ...\n",
            "     Details: {'id': 64, 'type_flags': [], 'state': [], 'text': '&Rating Curves ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: 3D View ...\n",
            "     Details: {'id': 65, 'type_flags': [], 'state': [], 'text': '3D View ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: &X-Y-Z Perspective Plots (Classic) ...\n",
            "     Details: {'id': 66, 'type_flags': [], 'state': [], 'text': '&X-Y-Z Perspective Plots (Classic) ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: &Stage and Flow Hydrographs ...\n",
            "     Details: {'id': 67, 'type_flags': [], 'state': [], 'text': '&Stage and Flow Hydrographs ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: &Hydraulic Property Tables ...\n",
            "     Details: {'id': 68, 'type_flags': [], 'state': [], 'text': '&Hydraulic Property Tables ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: \n",
            "     Details: {'id': 69, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 9: &Detailed Output Tables ...\n",
            "     Details: {'id': 70, 'type_flags': [], 'state': [], 'text': '&Detailed Output Tables ...', 'has_submenu': False}\n",
            "  ... and 13 more items\n",
            "\n",
            "Top Level Menu 4: &Options\n",
            "Details: {'id': 1050507, 'type_flags': [], 'state': [], 'text': '&Options', 'has_submenu': True}\n",
            "Contains 5 items:\n",
            "  \u2514\u2500 Item 0: &Program Setup\n",
            "     Details: {'id': 348720343, 'type_flags': [], 'state': [], 'text': '&Program Setup', 'has_submenu': True}\n",
            "     Has submenu with 5 items:\n",
            "       \u2514\u2500 Sub-item 0: Default File &Viewer ...\n",
            "          Details: {'id': 91, 'type_flags': [], 'state': [], 'text': 'Default File &Viewer ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 1: Default Project Folder ...\n",
            "          Details: {'id': 92, 'type_flags': [], 'state': [], 'text': 'Default Project Folder ...', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 2: &Open last project on startup\n",
            "          Details: {'id': 93, 'type_flags': [], 'state': [], 'text': '&Open last project on startup', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 3: Automatically Backup Data\n",
            "          Details: {'id': 94, 'type_flags': [], 'state': ['CHECKED'], 'text': 'Automatically Backup Data', 'has_submenu': False}\n",
            "       \u2514\u2500 Sub-item 4: Set Time for Automatic Backup ...\n",
            "          Details: {'id': 95, 'type_flags': [], 'state': [], 'text': 'Set Time for Automatic Backup ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: &Default Parameters\n",
            "     Details: {'id': 139527055, 'type_flags': [], 'state': [], 'text': '&Default Parameters', 'has_submenu': True}\n",
            "     Has submenu with 1 items:\n",
            "       \u2514\u2500 Sub-item 0: &Expansion and Contraction Coef ...\n",
            "          Details: {'id': 97, 'type_flags': [], 'state': [], 'text': '&Expansion and Contraction Coef ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: &Unit system (US Customary/SI) ...\n",
            "     Details: {'id': 98, 'type_flags': [], 'state': [], 'text': '&Unit system (US Customary/SI) ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: &Convert Project Units ...\n",
            "     Details: {'id': 99, 'type_flags': [], 'state': [], 'text': '&Convert Project Units ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: Convert &Horizontal Coordinate Systems ...\n",
            "     Details: {'id': 100, 'type_flags': [], 'state': [], 'text': 'Convert &Horizontal Coordinate Systems ...', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 5: &GIS Tools\n",
            "Details: {'id': 3737661, 'type_flags': [], 'state': [], 'text': '&GIS Tools', 'has_submenu': True}\n",
            "Contains 1 items:\n",
            "  \u2514\u2500 Item 0: RAS Mapper ...\n",
            "     Details: {'id': 102, 'type_flags': [], 'state': [], 'text': 'RAS Mapper ...', 'has_submenu': False}\n",
            "\n",
            "Top Level Menu 6: &Help\n",
            "Details: {'id': 2297713, 'type_flags': [], 'state': [], 'text': '&Help', 'has_submenu': True}\n",
            "Contains 21 items:\n",
            "  \u2514\u2500 Item 0: HEC-RAS Online Documentation ...\tF1\n",
            "     Details: {'id': 104, 'type_flags': [], 'state': [], 'text': 'HEC-RAS Online Documentation ...\\tF1', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 1: Release Notes ...\n",
            "     Details: {'id': 105, 'type_flags': [], 'state': [], 'text': 'Release Notes ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 2: Known Issues ...\n",
            "     Details: {'id': 106, 'type_flags': [], 'state': [], 'text': 'Known Issues ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 3: Community Support on Discourse ...\n",
            "     Details: {'id': 107, 'type_flags': [], 'state': [], 'text': 'Community Support on Discourse ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 4: \n",
            "     Details: {'id': 108, 'type_flags': ['SEPARATOR'], 'state': ['DISABLED', 'GRAYED'], 'text': '', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 5: Users Manual ...\n",
            "     Details: {'id': 109, 'type_flags': [], 'state': [], 'text': 'Users Manual ...', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 6: 2D Modeling User's Manual \u2026\n",
            "     Details: {'id': 110, 'type_flags': [], 'state': [], 'text': \"2D Modeling User's Manual \u2026\", 'has_submenu': False}\n",
            "  \u2514\u2500 Item 7: RAS Mapper User's Manual \u2026\n",
            "     Details: {'id': 111, 'type_flags': [], 'state': [], 'text': \"RAS Mapper User's Manual \u2026\", 'has_submenu': False}\n",
            "  \u2514\u2500 Item 8: Hydraulic Reference \u2026\n",
            "     Details: {'id': 112, 'type_flags': [], 'state': [], 'text': 'Hydraulic Reference \u2026', 'has_submenu': False}\n",
            "  \u2514\u2500 Item 9: Applications Guide \u2026\n",
            "     Details: {'id': 113, 'type_flags': [], 'state': [], 'text': 'Applications Guide \u2026', 'has_submenu': False}\n",
            "  ... and 11 more items\n",
            "\n",
            "================================================================================\n",
            "Performing detailed window hierarchy enumeration...\n",
            "Main window: HEC-RAS 6.6\n",
            "Window Handle: 1182634\n",
            "Class: ThunderRT6FormDC\n",
            "Text: HEC-RAS 6.6\n",
            "Position: (307, 50, 1157, 266)\n",
            "Style: 0x16CA0000\n",
            "Extended Style: 0x00040100\n",
            "Children: 36 found\n",
            "---\n",
            "  Window Handle: 2427996\n",
            "  Class: ThunderRT6Timer\n",
            "  Text: \n",
            "  Position: (438, 340, 438, 340)\n",
            "  Style: 0x44010000\n",
            "  Extended Style: 0x00000004\n",
            "---\n",
            "  Window Handle: 1050472\n",
            "  Class: ThunderRT6TextBox\n",
            "  Text: \n",
            "  Position: (1066, 104, 1091, 121)\n",
            "  Style: 0x440100C0\n",
            "  Extended Style: 0x00000204\n",
            "---\n",
            "  Window Handle: 919462\n",
            "  Class: ThunderRT6CheckBox\n",
            "  Text: \n",
            "  Position: (862, 100, 888, 126)\n",
            "  Style: 0x5401000B\n",
            "  Extended Style: 0x00000004\n",
            "---\n",
            "  Window Handle: 1050616\n",
            "  Class: ThunderRT6TextBox\n",
            "  Text: \n",
            "  Position: (1034, 104, 1063, 121)\n",
            "  Style: 0x440100C0\n",
            "  Extended Style: 0x00000204\n",
            "---\n",
            "  Window Handle: 919270\n",
            "  Class: ThunderRT6Timer\n",
            "  Text: \n",
            "  Position: (410, 340, 410, 340)\n",
            "  Style: 0x44010000\n",
            "  Extended Style: 0x00000004\n",
            "... and 31 more children\n",
            "\n",
            "================================================================================\n",
            "Detailed enumeration complete.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Part 1b: Deeper Menu and Window Object Enumeration using ctypes\n",
        "# ==============================================================================\n",
        "\n",
        "import win32gui\n",
        "import win32con\n",
        "import win32api\n",
        "import win32process\n",
        "import ctypes\n",
        "from ctypes import wintypes\n",
        "\n",
        "# Define MENUITEMINFO structure using ctypes\n",
        "class MENUITEMINFO(ctypes.Structure):\n",
        "    _fields_ = [\n",
        "        (\"cbSize\", wintypes.UINT),\n",
        "        (\"fMask\", wintypes.UINT),\n",
        "        (\"fType\", wintypes.UINT),\n",
        "        (\"fState\", wintypes.UINT),\n",
        "        (\"wID\", wintypes.UINT),\n",
        "        (\"hSubMenu\", wintypes.HMENU),\n",
        "        (\"hbmpChecked\", wintypes.HBITMAP),\n",
        "        (\"hbmpUnchecked\", wintypes.HBITMAP),\n",
        "        (\"dwItemData\", ctypes.POINTER(ctypes.c_ulong)),\n",
        "        (\"dwTypeData\", wintypes.LPWSTR),\n",
        "        (\"cch\", wintypes.UINT),\n",
        "        (\"hbmpItem\", wintypes.HBITMAP)\n",
        "    ]\n",
        "\n",
        "def get_menu_string(menu_handle, pos):\n",
        "    \"\"\"Get menu item string at position\"\"\"\n",
        "    buf_size = 256\n",
        "    buf = ctypes.create_unicode_buffer(buf_size)\n",
        "    user32 = ctypes.windll.user32\n",
        "    result = user32.GetMenuStringW(menu_handle, pos, buf, buf_size, MF_BYPOSITION)\n",
        "    if result:\n",
        "        return buf.value\n",
        "    return \"\"\n",
        "\n",
        "def enumerate_menu_item_details(menu_handle, item_index):\n",
        "    \"\"\"Get detailed information about a menu item using ctypes\"\"\"\n",
        "    # Create and initialize MENUITEMINFO structure\n",
        "    mii = MENUITEMINFO()\n",
        "    mii.cbSize = ctypes.sizeof(MENUITEMINFO)\n",
        "    mii.fMask = win32con.MIIM_STATE | win32con.MIIM_ID | win32con.MIIM_TYPE | win32con.MIIM_SUBMENU\n",
        "    \n",
        "    # Call GetMenuItemInfo using ctypes\n",
        "    user32 = ctypes.windll.user32\n",
        "    result = user32.GetMenuItemInfoW(\n",
        "        menu_handle, \n",
        "        item_index, \n",
        "        True,  # fByPosition\n",
        "        ctypes.byref(mii)\n",
        "    )\n",
        "    \n",
        "    if result:\n",
        "        # Parse state flags\n",
        "        state_flags = []\n",
        "        if mii.fState & win32con.MFS_CHECKED:\n",
        "            state_flags.append(\"CHECKED\")\n",
        "        if mii.fState & win32con.MFS_DISABLED:\n",
        "            state_flags.append(\"DISABLED\")\n",
        "        if mii.fState & win32con.MFS_GRAYED:\n",
        "            state_flags.append(\"GRAYED\")\n",
        "        if mii.fState & win32con.MFS_HILITE:\n",
        "            state_flags.append(\"HIGHLIGHTED\")\n",
        "        if mii.fState & win32con.MFS_DEFAULT:\n",
        "            state_flags.append(\"DEFAULT\")\n",
        "            \n",
        "        # Parse type flags\n",
        "        type_flags = []\n",
        "        if mii.fType & win32con.MFT_STRING:\n",
        "            type_flags.append(\"STRING\")\n",
        "        if mii.fType & win32con.MFT_SEPARATOR:\n",
        "            type_flags.append(\"SEPARATOR\")\n",
        "        if mii.fType & win32con.MFT_BITMAP:\n",
        "            type_flags.append(\"BITMAP\")\n",
        "        if mii.fType & win32con.MFT_OWNERDRAW:\n",
        "            type_flags.append(\"OWNERDRAW\")\n",
        "        \n",
        "        return {\n",
        "            \"id\": mii.wID,\n",
        "            \"type_flags\": type_flags,\n",
        "            \"state\": state_flags,\n",
        "            \"text\": get_menu_string(menu_handle, item_index),\n",
        "            \"has_submenu\": bool(mii.hSubMenu)\n",
        "        }\n",
        "    else:\n",
        "        # Fallback to simpler approach if GetMenuItemInfo fails\n",
        "        try:\n",
        "            menu_id = win32gui.GetMenuItemID(menu_handle, item_index)\n",
        "            menu_state = win32gui.GetMenuState(menu_handle, item_index, win32con.MF_BYPOSITION)\n",
        "            \n",
        "            state_flags = []\n",
        "            if menu_state & win32con.MF_CHECKED:\n",
        "                state_flags.append(\"CHECKED\")\n",
        "            if menu_state & win32con.MF_DISABLED:\n",
        "                state_flags.append(\"DISABLED\")\n",
        "            if menu_state & win32con.MF_GRAYED:\n",
        "                state_flags.append(\"GRAYED\")\n",
        "            if menu_state & win32con.MF_SEPARATOR:\n",
        "                state_flags.append(\"SEPARATOR\")\n",
        "            \n",
        "            return {\n",
        "                \"id\": menu_id if menu_id != -1 else None,\n",
        "                \"state\": state_flags,\n",
        "                \"text\": get_menu_string(menu_handle, item_index),\n",
        "                \"has_submenu\": win32gui.GetSubMenu(menu_handle, item_index) is not None,\n",
        "                \"fallback\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"text\": get_menu_string(menu_handle, item_index)\n",
        "            }\n",
        "\n",
        "def enumerate_window_details(hwnd, indent=0):\n",
        "    \"\"\"Recursively enumerate window details including styles and extended styles\"\"\"\n",
        "    if not hwnd:\n",
        "        return\n",
        "        \n",
        "    # Get basic window info\n",
        "    class_name = win32gui.GetClassName(hwnd)\n",
        "    window_text = win32gui.GetWindowText(hwnd)\n",
        "    \n",
        "    # Get window styles\n",
        "    style = win32gui.GetWindowLong(hwnd, win32con.GWL_STYLE)\n",
        "    ex_style = win32gui.GetWindowLong(hwnd, win32con.GWL_EXSTYLE)\n",
        "    \n",
        "    # Get window metrics\n",
        "    rect = win32gui.GetWindowRect(hwnd)\n",
        "    \n",
        "    # Print window details\n",
        "    indent_str = \"  \" * indent\n",
        "    print(f\"{indent_str}Window Handle: {hwnd}\")\n",
        "    print(f\"{indent_str}Class: {class_name}\")\n",
        "    print(f\"{indent_str}Text: {window_text}\")\n",
        "    print(f\"{indent_str}Position: {rect}\")\n",
        "    print(f\"{indent_str}Style: 0x{style:08X}\")\n",
        "    print(f\"{indent_str}Extended Style: 0x{ex_style:08X}\")\n",
        "    \n",
        "    # Enumerate child windows recursively (limit depth to avoid too much output)\n",
        "    if indent < 3:  # Limit recursion depth\n",
        "        try:\n",
        "            child_windows = []\n",
        "            win32gui.EnumChildWindows(\n",
        "                hwnd,\n",
        "                lambda child_hwnd, windows: windows.append(child_hwnd) or True,\n",
        "                child_windows\n",
        "            )\n",
        "            \n",
        "            if child_windows:\n",
        "                print(f\"{indent_str}Children: {len(child_windows)} found\")\n",
        "                for child_hwnd in child_windows[:5]:  # Show first 5 children\n",
        "                    print(f\"{indent_str}---\")\n",
        "                    enumerate_window_details(child_hwnd, indent + 1)\n",
        "                if len(child_windows) > 5:\n",
        "                    print(f\"{indent_str}... and {len(child_windows) - 5} more children\")\n",
        "        except Exception as e:\n",
        "            print(f\"{indent_str}Error enumerating children: {e}\")\n",
        "\n",
        "def enumerate_full_menu_tree(hwnd):\n",
        "    \"\"\"Enumerate complete menu tree with all available details\"\"\"\n",
        "    menu_bar = win32gui.GetMenu(hwnd)\n",
        "    if not menu_bar:\n",
        "        print(\"No menu bar found\")\n",
        "        return\n",
        "        \n",
        "    print(\"\\n=== Complete Menu Tree Analysis ===\\n\")\n",
        "    \n",
        "    menu_count = win32gui.GetMenuItemCount(menu_bar)\n",
        "    for i in range(menu_count):\n",
        "        menu_text = get_menu_string(menu_bar, i)\n",
        "        menu_details = enumerate_menu_item_details(menu_bar, i)\n",
        "        print(f\"\\nTop Level Menu {i}: {menu_text}\")\n",
        "        print(f\"Details: {menu_details}\")\n",
        "        \n",
        "        submenu = win32gui.GetSubMenu(menu_bar, i)\n",
        "        if submenu:\n",
        "            submenu_count = win32gui.GetMenuItemCount(submenu)\n",
        "            print(f\"Contains {submenu_count} items:\")\n",
        "            \n",
        "            for j in range(min(submenu_count, 10)):  # Show first 10 items\n",
        "                submenu_text = get_menu_string(submenu, j)\n",
        "                submenu_details = enumerate_menu_item_details(submenu, j)\n",
        "                print(f\"  \u2514\u2500 Item {j}: {submenu_text}\")\n",
        "                print(f\"     Details: {submenu_details}\")\n",
        "                \n",
        "                # Check for sub-submenus\n",
        "                sub_submenu = win32gui.GetSubMenu(submenu, j)\n",
        "                if sub_submenu:\n",
        "                    sub_count = win32gui.GetMenuItemCount(sub_submenu)\n",
        "                    print(f\"     Has submenu with {sub_count} items:\")\n",
        "                    for k in range(min(sub_count, 5)):  # Show first 5 sub-items\n",
        "                        sub_text = get_menu_string(sub_submenu, k)\n",
        "                        sub_details = enumerate_menu_item_details(sub_submenu, k)\n",
        "                        print(f\"       \u2514\u2500 Sub-item {k}: {sub_text}\")\n",
        "                        print(f\"          Details: {sub_details}\")\n",
        "            \n",
        "            if submenu_count > 10:\n",
        "                print(f\"  ... and {submenu_count - 10} more items\")\n",
        "\n",
        "# Main execution\n",
        "if 'hecras_pid' in globals() and hecras_pid is not None:\n",
        "    # Wait for windows to be ready\n",
        "    import time\n",
        "    time.sleep(1)\n",
        "    \n",
        "    # Find HEC-RAS windows\n",
        "    windows = get_windows_by_pid(hecras_pid)\n",
        "    hec_ras_hwnd, title = find_main_hecras_window(windows)\n",
        "    \n",
        "    if hec_ras_hwnd:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Performing detailed menu enumeration...\")\n",
        "        enumerate_full_menu_tree(hec_ras_hwnd)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Performing detailed window hierarchy enumeration...\")\n",
        "        print(f\"Main window: {title}\")\n",
        "        enumerate_window_details(hec_ras_hwnd)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Detailed enumeration complete.\")\n",
        "    else:\n",
        "        print(\"Could not find main HEC-RAS window\")\n",
        "else:\n",
        "    print(\"HEC-RAS process ID not found. Please run the cell that launches HEC-RAS first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This provides the basic building blocks for GUI-driven automations for HEC-RAS without the HECRASController"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_piptest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: C:\GH\ras-commander\examples\17_extracting_profiles_with_hecrascontroller and RasControl.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:26.598182Z",
          "iopub.status.busy": "2025-11-17T19:05:26.597941Z",
          "iopub.status.idle": "2025-11-17T19:05:28.305562Z",
          "shell.execute_reply": "2025-11-17T19:05:28.305031Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Working with Legacy HEC-RAS Using RasControl\n",
        "\n",
        "This notebook demonstrates **RasControl**, which provides a ras-commander style API for legacy HEC-RAS versions (3.x-4.x) using the HECRASController COM interface.\n",
        "\n",
        "## What is RasControl?\n",
        "\n",
        "**RasControl** wraps the HECRASController COM API with ras-commander conventions:\n",
        "\n",
        "- \u2705 **Use plan numbers** - `RasControl.run_plan(\"02\")` not file paths\n",
        "- \u2705 **Integrated with ras object** - Works with `init_ras_project()`\n",
        "- \u2705 **Steady AND unsteady** - Extract profiles and time series\n",
        "- \u2705 **Auto-sets current plan** - Just pass the plan number!\n",
        "- \u2705 **No COM complexity** - Clean public API\n",
        "\n",
        "## When to Use RasControl\n",
        "\n",
        "| Use RasControl | Use HDF Methods |\n",
        "|----------------|----------------|\n",
        "| HEC-RAS 3.1, 4.1 | HEC-RAS 6.0+ |\n",
        "| No HDF support | Modern versions |\n",
        "| Legacy models | 2D mesh data |\n",
        "| Version migration | Better performance |\n",
        "\n",
        "## Supported Versions\n",
        "\n",
        "3.0, 3.1, 4.0, 4.1, 5.0-5.0.7, 6.0-6.7 Beta\n",
        "\n",
        "Accepts: `\"4.1\"`, `\"41\"`, `\"5.0.6\"`, `\"506\"`, `\"6.6\"`, `\"66\"`, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.307996Z",
          "iopub.status.busy": "2025-11-17T19:05:28.307624Z",
          "iopub.status.idle": "2025-11-17T19:05:28.310145Z",
          "shell.execute_reply": "2025-11-17T19:05:28.309754Z"
        }
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade ras-commander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.312286Z",
          "iopub.status.busy": "2025-11-17T19:05:28.311987Z",
          "iopub.status.idle": "2025-11-17T19:05:28.315882Z",
          "shell.execute_reply": "2025-11-17T19:05:28.315329Z"
        }
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.318508Z",
          "iopub.status.busy": "2025-11-17T19:05:28.318153Z",
          "iopub.status.idle": "2025-11-17T19:05:28.321877Z",
          "shell.execute_reply": "2025-11-17T19:05:28.321380Z"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PLOTTING CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Set better default plotting parameters\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 9\n",
        "plt.rcParams['xtick.labelsize'] = 9\n",
        "plt.rcParams['ytick.labelsize'] = 9\n",
        "\n",
        "import numpy as np  # Add if not already imported\n",
        "\n",
        "print(\"\u2713 Plotting configuration loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.323634Z",
          "iopub.status.busy": "2025-11-17T19:05:28.323423Z",
          "iopub.status.idle": "2025-11-17T19:05:28.327130Z",
          "shell.execute_reply": "2025-11-17T19:05:28.326476Z"
        }
      },
      "outputs": [],
      "source": [
        "# Enable this cell for local development version of ras-commander\n",
        "import os\n",
        "import sys      \n",
        "from pathlib import Path\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.329457Z",
          "iopub.status.busy": "2025-11-17T19:05:28.329226Z",
          "iopub.status.idle": "2025-11-17T19:05:28.333188Z",
          "shell.execute_reply": "2025-11-17T19:05:28.332632Z"
        }
      },
      "outputs": [],
      "source": [
        "# 2. Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.336035Z",
          "iopub.status.busy": "2025-11-17T19:05:28.335610Z",
          "iopub.status.idle": "2025-11-17T19:05:28.345204Z",
          "shell.execute_reply": "2025-11-17T19:05:28.344712Z"
        }
      },
      "outputs": [],
      "source": [
        "# Helper Plotting Functions\n",
        "\n",
        "def plot_steady_profiles_by_reach(df, title_prefix=\"Steady Flow Profiles\"):\n",
        "    \"\"\"\n",
        "    Plot steady flow profiles separated by River/Reach.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Steady results from RasControl.get_steady_results()\n",
        "    title_prefix : str\n",
        "        Prefix for plot titles\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    # Group by River/Reach\n",
        "    for (river, reach), group_df in df.groupby(['river', 'reach']):\n",
        "        fig, ax = plt.subplots(figsize=(14, 6))\n",
        "        \n",
        "        # Sort by station (descending - upstream to downstream)\n",
        "        group_df_sorted = group_df.sort_values('node_id', ascending=False)\n",
        "        \n",
        "        # Plot each profile\n",
        "        profiles = group_df['profile'].unique()\n",
        "        for profile in profiles:\n",
        "            prof_data = group_df_sorted[group_df_sorted['profile'] == profile]\n",
        "            ax.plot(prof_data['node_id'], prof_data['wsel'], \n",
        "                    marker='o', label=f'{profile}', linewidth=2, markersize=4)\n",
        "        \n",
        "        # Add channel invert\n",
        "        invert = group_df_sorted.drop_duplicates('node_id')[['node_id', 'min_ch_el']]\n",
        "        ax.plot(invert['node_id'], invert['min_ch_el'], \n",
        "                'k--', label='Channel Invert', linewidth=2, alpha=0.7)\n",
        "        \n",
        "        ax.set_xlabel('River Station', fontsize=12)\n",
        "        ax.set_ylabel('Elevation (ft)', fontsize=12)\n",
        "        ax.set_title(f'{title_prefix}\\n{river} - {reach}', \n",
        "                     fontsize=14, fontweight='bold')\n",
        "        ax.legend(loc='best', fontsize=9, ncol=2)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.invert_xaxis()  # Upstream on left\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_unsteady_timeseries_multi_xs(df_timeseries, df_maxws, selected_xs=None, n_xs=5):\n",
        "    \"\"\"\n",
        "    Plot unsteady time series at multiple cross sections with Max WS annotations.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_timeseries : DataFrame\n",
        "        Unsteady results WITHOUT 'Max WS' rows, with 'datetime' column\n",
        "    df_maxws : DataFrame\n",
        "        Unsteady results for ONLY 'Max WS' rows\n",
        "    selected_xs : list, optional\n",
        "        List of specific cross sections to plot. If None, selects evenly spaced XS\n",
        "    n_xs : int\n",
        "        Number of cross sections to plot (used if selected_xs is None)\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    \n",
        "    # Select cross sections if not provided\n",
        "    if selected_xs is None:\n",
        "        all_xs = sorted(df_timeseries['node_id'].unique(), reverse=True)\n",
        "        step = max(1, len(all_xs) // n_xs)\n",
        "        selected_xs = all_xs[::step][:n_xs]\n",
        "    \n",
        "    # Create subplots\n",
        "    n_xs_plot = len(selected_xs)\n",
        "    fig, axes = plt.subplots(n_xs_plot, 1, figsize=(14, 4*n_xs_plot))\n",
        "    if n_xs_plot == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, xs in enumerate(selected_xs):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Get data for this XS\n",
        "        xs_data = df_timeseries[df_timeseries['node_id'] == xs].sort_values('datetime')\n",
        "        maxws_data = df_maxws[df_maxws['node_id'] == xs]\n",
        "        \n",
        "        # Plot time series\n",
        "        ax.plot(xs_data['datetime'], xs_data['wsel'], \n",
        "                'b-o', linewidth=2, markersize=4, label='WSE at Output Timesteps')\n",
        "        \n",
        "        # Get values for annotation\n",
        "        max_ws_value = maxws_data['wsel'].iloc[0] if len(maxws_data) > 0 else None\n",
        "        max_output_value = xs_data['wsel'].max()\n",
        "        \n",
        "        # Add horizontal reference for Max WS\n",
        "        if max_ws_value:\n",
        "            ax.axhline(max_ws_value, color='r', linestyle='--', \n",
        "                       linewidth=2, alpha=0.7, label='Max WS (computational)')\n",
        "        \n",
        "        # Add annotations\n",
        "        annotation_text = f\"Max WS (computational): {max_ws_value:.2f} ft\\n\"\n",
        "        annotation_text += f\"Max (output interval): {max_output_value:.2f} ft\"\n",
        "        \n",
        "        ax.text(0.02, 0.98, annotation_text, \n",
        "                transform=ax.transAxes, fontsize=10,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "        \n",
        "        ax.set_ylabel('WSE (ft)', fontsize=11)\n",
        "        ax.set_title(f'Station {xs}', fontsize=12, fontweight='bold')\n",
        "        ax.legend(loc='upper right', fontsize=9)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Format x-axis for dates\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))\n",
        "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    axes[-1].set_xlabel('Date/Time', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\u2713 Helper plotting functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.347374Z",
          "iopub.status.busy": "2025-11-17T19:05:28.347110Z",
          "iopub.status.idle": "2025-11-17T19:05:28.350335Z",
          "shell.execute_reply": "2025-11-17T19:05:28.349925Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import ras-commander\n",
        "sys.path.append(str(Path(os.getcwd()).parent))\n",
        "from ras_commander import RasExamples, init_ras_project, RasControl, ras, RasCmdr\n",
        "\n",
        "print(f\"RasControl supports: {list(RasControl.SUPPORTED_VERSIONS.keys())[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract and Initialize Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.352379Z",
          "iopub.status.busy": "2025-11-17T19:05:28.352111Z",
          "iopub.status.idle": "2025-11-17T19:05:28.490274Z",
          "shell.execute_reply": "2025-11-17T19:05:28.489921Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract Bald Eagle Creek (has steady Plan 02 and unsteady Plan 01)\n",
        "project_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "\n",
        "# Initialize with version (required for RasControl)\n",
        "init_ras_project(project_path, \"6.6\")  # or \"66\", \"6.5\", \"4.1\", \"41\", etc.\n",
        "\n",
        "print(f\"Project: {ras.project_name}\")\n",
        "print(f\"Version: {ras.ras_version}\")\n",
        "print(f\"\\nPlans:\")\n",
        "print(ras.plan_df[['plan_number', 'Plan Title', 'flow_type',\"full_path\"]])\n",
        "\n",
        "# Use the full_path from plan_df to do the following for each plain text plan file: \n",
        "# Find the following values and change them\n",
        "# Change \"Output Interval=10MIN\" \n",
        "# Change \"Mapping Interval=10MIN\" \n",
        "\n",
        "\n",
        "import re, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# ...existing code...\n",
        "def update_plan_intervals_from_plan_df(plan_df, plan_numbers=None,\n",
        "                                       output_interval=None, mapping_interval=None,\n",
        "                                       make_backup=True, encoding='utf-8'):\n",
        "    \"\"\"\n",
        "    Replace Output Interval and Mapping Interval in plain-text plan files referenced by plan_df['full_path'].\n",
        "    - plan_numbers: list of plan_number strings to limit changes (e.g. ['01','02']) or None for all.\n",
        "    - output_interval / mapping_interval: strings like '6MIN', '10MIN' (include unit).\n",
        "    \"\"\"\n",
        "    for _, row in plan_df.iterrows():\n",
        "        plan_no = str(row.get('plan_number', '')).zfill(2)\n",
        "        if plan_numbers and plan_no not in [str(p).zfill(2) for p in plan_numbers]:\n",
        "            continue\n",
        "\n",
        "        fp = Path(row.get('full_path', ''))\n",
        "        if not fp.exists():\n",
        "            print(f\"Missing file: {fp}\")\n",
        "            continue\n",
        "\n",
        "        text = fp.read_text(encoding=encoding, errors='ignore')\n",
        "        new_text = text\n",
        "\n",
        "        if output_interval:\n",
        "            # Use a callable replacement to avoid accidental numeric backreference parsing (e.g. \"\\16MIN\")\n",
        "            new_text = re.sub(\n",
        "                r'(?i)(Output\\s*Interval\\s*=\\s*)(\\S+)',\n",
        "                lambda m, out=output_interval: m.group(1) + out,\n",
        "                new_text\n",
        "            )\n",
        "\n",
        "        if mapping_interval:\n",
        "            new_text = re.sub(\n",
        "                r'(?i)(Mapping\\s*Interval\\s*=\\s*)(\\S+)',\n",
        "                lambda m, mp=mapping_interval: m.group(1) + mp,\n",
        "                new_text\n",
        "            )\n",
        "\n",
        "        if new_text != text:\n",
        "            if make_backup:\n",
        "                bak = fp.with_suffix(fp.suffix + '.bak')\n",
        "                shutil.copy(fp, bak)\n",
        "            fp.write_text(new_text, encoding=encoding)\n",
        "            print(f\"Updated {fp} (Plan {plan_no})\")\n",
        "        else:\n",
        "            print(f\"No change needed: {fp} (Plan {plan_no})\")\n",
        "# ...existing code...\n",
        "\n",
        "# Example usage: update Plan 01 (unsteady) to 6MIN intervals\n",
        "update_plan_intervals_from_plan_df(ras.plan_df, plan_numbers=['01'],\n",
        "                                   output_interval='10MIN', mapping_interval='10MIN',\n",
        "                                   make_backup=True)\n",
        "update_plan_intervals_from_plan_df(ras.plan_df, plan_numbers=['02'],\n",
        "                                   output_interval='10MIN', mapping_interval='10MIN',\n",
        "                                   make_backup=True)\n",
        "init_ras_project(project_path, \"6.6\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.492030Z",
          "iopub.status.busy": "2025-11-17T19:05:28.491822Z",
          "iopub.status.idle": "2025-11-17T19:05:28.505453Z",
          "shell.execute_reply": "2025-11-17T19:05:28.505039Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Steady State (Plan 02)\n",
        "\n",
        "Extract steady profiles. **Note:** `run_plan()` automatically sets Plan 02 as current!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:28.507423Z",
          "iopub.status.busy": "2025-11-17T19:05:28.507171Z",
          "iopub.status.idle": "2025-11-17T19:05:51.438318Z",
          "shell.execute_reply": "2025-11-17T19:05:51.437852Z"
        }
      },
      "outputs": [],
      "source": [
        "# Run Plan 02 (auto-sets as current, then runs)\n",
        "print(\"Running Plan 02 (Steady)...\")\n",
        "\n",
        "# NEW BEHAVIOR: run_plan() now checks if plan is current before running\n",
        "# - If plan is already current (results are up-to-date), it skips the computation\n",
        "# - To force recomputation regardless: RasControl.run_plan(\"02\", force_recompute=True)\n",
        "success, msgs = RasControl.run_plan(\"02\", force_recompute=True)\n",
        "print(f\"Success: {success}, Messages: {len(msgs)}\")\n",
        "print(msgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting Computation Messages (Steady Flow)\n",
        "\n",
        "After running the plan, we can extract detailed computation messages using `RasControl.get_comp_msgs()`. This method:\n",
        "- Reads from `.comp_msgs.txt` or `.computeMsgs.txt` files (version-dependent)\n",
        "- Falls back to HDF extraction if .txt files not available\n",
        "- Returns detailed information about the computation process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:51.440440Z",
          "iopub.status.busy": "2025-11-17T19:05:51.440210Z",
          "iopub.status.idle": "2025-11-17T19:05:51.449904Z",
          "shell.execute_reply": "2025-11-17T19:05:51.448693Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract computation messages for steady flow Plan 02\n",
        "print(\"=\"*80)\n",
        "print(\"COMPUTATION MESSAGES - Plan 02 (Steady Flow)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "msgs_steady = RasControl.get_comp_msgs(\"02\")\n",
        "\n",
        "if msgs_steady:\n",
        "    print(f\"\\nExtracted {len(msgs_steady)} characters of computation messages\\n\")\n",
        "    \n",
        "    # Display first 800 characters\n",
        "    print(\"Computation messages (first 800 characters):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(msgs_steady[:800])\n",
        "    \n",
        "    if len(msgs_steady) > 800:\n",
        "        print(\"\\n... (truncated) ...\")\n",
        "else:\n",
        "    print(\"No computation messages available for Plan 02\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:51.452979Z",
          "iopub.status.busy": "2025-11-17T19:05:51.452668Z",
          "iopub.status.idle": "2025-11-17T19:05:54.930528Z",
          "shell.execute_reply": "2025-11-17T19:05:54.929999Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract steady results (auto-sets Plan 02 as current)\n",
        "df_steady = RasControl.get_steady_results(\"02\")\n",
        "\n",
        "print(f\"Rows: {len(df_steady)}\")\n",
        "print(f\"Profiles: {df_steady['profile'].nunique()}\")\n",
        "print(f\"XS: {df_steady['node_id'].nunique()}\")\n",
        "df_steady.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:54.932440Z",
          "iopub.status.busy": "2025-11-17T19:05:54.932282Z",
          "iopub.status.idle": "2025-11-17T19:05:55.133019Z",
          "shell.execute_reply": "2025-11-17T19:05:55.132476Z"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEADY FLOW: Longitudinal Profiles by River/Reach\n",
        "# ============================================================================\n",
        "\n",
        "# Convert node_id to float for proper sorting\n",
        "df_steady['node_id'] = df_steady['node_id'].astype(float)\n",
        "\n",
        "# Group by River/Reach and create separate plots\n",
        "for (river, reach), group_df in df_steady.groupby(['river', 'reach']):\n",
        "    \n",
        "    # Sort by station (descending - upstream to downstream per HEC-RAS convention)\n",
        "    group_df_sorted = group_df.sort_values('node_id', ascending=False)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, 7))\n",
        "    \n",
        "    # Get unique profiles and plot each one\n",
        "    profiles = sorted(group_df['profile'].unique())\n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.9, len(profiles)))\n",
        "    \n",
        "    for idx, profile in enumerate(profiles):\n",
        "        prof_data = group_df_sorted[group_df_sorted['profile'] == profile]\n",
        "        ax.plot(prof_data['node_id'], prof_data['wsel'], \n",
        "                marker='o', markersize=3, linewidth=2, \n",
        "                color=colors[idx], label=f'WSE: {profile}', alpha=0.8)\n",
        "    \n",
        "    # Add channel invert (plot once, not for each profile)\n",
        "    invert = group_df_sorted.drop_duplicates('node_id')[['node_id', 'min_ch_el']].sort_values('node_id', ascending=False)\n",
        "    ax.plot(invert['node_id'], invert['min_ch_el'], \n",
        "            'k--', linewidth=2.5, alpha=0.7, label='Channel Invert')\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_xlabel('River Station', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel('Elevation (ft)', fontsize=13, fontweight='bold')\n",
        "    ax.set_title(f'{river} - {reach}\\nSteady Flow Water Surface Profiles', \n",
        "                 fontsize=15, fontweight='bold', pad=15)\n",
        "    ax.legend(loc='best', fontsize=10, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.invert_xaxis()  # Upstream (larger stations) on left\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\u2713 Plotted {len(profiles)} profiles for {river} - {reach}\")\n",
        "    print(f\"  Station range: {group_df['node_id'].min():.1f} to {group_df['node_id'].max():.1f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:55.135745Z",
          "iopub.status.busy": "2025-11-17T19:05:55.135457Z",
          "iopub.status.idle": "2025-11-17T19:05:55.149829Z",
          "shell.execute_reply": "2025-11-17T19:05:55.149281Z"
        }
      },
      "outputs": [],
      "source": [
        "# Export\n",
        "Path(\"working\").mkdir(exist_ok=True)\n",
        "df_steady.to_csv(\"working/steady_plan02.csv\", index=False)\n",
        "print(f\"Exported {len(df_steady)} rows to working/steady_plan02.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Unsteady Time Series (Plan 01)\n",
        "\n",
        "Extract unsteady results. **Note:** Methods automatically set Plan 01 as current!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:05:55.151959Z",
          "iopub.status.busy": "2025-11-17T19:05:55.151619Z",
          "iopub.status.idle": "2025-11-17T19:07:31.696996Z",
          "shell.execute_reply": "2025-11-17T19:07:31.696555Z"
        }
      },
      "outputs": [],
      "source": [
        "# Run Plan 01 (auto-sets as current, waits for completion)\n",
        "# This may take 5-10 minutes!\n",
        "print(\"Running Plan 01 (Unsteady)...\")\n",
        "# success, msgs = RasControl.run_plan(new_plan)  >> Don't use this, it always sets cores to max\n",
        "RasCmdr.compute_plan(\"01\", clear_geompre=True, num_cores=2)  ## Use this instead, it's ras-commander's direct command line wrapper with extra arguments\n",
        "   \n",
        "print(f\"Success: {success}\")\n",
        "print(f\"Messages: {msgs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:07:31.699394Z",
          "iopub.status.busy": "2025-11-17T19:07:31.699111Z",
          "iopub.status.idle": "2025-11-17T19:07:32.773695Z",
          "shell.execute_reply": "2025-11-17T19:07:32.773223Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get output times (auto-sets Plan 01 as current)\n",
        "times = RasControl.get_output_times(\"01\")\n",
        "print(f\"Found {len(times)} timesteps\")\n",
        "print(f\"First: {times[0]}\")\n",
        "print(f\"Last: {times[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting Computation Messages (Unsteady Flow)\n",
        "\n",
        "Similarly, we can extract computation messages for the unsteady flow plan to review:\n",
        "- Simulation timing and performance\n",
        "- Convergence information\n",
        "- Any warnings or errors encountered during computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:07:32.775767Z",
          "iopub.status.busy": "2025-11-17T19:07:32.775512Z",
          "iopub.status.idle": "2025-11-17T19:07:32.789775Z",
          "shell.execute_reply": "2025-11-17T19:07:32.789282Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract computation messages for unsteady flow Plan 01\n",
        "print(\"=\"*80)\n",
        "print(\"COMPUTATION MESSAGES - Plan 01 (Unsteady Flow)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "msgs_unsteady = RasControl.get_comp_msgs(\"01\")\n",
        "\n",
        "if msgs_unsteady:\n",
        "    print(f\"\\nExtracted {len(msgs_unsteady)} characters of computation messages\\n\")\n",
        "    \n",
        "    # Display first 800 characters\n",
        "    print(\"Computation messages (first 800 characters):\")\n",
        "    print(\"-\" * 80)\n",
        "    print(msgs_unsteady[:800])\n",
        "    \n",
        "    if len(msgs_unsteady) > 800:\n",
        "        print(\"\\n... (truncated) ...\")\n",
        "    \n",
        "    # Check for errors/warnings\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Checking for warnings/errors...\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    lines = msgs_unsteady.split('\\n')\n",
        "    issues = [l for l in lines if 'error' in l.lower() or 'warning' in l.lower()]\n",
        "    \n",
        "    if issues:\n",
        "        print(f\"Found {len(issues)} warning/error lines:\")\n",
        "        for issue in issues[:5]:  # Show first 5\n",
        "            print(f\"  - {issue.strip()}\")\n",
        "    else:\n",
        "        print(\"\u2713 No warnings or errors found\")\n",
        "else:\n",
        "    print(\"No computation messages available for Plan 01\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:07:32.791418Z",
          "iopub.status.busy": "2025-11-17T19:07:32.791276Z",
          "iopub.status.idle": "2025-11-17T19:08:04.208694Z",
          "shell.execute_reply": "2025-11-17T19:08:04.208082Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract unsteady (limit to 10 timesteps for demo)\n",
        "df_unsteady = RasControl.get_unsteady_results(\"01\")\n",
        "\n",
        "print(f\"Rows: {len(df_unsteady)}\")\n",
        "print(f\"Timesteps: {df_unsteady['time_index'].nunique()}\")\n",
        "print(f\"XS: {df_unsteady['node_id'].nunique()}\")\n",
        "df_unsteady.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding \"Max WS\" in Unsteady Output\n",
        "\n",
        "**Important:** HEC-RAS unsteady results include a special row with `time_string=\"Max WS\"` (time_index=1). This contains the **maximum values that occurred at ANY computational timestep** during the entire simulation, not just at output intervals.\n",
        "\n",
        "**Why this matters:**\n",
        "- Output intervals (e.g., every 1 hour) may miss the peak flow/WSE\n",
        "- Computational timesteps (e.g., every 30 seconds) capture the true maximum\n",
        "- \"Max WS\" shows the absolute peak, even if it wasn't saved to an output interval\n",
        "\n",
        "**How to use it:**\n",
        "- Include in DataFrame for reference (critical data!)\n",
        "- Filter out when plotting time series (it's not a timestep)\n",
        "- Show as horizontal reference line on plots to indicate peak\n",
        "\n",
        "The next cell demonstrates this pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:08:04.211178Z",
          "iopub.status.busy": "2025-11-17T19:08:04.210883Z",
          "iopub.status.idle": "2025-11-17T19:08:05.861386Z",
          "shell.execute_reply": "2025-11-17T19:08:05.860496Z"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UNSTEADY FLOW: Time Series at Multiple Cross Sections\n",
        "# ============================================================================\n",
        "# NOTE: This cell shows LEGACY manual datetime parsing for reference.\n",
        "# For v0.81.0+, see the cell below for automatic datetime usage!\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "\n",
        "# Convert node_id to float\n",
        "df_unsteady['node_id'] = df_unsteady['node_id'].astype(float)\n",
        "\n",
        "# Separate Max WS from timeseries data\n",
        "df_maxws = df_unsteady[df_unsteady['time_string'] == 'Max WS'].copy()\n",
        "df_timeseries = df_unsteady[df_unsteady['time_string'] != 'Max WS'].copy()\n",
        "\n",
        "# LEGACY: Parse datetime for timeseries (NOT NEEDED in v0.81.0+ - datetime column auto-included!)\n",
        "df_timeseries['datetime'] = pd.to_datetime(df_timeseries['time_string'], \n",
        "                                           format='%d%b%Y %H%M', errors='coerce')\n",
        "\n",
        "# Select cross sections to plot (every 20th station for manageable plot count)\n",
        "all_xs = sorted(df_timeseries['node_id'].unique(), reverse=True)  # Upstream to downstream\n",
        "selected_xs = all_xs[::20]  # Adjust step size as needed (20, 30, etc.)\n",
        "\n",
        "if len(selected_xs) == 0:\n",
        "    selected_xs = [all_xs[0]]  # At least plot one\n",
        "\n",
        "print(f\"Creating time series plots for {len(selected_xs)} cross sections:\")\n",
        "print(f\"  Stations: {[f'{xs:.1f}' for xs in selected_xs]}\\n\")\n",
        "\n",
        "# Create subplots - one per cross section\n",
        "n_xs = len(selected_xs)\n",
        "fig, axes = plt.subplots(n_xs, 1, figsize=(16, 4*n_xs))\n",
        "if n_xs == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, xs in enumerate(selected_xs):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get data for this cross section\n",
        "    xs_data = df_timeseries[df_timeseries['node_id'] == xs].sort_values('datetime')\n",
        "    maxws_data = df_maxws[df_maxws['node_id'] == xs]\n",
        "    \n",
        "    if len(xs_data) == 0:\n",
        "        ax.text(0.5, 0.5, f'No data for station {xs:.1f}', \n",
        "                ha='center', va='center', transform=ax.transAxes)\n",
        "        continue\n",
        "    \n",
        "    # Plot WSE time series\n",
        "    ax.plot(xs_data['datetime'], xs_data['wsel'], \n",
        "            'b-o', linewidth=2, markersize=5, label='WSE (output intervals)', \n",
        "            alpha=0.8)\n",
        "    \n",
        "    # Get max values\n",
        "    max_ws_value = maxws_data['wsel'].iloc[0] if len(maxws_data) > 0 else None\n",
        "    max_output_value = xs_data['wsel'].max()\n",
        "    max_output_time = xs_data.loc[xs_data['wsel'].idxmax(), 'datetime']\n",
        "    \n",
        "    # Add horizontal line for computational Max WS\n",
        "    #if max_ws_value:\n",
        "    #    ax.axhline(max_ws_value, color='r', linestyle='--', \n",
        "    #               linewidth=2, alpha=0.7, label='Max WS (computational)')\n",
        "    \n",
        "    # Create annotation text box\n",
        "    annotation_lines = [\n",
        "        f\"Max WS (computational): {max_ws_value:.2f} ft\" if max_ws_value else \"Max WS: N/A\",\n",
        "        f\"Max (output interval): {max_output_value:.2f} ft\",\n",
        "        f\"  at {max_output_time.strftime('%m/%d %H:%M')}\" if pd.notna(max_output_time) else \"\"\n",
        "    ]\n",
        "    annotation_text = '\\n'.join(annotation_lines)\n",
        "    \n",
        "    ax.text(0.02, 0.98, annotation_text, \n",
        "            transform=ax.transAxes, fontsize=10,\n",
        "            verticalalignment='top', horizontalalignment='left',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9, pad=0.5))\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_ylabel('WSE (ft)', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'Station {xs:.1f}', fontsize=12, fontweight='bold', loc='left')\n",
        "    ax.legend(loc='upper right', fontsize=9, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    \n",
        "    # Format x-axis for dates\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\\n%H:%M'))\n",
        "    ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))  # Adjust interval as needed\n",
        "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center', fontsize=9)\n",
        "\n",
        "# Add common x-label to bottom subplot\n",
        "axes[-1].set_xlabel('Date / Time', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Unsteady Flow: Water Surface Time Series at Selected Cross Sections',\n",
        "             fontsize=16, fontweight='bold', y=1.0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\u2713 Created time series plots for {len(selected_xs)} stations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NEW in v0.81.0: Automatic Datetime Parsing\n",
        "\n",
        "Starting in version 0.81.0, `get_unsteady_results()` automatically includes a `datetime` column with proper datetime64[ns] objects. **Manual parsing is no longer needed!**\n",
        "\n",
        "**Key Improvements:**\n",
        "- \u2705 `datetime` column added automatically\n",
        "- \u2705 Already in datetime64[ns] format (not strings)\n",
        "- \u2705 \"Max WS\" rows have `pd.NaT` for clean filtering\n",
        "- \u2705 Immediate compatibility with pandas datetime operations\n",
        "- \u2705 Backward compatible - `time_string` still included\n",
        "\n",
        "The cell above shows the old manual parsing method (kept for reference). The next cell demonstrates the modern approach using the automatic `datetime` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:08:05.867596Z",
          "iopub.status.busy": "2025-11-17T19:08:05.867396Z",
          "iopub.status.idle": "2025-11-17T19:08:05.884233Z",
          "shell.execute_reply": "2025-11-17T19:08:05.883715Z"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODERN APPROACH: Using Automatic datetime Column (v0.81.0+)\n",
        "# ============================================================================\n",
        "\n",
        "# Check that datetime column exists and is already parsed\n",
        "print(\"DataFrame columns:\")\n",
        "print(df_unsteady.columns.tolist())\n",
        "print(f\"\\ndatetime column type: {df_unsteady['datetime'].dtype}\")\n",
        "print(f\"Sample datetime values:\")\n",
        "print(df_unsteady[['time_string', 'datetime']].head(10))\n",
        "\n",
        "# Separate using datetime column (NaT for Max WS rows)\n",
        "df_maxws_modern = df_unsteady[df_unsteady['datetime'].isna()].copy()\n",
        "df_timeseries_modern = df_unsteady[df_unsteady['datetime'].notna()].copy()\n",
        "\n",
        "print(f\"\\nMax WS rows: {len(df_maxws_modern)}\")\n",
        "print(f\"Timeseries rows: {len(df_timeseries_modern)}\")\n",
        "\n",
        "# Use pandas datetime accessors directly - no manual parsing needed!\n",
        "print(\"\\nDatetime operations (no parsing required!):\")\n",
        "print(f\"  Simulation start: {df_timeseries_modern['datetime'].min()}\")\n",
        "print(f\"  Simulation end: {df_timeseries_modern['datetime'].max()}\")\n",
        "print(f\"  Duration: {df_timeseries_modern['datetime'].max() - df_timeseries_modern['datetime'].min()}\")\n",
        "print(f\"  Unique hours: {df_timeseries_modern['datetime'].dt.hour.unique()[:10]}\")\n",
        "\n",
        "# Time-based filtering (modern approach)\n",
        "# Example: Get data for a specific date\n",
        "specific_date = pd.Timestamp('1999-02-19')\n",
        "feb_19_data = df_timeseries_modern[df_timeseries_modern['datetime'].dt.date == specific_date.date()]\n",
        "print(f\"\\nData points on {specific_date.date()}: {len(feb_19_data)}\")\n",
        "\n",
        "# Example: Get data for specific time range\n",
        "start_time = pd.Timestamp('1999-02-18 12:00:00')\n",
        "end_time = pd.Timestamp('1999-02-20 12:00:00')\n",
        "time_range_data = df_timeseries_modern[\n",
        "    (df_timeseries_modern['datetime'] >= start_time) & \n",
        "    (df_timeseries_modern['datetime'] <= end_time)\n",
        "]\n",
        "print(f\"Data points between {start_time} and {end_time}: {len(time_range_data)}\")\n",
        "\n",
        "print(\"\\n\u2713 Modern datetime functionality demonstrated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:08:05.886246Z",
          "iopub.status.busy": "2025-11-17T19:08:05.885985Z",
          "iopub.status.idle": "2025-11-17T19:08:06.063285Z",
          "shell.execute_reply": "2025-11-17T19:08:06.062819Z"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UNSTEADY FLOW: Maximum Water Surface Envelope\n",
        "# ============================================================================\n",
        "\n",
        "# Sort by station for profile view\n",
        "max_wse_sorted = df_maxws.sort_values('node_id', ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 7))\n",
        "\n",
        "# Plot maximum WSE envelope\n",
        "ax.plot(max_wse_sorted['node_id'], max_wse_sorted['wsel'],\n",
        "        'r-o', linewidth=2.5, markersize=5, \n",
        "        label='Max WS Envelope (peak at any computational timestep)', \n",
        "        alpha=0.8)\n",
        "\n",
        "# Add channel invert\n",
        "invert = max_wse_sorted[['node_id', 'min_ch_el']].drop_duplicates('node_id').sort_values('node_id', ascending=False)\n",
        "ax.plot(invert['node_id'], invert['min_ch_el'],\n",
        "        'k--', linewidth=2.5, alpha=0.7, label='Channel Invert')\n",
        "\n",
        "# Fill between for visual clarity\n",
        "ax.fill_between(max_wse_sorted['node_id'], \n",
        "                max_wse_sorted['min_ch_el'], \n",
        "                max_wse_sorted['wsel'],\n",
        "                alpha=0.2, color='blue', label='Maximum Flow Depth')\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('River Station', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Elevation (ft)', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Maximum Water Surface Envelope\\n(Peak elevation reached at any computational timestep during simulation)',\n",
        "             fontsize=15, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=11, loc='best', framealpha=0.9)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.invert_xaxis()  # Upstream on left\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "max_depth = (max_wse_sorted['wsel'] - max_wse_sorted['min_ch_el']).max()\n",
        "max_depth_station = max_wse_sorted.loc[(max_wse_sorted['wsel'] - max_wse_sorted['min_ch_el']).idxmax(), 'node_id']\n",
        "\n",
        "print(f\"\\n\u2713 Maximum Water Surface Envelope\")\n",
        "print(f\"  Max depth: {max_depth:.2f} ft at station {max_depth_station:.1f}\")\n",
        "print(f\"  Highest WSE: {max_wse_sorted['wsel'].max():.2f} ft at station {max_wse_sorted.loc[max_wse_sorted['wsel'].idxmax(), 'node_id']:.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:08:06.065813Z",
          "iopub.status.busy": "2025-11-17T19:08:06.065276Z",
          "iopub.status.idle": "2025-11-17T19:08:06.214514Z",
          "shell.execute_reply": "2025-11-17T19:08:06.213873Z"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UNSTEADY FLOW: Velocity Hydrographs at Key Locations\n",
        "# ============================================================================\n",
        "\n",
        "# Select critical stations (upstream, middle, downstream)\n",
        "all_stations = sorted(df_timeseries['node_id'].unique(), reverse=True)\n",
        "n_stations = len(all_stations)\n",
        "\n",
        "if n_stations >= 3:\n",
        "    critical_xs = [\n",
        "        all_stations[0],                    # Upstream\n",
        "        all_stations[n_stations // 2],      # Middle\n",
        "        all_stations[-1]                    # Downstream\n",
        "    ]\n",
        "    labels = ['Upstream', 'Midstream', 'Downstream']\n",
        "else:\n",
        "    critical_xs = all_stations\n",
        "    labels = [f'Station {i+1}' for i in range(len(critical_xs))]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "# Plot velocity hydrographs\n",
        "for idx, (xs, label) in enumerate(zip(critical_xs, labels)):\n",
        "    xs_data = df_timeseries[df_timeseries['node_id'] == xs].sort_values('datetime')\n",
        "    ax.plot(xs_data['datetime'], xs_data['velocity'],\n",
        "            marker='o', linewidth=2, markersize=5,\n",
        "            color=colors[idx % len(colors)],\n",
        "            label=f'{label} (Sta {xs:.1f})', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Date / Time', fontsize=13, fontweight='bold')\n",
        "ax.set_ylabel('Velocity (ft/s)', fontsize=13, fontweight='bold')\n",
        "ax.set_title('Velocity Hydrographs at Key Cross Sections', \n",
        "              fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11, loc='best', framealpha=0.9)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d\\n%H:%M'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\u2713 Created velocity hydrographs for {len(critical_xs)} key locations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "## Complete RasControl API\n",
        "\n",
        "```python\n",
        "# Initialize with version (flexible formats)\n",
        "init_ras_project(path, \"4.1\")  # or \"41\", \"66\", \"5.0.6\", \"506\", etc.\n",
        "\n",
        "# Run plans (auto-sets as current, waits for completion)\n",
        "# NOTE: run_plan() now checks if plan is current before running\n",
        "# If results are up-to-date, it skips computation (faster workflow)\n",
        "success, msgs = RasControl.run_plan(\"02\")\n",
        "\n",
        "# To force recomputation regardless of current status:\n",
        "success, msgs = RasControl.run_plan(\"02\", force_recompute=True)\n",
        "\n",
        "# Extract steady (auto-sets as current)\n",
        "df_steady = RasControl.get_steady_results(\"02\")\n",
        "\n",
        "# Extract unsteady (auto-sets as current, includes Max WS)\n",
        "df_unsteady = RasControl.get_unsteady_results(\"01\")\n",
        "\n",
        "# Filter for time series plotting\n",
        "df_timeseries = df_unsteady[df_unsteady['time_string'] != 'Max WS']\n",
        "max_wse = df_unsteady[df_unsteady['time_string'] == 'Max WS']['wsel'].iloc[0]\n",
        "```\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- \u2705 Plan numbers (not file paths)\n",
        "- \u2705 Auto-sets current plan\n",
        "- \u2705 Blocks until completion\n",
        "- \u2705 Steady AND unsteady\n",
        "- \u2705 All versions 3.0-6.7\n",
        "- \u2705 Flexible version formats\n",
        "- \u2705 Includes Max WS data\n",
        "- \u2705 Multi-version comparison (optional)\n",
        "\n",
        "## What Was Demonstrated\n",
        "\n",
        "1. **Steady workflow** - Plan 02 extraction and plotting\n",
        "2. **Unsteady workflow** - Plan 01 time series with Max WS reference\n",
        "3. **Max WS handling** - Understanding and visualizing peak values\n",
        "4. **Multi-version comparison** - Optional cells for version validation\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Apply to your legacy HEC-RAS models\n",
        "- Run multi-version comparison for migration validation\n",
        "- For HEC-RAS 6.0+: Use HDF methods for better performance\n",
        "  - `19_steady_flow_analysis.ipynb`\n",
        "  - `10_1d_hdf_data_extraction.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What This Creates\n",
        "\n",
        "Running the multi-version comparison cells will:\n",
        "\n",
        "**New Plans in Project:**\n",
        "- `02_41`, `02_506`, `02_63`, `02_66` (steady)\n",
        "- `01_41`, `01_506`, `01_63`, `01_66` (unsteady)\n",
        "\n",
        "**CSV Files in working/:**\n",
        "- `steady_v41.csv`, `steady_v506.csv`, `steady_v63.csv`, `steady_v66.csv`\n",
        "- `unsteady_v41.csv`, `unsteady_v506.csv`, `unsteady_v63.csv`, `unsteady_v66.csv`\n",
        "\n",
        "**Results:**\n",
        "- All plans remain in project for further analysis\n",
        "- CSV files for external comparison\n",
        "- Plots showing version differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:08:06.216949Z",
          "iopub.status.busy": "2025-11-17T19:08:06.216484Z",
          "iopub.status.idle": "2025-11-17T19:17:53.446182Z",
          "shell.execute_reply": "2025-11-17T19:17:53.445593Z"
        }
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Multi-version unsteady comparison\n",
        "# Uncomment to run (may take 1-2 Hr)\n",
        "\n",
        "from ras_commander import RasPlan\n",
        "\n",
        "# Step 1: Update Plan 01 output intervals for more detail\n",
        "print(\"Step 1: Updating Plan 01 intervals...\")\n",
        "init_ras_project(project_path, \"6.6\")  # Use latest for modification\n",
        "\n",
        "# Update intervals: Output=15MIN, Mapping=15MIN\n",
        "RasPlan.update_plan_intervals(\"01\", \n",
        "                              output_interval=\"10MIN\",\n",
        "                              mapping_interval=\"10MIN\")\n",
        "print(\"  \u2713 Output Interval: 1HOUR \u2192 6MIN\")\n",
        "print(\"  \u2713 Mapping Interval: 1HOUR \u2192 6MIN\\n\")\n",
        "\n",
        "# Step 2: Run across versions\n",
        "# All versions with actual COM interfaces\n",
        "test_versions = [\n",
        "    (\"4.1\", \"41\"),       # HEC-RAS 4.1     \u2192 RAS41.HECRASController\n",
        "#    (\"5.0.1\", \"501\"),    # HEC-RAS 5.0.1   \u2192 RAS501.HECRASController  >> FREEZES, SKIP, LIKELY ISSUE WITH HECRASCONTROLLER\n",
        "#    (\"5.0.3\", \"503\"),    # HEC-RAS 5.0.3   \u2192 RAS503.HECRASController\n",
        "    (\"5.0.4\", \"504\"),    # HEC-RAS 5.0.4   \u2192 RAS504.HECRASController\n",
        "    (\"5.0.6\", \"506\"),    # HEC-RAS 5.0.6   \u2192 RAS506.HECRASController\n",
        "    (\"6.3.1\", \"631\"),    # HEC-RAS 6.3.1   \u2192 RAS631.HECRASController\n",
        "    (\"6.6\", \"66\"),       # HEC-RAS 6.6     \u2192 RAS66.HECRASController\n",
        "]\n",
        "\n",
        "unsteady_results = {}\n",
        "max_ws_data = {}  # Store Max WS separately\n",
        "\n",
        "print(\"=== MULTI-VERSION UNSTEADY COMPARISON ===\\n\")\n",
        "\n",
        "for version_name, version_code in test_versions:\n",
        "    print(f\"Processing HEC-RAS {version_name}...\")\n",
        "    \n",
        "    # Clone Plan 01 for this version\n",
        "    new_plan = RasPlan.clone_plan(\"01\",\n",
        "                      new_shortid=f\"Unsteady_{version_code}\",\n",
        "                      new_title=f\"Unsteady - v{version_name}\")\n",
        "    print(f\"  Cloned to Plan {new_plan}\")\n",
        "    \n",
        "    # Re-initialize with this version\n",
        "    init_ras_project(project_path, version_name)\n",
        "    \n",
        "    # Run the plan (this will take several minutes!)\n",
        "    print(f\"  Running Plan {new_plan} (may take 5-10 min)...\")\n",
        "    # NOTE: Using force_recompute=True for fresh cloned plans to ensure computation\n",
        "    # (Default behavior now checks if plan is current and skips if already computed)\n",
        "    #success, msgs = RasControl.run_plan(new_plan, force_recompute=True)\n",
        "    success, msgs = RasControl.run_plan(new_plan, force_recompute=True)\n",
        "    print(success, msgs)\n",
        "    if success:\n",
        "        # Extract results (limit to 20 timesteps for comparison)\n",
        "        df = RasControl.get_unsteady_results(new_plan)\n",
        "        \n",
        "        # Separate Max WS from timeseries\n",
        "        max_ws_data[version_name] = df[df['time_string'] == 'Max WS'].copy()\n",
        "        unsteady_results[version_name] = df[df['time_string'] != 'Max WS'].copy()\n",
        "        \n",
        "        # Save CSV\n",
        "        csv_path = Path(f\"working/unsteady_v{version_code}.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"  \u2713 Extracted {len(df)} rows -> {csv_path}\")\n",
        "    else:\n",
        "        print(f\"  \u2717 Failed\")\n",
        "    \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:17:53.448669Z",
          "iopub.status.busy": "2025-11-17T19:17:53.448393Z",
          "iopub.status.idle": "2025-11-17T19:18:03.889215Z",
          "shell.execute_reply": "2025-11-17T19:18:03.888522Z"
        }
      },
      "outputs": [],
      "source": [
        "# Plot comparison at multiple cross sections (every 5th station)\n",
        "if unsteady_results:\n",
        "    # Gather all station IDs across versions\n",
        "    xs_set = set()\n",
        "    for df in unsteady_results.values():\n",
        "        try:\n",
        "            xs_set.update(df['node_id'].astype(float).unique().tolist())\n",
        "        except Exception:\n",
        "            xs_set.update(df['node_id'].unique().tolist())\n",
        "\n",
        "    all_xs = sorted(xs_set, reverse=True)  # upstream -> downstream\n",
        "    if not all_xs:\n",
        "        print(\"No cross section data found in unsteady_results\")\n",
        "    else:\n",
        "        # Select every 5th cross section for plotting (adjust step as needed)\n",
        "        step = 5\n",
        "        selected_xs = all_xs[::step] if len(all_xs) > step else all_xs\n",
        "        print(f\"Plotting {len(selected_xs)} stations (every {step}th of {len(all_xs)} total)\")\n",
        "\n",
        "        for xs_id in selected_xs:\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 6))\n",
        "\n",
        "            # Plot WSE time series for each version at this station\n",
        "            for version, df in unsteady_results.items():\n",
        "                # FIX: Convert node_id to float for comparison\n",
        "                df_float = df.copy()\n",
        "                df_float['node_id'] = df_float['node_id'].astype(float)\n",
        "                xs_data = df_float[df_float['node_id'] == xs_id].sort_values('time_index')\n",
        "                if len(xs_data):\n",
        "                    ax1.plot(xs_data['time_index'], xs_data['wsel'],\n",
        "                             marker='o', label=f'v{version}', alpha=0.8)\n",
        "\n",
        "            # Add Max WS reference lines (if available) for this station\n",
        "            for version, df in max_ws_data.items():\n",
        "                try:\n",
        "                    df_float = df.copy()\n",
        "                    df_float['node_id'] = df_float['node_id'].astype(float)\n",
        "                    max_row = df_float[df_float['node_id'] == xs_id]\n",
        "                    if len(max_row):\n",
        "                        max_wse = float(max_row['wsel'].iloc[0])\n",
        "                        ax1.axhline(max_wse, linestyle='--', alpha=0.5, label=f'MaxWS v{version}')\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            ax1.set_xlabel('Time Index', fontsize=11)\n",
        "            ax1.set_ylabel('Water Surface Elevation (ft)', fontsize=11)\n",
        "            ax1.set_title(f'WSE Time Series at {xs_id} - Version Comparison',\n",
        "                          fontsize=13, fontweight='bold')\n",
        "            ax1.legend(fontsize=8)\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot Flow time series for each version at this station\n",
        "            for version, df in unsteady_results.items():\n",
        "                # FIX: Convert node_id to float for comparison\n",
        "                df_float = df.copy()\n",
        "                df_float['node_id'] = df_float['node_id'].astype(float)\n",
        "                xs_data = df_float[df_float['node_id'] == xs_id].sort_values('time_index')\n",
        "                if len(xs_data):\n",
        "                    ax2.plot(xs_data['time_index'], xs_data['flow'],\n",
        "                             marker='o', label=f'v{version}', alpha=0.8)\n",
        "\n",
        "            ax2.set_xlabel('Time Index', fontsize=11)\n",
        "            ax2.set_ylabel('Flow (cfs)', fontsize=11)\n",
        "            ax2.set_title(f'Flow Time Series at {xs_id} - Version Comparison',\n",
        "                          fontsize=13, fontweight='bold')\n",
        "            ax2.legend(fontsize=8)\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Basic per-station stats\n",
        "            print(f\"\\nStation {xs_id}:\")\n",
        "            for version, df in unsteady_results.items():\n",
        "                df_float = df.copy()\n",
        "                df_float['node_id'] = df_float['node_id'].astype(float)\n",
        "                xs_data = df_float[df_float['node_id'] == xs_id]\n",
        "                if len(xs_data):\n",
        "                    print(f\"  v{version}: timesteps={len(xs_data)}, max_wse={xs_data['wsel'].max():.2f} ft\")\n",
        "                else:\n",
        "                    print(f\"  v{version}: no data\")\n",
        "\n",
        "        # Summary of Max WS across versions for the first selected station (if any)\n",
        "        if selected_xs:\n",
        "            summary_xs = selected_xs[0]\n",
        "            print(f\"\\nMax WSE by version at station {summary_xs}:\")\n",
        "            for version, df in max_ws_data.items():\n",
        "                try:\n",
        "                    df_float = df.copy()\n",
        "                    df_float['node_id'] = df_float['node_id'].astype(float)\n",
        "                    max_row = df_float[df_float['node_id'] == summary_xs]\n",
        "                    if len(max_row):\n",
        "                        max_wse = float(max_row['wsel'].iloc[0])\n",
        "                        print(f\"  v{version}: {max_wse:.2f} ft\")\n",
        "                    else:\n",
        "                        print(f\"  v{version}: N/A\")\n",
        "                except Exception:\n",
        "                    print(f\"  v{version}: N/A\")\n",
        "\n",
        "else:\n",
        "    print(\"Uncomment code above to run multi-version comparison\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T19:18:03.891945Z",
          "iopub.status.busy": "2025-11-17T19:18:03.891491Z",
          "iopub.status.idle": "2025-11-17T19:18:33.587488Z",
          "shell.execute_reply": "2025-11-17T19:18:33.586876Z"
        }
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Multi-version steady state comparison\n",
        "\n",
        "from ras_commander import RasPlan\n",
        "\n",
        "# Versions to test - all versions with actual COM interfaces\n",
        "test_versions = [\n",
        "  #  (\"4.1\", \"41\"),       # HEC-RAS 4.1     \u2192 RAS41.HECRASController\n",
        "  #  (\"5.0.1\", \"501\"),    # HEC-RAS 5.0.1   \u2192 RAS501.HECRASController\n",
        "  #  (\"5.0.3\", \"503\"),    # HEC-RAS 5.0.3   \u2192 RAS503.HECRASController\n",
        "    (\"5.0.4\", \"504\"),    # HEC-RAS 5.0.4   \u2192 RAS504.HECRASController\n",
        "    (\"5.0.6\", \"506\"),    # HEC-RAS 5.0.6   \u2192 RAS506.HECRASController\n",
        "    (\"6.3.1\", \"631\"),    # HEC-RAS 6.3.1   \u2192 RAS631.HECRASController\n",
        "    (\"6.6\", \"66\"),       # HEC-RAS 6.6     \u2192 RAS66.HECRASController\n",
        "]\n",
        "\n",
        "steady_results = {}\n",
        "\n",
        "print(\"=== MULTI-VERSION STEADY STATE COMPARISON ===\\n\")\n",
        "\n",
        "for version_name, version_code in test_versions:\n",
        "    print(f\"Processing HEC-RAS {version_name}...\")\n",
        "    \n",
        "    # Clone Plan 02 for this version\n",
        "    new_plan = RasPlan.clone_plan(\"02\",\n",
        "                      new_shortid=f\"Steady_{version_code}\",\n",
        "                      new_title=f\"Steady - v{version_name}\")\n",
        "    print(f\"  Cloned to Plan {new_plan}\")\n",
        "    \n",
        "    # Re-initialize with this version\n",
        "    init_ras_project(project_path, version_name)\n",
        "    \n",
        "    # Run the plan using ras-commander's compute_plan() instead of RasControl.run_plan\n",
        "    print(f\"  Running Plan {new_plan} with 2 cores...\")\n",
        "    try:\n",
        "        # Use direct command line execution, preferred over RasControl.run_plan\n",
        "        #RasCmdr.compute_plan(new_plan, clear_geompre=True, num_cores=2)\n",
        "        success, msgs = RasControl.run_plan(new_plan, force_recompute=True)\n",
        "        print(success, msgs)\n",
        "        # Extract results\n",
        "        df = RasControl.get_steady_results(new_plan)\n",
        "        steady_results[version_name] = df\n",
        "\n",
        "        # Save CSV\n",
        "        csv_path = Path(f\"working/steady_v{version_code}.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"  \u2713 Extracted {len(df)} rows -> {csv_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Failed: {e}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "# Plot comparison - first profile from each version\n",
        "if steady_results:\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    \n",
        "    for version, df in steady_results.items():\n",
        "        first_prof = df[df['profile'] == df['profile'].iloc[0]]\n",
        "        ax.plot(range(len(first_prof)), first_prof['wsel'], \n",
        "                marker='o', label=f'v{version}', alpha=0.7)\n",
        "    \n",
        "    ax.set_xlabel('Cross Section Index', fontsize=12)\n",
        "    ax.set_ylabel('Water Surface Elevation (ft)', fontsize=12)\n",
        "    ax.set_title('Steady State Profile - Multi-Version Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n\u2713 Compared {len(steady_results)} versions\")\n",
        "\n",
        "else:\n",
        "    print(\"Uncomment code above to run multi-version comparison\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\18_breach_results_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:16.358451Z",
          "iopub.status.busy": "2025-11-17T17:45:16.358126Z",
          "iopub.status.idle": "2025-11-17T17:45:18.126139Z",
          "shell.execute_reply": "2025-11-17T17:45:18.125568Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dam Breach Results Extraction and Sensitivity Analysis\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Extracting baseline breach results** from HDF files\n",
        "2. **Reading breach parameters** from plan files\n",
        "3. **Modifying parameters iteratively** (one parameter at a time)\n",
        "4. **Comparing results** across different scenarios\n",
        "5. **Visualizing sensitivity** to parameter changes\n",
        "\n",
        "**Project:** BaldEagleCrkMulti2D (HEC-RAS Example)  \n",
        "**Baseline Plan:** 02  \n",
        "**Version:** 6.6\n",
        "\n",
        "**Workflow:**\n",
        "- Extract baseline results\n",
        "- Clone plan and modify one parameter\n",
        "- Re-extract results and compare\n",
        "- Repeat for multiple parameters\n",
        "- Plot all scenarios together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.129879Z",
          "iopub.status.busy": "2025-11-17T17:45:18.129416Z",
          "iopub.status.idle": "2025-11-17T17:45:18.132725Z",
          "shell.execute_reply": "2025-11-17T17:45:18.132205Z"
        }
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.134875Z",
          "iopub.status.busy": "2025-11-17T17:45:18.134619Z",
          "iopub.status.idle": "2025-11-17T17:45:18.137708Z",
          "shell.execute_reply": "2025-11-17T17:45:18.137058Z"
        }
      },
      "outputs": [],
      "source": [
        "#! pip uninstall -y ras-commander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.140385Z",
          "iopub.status.busy": "2025-11-17T17:45:18.140052Z",
          "iopub.status.idle": "2025-11-17T17:45:18.144723Z",
          "shell.execute_reply": "2025-11-17T17:45:18.144160Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.147113Z",
          "iopub.status.busy": "2025-11-17T17:45:18.146826Z",
          "iopub.status.idle": "2025-11-17T17:45:18.151896Z",
          "shell.execute_reply": "2025-11-17T17:45:18.151319Z"
        }
      },
      "outputs": [],
      "source": [
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "    \n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Now try to import again\n",
        "from ras_commander import *\n",
        "\n",
        "# Verify we're loading from the local copy\n",
        "import ras_commander\n",
        "local_path = Path(ras_commander.__file__).parent.parent\n",
        "print(f\"ras-commander loaded from: {local_path}\")\n",
        "print(f\"Expected local path: {rascmdr_directory}\")\n",
        "print(f\"Successfully using local copy: {local_path == rascmdr_directory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Extract and Initialize Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.156737Z",
          "iopub.status.busy": "2025-11-17T17:45:18.155957Z",
          "iopub.status.idle": "2025-11-17T17:45:18.160513Z",
          "shell.execute_reply": "2025-11-17T17:45:18.159539Z"
        }
      },
      "outputs": [],
      "source": [
        "# Use existing folder if present, else extract\n",
        "from pathlib import Path\n",
        "example_project_folder = Path(\"c:/GH/ras-commander/examples/example_projects/BaldEagleCrkMulti2D\")\n",
        "if example_project_folder.exists():\n",
        "    project_path = example_project_folder\n",
        "    print(f\"Project folder already exists: {project_path}\")\n",
        "else:\n",
        "    project_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\")\n",
        "    print(f\"Extracted project to: {project_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.168543Z",
          "iopub.status.busy": "2025-11-17T17:45:18.168236Z",
          "iopub.status.idle": "2025-11-17T17:45:18.259026Z",
          "shell.execute_reply": "2025-11-17T17:45:18.257370Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the project\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "print(f\"\\nInitialized project: {ras.project_name}\")\n",
        "print(f\"\\nAvailable plans:\")\n",
        "ras.plan_df\n",
        "\n",
        "# This is the SA to 2D Dam Break Run\n",
        "template_plan = \"19\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RasResultsBreach - Breach Results (HDF Files)\n",
        "**Use for:** Extracting breach simulation RESULTS from HDF output files (.p##.hdf)\n",
        "\n",
        "```python\n",
        "from ras_commander import HdfResultsBreach\n",
        "\n",
        "# Extract complete time series (flow, stage, breach geometry evolution)\n",
        "timeseries = HdfResultsBreach.get_breach_timeseries(\"02\", \"Dam\")\n",
        "\n",
        "# Get summary statistics (peaks, timing, final geometry)\n",
        "summary = HdfResultsBreach.get_breach_summary(\"02\")\n",
        "\n",
        "# Get breach-specific variables (width, depth, slopes over time)\n",
        "breach_vars = HdfResultsBreach.get_breaching_variables(\"02\", \"Dam\")\n",
        "```\n",
        "\n",
        "**Key Points:**\n",
        "- \u2705 Extracts results AFTER HEC-RAS has run\n",
        "- \u2705 Works with plan numbers or HDF file paths\n",
        "- \u2705 Requires .p##.hdf file to exist (created by HEC-RAS)\n",
        "- \u26a0\ufe0f Structure names in HDF may include prefixes (e.g., \"BaldEagleCr Dam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. BASELINE: Extract Existing Results from Plan 02\n",
        "\n",
        "First, extract and analyze the baseline breach behavior from the existing Plan 02 results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Important Note: HDF Results Files\n",
        "\n",
        "**This example project may not include pre-computed HDF results** (.p02.hdf files). These files are generated when HEC-RAS runs a simulation.\n",
        "\n",
        "**If you encounter \"HDF file not found\" errors:**\n",
        "1. Option A: Run HEC-RAS simulation for Plan 02 first\n",
        "2. Option B: Use the RasCmdr class to run simulation from Python (see cell below)\n",
        "3. Option C: Use a different project with existing results (e.g., Scott County)\n",
        "\n",
        "**The notebook will gracefully handle missing results and still demonstrate parameter modification.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.262887Z",
          "iopub.status.busy": "2025-11-17T17:45:18.262559Z",
          "iopub.status.idle": "2025-11-17T17:45:18.277939Z",
          "shell.execute_reply": "2025-11-17T17:45:18.277356Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize variables to None (prevents NameError in later cells)\n",
        "target_structure = None\n",
        "baseline_ts = pd.DataFrame()\n",
        "baseline_summary = pd.DataFrame()\n",
        "baseline_params = None\n",
        "baseline_geom = []\n",
        "scenarios = {}\n",
        "summaries = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Identify Breach Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.280814Z",
          "iopub.status.busy": "2025-11-17T17:45:18.280497Z",
          "iopub.status.idle": "2025-11-17T17:45:18.287087Z",
          "shell.execute_reply": "2025-11-17T17:45:18.286522Z"
        }
      },
      "outputs": [],
      "source": [
        "# List breach structures from PLAN FILE (for parameter operations)\n",
        "# This ensures structure names match between listing and read_breach_block()\n",
        "try:\n",
        "    breach_structures_list = RasBreach.list_breach_structures_plan(template_plan)\n",
        "    \n",
        "    print(\"Breach Structures in Plan File:\")\n",
        "    for struct in breach_structures_list:\n",
        "        if struct['structure']:  # Filter empty names\n",
        "            status = \"ACTIVE\" if struct['is_active'] else \"INACTIVE\"  \n",
        "            location = f\"{struct['river']}/{struct['reach']}/RS {struct['station']}\" if struct['river'] else \"No location\"\n",
        "            print(f\"  - {struct['structure']}: {status} ({location})\")\n",
        "    \n",
        "    # Get active structure names for parameter operations\n",
        "    breach_structures = [s['structure'] for s in breach_structures_list \n",
        "                        if s['structure'] and s['is_active']]\n",
        "    \n",
        "    if breach_structures:\n",
        "        target_structure = breach_structures[0]\n",
        "        print(f\"\\nTarget structure for analysis: {target_structure}\")\n",
        "        print(f\"  This name will work with RasBreach.read_breach_block()\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: No active breach structures found!\")\n",
        "        target_structure = None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error listing breach structures: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    target_structure = None\n",
        "    breach_structures = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if target_structure:\n",
        "    try:\n",
        "        # Extract complete breach time series from HDF results\n",
        "        baseline_ts = HdfResultsBreach.get_breach_timeseries(\"02\", target_structure)\n",
        "        \n",
        "        print(f\"Baseline Time Series Extracted: {baseline_ts.shape}\")\n",
        "        print(f\"\\nColumns: {list(baseline_ts.columns)}\")\n",
        "        print(f\"\\nFirst few timesteps:\")\n",
        "        print(baseline_ts.head())\n",
        "        \n",
        "        # Get summary statistics from HDF results\n",
        "        baseline_summary = HdfResultsBreach.get_breach_summary(\"02\", target_structure)\n",
        "        print(f\"\\nBaseline Summary Statistics:\")\n",
        "        print(baseline_summary.to_string(index=False))\n",
        "        \n",
        "        # Store baseline for later comparison\n",
        "        scenarios = {\n",
        "            'Baseline (Plan 02)': baseline_ts.copy()\n",
        "        }\n",
        "        summaries = {\n",
        "            'Baseline (Plan 02)': baseline_summary.copy()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Could not extract baseline time series: {e}\")\n",
        "        print(\"Continuing with parameter analysis only...\")\n",
        "        baseline_ts = pd.DataFrame()\n",
        "        baseline_summary = pd.DataFrame()\n",
        "        scenarios = {}\n",
        "        summaries = {}\n",
        "else:\n",
        "    print(\"Skipping baseline extraction - no breach structure available\")\n",
        "    scenarios = {}\n",
        "    summaries = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.290339Z",
          "iopub.status.busy": "2025-11-17T17:45:18.290075Z",
          "iopub.status.idle": "2025-11-17T17:45:18.296345Z",
          "shell.execute_reply": "2025-11-17T17:45:18.295805Z"
        }
      },
      "outputs": [],
      "source": [
        "template_plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.298966Z",
          "iopub.status.busy": "2025-11-17T17:45:18.298690Z",
          "iopub.status.idle": "2025-11-17T17:45:18.322474Z",
          "shell.execute_reply": "2025-11-17T17:45:18.321780Z"
        }
      },
      "outputs": [],
      "source": [
        "# Read current parameters\n",
        "params = RasBreach.read_breach_block(template_plan, \"Dam\")\n",
        "geom = [x.strip() for x in params['values']['Breach Geom'].split(',')]\n",
        "\n",
        "# Update Final Bottom Elevation (index 2)\n",
        "geom[2] = 605  # New elevation in feet\n",
        "\n",
        "# Write back\n",
        "RasBreach.update_breach_block(template_plan, \"Dam\", geom_values=geom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.325003Z",
          "iopub.status.busy": "2025-11-17T17:45:18.324748Z",
          "iopub.status.idle": "2025-11-17T17:45:18.334685Z",
          "shell.execute_reply": "2025-11-17T17:45:18.333910Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure:\n",
        "    try:\n",
        "        # Read breach parameters from plan file\n",
        "        baseline_params = RasBreach.read_breach_block(\"02\", target_structure)\n",
        "        \n",
        "        print(f\"Baseline Parameters for {target_structure}:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nActivation: {baseline_params['is_active']}\")\n",
        "        print(f\"\\nKey Parameter Values:\")\n",
        "        for key in ['Breach Method', 'Breach Geom', 'Breach Start', 'Breach Progression']:\n",
        "            if key in baseline_params['values']:\n",
        "                print(f\"  {key}: {baseline_params['values'][key]}\")\n",
        "        \n",
        "        # Parse geometry values for modification\n",
        "        geom_str = baseline_params['values'].get('Breach Geom', '')\n",
        "        baseline_geom = [x.strip() for x in geom_str.split(',') if x.strip()]\n",
        "        print(f\"\\nBaseline Geometry (parsed): {baseline_geom}\")\n",
        "        \n",
        "        # Explain Breach Geom field structure\n",
        "        if len(baseline_geom) >= 10:\n",
        "            print(\"\\nBreach Geom Field Structure (CSV, 10 fields):\")\n",
        "            print(f\"  [0] Centerline/Station: {baseline_geom[0]} ft\")\n",
        "            print(f\"  [1] Initial Bottom Width: {baseline_geom[1]} ft\")\n",
        "            print(f\"  [2] Final Bottom Elevation: {baseline_geom[2]} ft  <-- KEY PARAMETER\")\n",
        "            print(f\"  [3] Left Side Slope: {baseline_geom[3]} (H:V)\")\n",
        "            print(f\"  [4] Right Side Slope: {baseline_geom[4]} (H:V)\")\n",
        "            print(f\"  [5] Active Flag: {baseline_geom[5]}\")\n",
        "            print(f\"  [6] Weir Coefficient: {baseline_geom[6]}\")\n",
        "            print(f\"  [7] Top Elevation: {baseline_geom[7]} ft\")\n",
        "            print(f\"  [8] Formation Method: {baseline_geom[8]} (1=Time, 2=Trigger)\")\n",
        "            print(f\"  [9] Formation Time/Threshold: {baseline_geom[9]} hrs or ft\")\n",
        "            \n",
        "            print(\"\\n--- Example: Update Final Bottom Elevation ---\")\n",
        "            print(f\"Current value: {baseline_geom[2]} ft\")\n",
        "            print(\"To change to 605 ft:\")\n",
        "            print(\"  new_geom = baseline_geom.copy()\")\n",
        "            print(\"  new_geom[2] = 605\")\n",
        "            print('  RasBreach.update_breach_block(\"template_plan\", \"Dam\", geom_values=new_geom)')\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Could not read baseline parameters: {e}\")\n",
        "        baseline_params = None\n",
        "        baseline_geom = []\n",
        "else:\n",
        "    print(\"Skipping parameter reading - no breach structure available\")\n",
        "    baseline_params = None\n",
        "    baseline_geom = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.337998Z",
          "iopub.status.busy": "2025-11-17T17:45:18.337735Z",
          "iopub.status.idle": "2025-11-17T17:45:18.345597Z",
          "shell.execute_reply": "2025-11-17T17:45:18.345107Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure:\n",
        "    try:\n",
        "        # Extract complete breach time series from HDF results\n",
        "        baseline_ts = HdfResultsBreach.get_breach_timeseries(template_plan, target_structure)\n",
        "        \n",
        "        print(f\"Baseline Time Series Extracted: {baseline_ts.shape}\")\n",
        "        print(f\"\\nColumns: {list(baseline_ts.columns)}\")\n",
        "        print(f\"\\nFirst few timesteps:\")\n",
        "        print(baseline_ts.head())\n",
        "        \n",
        "        # Get summary statistics from HDF results\n",
        "        baseline_summary = HdfResultsBreach.get_breach_summary(template_plan, target_structure)\n",
        "        print(f\"\\nBaseline Summary Statistics:\")\n",
        "        print(baseline_summary.to_string(index=False))\n",
        "        \n",
        "        # Store baseline for later comparison\n",
        "        scenarios = {\n",
        "            f'Baseline (Plan {template_plan})': baseline_ts.copy()\n",
        "        }\n",
        "        summaries = {\n",
        "            f'Baseline (Plan {template_plan})': baseline_summary.copy()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Could not extract baseline time series: {e}\")\n",
        "        print(\"Continuing with parameter analysis only...\")\n",
        "        baseline_ts = pd.DataFrame()\n",
        "        baseline_summary = pd.DataFrame()\n",
        "        scenarios = {}\n",
        "        summaries = {}\n",
        "else:\n",
        "    print(\"Skipping baseline extraction - no breach structure available\")\n",
        "    scenarios = {}\n",
        "    summaries = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.348026Z",
          "iopub.status.busy": "2025-11-17T17:45:18.347730Z",
          "iopub.status.idle": "2025-11-17T17:45:18.354849Z",
          "shell.execute_reply": "2025-11-17T17:45:18.354178Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure:\n",
        "    try:\n",
        "        # Extract complete breach time series\n",
        "        baseline_ts = HdfResultsBreach.get_breach_timeseries(template_plan, target_structure)\n",
        "        \n",
        "        print(f\"Baseline Time Series Extracted: {baseline_ts.shape}\")\n",
        "        print(f\"\\nColumns: {list(baseline_ts.columns)}\")\n",
        "        print(f\"\\nFirst few timesteps:\")\n",
        "        print(baseline_ts.head())\n",
        "        \n",
        "        # Get summary statistics\n",
        "        baseline_summary = HdfResultsBreach.get_breach_summary(template_plan, target_structure)\n",
        "        print(f\"\\nBaseline Summary Statistics:\")\n",
        "        print(baseline_summary.to_string(index=False))\n",
        "        \n",
        "        # Store baseline for later comparison\n",
        "        scenarios = {\n",
        "            f'Baseline (Plan {template_plan})': baseline_ts.copy()\n",
        "        }\n",
        "        summaries = {\n",
        "            f'Baseline (Plan {template_plan})': baseline_summary.copy()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Could not extract baseline time series: {e}\")\n",
        "        print(\"Continuing with parameter analysis only...\")\n",
        "        baseline_ts = pd.DataFrame()\n",
        "        baseline_summary = pd.DataFrame()\n",
        "        scenarios = {}\n",
        "        summaries = {}\n",
        "else:\n",
        "    print(\"Skipping baseline extraction - no breach structure available\")\n",
        "    scenarios = {}\n",
        "    summaries = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Read Baseline Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.357926Z",
          "iopub.status.busy": "2025-11-17T17:45:18.357649Z",
          "iopub.status.idle": "2025-11-17T17:45:18.363862Z",
          "shell.execute_reply": "2025-11-17T17:45:18.363259Z"
        }
      },
      "outputs": [],
      "source": [
        "# List breach structures from plan file (for parameter operations)\n",
        "try:\n",
        "    breach_structures_list = RasBreach.list_breach_structures_plan(template_plan)\n",
        "\n",
        "    print(\"Breach Structures in Plan File:\")\n",
        "    for struct in breach_structures_list:\n",
        "        if struct['structure']:  # Filter empty names\n",
        "            status = \"ACTIVE\" if struct['is_active'] else \"INACTIVE\"\n",
        "            print(f\"  - {struct['structure']}: {status}\")\n",
        "\n",
        "    # Get active structure names\n",
        "    breach_structures = [s['structure'] for s in breach_structures_list\n",
        "                        if s['structure'] and s['is_active']]\n",
        "\n",
        "    if breach_structures:\n",
        "        target_structure = breach_structures[0]\n",
        "        print(f\"\\nTarget structure for analysis: {target_structure}\")\n",
        "    else:\n",
        "        target_structure = None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error listing breach structures: {e}\")\n",
        "    target_structure = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Easy Parameter Modification with set_breach_geom()\n",
        "\n",
        "**NEW FUNCTION:** `RasBreach.set_breach_geom()` provides a clean interface for modifying individual breach parameters without manually parsing/reconstructing the CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.367573Z",
          "iopub.status.busy": "2025-11-17T17:45:18.367159Z",
          "iopub.status.idle": "2025-11-17T17:45:18.372510Z",
          "shell.execute_reply": "2025-11-17T17:45:18.371822Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Update just Final Bottom Elevation (most common modification)\n",
        "if target_structure:\n",
        "    print(\"Example: Update Final Bottom Elevation to 605 ft\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nSimple approach using set_breach_geom():\")\n",
        "    print(\"  RasBreach.set_breach_geom('template_plan', 'Dam',\")\n",
        "    print(\"                            final_bottom_elev=605)\")\n",
        "    print(\"\\nThis automatically:\")\n",
        "    print(\"  1. Reads current Breach Geom values\")\n",
        "    print(\"  2. Updates ONLY the final_bottom_elev field (index 2)\")\n",
        "    print(\"  3. Preserves all other parameters\")\n",
        "    print(\"  4. Writes back to plan file with backup\")\n",
        "    \n",
        "    print(\"\\n\\nOther common modifications:\")\n",
        "    print(\"\\n# Increase breach width by 50%\")\n",
        "    print(\"  current_width = 200  # Read from baseline_params\")\n",
        "    print(\"  RasBreach.set_breach_geom('template_plan', 'Dam',\")\n",
        "    print(\"                            initial_width=current_width * 1.5)\")\n",
        "    \n",
        "    print(\"\\n# Change formation time\")\n",
        "    print(\"  RasBreach.set_breach_geom('template_plan', 'Dam',\")\n",
        "    print(\"                            formation_time=3.5)\")\n",
        "    \n",
        "    print(\"\\n# Update multiple parameters at once\")\n",
        "    print(\"  RasBreach.set_breach_geom('template_plan', 'Dam',\")\n",
        "    print(\"                            final_bottom_elev=605,\")\n",
        "    print(\"                            initial_width=250,\")\n",
        "    print(\"                            formation_time=3.0)\")\n",
        "else:\n",
        "    print(\"No target structure available for examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.375870Z",
          "iopub.status.busy": "2025-11-17T17:45:18.375429Z",
          "iopub.status.idle": "2025-11-17T17:45:18.381942Z",
          "shell.execute_reply": "2025-11-17T17:45:18.381335Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure:\n",
        "    try:\n",
        "        # Read breach parameters from plan file\n",
        "        baseline_params = RasBreach.read_breach_block(template_plan, target_structure)\n",
        "        \n",
        "        print(f\"Baseline Parameters for {target_structure}:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nActivation: {baseline_params['is_active']}\")\n",
        "        print(f\"\\nKey Parameter Values:\")\n",
        "        for key in ['Breach Method', 'Breach Geom', 'Breach Start', 'Breach Progression']:\n",
        "            if key in baseline_params['values']:\n",
        "                print(f\"  {key}: {baseline_params['values'][key]}\")\n",
        "        \n",
        "        # Parse geometry values for modification\n",
        "        geom_str = baseline_params['values'].get('Breach Geom', '')\n",
        "        baseline_geom = [x.strip() for x in geom_str.split(',') if x.strip()]\n",
        "        print(f\"\\nBaseline Geometry (parsed): {baseline_geom}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not read baseline parameters: {e}\")\n",
        "        baseline_params = None\n",
        "        baseline_geom = []\n",
        "else:\n",
        "    print(\"Skipping parameter reading - no breach structure available\")\n",
        "    baseline_params = None\n",
        "    baseline_geom = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. SCENARIO ANALYSIS: Modify Parameters and Compare Results\n",
        "\n",
        "Now we'll create multiple scenarios by modifying breach parameters one at a time.\n",
        "\n",
        "**Workflow for each scenario:**\n",
        "1. Clone Plan 02 to a new plan number\n",
        "2. Modify ONE parameter in the cloned plan\n",
        "3. **[User must run HEC-RAS simulation]**\n",
        "4. Extract results from the new plan\n",
        "5. Compare with baseline\n",
        "\n",
        "**Note:** This notebook demonstrates steps 1-2 and 4-5. You must run HEC-RAS (step 3) separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 1: Increase Breach Width by 50%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.384718Z",
          "iopub.status.busy": "2025-11-17T17:45:18.384460Z",
          "iopub.status.idle": "2025-11-17T17:45:18.487782Z",
          "shell.execute_reply": "2025-11-17T17:45:18.487130Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure and baseline_geom and len(baseline_geom) >= 2:\n",
        "    # Clone plan\n",
        "    scenario_1_plan = RasPlan.clone_plan(template_plan, \"Scenario 1: +50% Width\")\n",
        "    \n",
        "    # Modify breach width (assuming index 1 is width)\n",
        "    try:\n",
        "        new_geom = baseline_geom.copy()\n",
        "        original_width = float(baseline_geom[1])\n",
        "        new_width = original_width * 1.5\n",
        "        new_geom[1] = new_width\n",
        "        \n",
        "        print(f\"\\nModifying breach width:\")\n",
        "        print(f\"  Original: {original_width} ft\")\n",
        "        print(f\"  New: {new_width} ft (+50%)\")\n",
        "        \n",
        "        # Update the plan\n",
        "        RasBreach.update_breach_block(\n",
        "            scenario_1_plan,\n",
        "            target_structure,\n",
        "            geom_values=new_geom\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n\u2713 Scenario 1 plan created: {scenario_1_plan}\")\n",
        "        print(f\"  Next step: Run HEC-RAS simulation for plan {scenario_1_plan}\")\n",
        "    except (ValueError, IndexError) as e:\n",
        "        print(f\"Could not parse geometry values: {e}\")\n",
        "else:\n",
        "    print(\"Skipping Scenario 1 - insufficient baseline data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if target_structure:\n",
        "    # Scenario definitions\n",
        "    scenario_plans = {\n",
        "        'Scenario 1: +50% Width': '03',\n",
        "        'Scenario 2: -50% Formation Time': '04',\n",
        "        'Scenario 3: Different Method': '05'\n",
        "    }\n",
        "    \n",
        "    # Try to extract results for each scenario from HDF files\n",
        "    for scenario_name, plan_num in scenario_plans.items():\n",
        "        try:\n",
        "            # Extract breach results from HDF using HdfResultsBreach\n",
        "            ts = HdfResultsBreach.get_breach_timeseries(plan_num, target_structure)\n",
        "            summary = HdfResultsBreach.get_breach_summary(plan_num, target_structure)\n",
        "            \n",
        "            if not ts.empty:\n",
        "                scenarios[scenario_name] = ts\n",
        "                summaries[scenario_name] = summary\n",
        "                print(f\"\u2713 Extracted: {scenario_name}\")\n",
        "            else:\n",
        "                print(f\"\u26a0 No results for: {scenario_name} (run HEC-RAS first)\")\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"\u26a0 HDF not found for {scenario_name}: Plan {plan_num} (run HEC-RAS first)\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u26a0 Could not extract {scenario_name}: {e}\")\n",
        "    \n",
        "    print(f\"\\nTotal scenarios with results: {len(scenarios)}\")\n",
        "else:\n",
        "    print(\"Skipping scenario extraction - no breach structure available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 2: Decrease Breach Formation Time by 50%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.490868Z",
          "iopub.status.busy": "2025-11-17T17:45:18.490665Z",
          "iopub.status.idle": "2025-11-17T17:45:18.575282Z",
          "shell.execute_reply": "2025-11-17T17:45:18.574850Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure and baseline_geom and len(baseline_geom) >= 7:\n",
        "    # Clone plan\n",
        "    scenario_2_plan = RasPlan.clone_plan(template_plan, \"Scenario 2: -50% Formation Time\")\n",
        "    \n",
        "    # Modify formation time (assuming index 6 is formation time)\n",
        "    try:\n",
        "        new_geom = baseline_geom.copy()\n",
        "        original_time = float(baseline_geom[6])\n",
        "        new_time = original_time * 0.5\n",
        "        new_geom[6] = new_time\n",
        "        \n",
        "        print(f\"\\nModifying breach formation time:\")\n",
        "        print(f\"  Original: {original_time} hrs\")\n",
        "        print(f\"  New: {new_time} hrs (-50%)\")\n",
        "        \n",
        "        # Update the plan\n",
        "        RasBreach.update_breach_block(\n",
        "            scenario_2_plan,\n",
        "            target_structure,\n",
        "            geom_values=new_geom\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n\u2713 Scenario 2 plan created: {scenario_2_plan}\")\n",
        "        print(f\"  Next step: Run HEC-RAS simulation for plan {scenario_2_plan}\")\n",
        "    except (ValueError, IndexError) as e:\n",
        "        print(f\"Could not parse geometry values: {e}\")\n",
        "else:\n",
        "    print(\"Skipping Scenario 2 - insufficient baseline data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:18.580395Z",
          "iopub.status.busy": "2025-11-17T17:45:18.580150Z",
          "iopub.status.idle": "2025-11-17T17:46:23.519281Z",
          "shell.execute_reply": "2025-11-17T17:46:23.518732Z"
        }
      },
      "outputs": [],
      "source": [
        "parallel_computed_folder = example_project_folder.parent / f\"{example_project_folder.name}_parallelcomputed\"\n",
        "RasCmdr.compute_parallel([template_plan, scenario_1_plan, scenario_2_plan], max_workers=4, num_cores=2, dest_folder=Path(parallel_computed_folder), overwrite_dest=True)\n",
        "# Re-initialize in new folder where results are present\n",
        "init_ras_project(parallel_computed_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.521307Z",
          "iopub.status.busy": "2025-11-17T17:46:23.521003Z",
          "iopub.status.idle": "2025-11-17T17:46:23.523792Z",
          "shell.execute_reply": "2025-11-17T17:46:23.523283Z"
        }
      },
      "outputs": [],
      "source": [
        "# Scenario definitions\n",
        "scenario_plans = {\n",
        "    'Scenario 1: +50% Width': scenario_1_plan,\n",
        "    'Scenario 2: -50% Formation Time': scenario_2_plan,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.525887Z",
          "iopub.status.busy": "2025-11-17T17:46:23.525728Z",
          "iopub.status.idle": "2025-11-17T17:46:23.642094Z",
          "shell.execute_reply": "2025-11-17T17:46:23.641636Z"
        }
      },
      "outputs": [],
      "source": [
        "# Try to extract results for each scenario from HDF files\n",
        "for scenario_name, plan_num in scenario_plans.items():\n",
        "    try:\n",
        "        # Extract breach results from HDF using HdfResultsBreach\n",
        "        ts = HdfResultsBreach.get_breach_timeseries(plan_num, target_structure)\n",
        "        summary = HdfResultsBreach.get_breach_summary(plan_num, target_structure)\n",
        "        \n",
        "        if not ts.empty:\n",
        "            scenarios[scenario_name] = ts\n",
        "            summaries[scenario_name] = summary\n",
        "            print(f\"\u2713 Extracted: {scenario_name}\")\n",
        "        else:\n",
        "            print(f\"\u26a0 No results for: {scenario_name} (run HEC-RAS first)\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\u26a0 HDF not found for {scenario_name}: Plan {plan_num} (run HEC-RAS first)\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0 Could not extract {scenario_name}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal scenarios with results: {len(scenarios)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.643967Z",
          "iopub.status.busy": "2025-11-17T17:46:23.643790Z",
          "iopub.status.idle": "2025-11-17T17:46:23.654859Z",
          "shell.execute_reply": "2025-11-17T17:46:23.654360Z"
        }
      },
      "outputs": [],
      "source": [
        "ts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.656789Z",
          "iopub.status.busy": "2025-11-17T17:46:23.656427Z",
          "iopub.status.idle": "2025-11-17T17:46:23.663672Z",
          "shell.execute_reply": "2025-11-17T17:46:23.663158Z"
        }
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Results from All Scenarios\n",
        "\n",
        "**\u26a0\ufe0f IMPORTANT:** You must run HEC-RAS simulations for plans 03, 04, and 05 before this section will work.\n",
        "\n",
        "This section attempts to extract results from all scenarios. If HDF files don't exist, it will skip gracefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.665685Z",
          "iopub.status.busy": "2025-11-17T17:46:23.665530Z",
          "iopub.status.idle": "2025-11-17T17:46:23.673536Z",
          "shell.execute_reply": "2025-11-17T17:46:23.672950Z"
        }
      },
      "outputs": [],
      "source": [
        "from ras_commander import get_logger\n",
        "import textwrap\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "# Get the raw computation messages string from the results HDF for the scenario 1 plan\n",
        "comp_msgs = HdfResultsPlan.get_compute_messages(scenario_1_plan)\n",
        "\n",
        "def pretty_print_compute_messages(msg: str) -> None:\n",
        "    \"\"\"\n",
        "    Nicely format and print RAS compute messages. Strips unnecessary escapes,\n",
        "    ensures readable blocks, and optionally highlights warnings.\n",
        "    \"\"\"\n",
        "    if not msg:\n",
        "        print(\"No computation messages found.\")\n",
        "        return\n",
        "\n",
        "    # Replace carriage returns, unify newlines\n",
        "    msg = msg.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    # Collapse excessive blank lines to at most 2\n",
        "    lines = msg.split('\\n')\n",
        "    pretty_lines = []\n",
        "    blank_count = 0\n",
        "    for line in lines:\n",
        "        if line.strip() == '':\n",
        "            blank_count += 1\n",
        "            if blank_count <= 2:\n",
        "                pretty_lines.append('')\n",
        "        else:\n",
        "            blank_count = 0\n",
        "            # Optionally add highlighting for warnings/errors\n",
        "            l_strip = line.lstrip()\n",
        "            if l_strip.lower().startswith(\"warning\") or \"error\" in l_strip.lower():\n",
        "                pretty_lines.append(\"\u26a0\ufe0f \" + line)\n",
        "            else:\n",
        "                pretty_lines.append(line)\n",
        "    # Optionally wrap long lines for readability\n",
        "    final_lines = []\n",
        "    for l in pretty_lines:\n",
        "        if len(l) > 120:\n",
        "            final_lines.extend(textwrap.wrap(l, width=120))\n",
        "        else:\n",
        "            final_lines.append(l)\n",
        "    # Print result\n",
        "    print('\\n'.join(final_lines))\n",
        "\n",
        "pretty_print_compute_messages(comp_msgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.675608Z",
          "iopub.status.busy": "2025-11-17T17:46:23.675427Z",
          "iopub.status.idle": "2025-11-17T17:46:23.695348Z",
          "shell.execute_reply": "2025-11-17T17:46:23.694955Z"
        }
      },
      "outputs": [],
      "source": [
        "# Try to list SA/2D connection structures in HDF results\n",
        "try:\n",
        "    hdf_structures = HdfStruc.list_sa2d_connections(template_plan)\n",
        "    print(\"SA/2D Connection Structures in HDF:\")\n",
        "    for struct in hdf_structures:\n",
        "        print(f\"  - {struct}\")\n",
        "\n",
        "    # Get breach capability information\n",
        "    breach_info = HdfStruc.get_sa2d_breach_info(template_plan)\n",
        "    print(\"\\nBreach Capability Information:\")\n",
        "    print(breach_info.to_string(index=False))\n",
        "\n",
        "    # Get list of structures with breach capability\n",
        "    breach_structures = breach_info[breach_info['has_breach']]['structure'].tolist()\n",
        "    print(f\"\\nStructures with breach capability: {breach_structures}\")\n",
        "\n",
        "    # Select first breach structure for analysis\n",
        "    if breach_structures:\n",
        "        target_structure = breach_structures[0]\n",
        "        print(f\"\\nTarget structure for analysis: {target_structure}\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: No breach structures found! Cannot proceed with analysis.\")\n",
        "        target_structure = None\n",
        "        \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n\u26a0\ufe0f HDF RESULTS FILE NOT FOUND\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(f\"\\nThis means HEC-RAS has not been run for Plan 02 yet.\")\n",
        "    print(f\"To generate results, either:\")\n",
        "    print(f\"  1. Uncomment and run the simulation cell above, OR\")\n",
        "    print(f\"  2. Open HEC-RAS and manually run Plan 02\")\n",
        "    print(f\"\\nThe notebook will continue to demonstrate parameter modification.\")\n",
        "    target_structure = None\n",
        "    breach_structures = []\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u26a0\ufe0f UNEXPECTED ERROR: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    target_structure = None\n",
        "    breach_structures = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.697289Z",
          "iopub.status.busy": "2025-11-17T17:46:23.697016Z",
          "iopub.status.idle": "2025-11-17T17:46:23.721684Z",
          "shell.execute_reply": "2025-11-17T17:46:23.721265Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get breach-specific variables (width, depth, slopes over time)\n",
        "breach_vars = HdfResultsBreach.get_breaching_variables(template_plan, \"Dam\")\n",
        "breach_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.723606Z",
          "iopub.status.busy": "2025-11-17T17:46:23.723369Z",
          "iopub.status.idle": "2025-11-17T17:46:23.735510Z",
          "shell.execute_reply": "2025-11-17T17:46:23.735109Z"
        }
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Also check HDF for breach capability (requires HDF file to exist)\n",
        "print(\"\\n--- HDF Breach Capability Check (optional) ---\")\n",
        "try:\n",
        "    hdf_structures = HdfStruc.list_sa2d_connections(template_plan)\n",
        "    print(f\"SA/2D Connections in HDF: {hdf_structures}\")\n",
        "    \n",
        "    breach_info = HdfStruc.get_sa2d_breach_info(template_plan)\n",
        "    print(f\"\\nBreach capability info:\")\n",
        "    print(breach_info[['structure', 'has_breach']].to_string(index=False))\n",
        "    \n",
        "    print(f\"\\nNOTE: HDF names may differ from plan file names!\")\n",
        "    print(f\"  Plan file: '{target_structure}'\")\n",
        "    print(f\"  HDF may show: 'AreaName {target_structure}'\")\n",
        "except FileNotFoundError:\n",
        "    print(\"HDF file not found (HEC-RAS not run yet)\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not read HDF: {e}\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Visualize Baseline Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "**\u2705 Baseline Analysis:**\n",
        "- Extracted existing breach results from Plan 02\n",
        "- Read baseline breach parameters\n",
        "- Visualized baseline behavior\n",
        "\n",
        "**\u2705 Scenario Creation:**\n",
        "- Cloned Plan 02 to create new scenarios\n",
        "- Modified breach parameters one at a time:\n",
        "  - Scenario 1: Increased breach width by 50%\n",
        "  - Scenario 2: Decreased formation time by 50%\n",
        "  - Scenario 3: Changed breach method\n",
        "\n",
        "**\u2705 Results Comparison:**\n",
        "- Extracted results from all scenarios (if HEC-RAS was run)\n",
        "- Compared flow hydrographs\n",
        "- Compared peak flows\n",
        "- Compared breach geometry evolution\n",
        "- Created comprehensive comparison dashboard\n",
        "\n",
        "**\u2705 Data Export:**\n",
        "- Exported time series for all scenarios\n",
        "- Exported comparison summary table\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Run HEC-RAS Simulations:**\n",
        "   - Open HEC-RAS\n",
        "   - Run plans 03, 04, and 05\n",
        "   - Re-run this notebook to extract and compare results\n",
        "\n",
        "2. **Additional Scenarios:**\n",
        "   - Create more scenarios by modifying other parameters\n",
        "   - Test combinations of parameters\n",
        "   - Explore full sensitivity range\n",
        "\n",
        "3. **Advanced Analysis:**\n",
        "   - Statistical analysis of parameter sensitivity\n",
        "   - Uncertainty quantification\n",
        "   - Flood impact assessment downstream\n",
        "\n",
        "### Key Functions Used:\n",
        "\n",
        "```python\n",
        "# HDF Results Extraction (use HdfResultsBreach and HdfStruc)\n",
        "HdfStruc.list_sa2d_connections(plan)           # List structures in HDF\n",
        "HdfStruc.get_sa2d_breach_info(plan)            # Get breach capability info\n",
        "HdfResultsBreach.get_breach_timeseries(plan, structure)  # Extract time series\n",
        "HdfResultsBreach.get_breach_summary(plan, structure)     # Extract summary stats\n",
        "\n",
        "# Plan File Parameter Management (use RasBreach)\n",
        "RasBreach.list_breach_structures_plan(plan)    # List structures in plan file\n",
        "RasBreach.read_breach_block(plan, structure)   # Read parameters\n",
        "RasBreach.update_breach_block(plan, structure, **params)  # Modify parameters\n",
        "```\n",
        "\n",
        "### Architectural Pattern:\n",
        "\n",
        "**ras-commander separates HDF and plain text operations:**\n",
        "- **RasBreach** \u2192 Breach PARAMETERS in plan files (.p##)\n",
        "- **HdfResultsBreach** \u2192 Breach RESULTS from HDF files (.p##.hdf)\n",
        "- **HdfStruc** \u2192 Structure listings and metadata from HDF\n",
        "\n",
        "Use plan file methods for parameter operations to ensure structure names match!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.737259Z",
          "iopub.status.busy": "2025-11-17T17:46:23.737124Z",
          "iopub.status.idle": "2025-11-17T17:46:23.840225Z",
          "shell.execute_reply": "2025-11-17T17:46:23.839751Z"
        }
      },
      "outputs": [],
      "source": [
        "if target_structure:\n",
        "    # Scenario definitions\n",
        "    scenario_plans = {\n",
        "        'Scenario 1: +50% Width': scenario_1_plan,\n",
        "        'Scenario 2: -50% Formation Time': scenario_2_plan,\n",
        "    }\n",
        "    \n",
        "    # Try to extract results for each scenario\n",
        "    for scenario_name, plan_num in scenario_plans.items():\n",
        "        try:\n",
        "            ts = HdfResultsBreach.get_breach_timeseries(plan_num, target_structure)\n",
        "            summary = HdfResultsBreach.get_breach_summary(plan_num, target_structure)\n",
        "            \n",
        "            if not ts.empty:\n",
        "                scenarios[scenario_name] = ts\n",
        "                summaries[scenario_name] = summary\n",
        "                print(f\"\u2713 Extracted: {scenario_name}\")\n",
        "            else:\n",
        "                print(f\"\u26a0 No results for: {scenario_name} (run HEC-RAS first)\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u26a0 Could not extract {scenario_name}: {e}\")\n",
        "    \n",
        "    print(f\"\\nTotal scenarios with results: {len(scenarios)}\")\n",
        "else:\n",
        "    print(\"Skipping scenario extraction - no breach structure available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Results Across All Scenarios\n",
        "\n",
        "Visualize all scenarios together to understand parameter sensitivity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Flow Hydrograph Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.842191Z",
          "iopub.status.busy": "2025-11-17T17:46:23.841892Z",
          "iopub.status.idle": "2025-11-17T17:46:23.990599Z",
          "shell.execute_reply": "2025-11-17T17:46:23.990022Z"
        }
      },
      "outputs": [],
      "source": [
        "if scenarios:\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    \n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "    linestyles = ['-', '--', '-.', ':', '-']\n",
        "    \n",
        "    for idx, (scenario_name, ts_data) in enumerate(scenarios.items()):\n",
        "        color = colors[idx % len(colors)]\n",
        "        linestyle = linestyles[idx % len(linestyles)]\n",
        "        \n",
        "        ax.plot(ts_data['datetime'], ts_data['total_flow'],\n",
        "               label=scenario_name, color=color, linestyle=linestyle, linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Time', fontsize=12)\n",
        "    ax.set_ylabel('Total Flow (cfs)', fontsize=12)\n",
        "    ax.set_title(f'{target_structure} - Flow Hydrograph Comparison', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.legend(loc='best', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No scenario data available for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Peak Flow Comparison (Bar Chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:23.992637Z",
          "iopub.status.busy": "2025-11-17T17:46:23.992366Z",
          "iopub.status.idle": "2025-11-17T17:46:24.078621Z",
          "shell.execute_reply": "2025-11-17T17:46:24.078072Z"
        }
      },
      "outputs": [],
      "source": [
        "if summaries:\n",
        "    # Extract peak flows\n",
        "    scenario_names = list(summaries.keys())\n",
        "    peak_flows = [summaries[name].iloc[0]['max_total_flow'] \n",
        "                 for name in scenario_names]\n",
        "    \n",
        "    # Create bar chart\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bars = ax.bar(range(len(scenario_names)), peak_flows, \n",
        "                  color=['blue', 'red', 'green', 'orange', 'purple'][:len(scenario_names)])\n",
        "    \n",
        "    ax.set_xticks(range(len(scenario_names)))\n",
        "    ax.set_xticklabels(scenario_names, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Peak Flow (cfs)', fontsize=12)\n",
        "    ax.set_title(f'{target_structure} - Peak Flow Comparison', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, peak_flows):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{value:.0f}',\n",
        "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print percent differences from baseline\n",
        "    if len(peak_flows) > 1:\n",
        "        baseline_flow = peak_flows[0]\n",
        "        print(\"\\nPeak Flow Differences from Baseline:\")\n",
        "        print(\"=\" * 60)\n",
        "        for i, (name, flow) in enumerate(zip(scenario_names, peak_flows)):\n",
        "            if i == 0:\n",
        "                print(f\"{name}: {flow:.0f} cfs (baseline)\")\n",
        "            else:\n",
        "                diff_pct = ((flow - baseline_flow) / baseline_flow) * 100\n",
        "                print(f\"{name}: {flow:.0f} cfs ({diff_pct:+.1f}%)\")\n",
        "else:\n",
        "    print(\"No summary data available for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Breach Width Evolution Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:24.080940Z",
          "iopub.status.busy": "2025-11-17T17:46:24.080762Z",
          "iopub.status.idle": "2025-11-17T17:46:24.206707Z",
          "shell.execute_reply": "2025-11-17T17:46:24.206056Z"
        }
      },
      "outputs": [],
      "source": [
        "if scenarios:\n",
        "    # Check if any scenario has breach width data\n",
        "    has_width_data = any(ts['bottom_width'].notna().any() for ts in scenarios.values())\n",
        "    \n",
        "    if has_width_data:\n",
        "        fig, ax = plt.subplots(figsize=(14, 6))\n",
        "        \n",
        "        for idx, (scenario_name, ts_data) in enumerate(scenarios.items()):\n",
        "            if ts_data['bottom_width'].notna().any():\n",
        "                color = colors[idx % len(colors)]\n",
        "                linestyle = linestyles[idx % len(linestyles)]\n",
        "                \n",
        "                ax.plot(ts_data['datetime'], ts_data['bottom_width'],\n",
        "                       label=scenario_name, color=color, linestyle=linestyle, \n",
        "                       linewidth=2, marker='o', markersize=4)\n",
        "        \n",
        "        ax.set_xlabel('Time', fontsize=12)\n",
        "        ax.set_ylabel('Breach Width (ft)', fontsize=12)\n",
        "        ax.set_title(f'{target_structure} - Breach Width Evolution Comparison', \n",
        "                    fontsize=14, fontweight='bold')\n",
        "        ax.legend(loc='best', fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No breach width data available (breach may not have formed)\")\n",
        "else:\n",
        "    print(\"No scenario data available for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Summary Table Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:24.208965Z",
          "iopub.status.busy": "2025-11-17T17:46:24.208784Z",
          "iopub.status.idle": "2025-11-17T17:46:24.219501Z",
          "shell.execute_reply": "2025-11-17T17:46:24.218896Z"
        }
      },
      "outputs": [],
      "source": [
        "if summaries:\n",
        "    # Combine all summaries into a comparison table\n",
        "    comparison_data = []\n",
        "    for scenario_name, summary_df in summaries.items():\n",
        "        row = summary_df.iloc[0].to_dict()\n",
        "        row['Scenario'] = scenario_name\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Select key columns for display\n",
        "    display_cols = ['Scenario', 'max_total_flow', 'max_breach_flow', \n",
        "                   'final_breach_width', 'final_breach_depth', \n",
        "                   'max_hw', 'max_tw']\n",
        "    \n",
        "    # Filter to available columns\n",
        "    display_cols = [col for col in display_cols if col in comparison_df.columns]\n",
        "    \n",
        "    print(\"\\nScenario Comparison Summary:\")\n",
        "    print(\"=\" * 100)\n",
        "    print(comparison_df[display_cols].to_string(index=False))\n",
        "    \n",
        "    # Export to CSV\n",
        "    output_file = project_path / \"breach_scenario_comparison.csv\"\n",
        "    comparison_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nComparison table exported to: {output_file}\")\n",
        "else:\n",
        "    print(\"No summary data available for comparison table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Comprehensive Multi-Panel Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:24.221528Z",
          "iopub.status.busy": "2025-11-17T17:46:24.221370Z",
          "iopub.status.idle": "2025-11-17T17:46:24.628102Z",
          "shell.execute_reply": "2025-11-17T17:46:24.627569Z"
        }
      },
      "outputs": [],
      "source": [
        "if scenarios and len(scenarios) > 1:\n",
        "    # Create 2x2 subplot grid\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Plot 1: Flow comparison\n",
        "    for idx, (scenario_name, ts_data) in enumerate(scenarios.items()):\n",
        "        color = colors[idx % len(colors)]\n",
        "        axes[0, 0].plot(ts_data['datetime'], ts_data['total_flow'],\n",
        "                       label=scenario_name, color=color, linewidth=2)\n",
        "    axes[0, 0].set_ylabel('Total Flow (cfs)', fontsize=11)\n",
        "    axes[0, 0].set_title('Flow Hydrographs', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].legend(fontsize=8)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Peak flows bar chart\n",
        "    scenario_names = list(summaries.keys())\n",
        "    peak_flows = [summaries[name].iloc[0]['max_total_flow'] for name in scenario_names]\n",
        "    bars = axes[0, 1].bar(range(len(scenario_names)), peak_flows,\n",
        "                          color=colors[:len(scenario_names)])\n",
        "    axes[0, 1].set_xticks(range(len(scenario_names)))\n",
        "    axes[0, 1].set_xticklabels(range(len(scenario_names)))\n",
        "    axes[0, 1].set_ylabel('Peak Flow (cfs)', fontsize=11)\n",
        "    axes[0, 1].set_title('Peak Flow Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].grid(True, axis='y', alpha=0.3)\n",
        "    \n",
        "    # Plot 3: HW/TW for baseline\n",
        "    baseline_ts = scenarios[list(scenarios.keys())[0]]\n",
        "    axes[1, 0].plot(baseline_ts['datetime'], baseline_ts['hw'], \n",
        "                   label='HW', color='blue', linewidth=2)\n",
        "    axes[1, 0].plot(baseline_ts['datetime'], baseline_ts['tw'], \n",
        "                   label='TW', color='red', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Time', fontsize=11)\n",
        "    axes[1, 0].set_ylabel('Elevation (ft)', fontsize=11)\n",
        "    axes[1, 0].set_title('Baseline Water Levels', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Breach width comparison (if available)\n",
        "    has_width = False\n",
        "    for idx, (scenario_name, ts_data) in enumerate(scenarios.items()):\n",
        "        if ts_data['bottom_width'].notna().any():\n",
        "            color = colors[idx % len(colors)]\n",
        "            axes[1, 1].plot(ts_data['datetime'], ts_data['bottom_width'],\n",
        "                           label=scenario_name, color=color, linewidth=2, marker='o')\n",
        "            has_width = True\n",
        "    \n",
        "    if has_width:\n",
        "        axes[1, 1].set_xlabel('Time', fontsize=11)\n",
        "        axes[1, 1].set_ylabel('Breach Width (ft)', fontsize=11)\n",
        "        axes[1, 1].set_title('Breach Width Evolution', fontsize=12, fontweight='bold')\n",
        "        axes[1, 1].legend(fontsize=8)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'No Breach Width Data',\n",
        "                       ha='center', va='center', fontsize=14,\n",
        "                       transform=axes[1, 1].transAxes)\n",
        "    \n",
        "    fig.suptitle(f'{target_structure} - Breach Scenario Analysis Dashboard',\n",
        "                fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Need at least 2 scenarios for comprehensive comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Export All Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:24.630278Z",
          "iopub.status.busy": "2025-11-17T17:46:24.630122Z",
          "iopub.status.idle": "2025-11-17T17:46:24.640652Z",
          "shell.execute_reply": "2025-11-17T17:46:24.640047Z"
        }
      },
      "outputs": [],
      "source": [
        "if scenarios:\n",
        "    # Export each scenario's time series\n",
        "    for scenario_name, ts_data in scenarios.items():\n",
        "        # Create safe filename\n",
        "        safe_name = scenario_name.replace(' ', '_').replace(':', '').replace('+', 'plus')\n",
        "        filename = project_path / f\"breach_{safe_name}.csv\"\n",
        "        ts_data.to_csv(filename, index=False)\n",
        "        print(f\"Exported: {filename.name}\")\n",
        "    \n",
        "    print(f\"\\nAll scenario data exported to: {project_path}\")\n",
        "else:\n",
        "    print(\"No scenario data to export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "**\u2705 Baseline Analysis:**\n",
        "- Extracted existing breach results from HDF\n",
        "- Read baseline breach parameters from plan file\n",
        "- Visualized baseline behavior\n",
        "\n",
        "**\u2705 Scenario Creation:**\n",
        "- Cloned plans to create new scenarios\n",
        "- Modified breach parameters one at a time:\n",
        "  - Scenario 1: Increased breach width by 50%\n",
        "  - Scenario 2: Decreased formation time by 50%\n",
        "\n",
        "**\u2705 Results Comparison:**\n",
        "- Extracted results from all scenarios\n",
        "- Compared flow hydrographs\n",
        "- Compared peak flows\n",
        "- Compared breach geometry evolution\n",
        "- Created comprehensive comparison dashboard\n",
        "\n",
        "**\u2705 Data Export:**\n",
        "- Exported time series for all scenarios\n",
        "- Exported comparison summary table\n",
        "\n",
        "### Key Functions Used:\n",
        "\n",
        "```python\n",
        "# HDF Results Extraction (use HdfResultsBreach and HdfStruc)\n",
        "HdfStruc.list_sa2d_connections(plan)              # List structures in HDF\n",
        "HdfStruc.get_sa2d_breach_info(plan)               # Get breach capability info\n",
        "HdfResultsBreach.get_breach_timeseries(plan, structure)   # Extract time series\n",
        "HdfResultsBreach.get_breach_summary(plan, structure)      # Extract summary stats\n",
        "HdfResultsBreach.get_breaching_variables(plan, structure) # Breach geometry evolution\n",
        "HdfResultsBreach.get_structure_variables(plan, structure) # Structure flow variables\n",
        "\n",
        "# Plan File Parameter Management (use RasBreach)\n",
        "RasBreach.list_breach_structures_plan(plan)      # List structures in plan file\n",
        "RasBreach.read_breach_block(plan, structure)     # Read parameters\n",
        "RasBreach.update_breach_block(plan, structure, geom_values=[...])  # Modify parameters\n",
        "```\n",
        "\n",
        "### Architectural Pattern:\n",
        "\n",
        "**ras-commander separates HDF and plain text operations:**\n",
        "- **RasBreach** \u2192 Breach PARAMETERS in plan files (.p##)\n",
        "- **HdfResultsBreach** \u2192 Breach RESULTS from HDF files (.p##.hdf)\n",
        "- **HdfStruc** \u2192 Structure listings and metadata from HDF\n",
        "\n",
        "**Important:** Use plan file methods for parameter operations to ensure structure names match!\n",
        "\n",
        "### Breach Geom Field Structure:\n",
        "```python\n",
        "# Breach Geom CSV format (10 fields):\n",
        "[0] Centerline/Station     # ft\n",
        "[1] Initial Bottom Width   # ft\n",
        "[2] Final Bottom Elevation # ft  <-- Example: change this to 605\n",
        "[3] Left Side Slope        # H:V ratio\n",
        "[4] Right Side Slope       # H:V ratio\n",
        "[5] Active Flag            # True/False\n",
        "[6] Weir Coefficient       # dimensionless\n",
        "[7] Top Elevation          # ft\n",
        "[8] Formation Method       # 1=Time, 2=Trigger\n",
        "[9] Formation Time/Threshold # hrs or ft\n",
        "\n",
        "# Example: Update Final Bottom Elevation\n",
        "new_geom = baseline_geom.copy()\n",
        "new_geom[2] = 605  # Set to 605 ft\n",
        "RasBreach.update_breach_block(\"template_plan\", \"Dam\", geom_values=new_geom)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\19_steady_flow_analysis.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS Steady State Flow Analysis\n",
        "\n",
        "This notebook demonstrates how to extract and analyze steady state flow results from HEC-RAS using the ras-commander library. It showcases the new steady state functionality in `HdfResultsPlan`.\n",
        "\n",
        "## New Steady State Methods\n",
        "\n",
        "The library now includes full support for steady state analysis:\n",
        "- `is_steady_plan()` - Check if HDF contains steady state results\n",
        "- `get_steady_profile_names()` - Extract steady state profile names\n",
        "- `get_steady_wse()` - Extract water surface elevations for profiles\n",
        "- `get_steady_info()` - Extract steady flow metadata and attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Steady Flow Computation Messages\n",
        "\n",
        "For steady flow analyses, computation messages provide valuable information about:\n",
        "- Hydraulic computations and convergence\n",
        "- Warning messages for critical flow or other conditions\n",
        "- Computation timing and performance\n",
        "\n",
        "We can extract these using `HdfResultsPlan.get_compute_messages()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n",
            "2025-12-02 21:07:36 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        }
      ],
      "source": [
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_number = \"01\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:07:36 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "data": {
            "text/plain": "['<ras_commander.RasPrj.RasPrj at 0x1f6cf0a1400>']"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "init_ras_project(bald_eagle_path, \"6.6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['  plan_number unsteady_number geometry_number                     Plan Title  \\\\\\n', '0          01              02              01  Unsteady with Bridges and Dam   \\n', '1          02            None              01                Steady Flow Run   \\n', '\\n', '  Program Version Short Identifier                  Simulation Date  \\\\\\n', '0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \\n', '1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \\n', '\\n', '  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\\\\n', '0                 2MIN            1HOUR        1  ...     None      dss   \\n', '1                 2MIN              NaN        1  ...     None      dss   \\n', '\\n', '  Friction Slope Method HDF_Results_Path Geom File  \\\\\\n', '0                     2             None        01   \\n', '1                     1             None        01   \\n', '\\n', '                                           Geom Path  Flow File  \\\\\\n', '0  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...         02   \\n', '1  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...         02   \\n', '\\n', '                                           Flow Path  \\\\\\n', '0  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '1  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...   \\n', '\\n', '                                           full_path flow_type  \\n', '0  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  Unsteady  \\n', '1  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...    Steady  \\n', '\\n', '[2 rows x 27 columns]']"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:07:36 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "2025-12-02 21:07:36 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 21:07:36 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01\"\n",
            "2025-12-02 21:09:02 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-12-02 21:09:02 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 86.56 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": "['True']"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RasCmdr.compute_plan(plan_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Steady Flow Computation Messages\n",
        "\n",
        "For steady flow analyses, computation messages provide valuable information about:\n",
        "- Hydraulic computations and convergence\n",
        "- Warning messages for critical flow or other conditions\n",
        "- Computation timing and performance\n",
        "\n",
        "We can extract these using `HdfResultsPlan.get_compute_messages()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:02 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01.hdf\n",
            "2025-12-02 21:09:02 - ras_commander.hdf.HdfResultsPlan - INFO - Reading computation messages from HDF: BaldEagle.p01.hdf\n",
            "2025-12-02 21:09:02 - ras_commander.hdf.HdfResultsPlan - INFO - Successfully extracted 1693 characters from HDF\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEADY FLOW COMPUTATION MESSAGES\n",
            "================================================================================\n",
            "\n",
            "Extracted 1693 characters\n",
            "\n",
            "Computation messages:\n",
            "--------------------------------------------------------------------------------\n",
            "Plan: 'Unsteady with Bridges and Dam' (BaldEagle.p01)\n",
            "Simulation started at: 02Dec2025 09:07:37 PM\n",
            "\n",
            "Writing Plan GIS Data...\n",
            "Completed Writing Plan GIS Data\n",
            "Writing Geometry...\n",
            "Computing Bank Lines\n",
            "Bank lines generated in 108 ms\n",
            "Computing Edge Lines\n",
            "Edge Lines generated in 46 ms\n",
            "Computing XS Interpolation Surface\n",
            "XS Interpolation Surface generated in 109 ms\n",
            "Completed Writing Geometry\n",
            "Writing Event Conditions ...\n",
            "Completed Writing Event Condition Data\n",
            "\n",
            "\t\n",
            "Geometric Preprocessor HEC-RAS 6.6 September 2024\n",
            " \n",
            "\n",
            "Finished Processing Geometry\n",
            "\n",
            "\n",
            "Performing Unsteady Flow Simulation  HEC-RAS 6.6 September 2024\n",
            " \n",
            "\t\n",
            "Unsteady Input Summary:\n",
            "     1D Unsteady Finite Difference Numerical Solution\n",
            "\n",
            "Overall Volume Accounting Error in Acre Feet:    -29.5468461514\n",
            "Overall Volume Accounting Error as percentage:           0.01407\n",
            "Please review \"Computational Log File\" output for volume accounting details\n",
            "\n",
            "Writing Results to DSS\n",
            "\n",
            "Finished Unsteady Flow Simulation\n",
            "\n",
            "Reading U\n",
            "\n",
            "... (truncated for display) ...\n",
            "\n",
            "================================================================================\n",
            "Checking for critical flow or warnings...\n",
            "================================================================================\n",
            "\u2713 No critical flow or warning messages found\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Extract computation messages for steady flow analysis\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEADY FLOW COMPUTATION MESSAGES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extract messages (works with plan number or HDF path)\n",
        "steady_msgs = HdfResultsPlan.get_compute_messages(plan_number)\n",
        "\n",
        "if steady_msgs:\n",
        "    print(f\"\\nExtracted {len(steady_msgs)} characters\\n\")\n",
        "    \n",
        "    # Display messages\n",
        "    print(\"Computation messages:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(steady_msgs[:1000])  # First 1000 characters\n",
        "    \n",
        "    if len(steady_msgs) > 1000:\n",
        "        print(\"\\n... (truncated for display) ...\")\n",
        "    \n",
        "    # Look for critical information\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Checking for critical flow or warnings...\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    lines = steady_msgs.split('\\n')\n",
        "    critical = [l for l in lines if 'critical' in l.lower() or 'warning' in l.lower()]\n",
        "    \n",
        "    if critical:\n",
        "        print(f\"Found {len(critical)} lines with critical flow or warnings:\")\n",
        "        for line in critical[:10]:\n",
        "            print(f\"  - {line.strip()}\")\n",
        "    else:\n",
        "        print(\"\u2713 No critical flow or warning messages found\")\n",
        "else:\n",
        "    print(\"No computation messages available\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "# !pip install --upgrade ras-commander\n",
        "\n",
        "# Set to False to disable plot generation for llm-friendly outputs\n",
        "generate_plots = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enable this cell for local development version of ras-commander\n",
        "import os\n",
        "import sys      \n",
        "from pathlib import Path\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "from ras_commander import *\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Set pandas display options\n",
        "pd.set_option('display.max_rows', 10)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Bald Eagle Creek Example Project\n",
        "\n",
        "This project contains both unsteady (Plan 01) and **steady state** (Plan 02) flow analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:02 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bald Eagle Creek project already exists\n",
            "Initialized project: BaldEagle\n"
          ]
        }
      ],
      "source": [
        "# Extract and initialize the Bald Eagle Creek project\n",
        "current_dir = Path.cwd()\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"Balde Eagle Creek\"\n",
        "\n",
        "# Extract project if needed\n",
        "if not bald_eagle_path.exists():\n",
        "    RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "    print(\"Extracted Bald Eagle Creek project\")\n",
        "else:\n",
        "    print(\"Bald Eagle Creek project already exists\")\n",
        "\n",
        "# Initialize the project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized project: {ras.project_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plans in this project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>Program Version</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>02</td>\\n', '      <td>5.00</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>None</td>\\n', '      <td>NaN</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['  plan_number                     Plan Title unsteady_number Program Version\\n', '0          01  Unsteady with Bridges and Dam              02            5.00\\n', '1          02                Steady Flow Run            None             NaN']"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View all plans in the project\n",
        "print(\"Plans in this project:\")\n",
        "ras.plan_df[['plan_number', 'Plan Title', 'unsteady_number', 'Program Version']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Steady State Plan (Plan 02)\n",
        "\n",
        "Execute the steady state plan if results don't already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:02 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
            "2025-12-02 21:09:02 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-12-02 21:09:02 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Plan 02 (Steady State)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 02\n",
            "2025-12-02 21:09:06 - ras_commander.RasCmdr - INFO - Total run time for plan 02: 3.72 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan 02 executed successfully\n"
          ]
        }
      ],
      "source": [
        "# Define plan number\n",
        "plan_number = \"02\"\n",
        "\n",
        "# Check if results exist\n",
        "plan02_hdf = bald_eagle_path / \"BaldEagle.p02.hdf\"\n",
        "\n",
        "if not plan02_hdf.exists():\n",
        "    print(f\"Running Plan {plan_number} (Steady State)...\")\n",
        "    success = RasCmdr.compute_plan(plan_number)\n",
        "    if success:\n",
        "        print(f\"Plan {plan_number} executed successfully\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed\")\n",
        "else:\n",
        "    print(f\"Plan {plan_number} results already exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check if Plan Contains Steady State Results\n",
        "\n",
        "Use `is_steady_plan()` to verify the HDF contains steady state results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is Plan 02 a steady state plan? True\n"
          ]
        }
      ],
      "source": [
        "# Check if this is a steady state plan\n",
        "is_steady = HdfResultsPlan.is_steady_plan(plan_number)\n",
        "print(f\"Is Plan {plan_number} a steady state plan? {is_steady}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract Steady State Profile Names\n",
        "\n",
        "Get the list of all steady state profiles (e.g., different return periods)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02.hdf\n",
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Found 8 steady state profiles: ['.5 year', '1 year', '2 year', '5 year', '10 year', '25 year', '50 year', '100 year']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 steady state profiles:\n",
            "  1. .5 year\n",
            "  2. 1 year\n",
            "  3. 2 year\n",
            "  4. 5 year\n",
            "  5. 10 year\n",
            "  6. 25 year\n",
            "  7. 50 year\n",
            "  8. 100 year\n"
          ]
        }
      ],
      "source": [
        "# Get profile names\n",
        "profiles = HdfResultsPlan.get_steady_profile_names(plan_number)\n",
        "\n",
        "print(f\"Found {len(profiles)} steady state profiles:\")\n",
        "for i, profile in enumerate(profiles, 1):\n",
        "    print(f\"  {i}. {profile}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract Water Surface Elevations (WSE)\n",
        "\n",
        "Extract WSE data for specific profiles or all profiles at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3a. Extract Single Profile by Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02.hdf\n",
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Extracted WSE data for 1 profile(s), 178 cross sections\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WSE Data for 100-year profile:\n",
            "Shape: (178, 4)\n",
            "Columns: ['River', 'Reach', 'Station', 'WSE']\n",
            "\n",
            "First 5 cross sections:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>River</th>\\n', '      <th>Reach</th>\\n', '      <th>Station</th>\\n', '      <th>WSE</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>138154.4</td>\\n', '      <td>669.521484</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>137690.8</td>\\n', '      <td>669.346863</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>137327.0</td>\\n', '      <td>668.883057</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>136564.9</td>\\n', '      <td>666.177979</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>136202.3</td>\\n', '      <td>666.057739</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['        River    Reach   Station         WSE\\n', '0  Bald Eagle  Loc Hav  138154.4  669.521484\\n', '1  Bald Eagle  Loc Hav  137690.8  669.346863\\n', '2  Bald Eagle  Loc Hav  137327.0  668.883057\\n', '3  Bald Eagle  Loc Hav  136564.9  666.177979\\n', '4  Bald Eagle  Loc Hav  136202.3  666.057739']"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract WSE for 100-year profile\n",
        "wse_100yr = HdfResultsPlan.get_steady_wse(plan_number, profile_name='100 year')\n",
        "\n",
        "print(f\"WSE Data for 100-year profile:\")\n",
        "print(f\"Shape: {wse_100yr.shape}\")\n",
        "print(f\"Columns: {list(wse_100yr.columns)}\")\n",
        "print(\"\\nFirst 5 cross sections:\")\n",
        "wse_100yr.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3b. Extract Single Profile by Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02.hdf\n",
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Extracted WSE data for 1 profile(s), 178 cross sections\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WSE Data for .5 year profile:\n",
            "Shape: (178, 4)\n",
            "\n",
            "Summary statistics:\n"
          ]
        },
        {
          "data": {
            "text/plain": "['count    178.000000\\n', 'mean     590.215864\\n', 'std       39.202902\\n', 'min      537.500000\\n', '25%      557.109406\\n', '50%      579.655151\\n', '75%      623.142242\\n', 'max      660.588928\\n', 'Name: WSE, dtype: float64']"
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract WSE for first profile (0.5-year) using index\n",
        "wse_05yr = HdfResultsPlan.get_steady_wse(plan_number, profile_index=0)\n",
        "\n",
        "print(f\"WSE Data for {profiles[0]} profile:\")\n",
        "print(f\"Shape: {wse_05yr.shape}\")\n",
        "print(\"\\nSummary statistics:\")\n",
        "wse_05yr['WSE'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3c. Extract All Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02.hdf\n",
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Extracted WSE data for 8 profile(s), 178 cross sections\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WSE Data for all 8 profiles:\n",
            "Shape: (1424, 5)\n",
            "Columns: ['River', 'Reach', 'Station', 'Profile', 'WSE']\n",
            "\n",
            "Profiles included: ['.5 year', '1 year', '2 year', '5 year', '10 year', '25 year', '50 year', '100 year']\n",
            "\n",
            "Sample data:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>River</th>\\n', '      <th>Reach</th>\\n', '      <th>Station</th>\\n', '      <th>Profile</th>\\n', '      <th>WSE</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>138154.4</td>\\n', '      <td>.5 year</td>\\n', '      <td>660.588928</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>137690.8</td>\\n', '      <td>.5 year</td>\\n', '      <td>659.914612</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>137327.0</td>\\n', '      <td>.5 year</td>\\n', '      <td>659.465759</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>136564.9</td>\\n', '      <td>.5 year</td>\\n', '      <td>658.126160</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>Bald Eagle</td>\\n', '      <td>Loc Hav</td>\\n', '      <td>136202.3</td>\\n', '      <td>.5 year</td>\\n', '      <td>657.173157</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['        River    Reach   Station  Profile         WSE\\n', '0  Bald Eagle  Loc Hav  138154.4  .5 year  660.588928\\n', '1  Bald Eagle  Loc Hav  137690.8  .5 year  659.914612\\n', '2  Bald Eagle  Loc Hav  137327.0  .5 year  659.465759\\n', '3  Bald Eagle  Loc Hav  136564.9  .5 year  658.126160\\n', '4  Bald Eagle  Loc Hav  136202.3  .5 year  657.173157\\n', '5  Bald Eagle  Loc Hav  135591.4  .5 year  656.520264\\n', '6  Bald Eagle  Loc Hav  135068.7  .5 year  655.880676\\n', '7  Bald Eagle  Loc Hav  134487.2  .5 year  654.593140\\n', '8  Bald Eagle  Loc Hav  133881.0  .5 year  653.786621\\n', '9  Bald Eagle  Loc Hav  133446.1  .5 year  653.006958']"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract WSE for all profiles\n",
        "wse_all = HdfResultsPlan.get_steady_wse(plan_number)\n",
        "\n",
        "print(f\"WSE Data for all {len(profiles)} profiles:\")\n",
        "print(f\"Shape: {wse_all.shape}\")\n",
        "print(f\"Columns: {list(wse_all.columns)}\")\n",
        "print(f\"\\nProfiles included: {wse_all['Profile'].unique().tolist()}\")\n",
        "print(\"\\nSample data:\")\n",
        "wse_all.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Steady Flow Metadata\n",
        "\n",
        "Get plan information, program version, solution status, and flow file details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02.hdf\n",
            "2025-12-02 21:09:06 - ras_commander.hdf.HdfResultsPlan - INFO - Extracted 8 steady state attributes\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steady Flow Information (8 attributes):\n",
            "\n",
            "Key attributes:\n",
            "  Program Version: HEC-RAS 6.6 September 2024\n",
            "  Solution: Steady Finished Successfully\n",
            "  Flow Title: Steady Flow Data\n",
            "  Flow Filename: BaldEagle.f02\n",
            "\n",
            "All attributes:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>0</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>Program Name</th>\\n', '      <td>HEC-RAS - River Analysis System</td>\\n', '    </tr><tr>\\n', '      <th>Program Version</th>\\n', '      <td>HEC-RAS 6.6 September 2024</td>\\n', '    </tr><tr>\\n', '      <th>Project File Name</th>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>Type of Run</th>\\n', '      <td>Steady Flow Analysis</td>\\n', '    </tr><tr>\\n', '      <th>Run Time Window</th>\\n', '      <td>02DEC2025 21:09:05 to 02DEC2025 21:09:06</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['                                                                   0\\n', 'Program Name                         HEC-RAS - River Analysis System\\n', 'Program Version                           HEC-RAS 6.6 September 2024\\n', 'Project File Name  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...\\n', 'Type of Run                                     Steady Flow Analysis\\n', 'Run Time Window             02DEC2025 21:09:05 to 02DEC2025 21:09:06\\n', 'Solution                                Steady Finished Successfully\\n', 'Flow Filename                                          BaldEagle.f02\\n', 'Flow Title                                          Steady Flow Data']"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get steady flow information\n",
        "steady_info = HdfResultsPlan.get_steady_info(plan_number)\n",
        "\n",
        "print(f\"Steady Flow Information ({len(steady_info.columns)} attributes):\")\n",
        "print(\"\\nKey attributes:\")\n",
        "for col in ['Program Version', 'Solution', 'Flow Title', 'Flow Filename']:\n",
        "    if col in steady_info.columns:\n",
        "        print(f\"  {col}: {steady_info[col].values[0]}\")\n",
        "\n",
        "print(\"\\nAll attributes:\")\n",
        "steady_info.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Water Surface Profiles\n",
        "\n",
        "Plot WSE vs. station for different return periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Profile Comparison (Maximum WSE):\n",
            "  .5 year   : 660.59 ft\n",
            "  1 year    : 661.43 ft\n",
            "  2 year    : 662.60 ft\n",
            "  5 year    : 664.67 ft\n",
            "  10 year   : 666.19 ft\n",
            "  25 year   : 667.46 ft\n",
            "  50 year   : 668.54 ft\n",
            "  100 year  : 669.52 ft\n"
          ]
        }
      ],
      "source": [
        "if generate_plots:\n",
        "    # Create a plot comparing all profiles\n",
        "    fig, ax = plt.subplots(figsize=(15, 8))\n",
        "    \n",
        "    # Plot each profile\n",
        "    for profile in profiles:\n",
        "        profile_data = wse_all[wse_all['Profile'] == profile]\n",
        "        # Convert station to numeric for plotting\n",
        "        stations = pd.to_numeric(profile_data['Station'], errors='coerce')\n",
        "        ax.plot(stations, profile_data['WSE'], label=profile, linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('River Station (ft)', fontsize=12)\n",
        "    ax.set_ylabel('Water Surface Elevation (ft)', fontsize=12)\n",
        "    ax.set_title('Steady State Water Surface Profiles\\nBald Eagle Creek', fontsize=14)\n",
        "    ax.legend(title='Return Period', loc='best', fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Invert X axis so upstream (higher stations) is on the left\n",
        "    ax.invert_xaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print profile comparison stats\n",
        "    print(\"\\nProfile Comparison (Maximum WSE):\")\n",
        "    for profile in profiles:\n",
        "        max_wse = wse_all[wse_all['Profile'] == profile]['WSE'].max()\n",
        "        print(f\"  {profile:10s}: {max_wse:.2f} ft\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze WSE Differences Between Profiles\n",
        "\n",
        "Compare water surface elevations between different return periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Water Surface Elevations by Profile and Station:\n",
            "\n",
            "First 10 stations:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th>Profile</th>\\n', '      <th>.5 year</th>\\n', '      <th>1 year</th>\\n', '      <th>10 year</th>\\n', '      <th>100 year</th>\\n', '      <th>2 year</th>\\n', '      <th>25 year</th>\\n', '      <th>5 year</th>\\n', '      <th>50 year</th>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>River</th>\\n', '      <th>Reach</th>\\n', '      <th>Station</th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th rowspan=\"10\" valign=\"top\">Bald Eagle</th>\\n', '      <th rowspan=\"10\" valign=\"top\">Loc Hav</th>\\n', '      <th>100657.3</th>\\n', '      <td>620.031982</td>\\n', '      <td>657.029053</td>\\n', '      <td>660.006348</td>\\n', '      <td>662.170227</td>\\n', '      <td>657.916138</td>\\n', '      <td>660.793945</td>\\n', '      <td>659.094055</td>\\n', '      <td>661.508545</td>\\n', '    </tr><tr>\\n', '      <th>101440.3</th>\\n', '      <td>620.033875</td>\\n', '      <td>657.029053</td>\\n', '      <td>660.006470</td>\\n', '      <td>662.170593</td>\\n', '      <td>657.916138</td>\\n', '      <td>660.794067</td>\\n', '      <td>659.094116</td>\\n', '      <td>661.508850</td>\\n', '    </tr><tr>\\n', '      <th>10221.14</th>\\n', '      <td>540.174683</td>\\n', '      <td>541.939697</td>\\n', '      <td>556.019287</td>\\n', '      <td>563.945190</td>\\n', '      <td>544.468933</td>\\n', '      <td>559.538452</td>\\n', '      <td>550.165833</td>\\n', '      <td>561.924500</td>\\n', '    </tr><tr>\\n', '      <th>103122.3</th>\\n', '      <td>620.044861</td>\\n', '      <td>657.029053</td>\\n', '      <td>660.006836</td>\\n', '      <td>662.171753</td>\\n', '      <td>657.916138</td>\\n', '      <td>660.794678</td>\\n', '      <td>659.094299</td>\\n', '      <td>661.509705</td>\\n', '    </tr><tr>\\n', '      <th>103369.7</th>\\n', '      <td>620.055420</td>\\n', '      <td>657.029480</td>\\n', '      <td>660.012878</td>\\n', '      <td>662.186707</td>\\n', '      <td>657.917175</td>\\n', '      <td>660.803711</td>\\n', '      <td>659.097473</td>\\n', '      <td>661.521729</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['Profile                         .5 year      1 year     10 year    100 year  \\\\\\n', 'River      Reach   Station                                                    \\n', 'Bald Eagle Loc Hav 100657.3  620.031982  657.029053  660.006348  662.170227   \\n', '                   101440.3  620.033875  657.029053  660.006470  662.170593   \\n', '                   10221.14  540.174683  541.939697  556.019287  563.945190   \\n', '                   103122.3  620.044861  657.029053  660.006836  662.171753   \\n', '                   103369.7  620.055420  657.029480  660.012878  662.186707   \\n', '                   103854.0  620.059082  657.029480  660.012939  662.187012   \\n', '                   104195.0  620.059875  657.029480  660.012878  662.186829   \\n', '                   104647.2  620.066528  657.029480  660.013000  662.187378   \\n', '                   105178.6  620.075806  657.029480  660.013367  662.188599   \\n', '                   106466.0  620.093262  657.029419  660.013428  662.1887\n...\n[Output truncated, 2009 characters total]"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a pivot table for easy comparison\n",
        "wse_pivot = wse_all.pivot_table(\n",
        "    index=['River', 'Reach', 'Station'],\n",
        "    columns='Profile',\n",
        "    values='WSE'\n",
        ")\n",
        "\n",
        "print(\"Water Surface Elevations by Profile and Station:\")\n",
        "print(\"\\nFirst 10 stations:\")\n",
        "wse_pivot.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Difference between 100-year and 0.5-year profiles:\n",
            "  Maximum difference: 42.14 ft\n",
            "  Minimum difference: 7.90 ft\n",
            "  Average difference: 21.62 ft\n",
            "\n",
            "Stations with largest differences:\n",
            "Profile                        100 year     .5 year  Diff_100yr_vs_05yr\n",
            "River      Reach   Station                                             \n",
            "Bald Eagle Loc Hav 96370.43  662.169006  620.027832           42.141174\n",
            "                   94560.01  662.168457  620.027466           42.140991\n",
            "                   93391.71  662.167664  620.027283           42.140381\n",
            "                   97607.35  662.168640  620.028320           42.140320\n",
            "                   98206.87  662.169983  620.029846           42.140137\n"
          ]
        }
      ],
      "source": [
        "# Calculate differences between profiles\n",
        "if '100 year' in wse_pivot.columns and '.5 year' in wse_pivot.columns:\n",
        "    wse_pivot['Diff_100yr_vs_05yr'] = wse_pivot['100 year'] - wse_pivot['.5 year']\n",
        "    \n",
        "    print(\"\\nDifference between 100-year and 0.5-year profiles:\")\n",
        "    print(f\"  Maximum difference: {wse_pivot['Diff_100yr_vs_05yr'].max():.2f} ft\")\n",
        "    print(f\"  Minimum difference: {wse_pivot['Diff_100yr_vs_05yr'].min():.2f} ft\")\n",
        "    print(f\"  Average difference: {wse_pivot['Diff_100yr_vs_05yr'].mean():.2f} ft\")\n",
        "    \n",
        "    print(\"\\nStations with largest differences:\")\n",
        "    top_diff = wse_pivot.nlargest(5, 'Diff_100yr_vs_05yr')[['100 year', '.5 year', 'Diff_100yr_vs_05yr']]\n",
        "    print(top_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the new steady state functionality in ras-commander:\n",
        "\n",
        "1. \u2705 Checked if a plan contains steady state results\n",
        "2. \u2705 Extracted profile names for different return periods\n",
        "3. \u2705 Retrieved WSE data for individual and all profiles\n",
        "4. \u2705 Accessed steady flow metadata and attributes\n",
        "5. \u2705 Visualized water surface profiles\n",
        "6. \u2705 Analyzed differences between profiles\n",
        "\n",
        "These tools enable comprehensive steady state flow analysis and comparison of hydraulic conditions across different design storms or flow scenarios."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\20_plaintext_geometry_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:15.924893Z",
          "iopub.status.busy": "2025-11-17T17:43:15.924374Z",
          "iopub.status.idle": "2025-11-17T17:43:18.494495Z",
          "shell.execute_reply": "2025-11-17T17:43:18.493987Z"
        }
      },
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Guide to HEC-RAS Geometry Operations\n",
        "\n",
        "This notebook demonstrates comprehensive geometry parsing and manipulation capabilities using the RasGeometry and HdfHydraulicTables classes. We'll explore how to extract, analyze, and modify various geometry types from HEC-RAS files.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Cross Section Operations**: Extract and modify station/elevation data\n",
        "2. **Hydraulic Property Tables (HTAB)**: Generate rating curves from preprocessed geometry\n",
        "3. **Storage Areas**: Extract elevation-volume curves for reservoirs and detention basins\n",
        "4. **Lateral Structures**: Analyze overflow weirs and lateral connections\n",
        "5. **SA/2D Connections**: Extract dam breach connections, weir profiles, and gates\n",
        "6. **Batch Processing**: Automate operations across multiple geometry elements\n",
        "\n",
        "These capabilities enable:\n",
        "- Automated geometry modification for sensitivity studies\n",
        "- Rating curve generation without re-running HEC-RAS\n",
        "- Dam breach and detention basin analysis\n",
        "- Model setup validation and QA/QC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.497163Z",
          "iopub.status.busy": "2025-11-17T17:43:18.496747Z",
          "iopub.status.idle": "2025-11-17T17:43:18.501218Z",
          "shell.execute_reply": "2025-11-17T17:43:18.500710Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "# Add ras-commander to path (for local development)\n",
        "current_file = Path.cwd()\n",
        "ras_commander_root = current_file.parent\n",
        "sys.path.insert(0, str(ras_commander_root))\n",
        "\n",
        "# Import RAS Commander geometry modules\n",
        "from ras_commander import (\n",
        "    RasGeometry,\n",
        "    RasGeometryUtils,\n",
        "    HdfHydraulicTables,\n",
        "    RasExamples,\n",
        "    init_ras_project,\n",
        "    RasCmdr,\n",
        "    ras\n",
        ")\n",
        "\n",
        "print(\"\u2713 Packages loaded successfully!\")\n",
        "print(f\"Working directory: {Path.cwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Cross Section Operations\n",
        "\n",
        "Cross sections define the channel geometry in 1D models. We'll explore how to list cross sections, extract their station/elevation data, and modify them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.503421Z",
          "iopub.status.busy": "2025-11-17T17:43:18.503158Z",
          "iopub.status.idle": "2025-11-17T17:43:18.607554Z",
          "shell.execute_reply": "2025-11-17T17:43:18.606901Z"
        }
      },
      "outputs": [],
      "source": [
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(bald_eagle_path)\n",
        "init_ras_project(bald_eagle_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.610230Z",
          "iopub.status.busy": "2025-11-17T17:43:18.609921Z",
          "iopub.status.idle": "2025-11-17T17:43:18.622596Z",
          "shell.execute_reply": "2025-11-17T17:43:18.622128Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.625332Z",
          "iopub.status.busy": "2025-11-17T17:43:18.625053Z",
          "iopub.status.idle": "2025-11-17T17:43:18.631811Z",
          "shell.execute_reply": "2025-11-17T17:43:18.631242Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Setup: Define Geometry File Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.634411Z",
          "iopub.status.busy": "2025-11-17T17:43:18.633914Z",
          "iopub.status.idle": "2025-11-17T17:43:18.638831Z",
          "shell.execute_reply": "2025-11-17T17:43:18.638391Z"
        }
      },
      "outputs": [],
      "source": [
        "# Lookup geometry file path and HDF path from ras.geom_df by geom_number\n",
        "geom_number = \"01\"\n",
        "geom_row = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number].iloc[0]\n",
        "\n",
        "geom_file = Path(geom_row[\"full_path\"])\n",
        "geom_hdf = Path(geom_row[\"hdf_path\"])\n",
        "\n",
        "print(f\"Geometry file: {geom_file}\")\n",
        "print(f\"File exists: {geom_file.exists()}\")\n",
        "if geom_file.exists():\n",
        "    print(f\"File size: {geom_file.stat().st_size / 1024:.1f} KB\")\n",
        "else:\n",
        "    print(\"Geometry file does not exist!\")\n",
        "print(f\"HDF file: {geom_hdf}\")\n",
        "print(f\"HDF exists: {geom_hdf.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 List All Cross Sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.641447Z",
          "iopub.status.busy": "2025-11-17T17:43:18.641092Z",
          "iopub.status.idle": "2025-11-17T17:43:18.669169Z",
          "shell.execute_reply": "2025-11-17T17:43:18.668693Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract all cross sections\n",
        "xs_df = RasGeometry.get_cross_sections(geom_file)\n",
        "\n",
        "print(f\"Total cross sections: {len(xs_df)}\")\n",
        "print(f\"Rivers: {xs_df['River'].unique().tolist()}\")\n",
        "print(f\"Reaches: {xs_df['Reach'].unique().tolist()}\")\n",
        "\n",
        "print(\"\\nFirst 10 cross sections:\")\n",
        "display.display(xs_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Extract Station/Elevation for a Cross Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.671323Z",
          "iopub.status.busy": "2025-11-17T17:43:18.671106Z",
          "iopub.status.idle": "2025-11-17T17:43:18.682170Z",
          "shell.execute_reply": "2025-11-17T17:43:18.681713Z"
        }
      },
      "outputs": [],
      "source": [
        "# Select first cross section\n",
        "first_xs = xs_df.iloc[0]\n",
        "river = first_xs['River']\n",
        "reach = first_xs['Reach']\n",
        "rs = first_xs['RS']\n",
        "\n",
        "print(f\"Selected cross section: {river} / {reach} / RS {rs}\")\n",
        "\n",
        "# Extract station/elevation data\n",
        "sta_elev = RasGeometry.get_station_elevation(geom_file, river, reach, rs)\n",
        "\n",
        "print(f\"\\nStation/Elevation Data:\")\n",
        "print(f\"  Points: {len(sta_elev)}\")\n",
        "print(f\"  Station range: {sta_elev['Station'].min():.2f} to {sta_elev['Station'].max():.2f} ft\")\n",
        "print(f\"  Elevation range: {sta_elev['Elevation'].min():.2f} to {sta_elev['Elevation'].max():.2f} ft\")\n",
        "print(f\"  Channel width: {sta_elev['Station'].max() - sta_elev['Station'].min():.2f} ft\")\n",
        "\n",
        "print(\"\\nFirst 10 points:\")\n",
        "display.display(sta_elev.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Visualize Cross Section Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.684431Z",
          "iopub.status.busy": "2025-11-17T17:43:18.684049Z",
          "iopub.status.idle": "2025-11-17T17:43:18.856972Z",
          "shell.execute_reply": "2025-11-17T17:43:18.856416Z"
        }
      },
      "outputs": [],
      "source": [
        "# Plot cross section profile\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "ax.plot(sta_elev['Station'], sta_elev['Elevation'], 'b-', linewidth=2, label='Cross Section')\n",
        "ax.fill_between(sta_elev['Station'], sta_elev['Elevation'],\n",
        "                 sta_elev['Elevation'].min() - 5,\n",
        "                 alpha=0.3)\n",
        "\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlabel('Station (ft)', fontsize=12)\n",
        "ax.set_ylabel('Elevation (ft)', fontsize=12)\n",
        "ax.set_title(f'Cross Section Profile: {river} / {reach} / RS {rs}',\n",
        "             fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add statistics annotation\n",
        "stats_text = f'Points: {len(sta_elev)}\\n'\n",
        "stats_text += f'Width: {sta_elev[\"Station\"].max() - sta_elev[\"Station\"].min():.1f} ft\\n'\n",
        "stats_text += f'Elev Range: {sta_elev[\"Elevation\"].min():.1f} - {sta_elev[\"Elevation\"].max():.1f} ft'\n",
        "ax.text(0.02, 0.98, stats_text,\n",
        "        transform=ax.transAxes,\n",
        "        verticalalignment='top',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "        fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Modify Cross Section Geometry (Round-Trip Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.859568Z",
          "iopub.status.busy": "2025-11-17T17:43:18.859317Z",
          "iopub.status.idle": "2025-11-17T17:43:18.863036Z",
          "shell.execute_reply": "2025-11-17T17:43:18.862455Z"
        }
      },
      "outputs": [],
      "source": [
        "# NOTE: This cell modifies the geometry file!\n",
        "# Uncomment to actually modify the file (backup is created automatically)\n",
        "\n",
        "# # Read original data\n",
        "# original_sta_elev = RasGeometry.get_station_elevation(geom_file, river, reach, rs)\n",
        "# print(f\"Original mean elevation: {original_sta_elev['Elevation'].mean():.3f} ft\")\n",
        "\n",
        "# # Modify elevations (raise by 1 foot)\n",
        "# modified_sta_elev = original_sta_elev.copy()\n",
        "# modified_sta_elev['Elevation'] += 1.0\n",
        "\n",
        "# # Write back to file (creates .bak backup automatically)\n",
        "# RasGeometry.set_station_elevation(geom_file, river, reach, rs, modified_sta_elev)\n",
        "# print(f\"\\n\u2713 Modified geometry written to file\")\n",
        "# print(f\"  Backup created: {geom_file}.bak\")\n",
        "\n",
        "# # Verify round-trip\n",
        "# readback_sta_elev = RasGeometry.get_station_elevation(geom_file, river, reach, rs)\n",
        "# print(f\"\\nReadback mean elevation: {readback_sta_elev['Elevation'].mean():.3f} ft\")\n",
        "# print(f\"Difference: {abs(modified_sta_elev['Elevation'].mean() - readback_sta_elev['Elevation'].mean()):.6f} ft\")\n",
        "\n",
        "# # Restore original (using backup)\n",
        "# import shutil\n",
        "# shutil.copy2(str(geom_file) + '.bak', geom_file)\n",
        "# print(f\"\\n\u2713 Original geometry restored from backup\")\n",
        "\n",
        "print(\"Modification example is commented out to protect geometry file.\")\n",
        "print(\"Uncomment the code above to test round-trip operations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Hydraulic Property Tables (HTAB)\n",
        "\n",
        "Property tables contain preprocessed hydraulic properties (area, conveyance, wetted perimeter, etc.) as functions of elevation. These enable hydraulic analysis without re-running HEC-RAS.\n",
        "\n",
        "**IMPORTANT:** Property tables only exist in preprocessed geometry HDF files. Run `RasCmdr.compute_plan()` to generate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Generate Preprocessed Geometry HDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:43:18.865449Z",
          "iopub.status.busy": "2025-11-17T17:43:18.865115Z",
          "iopub.status.idle": "2025-11-17T17:44:57.044563Z",
          "shell.execute_reply": "2025-11-17T17:44:57.043803Z"
        }
      },
      "outputs": [],
      "source": [
        "# NOTE: This cell runs HEC-RAS and may take 1-2 minutes!\n",
        "# Uncomment to generate preprocessed HDF\n",
        "\n",
        "# Extract and initialize project\n",
        "project_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "# Compute plan to generate preprocessed geometry HDF\n",
        "print(\"Computing plan 01 to generate preprocessed geometry HDF...\")\n",
        "print(\"(This may take 1-2 minutes)\\n\")\n",
        "result = RasCmdr.compute_plan(\"01\")\n",
        "\n",
        "if result:\n",
        "    geom_hdf = ras.geom_df.iloc[0]['full_path'] + '.hdf'\n",
        "    print(f\"\\n\u2713 Preprocessed HDF created: {Path(geom_hdf).name}\")\n",
        "    print(f\"  Location: {geom_hdf}\")\n",
        "else:\n",
        "    print(\"ERROR: Plan computation failed\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Extract HTAB for a Cross Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:44:57.047143Z",
          "iopub.status.busy": "2025-11-17T17:44:57.046705Z",
          "iopub.status.idle": "2025-11-17T17:44:57.074674Z",
          "shell.execute_reply": "2025-11-17T17:44:57.073946Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract hydraulic property table\n",
        "river = \"Bald Eagle\"\n",
        "reach = \"Loc Hav\"\n",
        "rs = \"138154.4\"\n",
        "\n",
        "htab = HdfHydraulicTables.get_xs_htab(geom_hdf, river, reach, rs)\n",
        "\n",
        "print(f\"Hydraulic Property Table for {river} / {reach} / RS {rs}\")\n",
        "print(f\"\\nTable dimensions: {len(htab)} elevations \u00d7 {len(htab.columns)} properties\")\n",
        "print(f\"Elevation range: {htab['Elevation'].min():.2f} to {htab['Elevation'].max():.2f} ft\")\n",
        "\n",
        "print(\"\\nAvailable properties:\")\n",
        "for col in htab.columns:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display.display(htab.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Generate Rating Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:44:57.077186Z",
          "iopub.status.busy": "2025-11-17T17:44:57.076944Z",
          "iopub.status.idle": "2025-11-17T17:44:57.469189Z",
          "shell.execute_reply": "2025-11-17T17:44:57.468645Z"
        }
      },
      "outputs": [],
      "source": [
        "# Calculate hydraulic radius\n",
        "htab['Hydraulic_Radius'] = htab['Area_Total'] / htab['Wetted_Perimeter_Total']\n",
        "htab['Hydraulic_Radius'] = htab['Hydraulic_Radius'].replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Create 4-panel rating curve plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Area vs Elevation\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(htab['Area_Total'], htab['Elevation'], 'b-', linewidth=2)\n",
        "ax1.fill_betweenx(htab['Elevation'], 0, htab['Area_Total'], alpha=0.3)\n",
        "ax1.set_xlabel('Flow Area (sq ft)', fontsize=11)\n",
        "ax1.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "ax1.set_title('Area-Elevation Curve', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Conveyance vs Elevation\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(htab['Conveyance_Total'], htab['Elevation'], 'g-', linewidth=2)\n",
        "ax2.fill_betweenx(htab['Elevation'], 0, htab['Conveyance_Total'], alpha=0.3, color='green')\n",
        "ax2.set_xlabel('Conveyance (cfs)', fontsize=11)\n",
        "ax2.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "ax2.set_title('Conveyance-Elevation Curve', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Top Width vs Elevation\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(htab['Top_Width'], htab['Elevation'], 'r-', linewidth=2)\n",
        "ax3.fill_betweenx(htab['Elevation'], 0, htab['Top_Width'], alpha=0.3, color='red')\n",
        "ax3.set_xlabel('Top Width (ft)', fontsize=11)\n",
        "ax3.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "ax3.set_title('Top Width-Elevation Curve', fontsize=12, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Hydraulic Radius vs Elevation\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(htab['Hydraulic_Radius'], htab['Elevation'], 'm-', linewidth=2)\n",
        "ax4.fill_betweenx(htab['Elevation'], 0, htab['Hydraulic_Radius'], alpha=0.3, color='purple')\n",
        "ax4.set_xlabel('Hydraulic Radius (ft)', fontsize=11)\n",
        "ax4.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "ax4.set_title('Hydraulic Radius-Elevation Curve', fontsize=12, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "fig.suptitle(f'Hydraulic Property Curves: {river} / {reach} / RS {rs}',\n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nMaximum Hydraulic Properties:\")\n",
        "print(f\"  Max Area: {htab['Area_Total'].max():.1f} sq ft\")\n",
        "print(f\"  Max Conveyance: {htab['Conveyance_Total'].max():.1f} cfs\")\n",
        "print(f\"  Max Hydraulic Radius: {htab['Hydraulic_Radius'].max():.2f} ft\")\n",
        "print(f\"  Max Top Width: {htab['Top_Width'].max():.1f} ft\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Find Hydraulic Properties at Specific Elevation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:44:57.471161Z",
          "iopub.status.busy": "2025-11-17T17:44:57.470937Z",
          "iopub.status.idle": "2025-11-17T17:44:57.476034Z",
          "shell.execute_reply": "2025-11-17T17:44:57.475617Z"
        }
      },
      "outputs": [],
      "source": [
        "# Find properties at specific water surface elevation\n",
        "target_elev = 665.0\n",
        "\n",
        "# Find closest elevation in table\n",
        "idx = (htab['Elevation'] - target_elev).abs().idxmin()\n",
        "actual_elev = htab.loc[idx, 'Elevation']\n",
        "\n",
        "print(f\"Target elevation: {target_elev:.2f} ft\")\n",
        "print(f\"Closest table elevation: {actual_elev:.2f} ft\\n\")\n",
        "\n",
        "print(f\"Hydraulic properties at elevation {actual_elev:.2f} ft:\")\n",
        "print(f\"  Flow Area: {htab.loc[idx, 'Area_Total']:.1f} sq ft\")\n",
        "print(f\"  Conveyance: {htab.loc[idx, 'Conveyance_Total']:.1f} cfs\")\n",
        "print(f\"  Wetted Perimeter: {htab.loc[idx, 'Wetted_Perimeter_Total']:.1f} ft\")\n",
        "print(f\"  Top Width: {htab.loc[idx, 'Top_Width']:.1f} ft\")\n",
        "print(f\"  Hydraulic Radius: {htab.loc[idx, 'Hydraulic_Radius']:.2f} ft\")\n",
        "print(f\"  Velocity Coefficient (Alpha): {htab.loc[idx, 'Alpha']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Batch Extract HTABs for Multiple Cross Sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:44:57.478088Z",
          "iopub.status.busy": "2025-11-17T17:44:57.477901Z",
          "iopub.status.idle": "2025-11-17T17:44:58.072795Z",
          "shell.execute_reply": "2025-11-17T17:44:58.072089Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract HTABs for all cross sections\n",
        "print(\"Extracting property tables for all cross sections...\")\n",
        "print(\"(This may take a few seconds)\\n\")\n",
        "\n",
        "all_htabs = HdfHydraulicTables.get_all_xs_htabs(geom_hdf)\n",
        "\n",
        "print(f\"Total HTABs extracted: {len(all_htabs)}\")\n",
        "\n",
        "# Calculate statistics across all cross sections\n",
        "stats_list = []\n",
        "for (river, reach, rs), htab_df in list(all_htabs.items())[:10]:  # First 10 for demo\n",
        "    htab_df['Hydraulic_Radius'] = htab_df['Area_Total'] / htab_df['Wetted_Perimeter_Total']\n",
        "    htab_df['Hydraulic_Radius'] = htab_df['Hydraulic_Radius'].replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    stats_list.append({\n",
        "        'RS': rs,\n",
        "        'Max_Area': htab_df['Area_Total'].max(),\n",
        "        'Max_Conveyance': htab_df['Conveyance_Total'].max(),\n",
        "        'Max_Hydraulic_Radius': htab_df['Hydraulic_Radius'].max(),\n",
        "        'Num_Elevations': len(htab_df)\n",
        "    })\n",
        "\n",
        "stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "print(\"\\nStatistics for first 10 cross sections:\")\n",
        "display.display(stats_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Storage Area Operations\n",
        "\n",
        "Storage areas represent reservoirs, detention basins, or other storage volumes. We'll extract elevation-volume curves that define storage capacity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 List Storage Areas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Change to Geometry 12 (SA to 2D Connection) of BaldEagleCrkMulti2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:44:58.075854Z",
          "iopub.status.busy": "2025-11-17T17:44:58.075606Z",
          "iopub.status.idle": "2025-11-17T17:45:00.300360Z",
          "shell.execute_reply": "2025-11-17T17:45:00.299942Z"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set extraction path for storage area operations\n",
        "storageareasoperations_path = Path.cwd() / \"example_projects\" / \"StorageAreaOperations\"\n",
        "\n",
        "# Extract the example project to the designated folder\n",
        "bald_eagle2D_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\", output_path=storageareasoperations_path)\n",
        "print(bald_eagle2D_path)\n",
        "\n",
        "init_ras_project(bald_eagle2D_path)\n",
        "\n",
        "geom_number = \"12\"\n",
        "geom_row = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number].iloc[0]\n",
        "\n",
        "geom_file = Path(geom_row[\"full_path\"])\n",
        "geom_hdf = Path(geom_row[\"hdf_path\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.302501Z",
          "iopub.status.busy": "2025-11-17T17:45:00.302264Z",
          "iopub.status.idle": "2025-11-17T17:45:00.308795Z",
          "shell.execute_reply": "2025-11-17T17:45:00.308194Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.311039Z",
          "iopub.status.busy": "2025-11-17T17:45:00.310801Z",
          "iopub.status.idle": "2025-11-17T17:45:00.329838Z",
          "shell.execute_reply": "2025-11-17T17:45:00.329297Z"
        }
      },
      "outputs": [],
      "source": [
        "# Path to dam breach geometry with storage areas\n",
        "dam_geom_file = geom_file\n",
        "\n",
        "print(f\"Geometry file: {dam_geom_file.name}\")\n",
        "\n",
        "# Get storage areas (excluding 2D flow areas)\n",
        "storage_areas = RasGeometry.get_storage_areas(dam_geom_file, exclude_2d=True)\n",
        "\n",
        "print(f\"\\nTraditional storage areas found: {len(storage_areas)}\")\n",
        "for i, name in enumerate(storage_areas, 1):\n",
        "    print(f\"  {i}. {name}\")\n",
        "\n",
        "# Get all storage areas (including 2D)\n",
        "all_storage = RasGeometry.get_storage_areas(dam_geom_file, exclude_2d=False)\n",
        "\n",
        "print(f\"\\nAll storage areas (including 2D): {len(all_storage)}\")\n",
        "for i, name in enumerate(all_storage, 1):\n",
        "    print(f\"  {i}. {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Extract Elevation-Volume Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.332749Z",
          "iopub.status.busy": "2025-11-17T17:45:00.332462Z",
          "iopub.status.idle": "2025-11-17T17:45:00.345364Z",
          "shell.execute_reply": "2025-11-17T17:45:00.344746Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get elevation-volume curve for first storage area\n",
        "if len(storage_areas) > 0:\n",
        "    area_name = storage_areas[0]\n",
        "\n",
        "    print(f\"Extracting elevation-volume curve for: {area_name}\")\n",
        "\n",
        "    elev_vol = RasGeometry.get_storage_elevation_volume(dam_geom_file, area_name)\n",
        "\n",
        "    print(f\"\\nStorage Curve Data:\")\n",
        "    print(f\"  Points: {len(elev_vol)}\")\n",
        "    print(f\"  Elevation range: {elev_vol['Elevation'].min():.2f} to {elev_vol['Elevation'].max():.2f} ft\")\n",
        "    print(f\"  Volume range: {elev_vol['Volume'].min():.0f} to {elev_vol['Volume'].max():.0f} cu ft\")\n",
        "\n",
        "    print(\"\\nFirst 10 points:\")\n",
        "    display.display(elev_vol.head(10))\n",
        "else:\n",
        "    print(\"No traditional storage areas found in this geometry file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Visualize Storage Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.348637Z",
          "iopub.status.busy": "2025-11-17T17:45:00.348280Z",
          "iopub.status.idle": "2025-11-17T17:45:00.553415Z",
          "shell.execute_reply": "2025-11-17T17:45:00.552873Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(storage_areas) > 0:\n",
        "    # Plot elevation-volume curve\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot 1: Volume vs Elevation\n",
        "    ax1.plot(elev_vol['Volume'], elev_vol['Elevation'], 'b-', linewidth=2)\n",
        "    ax1.fill_betweenx(elev_vol['Elevation'], 0, elev_vol['Volume'], alpha=0.3)\n",
        "    ax1.set_xlabel('Storage Volume (cu ft)', fontsize=11)\n",
        "    ax1.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "    ax1.set_title(f'Storage Curve: {area_name}', fontsize=12, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Incremental Storage (dV/dZ)\n",
        "    elev_vol['dV'] = elev_vol['Volume'].diff()\n",
        "    elev_vol['dZ'] = elev_vol['Elevation'].diff()\n",
        "    elev_vol['Surface_Area'] = elev_vol['dV'] / elev_vol['dZ']\n",
        "\n",
        "    ax2.plot(elev_vol['Surface_Area'], elev_vol['Elevation'], 'r-', linewidth=2)\n",
        "    ax2.fill_betweenx(elev_vol['Elevation'], 0, elev_vol['Surface_Area'], alpha=0.3, color='red')\n",
        "    ax2.set_xlabel('Surface Area (sq ft)', fontsize=11)\n",
        "    ax2.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "    ax2.set_title('Surface Area vs Elevation (dV/dZ)', fontsize=12, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nStorage Statistics:\")\n",
        "    print(f\"  Total storage capacity: {elev_vol['Volume'].max():.0f} cu ft\")\n",
        "    print(f\"  Elevation range: {elev_vol['Elevation'].max() - elev_vol['Elevation'].min():.1f} ft\")\n",
        "    print(f\"  Average surface area: {elev_vol['Surface_Area'].mean():.0f} sq ft\")\n",
        "else:\n",
        "    print(\"Skipping storage visualization - no traditional storage areas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Lateral Structure Operations\n",
        "\n",
        "Lateral structures are overflow weirs or connections along the side of a channel. Common in urban drainage and irrigation systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Geometry XX in project XX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.555560Z",
          "iopub.status.busy": "2025-11-17T17:45:00.555317Z",
          "iopub.status.idle": "2025-11-17T17:45:00.933267Z",
          "shell.execute_reply": "2025-11-17T17:45:00.932774Z"
        }
      },
      "outputs": [],
      "source": [
        "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
        "print(muncie_path)\n",
        "\n",
        "init_ras_project(muncie_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.935212Z",
          "iopub.status.busy": "2025-11-17T17:45:00.934987Z",
          "iopub.status.idle": "2025-11-17T17:45:00.945400Z",
          "shell.execute_reply": "2025-11-17T17:45:00.944945Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.947252Z",
          "iopub.status.busy": "2025-11-17T17:45:00.947034Z",
          "iopub.status.idle": "2025-11-17T17:45:00.952359Z",
          "shell.execute_reply": "2025-11-17T17:45:00.951938Z"
        }
      },
      "outputs": [],
      "source": [
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.954173Z",
          "iopub.status.busy": "2025-11-17T17:45:00.953822Z",
          "iopub.status.idle": "2025-11-17T17:45:00.957279Z",
          "shell.execute_reply": "2025-11-17T17:45:00.956815Z"
        }
      },
      "outputs": [],
      "source": [
        "geom_number = \"01\"\n",
        "geom_row = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number].iloc[0]\n",
        "\n",
        "geom_file = Path(geom_row[\"full_path\"])\n",
        "geom_hdf = Path(geom_row[\"hdf_path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 List Lateral Structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.959556Z",
          "iopub.status.busy": "2025-11-17T17:45:00.959196Z",
          "iopub.status.idle": "2025-11-17T17:45:00.962144Z",
          "shell.execute_reply": "2025-11-17T17:45:00.961688Z"
        }
      },
      "outputs": [],
      "source": [
        "# Path to geometry with lateral structures\n",
        "lateral_geom_file = geom_file\n",
        "\n",
        "print(f\"Geometry file: {lateral_geom_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.964025Z",
          "iopub.status.busy": "2025-11-17T17:45:00.963639Z",
          "iopub.status.idle": "2025-11-17T17:45:00.982759Z",
          "shell.execute_reply": "2025-11-17T17:45:00.982328Z"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Extract lateral structures\n",
        "lat_strucs = RasGeometry.get_lateral_structures(lateral_geom_file)\n",
        "\n",
        "print(f\"\\nLateral structures found: {len(lat_strucs)}\")\n",
        "\n",
        "if len(lat_strucs) > 0:\n",
        "    print(\"\\nLateral structure inventory:\")\n",
        "    display.display(lat_strucs[['River', 'Reach', 'RS', 'Position', 'Width', 'Coefficient', 'Distance', 'Description']])\n",
        "else:\n",
        "    print(\"No lateral structures found in this geometry file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Extract and Visualize Lateral Weir Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:00.984720Z",
          "iopub.status.busy": "2025-11-17T17:45:00.984454Z",
          "iopub.status.idle": "2025-11-17T17:45:01.080629Z",
          "shell.execute_reply": "2025-11-17T17:45:01.080015Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(lat_strucs) > 0:\n",
        "    # Get profile for first lateral structure\n",
        "    first_lat = lat_strucs.iloc[0]\n",
        "    river = first_lat['River']\n",
        "    reach = first_lat['Reach']\n",
        "    rs = first_lat['RS']\n",
        "    position = first_lat['Position']\n",
        "\n",
        "    print(f\"Extracting lateral weir profile:\")\n",
        "    print(f\"  Location: {river} / {reach} / RS {rs}\")\n",
        "    print(f\"  Position: {position}\")\n",
        "    print(f\"  Description: {first_lat['Description']}\")\n",
        "\n",
        "    profile = RasGeometry.get_lateral_weir_profile(lateral_geom_file, river, reach, rs, position)\n",
        "\n",
        "    print(f\"\\nWeir Profile:\")\n",
        "    print(f\"  Points: {len(profile)}\")\n",
        "    print(f\"  Station range: {profile['Station'].min():.1f} to {profile['Station'].max():.1f} ft\")\n",
        "    print(f\"  Elevation range: {profile['Elevation'].min():.2f} to {profile['Elevation'].max():.2f} ft\")\n",
        "\n",
        "    # Plot weir profile\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.plot(profile['Station'], profile['Elevation'], 'go-', linewidth=2, markersize=6)\n",
        "    ax.fill_between(profile['Station'], profile['Elevation'],\n",
        "                     profile['Elevation'].min() - 2, alpha=0.3, color='green')\n",
        "    ax.set_xlabel('Station (ft)', fontsize=11)\n",
        "    ax.set_ylabel('Weir Crest Elevation (ft)', fontsize=11)\n",
        "    ax.set_title(f'Lateral Weir Crest Profile: RS {rs}, Position {position}',\n",
        "                 fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping lateral weir visualization - no laterals found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: SA/2D Connection Operations\n",
        "\n",
        "Connections link storage areas to 2D flow areas for dam breach modeling, levee overtopping, and floodplain connectivity. We'll analyze connections, weir profiles, and gate structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 List All Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.083154Z",
          "iopub.status.busy": "2025-11-17T17:45:01.082861Z",
          "iopub.status.idle": "2025-11-17T17:45:01.094111Z",
          "shell.execute_reply": "2025-11-17T17:45:01.093586Z"
        }
      },
      "outputs": [],
      "source": [
        "# Extract all connections\n",
        "connections = RasGeometry.get_connections(dam_geom_file)\n",
        "\n",
        "print(f\"SA/2D Connections found: {len(connections)}\")\n",
        "\n",
        "if len(connections) > 0:\n",
        "    print(\"\\nConnection inventory:\")\n",
        "    display.display(connections[['Connection_Name', 'Upstream_Area', 'Downstream_Area',\n",
        "                                  'Weir_Width', 'Weir_Coefficient', 'SE_Count', 'Num_Gates']])\n",
        "\n",
        "    print(\"\\nConnection summary:\")\n",
        "    print(f\"  Total weir width: {connections['Weir_Width'].sum():.1f} ft\")\n",
        "    print(f\"  Total gates: {connections['Num_Gates'].sum():.0f}\")\n",
        "    print(f\"  Weir coefficient range: {connections['Weir_Coefficient'].min():.2f} to {connections['Weir_Coefficient'].max():.2f}\")\n",
        "else:\n",
        "    print(\"No connections found in this geometry file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Extract Dam Crest Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.096545Z",
          "iopub.status.busy": "2025-11-17T17:45:01.096140Z",
          "iopub.status.idle": "2025-11-17T17:45:01.105911Z",
          "shell.execute_reply": "2025-11-17T17:45:01.105330Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(connections) > 0:\n",
        "    # Get weir profile for first connection\n",
        "    conn_name = connections.iloc[0]['Connection_Name']\n",
        "\n",
        "    print(f\"Extracting weir profile for connection: {conn_name}\")\n",
        "    print(f\"  Upstream: {connections.iloc[0]['Upstream_Area']}\")\n",
        "    print(f\"  Downstream: {connections.iloc[0]['Downstream_Area']}\")\n",
        "\n",
        "    weir_profile = RasGeometry.get_connection_weir_profile(dam_geom_file, conn_name)\n",
        "\n",
        "    print(f\"\\nWeir/Dam Crest Profile:\")\n",
        "    print(f\"  Points: {len(weir_profile)}\")\n",
        "    print(f\"  Station range: {weir_profile['Station'].min():.1f} to {weir_profile['Station'].max():.1f} ft\")\n",
        "    print(f\"  Elevation range: {weir_profile['Elevation'].min():.2f} to {weir_profile['Elevation'].max():.2f} ft\")\n",
        "    print(f\"  Crest length: {weir_profile['Station'].max() - weir_profile['Station'].min():.1f} ft\")\n",
        "\n",
        "    print(\"\\nFirst 10 points:\")\n",
        "    display.display(weir_profile.head(10))\n",
        "else:\n",
        "    print(\"Skipping weir profile extraction - no connections found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Visualize Dam Crest Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.108118Z",
          "iopub.status.busy": "2025-11-17T17:45:01.107855Z",
          "iopub.status.idle": "2025-11-17T17:45:01.237014Z",
          "shell.execute_reply": "2025-11-17T17:45:01.236378Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(connections) > 0:\n",
        "    # Plot weir/dam crest profile\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    ax.plot(weir_profile['Station'], weir_profile['Elevation'],\n",
        "            'ro-', linewidth=2, markersize=6, label='Dam Crest')\n",
        "    ax.fill_between(weir_profile['Station'], weir_profile['Elevation'],\n",
        "                     weir_profile['Elevation'].min() - 10,\n",
        "                     alpha=0.3, color='brown')\n",
        "\n",
        "    ax.set_xlabel('Station Along Dam (ft)', fontsize=12)\n",
        "    ax.set_ylabel('Crest Elevation (ft)', fontsize=12)\n",
        "    ax.set_title(f'Dam Crest Profile: {conn_name}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(fontsize=10)\n",
        "\n",
        "    # Add statistics\n",
        "    stats_text = f'Points: {len(weir_profile)}\\n'\n",
        "    stats_text += f'Length: {weir_profile[\"Station\"].max() - weir_profile[\"Station\"].min():.1f} ft\\n'\n",
        "    stats_text += f'Elev Range: {weir_profile[\"Elevation\"].min():.1f} - {weir_profile[\"Elevation\"].max():.1f} ft\\n'\n",
        "    stats_text += f'Weir Coef: {connections.iloc[0][\"Weir_Coefficient\"]:.2f}'\n",
        "    ax.text(0.02, 0.98, stats_text,\n",
        "            transform=ax.transAxes,\n",
        "            verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "            fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping visualization - no connections\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Extract Gate Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.239262Z",
          "iopub.status.busy": "2025-11-17T17:45:01.239049Z",
          "iopub.status.idle": "2025-11-17T17:45:01.437455Z",
          "shell.execute_reply": "2025-11-17T17:45:01.436925Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(connections) > 0:\n",
        "    # Check which connections have gates\n",
        "    conn_with_gates = connections[connections['Num_Gates'] > 0]\n",
        "\n",
        "    if len(conn_with_gates) > 0:\n",
        "        # Get gates for first connection with gates\n",
        "        conn_name_with_gates = conn_with_gates.iloc[0]['Connection_Name']\n",
        "\n",
        "        print(f\"Extracting gates for connection: {conn_name_with_gates}\")\n",
        "\n",
        "        gates = RasGeometry.get_connection_gates(dam_geom_file, conn_name_with_gates)\n",
        "\n",
        "        print(f\"\\nGates found: {len(gates)}\")\n",
        "\n",
        "        if len(gates) > 0:\n",
        "            print(\"\\nGate parameters:\")\n",
        "            display.display(gates[['Gate_Name', 'Width', 'Height', 'Invert', 'Gate_Coefficient']])\n",
        "\n",
        "            # Visualize gate geometry\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "            for idx, gate in gates.iterrows():\n",
        "                # Draw gate as rectangle\n",
        "                width = gate['Width']\n",
        "                height = gate['Height']\n",
        "                invert = gate['Invert']\n",
        "\n",
        "                rect = plt.Rectangle((idx * 20, invert), width, height,\n",
        "                                      linewidth=2, edgecolor='blue',\n",
        "                                      facecolor='lightblue', alpha=0.5)\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "                # Label gate\n",
        "                ax.text(idx * 20 + width/2, invert + height + 2,\n",
        "                        gate['Gate_Name'],\n",
        "                        ha='center', fontsize=10, fontweight='bold')\n",
        "                ax.text(idx * 20 + width/2, invert + height/2,\n",
        "                        f\"{width}' \u00d7 {height}'\\nInvert: {invert}'\",\n",
        "                        ha='center', va='center', fontsize=9)\n",
        "\n",
        "            ax.set_xlim(-5, len(gates) * 20 + 15)\n",
        "            ax.set_ylim(invert - 10, invert + height + 20)\n",
        "            ax.set_xlabel('Position', fontsize=11)\n",
        "            ax.set_ylabel('Elevation (ft)', fontsize=11)\n",
        "            ax.set_title(f'Gate Configuration: {conn_name_with_gates}',\n",
        "                         fontsize=12, fontweight='bold')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            ax.set_aspect('equal')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"No gates found in any connections\")\n",
        "else:\n",
        "    print(\"Skipping gate extraction - no connections\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Analyze All Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.439845Z",
          "iopub.status.busy": "2025-11-17T17:45:01.439548Z",
          "iopub.status.idle": "2025-11-17T17:45:01.451004Z",
          "shell.execute_reply": "2025-11-17T17:45:01.450566Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(connections) > 0:\n",
        "    # Extract profiles for all connections\n",
        "    print(\"Processing all connections...\\n\")\n",
        "\n",
        "    for idx, conn in connections.iterrows():\n",
        "        conn_name = conn['Connection_Name']\n",
        "\n",
        "        try:\n",
        "            # Get weir profile\n",
        "            profile = RasGeometry.get_connection_weir_profile(dam_geom_file, conn_name)\n",
        "\n",
        "            # Get gates\n",
        "            gates = RasGeometry.get_connection_gates(dam_geom_file, conn_name)\n",
        "\n",
        "            print(f\"{idx+1}. {conn_name}:\")\n",
        "            print(f\"   {conn['Upstream_Area']} \u2192 {conn['Downstream_Area']}\")\n",
        "            print(f\"   Weir profile: {len(profile)} points, length={profile['Station'].max():.0f} ft\")\n",
        "            print(f\"   Gates: {len(gates)}\")\n",
        "            if len(gates) > 0:\n",
        "                gate = gates.iloc[0]\n",
        "                print(f\"     {gate['Gate_Name']}: {gate['Width']:.0f}' W \u00d7 {gate['Height']:.0f}' H, Invert={gate['Invert']:.0f}'\")\n",
        "            print()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{idx+1}. {conn_name}: ERROR - {e}\\n\")\n",
        "else:\n",
        "    print(\"No connections to process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Batch Processing Example\n",
        "\n",
        "Demonstrate how to automate operations across multiple geometry elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Calculate Statistics for All Cross Sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.453506Z",
          "iopub.status.busy": "2025-11-17T17:45:01.453210Z",
          "iopub.status.idle": "2025-11-17T17:45:01.510796Z",
          "shell.execute_reply": "2025-11-17T17:45:01.510188Z"
        }
      },
      "outputs": [],
      "source": [
        "# Get all cross sections\n",
        "xs_df = RasGeometry.get_cross_sections(geom_file)\n",
        "\n",
        "print(f\"Processing {len(xs_df)} cross sections...\")\n",
        "\n",
        "# Calculate geometry statistics for first 10 XS (for speed)\n",
        "xs_stats = []\n",
        "for idx, xs in xs_df.head(10).iterrows():\n",
        "    try:\n",
        "        sta_elev = RasGeometry.get_station_elevation(geom_file, xs['River'], xs['Reach'], xs['RS'])\n",
        "\n",
        "        stats = {\n",
        "            'RS': xs['RS'],\n",
        "            'Points': len(sta_elev),\n",
        "            'Min_Elev': sta_elev['Elevation'].min(),\n",
        "            'Max_Elev': sta_elev['Elevation'].max(),\n",
        "            'Relief': sta_elev['Elevation'].max() - sta_elev['Elevation'].min(),\n",
        "            'Width': sta_elev['Station'].max() - sta_elev['Station'].min(),\n",
        "            'Channel_Length': xs['Length_Channel']\n",
        "        }\n",
        "        xs_stats.append(stats)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Could not process RS {xs['RS']}: {e}\")\n",
        "\n",
        "xs_stats_df = pd.DataFrame(xs_stats)\n",
        "\n",
        "print(f\"\\nStatistics for first 10 cross sections:\")\n",
        "display.display(xs_stats_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Visualize Cross Section Statistics Along Reach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.513364Z",
          "iopub.status.busy": "2025-11-17T17:45:01.513064Z",
          "iopub.status.idle": "2025-11-17T17:45:01.992322Z",
          "shell.execute_reply": "2025-11-17T17:45:01.991697Z"
        }
      },
      "outputs": [],
      "source": [
        "# Plot cross section properties along the reach\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Convert RS to numeric for plotting\n",
        "xs_stats_df['RS_num'] = pd.to_numeric(xs_stats_df['RS'], errors='coerce')\n",
        "\n",
        "# Plot 1: Channel width along reach\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(xs_stats_df['RS_num'], xs_stats_df['Width'], 'b-o', linewidth=2, markersize=5)\n",
        "ax1.set_xlabel('River Station', fontsize=11)\n",
        "ax1.set_ylabel('Channel Width (ft)', fontsize=11)\n",
        "ax1.set_title('Channel Width Along Reach', fontsize=12, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.invert_xaxis()  # Invert so upstream is on left\n",
        "\n",
        "# Plot 2: Relief (elevation change across XS)\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(xs_stats_df['RS_num'], xs_stats_df['Relief'], 'g-o', linewidth=2, markersize=5)\n",
        "ax2.set_xlabel('River Station', fontsize=11)\n",
        "ax2.set_ylabel('Relief (ft)', fontsize=11)\n",
        "ax2.set_title('Cross Section Relief Along Reach', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.invert_xaxis()\n",
        "\n",
        "# Plot 3: Minimum elevation (thalweg)\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(xs_stats_df['RS_num'], xs_stats_df['Min_Elev'], 'r-o', linewidth=2, markersize=5)\n",
        "ax3.set_xlabel('River Station', fontsize=11)\n",
        "ax3.set_ylabel('Minimum Elevation (ft)', fontsize=11)\n",
        "ax3.set_title('Thalweg Profile', fontsize=12, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.invert_xaxis()\n",
        "\n",
        "# Plot 4: Number of points per XS\n",
        "ax4 = axes[1, 1]\n",
        "ax4.bar(xs_stats_df['RS_num'], xs_stats_df['Points'], color='purple', alpha=0.6)\n",
        "ax4.set_xlabel('River Station', fontsize=11)\n",
        "ax4.set_ylabel('Number of Points', fontsize=11)\n",
        "ax4.set_title('Cross Section Detail Level', fontsize=12, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "ax4.invert_xaxis()\n",
        "\n",
        "fig.suptitle('Cross Section Statistics Along Reach', fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Practical Applications\n",
        "\n",
        "Real-world use cases combining multiple geometry operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Application: Dam Breach Analysis Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:01.994932Z",
          "iopub.status.busy": "2025-11-17T17:45:01.994562Z",
          "iopub.status.idle": "2025-11-17T17:45:02.018238Z",
          "shell.execute_reply": "2025-11-17T17:45:02.017624Z"
        }
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DAM BREACH ANALYSIS WORKFLOW\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Step 1: Identify storage areas\n",
        "print(\"\\nStep 1: Storage Areas\")\n",
        "storage_areas = RasGeometry.get_storage_areas(dam_geom_file)\n",
        "print(f\"  Found {len(storage_areas)} storage area(s)\")\n",
        "for area in storage_areas:\n",
        "    print(f\"    - {area}\")\n",
        "\n",
        "# Step 2: Get storage capacity\n",
        "if len(storage_areas) > 0:\n",
        "    print(\"\\nStep 2: Storage Capacity\")\n",
        "    elev_vol = RasGeometry.get_storage_elevation_volume(dam_geom_file, storage_areas[0])\n",
        "    print(f\"  Maximum storage: {elev_vol['Volume'].max():.0f} cu ft\")\n",
        "    print(f\"  Pool elevation range: {elev_vol['Elevation'].min():.1f} to {elev_vol['Elevation'].max():.1f} ft\")\n",
        "\n",
        "# Step 3: Identify connections\n",
        "print(\"\\nStep 3: Connections\")\n",
        "connections = RasGeometry.get_connections(dam_geom_file)\n",
        "print(f\"  Found {len(connections)} connection(s)\")\n",
        "for idx, conn in connections.iterrows():\n",
        "    print(f\"    - {conn['Connection_Name']}: {conn['Upstream_Area']} \u2192 {conn['Downstream_Area']}\")\n",
        "\n",
        "# Step 4: Analyze dam crest\n",
        "if len(connections) > 0:\n",
        "    dam_connections = connections[connections['Connection_Name'].str.contains('Dam', case=False, na=False)]\n",
        "\n",
        "    if len(dam_connections) > 0:\n",
        "        print(\"\\nStep 4: Dam Crest Geometry\")\n",
        "        dam_name = dam_connections.iloc[0]['Connection_Name']\n",
        "        dam_profile = RasGeometry.get_connection_weir_profile(dam_geom_file, dam_name)\n",
        "\n",
        "        print(f\"  Dam: {dam_name}\")\n",
        "        print(f\"  Crest length: {dam_profile['Station'].max() - dam_profile['Station'].min():.0f} ft\")\n",
        "        print(f\"  Crest elevation range: {dam_profile['Elevation'].min():.1f} to {dam_profile['Elevation'].max():.1f} ft\")\n",
        "        print(f\"  Min crest elevation: {dam_profile['Elevation'].min():.1f} ft\")\n",
        "\n",
        "# Step 5: Check for gates\n",
        "if len(connections) > 0:\n",
        "    print(\"\\nStep 5: Gate Inventory\")\n",
        "    total_gates = connections['Num_Gates'].sum()\n",
        "    print(f\"  Total gates in model: {total_gates:.0f}\")\n",
        "\n",
        "    if total_gates > 0:\n",
        "        conn_with_gates = connections[connections['Num_Gates'] > 0]\n",
        "        for idx, conn in conn_with_gates.iterrows():\n",
        "            gates = RasGeometry.get_connection_gates(dam_geom_file, conn['Connection_Name'])\n",
        "            print(f\"  {conn['Connection_Name']}: {len(gates)} gate(s)\")\n",
        "            for _, gate in gates.iterrows():\n",
        "                print(f\"    \u2022 {gate['Gate_Name']}: {gate['Width']:.0f}' \u00d7 {gate['Height']:.0f}', Invert {gate['Invert']:.0f}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Application: Sensitivity Study Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:02.020658Z",
          "iopub.status.busy": "2025-11-17T17:45:02.020352Z",
          "iopub.status.idle": "2025-11-17T17:45:02.025927Z",
          "shell.execute_reply": "2025-11-17T17:45:02.025120Z"
        }
      },
      "outputs": [],
      "source": [
        "# Example: Prepare geometry for Manning's n sensitivity study\n",
        "# (Conceptual - actual Manning's n methods not yet implemented)\n",
        "\n",
        "print(\"Sensitivity Study Example:\")\n",
        "print(\"\\nPreparing geometry for parameter sensitivity analysis...\")\n",
        "\n",
        "# Get cross sections for modification\n",
        "xs_subset = xs_df.head(5)\n",
        "\n",
        "print(f\"\\nSelected {len(xs_subset)} cross sections for study:\")\n",
        "for idx, xs in xs_subset.iterrows():\n",
        "    print(f\"  - RS {xs['RS']}\")\n",
        "\n",
        "print(\"\\nFor each cross section, you could:\")\n",
        "print(\"  1. Extract current geometry with get_station_elevation()\")\n",
        "print(\"  2. Extract current Manning's n (future: get_mannings_n())\")\n",
        "print(\"  3. Modify Manning's n values (e.g., \u00b110%, \u00b120%)\")\n",
        "print(\"  4. Write modified values (future: set_mannings_n())\")\n",
        "print(\"  5. Run RasCmdr.compute_plan() for each scenario\")\n",
        "print(\"  6. Compare results to assess sensitivity\")\n",
        "\n",
        "print(\"\\nThis workflow enables automated sensitivity studies!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Summary and Key Takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Methods Demonstrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:45:02.028545Z",
          "iopub.status.busy": "2025-11-17T17:45:02.028173Z",
          "iopub.status.idle": "2025-11-17T17:45:02.032756Z",
          "shell.execute_reply": "2025-11-17T17:45:02.032278Z"
        }
      },
      "outputs": [],
      "source": [
        "print(\"GEOMETRY PARSING METHODS DEMONSTRATED:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\ud83d\udcd0 Cross Section Operations (3 methods):\")\n",
        "print(\"  \u2713 RasGeometry.get_cross_sections()\")\n",
        "print(\"  \u2713 RasGeometry.get_station_elevation()\")\n",
        "print(\"  \u2713 RasGeometry.set_station_elevation()\")\n",
        "\n",
        "print(\"\\n\ud83d\udcca Hydraulic Property Tables - HTAB (2 methods):\")\n",
        "print(\"  \u2713 HdfHydraulicTables.get_xs_htab()\")\n",
        "print(\"  \u2713 HdfHydraulicTables.get_all_xs_htabs()\")\n",
        "\n",
        "print(\"\\n\ud83c\udfca Storage Area Operations (2 methods):\")\n",
        "print(\"  \u2713 RasGeometry.get_storage_areas()\")\n",
        "print(\"  \u2713 RasGeometry.get_storage_elevation_volume()\")\n",
        "\n",
        "print(\"\\n\ud83c\udf0a Lateral Structure Operations (2 methods):\")\n",
        "print(\"  \u2713 RasGeometry.get_lateral_structures()\")\n",
        "print(\"  \u2713 RasGeometry.get_lateral_weir_profile()\")\n",
        "\n",
        "print(\"\\n\ud83d\udea7 SA/2D Connection Operations (3 methods):\")\n",
        "print(\"  \u2713 RasGeometry.get_connections()\")\n",
        "print(\"  \u2713 RasGeometry.get_connection_weir_profile()\")\n",
        "print(\"  \u2713 RasGeometry.get_connection_gates()\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOTAL: 12 methods demonstrated\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Key Capabilities Enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook demonstrated how to:\n",
        "\n",
        "**1. Extract Geometry Data**\n",
        "- List and filter cross sections by river/reach\n",
        "- Extract station/elevation profiles\n",
        "- Get hydraulic property tables (HTAB)\n",
        "- List storage areas and connections\n",
        "- Extract weir profiles and gate configurations\n",
        "\n",
        "**2. Modify Geometry**\n",
        "- Change cross section elevations\n",
        "- Round-trip testing (read \u2192 modify \u2192 write \u2192 verify)\n",
        "- Automatic backup creation before modifications\n",
        "\n",
        "**3. Analyze Hydraulics**\n",
        "- Generate rating curves (area, conveyance, etc.)\n",
        "- Calculate hydraulic properties at specific stages\n",
        "- Find properties without re-running HEC-RAS\n",
        "\n",
        "**4. Visualize Results**\n",
        "- Cross section profiles\n",
        "- Rating curves (area, conveyance, WP, hydraulic radius)\n",
        "- Storage elevation-volume curves\n",
        "- Dam crest profiles\n",
        "- Gate configurations\n",
        "- Statistics along reach\n",
        "\n",
        "**5. Batch Processing**\n",
        "- Automate operations across multiple cross sections\n",
        "- Calculate reach-wide statistics\n",
        "- Process all connections in a model\n",
        "\n",
        "**Use Cases:**\n",
        "- \u2705 Automated geometry modification for sensitivity studies\n",
        "- \u2705 Rating curve generation without HEC-RAS computation\n",
        "- \u2705 Dam breach pre/post-processing\n",
        "- \u2705 Model QA/QC and validation\n",
        "- \u2705 GIS data extraction for visualization\n",
        "- \u2705 Batch processing for calibration studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Important Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**HTAB Data Requirements:**\n",
        "- Property tables only exist in **preprocessed geometry HDF files**\n",
        "- Run `RasCmdr.compute_plan(\"XX\")` to generate preprocessed HDF\n",
        "- Geometry HDF location: Same as geometry file with `.hdf` extension\n",
        "\n",
        "**Data Formats:**\n",
        "- Plain text geometry files (.g##): Editable with RasGeometry methods\n",
        "- Geometry HDF files (.g##.hdf): Read-only, accessed with HdfHydraulicTables\n",
        "- Fixed-width format (8-char columns): Used for station/elevation, Manning's n, storage curves\n",
        "- CSV format: Used for gates and some metadata\n",
        "\n",
        "**Safety Features:**\n",
        "- Automatic .bak backup before any file modification\n",
        "- Round-trip validation ensures data integrity\n",
        "- Comprehensive error messages for debugging\n",
        "- Logging of all operations\n",
        "\n",
        "**Performance:**\n",
        "- Single cross section extraction: < 0.5 seconds\n",
        "- Batch HTAB extraction (178 XS): < 2 seconds\n",
        "- File modification: < 1 second\n",
        "\n",
        "**Next Steps:**\n",
        "- Explore additional geometry operations (Manning's n, bank stations, ineffective areas)\n",
        "- Implement 2D flow area operations (perimeter, mesh parameters)\n",
        "- Create automated workflows for calibration and sensitivity studies\n",
        "- Export geometry data to GIS formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "**Documentation:**\n",
        "- API Reference: `research/geometry file parsing/api-geom.md`\n",
        "- Geometry Inventory: `research/geometry file parsing/Example Geometries/GEOMETRY_INVENTORY.md`\n",
        "- Parsing Patterns: `research/geometry file parsing/geometry_docs/_PARSING_PATTERNS_REFERENCE.md`\n",
        "\n",
        "**Working Scripts:**\n",
        "- `research/geometry file parsing/working_scripts/01_extract_cross_sections.py`\n",
        "- `research/geometry file parsing/working_scripts/02_htab_rating_curves.py`\n",
        "\n",
        "**Test Files:**\n",
        "- `tests/test_ras_geometry_xs.py` - Cross section tests\n",
        "- `tests/test_hdf_hydraulic_tables.py` - HTAB tests\n",
        "- `tests/test_ras_geometry_storage.py` - Storage area tests\n",
        "- `tests/test_ras_geometry_lateral.py` - Lateral structure tests\n",
        "- `tests/test_ras_geometry_connections.py` - Connection tests\n",
        "\n",
        "**RAS Commander Documentation:**\n",
        "- Main API: `C:\\GH\\ras-commander\\api-ras.md`\n",
        "- Development Guide: `C:\\GH\\ras-commander\\CLAUDE.md`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\21_rasmap_raster_exports.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: RASMapper Operations\n",
        "\n",
        "This notebook demonstrates how to work with RASMapper configuration files and raster outputs using the `ras-commander` library.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Initialize Project**: Set up the HEC-RAS project and explore the rasmap_df\n",
        "2. **Generate Stored Maps**: Use `RasMap.postprocess_stored_maps()` to create raster outputs\n",
        "3. **Locate Results**: Use `RasMap.get_results_folder()` and `RasMap.get_results_raster()` to find outputs\n",
        "4. **Visualize**: Load and display the generated raster files\n",
        "\n",
        "## Key Functions\n",
        "\n",
        "- `RasMap.parse_rasmap()` - Parse .rasmap XML configuration files\n",
        "- `RasMap.get_terrain_names()` - Extract available terrain layers\n",
        "- `RasMap.postprocess_stored_maps()` - Automate raster output generation\n",
        "- `RasMap.get_results_folder()` - Find the output folder for a plan\n",
        "- `RasMap.get_results_raster()` - Get the path to a specific VRT file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import additional libraries for visualization\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Project and Explore RASMapper Configuration\n",
        "\n",
        "We'll use the Muncie example project, which is a 2D unsteady flow model. Let's initialize the project and examine the `rasmap_df` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:28:19 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project initialized: Muncie\n",
            "Project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n",
            "RAS version: 6.6\n"
          ]
        }
      ],
      "source": [
        "# Extract Muncie project if it doesn't exist\n",
        "project_path = Path(\"./example_projects/Muncie\").resolve()\n",
        "\n",
        "if not os.path.exists(project_path):\n",
        "    project_path = RasExamples.extract_project(\"Muncie\")\n",
        "\n",
        "# Initialize the RAS project with version 6.6\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "print(f\"Project initialized: {ras.project_name}\")\n",
        "print(f\"Project folder: {ras.project_folder}\")\n",
        "print(f\"RAS version: {ras.ras_version}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:28:19 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\n",
            "2025-11-17 21:28:19 - ras_commander.RasUtils - INFO - Using provided plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01\n",
            "2025-11-17 21:28:19 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01\n",
            "2025-11-17 21:28:19 - ras_commander.RasCmdr - INFO - Set number of cores to 4 for plan: 01\n",
            "2025-11-17 21:28:19 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
            "2025-11-17 21:28:19 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01\"\n",
            "2025-11-17 21:28:34 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
            "2025-11-17 21:28:34 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 14.86 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": "['True']"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We'll select a plan to work with (using plan number \"01\")\n",
        "plan_number = \"01\"\n",
        "success = RasCmdr.compute_plan(plan_number, num_cores=4)\n",
        "success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the rasmap_df\n",
        "\n",
        "The `rasmap_df` dataframe is created during project initialization by parsing the `.rasmap` XML file. It contains all RASMapper configuration information:\n",
        "\n",
        "- **projection_path**: Coordinate system projection file\n",
        "- **profile_lines_path**: XS cut lines for extracting profiles\n",
        "- **soil_layer_path**: Hydrologic soil group layers\n",
        "- **infiltration_hdf_path**: Infiltration parameter layers\n",
        "- **landcover_hdf_path**: Land cover classification layers\n",
        "- **terrain_hdf_path**: Digital elevation model (DEM) layers\n",
        "- **current_settings**: Various RASMapper display and export settings\n",
        "\n",
        "This dataframe provides a programmatic way to access all the spatial data layers and configuration used in RASMapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RASMapper Configuration DataFrame:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>projection_path</th>\\n', '      <th>profile_lines_path</th>\\n', '      <th>soil_layer_path</th>\\n', '      <th>infiltration_hdf_path</th>\\n', '      <th>landcover_hdf_path</th>\\n', '      <th>terrain_hdf_path</th>\\n', '      <th>current_settings</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>[C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects...</td>\\n', '      <td>[]</td>\\n', '      <td>[]</td>\\n', '      <td>[]</td>\\n', '      <td>[C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects...</td>\\n', \"      <td>{'RiverStationUnits': 'Feet', 'RiverStationDec...</td>\\n\", '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "                                     projection_path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                  profile_lines_path soil_layer_path  \\\n",
              "0  [C:\\GH\\ras-commander\\examples\\example_projects...              []   \n",
              "\n",
              "  infiltration_hdf_path landcover_hdf_path  \\\n",
              "0                    []                 []   \n",
              "\n",
              "                                    terrain_hdf_path  \\\n",
              "0  [C:\\GH\\ras-commander\\examples\\example_projects...   \n",
              "\n",
              "                                    current_settings  \n",
              "0  {'RiverStationUnits': 'Feet', 'RiverStationDec...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Column Details:\n",
            "============================================================\n",
            "\n",
            "projection_path:\n",
            "  C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\GIS_Data\\Muncie_IA_Clip.prj\n",
            "\n",
            "profile_lines_path:\n",
            "  - C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Features\\Profile Lines.shp\n",
            "\n",
            "soil_layer_path:\n",
            "  (empty list)\n",
            "\n",
            "infiltration_hdf_path:\n",
            "  (empty list)\n",
            "\n",
            "landcover_hdf_path:\n",
            "  (empty list)\n",
            "\n",
            "terrain_hdf_path:\n",
            "  - C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Terrain\\Terrain.hdf\n",
            "  - C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Terrain\\TerrainWithChannel.hdf\n",
            "\n",
            "current_settings:\n",
            "  RiverStationUnits: Feet\n",
            "  RiverStationDecimalPlaces: 0\n",
            "  HorizontalDecimalPlaces: 1\n",
            "  VerticalDecimalPlaces: 2\n",
            "  XSMaxPoints: 450\n",
            "  LSMaxPoints: 1000\n",
            "  ProfilePointMinCount: 0\n",
            "  ShowLegend: True\n",
            "  ShowNorthArrow: False\n",
            "  ShowScaleBar: True\n",
            "  ShowGreaterThanInLegend: False\n",
            "  TerrainDestinationFolder: .\\Terrain\n",
            "  AddDataFolder: .\\LandCover\n",
            "  LandCoverDestinationFolder: ..\\Muncie\n",
            "  TerrainSourceFolder: .\\Terrain\n"
          ]
        }
      ],
      "source": [
        "# Display the rasmap_df\n",
        "print(\"\\nRASMapper Configuration DataFrame:\")\n",
        "display(ras.rasmap_df)\n",
        "\n",
        "# Show what's in each column\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Column Details:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for col in ras.rasmap_df.columns:\n",
        "    value = ras.rasmap_df[col].iloc[0]\n",
        "    print(f\"\\n{col}:\")\n",
        "    if isinstance(value, list):\n",
        "        if len(value) > 0:\n",
        "            for item in value:\n",
        "                print(f\"  - {item}\")\n",
        "        else:\n",
        "            print(\"  (empty list)\")\n",
        "    elif isinstance(value, dict):\n",
        "        if len(value) > 0:\n",
        "            for key, val in value.items():\n",
        "                print(f\"  {key}: {val}\")\n",
        "        else:\n",
        "            print(\"  (empty dict)\")\n",
        "    else:\n",
        "        print(f\"  {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Available Terrains\n",
        "\n",
        "Before generating stored maps, we need to know which terrain layers are available. The `get_terrain_names()` function extracts this from the .rasmap file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Extracted terrain names: ['Terrain', 'TerrainWithChannel']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available terrains: ['Terrain', 'TerrainWithChannel']\n",
            "\n",
            "Using terrain: Terrain\n"
          ]
        }
      ],
      "source": [
        "# Get the rasmap file path\n",
        "rasmap_path = ras.project_folder / f\"{ras.project_name}.rasmap\"\n",
        "\n",
        "# Extract terrain names\n",
        "terrains = RasMap.get_terrain_names(rasmap_path)\n",
        "print(f\"Available terrains: {terrains}\")\n",
        "\n",
        "# Select the first terrain for our mapping\n",
        "target_terrain = None\n",
        "if terrains:\n",
        "    target_terrain = terrains[0]\n",
        "    print(f\"\\nUsing terrain: {target_terrain}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check for Existing Results\n",
        "\n",
        "Let's examine what plans are available and whether they have been computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>UNET D2 SolverType</th>\\n', '      <th>UNET D2 Name</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady Multi  9-SA run</td>\\n', '      <td>5.00</td>\\n', '      <td>9-SAs</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>15SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>NaN</td>\\n', '      <td>NaN</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady Run with 2D 50ft Grid</td>\\n', '      <td>5.10</td>\\n', '      <td>2D 50ft Grid</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>-1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>01</td>\\n', '      <td>04</td>\\n', '      <td>Unsteady Run with 2D 50ft User n Value R</td>\\n', '      <td>5.10</td>\\n', '      <td>50ft User n Regions</td>\\n', '      <td>02JAN1900,0000,02JAN1900,2400</td>\\n', '      <td>10SEC</td>\\n', '      <td>5MIN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>Pardiso (Direct)</td>\\n', '      <td>2D Interior Area</td>\\n', '      <td>None</td>\\n', '      <td>04</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['  plan_number unsteady_number geometry_number  \\\\\\n', '0          01              01              01   \\n', '1          03              01              02   \\n', '2          04              01              04   \\n', '\\n', '                                 Plan Title Program Version  \\\\\\n', '0                  Unsteady Multi  9-SA run            5.00   \\n', '1            Unsteady Run with 2D 50ft Grid            5.10   \\n', '2  Unsteady Run with 2D 50ft User n Value R            5.10   \\n', '\\n', '      Short Identifier                Simulation Date Computation Interval  \\\\\\n', '0                9-SAs  02JAN1900,0000,02JAN1900,2400                15SEC   \\n', '1         2D 50ft Grid  02JAN1900,0000,02JAN1900,2400                10SEC   \\n', '2  50ft User n Regions  02JAN1900,0000,02JAN1900,2400                10SEC   \\n', '\\n', '  Mapping Interval Run HTab  ... DSS File Friction Slope Method  \\\\\\n', '0             5MIN        1  ...      dss                     1   \\n', '1           \n...\n[Output truncated, 2041 characters total]"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available plans and HDF results status:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>9-SAs</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>03</td>\\n', '      <td>2D 50ft Grid</td>\\n', '      <td>None</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>04</td>\\n', '      <td>50ft User n Regions</td>\\n', '      <td>None</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['  plan_number     Short Identifier  \\\\\\n', '0          01                9-SAs   \\n', '1          03         2D 50ft Grid   \\n', '2          04  50ft User n Regions   \\n', '\\n', '                                    HDF_Results_Path  \\n', '0  C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...  \\n', '1                                               None  \\n', '2                                               None  ']"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Check for Existing Results and Display Status by Plan\n",
        "\n",
        "# Show which plans exist and which have results HDF files present.\n",
        "import pandas as pd\n",
        "\n",
        "# Choose relevant columns for clarity\n",
        "plan_results_cols = ['plan_number', 'Short Identifier', 'HDF_Results_Path']\n",
        "plan_results_df = ras.plan_df[plan_results_cols]\n",
        "\n",
        "print(\"Available plans and HDF results status:\")\n",
        "plan_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results for plan 01 are already present at:\n",
            "  C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.hdf\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Check for the computed HDF results file (outputs typically end with .pXX.hdf)\n",
        "plan_hdf_path = ras.project_folder / f\"{ras.project_name}.p{plan_number}.hdf\"\n",
        "\n",
        "if plan_hdf_path.exists():\n",
        "    print(f\"\\nResults for plan {plan_number} are already present at:\\n  {plan_hdf_path}\")\n",
        "else:\n",
        "    print(f\"\\nResults for plan {plan_number} not found. \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate Stored Maps Using postprocess_stored_maps()\n",
        "\n",
        "The `postprocess_stored_maps()` function automates the process of creating raster outputs:\n",
        "\n",
        "1. Backs up the original plan and .rasmap files\n",
        "2. Modifies plan flags to only run floodplain mapping (not the full simulation)\n",
        "3. Adds stored map definitions to the .rasmap file\n",
        "4. Opens HEC-RAS for you to run the plan\n",
        "5. Restores original files after completion\n",
        "\n",
        "**Note:** HEC-RAS 6.x requires the GUI to be open for floodplain mapping. The function will open HEC-RAS and wait for you to close it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Backing up plan file C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01 to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.storedmap.bak\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Updating plan run flags for floodplain mapping for plan 01...\n",
            "2025-11-17 21:28:34 - ras_commander.RasPlan - INFO - Successfully updated run flags in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01 (flags modified: 4)\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Backing up rasmap file C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap to C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap.storedmap.bak\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Added 'WSE' stored map to results layer for plan 01.\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Added 'Depth' stored map to results layer for plan 01.\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Added 'Velocity' stored map to results layer for plan 01.\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Filtered terrains, keeping only 'Terrain'.\n",
            "2025-11-17 21:28:34 - ras_commander.RasMap - INFO - Using GUI automation to run floodplain mapping...\n",
            "2025-11-17 21:28:34 - ras_commander.RasGuiAutomation - INFO - Setting current plan to 01 in project file...\n",
            "2025-11-17 21:28:34 - ras_commander.RasPrj - INFO - Set current plan to p01 in C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.prj\n",
            "2025-11-17 21:28:34 - ras_commander.RasGuiAutomation - INFO - Current plan set to 01 in C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.prj\n",
            "2025-11-17 21:28:34 - ras_commander.RasGuiAutomation - INFO - Opening HEC-RAS...\n",
            "2025-11-17 21:28:34 - ras_commander.RasGuiAutomation - INFO - HEC-RAS opened with Process ID: 428556\n",
            "2025-11-17 21:28:34 - ras_commander.RasGuiAutomation - INFO - Waiting for HEC-RAS main window...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating stored maps for WSE, Depth, and Velocity...\n",
            "\n",
            "HEC-RAS will open. Please:\n",
            "1. Click 'Compute' (or use Run > Unsteady Flow Analysis)\n",
            "2. Wait for the floodplain mapping to complete\n",
            "3. Close HEC-RAS when finished\n",
            "\n",
            "The script will continue automatically after you close HEC-RAS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:28:47 - ras_commander.RasGuiAutomation - INFO - Found HEC-RAS main window: HEC-RAS 6.6\n",
            "2025-11-17 21:28:48 - ras_commander.RasGuiAutomation - INFO - Clicking 'Run > Unsteady Flow Analysis' menu...\n",
            "2025-11-17 21:28:49 - ras_commander.RasGuiAutomation - INFO - Clicked menu item ID: 47\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Looking for Unsteady Flow Analysis dialog...\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Found Unsteady Flow Analysis dialog\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Looking for Compute button...\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - WARNING - Could not find Compute button - user must click manually\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Trying keyboard shortcut as fallback...\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Sent Enter key to dialog\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Waiting for user to close HEC-RAS...\n",
            "2025-11-17 21:28:51 - ras_commander.RasGuiAutomation - INFO - Please monitor plan 01 execution and close HEC-RAS when complete\n",
            "2025-11-17 21:29:43 - ras_commander.RasGuiAutomation - INFO - HEC-RAS has been closed\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Floodplain mapping computation successful.\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Restoring original plan file from C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.p01.storedmap.bak\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Restoring original rasmap file from C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\Muncie.rasmap.storedmap.bak\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Successfully generated stored maps!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Generate stored maps for WSE, Depth, and Velocity\n",
        "print(\"Generating stored maps for WSE, Depth, and Velocity...\")\n",
        "print(\"\\nHEC-RAS will open. Please:\")\n",
        "print(\"1. Click 'Compute' (or use Run > Unsteady Flow Analysis)\")\n",
        "print(\"2. Wait for the floodplain mapping to complete\")\n",
        "print(\"3. Close HEC-RAS when finished\")\n",
        "print(\"\\nThe script will continue automatically after you close HEC-RAS.\")\n",
        "\n",
        "success = RasMap.postprocess_stored_maps(\n",
        "    plan_number=plan_number,\n",
        "    specify_terrain=target_terrain,\n",
        "    layers=['WSE', 'Depth', 'Velocity']  # Generate all three variables\n",
        ")\n",
        "\n",
        "if success:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Successfully generated stored maps!\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\nFailed to generate stored maps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Locate Results Using get_results_folder() and get_results_raster()\n",
        "\n",
        "Now that we've generated the raster outputs, we can use the new functions to locate them programmatically.\n",
        "\n",
        "### Understanding the Output Folder Structure\n",
        "\n",
        "HEC-RAS creates output folders based on the plan's **Short Identifier**. Windows folder naming replaces special characters (like `/`, `\\`, `:`, `*`, `?`, `\"`, `<`, `>`, `|`, `+`, ` `) with underscores (`_`).\n",
        "\n",
        "The `get_results_folder()` function handles this normalization automatically and finds the correct folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results folder: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n",
            "\n",
            "Files in results folder:\n",
            "  - Depth (Max).Terrain.muncie_clip.tif\n",
            "  - Depth (Max).vrt\n",
            "  - PostProcessing.hdf\n",
            "  - Velocity (Max).Terrain.muncie_clip.tif\n",
            "  - Velocity (Max).vrt\n",
            "  - WSE (Max).Terrain.muncie_clip.tif\n",
            "  - WSE (Max).vrt\n"
          ]
        }
      ],
      "source": [
        "# Get the results folder for this plan\n",
        "results_folder = RasMap.get_results_folder(plan_number)\n",
        "print(f\"Results folder: {results_folder}\")\n",
        "\n",
        "# List all files in the results folder\n",
        "print(\"\\nFiles in results folder:\")\n",
        "for file in sorted(results_folder.iterdir()):\n",
        "    print(f\"  - {file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Specific Raster Files\n",
        "\n",
        "The `get_results_raster()` function searches for VRT files matching a variable name. VRT (Virtual Raster) files are lightweight files that reference the underlying TIFF tiles.\n",
        "\n",
        "The function performs a **case-insensitive substring match**, so:\n",
        "- `\"WSE\"` will match `\"WSE (Max).vrt\"` or `\"WSE (10Pct).vrt\"`\n",
        "- `\"WSE (Max)\"` will match only `\"WSE (Max).vrt\"`\n",
        "\n",
        "If multiple files match, it will raise an error and show you all matching files so you can be more specific."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found matching VRT file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\\WSE (Max).vrt\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found matching VRT file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\\Depth (Max).vrt\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found matching VRT file: C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\\Velocity (Max).vrt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found raster files:\n",
            "  WSE:      WSE (Max).vrt\n",
            "  Depth:    Depth (Max).vrt\n",
            "  Velocity: Velocity (Max).vrt\n"
          ]
        }
      ],
      "source": [
        "# Get specific raster files\n",
        "wse_vrt = RasMap.get_results_raster(plan_number, \"WSE (Max)\")\n",
        "depth_vrt = RasMap.get_results_raster(plan_number, \"Depth (Max)\")\n",
        "velocity_vrt = RasMap.get_results_raster(plan_number, \"Velocity (Max)\")\n",
        "\n",
        "print(\"Found raster files:\")\n",
        "print(f\"  WSE:      {wse_vrt.name}\")\n",
        "print(f\"  Depth:    {depth_vrt.name}\")\n",
        "print(f\"  Velocity: {velocity_vrt.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demonstrating Flexible Plan Number Handling\n",
        "\n",
        "Both functions accept plan numbers in multiple formats for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n",
            "2025-11-17 21:29:43 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flexible plan number handling:\n",
            "  get_results_folder(1)     -> True\n",
            "  get_results_folder('01')  -> True\n",
            "  get_results_folder('001') -> True\n"
          ]
        }
      ],
      "source": [
        "# All of these work the same way:\n",
        "folder1 = RasMap.get_results_folder(1)          # Integer\n",
        "folder2 = RasMap.get_results_folder(\"01\")       # Two-digit string\n",
        "folder3 = RasMap.get_results_folder(\"001\")      # Three-digit string\n",
        "\n",
        "print(\"Flexible plan number handling:\")\n",
        "print(f\"  get_results_folder(1)     -> {folder1 == results_folder}\")\n",
        "print(f\"  get_results_folder('01')  -> {folder2 == results_folder}\")\n",
        "print(f\"  get_results_folder('001') -> {folder3 == results_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Visualize the Results\n",
        "\n",
        "Now let's load and visualize the raster files we've located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import numpy as np\n",
        "\n",
        "def get_valid_data_bounds(src) -> tuple:\n",
        "    \"\"\"\n",
        "    Returns the bounding box of valid (non-nodata) values in raster coordinates,\n",
        "    expanded by 10% on all sides and transformed to spatial coordinates.\n",
        "    \"\"\"\n",
        "    data = src.read(1, masked=True)\n",
        "    # Find where data is not nodata\n",
        "    valid_mask = np.ma.getmaskarray(data) == 0\n",
        "    rows, cols = np.where(valid_mask)\n",
        "    if rows.size == 0 or cols.size == 0:\n",
        "        # No data found, fallback to raster extents\n",
        "        bounds = src.bounds\n",
        "        return bounds.left, bounds.right, bounds.bottom, bounds.top\n",
        "\n",
        "    # Find min/max in pixel coordinates\n",
        "    row_min, row_max = rows.min(), rows.max()\n",
        "    col_min, col_max = cols.min(), cols.max()\n",
        "\n",
        "    # Transform corners to map coordinates\n",
        "    top_left = src.transform * (col_min, row_min)\n",
        "    bottom_right = src.transform * (col_max + 1, row_max + 1)  # +1 to include the edge\n",
        "\n",
        "    xmin = top_left[0]\n",
        "    ymax = top_left[1]\n",
        "    xmax = bottom_right[0]\n",
        "    ymin = bottom_right[1]\n",
        "\n",
        "    # Expand bounds by 10%\n",
        "    width = xmax - xmin\n",
        "    height = ymax - ymin\n",
        "    xpad = width * 0.10 / 2.0\n",
        "    ypad = height * 0.10 / 2.0\n",
        "\n",
        "    return (\n",
        "        xmin - xpad,\n",
        "        xmax + xpad,\n",
        "        ymin - ypad,\n",
        "        ymax + ypad\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_raster_with_valid_bounds(raster_path, cmap, title):\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        valid_bounds = get_valid_data_bounds(src)\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        img = ax.imshow(\n",
        "            src.read(1, masked=True),\n",
        "            cmap=cmap,\n",
        "            extent=[src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top],\n",
        "            origin='upper'\n",
        "        )\n",
        "        ax.set_xlim(valid_bounds[0], valid_bounds[1])\n",
        "        ax.set_ylim(valid_bounds[2], valid_bounds[3])\n",
        "        ax.set_xlabel('Easting')\n",
        "        ax.set_ylabel('Northing')\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Plot WSE individually (zoom to actual data mask)\n",
        "plot_raster_with_valid_bounds(\n",
        "    wse_vrt, cmap='terrain', title='Maximum Water Surface Elevation'\n",
        ")\n",
        "\n",
        "# Plot Depth individually (zoom to actual data mask)\n",
        "plot_raster_with_valid_bounds(\n",
        "    depth_vrt, cmap='Blues', title='Maximum Depth'\n",
        ")\n",
        "\n",
        "# Plot Velocity individually (zoom to actual data mask)\n",
        "plot_raster_with_valid_bounds(\n",
        "    velocity_vrt, cmap='YlOrRd', title='Maximum Velocity'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Raster Metadata\n",
        "\n",
        "We can use rasterio to examine the raster properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depth Raster Metadata:\n",
            "  CRS:        PROJCS[\"NAD83 / Indiana East (ftUS)\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101004,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",37.5],PARAMETER[\"central_meridian\",-85.6666666666667],PARAMETER[\"scale_factor\",0.999966666666667],PARAMETER[\"false_easting\",328083.333333333],PARAMETER[\"false_northing\",820208.333333333],UNIT[\"US survey foot\",0.304800609601219,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n",
            "  Dimensions: 7892 x 4538\n",
            "  Resolution: (5.0, 5.0)\n",
            "  Bounds:     BoundingBox(left=384977.84867792, bottom=1788863.5402636, right=424437.84867792, top=1811553.5402636)\n",
            "  NoData:     -9999.0\n",
            "  Data Type:  float32\n"
          ]
        }
      ],
      "source": [
        "# Examine depth raster properties\n",
        "with rasterio.open(depth_vrt) as src:\n",
        "    print(\"Depth Raster Metadata:\")\n",
        "    print(f\"  CRS:        {src.crs}\")\n",
        "    print(f\"  Dimensions: {src.width} x {src.height}\")\n",
        "    print(f\"  Resolution: {src.res}\")\n",
        "    print(f\"  Bounds:     {src.bounds}\")\n",
        "    print(f\"  NoData:     {src.nodata}\")\n",
        "    print(f\"  Data Type:  {src.dtypes[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Working with Multiple Plans\n",
        "\n",
        "The `get_results_folder()` and `get_results_raster()` functions make it easy to work with multiple plans programmatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 21:29:58 - ras_commander.RasMap - INFO - Found output folder (exact match): C:\\GH\\ras-commander\\examples\\example_projects\\Muncie\\9-SAs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results folders for all plans:\n",
            "============================================================\n",
            "Plan 01 (9-SAs):\n",
            "  Folder: 9-SAs\n",
            "  VRT files: 3\n",
            "    - Depth (Max).vrt\n",
            "    - Velocity (Max).vrt\n",
            "    - WSE (Max).vrt\n",
            "\n",
            "Plan 03 (2D 50ft Grid): Not computed\n",
            "\n",
            "Plan 04 (50ft User n Regions): Not computed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get results for all computed plans\n",
        "print(\"Results folders for all plans:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for idx, row in ras.plan_df.iterrows():\n",
        "    plan_num = row['plan_number']\n",
        "    short_id = row['Short Identifier']\n",
        "    \n",
        "    # Check if HDF results exist\n",
        "    hdf_path = ras.project_folder / f\"{ras.project_name}.p{plan_num}.hdf\"\n",
        "    if hdf_path.exists():\n",
        "        try:\n",
        "            folder = RasMap.get_results_folder(plan_num)\n",
        "            print(f\"Plan {plan_num} ({short_id}):\")\n",
        "            print(f\"  Folder: {folder.name}\")\n",
        "            \n",
        "            # Count VRT files\n",
        "            vrt_files = list(folder.glob(\"*.vrt\"))\n",
        "            print(f\"  VRT files: {len(vrt_files)}\")\n",
        "            for vrt in vrt_files:\n",
        "                print(f\"    - {vrt.name}\")\n",
        "            print()\n",
        "        except ValueError as e:\n",
        "            print(f\"Plan {plan_num} ({short_id}): No results folder (likely no stored maps)\")\n",
        "            print()\n",
        "    else:\n",
        "        print(f\"Plan {plan_num} ({short_id}): Not computed\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete RASMapper workflow:\n",
        "\n",
        "1. **Project Initialization**: Loaded project and examined `rasmap_df` configuration\n",
        "2. **Terrain Discovery**: Used `get_terrain_names()` to find available DEMs\n",
        "3. **Stored Map Generation**: Used `postprocess_stored_maps()` to create WSE, Depth, and Velocity rasters\n",
        "4. **Results Location**: Used `get_results_folder()` and `get_results_raster()` to find outputs\n",
        "5. **Visualization**: Loaded and displayed raster files with rasterio\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- The `rasmap_df` provides programmatic access to all RASMapper configuration\n",
        "- `postprocess_stored_maps()` automates the tedious process of generating raster outputs\n",
        "- `get_results_folder()` handles Windows folder naming normalization automatically\n",
        "- `get_results_raster()` provides smart matching for finding specific variable outputs\n",
        "- Both functions accept flexible plan number formats (1, \"01\", \"001\")\n",
        "- VRT files are lightweight references to underlying TIFF tiles\n",
        "\n",
        "These functions significantly streamline workflows involving raster output generation and processing for multiple plans or scenarios."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\22_dss_boundary_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:38.035942Z",
          "iopub.status.busy": "2025-11-17T17:46:38.035768Z",
          "iopub.status.idle": "2025-11-17T17:46:39.238862Z",
          "shell.execute_reply": "2025-11-17T17:46:39.238068Z"
        }
      },
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DSS Boundary Condition Extraction\n",
        "\n",
        "This notebook demonstrates how to extract DSS (Data Storage System) boundary condition data from HEC-RAS projects using ras-commander.\n",
        "\n",
        "**Example Project**: BaldEagleCrkMulti2D, Plan 07\n",
        "\n",
        "## Features Demonstrated\n",
        "- Reading DSS file catalogs\n",
        "- Extracting individual time series\n",
        "- Automatic boundary condition extraction\n",
        "- Plotting DSS boundary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:39.241141Z",
          "iopub.status.busy": "2025-11-17T17:46:39.240862Z",
          "iopub.status.idle": "2025-11-17T17:46:40.918172Z",
          "shell.execute_reply": "2025-11-17T17:46:40.917709Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyjnius in c:\\users\\billk_clb\\anaconda3\\envs\\rascmdr_local\\lib\\site-packages (1.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyjnius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:40.920260Z",
          "iopub.status.busy": "2025-11-17T17:46:40.920101Z",
          "iopub.status.idle": "2025-11-17T17:46:40.924671Z",
          "shell.execute_reply": "2025-11-17T17:46:40.924113Z"
        }
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flexible imports for development vs installed package\n",
        "try:\n",
        "    from ras_commander import init_ras_project, RasExamples\n",
        "    from ras_commander import RasDss\n",
        "except ImportError:\n",
        "    current_file = Path.cwd()\n",
        "    parent_directory = current_file.parent\n",
        "    sys.path.append(str(parent_directory))\n",
        "    from ras_commander import init_ras_project, RasExamples\n",
        "    from ras_commander.RasDss import RasDss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Extract Example Project\n",
        "\n",
        "Extract the Bald Eagle Creek Multi-2D example project which contains DSS boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:40.926914Z",
          "iopub.status.busy": "2025-11-17T17:46:40.926567Z",
          "iopub.status.idle": "2025-11-17T17:46:42.397243Z",
          "shell.execute_reply": "2025-11-17T17:46:42.396709Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - Extracting project 'BaldEagleCrkMulti2D'\n",
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - Project 'BaldEagleCrkMulti2D' already exists. Deleting existing folder...\n",
            "2025-12-02 13:46:59 - ras_commander.RasExamples - INFO - Existing folder for project 'BaldEagleCrkMulti2D' has been deleted.\n",
            "2025-12-02 13:47:00 - ras_commander.RasExamples - INFO - Successfully extracted project 'BaldEagleCrkMulti2D' to c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project extracted to: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\n"
          ]
        }
      ],
      "source": [
        "# Extract BaldEagleCrkMulti2D example project\n",
        "project_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\")\n",
        "print(f\"Project extracted to: {project_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize Project\n",
        "\n",
        "Initialize the HEC-RAS project to access boundary condition data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:42.399692Z",
          "iopub.status.busy": "2025-11-17T17:46:42.399166Z",
          "iopub.status.idle": "2025-11-17T17:46:42.514823Z",
          "shell.execute_reply": "2025-11-17T17:46:42.514207Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 13:47:00 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\BaldEagleDamBrk.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Project: BaldEagleDamBrk\n",
            "Plans: 11\n",
            "Boundaries: 51\n"
          ]
        }
      ],
      "source": [
        "# Initialize project\n",
        "ras = init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "print(f\"\\nProject: {ras.project_name}\")\n",
        "print(f\"Plans: {len(ras.plan_df)}\")\n",
        "print(f\"Boundaries: {len(ras.boundaries_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examine Boundary Conditions\n",
        "\n",
        "View boundary conditions and identify which are defined by DSS files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:42.517421Z",
          "iopub.status.busy": "2025-11-17T17:46:42.517189Z",
          "iopub.status.idle": "2025-11-17T17:46:42.523928Z",
          "shell.execute_reply": "2025-11-17T17:46:42.523454Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan 07 has 10 boundary conditions\n",
            "\n",
            "DSS-defined boundaries: 0\n",
            "\n",
            "Empty DataFrame\n",
            "Columns: [boundary_condition_number, bc_type, Use DSS, DSS File, DSS Path]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Show boundaries for plan 07\n",
        "plan_07_boundaries = ras.boundaries_df[ras.boundaries_df['unsteady_number'] == '07'].copy()\n",
        "\n",
        "print(f\"Plan 07 has {len(plan_07_boundaries)} boundary conditions\\n\")\n",
        "\n",
        "# Show DSS-defined boundaries\n",
        "dss_boundaries = plan_07_boundaries[plan_07_boundaries['Use DSS'] == True]\n",
        "print(f\"DSS-defined boundaries: {len(dss_boundaries)}\\n\")\n",
        "\n",
        "# Display key columns\n",
        "display_cols = ['boundary_condition_number', 'bc_type', 'Use DSS', 'DSS File', 'DSS Path']\n",
        "print(dss_boundaries[display_cols].to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Inspect DSS File\n",
        "\n",
        "Examine what's in the DSS file before extracting data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:42.526425Z",
          "iopub.status.busy": "2025-11-17T17:46:42.526099Z",
          "iopub.status.idle": "2025-11-17T17:46:43.007655Z",
          "shell.execute_reply": "2025-11-17T17:46:43.007161Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring Java VM for DSS operations...\n",
            "  Found Java: C:\\Program Files\\Java\\jre1.8.0_471\n",
            "[OK] Java VM configured\n",
            "DSS File: Bald_Eagle_Creek.dss\n",
            "Size: 29.27 MB\n",
            "Total paths: 1270\n",
            "\n",
            "First 10 paths:\n",
            "  1. //BALD EAGLE AT MILESBURG/FLOW/01SEP2004/15MIN/GAGE/\n",
            "  2. //FISHING CREEK/FLOW-BASE/01JUN1972/15MIN/RUN:1972 CALIBRATION EVENT/\n",
            "  3. //LOCAL DOWNSTREAM OF DAM/FLOW/01JUN1972/15MIN/RUN:1972 CALIBRATION EVENT/\n",
            "  4. //MARSH CREEK/INFILTRATION/01JAN1996/15MIN/RUN:1996 CALIBRATION EVENT/\n",
            "  5. //SAYERS - ELEVATION-STORAGE/ELEVATION-STORAGE///TABLE/\n",
            "  6. //MARSH CREEK GAGE/FLOW-OBSERVED/01JUN1972/15MIN/RUN:1972 CALIBRATION EVENT/\n",
            "  7. //BALD EAGLE AT MILESBURG/FLOW-OBSERVED/01JUN1972/15MIN/RUN:1972 CALIBRATION EVENT/\n",
            "  8. //MARSH CREEK/PRECIP-INC/01JAN2000/15MIN/DAA:1% EVENT>BALD EAGLE BL BEECH/\n",
            "  9. //BASIN TEMPERATURE/TEMPERATURE/01JUN1972/1HOUR/GAGE/\n",
            "  10. //BALD EAGLE HW/INFILTRATION/01JAN2000/15MIN/RUN:1% EVENT/\n"
          ]
        }
      ],
      "source": [
        "# Get DSS file path\n",
        "dss_file = project_path / \"Bald_Eagle_Creek.dss\"\n",
        "\n",
        "if dss_file.exists():\n",
        "    # Get file info\n",
        "    info = RasDss.get_info(dss_file)\n",
        "    \n",
        "    print(f\"DSS File: {info['filename']}\")\n",
        "    print(f\"Size: {info['file_size_mb']:.2f} MB\")\n",
        "    print(f\"Total paths: {info['total_paths']}\")\n",
        "    \n",
        "    print(f\"\\nFirst 10 paths:\")\n",
        "    catalog = RasDss.get_catalog(dss_file)\n",
        "    for i, path in enumerate(catalog[:10]):\n",
        "        print(f\"  {i+1}. {path}\")\n",
        "else:\n",
        "    print(f\"DSS file not found: {dss_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Extract Single Time Series\n",
        "\n",
        "Demonstrate extracting a single DSS time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:43.010199Z",
          "iopub.status.busy": "2025-11-17T17:46:43.009777Z",
          "iopub.status.idle": "2025-11-17T17:46:43.020258Z",
          "shell.execute_reply": "2025-11-17T17:46:43.019760Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>boundary_condition_number</th>\\n', '      <th>river_reach_name</th>\\n', '      <th>river_station</th>\\n', '      <th>storage_area_name</th>\\n', '      <th>pump_station_name</th>\\n', '      <th>bc_type</th>\\n', '      <th>hydrograph_type</th>\\n', '      <th>Interval</th>\\n', '      <th>DSS File</th>\\n', '      <th>...</th>\\n', '      <th>Flow Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Use Restart</th>\\n', '      <th>Precipitation Mode</th>\\n', '      <th>Wind Mode</th>\\n', '      <th>Met BC=Precipitation|Mode</th>\\n', '      <th>Met BC=Evapotranspiration|Mode</th>\\n', '      <th>Met BC=Precipitation|Expanded View</th>\\n', '      <th>Met BC=Precipitation|Constant Units</th>\\n', '      <th>Met BC=Precipitation|Gridded Source</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    \n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "['Empty DataFrame\\n', 'Columns: [unsteady_number, boundary_condition_number, river_reach_name, river_station, storage_area_name, pump_station_name, bc_type, hydrograph_type, Interval, DSS File, DSS Path, Use DSS, Use Fixed Start Time, Fixed Start Date/Time, Is Critical Boundary, Critical Boundary Flow, hydrograph_num_values, hydrograph_values, full_path, Flow Title, Program Version, Use Restart, Precipitation Mode, Wind Mode, Met BC=Precipitation|Mode, Met BC=Evapotranspiration|Mode, Met BC=Precipitation|Expanded View, Met BC=Precipitation|Constant Units, Met BC=Precipitation|Gridded Source]\\n', 'Index: []\\n', '\\n', '[0 rows x 29 columns]']"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dss_boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:43.022529Z",
          "iopub.status.busy": "2025-11-17T17:46:43.022259Z",
          "iopub.status.idle": "2025-11-17T17:46:43.026791Z",
          "shell.execute_reply": "2025-11-17T17:46:43.026288Z"
        }
      },
      "outputs": [],
      "source": [
        "if len(dss_boundaries) > 0:\n",
        "    # Get first DSS boundary\n",
        "    first_dss = dss_boundaries.iloc[0]\n",
        "    \n",
        "    print(f\"Boundary Type: {first_dss['bc_type']}\")\n",
        "    print(f\"DSS Path: {first_dss['DSS Path']}\")\n",
        "    \n",
        "    # Extract time series\n",
        "    dss_file_path = project_path / first_dss['DSS File']\n",
        "    df_ts = RasDss.read_timeseries(dss_file_path, first_dss['DSS Path'])\n",
        "    \n",
        "    print(f\"\\nExtracted {len(df_ts)} data points\")\n",
        "    print(f\"Date range: {df_ts.index.min()} to {df_ts.index.max()}\")\n",
        "    print(f\"Value range: {df_ts['value'].min():.2f} to {df_ts['value'].max():.2f}\")\n",
        "    print(f\"Units: {df_ts.attrs.get('units', 'N/A')}\")\n",
        "    \n",
        "    # Display first/last rows\n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    display(df_ts.head())\n",
        "    \n",
        "    print(f\"\\nLast 5 rows:\")\n",
        "    display(df_ts.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Plot Time Series\n",
        "\n",
        "Visualize the extracted DSS boundary condition data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:43.029190Z",
          "iopub.status.busy": "2025-11-17T17:46:43.028925Z",
          "iopub.status.idle": "2025-11-17T17:46:43.159527Z",
          "shell.execute_reply": "2025-11-17T17:46:43.158990Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Found 7 DSS-defined boundaries\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 0: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 2: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 4: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 5: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 6: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 7: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Row 8: Extracted 673 points from Bald_Eagle_Creek.dss\n",
            "2025-12-02 13:47:01 - ras_commander.dss.RasDss - INFO - Extraction complete: 7 success, 0 failed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extraction complete!\n",
            "Total boundaries: 10\n",
            "DSS-defined: 7\n",
            "\n",
            "DSS Boundary Summary:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Flow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //BALD EAGLE 40/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 719.78 to 193738.20 CFS\n",
            "\n",
            "Lateral Inflow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //FISHING CREEK/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 345.89 to 28510.08 CFS\n",
            "\n",
            "Uniform Lateral Inflow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //RESERVOIR LOCAL/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 209.15 to 75262.30 CFS\n",
            "\n",
            "Uniform Lateral Inflow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //LOCAL DOWNSTREAM OF DAM/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 9.98 to 5568.15 CFS\n",
            "\n",
            "Lateral Inflow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //MARSH CREEK/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 94.61 to 37820.33 CFS\n",
            "\n",
            "Lateral Inflow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //BEECH CREEK FLOW/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 382.69 to 32872.19 CFS\n",
            "\n",
            "Uniform Lateral Inflow Hydrograph:\n",
            "  Location: Bald Eagle Cr. RS Lock Haven\n",
            "  DSS Path: //BALD EAGLE LOCAL/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  Points: 673\n",
            "  Date range: 1999-01-01 00:00:00 to 1999-01-08 00:00:00\n",
            "  Value range: 100.19 to 27428.17 CFS\n"
          ]
        }
      ],
      "source": [
        "# Extract all DSS boundary time series for plan 07\n",
        "enhanced_boundaries = RasDss.extract_boundary_timeseries(\n",
        "    plan_07_boundaries, \n",
        "    ras_object=ras\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(f\"\\nExtraction complete!\")\n",
        "print(f\"Total boundaries: {len(enhanced_boundaries)}\")\n",
        "\n",
        "# Count DSS-defined boundaries (handle string 'True')\n",
        "dss_count = ((enhanced_boundaries['Use DSS'] == True) | (enhanced_boundaries['Use DSS'] == 'True')).sum()\n",
        "print(f\"DSS-defined: {dss_count}\")\n",
        "\n",
        "# Show extracted data summary\n",
        "print(f\"\\nDSS Boundary Summary:\")\n",
        "print(\"-\" * 80)\n",
        "for idx, row in enhanced_boundaries.iterrows():\n",
        "    # Check if DSS boundary (handle string 'True')\n",
        "    is_dss = (row['Use DSS'] == True) or (row['Use DSS'] == 'True')\n",
        "    if is_dss and row['dss_timeseries'] is not None:\n",
        "        df = row['dss_timeseries']\n",
        "        print(f\"\\n{row['bc_type']}:\")\n",
        "        print(f\"  Location: {row['river_reach_name']} RS {row['river_station']}\")\n",
        "        print(f\"  DSS Path: {row['DSS Path']}\")\n",
        "        print(f\"  Points: {len(df)}\")\n",
        "        print(f\"  Date range: {df.index.min()} to {df.index.max()}\")\n",
        "        print(f\"  Value range: {df['value'].min():.2f} to {df['value'].max():.2f} {df.attrs.get('units', '')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Extract ALL Boundary DSS Data\n",
        "\n",
        "Use the `extract_boundary_timeseries()` function to automatically extract ALL DSS boundary data in one call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:43.161938Z",
          "iopub.status.busy": "2025-11-17T17:46:43.161649Z",
          "iopub.status.idle": "2025-11-17T17:46:43.169344Z",
          "shell.execute_reply": "2025-11-17T17:46:43.168886Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boundary Definition Summary:\n",
            "================================================================================\n",
            "\n",
            "Manual boundaries (defined in .u## file): 3\n",
            "  Types:\n",
            "    - Gate Opening: 1\n",
            "    - Lateral Inflow Hydrograph: 1\n",
            "    - Normal Depth: 1\n",
            "\n",
            "DSS boundaries (defined in DSS file): 7\n",
            "  Types:\n",
            "    - Lateral Inflow Hydrograph: 3\n",
            "    - Uniform Lateral Inflow Hydrograph: 3\n",
            "    - Flow Hydrograph: 1\n",
            "\n",
            "Total boundaries: 10\n"
          ]
        }
      ],
      "source": [
        "# Count boundary types\n",
        "print(\"Boundary Definition Summary:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Handle both string and boolean 'Use DSS' values\n",
        "manual_bc = enhanced_boundaries[\n",
        "    (enhanced_boundaries['Use DSS'] == False) | (enhanced_boundaries['Use DSS'] == 'False') | (enhanced_boundaries['Use DSS'].isna())\n",
        "]\n",
        "dss_bc = enhanced_boundaries[\n",
        "    (enhanced_boundaries['Use DSS'] == True) | (enhanced_boundaries['Use DSS'] == 'True')\n",
        "]\n",
        "\n",
        "print(f\"\\nManual boundaries (defined in .u## file): {len(manual_bc)}\")\n",
        "if len(manual_bc) > 0:\n",
        "    print(\"  Types:\")\n",
        "    for bc_type, count in manual_bc['bc_type'].value_counts().items():\n",
        "        print(f\"    - {bc_type}: {count}\")\n",
        "\n",
        "print(f\"\\nDSS boundaries (defined in DSS file): {len(dss_bc)}\")\n",
        "if len(dss_bc) > 0:\n",
        "    print(\"  Types:\")\n",
        "    for bc_type, count in dss_bc['bc_type'].value_counts().items():\n",
        "        print(f\"    - {bc_type}: {count}\")\n",
        "\n",
        "print(f\"\\nTotal boundaries: {len(enhanced_boundaries)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Compare Manual vs DSS Boundaries\n",
        "\n",
        "Show the difference between manually-defined and DSS-defined boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:43.171887Z",
          "iopub.status.busy": "2025-11-17T17:46:43.171588Z",
          "iopub.status.idle": "2025-11-17T17:46:44.304607Z",
          "shell.execute_reply": "2025-11-17T17:46:44.304096Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1400x2800 with 7 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get all successfully extracted DSS boundaries\n",
        "successful_dss = enhanced_boundaries[\n",
        "    ((enhanced_boundaries['Use DSS'] == True) | (enhanced_boundaries['Use DSS'] == 'True')) & \n",
        "    (enhanced_boundaries['dss_timeseries'].notna())\n",
        "]\n",
        "\n",
        "if len(successful_dss) > 0:\n",
        "    # Create subplots\n",
        "    n_plots = len(successful_dss)\n",
        "    fig, axes = plt.subplots(n_plots, 1, figsize=(14, 4*n_plots))\n",
        "    \n",
        "    if n_plots == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ax, (idx, row) in zip(axes, successful_dss.iterrows()):\n",
        "        df = row['dss_timeseries']\n",
        "        \n",
        "        # Plot\n",
        "        df['value'].plot(ax=ax, linewidth=1.5, color='steelblue')\n",
        "        \n",
        "        # Format\n",
        "        title = f\"{row['bc_type']} - {row['river_reach_name']} RS {row['river_station']}\"\n",
        "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
        "        ax.set_xlabel('Date/Time', fontsize=9)\n",
        "        ax.set_ylabel(f\"Flow ({df.attrs.get('units', '')})\", fontsize=9)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add DSS path as text\n",
        "        ax.text(0.02, 0.98, f\"DSS: {row['DSS Path']}\", \n",
        "                transform=ax.transAxes, fontsize=8, \n",
        "                verticalalignment='top', family='monospace',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No successful DSS extractions to plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Plot Multiple DSS Boundaries\n",
        "\n",
        "Create a multi-panel plot showing all DSS boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:44.308678Z",
          "iopub.status.busy": "2025-11-17T17:46:44.308375Z",
          "iopub.status.idle": "2025-11-17T17:46:44.326171Z",
          "shell.execute_reply": "2025-11-17T17:46:44.325708Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported to: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\boundaries_with_dss_summary.csv\n",
            "\n",
            "DSS Boundary Statistics:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>bc_type</th>\\n', '      <th>Use DSS</th>\\n', '      <th>dss_points</th>\\n', '      <th>dss_mean</th>\\n', '      <th>dss_max</th>\\n', '      <th>dss_min</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>Flow Hydrograph</td>\\n', '      <td>True</td>\\n', '      <td>673.0</td>\\n', '      <td>23749.776843</td>\\n', '      <td>193738.197396</td>\\n', '      <td>719.775321</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>Lateral Inflow Hydrograph</td>\\n', '      <td>True</td>\\n', '      <td>673.0</td>\\n', '      <td>7554.055251</td>\\n', '      <td>28510.083069</td>\\n', '      <td>345.889757</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>Uniform Lateral Inflow Hydrograph</td>\\n', '      <td>True</td>\\n', '      <td>673.0</td>\\n', '      <td>6448.671063</td>\\n', '      <td>75262.300507</td>\\n', '      <td>209.150354</td>\\n', '    </tr><tr>\\n', '      <th>5</th>\\n', '      <td>Uniform Lateral Inflow Hydrograph</td>\\n', '      <td>True</td>\\n', '      <td>673.0</td>\\n', '      <td>539.962137</td>\\n', '      <td>5568.152787</td>\\n', '      <td>9.979876</td>\\n', '    </tr><tr>\\n', '      <th>6</th>\\n', '      <td>Lateral Inflow Hydrograph</td>\\n', '      <td>True</td>\\n', '      <td>673.0</td>\\n', '      <td>4343.093777</td>\\n', '      <td>37820.325998</td>\\n', '      <td>94.606990</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "                             bc_type Use DSS  dss_points      dss_mean  \\\n",
              "0                    Flow Hydrograph    True       673.0  23749.776843   \n",
              "2          Lateral Inflow Hydrograph    True       673.0   7554.055251   \n",
              "4  Uniform Lateral Inflow Hydrograph    True       673.0   6448.671063   \n",
              "5  Uniform Lateral Inflow Hydrograph    True       673.0    539.962137   \n",
              "6          Lateral Inflow Hydrograph    True       673.0   4343.093777   \n",
              "7          Lateral Inflow Hydrograph    True       673.0   6971.806773   \n",
              "8  Uniform Lateral Inflow Hydrograph    True       673.0   2710.452795   \n",
              "\n",
              "         dss_max     dss_min  \n",
              "0  193738.197396  719.775321  \n",
              "2   28510.083069  345.889757  \n",
              "4   75262.300507  209.150354  \n",
              "5    5568.152787    9.979876  \n",
              "6   37820.325998   94.606990  \n",
              "7   32872.193876  382.693009  \n",
              "8   27428.172923  100.189004  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Export to CSV (without the DataFrame column)\n",
        "export_df = enhanced_boundaries.drop(columns=['dss_timeseries']).copy()\n",
        "\n",
        "# Add summary statistics for DSS boundaries\n",
        "for idx, row in enhanced_boundaries.iterrows():\n",
        "    is_dss = (row['Use DSS'] == True) or (row['Use DSS'] == 'True')\n",
        "    if is_dss and row['dss_timeseries'] is not None:\n",
        "        df = row['dss_timeseries']\n",
        "        export_df.at[idx, 'dss_points'] = len(df)\n",
        "        export_df.at[idx, 'dss_mean'] = df['value'].mean()\n",
        "        export_df.at[idx, 'dss_max'] = df['value'].max()\n",
        "        export_df.at[idx, 'dss_min'] = df['value'].min()\n",
        "\n",
        "# Save\n",
        "output_file = project_path / \"boundaries_with_dss_summary.csv\"\n",
        "export_df.to_csv(output_file, index=False)\n",
        "print(f\"Exported to: {output_file}\")\n",
        "\n",
        "# Show summary\n",
        "dss_summary_cols = ['bc_type', 'Use DSS', 'dss_points', 'dss_mean', 'dss_max', 'dss_min']\n",
        "available_cols = [c for c in dss_summary_cols if c in export_df.columns]\n",
        "print(f\"\\nDSS Boundary Statistics:\")\n",
        "\n",
        "# Filter for DSS boundaries\n",
        "dss_summary = export_df[\n",
        "    (export_df['Use DSS'] == True) | (export_df['Use DSS'] == 'True')\n",
        "][available_cols]\n",
        "display(dss_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Export Boundary Data\n",
        "\n",
        "Save extracted boundary condition data for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:44.328268Z",
          "iopub.status.busy": "2025-11-17T17:46:44.328039Z",
          "iopub.status.idle": "2025-11-17T17:46:44.338104Z",
          "shell.execute_reply": "2025-11-17T17:46:44.337573Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported to: c:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\boundaries_with_dss_summary.csv\n",
            "\n",
            "DSS Boundary Statistics:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>bc_type</th>\\n', '      <th>Use DSS</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    \n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [bc_type, Use DSS]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Export to CSV (without the DataFrame column)\n",
        "export_df = enhanced_boundaries.drop(columns=['dss_timeseries']).copy()\n",
        "\n",
        "# Add summary statistics for DSS boundaries\n",
        "for idx, row in enhanced_boundaries[enhanced_boundaries['Use DSS'] == True].iterrows():\n",
        "    if row['dss_timeseries'] is not None:\n",
        "        df = row['dss_timeseries']\n",
        "        export_df.at[idx, 'dss_points'] = len(df)\n",
        "        export_df.at[idx, 'dss_mean'] = df['value'].mean()\n",
        "        export_df.at[idx, 'dss_max'] = df['value'].max()\n",
        "        export_df.at[idx, 'dss_min'] = df['value'].min()\n",
        "\n",
        "# Save\n",
        "output_file = project_path / \"boundaries_with_dss_summary.csv\"\n",
        "export_df.to_csv(output_file, index=False)\n",
        "print(f\"Exported to: {output_file}\")\n",
        "\n",
        "# Show summary\n",
        "dss_summary_cols = ['bc_type', 'Use DSS', 'dss_points', 'dss_mean', 'dss_max', 'dss_min']\n",
        "available_cols = [c for c in dss_summary_cols if c in export_df.columns]\n",
        "print(f\"\\nDSS Boundary Statistics:\")\n",
        "display(export_df[export_df['Use DSS'] == True][available_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Access Individual DSS Time Series\n",
        "\n",
        "Access extracted data from the enhanced boundaries_df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T17:46:44.340353Z",
          "iopub.status.busy": "2025-11-17T17:46:44.340113Z",
          "iopub.status.idle": "2025-11-17T17:46:44.347833Z",
          "shell.execute_reply": "2025-11-17T17:46:44.347307Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessing DSS data for boundary 0:\n",
            "  Type: Flow Hydrograph\n",
            "  Data points: 673\n",
            "\n",
            "Data statistics:\n",
            "count       673.000000\n",
            "mean      23749.776843\n",
            "std       42452.303066\n",
            "min         719.775321\n",
            "25%        4332.272683\n",
            "50%        7792.191249\n",
            "75%       14070.993090\n",
            "max      193738.197396\n",
            "Name: value, dtype: float64\n",
            "\n",
            "Metadata:\n",
            "  pathname: //BALD EAGLE 40/FLOW/01JAN1999/15MIN/RUN:PMF-EVENT/\n",
            "  units: CFS\n",
            "  type: INST-VAL\n",
            "  interval: 15\n",
            "  dss_file: C:\\GH\\ras-commander\\examples\\example_projects\\BaldEagleCrkMulti2D\\Bald_Eagle_Creek.dss\n"
          ]
        }
      ],
      "source": [
        "# Access specific boundary by index\n",
        "if len(successful_dss) > 0:\n",
        "    # Get first successful DSS boundary\n",
        "    idx = successful_dss.index[0]\n",
        "    boundary_data = enhanced_boundaries.loc[idx, 'dss_timeseries']\n",
        "    \n",
        "    print(f\"Accessing DSS data for boundary {idx}:\")\n",
        "    print(f\"  Type: {enhanced_boundaries.loc[idx, 'bc_type']}\")\n",
        "    print(f\"  Data points: {len(boundary_data)}\")\n",
        "    \n",
        "    # Show statistics\n",
        "    print(f\"\\nData statistics:\")\n",
        "    print(boundary_data['value'].describe())\n",
        "    \n",
        "    # Access metadata\n",
        "    print(f\"\\nMetadata:\")\n",
        "    for key, value in boundary_data.attrs.items():\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. \u2705 Reading DSS file catalogs\n",
        "2. \u2705 Extracting individual time series from DSS files\n",
        "3. \u2705 Automatic extraction of ALL DSS boundary data with `extract_boundary_timeseries()`\n",
        "4. \u2705 Plotting and analyzing DSS data\n",
        "5. \u2705 Exporting results\n",
        "\n",
        "### Key Features\n",
        "- **Unified API** - Same DataFrame structure for manual and DSS boundaries\n",
        "- **Automatic extraction** - One function call extracts all DSS data\n",
        "- **V6 and V7 support** - Works with both DSS formats\n",
        "- **Auto-download** - HEC Monolith libraries downloaded automatically on first use\n",
        "\n",
        "### Next Steps\n",
        "- Use extracted data for boundary condition analysis\n",
        "- Compare DSS vs manual boundary definitions\n",
        "- Modify DSS data and write back to files (future enhancement)\n",
        "- Integrate DSS data with HEC-RAS model workflows"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\23_remote_execution_psexec.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remote Execution with PsExec\n",
        "\n",
        "This notebook demonstrates how to execute HEC-RAS plans on remote Windows machines using PsExec.\n",
        "\n",
        "**Features:**\n",
        "- Distributed execution across multiple remote machines\n",
        "- Automatic project deployment via network shares\n",
        "- Parallel execution with configurable workers\n",
        "- Result collection and consolidation\n",
        "- **Automatic PsExec.exe download** (no manual setup required)\n",
        "\n",
        "**Requirements:**\n",
        "- Remote machine(s) configured per REMOTE_WORKER_SETUP_GUIDE.md (see feature_dev_notes/RasRemote/)\n",
        "- Network share accessible from control machine\n",
        "- HEC-RAS installed on remote machine(s)\n",
        "\n",
        "**Note:** PsExec.exe will be automatically downloaded to `C:\\Users\\{username}\\psexec\\` if not found.\n",
        "\n",
        "**Author:** William (Bill) Katzenmeyer, P.E., C.F.M.\n",
        "\n",
        "**Date:** 2025-11-24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Remote Workers\n",
        "\n",
        "Load worker configurations from `RemoteWorkers.json` file using `load_workers_from_json()`.\n",
        "\n",
        "**First time setup:**\n",
        "1. Copy `RemoteWorkers.json.template` to `RemoteWorkers.json`\n",
        "2. Edit `RemoteWorkers.json` with your remote machine details\n",
        "3. The JSON file is in `.gitignore` for security (credentials won't be committed)\n",
        "\n",
        "**JSON Format:**\n",
        "```json\n",
        "{\n",
        "  \"workers\": [\n",
        "    {\n",
        "      \"name\": \"Local Compute\",\n",
        "      \"worker_type\": \"local\",\n",
        "      \"worker_folder\": \"C:\\\\RasRemote\",\n",
        "      \"process_priority\": \"low\",\n",
        "      \"queue_priority\": 0,\n",
        "      \"cores_total\": 4,\n",
        "      \"cores_per_plan\": 2,\n",
        "      \"enabled\": true\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"Remote Workstation\",\n",
        "      \"worker_type\": \"psexec\",\n",
        "      \"hostname\": \"192.168.1.100\",\n",
        "      \"share_path\": \"\\\\\\\\192.168.1.100\\\\RasRemote\",\n",
        "      \"worker_folder\": \"C:\\\\RasRemote\",\n",
        "      \"username\": \"your_username\",\n",
        "      \"password\": \"your_password\",\n",
        "      \"session_id\": 2,\n",
        "      \"process_priority\": \"low\",\n",
        "      \"queue_priority\": 1,\n",
        "      \"cores_total\": 16,\n",
        "      \"cores_per_plan\": 4,\n",
        "      \"enabled\": true\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Key Changes (v0.85.0):**\n",
        "- `ras_exe_path` is no longer required - automatically obtained from the initialized RAS project\n",
        "- Use `load_workers_from_json()` to load all workers from a JSON file\n",
        "- `worker_type` field is now required in each worker configuration\n",
        "- `worker_folder` replaces `local_path` - specifies where temp folders are created\n",
        "\n",
        "**Configuration Fields:**\n",
        "- `worker_type`: Required - \"psexec\", \"local\", \"ssh\", etc.\n",
        "- `worker_folder`: Local path where temporary worker folders are created during execution\n",
        "- `share_path`: (psexec only) UNC path to network share that maps to worker_folder\n",
        "- `process_priority`: OS process priority for HEC-RAS execution\n",
        "  - Valid values: `\"low\"` (default, recommended), `\"below normal\"`, `\"normal\"`\n",
        "- `queue_priority`: Execution queue priority (0-9)\n",
        "  - Lower values execute first (0 = highest priority)\n",
        "- `cores_total`: Total CPU cores on the remote machine (enables parallel execution)\n",
        "- `cores_per_plan`: Cores allocated to each HEC-RAS plan\n",
        "- **Parallel plans**: cores_total / cores_per_plan (e.g., 16/4 = 4 plans in parallel)\n",
        "\n",
        "**Session ID:** Use `query user` on remote machine to find (typically 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load remote worker configurations using the new load_workers_from_json() function\n",
        "# Note: Workers are loaded AFTER init_ras_project() so ras_exe_path is obtained automatically\n",
        "\n",
        "config_file = Path(\"RemoteWorkers.json\")\n",
        "\n",
        "if not config_file.exists():\n",
        "    print(\"ERROR: RemoteWorkers.json not found!\")\n",
        "    print()\n",
        "    print(\"First time setup:\")\n",
        "    print(\"1. Copy RemoteWorkers.json.template to RemoteWorkers.json\")\n",
        "    print(\"2. Edit RemoteWorkers.json with your remote machine details\")\n",
        "    print(\"3. Run this cell again\")\n",
        "    print()\n",
        "    print(\"The RemoteWorkers.json file should be in the same folder as this notebook.\")\n",
        "    raise FileNotFoundError(\"RemoteWorkers.json not found. See instructions above.\")\n",
        "\n",
        "# Preview the JSON configuration (without loading workers yet)\n",
        "import json\n",
        "with open(config_file, 'r') as f:\n",
        "    worker_configs = json.load(f)\n",
        "\n",
        "# Get enabled workers for display\n",
        "enabled_configs = [w for w in worker_configs[\"workers\"] if w.get(\"enabled\", True)]\n",
        "\n",
        "print(f\"Found {len(enabled_configs)} enabled worker(s) in RemoteWorkers.json:\")\n",
        "for w in enabled_configs:\n",
        "    cores_total = w.get('cores_total', 'Not set')\n",
        "    cores_per_plan = w.get('cores_per_plan', 4)\n",
        "    process_priority = w.get('process_priority', 'low')\n",
        "    queue_priority = w.get('queue_priority', 0)\n",
        "    \n",
        "    if w.get('cores_total'):\n",
        "        max_parallel = w['cores_total'] // cores_per_plan\n",
        "        parallel_info = f\"{max_parallel} plans in parallel\"\n",
        "    else:\n",
        "        parallel_info = \"Sequential execution\"\n",
        "\n",
        "    print(f\"  - {w.get('name', 'unnamed')} ({w.get('hostname', 'localhost')})\")\n",
        "    print(f\"    Type: {w.get('worker_type', 'unknown')}\")\n",
        "    print(f\"    Cores: {cores_total} total, {cores_per_plan} per plan \u2192 {parallel_info}\")\n",
        "    print(f\"    Process Priority: {process_priority}, Queue Priority: {queue_priority}\")\n",
        "\n",
        "print()\n",
        "print(\"NOTE: Workers will be loaded after init_ras_project() to get ras_exe_path automatically\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Example 1: Execute Single Plan (Muncie)\n",
        "\n",
        "Simple example executing one plan from the Muncie example project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Muncie example project\n",
        "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
        "print(f\"Project extracted to: {muncie_path}\")\n",
        "\n",
        "# Initialize project (updates global ras object)\n",
        "init_ras_project(muncie_path, \"6.6\")\n",
        "print(f\"Project initialized: {ras.project_name}\")\n",
        "print(f\"Available plans: {list(ras.plan_df.index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load workers from JSON - ras_exe_path is automatically obtained from the ras object\n",
        "# This must be called AFTER init_ras_project() so the RAS executable path is known\n",
        "\n",
        "workers = load_workers_from_json(\"RemoteWorkers.json\")\n",
        "\n",
        "print(f\"Loaded {len(workers)} worker(s):\")\n",
        "for w in workers:\n",
        "    print(f\"  - {w.worker_id} ({w.worker_type})\")\n",
        "    print(f\"    Hostname: {w.hostname}\")\n",
        "    print(f\"    RAS Exe: {w.ras_exe_path}\")\n",
        "    print(f\"    Session ID: {getattr(w, 'session_id', 'N/A')}\")\n",
        "    print(f\"    Process Priority: {getattr(w, 'process_priority', 'N/A')}\")\n",
        "    print(f\"    Queue Priority: {getattr(w, 'queue_priority', 'N/A')}\")\n",
        "    if hasattr(w, 'max_parallel_plans') and w.max_parallel_plans > 1:\n",
        "        print(f\"    Parallel Capacity: {w.max_parallel_plans} plans simultaneously\")\n",
        "    print()\n",
        "\n",
        "# Use first worker for single-plan examples\n",
        "if workers:\n",
        "    worker = workers[0]\n",
        "    print(f\"Using worker for examples: {worker.worker_id}\")\n",
        "else:\n",
        "    raise ValueError(\"No workers loaded from RemoteWorkers.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute Plan 01 remotely\n",
        "# autoclean=True (default) deletes worker folders after execution\n",
        "# Set autoclean=False for debugging to preserve worker folders on the remote machine\n",
        "\n",
        "print(\"Executing Plan 01 on remote machine...\")\n",
        "print(\"This will take ~30-60 seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results = compute_parallel_remote(\n",
        "    plan_numbers=\"01\",\n",
        "    workers=[worker],\n",
        "    num_cores=4,\n",
        "    autoclean=True  # Default is True - deletes temp folders after execution\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"\\nExecution complete in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
        "print(f\"\\nResults:\")\n",
        "for plan_num, result in results.items():\n",
        "    if result.success:\n",
        "        print(f\"  Plan {plan_num}: SUCCESS\")\n",
        "        print(f\"    HDF Path: {result.hdf_path}\")\n",
        "        print(f\"    Execution Time: {result.execution_time:.1f}s\")\n",
        "    else:\n",
        "        print(f\"  Plan {plan_num}: FAILED - {result.error_message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Muncie results using HDF analysis\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "hdf_path = Path(muncie_path) / \"Muncie.p01.hdf\"\n",
        "\n",
        "if hdf_path.exists():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MUNCIE PLAN 01 - RESULT VERIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    \n",
        "    # Get basic info\n",
        "    size_mb = hdf_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"HDF File: {hdf_path.name}\")\n",
        "    print(f\"Size: {size_mb:.2f} MB\")\n",
        "    print()\n",
        "    \n",
        "    # Get compute messages (static method)\n",
        "    msgs = HdfResultsPlan.get_compute_messages(hdf_path)\n",
        "    \n",
        "    if \"completed successfully\" in msgs.lower() or \"complete process\" in msgs.lower():\n",
        "        print(\"Compute Status: \u2705 Successful\")\n",
        "    else:\n",
        "        print(\"Compute Status: \u26a0\ufe0f Check messages\")\n",
        "    \n",
        "    # Show last part of compute messages\n",
        "    print(\"\\nCompute Messages (last 250 chars):\")\n",
        "    print(msgs[-250:])\n",
        "    print()\n",
        "    \n",
        "    # Get steady flow results\n",
        "    is_steady = HdfResultsPlan.is_steady_plan(hdf_path)\n",
        "    if is_steady:\n",
        "        profiles = HdfResultsPlan.get_steady_profile_names(hdf_path)\n",
        "        print(f\"Steady Flow Profiles: {profiles}\")\n",
        "        \n",
        "        # Get WSE for first profile\n",
        "        if profiles:\n",
        "            wse_df = HdfResultsPlan.get_steady_wse(hdf_path, profiles[0])\n",
        "            if wse_df is not None and len(wse_df) > 0:\n",
        "                print(f\"Cross Sections: {len(wse_df)}\")\n",
        "                print(f\"WSE Range: {wse_df['W.S. Elev'].min():.2f} to {wse_df['W.S. Elev'].max():.2f} ft\")\n",
        "    \n",
        "    # Get volume accounting\n",
        "    try:\n",
        "        vol = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "        if vol is not None:\n",
        "            print(f\"\\nVolume Accounting: Available ({len(vol)} entries)\")\n",
        "            print(vol)\n",
        "    except:\n",
        "        print(\"\\nVolume Accounting: Not available\")\n",
        "    \n",
        "    print()\n",
        "    print(\"\u2705 Remote execution verified - HDF results successfully collected!\")\n",
        "    print()\n",
        "else:\n",
        "    print(\"\u274c HDF file not found - execution may have failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract BaldEagleCrkMulti2D project\n",
        "baldeagle_path = RasExamples.extract_project(\"BaldEagleCrkMulti2D\")\n",
        "print(f\"Project extracted to: {baldeagle_path}\")\n",
        "\n",
        "# Initialize project (updates global ras object)\n",
        "init_ras_project(baldeagle_path, \"6.6\")\n",
        "print(f\"Project initialized: {ras.project_name}\")\n",
        "print(f\"Available plans: {list(ras.plan_df.index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute expanded set of plans to test queue priority and parallel execution\n",
        "# Plans 03, 04, 06, 13, 15, 17, 18, 19 - 8 plans total\n",
        "# This tests the queue-aware scheduling with multiple sub-workers\n",
        "\n",
        "test_plans = [\"03\", \"04\", \"06\", \"13\", \"15\", \"17\", \"18\", \"19\"]\n",
        "print(f\"Executing {len(test_plans)} plans on remote machine: {test_plans}\")\n",
        "print(\"These are 2D unsteady models - may take 10-20 minutes total\")\n",
        "print(\"Watch the logs to observe queue priority and wave scheduling\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results = compute_parallel_remote(\n",
        "    plan_numbers=test_plans,\n",
        "    workers=[worker],\n",
        "    num_cores=4,\n",
        "    autoclean=True  # Default is True - deletes temp folders after execution\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "print(f\"\\nExecution complete in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
        "print(f\"\\nResults:\")\n",
        "success_count = 0\n",
        "for plan_num, result in results.items():\n",
        "    if result.success:\n",
        "        print(f\"  Plan {plan_num}: SUCCESS ({result.execution_time:.1f}s)\")\n",
        "        success_count += 1\n",
        "    else:\n",
        "        print(f\"  Plan {plan_num}: FAILED - {result.error_message}\")\n",
        "\n",
        "print(f\"\\nSummary: {success_count}/{len(results)} plans succeeded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify BaldEagle results using HDF analysis\n",
        "from ras_commander import HdfResultsPlan, HdfResultsMesh\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"BALDEAGLE PLANS 06 & 19 - RESULT VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "for plan_num in [\"06\", \"19\"]:\n",
        "    hdf_path = Path(baldeagle_path) / f\"BaldEagleDamBrk.p{plan_num}.hdf\"\n",
        "    \n",
        "    if hdf_path.exists():\n",
        "        print(f\"Plan {plan_num}:\")\n",
        "        size_mb = hdf_path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  HDF Size: {size_mb:.2f} MB\")\n",
        "        \n",
        "        # Get compute messages (static method)\n",
        "        msgs = HdfResultsPlan.get_compute_messages(hdf_path)\n",
        "        if \"completed successfully\" in msgs.lower() or \"complete process\" in msgs.lower():\n",
        "            print(f\"  Status: \u2705 Computation successful\")\n",
        "        else:\n",
        "            print(f\"  Status: \u26a0\ufe0f Check compute messages\")\n",
        "        \n",
        "        # Get unsteady summary\n",
        "        try:\n",
        "            summary = HdfResultsPlan.get_unsteady_summary(hdf_path)\n",
        "            if summary is not None:\n",
        "                print(f\"  Unsteady Summary: Available\")\n",
        "        except:\n",
        "            print(f\"  Unsteady Summary: Not available\")\n",
        "        \n",
        "        # Get volume accounting\n",
        "        try:\n",
        "            vol = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "            if vol is not None and len(vol) > 0:\n",
        "                print(f\"  Volume Accounting: {len(vol)} entries\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Get mesh timesteps for 2D\n",
        "        try:\n",
        "            mesh_times = HdfResultsMesh.get_output_times(hdf_path)\n",
        "            if mesh_times is not None:\n",
        "                print(f\"  Output Timesteps: {len(mesh_times)}\")\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        print()\n",
        "    else:\n",
        "        print(f\"Plan {plan_num}: \u274c HDF file not found\")\n",
        "        print()\n",
        "\n",
        "print(\"\u2705 Remote execution verified - 2D model results successfully collected!\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Example 3: Multiple Remote Workers (Parallel)\n",
        "\n",
        "Execute plans across multiple remote machines simultaneously.\n",
        "\n",
        "**Note:** This example uses ALL enabled workers from `RemoteWorkers.json`.\n",
        "To use multiple machines, add additional workers to the JSON file and set `enabled: true`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute multiple plans across all loaded workers\n",
        "# Plans will be distributed based on queue_priority (0 first, then 1, etc.)\n",
        "\n",
        "# Workers were already loaded in cell-7 using load_workers_from_json()\n",
        "if len(workers) > 1:\n",
        "    print(f\"Executing plans across {len(workers)} worker(s)...\")\n",
        "    for w in workers:\n",
        "        print(f\"  - {w.worker_id} ({w.hostname}) - Queue {getattr(w, 'queue_priority', 0)}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    results = compute_parallel_remote(\n",
        "        plan_numbers=[\"06\", \"19\"],\n",
        "        workers=workers,\n",
        "        num_cores=4,\n",
        "        clear_geompre=False,\n",
        "        autoclean=True  # Default is True - deletes temp folders after execution\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nTotal execution time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
        "    print(f\"\\nResults:\")\n",
        "    for plan_num, result in results.items():\n",
        "        status = \"SUCCESS\" if result.success else f\"FAILED: {result.error_message}\"\n",
        "        print(f\"  Plan {plan_num}: {status}\")\n",
        "    \n",
        "    # Calculate speedup\n",
        "    successful = sum(1 for r in results.values() if r.success)\n",
        "    print(f\"\\nSummary: {successful}/{len(results)} plans succeeded\")\n",
        "else:\n",
        "    print(f\"Only 1 worker loaded - skipping multi-worker example\")\n",
        "    print(f\"To test parallel execution:\")\n",
        "    print(f\"  1. Add more workers to RemoteWorkers.json\")\n",
        "    print(f\"  2. Set enabled=true for each\")\n",
        "    print(f\"  3. Re-run the notebook from the beginning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Manually initialize a worker without JSON file\n",
        "# This demonstrates the init_ras_worker() function directly\n",
        "# Note: ras_exe_path is automatically obtained from the ras object\n",
        "\n",
        "manual_worker = init_ras_worker(\n",
        "    \"psexec\",\n",
        "    hostname=\"192.168.3.8\",  # Replace with your hostname\n",
        "    share_path=r\"\\\\192.168.3.8\\RasRemote\",  # Replace with your share path\n",
        "    worker_folder=r\"C:\\RasRemote\",  # Local path on remote machine corresponding to share_path\n",
        "    credentials={\n",
        "        \"username\": \".\\\\bill\",  # Replace with your username\n",
        "        \"password\": \"YourPassword\"  # Replace with your password\n",
        "    },\n",
        "    # ras_exe_path is NOT required - obtained from ras object automatically\n",
        "    session_id=2,\n",
        "    process_priority=\"low\",\n",
        "    queue_priority=0,\n",
        "    cores_total=8,\n",
        "    cores_per_plan=2\n",
        ")\n",
        "\n",
        "print(f\"Manual worker initialized:\")\n",
        "print(f\"  Worker ID: {manual_worker.worker_id}\")\n",
        "print(f\"  Hostname: {manual_worker.hostname}\")\n",
        "print(f\"  Worker Folder: {manual_worker.worker_folder}\")\n",
        "print(f\"  RAS Exe: {manual_worker.ras_exe_path}\")  # Automatically set from ras object\n",
        "print(f\"  Parallel Capacity: {manual_worker.max_parallel_plans} plans\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Verify Results\n",
        "\n",
        "Check that HDF files were created and results collected properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List only .pXX.hdf files in results folder (plan result HDFs)\n",
        "import re\n",
        "\n",
        "results_path = Path(baldeagle_path).parent / \"multi_worker_results\" / \"BaldEagleDamBrk\"\n",
        "\n",
        "pattern = re.compile(r\"\\.p\\d{2}\\.hdf$\", re.IGNORECASE)\n",
        "\n",
        "if results_path.exists():\n",
        "    hdf_files = [hdf for hdf in results_path.glob(\"*.hdf\") if pattern.search(hdf.name)]\n",
        "    print(f\"Plan HDF files (.pXX.hdf) in results folder: {len(hdf_files)}\")\n",
        "    for hdf in hdf_files:\n",
        "        size_mb = hdf.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  {hdf.name}: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"Results folder not found: {results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Configuration\n",
        "\n",
        "### Session ID Determination\n",
        "\n",
        "Find the active session ID on a remote machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query active sessions on remote machine\n",
        "# Uses the first loaded worker to get psexec_path and credentials\n",
        "import subprocess\n",
        "\n",
        "if workers:\n",
        "    w = workers[0]\n",
        "    psexec = getattr(w, 'psexec_path', None)\n",
        "    \n",
        "    if psexec and hasattr(w, 'credentials') and w.credentials:\n",
        "        cmd = [\n",
        "            psexec,\n",
        "            f\"\\\\\\\\{w.hostname}\",\n",
        "            \"-u\", w.credentials.get(\"username\", \"\"),\n",
        "            \"-p\", w.credentials.get(\"password\", \"\"),\n",
        "            \"-accepteula\",\n",
        "            \"cmd\", \"/c\", \"query\", \"user\"\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n",
        "            print(\"Active sessions on remote machine:\")\n",
        "            print(result.stdout)\n",
        "            print(\"\\nLook for the ID column - typically 2 for workstations\")\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"Timeout querying sessions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not query sessions: {e}\")\n",
        "    else:\n",
        "        print(\"Worker doesn't have psexec_path or credentials set\")\n",
        "        print(\"Try session_id=2 (most common for single-user workstations)\")\n",
        "else:\n",
        "    print(\"No workers loaded - run previous cells first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Priority Levels\n",
        "\n",
        "Control OS process priority for remote HEC-RAS execution:\n",
        "\n",
        "- `\"low\"` - Low priority (recommended for background work, minimal impact on remote user)\n",
        "- `\"below normal\"` - Below normal priority\n",
        "- `\"normal\"` - Normal priority (default Windows priority)\n",
        "\n",
        "**Note:** Higher priorities (above normal, high, realtime) are NOT supported to avoid impacting remote user operations.\n",
        "\n",
        "### Queue Priority\n",
        "\n",
        "Control execution order across workers:\n",
        "\n",
        "- `queue_priority` is an integer from 0-9 (lower = higher priority)\n",
        "- Workers at queue level 0 are filled before queue level 1, etc.\n",
        "- Within each queue level, wave scheduling applies (one plan per machine first, then additional)\n",
        "- Use for tiered bursting: local workers (queue 0) execute first, then remote (queue 1), then cloud (queue 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Viewing worker configuration with low process priority\n",
        "# Workers loaded from JSON already have these settings applied\n",
        "\n",
        "if workers:\n",
        "    w = workers[0]\n",
        "    print(f\"Worker: {w.worker_id}\")\n",
        "    print(f\"  Process Priority: {getattr(w, 'process_priority', 'N/A')}\")\n",
        "    print(f\"  Queue Priority: {getattr(w, 'queue_priority', 'N/A')}\")\n",
        "    print(f\"  RAS Exe Path: {w.ras_exe_path}\")\n",
        "    print()\n",
        "    print(\"To change settings, edit RemoteWorkers.json and reload workers:\")\n",
        "    print(\"  workers = load_workers_from_json('RemoteWorkers.json')\")\n",
        "else:\n",
        "    print(\"No workers loaded - run previous cells first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Troubleshooting (Optional)\n",
        "\n",
        "### Test Remote Connections using psexec\n",
        "\n",
        "Change the cell below to a code cell, enter your username and password for use in testing. \n",
        "\n",
        "Don't leave your passwords here, it can get synced back to git.  Use RemoteWorkers.json, it is already in the .gitignore for this repo.  \n",
        "Use the code cell below for testing only, not as a design pattern for production usage: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build REMOTE_CONFIG from the first psexec worker in workers list\n",
        "# This uses the credentials already loaded from RemoteWorkers.json\n",
        "\n",
        "REMOTE_CONFIG = None\n",
        "\n",
        "if workers:\n",
        "    # Find first psexec worker\n",
        "    for w in workers:\n",
        "        if w.worker_type == \"psexec\":\n",
        "            REMOTE_CONFIG = {\n",
        "                \"hostname\": w.hostname,\n",
        "                \"share_path\": w.share_path,\n",
        "                \"username\": w.credentials.get(\"username\", \"\") if hasattr(w, 'credentials') and w.credentials else \"\",\n",
        "                \"password\": w.credentials.get(\"password\", \"\") if hasattr(w, 'credentials') and w.credentials else \"\",\n",
        "                \"ras_exe_path\": w.ras_exe_path,\n",
        "                \"session_id\": getattr(w, 'session_id', 2)\n",
        "            }\n",
        "            print(f\"REMOTE_CONFIG built from worker: {w.worker_id}\")\n",
        "            print(f\"  Hostname: {REMOTE_CONFIG['hostname']}\")\n",
        "            print(f\"  Share Path: {REMOTE_CONFIG['share_path']}\")\n",
        "            print(f\"  Session ID: {REMOTE_CONFIG['session_id']}\")\n",
        "            break\n",
        "\n",
        "if REMOTE_CONFIG is None:\n",
        "    print(\"WARNING: No psexec workers found in workers list.\")\n",
        "    print(\"Define REMOTE_CONFIG manually or add psexec workers to RemoteWorkers.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test basic PsExec connectivity\n",
        "import subprocess\n",
        "\n",
        "if REMOTE_CONFIG is None:\n",
        "    print(\"REMOTE_CONFIG not set - run the cell above first\")\n",
        "else:\n",
        "    # Get psexec path from the initialized worker\n",
        "    try:\n",
        "        temp_worker = init_ras_worker(\n",
        "            \"psexec\",\n",
        "            hostname=REMOTE_CONFIG[\"hostname\"],\n",
        "            share_path=REMOTE_CONFIG[\"share_path\"],\n",
        "            credentials={\n",
        "                \"username\": REMOTE_CONFIG[\"username\"],\n",
        "                \"password\": REMOTE_CONFIG[\"password\"]\n",
        "            },\n",
        "            session_id=REMOTE_CONFIG[\"session_id\"]\n",
        "        )\n",
        "        psexec_path = temp_worker.psexec_path\n",
        "\n",
        "        test_cmd = [\n",
        "            psexec_path,\n",
        "            f\"\\\\\\\\{REMOTE_CONFIG['hostname']}\",\n",
        "            \"-u\", REMOTE_CONFIG[\"username\"],\n",
        "            \"-p\", REMOTE_CONFIG[\"password\"],\n",
        "            \"-i\", str(REMOTE_CONFIG[\"session_id\"]),\n",
        "            \"-accepteula\",\n",
        "            \"cmd\", \"/c\", \"echo\", \"SUCCESS\"\n",
        "        ]\n",
        "\n",
        "        result = subprocess.run(test_cmd, capture_output=True, text=True, timeout=30)\n",
        "        if \"SUCCESS\" in result.stdout:\n",
        "            print(\"[OK] PsExec connection successful!\")\n",
        "        else:\n",
        "            print(\"[WARNING] Unexpected output:\")\n",
        "            print(result.stdout)\n",
        "            print(result.stderr)\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"[FAIL] Connection timeout - check firewall and services\")\n",
        "    except Exception as e:\n",
        "        print(f\"[FAIL] Connection error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Share Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test if share is accessible\n",
        "from pathlib import WindowsPath\n",
        "\n",
        "if REMOTE_CONFIG is None:\n",
        "    print(\"REMOTE_CONFIG not set - run the 'Build REMOTE_CONFIG' cell first\")\n",
        "else:\n",
        "    share_path = Path(REMOTE_CONFIG[\"share_path\"])\n",
        "\n",
        "    try:\n",
        "        # This may fail without authenticated session - that's OK\n",
        "        if share_path.exists():\n",
        "            print(f\"[OK] Share accessible: {share_path}\")\n",
        "            files = list(share_path.iterdir())[:5]\n",
        "            print(f\"     Contents: {len(list(share_path.iterdir()))} items\")\n",
        "        else:\n",
        "            print(f\"[INFO] Share not accessible via Path.exists() (authentication may be required)\")\n",
        "            print(f\"      This is normal - share will be accessed during execution with credentials\")\n",
        "    except Exception as e:\n",
        "        print(f\"[INFO] Cannot test share access: {e}\")\n",
        "        print(f\"      This is normal - share will be accessed during execution with credentials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Notes and Best Practices\n",
        "\n",
        "### Remote Worker Configuration:\n",
        "- Credentials stored in `RemoteWorkers.json` (not committed to git)\n",
        "- See **REMOTE_WORKERS_README.md** for JSON format and setup\n",
        "- Template provided: `RemoteWorkers.json.template`\n",
        "\n",
        "### Remote Worker Requirements:\n",
        "1. \u2705 Network share created and accessible\n",
        "2. \u2705 User in local Administrators group\n",
        "3. \u2705 Group Policy: User added to network access, local logon, batch job policies\n",
        "4. \u2705 Registry: LocalAccountTokenFilterPolicy = 1\n",
        "5. \u2705 Remote Registry service running\n",
        "6. \u2705 Windows Firewall configured\n",
        "7. \u2705 Machine rebooted after changes\n",
        "\n",
        "### Session ID:\n",
        "- Session ID 2 is typical for single-user workstations\n",
        "- Use `query user` on remote machine to verify\n",
        "- User must be logged in for session to be active\n",
        "- Session ID can change if user logs off/on\n",
        "\n",
        "### HEC-RAS Considerations:\n",
        "- HEC-RAS is a GUI application\n",
        "- MUST use session-based execution (`system_account=False`)\n",
        "- NEVER use SYSTEM account (`system_account=True`) for HEC-RAS\n",
        "- HEC-RAS window will start on the desktop of the remote desktop\n",
        "- Ensure HEC-RAS version matches on all workers, and TOS has been accepted.\n",
        "\n",
        "### Performance:\n",
        "- Network share speed affects file transfer\n",
        "- Use Gigabit Ethernet for best performance\n",
        "- 2-4 workers per machine optimal (depends on cores/RAM)\n",
        "- Plans execute sequentially on each worker\n",
        "- Multiple workers enable true parallel execution\n",
        "\n",
        "### Security:\n",
        "- Credentials in `RemoteWorkers.json` (in .gitignore)\n",
        "- Never commit credentials to git\n",
        "- See setup instructions for required group policy and registry changes\n",
        "\n",
        "### Debugging:\n",
        "- Check logs in ras_commander.log\n",
        "- Inspect compute messages: `project.p##.computeMsgs.txt`\n",
        "- Verify temp folders on remote share\n",
        "- Test PsExec manually with provided batch files\n",
        "\n",
        "---\n",
        "\n",
        "**For complete setup instructions, see:**\n",
        "- `feature_dev_notes/RasRemote/REMOTE_WORKER_SETUP_GUIDE.md` - Remote machine setup\n",
        "- `REMOTE_WORKERS_README.md` - JSON credential file format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup Remote Worker Folders\n",
        "\n",
        "The `autoclean=True` parameter (default) automatically deletes worker folders after execution.\n",
        "However, if you used `autoclean=False` for debugging or if executions were interrupted,\n",
        "you may have leftover folders on the remote shares.\n",
        "\n",
        "**All files in the RasRemote share are considered temporary** and can be safely deleted\n",
        "to preserve disk space on the remote machines.\n",
        "\n",
        "Run the cells below to manually clean up any remaining worker folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List and optionally clean up worker folders on remote machines\n",
        "# This cleans ALL files in the RasRemote share - all contents are temporary\n",
        "\n",
        "def cleanup_remote_shares(workers, dry_run=True):\n",
        "    \"\"\"\n",
        "    Clean up worker folders from remote shares.\n",
        "    \n",
        "    Args:\n",
        "        workers: List of worker objects with share_path attribute\n",
        "        dry_run: If True, only list folders without deleting (default True for safety)\n",
        "    \n",
        "    Returns:\n",
        "        dict: {hostname: {\"folders\": count, \"size_mb\": total_size}}\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "    \n",
        "    results = {}\n",
        "    seen_shares = set()\n",
        "    \n",
        "    for w in workers:\n",
        "        if not hasattr(w, 'share_path') or not w.share_path:\n",
        "            continue\n",
        "            \n",
        "        share_path = Path(w.share_path)\n",
        "        share_key = str(share_path)\n",
        "        \n",
        "        # Skip if we've already processed this share\n",
        "        if share_key in seen_shares:\n",
        "            continue\n",
        "        seen_shares.add(share_key)\n",
        "        \n",
        "        hostname = getattr(w, 'hostname', 'unknown')\n",
        "        \n",
        "        try:\n",
        "            if not share_path.exists():\n",
        "                print(f\"Share not accessible: {share_path}\")\n",
        "                continue\n",
        "                \n",
        "            folders = [f for f in share_path.iterdir() if f.is_dir()]\n",
        "            total_size = 0\n",
        "            \n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Share: {share_path} ({hostname})\")\n",
        "            print(f\"{'='*60}\")\n",
        "            \n",
        "            if not folders:\n",
        "                print(\"  No folders found - share is clean\")\n",
        "                results[hostname] = {\"folders\": 0, \"size_mb\": 0}\n",
        "                continue\n",
        "                \n",
        "            for folder in folders:\n",
        "                # Calculate folder size\n",
        "                folder_size = sum(f.stat().st_size for f in folder.rglob('*') if f.is_file())\n",
        "                folder_size_mb = folder_size / (1024 * 1024)\n",
        "                total_size += folder_size_mb\n",
        "                \n",
        "                if dry_run:\n",
        "                    print(f\"  [WOULD DELETE] {folder.name} ({folder_size_mb:.1f} MB)\")\n",
        "                else:\n",
        "                    print(f\"  [DELETING] {folder.name} ({folder_size_mb:.1f} MB)\")\n",
        "                    shutil.rmtree(folder, ignore_errors=True)\n",
        "            \n",
        "            results[hostname] = {\"folders\": len(folders), \"size_mb\": total_size}\n",
        "            \n",
        "            if dry_run:\n",
        "                print(f\"\\n  Summary: {len(folders)} folders, {total_size:.1f} MB total\")\n",
        "                print(f\"  Set dry_run=False to delete these folders\")\n",
        "            else:\n",
        "                print(f\"\\n  Deleted: {len(folders)} folders, {total_size:.1f} MB freed\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing {share_path}: {e}\")\n",
        "            \n",
        "    return results\n",
        "\n",
        "# DRY RUN - List folders without deleting\n",
        "print(\"=\" * 70)\n",
        "print(\"CLEANUP PREVIEW (dry_run=True)\")\n",
        "print(\"=\" * 70)\n",
        "cleanup_results = cleanup_remote_shares(workers, dry_run=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ACTUALLY DELETE - Uncomment and run to delete all worker folders\n",
        "# WARNING: This permanently deletes all folders in the RasRemote shares!\n",
        "\n",
        "# print(\"=\" * 70)\n",
        "# print(\"CLEANUP EXECUTION (dry_run=False)\")\n",
        "# print(\"=\" * 70)\n",
        "# cleanup_results = cleanup_remote_shares(workers, dry_run=False)\n",
        "# print(\"\\nCleanup complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\examples\AGENTS.md
==================================================
**Agent Guide: Examples Notebooks**

Purpose: Help an agent navigate each notebook quickly and extract only the logic needed to build scripts. Treat notebooks as reference; prefer cleaned copies (no images/outputs) when available.

How agents should use notebooks
- Read code cells, ignore long outputs and images.
- Understand the intent and data flow; note any environment-specific paths.
- Extract relevant cells into a script, replacing Jupyter-only commands (e.g., `!pip install`) with `uv pip install` run in the terminal.
- Execute scripts via `uv run ...` with inputs/outputs in a writable folder (e.g., `working/`).

00_Using_RasExamples.ipynb
- Focus: Download/list/extract official example projects.
- Functions: [RasExamples.get_example_projects](../ras_commander/RasExamples.py), [RasExamples.list_categories](../ras_commander/RasExamples.py), [RasExamples.list_projects](../ras_commander/RasExamples.py), [RasExamples.extract_project](../ras_commander/RasExamples.py), [RasExamples.is_project_extracted](../ras_commander/RasExamples.py), [RasExamples.clean_projects_directory](../ras_commander/RasExamples.py).
- Pattern: Extract to a writable workspace folder (not inside repo) to avoid committing large assets.

01_project_initialization.ipynb
- Focus: Initialize a project and inspect metadata.
- Functions: [init_ras_project](../ras_commander/RasPrj.py), [RasPrj](../ras_commander/RasPrj.py), [get_ras_exe](../ras_commander/RasPrj.py); ras dataframes: `plan_df`, `geom_df`, `flow_df`, `unsteady_df`, `boundaries_df`, [get_hdf_entries](../ras_commander/RasPrj.py).
- Pattern: Use global `ras` for single-project workflows; prefer separate `RasPrj` instances for multi-project work.

02_plan_and_geometry_operations.ipynb
- Focus: Clone/retarget plans, geometry, and intervals.
- Functions: [RasPlan.clone_plan](../ras_commander/RasPlan.py), [RasPlan.clone_geom](../ras_commander/RasPlan.py), [RasPlan.clone_unsteady](../ras_commander/RasPlan.py), [RasPlan.clone_steady](../ras_commander/RasPlan.py), [RasPlan.set_geom](../ras_commander/RasPlan.py), [RasPlan.set_steady](../ras_commander/RasPlan.py), [RasPlan.set_unsteady](../ras_commander/RasPlan.py), [RasPlan.set_geom_preprocessor](../ras_commander/RasPlan.py), [RasPlan.set_num_cores](../ras_commander/RasPlan.py), [RasPlan.update_run_flags](../ras_commander/RasPlan.py), [RasPlan.update_plan_intervals](../ras_commander/RasPlan.py), [RasPlan.update_simulation_date](../ras_commander/RasPlan.py), [RasPlan.update_plan_description](../ras_commander/RasPlan.py), [RasPlan.get_shortid](../ras_commander/RasPlan.py)/[set_shortid](../ras_commander/RasPlan.py).
- Pattern: Two-digit plan numbers; clear geompre when geometry changes.

03_unsteady_flow_operations.ipynb
- Focus: Inspect/modify unsteady (.uXX) files and BC tables.
- Functions: [RasUnsteady.update_flow_title](../ras_commander/RasUnsteady.py), [RasUnsteady.update_restart_settings](../ras_commander/RasUnsteady.py), [RasUnsteady.extract_boundary_and_tables](../ras_commander/RasUnsteady.py), [RasUnsteady.identify_tables](../ras_commander/RasUnsteady.py), [RasUnsteady.write_table_to_file](../ras_commander/RasUnsteady.py).
- Pattern: Clone unsteady first, then edit; reassign plan to updated unsteady.

04_multiple_project_operations.ipynb
- Focus: Work with several projects at once.
- Functions: `RasPrj` per project; [RasCmdr.compute_plan](../ras_commander/RasCmdr.py)/[compute_parallel](../ras_commander/RasCmdr.py) with `ras_object`.
- Pattern: Isolate compute folders per project; do not mix global `ras` and custom `RasPrj` instances.

05_single_plan_execution.ipynb
- Focus: Run a single plan with options.
- Functions: [RasCmdr.compute_plan](../ras_commander/RasCmdr.py); [RasGeo.clear_geompre_files](../ras_commander/RasGeo.py); [RasPlan.set_num_cores](../ras_commander/RasPlan.py).
- Pattern: Use `dest_folder` and `overwrite_dest` to keep originals clean.

06_executing_plan_sets.ipynb
- Focus: Specify and execute sets of plans.
- Functions: [RasCmdr.compute_plan](../ras_commander/RasCmdr.py) (loop), [RasCmdr.compute_parallel](../ras_commander/RasCmdr.py), [RasCmdr.compute_test_mode](../ras_commander/RasCmdr.py).
- Pattern: Use lists for plan selection; choose sequential vs parallel based on dependencies.

07_sequential_plan_execution.ipynb
- Focus: Ordered runs with isolation.
- Functions: [RasCmdr.compute_test_mode](../ras_commander/RasCmdr.py) (with `plan_number`, `dest_folder_suffix`, `num_cores`, `clear_geompre`).
- Pattern: Good for dependent plans or reproducible test folders.

08_parallel_execution.ipynb
- Focus: Throughput with multiple workers.
- Functions: [RasCmdr.compute_parallel](../ras_commander/RasCmdr.py) (`max_workers`, `num_cores`, `dest_folder`, `overwrite_dest`).
- Pattern: Balance `max_workers * num_cores` to available cores/RAM.

09_plan_parameter_operations.ipynb
- Focus: Edit plan-level parameters.
- Functions: [RasPlan.get_plan_value](../ras_commander/RasPlan.py), [RasPlan.update_run_flags](../ras_commander/RasPlan.py), [RasPlan.update_plan_intervals](../ras_commander/RasPlan.py), [RasPlan.update_simulation_date](../ras_commander/RasPlan.py), [RasPlan.update_plan_description](../ras_commander/RasPlan.py), [RasPlan.get_plan_title](../ras_commander/RasPlan.py)/[set_plan_title](../ras_commander/RasPlan.py).
- Pattern: Verify changes via ras.plan_df and file reads.

10_1d_hdf_data_extraction.ipynb
- Focus: 1D geometry/results and timeseries queries.
- Functions: [HdfXsec.get_cross_sections](../ras_commander/HdfXsec.py), [HdfResultsXsec.get_xsec_timeseries](../ras_commander/HdfResultsXsec.py), [HdfResultsXsec.get_ref_lines_timeseries](../ras_commander/HdfResultsXsec.py), [HdfResultsXsec.get_ref_points_timeseries](../ras_commander/HdfResultsXsec.py); [HdfResultsPlan.get_runtime_data](../ras_commander/HdfResultsPlan.py).
- Notable cells: locate and print `HDF_Results_Path` and geometry HDF; demonstrate use of decorators to accept plan numbers or paths; optional GeoPandas plots for xsec lines.

11_2d_hdf_data_extraction.ipynb
- Focus: 2D mesh geometry/results basics with spatial plotting.
- Functions: [HdfMesh.get_mesh_area_names](../ras_commander/HdfMesh.py), [HdfMesh.get_mesh_areas](../ras_commander/HdfMesh.py), [HdfMesh.get_mesh_cell_polygons](../ras_commander/HdfMesh.py), [HdfMesh.get_mesh_cell_points](../ras_commander/HdfMesh.py), [HdfMesh.get_mesh_cell_faces](../ras_commander/HdfMesh.py), [HdfBndry.get_breaklines](../ras_commander/HdfBndry.py), [HdfBndry.get_bc_lines](../ras_commander/HdfBndry.py), [HdfResultsMesh.get_mesh_max_ws](../ras_commander/HdfResultsMesh.py)/[get_mesh_min_ws](../ras_commander/HdfResultsMesh.py)/[get_mesh_max_face_v](../ras_commander/HdfResultsMesh.py)/[get_mesh_max_ws_err](../ras_commander/HdfResultsMesh.py), [HdfResultsMesh.get_mesh_timeseries](../ras_commander/HdfResultsMesh.py).
- Notable cells: parse `.rasmap`, list plans, derive run windows; join result attributes back to geometry; toggle `generate_plots` to minimize output size.

12_2d_hdf_data_extraction pipes and pumps.ipynb
- Focus: New 6.6+ pipe networks: conduits, nodes, pumps, timeseries.
- Functions: [HdfPipe.get_pipe_conduits](../ras_commander/HdfPipe.py), [HdfPipe.get_pipe_nodes](../ras_commander/HdfPipe.py), [HdfPipe.get_pipe_network_timeseries](../ras_commander/HdfPipe.py), [HdfPipe.get_pipe_conduit_timeseries](../ras_commander/HdfPipe.py), [HdfPump.get_pump_stations](../ras_commander/HdfPump.py), [HdfPump.get_pump_groups](../ras_commander/HdfPump.py), [HdfPump.get_pump_station_timeseries](../ras_commander/HdfPump.py).
- Notable cells: build network GeoDataFrames, summarize pump groups, plot selected elements; record plan run and extract pump/conduit curves.

13_2d_detail_face_data_extraction.ipynb
- Focus: Face-level analytics along profile lines; unique notebook-only helpers.
- Functions: [HdfMesh.get_mesh_face_property_tables](../ras_commander/HdfMesh.py), [HdfMesh.get_mesh_cell_property_tables](../ras_commander/HdfMesh.py); [HdfResultsMesh.get_mesh_faces_timeseries](../ras_commander/HdfResultsMesh.py).
- Notable cells:
  - Extract `mesh_cell_faces` GeoDataFrame and preview attributes.
  - Unique function `find_nearest_cell_face(point, cell_faces_df)`; returns `(face_id, distance)` and supports plotting the nearest face vs all faces.
  - Profile-line selection along perpendicular faces; compute discharge-weighted velocity; enforce positive flow direction before aggregation.

14_fluvial_pluvial_delineation.ipynb
- Focus: Classify flooding mechanism by timing.
- Functions: [HdfResultsMesh.get_mesh_max_ws](../ras_commander/HdfResultsMesh.py), plus HdfMesh/HdfBndry for polygons/lines.
- Notable cells: compare timing in adjacent cells; map and export boundaries (GeoJSON) after smoothing/filtering short segments.

15_stored_map_generation.ipynb
- Focus: Automate stored map outputs.
- Functions: [RasMap.parse_rasmap](../ras_commander/RasMap.py), [RasMap.postprocess_stored_maps](../ras_commander/RasMap.py).
- Notable cells: backup/modify `.rasmap` and plan flags, launch RAS to bake maps, restore originals after generation.

16_automating_ras_with_win32com.ipynb
- Focus: Open HEC‑RAS/RAS Mapper to refresh stored-map configs.
- Functions: [RasMap.parse_rasmap](../ras_commander/RasMap.py); external `subprocess` to `Ras.exe`.
- Notable cells: manual steps to update mapper, then resume automation.

17_extracting_profiles_with_hecrascontroller.ipynb
- Focus: Extract steady AND unsteady results from legacy HEC-RAS versions (3.x-4.x) using RasControl.
- Functions: [RasControl.run_plan](../ras_commander/RasControl.py), [RasControl.get_steady_results](../ras_commander/RasControl.py), [RasControl.get_unsteady_results](../ras_commander/RasControl.py), [RasControl.get_output_times](../ras_commander/RasControl.py), [RasControl.set_current_plan](../ras_commander/RasControl.py).
- Pattern: ras-commander style API - use plan numbers ("02") not file paths. Open-operate-close pattern. Integrates with init_ras_project().
- Notable cells: Steady workflow (Plan 02), unsteady workflow (Plan 01), time series visualization, DataFrame exports.
- Supported versions: 3.1, 4.1, 5.0.x (501, 503, 505, 506), 6.0, 6.3, 6.6.
- Key: Must specify version in init_ras_project(path, "4.1") for RasControl to work.

101_Core_Sensitivity(.ipynb, _aircooled)
- Focus: Runtime vs core count experiments.
- Functions: [RasCmdr.compute_plan](../ras_commander/RasCmdr.py); [RasPlan.set_num_cores](../ras_commander/RasPlan.py).
- Notable cells: sweep cores, record walltime, plot scaling; choose efficient settings.

102_benchmarking_versions_6.1_to_6.6.ipynb
- Focus: Cross-version performance comparison.
- Functions: [init_ras_project](../ras_commander/RasPrj.py) (vary Ras.exe), [RasCmdr.compute_plan](../ras_commander/RasCmdr.py), [HdfResultsPlan.get_runtime_data](../ras_commander/HdfResultsPlan.py).
- Notable cells: control versioned runs, tabulate walltimes per version; keep outputs isolated per version.

103_Running_AEP_Events_from_Atlas_14.ipynb
- Focus: Generate hyetographs from NOAA Atlas 14 and batch scenarios.
- Functions: [RasPlan.clone_plan](../ras_commander/RasPlan.py)/[set_unsteady](../ras_commander/RasPlan.py), [RasUnsteady.*](../ras_commander/RasUnsteady.py), [RasCmdr.compute_parallel](../ras_commander/RasCmdr.py), [HdfResultsMesh](../ras_commander/HdfResultsMesh.py)/[HdfResultsPlan](../ras_commander/HdfResultsPlan.py).
- Notable cells:
  - Read precipitation frequency from Atlas 14 CSVs in `examples/data` and generate balanced storm hyetographs via Alternating Block Method.
  - Parameterize AEP events, clone plans, write unsteady settings, and compute in parallel.
  - Aggregate key metrics from mesh/plan results; optional plots (disable to keep outputs light).

Notebook‑only utilities and unique logic
- `find_nearest_cell_face(...)` (13_2d_detail_face_data_extraction): nearest face selection and plotting; not part of the library API.
- Hyetograph generation for Atlas 14 AEP events (103_*): end‑to‑end pattern from CSV → plan clones → batch compute.
- Profile‑based face aggregation (13_*): discharge‑weighted velocity and flow‑direction normalization.

---

## Notebook Import Cell Management

All example notebooks follow a standardized 2-cell import pattern:

**Cell 0 (Code - ACTIVE by default):**
```python
# Uncomment to install/upgrade ras-commander from pip
#!pip install --upgrade ras-commander

#Import the ras-commander package
from ras_commander import *
```

**Cell 1 (Markdown - INACTIVE by default):**
Contains development mode instructions with code block for local copy usage.

**Cell 2 (Code - When needed):**
Notebook-specific imports (numpy, pandas, matplotlib, etc.)

### Toggling Between Pip and Dev Modes

**For pip-installed package testing (default state):**
- Cell 0 remains as code (active)
- Cell 1 remains as markdown (inactive)
- Run notebooks as-is

**For local development copy testing:**
1. Convert Cell 1 from markdown to code
2. Convert Cell 0 from code to markdown
3. Run the modified notebooks
4. **IMPORTANT:** Restore to default state before committing

**Warning:** Never have both Cell 0 and Cell 1 as code cells simultaneously. This will cause import conflicts.

---

## Running Notebook Tests

### Prerequisites
- Install test dependencies: `uv pip install notebook jupyter ipykernel`
- Ensure HEC-RAS is installed and in PATH
- Verify sufficient disk space for example projects (~5 GB)

### Testing All Notebooks with Subagents

**For pip-installed package:**
```python
# Launch subagent to run all notebooks and review results
# Default state (Cell 0=code, Cell 1=markdown) is correct
task_prompt = """
Run all example notebooks in C:\\GH\\ras-commander\\examples\\ and verify:
1. No import errors
2. No unhandled exceptions
3. Warnings are reviewed and acceptable
4. Long-running cells complete successfully
5. Results match expected patterns

Report any failures, unexpected warnings, or behavioral changes.
"""
```

**For local development copy:**
```python
# First toggle cells, then test
task_prompt = """
1. For each notebook in C:\\GH\\ras-commander\\examples\\:
   - Convert Cell 0 (pip mode) from code to markdown
   - Convert Cell 1 (dev mode) from markdown to code
2. Run all notebooks and verify functionality
3. After testing, restore default state:
   - Convert Cell 0 back to code
   - Convert Cell 1 back to markdown
4. Report results and any issues
"""
```

### Expected Execution Times
- **Quick notebooks** (<30 seconds): 00, 01, 02, 03, 09
- **Medium notebooks** (1-5 minutes): 10, 11, 13, 14
- **Long-running notebooks** (5-30 minutes): 04, 05, 06, 07, 08, 12, 15, 101, 102, 103
- **Manual intervention required:** 16 (GUI automation)

### Reviewing Results

Check for:
- **Import errors:** Indicates missing dependencies or broken imports
- **HEC-RAS errors:** Check HDF files exist and compute messages are clean
- **Warnings:** Review pandas/numpy/geopandas deprecation warnings
- **Data quality:** Spot-check DataFrames, plots, and extracted values
- **Performance:** Note if runtimes significantly increase/decrease

---

## Pre-Commit Checklist for Notebooks

Before committing modified notebooks:

1. **Verify Import Cell State:**
   - [ ] Cell 0 is code (pip mode active)
   - [ ] Cell 1 is markdown (dev mode inactive)
   - [ ] Notebook-specific imports in Cell 2 (if applicable)

2. **Clear All Outputs:**
   ```python
   # Run this to clear outputs from all notebooks
   python -c "import json; from pathlib import Path; import glob;
   notebooks = glob.glob(r'C:\GH\ras-commander\examples\*.ipynb');
   [json.dump((lambda nb: (nb.update({'cells': [dict(cell, outputs=[], execution_count=None)
   if cell['cell_type'] == 'code' else cell for cell in nb['cells']]}) or nb))
   (json.load(open(nb, encoding='utf-8'))), open(nb, 'w', encoding='utf-8'),
   indent=1, ensure_ascii=False) for nb in notebooks]"
   ```

3. **Verify Notebook-Specific Imports Preserved:**
   - Check notebooks 03, 04-09, 12, 16, 101, 102, 105 retain their special imports
   - Ensure imports are consolidated in Cell 2, not scattered

4. **Git Diff Review:**
   - Verify only intended cells were modified
   - Check no accidental deletions of content cells
   - Ensure no large binary outputs were committed

---

General Tips
- Use two-digit plan numbers (e.g., "01").
- Keep original projects immutable; use `dest_folder`/suffixes.
- Plotting: GeoPandas for spatial layers; xarray/pandas for timeseries.
- Logging: library functions are decorated; prefer `get_logger(__name__)` for extra context.


==================================================

File: C:\GH\ras-commander\examples\flood_polygons.geojson
==================================================
{
"type": "FeatureCollection",
"name": "flood_polygons",
"crs": { "type": "name", "properties": { "name": "urn:ogc:def:crs:EPSG::2271" } },
"features": [
{ "type": "Feature", "properties": { "classification": "ambiguous", "mesh_name": "BaldEagleCr", "cell_id": 10, "area_acres": 795.65021094283441 }, "geometry": { "type": "MultiPolygon", "coordinates": [ [ [ [ 1967375.0, 291375.0 ], [ 1967375.0, 291625.0 ], [ 1967625.0, 291625.0 ], [ 1967625.0, 291375.0 ], [ 1967375.0, 291375.0 ] ] ], [ [ [ 1969875.0, 292875.0 ], [ 1970125.0, 292875.0 ], [ 1970125.0, 293125.0 ], [ 1969875.0, 293125.0 ], [ 1969875.0, 293375.0 ], [ 1970125.0, 293375.0 ], [ 1970375.0, 293375.0 ], [ 1970375.0, 293125.0 ], [ 1970625.0, 293125.0 ], [ 1970625.0, 292875.0 ], [ 1970875.0, 292875.0 ], [ 1970875.0, 292625.0 ], [ 1971125.0, 292625.0 ], [ 1971125.0, 292375.0 ], [ 1971125.0, 292125.0 ], [ 1971268.222361444029957, 291981.777638555911835 ], [ 1971194.582610715879127, 291875.0 ], [ 1971159.15496135991998, 291823.629908433998935 ], [ 1970971.95266490872018, 291528.047335091221612 ], [ 1970778.075113887898624, 291221.924886112101376 ], [ 1970716.689352683257312, 291125.0 ], [ 1970524.232588320039213, 290821.120898374996614 ], [ 1970483.27829177165404, 290766.721708228287753 ], [ 1970268.528428039280698, 290481.47157196077751 ], [ 1970188.37156687467359, 290375.0 ], [ 1969946.40363244060427, 290053.59636755939573 ], [ 1969731.653768708230928, 289768.346231291885488 ], [ 1969625.0, 289875.0 ], [ 1969625.0, 290125.0 ], [ 1969625.0, 290375.0 ], [ 1969625.0, 290625.0 ], [ 1969375.0, 290625.0 ], [ 1969375.0, 290875.0 ], [ 1969625.0, 290875.0 ], [ 1969875.0, 290875.0 ], [ 1969875.0, 291125.0 ], [ 1969875.0, 291375.0 ], [ 1969875.0, 291625.0 ], [ 1969875.0, 291875.0 ], [ 1969875.0, 292125.0 ], [ 1969625.0, 292125.0 ], [ 1969625.0, 292375.0 ], [ 1969625.0, 292625.0 ], [ 1969875.0, 292625.0 ], [ 1969875.0, 292875.0 ] ] ], [ [ [ 1967125.0, 291625.0 ], [ 1967125.0, 291875.0 ], [ 1967375.0, 291875.0 ], [ 1967375.0, 291625.0 ], [ 1967125.0, 291625.0 ] ] ], [ [ [ 1966875.0, 291875.0 ], [ 1966875.0, 292125.0 ], [ 1967125.0, 292125.0 ], [ 1967125.0, 291875.0 ], [ 1966875.0, 291875.0 ] ] ], [ [ [ 1968375.0, 291875.0 ], [ 1968125.0, 291875.0 ], [ 1967875.0, 291875.0 ], [ 1967875.0, 292125.0 ], [ 1968125.0, 292125.0 ], [ 1968375.0, 292125.0 ], [ 1968375.0, 291875.0 ] ] ], [ [ [ 1973875.0, 293125.0 ], [ 1974125.0, 293125.0 ], [ 1974125.0, 292730.211061998503283 ], [ 1974033.059430368477479, 292716.940569631522521 ], [ 1973632.010519499890506, 292659.054083483002614 ], [ 1973625.0, 292659.573381223715842 ], [ 1973191.671768725616857, 292691.671768725675065 ], [ 1973375.0, 292875.0 ], [ 1973625.0, 292875.0 ], [ 1973625.0, 293125.0 ], [ 1973875.0, 293125.0 ] ] ], [ [ [ 1974625.0, 293125.0 ], [ 1974875.0, 293125.0 ], [ 1974875.0, 292838.464353392249905 ], [ 1974625.0, 292802.379922927648295 ], [ 1974625.0, 293125.0 ] ] ], [ [ [ 1975125.0, 292874.548783856793307 ], [ 1975125.0, 293125.0 ], [ 1975375.0, 293125.0 ], [ 1975375.0, 292910.633214321394917 ], [ 1975125.0, 292874.548783856793307 ] ] ], [ [ [ 1967955.15116493916139, 294294.848835060896818 ], [ 1968005.056607259437442, 294375.0 ], [ 1968185.044898180058226, 294664.072103601007257 ], [ 1968239.674949259730056, 294760.325050740211736 ], [ 1968375.0, 294625.0 ], [ 1968375.0, 294375.0 ], [ 1968375.0, 294125.0 ], [ 1968375.0, 293875.0 ], [ 1968375.0, 293625.0 ], [ 1968375.0, 293375.0 ], [ 1968125.0, 293375.0 ], [ 1967875.0, 293375.0 ], [ 1967625.0, 293375.0 ], [ 1967625.0, 293125.0 ], [ 1967625.0, 292875.0 ], [ 1967375.0, 292875.0 ], [ 1967375.0, 292625.0 ], [ 1967125.0, 292625.0 ], [ 1967125.0, 292375.0 ], [ 1966875.0, 292375.0 ], [ 1966875.0, 292720.762558158952743 ], [ 1966959.246386470505968, 292790.75361352955224 ], [ 1967060.651538825361058, 292875.0 ], [ 1967082.284987119957805, 292892.972852497012354 ], [ 1967283.639537033857778, 293216.360462966142222 ], [ 1967475.500002149725333, 293524.499997850391082 ], [ 1967538.075475185876712, 293625.0 ], [ 1967763.290699823293835, 293986.709300176706165 ], [ 1967955.15116493916139, 294294.848835060896818 ] ] ], [ [ [ 1971125.0, 293625.0 ], [ 1971125.0, 293875.0 ], [ 1971375.0, 293875.0 ], [ 1971375.0, 293625.0 ], [ 1971125.0, 293625.0 ] ] ], [ [ [ 1970125.0, 295625.0 ], [ 1970375.0, 295625.0 ], [ 1970375.0, 295375.0 ], [ 1970125.0, 295375.0 ], [ 1970125.0, 295625.0 ] ] ], [ [ [ 1969875.0, 295625.0 ], [ 1969875.0, 295875.0 ], [ 1970125.0, 295875.0 ], [ 1970125.0, 295625.0 ], [ 1969875.0, 295625.0 ] ] ], [ [ [ 1980375.0, 296375.0 ], [ 1980375.0, 296625.0 ], [ 1980625.0, 296625.0 ], [ 1980764.261157453991473, 296485.738842546066735 ], [ 1980480.477373670320958, 296269.522626329620834 ], [ 1980375.0, 296375.0 ] ] ], [ [ [ 1982375.0, 300875.0 ], [ 1982125.0, 300875.0 ], [ 1982125.0, 301125.0 ], [ 1982375.0, 301125.0 ], [ 1982375.0, 300875.0 ] ] ], [ [ [ 1982875.0, 301875.0 ], [ 1982625.0, 301875.0 ], [ 1982625.0, 302125.0 ], [ 1982875.0, 302125.0 ], [ 1983125.0, 302125.0 ], [ 1983125.0, 301875.0 ], [ 1982875.0, 301875.0 ] ] ], [ [ [ 1983625.0, 302125.0 ], [ 1983375.0, 302125.0 ], [ 1983375.0, 302375.0 ], [ 1983625.0, 302375.0 ], [ 1983625.0, 302125.0 ] ] ], [ [ [ 1981875.0, 302125.0 ], [ 1981625.0, 302125.0 ], [ 1981625.0, 302375.0 ], [ 1981875.0, 302375.0 ], [ 1981875.0, 302625.0 ], [ 1982125.0, 302625.0 ], [ 1982125.0, 302375.0 ], [ 1982125.0, 302125.0 ], [ 1981875.0, 302125.0 ] ] ], [ [ [ 1982375.0, 303375.0 ], [ 1982235.022394865285605, 303514.977605134656187 ], [ 1982464.530591586604714, 303785.469408413337078 ], [ 1982625.0, 303625.0 ], [ 1982625.0, 303375.0 ], [ 1982375.0, 303375.0 ] ] ], [ [ [ 1984625.0, 305375.0 ], [ 1984625.0, 305625.0 ], [ 1984875.0, 305625.0 ], [ 1984875.0, 305375.0 ], [ 1984625.0, 305375.0 ] ] ], [ [ [ 1985875.0, 306625.0 ], [ 1985875.0, 306875.0 ], [ 1986125.0, 306875.0 ], [ 1986125.0, 306625.0 ], [ 1985875.0, 306625.0 ] ] ], [ [ [ 1993875.0, 307375.0 ], [ 1994125.0, 307375.0 ], [ 1994286.050033164443448, 307213.949966835556552 ], [ 1994011.275258389534429, 306988.724741610349156 ], [ 1993875.0, 307125.0 ], [ 1993875.0, 307375.0 ] ] ], [ [ [ 1988375.0, 307125.0 ], [ 1988375.0, 307375.0 ], [ 1988625.0, 307375.0 ], [ 1988625.0, 307125.0 ], [ 1988375.0, 307125.0 ] ] ], [ [ [ 1994625.0, 307875.0 ], [ 1994875.0, 307875.0 ], [ 1994875.0, 308125.0 ], [ 1995125.0, 308125.0 ], [ 1995247.761744876159355, 308002.238255123840645 ], [ 1994972.986970101250336, 307777.013029898633249 ], [ 1994698.212195326574147, 307551.787804673425853 ], [ 1994625.0, 307625.0 ], [ 1994625.0, 307875.0 ] ] ], [ [ [ 1987625.0, 308125.0 ], [ 1987375.0, 308125.0 ], [ 1987375.0, 308375.0 ], [ 1987625.0, 308375.0 ], [ 1987625.0, 308125.0 ] ] ], [ [ [ 1995375.0, 308875.0 ], [ 1995375.0, 309125.0 ], [ 1995625.0, 309125.0 ], [ 1995625.0, 308875.0 ], [ 1995375.0, 308875.0 ] ] ], [ [ [ 1996221.519480956485495, 309028.480519043572713 ], [ 1996027.641929935663939, 308722.358070064452477 ], [ 1995875.0, 308875.0 ], [ 1995875.0, 309125.0 ], [ 1996125.0, 309125.0 ], [ 1996125.0, 309375.0 ], [ 1996440.981818896485493, 309375.0 ], [ 1996221.519480956485495, 309028.480519043572713 ] ] ], [ [ [ 1995875.0, 309375.0 ], [ 1995875.0, 309125.0 ], [ 1995625.0, 309125.0 ], [ 1995625.0, 309375.0 ], [ 1995875.0, 309375.0 ] ] ], [ [ [ 1987523.19201434077695, 309476.80798565922305 ], [ 1987760.996892389841378, 309739.003107610158622 ], [ 1987875.0, 309625.0 ], [ 1987875.0, 309375.0 ], [ 1987625.0, 309375.0 ], [ 1987523.19201434077695, 309476.80798565922305 ] ] ], [ [ [ 1989022.23288223403506, 311227.767117765906733 ], [ 1989125.0, 311125.0 ], [ 1989125.0, 310875.0 ], [ 1988875.0, 310875.0 ], [ 1988797.23288223426789, 310952.767117765673902 ], [ 1989022.23288223403506, 311227.767117765906733 ] ] ], [ [ [ 1991875.0, 313875.0 ], [ 1991875.0, 313625.0 ], [ 1991625.0, 313625.0 ], [ 1991363.417010633274913, 313625.0 ], [ 1991523.286694810492918, 313976.713305189507082 ], [ 1991625.0, 313875.0 ], [ 1991875.0, 313875.0 ] ] ], [ [ [ 1993125.0, 314625.0 ], [ 1992875.0, 314625.0 ], [ 1992875.0, 314848.133553712745197 ], [ 1993125.0, 314854.383553712745197 ], [ 1993125.0, 314625.0 ] ] ], [ [ [ 1998375.0, 315375.0 ], [ 1998125.0, 315375.0 ], [ 1998125.0, 315625.0 ], [ 1998125.0, 315875.0 ], [ 1998375.0, 315875.0 ], [ 1998375.0, 315625.0 ], [ 1998375.0, 315375.0 ] ] ], [ [ [ 1995125.0, 316875.0 ], [ 1995375.0, 316875.0 ], [ 1995375.0, 316625.0 ], [ 1995125.0, 316625.0 ], [ 1995125.0, 316875.0 ] ] ], [ [ [ 1995875.0, 317574.561949152790476 ], [ 1995875.0, 317375.0 ], [ 1995625.0, 317375.0 ], [ 1995445.872684899717569, 317554.127315100340638 ], [ 1995875.0, 317574.561949152790476 ] ] ], [ [ [ 2000375.0, 317375.0 ], [ 2000375.0, 317625.0 ], [ 2000375.0, 317875.0 ], [ 2000625.0, 317875.0 ], [ 2000625.0, 318125.0 ], [ 2000875.0, 318125.0 ], [ 2000875.0, 317875.0 ], [ 2001125.0, 317875.0 ], [ 2001125.0, 318125.0 ], [ 2001375.0, 318125.0 ], [ 2001375.0, 317875.0 ], [ 2001375.0, 317625.0 ], [ 2001375.0, 317375.0 ], [ 2001125.0, 317375.0 ], [ 2000875.0, 317375.0 ], [ 2000625.0, 317375.0 ], [ 2000375.0, 317375.0 ] ] ], [ [ [ 2001375.0, 318625.0 ], [ 2001625.0, 318625.0 ], [ 2001625.0, 318375.0 ], [ 2001375.0, 318375.0 ], [ 2001375.0, 318625.0 ] ] ], [ [ [ 2004625.0, 321125.0 ], [ 2004625.0, 321375.0 ], [ 2004875.0, 321375.0 ], [ 2004875.0, 321125.0 ], [ 2004625.0, 321125.0 ] ] ], [ [ [ 2005125.0, 321375.0 ], [ 2004875.0, 321375.0 ], [ 2004875.0, 321625.0 ], [ 2005125.0, 321625.0 ], [ 2005125.0, 321375.0 ] ] ], [ [ [ 2000719.88580773328431, 322280.11419226671569 ], [ 2000990.663948226952925, 322509.336051772930659 ], [ 2001125.0, 322375.0 ], [ 2001125.0, 322125.0 ], [ 2000875.0, 322125.0 ], [ 2000719.88580773328431, 322280.11419226671569 ] ] ], [ [ [ 2001261.442088720854372, 322738.557911279203836 ], [ 2001532.220229214522988, 322967.779770785477012 ], [ 2001625.0, 322875.0 ], [ 2001625.0, 322625.0 ], [ 2001375.0, 322625.0 ], [ 2001261.442088720854372, 322738.557911279203836 ] ] ], [ [ [ 2005125.0, 323380.201065170695074 ], [ 2005344.927198968362063, 323344.927198968245648 ], [ 2005352.183974175946787, 323250.94473008584464 ], [ 2005265.429, 323086.270599999988917 ], [ 2005156.825999999884516, 323118.102399999974295 ], [ 2004965.835, 323153.6791 ], [ 2005108.617624151753262, 323367.808860811113846 ], [ 2005125.0, 323380.201065170695074 ] ] ], [ [ [ 2002375.0, 323363.963568864972331 ], [ 2002640.221391269238666, 323263.745681104890537 ], [ 2002613.391619356349111, 323125.0 ], [ 2002375.0, 323125.0 ], [ 2002375.0, 323363.963568864972331 ] ] ], [ [ [ 2007375.0, 323875.0 ], [ 2007375.0, 323625.0 ], [ 2007125.0, 323625.0 ], [ 2007125.0, 323875.0 ], [ 2007375.0, 323875.0 ] ] ], [ [ [ 2006125.0, 326375.0 ], [ 2006125.0, 326625.0 ], [ 2006375.0, 326625.0 ], [ 2006375.0, 326375.0 ], [ 2006125.0, 326375.0 ] ] ], [ [ [ 2002875.0, 326875.0 ], [ 2002720.589737881906331, 326720.589737881789915 ], [ 2002625.0, 326774.443111336440779 ], [ 2002297.006994920084253, 326959.227902930986602 ], [ 2002235.943278753664345, 326985.943278753547929 ], [ 2002375.0, 327125.0 ], [ 2002625.0, 327125.0 ], [ 2002875.0, 327125.0 ], [ 2002875.0, 326875.0 ] ] ], [ [ [ 2007375.0, 327125.0 ], [ 2007125.0, 327125.0 ], [ 2007125.0, 326875.0 ], [ 2006875.0, 326875.0 ], [ 2006625.0, 326875.0 ], [ 2006625.0, 327198.01929838018259 ], [ 2006875.0, 327230.486830847687088 ], [ 2006948.066138840047643, 327239.975939787982497 ], [ 2006986.789839697536081, 327263.210160302405711 ], [ 2007375.0, 327496.136256483208854 ], [ 2007375.0, 327125.0 ] ] ], [ [ [ 2004625.0, 327375.0 ], [ 2004769.156847949139774, 327519.156847949197982 ], [ 2004875.0, 327413.313695900083985 ], [ 2004875.0, 327125.0 ], [ 2004625.0, 327125.0 ], [ 2004625.0, 327375.0 ] ] ], [ [ [ 2007875.0, 327375.0 ], [ 2007625.0, 327375.0 ], [ 2007455.539839698001742, 327544.46016030194005 ], [ 2007768.039839698467404, 327731.960160301590804 ], [ 2007875.0, 327625.0 ], [ 2007875.0, 327375.0 ] ] ], [ [ [ 2002125.0, 328375.0 ], [ 2002375.0, 328375.0 ], [ 2002375.0, 328125.0 ], [ 2002125.0, 328125.0 ], [ 2002125.0, 328375.0 ] ] ], [ [ [ 2001375.0, 328625.0 ], [ 2001375.0, 328875.0 ], [ 2001625.0, 328875.0 ], [ 2001625.0, 328625.0 ], [ 2001375.0, 328625.0 ] ] ], [ [ [ 2003947.537490927381441, 328875.0 ], [ 2003625.0, 328875.0 ], [ 2003625.0, 329232.355279027775396 ], [ 2003766.255054469918832, 329177.137394098972436 ], [ 2003947.537490927381441, 328875.0 ] ] ], [ [ [ 2004875.0, 337375.0 ], [ 2004875.0, 337777.385589807992801 ], [ 2005125.0, 337777.385589807992801 ], [ 2005272.936185600003228, 337777.385589807992801 ], [ 2005375.0, 337727.439467867545318 ], [ 2005375.0, 337375.0 ], [ 2005125.0, 337375.0 ], [ 2004875.0, 337375.0 ] ] ], [ [ [ 2010375.0, 322375.0 ], [ 2010518.892821161542088, 322231.107178838399705 ], [ 2010254.688275706255808, 321995.311724293627776 ], [ 2010125.0, 322125.0 ], [ 2010125.0, 322375.0 ], [ 2010375.0, 322375.0 ] ] ], [ [ [ 2011125.0, 323125.0 ], [ 2011375.0, 323125.0 ], [ 2011625.0, 323125.0 ], [ 2011720.56660290970467, 323029.433397090237122 ], [ 2011375.0, 322815.650668170070276 ], [ 2011257.21581757068634, 322742.784182429371867 ], [ 2011125.0, 322875.0 ], [ 2011125.0, 323125.0 ] ] ], [ [ [ 2008625.0, 323375.0 ], [ 2008625.0, 323125.0 ], [ 2008375.0, 323125.0 ], [ 2008375.0, 322875.0 ], [ 2008125.0, 322875.0 ], [ 2008125.0, 323125.0 ], [ 2008125.0, 323375.0 ], [ 2008375.0, 323375.0 ], [ 2008375.0, 323625.0 ], [ 2008625.0, 323625.0 ], [ 2008625.0, 323375.0 ] ] ], [ [ [ 2010875.0, 323125.0 ], [ 2010875.0, 323375.0 ], [ 2011125.0, 323375.0 ], [ 2011125.0, 323125.0 ], [ 2010875.0, 323125.0 ] ] ], [ [ [ 2012125.0, 323625.0 ], [ 2011875.0, 323625.0 ], [ 2011875.0, 323875.0 ], [ 2012125.0, 323875.0 ], [ 2012125.0, 323625.0 ] ] ], [ [ [ 2013875.0, 323875.0 ], [ 2013875.0, 324125.0 ], [ 2014125.0, 324125.0 ], [ 2014125.0, 323875.0 ], [ 2014298.791663677198812, 323701.20833632274298 ], [ 2013966.767060260055587, 323626.672200862027239 ], [ 2013875.0, 323614.831289860594552 ], [ 2013875.0, 323875.0 ] ] ], [ [ [ 2007625.0, 324125.0 ], [ 2007625.0, 324375.0 ], [ 2007875.0, 324375.0 ], [ 2007875.0, 324125.0 ], [ 2007625.0, 324125.0 ] ] ], [ [ [ 2015287.130086361663416, 324212.869913638220169 ], [ 2014966.770804925588891, 324033.229195074411109 ], [ 2014625.0, 323841.582014740735758 ], [ 2014425.322187130106613, 323729.613147709984332 ], [ 2014375.0, 323718.316330191039015 ], [ 2014375.0, 324125.0 ], [ 2014625.0, 324125.0 ], [ 2014875.0, 324125.0 ], [ 2014875.0, 324375.0 ], [ 2015125.0, 324375.0 ], [ 2015125.0, 324625.0 ], [ 2015375.0, 324625.0 ], [ 2015625.0, 324625.0 ], [ 2015625.0, 324875.0 ], [ 2015875.0, 324875.0 ], [ 2015875.0, 324569.132067444443237 ], [ 2015755.173654922051355, 324494.826345077890437 ], [ 2015426.656851910054684, 324291.109221422986593 ], [ 2015375.0, 324262.1427624077769 ], [ 2015287.130086361663416, 324212.869913638220169 ] ] ], [ [ [ 2014125.0, 324375.0 ], [ 2014125.0, 324625.0 ], [ 2014375.0, 324625.0 ], [ 2014375.0, 324375.0 ], [ 2014125.0, 324375.0 ] ] ], [ [ [ 2008375.0, 326375.0 ], [ 2008375.0, 326625.0 ], [ 2008625.0, 326625.0 ], [ 2008625.0, 326375.0 ], [ 2008375.0, 326375.0 ] ] ], [ [ [ 2009125.0, 326375.0 ], [ 2009125.0, 326625.0 ], [ 2009375.0, 326625.0 ], [ 2009375.0, 326375.0 ], [ 2009125.0, 326375.0 ] ] ], [ [ [ 2010125.0, 326625.0 ], [ 2010125.0, 326875.0 ], [ 2010375.0, 326875.0 ], [ 2010375.0, 326625.0 ], [ 2010375.0, 326375.0 ], [ 2010125.0, 326375.0 ], [ 2010125.0, 326625.0 ] ] ], [ [ [ 2008125.0, 327125.0 ], [ 2008125.0, 326875.0 ], [ 2008125.0, 326625.0 ], [ 2007875.0, 326625.0 ], [ 2007875.0, 326875.0 ], [ 2007875.0, 327125.0 ], [ 2008125.0, 327125.0 ] ] ], [ [ [ 2011375.0, 326625.0 ], [ 2011125.0, 326625.0 ], [ 2011125.0, 326875.0 ], [ 2011125.0, 327125.0 ], [ 2011375.0, 327125.0 ], [ 2011375.0, 326875.0 ], [ 2011375.0, 326625.0 ] ] ], [ [ [ 2010625.0, 327125.0 ], [ 2010875.0, 327125.0 ], [ 2010875.0, 326875.0 ], [ 2010625.0, 326875.0 ], [ 2010625.0, 327125.0 ] ] ], [ [ [ 2010875.0, 327125.0 ], [ 2010875.0, 327375.0 ], [ 2011125.0, 327375.0 ], [ 2011125.0, 327125.0 ], [ 2010875.0, 327125.0 ] ] ], [ [ [ 2009875.0, 327375.0 ], [ 2009625.0, 327375.0 ], [ 2009625.0, 327625.0 ], [ 2009875.0, 327625.0 ], [ 2009875.0, 327375.0 ] ] ], [ [ [ 2012375.0, 327625.0 ], [ 2012125.0, 327625.0 ], [ 2012125.0, 327875.0 ], [ 2012375.0, 327875.0 ], [ 2012375.0, 327625.0 ] ] ], [ [ [ 2012625.0, 328125.0 ], [ 2012875.0, 328125.0 ], [ 2013125.0, 328125.0 ], [ 2013125.0, 327875.0 ], [ 2013375.0, 327875.0 ], [ 2013625.0, 327875.0 ], [ 2013625.0, 327625.0 ], [ 2013375.0, 327625.0 ], [ 2013125.0, 327625.0 ], [ 2012875.0, 327625.0 ], [ 2012875.0, 327875.0 ], [ 2012625.0, 327875.0 ], [ 2012625.0, 328125.0 ] ] ], [ [ [ 2010625.0, 328375.0 ], [ 2010625.0, 328625.0 ], [ 2010875.0, 328625.0 ], [ 2010875.0, 328375.0 ], [ 2010625.0, 328375.0 ] ] ], [ [ [ 2009625.0, 328375.0 ], [ 2009625.0, 328625.0 ], [ 2009875.0, 328625.0 ], [ 2009875.0, 328375.0 ], [ 2009625.0, 328375.0 ] ] ], [ [ [ 2013875.0, 328875.0 ], [ 2014125.0, 328875.0 ], [ 2014125.0, 328625.0 ], [ 2013875.0, 328625.0 ], [ 2013875.0, 328875.0 ] ] ], [ [ [ 2012375.0, 329375.0 ], [ 2012625.0, 329375.0 ], [ 2012625.0, 329125.0 ], [ 2012375.0, 329125.0 ], [ 2012125.0, 329125.0 ], [ 2012125.0, 329375.0 ], [ 2012375.0, 329375.0 ] ] ], [ [ [ 2010375.0, 329125.0 ], [ 2010125.0, 329125.0 ], [ 2010125.0, 329375.0 ], [ 2010375.0, 329375.0 ], [ 2010375.0, 329125.0 ] ] ], [ [ [ 2012625.0, 329375.0 ], [ 2012625.0, 329625.0 ], [ 2012875.0, 329625.0 ], [ 2012875.0, 329375.0 ], [ 2012625.0, 329375.0 ] ] ], [ [ [ 2015625.0, 329375.0 ], [ 2015625.0, 329625.0 ], [ 2015875.0, 329625.0 ], [ 2015875.0, 329375.0 ], [ 2015625.0, 329375.0 ] ] ], [ [ [ 2010875.0, 329875.0 ], [ 2010875.0, 330125.0 ], [ 2011125.0, 330125.0 ], [ 2011125.0, 329875.0 ], [ 2010875.0, 329875.0 ] ] ], [ [ [ 2010125.0, 329875.0 ], [ 2010125.0, 330125.0 ], [ 2010375.0, 330125.0 ], [ 2010375.0, 329875.0 ], [ 2010125.0, 329875.0 ] ] ], [ [ [ 2013625.0, 330125.0 ], [ 2013625.0, 329875.0 ], [ 2013375.0, 329875.0 ], [ 2013375.0, 330125.0 ], [ 2013375.0, 330375.0 ], [ 2013625.0, 330375.0 ], [ 2013625.0, 330125.0 ] ] ], [ [ [ 2016375.0, 330375.0 ], [ 2016625.0, 330375.0 ], [ 2016625.0, 330125.0 ], [ 2016625.0, 329875.0 ], [ 2016375.0, 329875.0 ], [ 2016375.0, 330125.0 ], [ 2016375.0, 330375.0 ] ] ], [ [ [ 2012375.0, 330125.0 ], [ 2012375.0, 330375.0 ], [ 2012625.0, 330375.0 ], [ 2012875.0, 330375.0 ], [ 2013125.0, 330375.0 ], [ 2013125.0, 330125.0 ], [ 2012875.0, 330125.0 ], [ 2012625.0, 330125.0 ], [ 2012375.0, 330125.0 ] ] ], [ [ [ 2012375.0, 330375.0 ], [ 2012125.0, 330375.0 ], [ 2012125.0, 330625.0 ], [ 2012375.0, 330625.0 ], [ 2012375.0, 330375.0 ] ] ], [ [ [ 2014625.0, 330375.0 ], [ 2014375.0, 330375.0 ], [ 2014375.0, 330625.0 ], [ 2014625.0, 330625.0 ], [ 2014625.0, 330375.0 ] ] ], [ [ [ 2018375.0, 330625.0 ], [ 2018375.0, 330375.0 ], [ 2018125.0, 330375.0 ], [ 2018125.0, 330625.0 ], [ 2018125.0, 330875.0 ], [ 2018375.0, 330875.0 ], [ 2018375.0, 330625.0 ] ] ], [ [ [ 2009625.0, 330625.0 ], [ 2009555.910272921202704, 330555.910272921260912 ], [ 2009530.948077919892967, 330599.594114172970876 ], [ 2009303.612880811095238, 330803.61288081103703 ], [ 2009040.099367298651487, 331040.099367298651487 ], [ 2009125.0, 331125.0 ], [ 2009375.0, 331125.0 ], [ 2009375.0, 330875.0 ], [ 2009625.0, 330875.0 ], [ 2009625.0, 330625.0 ] ] ], [ [ [ 2012625.0, 330625.0 ], [ 2012375.0, 330625.0 ], [ 2012375.0, 330875.0 ], [ 2012375.0, 331125.0 ], [ 2012625.0, 331125.0 ], [ 2012625.0, 330875.0 ], [ 2012625.0, 330625.0 ] ] ], [ [ [ 2013125.0, 331125.0 ], [ 2013375.0, 331125.0 ], [ 2013375.0, 330875.0 ], [ 2013125.0, 330875.0 ], [ 2012875.0, 330875.0 ], [ 2012875.0, 331125.0 ], [ 2013125.0, 331125.0 ] ] ], [ [ [ 2017875.0, 331125.0 ], [ 2017875.0, 331375.0 ], [ 2018125.0, 331375.0 ], [ 2018125.0, 331125.0 ], [ 2017875.0, 331125.0 ] ] ], [ [ [ 2009375.0, 331625.0 ], [ 2009375.0, 331875.0 ], [ 2009625.0, 331875.0 ], [ 2009625.0, 331625.0 ], [ 2009375.0, 331625.0 ] ] ], [ [ [ 2012875.0, 332125.0 ], [ 2012625.0, 332125.0 ], [ 2012625.0, 332375.0 ], [ 2012875.0, 332375.0 ], [ 2012875.0, 332125.0 ] ] ], [ [ [ 2014875.0, 332125.0 ], [ 2014875.0, 332375.0 ], [ 2015125.0, 332375.0 ], [ 2015125.0, 332125.0 ], [ 2014875.0, 332125.0 ] ] ], [ [ [ 2008625.0, 332625.0 ], [ 2008875.0, 332625.0 ], [ 2008875.0, 332375.0 ], [ 2008625.0, 332375.0 ], [ 2008625.0, 332625.0 ] ] ], [ [ [ 2008375.0, 332625.0 ], [ 2008375.0, 332875.0 ], [ 2008625.0, 332875.0 ], [ 2008625.0, 332625.0 ], [ 2008375.0, 332625.0 ] ] ], [ [ [ 2019875.0, 332875.0 ], [ 2020125.0, 332875.0 ], [ 2020125.0, 332625.0 ], [ 2019875.0, 332625.0 ], [ 2019625.0, 332625.0 ], [ 2019375.0, 332625.0 ], [ 2019375.0, 332875.0 ], [ 2019625.0, 332875.0 ], [ 2019875.0, 332875.0 ] ] ], [ [ [ 2012125.0, 333375.0 ], [ 2011875.0, 333375.0 ], [ 2011875.0, 333625.0 ], [ 2011875.0, 333986.382859602512326 ], [ 2012020.247338050045073, 333903.062681186012924 ], [ 2012162.843584816204384, 333625.0 ], [ 2012207.412695959908888, 333538.09023327199975 ], [ 2012241.619971779873595, 333491.61997177999001 ], [ 2012125.0, 333375.0 ] ] ], [ [ [ 2008125.0, 333625.0 ], [ 2008125.0, 333875.0 ], [ 2008375.0, 333875.0 ], [ 2008375.0, 333625.0 ], [ 2008125.0, 333625.0 ] ] ], [ [ [ 2010375.0, 334125.0 ], [ 2010375.0, 333875.0 ], [ 2010125.0, 333875.0 ], [ 2009875.0, 333875.0 ], [ 2009875.0, 334125.0 ], [ 2009875.0, 334375.0 ], [ 2010125.0, 334375.0 ], [ 2010125.0, 334125.0 ], [ 2010375.0, 334125.0 ] ] ], [ [ [ 2008125.0, 334375.0 ], [ 2008375.0, 334375.0 ], [ 2008375.0, 334125.0 ], [ 2008125.0, 334125.0 ], [ 2008125.0, 334375.0 ] ] ], [ [ [ 2009625.0, 335125.0 ], [ 2009625.0, 335478.111655970860738 ], [ 2009971.047671458451077, 335221.047671458509285 ], [ 2009875.0, 335125.0 ], [ 2009625.0, 335125.0 ] ] ], [ [ [ 2020625.0, 336375.0 ], [ 2020541.324557156302035, 336458.67544284381438 ], [ 2020875.0, 336685.791474982397631 ], [ 2020875.0, 336375.0 ], [ 2020625.0, 336375.0 ] ] ], [ [ [ 2021731.340660215821117, 337268.659339784178883 ], [ 2021875.0, 337125.0 ], [ 2021875.0, 336875.0 ], [ 2021625.0, 336875.0 ], [ 2021375.0, 336875.0 ], [ 2021285.084621568443254, 336964.915378431556746 ], [ 2021625.0, 337196.278619772638194 ], [ 2021731.340660215821117, 337268.659339784178883 ] ] ], [ [ [ 2022286.267219109926373, 337646.369839275022969 ], [ 2022375.0, 337703.769679732096847 ], [ 2022478.972284559160471, 337771.027715440897737 ], [ 2022625.0, 337625.0 ], [ 2022625.0, 337375.0 ], [ 2022375.0, 337375.0 ], [ 2022125.0, 337375.0 ], [ 2022028.844685980817303, 337471.155314019299112 ], [ 2022286.267219109926373, 337646.369839275022969 ] ] ], [ [ [ 2023125.0, 329875.0 ], [ 2023209.070283957989886, 329790.929716042068321 ], [ 2022875.0, 329554.443120293261018 ], [ 2022875.0, 329875.0 ], [ 2023125.0, 329875.0 ] ] ], [ [ [ 2024975.861002979800105, 331024.138997020141687 ], [ 2024625.0, 330786.995789472362958 ], [ 2024528.337418074253947, 330721.662581925687846 ], [ 2024375.0, 330875.0 ], [ 2024375.0, 331125.0 ], [ 2024625.0, 331125.0 ], [ 2024875.0, 331125.0 ], [ 2024975.861002979800105, 331024.138997020141687 ] ] ], [ [ [ 2023625.0, 331375.0 ], [ 2023375.0, 331375.0 ], [ 2023375.0, 331625.0 ], [ 2023625.0, 331625.0 ], [ 2023625.0, 331375.0 ] ] ], [ [ [ 2023875.0, 331375.0 ], [ 2023875.0, 331625.0 ], [ 2024125.0, 331625.0 ], [ 2024125.0, 331375.0 ], [ 2023875.0, 331375.0 ] ] ], [ [ [ 2025375.0, 332125.0 ], [ 2025375.0, 332375.0 ], [ 2025625.0, 332375.0 ], [ 2025625.0, 332125.0 ], [ 2025625.0, 331875.0 ], [ 2025875.0, 331875.0 ], [ 2026020.082701092818752, 331729.917298907181248 ], [ 2025721.733644489198923, 331528.266355510859285 ], [ 2025375.0, 331293.912785520078614 ], [ 2025375.0, 331625.0 ], [ 2025375.0, 331875.0 ], [ 2025125.0, 331875.0 ], [ 2025125.0, 332125.0 ], [ 2025375.0, 332125.0 ] ] ], [ [ [ 2026375.0, 332375.0 ], [ 2026625.0, 332375.0 ], [ 2026765.955342601984739, 332234.044657397898845 ], [ 2026467.60628599836491, 332032.39371400163509 ], [ 2026375.0, 332125.0 ], [ 2026375.0, 332375.0 ] ] ], [ [ [ 2024625.0, 333125.0 ], [ 2024625.0, 333375.0 ], [ 2024875.0, 333375.0 ], [ 2024875.0, 333125.0 ], [ 2024625.0, 333125.0 ] ] ], [ [ [ 2026125.0, 333625.0 ], [ 2026375.0, 333625.0 ], [ 2026375.0, 333375.0 ], [ 2026125.0, 333375.0 ], [ 2026125.0, 333625.0 ] ] ], [ [ [ 2023625.0, 334875.0 ], [ 2023375.0, 334875.0 ], [ 2023375.0, 335125.0 ], [ 2023625.0, 335125.0 ], [ 2023625.0, 334875.0 ] ] ], [ [ [ 2024625.0, 335375.0 ], [ 2024375.0, 335375.0 ], [ 2024375.0, 335625.0 ], [ 2024625.0, 335625.0 ], [ 2024875.0, 335625.0 ], [ 2025125.0, 335625.0 ], [ 2025125.0, 335375.0 ], [ 2024875.0, 335375.0 ], [ 2024875.0, 335125.0 ], [ 2024625.0, 335125.0 ], [ 2024625.0, 335375.0 ] ] ], [ [ [ 2025625.0, 335875.0 ], [ 2025375.0, 335875.0 ], [ 2025375.0, 336125.0 ], [ 2025625.0, 336125.0 ], [ 2025625.0, 335875.0 ] ] ], [ [ [ 2026875.0, 335875.0 ], [ 2026625.0, 335875.0 ], [ 2026625.0, 336125.0 ], [ 2026875.0, 336125.0 ], [ 2026875.0, 335875.0 ] ] ], [ [ [ 2025375.0, 336625.0 ], [ 2025375.0, 336875.0 ], [ 2025625.0, 336875.0 ], [ 2025625.0, 336625.0 ], [ 2025375.0, 336625.0 ] ] ], [ [ [ 2026875.0, 337125.0 ], [ 2026625.0, 337125.0 ], [ 2026625.0, 336875.0 ], [ 2026375.0, 336875.0 ], [ 2026375.0, 336625.0 ], [ 2026125.0, 336625.0 ], [ 2025875.0, 336625.0 ], [ 2025875.0, 336875.0 ], [ 2026125.0, 336875.0 ], [ 2026125.0, 337125.0 ], [ 2026375.0, 337125.0 ], [ 2026375.0, 337375.0 ], [ 2026625.0, 337375.0 ], [ 2026875.0, 337375.0 ], [ 2026875.0, 337625.0 ], [ 2027125.0, 337625.0 ], [ 2027375.0, 337625.0 ], [ 2027375.0, 337375.0 ], [ 2027375.0, 337125.0 ], [ 2027125.0, 337125.0 ], [ 2026875.0, 337125.0 ] ] ], [ [ [ 2028125.0, 337125.0 ], [ 2027875.0, 337125.0 ], [ 2027875.0, 337375.0 ], [ 2028125.0, 337375.0 ], [ 2028125.0, 337125.0 ] ] ], [ [ [ 2025875.0, 337375.0 ], [ 2025875.0, 337625.0 ], [ 2026125.0, 337625.0 ], [ 2026125.0, 337375.0 ], [ 2025875.0, 337375.0 ] ] ], [ [ [ 2027375.0, 337625.0 ], [ 2027375.0, 337875.0 ], [ 2027625.0, 337875.0 ], [ 2027625.0, 337625.0 ], [ 2027375.0, 337625.0 ] ] ], [ [ [ 2029875.0, 337875.0 ], [ 2030125.0, 337875.0 ], [ 2030125.0, 337625.0 ], [ 2029875.0, 337625.0 ], [ 2029875.0, 337875.0 ] ] ], [ [ [ 2023875.0, 338125.0 ], [ 2024125.0, 338125.0 ], [ 2024125.0, 337875.0 ], [ 2023875.0, 337875.0 ], [ 2023875.0, 338125.0 ] ] ], [ [ [ 2027875.0, 337875.0 ], [ 2027875.0, 338125.0 ], [ 2028125.0, 338125.0 ], [ 2028125.0, 337875.0 ], [ 2027875.0, 337875.0 ] ] ], [ [ [ 2030125.0, 337875.0 ], [ 2030125.0, 338125.0 ], [ 2030375.0, 338125.0 ], [ 2030625.0, 338125.0 ], [ 2030625.0, 337875.0 ], [ 2030375.0, 337875.0 ], [ 2030125.0, 337875.0 ] ] ], [ [ [ 2023625.0, 337875.0 ], [ 2023625.0, 337625.0 ], [ 2023375.0, 337625.0 ], [ 2023375.0, 337875.0 ], [ 2023375.0, 338125.0 ], [ 2023237.981293568154797, 338262.018706431786995 ], [ 2023541.584897171938792, 338458.415102828177623 ], [ 2023875.0, 338674.096089227299672 ], [ 2023996.990302577381954, 338753.009697422676254 ], [ 2024300.593906180933118, 338949.406093819066882 ], [ 2024375.0, 338875.0 ], [ 2024375.0, 338625.0 ], [ 2024375.0, 338375.0 ], [ 2024375.0, 338125.0 ], [ 2024125.0, 338125.0 ], [ 2024125.0, 338375.0 ], [ 2023875.0, 338375.0 ], [ 2023625.0, 338375.0 ], [ 2023625.0, 338125.0 ], [ 2023625.0, 337875.0 ] ] ], [ [ [ 2025875.0, 338875.0 ], [ 2025625.0, 338875.0 ], [ 2025625.0, 339125.0 ], [ 2025875.0, 339125.0 ], [ 2025875.0, 338875.0 ] ] ], [ [ [ 2025625.0, 339625.0 ], [ 2025875.0, 339625.0 ], [ 2025875.0, 339375.0 ], [ 2025625.0, 339375.0 ], [ 2025625.0, 339625.0 ] ] ], [ [ [ 2031375.0, 339875.0 ], [ 2031625.0, 339875.0 ], [ 2031625.0, 339625.0 ], [ 2031375.0, 339625.0 ], [ 2031375.0, 339875.0 ] ] ], [ [ [ 2027375.0, 339875.0 ], [ 2027375.0, 340125.0 ], [ 2027625.0, 340125.0 ], [ 2027625.0, 339875.0 ], [ 2027375.0, 339875.0 ] ] ], [ [ [ 2026625.0, 340125.0 ], [ 2026875.0, 340125.0 ], [ 2026875.0, 339875.0 ], [ 2026625.0, 339875.0 ], [ 2026625.0, 340125.0 ] ] ], [ [ [ 2027375.0, 340375.0 ], [ 2027375.0, 340125.0 ], [ 2027125.0, 340125.0 ], [ 2026875.0, 340125.0 ], [ 2026875.0, 340375.0 ], [ 2027125.0, 340375.0 ], [ 2027375.0, 340375.0 ] ] ], [ [ [ 2033625.0, 340375.0 ], [ 2033625.0, 340125.0 ], [ 2033375.0, 340125.0 ], [ 2033125.0, 340125.0 ], [ 2033125.0, 340375.0 ], [ 2033375.0, 340375.0 ], [ 2033625.0, 340375.0 ] ] ], [ [ [ 2026231.005073083331808, 340768.994926916609984 ], [ 2026375.0, 340625.0 ], [ 2026375.0, 340375.0 ], [ 2026125.0, 340375.0 ], [ 2026125.0, 340749.467676611733623 ], [ 2026231.005073083331808, 340768.994926916609984 ] ] ], [ [ [ 2032375.0, 340625.0 ], [ 2032125.0, 340625.0 ], [ 2032125.0, 340875.0 ], [ 2032375.0, 340875.0 ], [ 2032625.0, 340875.0 ], [ 2032625.0, 340625.0 ], [ 2032375.0, 340625.0 ] ] ], [ [ [ 2034375.0, 340875.0 ], [ 2034375.0, 340625.0 ], [ 2034125.0, 340625.0 ], [ 2034125.0, 340875.0 ], [ 2034125.0, 341125.0 ], [ 2034375.0, 341125.0 ], [ 2034375.0, 340875.0 ] ] ], [ [ [ 2027212.347141260746866, 341287.652858739311341 ], [ 2027375.0, 341125.0 ], [ 2027375.0, 340875.0 ], [ 2027375.0, 340625.0 ], [ 2027125.0, 340625.0 ], [ 2027125.0, 340875.0 ], [ 2026875.0, 340875.0 ], [ 2026751.611554779810831, 340998.388445220189169 ], [ 2027125.0, 341232.81355322280433 ], [ 2027212.347141260746866, 341287.652858739311341 ] ] ], [ [ [ 2032875.0, 340875.0 ], [ 2032875.0, 341125.0 ], [ 2032875.0, 341375.0 ], [ 2032875.0, 341625.0 ], [ 2033125.0, 341625.0 ], [ 2033125.0, 341375.0 ], [ 2033125.0, 341125.0 ], [ 2033125.0, 340875.0 ], [ 2032875.0, 340875.0 ] ] ], [ [ [ 2030125.0, 342125.0 ], [ 2029875.0, 342125.0 ], [ 2029875.0, 342375.0 ], [ 2029875.0, 342625.0 ], [ 2030125.0, 342625.0 ], [ 2030125.0, 342375.0 ], [ 2030125.0, 342125.0 ] ] ], [ [ [ 2034625.0, 342625.0 ], [ 2034875.0, 342625.0 ], [ 2034875.0, 342375.0 ], [ 2034625.0, 342375.0 ], [ 2034625.0, 342625.0 ] ] ], [ [ [ 2035375.0, 342875.0 ], [ 2035625.0, 342875.0 ], [ 2035625.0, 342625.0 ], [ 2035375.0, 342625.0 ], [ 2035125.0, 342625.0 ], [ 2035125.0, 342875.0 ], [ 2035375.0, 342875.0 ] ] ], [ [ [ 2033875.0, 342875.0 ], [ 2033875.0, 343125.0 ], [ 2034125.0, 343125.0 ], [ 2034125.0, 342875.0 ], [ 2033875.0, 342875.0 ] ] ], [ [ [ 2030006.838232669979334, 343092.881754293979611 ], [ 2030033.358234973857179, 343216.641765026201028 ], [ 2030125.0, 343125.0 ], [ 2030125.0, 342875.0 ], [ 2029875.0, 342875.0 ], [ 2029801.630637013586238, 342948.36936298647197 ], [ 2030006.838232669979334, 343092.881754293979611 ] ] ], [ [ [ 2034375.0, 343125.0 ], [ 2034125.0, 343125.0 ], [ 2034125.0, 343375.0 ], [ 2034375.0, 343375.0 ], [ 2034375.0, 343125.0 ] ] ], [ [ [ 2033375.0, 343125.0 ], [ 2033125.0, 343125.0 ], [ 2033125.0, 343375.0 ], [ 2033125.0, 343625.0 ], [ 2033375.0, 343625.0 ], [ 2033625.0, 343625.0 ], [ 2033625.0, 343375.0 ], [ 2033625.0, 343125.0 ], [ 2033375.0, 343125.0 ] ] ], [ [ [ 2031875.0, 343875.0 ], [ 2032125.0, 343875.0 ], [ 2032125.0, 343625.0 ], [ 2031875.0, 343625.0 ], [ 2031875.0, 343875.0 ] ] ], [ [ [ 2035375.0, 343625.0 ], [ 2035375.0, 343875.0 ], [ 2035625.0, 343875.0 ], [ 2035625.0, 343625.0 ], [ 2035375.0, 343625.0 ] ] ], [ [ [ 2029875.0, 344567.421952598670032 ], [ 2029922.613821610109881, 344552.771545948984567 ], [ 2030109.779179519973695, 344618.279421215003822 ], [ 2030125.0, 344628.064234380959533 ], [ 2030125.0, 344375.0 ], [ 2029875.0, 344375.0 ], [ 2029625.0, 344375.0 ], [ 2029625.0, 344644.489058320061304 ], [ 2029679.29885634011589, 344627.637689110997599 ], [ 2029875.0, 344567.421952598670032 ] ] ], [ [ [ 2031733.288588577648625, 344266.711411422467791 ], [ 2032021.718340643448755, 344478.28165935643483 ], [ 2032310.148092709481716, 344689.851907290460076 ], [ 2032625.0, 344920.803449601109605 ], [ 2032625.0, 344625.0 ], [ 2032375.0, 344625.0 ], [ 2032375.0, 344375.0 ], [ 2032625.0, 344375.0 ], [ 2032625.0, 344125.0 ], [ 2032375.0, 344125.0 ], [ 2032125.0, 344125.0 ], [ 2031875.0, 344125.0 ], [ 2031733.288588577648625, 344266.711411422467791 ] ] ], [ [ [ 2035625.0, 346625.0 ], [ 2035875.0, 346625.0 ], [ 2035875.0, 346375.0 ], [ 2035625.0, 346375.0 ], [ 2035625.0, 346625.0 ] ] ], [ [ [ 2035875.0, 347125.0 ], [ 2035875.0, 347375.0 ], [ 2036125.0, 347375.0 ], [ 2036125.0, 347125.0 ], [ 2036125.0, 346875.0 ], [ 2036125.0, 346625.0 ], [ 2035875.0, 346625.0 ], [ 2035875.0, 346875.0 ], [ 2035875.0, 347125.0 ] ] ], [ [ [ 2035625.0, 347625.0 ], [ 2035625.0, 347375.0 ], [ 2035625.0, 347125.0 ], [ 2035625.0, 346875.0 ], [ 2035625.0, 346625.0 ], [ 2035375.0, 346625.0 ], [ 2035375.0, 346875.0 ], [ 2035375.0, 347125.0 ], [ 2035375.0, 347375.0 ], [ 2035375.0, 347625.0 ], [ 2035625.0, 347625.0 ] ] ], [ [ [ 2034984.327424573944882, 347765.672575426171534 ], [ 2035057.212374291615561, 347875.0 ], [ 2035125.810771360062063, 347977.897595599992201 ], [ 2035264.198325752746314, 348235.801674247137271 ], [ 2035375.0, 348125.0 ], [ 2035375.0, 347875.0 ], [ 2035375.0, 347625.0 ], [ 2035125.0, 347625.0 ], [ 2034984.327424573944882, 347765.672575426171534 ] ] ], [ [ [ 2036375.0, 349125.0 ], [ 2036625.0, 349125.0 ], [ 2036625.0, 348875.0 ], [ 2036375.0, 348875.0 ], [ 2036375.0, 349125.0 ] ] ], [ [ [ 2035719.413591908058152, 349280.586408092058264 ], [ 2035723.044883902417496, 349375.0 ], [ 2036125.0, 349375.0 ], [ 2036125.0, 349125.0 ], [ 2035875.0, 349125.0 ], [ 2035719.413591908058152, 349280.586408092058264 ] ] ], [ [ [ 2035454.028788686497137, 350375.0 ], [ 2035490.783219269942492, 350551.421266785997432 ], [ 2035536.331958879483864, 350625.0 ], [ 2035612.440701910061762, 350747.944892585976049 ], [ 2035780.889524020021781, 350850.88583943300182 ], [ 2035949.338346139993519, 350860.244107328995597 ], [ 2035998.097667711554095, 350748.09766771143768 ], [ 2036042.921025089919567, 350645.003945738018956 ], [ 2036125.0, 350576.604799978144001 ], [ 2036125.0, 350375.0 ], [ 2035875.0, 350375.0 ], [ 2035875.0, 350093.678138779476285 ], [ 2035855.755667180055752, 350120.940943606023211 ], [ 2035621.798969799885526, 350242.598426243988797 ], [ 2035443.991879790090024, 350326.822837301006075 ], [ 2035454.028788686497137, 350375.0 ] ] ], [ [ [ 2044811.618042910005897, 329138.691991147992667 ], [ 2044758.410026984056458, 329258.410026984114666 ], [ 2044875.0, 329375.0 ], [ 2045125.0, 329375.0 ], [ 2045375.0, 329375.0 ], [ 2045625.0, 329375.0 ], [ 2045625.0, 329125.0 ], [ 2045625.0, 328875.0 ], [ 2045375.0, 328875.0 ], [ 2045245.162522831233218, 328745.162522831233218 ], [ 2045021.478312307735905, 329021.478312307677697 ], [ 2045017.49993660999462, 329026.392776404973119 ], [ 2044811.618042910005897, 329138.691991147992667 ] ] ], [ [ [ 2043875.0, 329874.822982012876309 ], [ 2043875.0, 330125.0 ], [ 2044125.0, 330125.0 ], [ 2044125.0, 329891.489648679620586 ], [ 2043875.0, 329874.822982012876309 ] ] ], [ [ [ 2038875.0, 330875.0 ], [ 2038875.0, 330625.0 ], [ 2038875.0, 330375.0 ], [ 2039125.0, 330375.0 ], [ 2039375.0, 330375.0 ], [ 2039548.440754137234762, 330201.559245862823445 ], [ 2039140.507698409957811, 330121.31012014602311 ], [ 2039125.0, 330126.949283204041421 ], [ 2038831.684857859974727, 330233.609334888984449 ], [ 2038791.728163316845894, 330291.728163316787686 ], [ 2038625.802964169997722, 330533.073907535988837 ], [ 2038608.566821833373979, 330625.0 ], [ 2038569.653356800088659, 330832.538480182993226 ], [ 2038595.783522840589285, 330875.0 ], [ 2038875.0, 330875.0 ] ] ], [ [ [ 2041375.0, 331375.0 ], [ 2041480.59065587236546, 331269.409344127692748 ], [ 2041218.043171149911359, 331057.136909668974113 ], [ 2041204.391320103546605, 331045.608679896511603 ], [ 2041125.0, 331125.0 ], [ 2041125.0, 331375.0 ], [ 2041375.0, 331375.0 ] ] ], [ [ [ 2042625.0, 330625.0 ], [ 2042375.0, 330625.0 ], [ 2042375.0, 330875.0 ], [ 2042375.0, 331125.0 ], [ 2042375.0, 331375.0 ], [ 2042375.0, 331625.0 ], [ 2042375.0, 331875.0 ], [ 2042375.0, 332125.0 ], [ 2042625.0, 332125.0 ], [ 2042625.0, 332375.0 ], [ 2042875.0, 332375.0 ], [ 2042875.0, 332125.0 ], [ 2042875.0, 331875.0 ], [ 2042875.0, 331625.0 ], [ 2042625.0, 331625.0 ], [ 2042625.0, 331375.0 ], [ 2042625.0, 331125.0 ], [ 2042625.0, 330875.0 ], [ 2042625.0, 330625.0 ] ] ], [ [ [ 2041125.0, 332375.0 ], [ 2041375.0, 332375.0 ], [ 2041375.0, 332625.0 ], [ 2041625.0, 332625.0 ], [ 2041875.0, 332625.0 ], [ 2042125.0, 332625.0 ], [ 2042125.0, 332375.0 ], [ 2042125.0, 332125.0 ], [ 2041875.0, 332125.0 ], [ 2041875.0, 331875.0 ], [ 2041625.0, 331875.0 ], [ 2041375.0, 331875.0 ], [ 2041375.0, 331625.0 ], [ 2041125.0, 331625.0 ], [ 2040875.0, 331625.0 ], [ 2040875.0, 331375.0 ], [ 2040625.0, 331375.0 ], [ 2040625.0, 331125.0 ], [ 2040375.0, 331125.0 ], [ 2040125.0, 331125.0 ], [ 2040125.0, 330875.0 ], [ 2039875.0, 330875.0 ], [ 2039875.0, 330625.0 ], [ 2039625.0, 330625.0 ], [ 2039625.0, 330875.0 ], [ 2039625.0, 331125.0 ], [ 2039625.0, 331375.0 ], [ 2039479.618512869346887, 331520.381487130653113 ], [ 2039627.13762895995751, 331571.841643906023819 ], [ 2039802.535532959736884, 331697.464467040321324 ], [ 2040125.0, 331928.418747487070505 ], [ 2040239.543406976154074, 332010.456593023729511 ], [ 2040319.649453209945932, 332067.829842353006825 ], [ 2040538.916008312953636, 332211.083991687046364 ], [ 2040875.0, 332430.65886625595158 ], [ 2040992.545040570897982, 332507.454959429043811 ], [ 2041125.0, 332375.0 ] ] ], [ [ [ 2042375.0, 332375.0 ], [ 2042375.0, 332625.0 ], [ 2042625.0, 332625.0 ], [ 2042625.0, 332375.0 ], [ 2042375.0, 332375.0 ] ] ], [ [ [ 2043847.716449700063094, 333087.881042931985576 ], [ 2043800.409551271703094, 332875.0 ], [ 2043791.566842329921201, 332835.20780976099195 ], [ 2043854.061056044185534, 332625.0 ], [ 2043894.507789179915562, 332488.951897637976799 ], [ 2043921.177382243564352, 332375.0 ], [ 2043979.688020539935678, 332125.0 ], [ 2043625.0, 332125.0 ], [ 2043625.0, 332375.0 ], [ 2043625.0, 332625.0 ], [ 2043625.0, 332875.0 ], [ 2043625.0, 333125.0 ], [ 2043907.106781009119004, 333125.0 ], [ 2043847.716449700063094, 333087.881042931985576 ] ] ], [ [ [ 2042125.0, 332625.0 ], [ 2042125.0, 332875.0 ], [ 2042375.0, 332875.0 ], [ 2042375.0, 332625.0 ], [ 2042125.0, 332625.0 ] ] ], [ [ [ 2041161.893563769990578, 333144.030650303000584 ], [ 2041208.471752394689247, 333291.528247605252545 ], [ 2041375.0, 333125.0 ], [ 2041375.0, 332875.0 ], [ 2041156.04507137532346, 332875.0 ], [ 2041161.893563769990578, 333144.030650303000584 ] ] ], [ [ [ 2045625.0, 333549.322385145525914 ], [ 2045625.0, 333875.0 ], [ 2045875.0, 333875.0 ], [ 2046125.0, 333875.0 ], [ 2046228.662012284621596, 333771.337987715494819 ], [ 2046168.566887720022351, 333714.884991911996622 ], [ 2045875.0, 333547.132484641566407 ], [ 2045841.027511389926076, 333527.71963400702225 ], [ 2045625.0, 333549.322385145525914 ] ] ], [ [ [ 2045125.0, 333875.0 ], [ 2045125.0, 334125.0 ], [ 2045375.0, 334125.0 ], [ 2045625.0, 334125.0 ], [ 2045625.0, 333875.0 ], [ 2045375.0, 333875.0 ], [ 2045125.0, 333875.0 ] ] ], [ [ [ 2042375.0, 334125.0 ], [ 2042125.0, 334125.0 ], [ 2041974.989066767739132, 334275.01093323220266 ], [ 2042235.703352481825277, 334514.29664751823293 ], [ 2042375.0, 334375.0 ], [ 2042375.0, 334125.0 ] ] ], [ [ [ 2046625.0, 334875.0 ], [ 2046625.0, 335125.0 ], [ 2046875.0, 335125.0 ], [ 2046875.0, 334875.0 ], [ 2046875.0, 334625.0 ], [ 2046625.0, 334625.0 ], [ 2046625.0, 334875.0 ] ] ], [ [ [ 2046125.0, 335125.0 ], [ 2046375.0, 335125.0 ], [ 2046375.0, 334875.0 ], [ 2046125.0, 334875.0 ], [ 2046125.0, 335125.0 ] ] ], [ [ [ 2042875.0, 335375.0 ], [ 2042875.0, 335625.0 ], [ 2042722.73240717779845, 335777.267592822259758 ], [ 2042724.724302279995754, 335783.0621967560146 ], [ 2042750.086454898351803, 335875.0 ], [ 2043125.0, 335875.0 ], [ 2043125.0, 335625.0 ], [ 2043125.0, 335375.0 ], [ 2042875.0, 335375.0 ] ] ], [ [ [ 2045897.17711876006797, 336475.574021001986694 ], [ 2045892.986534549389035, 336375.0 ], [ 2045625.0, 336375.0 ], [ 2045625.0, 336625.0 ], [ 2045375.0, 336625.0 ], [ 2045375.0, 336942.511958842398599 ], [ 2045644.503885590005666, 336803.113397334993351 ], [ 2045733.138612741837278, 336733.138612741895486 ], [ 2045822.310975600033998, 336662.739378907019272 ], [ 2045897.17711876006797, 336475.574021001986694 ] ] ], [ [ [ 2042930.689328896580264, 339680.689328896638472 ], [ 2043070.980214399984106, 339526.369354845024645 ], [ 2043202.605740349041298, 339375.0 ], [ 2043258.145572300069034, 339311.129193254979327 ], [ 2043289.167104511754587, 339125.0 ], [ 2043301.428946725092828, 339051.428946725151036 ], [ 2043125.0, 338875.0 ], [ 2043125.0, 338625.0 ], [ 2043125.0, 338375.0 ], [ 2042875.0, 338375.0 ], [ 2042875.0, 338125.0 ], [ 2042875.0, 337875.0 ], [ 2042875.0, 337625.0 ], [ 2042754.983489477075636, 337504.983489477017429 ], [ 2042715.366034379927441, 337504.983489477017429 ], [ 2042603.066819640109316, 337476.908685791015159 ], [ 2042397.184925939887762, 337411.400810524006374 ], [ 2042294.243979099905118, 337411.400810524006374 ], [ 2042219.377835940103978, 337476.908685791015159 ], [ 2042210.019568040035665, 337542.416561056976207 ], [ 2042378.468390149995685, 337664.074043695 ], [ 2042443.97626541997306, 337767.014990543015301 ], [ 2042541.676988262683153, 337875.0 ], [ 2042621.783355430001393, 337963.538616342004389 ], [ 2042659.216427010018378, 338057.121295295015443 ], [ 2042659.216427010018378, 338125.0 ], [ 2042659.216427010018378, 338150.70397424697876 ], [ 2042549.454542066901922, 338375.0 ], [ 2042492.133763959165663, 338492.13376395922387 ], [ 2042443.97626541997306, 338590.542565322015435 ], [ 2042350.393586470047012, 338749.633119540987536 ], [ 2042329.144962663995102, 338875.0 ], [ 2042286.772081308998168, 339125.0 ], [ 2042263.326852134196088, 339263.32685213413788 ], [ 2042256.810907519888133, 339301.770925358985551 ], [ 2042228.736103829927742, 339414.070140102005098 ], [ 2042081.724383295048028, 339625.0 ], [ 2042013.495942240115255, 339722.892980644013733 ], [ 2041987.657693951623514, 339737.657693951681722 ], [ 2041875.0, 339802.03351906692842 ], [ 2041816.972316439962015, 339835.192195386975072 ], [ 2041539.783753642113879, 340039.783753642172087 ], [ 2041625.0, 340125.0 ], [ 2041875.0, 340125.0 ], [ 2042125.0, 340125.0 ], [ 2042125.0, 339875.0 ], [ 2042375.0, 339875.0 ], [ 2042375.0, 340125.0 ], [ 2042689.136431750608608, 340125.0 ], [ 2042737.204595950897783, 339987.204595950955991 ], [ 2042790.232177539961413, 339835.192195386975072 ], [ 2042930.689328896580264, 339680.689328896638472 ] ] ], [ [ [ 2037125.0, 339125.0 ], [ 2037265.488501320593059, 338984.511498679406941 ], [ 2036965.113031983841211, 338784.886968016100582 ], [ 2036875.0, 338875.0 ], [ 2036875.0, 339125.0 ], [ 2036875.0, 339375.0 ], [ 2037125.0, 339375.0 ], [ 2037125.0, 339125.0 ] ] ], [ [ [ 2038125.0, 339875.0 ], [ 2038375.0, 339875.0 ], [ 2038466.990378667600453, 339783.009621332399547 ], [ 2038125.0, 339555.728515509690624 ], [ 2038125.0, 339875.0 ] ] ], [ [ [ 2037125.0, 339875.0 ], [ 2037375.0, 339875.0 ], [ 2037625.0, 339875.0 ], [ 2037625.0, 339625.0 ], [ 2037375.0, 339625.0 ], [ 2037125.0, 339625.0 ], [ 2037125.0, 339875.0 ] ] ], [ [ [ 2040125.0, 341375.0 ], [ 2040375.0, 341375.0 ], [ 2040375.0, 341125.0 ], [ 2040125.0, 341125.0 ], [ 2039875.0, 341125.0 ], [ 2039875.0, 341375.0 ], [ 2040125.0, 341375.0 ] ] ], [ [ [ 2041875.0, 341875.0 ], [ 2042125.0, 341875.0 ], [ 2042375.0, 341875.0 ], [ 2042625.0, 341875.0 ], [ 2042625.0, 342125.0 ], [ 2042875.0, 342125.0 ], [ 2042934.929263087222353, 342065.070736912835855 ], [ 2042743.440838069887832, 341903.369400230993051 ], [ 2042676.474764458369464, 341823.525235541630536 ], [ 2042500.125872790114954, 341613.263095478992909 ], [ 2042485.427302830619738, 341514.57269716943847 ], [ 2042464.639879849739373, 341375.0 ], [ 2042434.617997529916465, 341173.424504404014442 ], [ 2042443.538300971966237, 341125.0 ], [ 2042489.590932546183467, 340875.0 ], [ 2042500.125872790114954, 340817.810324385005515 ], [ 2042514.009862861363217, 340764.00986286130501 ], [ 2042375.0, 340625.0 ], [ 2042125.0, 340625.0 ], [ 2042125.0, 340875.0 ], [ 2042125.0, 341125.0 ], [ 2041875.0, 341125.0 ], [ 2041625.0, 341125.0 ], [ 2041375.0, 341125.0 ], [ 2041375.0, 341375.0 ], [ 2041375.0, 341625.0 ], [ 2041125.0, 341625.0 ], [ 2041125.0, 341875.0 ], [ 2041125.0, 342125.0 ], [ 2041125.0, 342375.0 ], [ 2041375.0, 342375.0 ], [ 2041375.0, 342125.0 ], [ 2041375.0, 341875.0 ], [ 2041625.0, 341875.0 ], [ 2041625.0, 342125.0 ], [ 2041875.0, 342125.0 ], [ 2041875.0, 341875.0 ] ] ], [ [ [ 2039875.0, 341875.0 ], [ 2039625.0, 341875.0 ], [ 2039625.0, 341625.0 ], [ 2039625.0, 341375.0 ], [ 2039375.0, 341375.0 ], [ 2039375.0, 341625.0 ], [ 2039125.0, 341625.0 ], [ 2038875.0, 341625.0 ], [ 2038625.0, 341625.0 ], [ 2038375.0, 341625.0 ], [ 2038125.0, 341625.0 ], [ 2037875.0, 341625.0 ], [ 2037875.0, 341875.0 ], [ 2038125.0, 341875.0 ], [ 2038375.0, 341875.0 ], [ 2038625.0, 341875.0 ], [ 2038875.0, 341875.0 ], [ 2039125.0, 341875.0 ], [ 2039375.0, 341875.0 ], [ 2039375.0, 342125.0 ], [ 2039375.0, 342375.0 ], [ 2039625.0, 342375.0 ], [ 2039875.0, 342375.0 ], [ 2040125.0, 342375.0 ], [ 2040125.0, 342125.0 ], [ 2039875.0, 342125.0 ], [ 2039875.0, 341875.0 ] ] ], [ [ [ 2041625.0, 342375.0 ], [ 2041375.0, 342375.0 ], [ 2041375.0, 342625.0 ], [ 2041625.0, 342625.0 ], [ 2041625.0, 342375.0 ] ] ], [ [ [ 2043125.0, 343625.0 ], [ 2043125.0, 343375.0 ], [ 2043125.0, 343125.0 ], [ 2043125.0, 342875.0 ], [ 2043125.0, 342625.0 ], [ 2043375.0, 342625.0 ], [ 2043477.097937785089016, 342522.902062214910984 ], [ 2043206.013600436039269, 342293.986399563902523 ], [ 2043125.0, 342375.0 ], [ 2042875.0, 342375.0 ], [ 2042625.0, 342375.0 ], [ 2042375.0, 342375.0 ], [ 2042375.0, 342125.0 ], [ 2042125.0, 342125.0 ], [ 2042125.0, 342375.0 ], [ 2042125.0, 342625.0 ], [ 2042125.0, 342875.0 ], [ 2042375.0, 342875.0 ], [ 2042375.0, 343125.0 ], [ 2042625.0, 343125.0 ], [ 2042625.0, 343375.0 ], [ 2042875.0, 343375.0 ], [ 2042875.0, 343625.0 ], [ 2042875.0, 343875.0 ], [ 2043125.0, 343875.0 ], [ 2043375.0, 343875.0 ], [ 2043375.0, 343625.0 ], [ 2043125.0, 343625.0 ] ] ], [ [ [ 2041875.0, 343875.0 ], [ 2041875.0, 343625.0 ], [ 2041875.0, 343375.0 ], [ 2041875.0, 343125.0 ], [ 2041875.0, 342875.0 ], [ 2041875.0, 342625.0 ], [ 2041625.0, 342625.0 ], [ 2041625.0, 342875.0 ], [ 2041625.0, 343125.0 ], [ 2041625.0, 343375.0 ], [ 2041625.0, 343625.0 ], [ 2041625.0, 343875.0 ], [ 2041875.0, 343875.0 ] ] ], [ [ [ 2040375.0, 343625.0 ], [ 2040125.0, 343625.0 ], [ 2040125.0, 343875.0 ], [ 2039875.0, 343875.0 ], [ 2039875.0, 344125.0 ], [ 2040125.0, 344125.0 ], [ 2040375.0, 344125.0 ], [ 2040375.0, 343875.0 ], [ 2040625.0, 343875.0 ], [ 2040875.0, 343875.0 ], [ 2040875.0, 343625.0 ], [ 2040875.0, 343375.0 ], [ 2040625.0, 343375.0 ], [ 2040625.0, 343625.0 ], [ 2040375.0, 343625.0 ] ] ], [ [ [ 2044125.0, 344375.0 ], [ 2044375.0, 344375.0 ], [ 2044375.0, 344625.0 ], [ 2044125.0, 344625.0 ], [ 2043875.0, 344625.0 ], [ 2043625.0, 344625.0 ], [ 2043625.0, 344875.0 ], [ 2043625.0, 345125.0 ], [ 2043875.0, 345125.0 ], [ 2043875.0, 344875.0 ], [ 2044125.0, 344875.0 ], [ 2044375.0, 344875.0 ], [ 2044375.0, 345125.0 ], [ 2044625.0, 345125.0 ], [ 2044625.0, 344875.0 ], [ 2044625.0, 344625.0 ], [ 2044875.0, 344625.0 ], [ 2045125.0, 344625.0 ], [ 2045125.0, 344375.0 ], [ 2045375.0, 344375.0 ], [ 2045375.0, 344625.0 ], [ 2045625.0, 344625.0 ], [ 2045625.0, 344375.0 ], [ 2045625.0, 344125.0 ], [ 2045708.068354367977008, 344041.931645632139407 ], [ 2045375.0, 343832.113254995492753 ], [ 2045247.933219232363626, 343752.066780767578166 ], [ 2044875.0, 343517.13528142782161 ], [ 2044787.798084096983075, 343462.201915903075133 ], [ 2044481.041327339829877, 343268.958672660053708 ], [ 2044125.0, 343044.668321076314896 ], [ 2044020.906192204449326, 342979.093807795550674 ], [ 2044006.807003919966519, 342970.211940285982564 ], [ 2043748.182275134138763, 342751.817724865977652 ], [ 2043625.0, 342875.0 ], [ 2043625.0, 343125.0 ], [ 2043375.0, 343125.0 ], [ 2043375.0, 343375.0 ], [ 2043625.0, 343375.0 ], [ 2043625.0, 343625.0 ], [ 2043625.0, 343875.0 ], [ 2043625.0, 344125.0 ], [ 2043375.0, 344125.0 ], [ 2043125.0, 344125.0 ], [ 2043125.0, 344375.0 ], [ 2043375.0, 344375.0 ], [ 2043625.0, 344375.0 ], [ 2043875.0, 344375.0 ], [ 2044125.0, 344375.0 ] ] ], [ [ [ 2038875.0, 344625.0 ], [ 2038875.0, 344375.0 ], [ 2038875.0, 344125.0 ], [ 2039125.0, 344125.0 ], [ 2039125.0, 343875.0 ], [ 2038875.0, 343875.0 ], [ 2038875.0, 343625.0 ], [ 2038625.0, 343625.0 ], [ 2038375.0, 343625.0 ], [ 2038125.0, 343625.0 ], [ 2038125.0, 343875.0 ], [ 2038125.0, 344125.0 ], [ 2038375.0, 344125.0 ], [ 2038375.0, 344375.0 ], [ 2038125.0, 344375.0 ], [ 2038125.0, 344625.0 ], [ 2038375.0, 344625.0 ], [ 2038375.0, 344875.0 ], [ 2038375.0, 345125.0 ], [ 2038625.0, 345125.0 ], [ 2038875.0, 345125.0 ], [ 2038875.0, 344875.0 ], [ 2038875.0, 344625.0 ] ] ], [ [ [ 2041875.0, 344625.0 ], [ 2041875.0, 344375.0 ], [ 2042125.0, 344375.0 ], [ 2042375.0, 344375.0 ], [ 2042625.0, 344375.0 ], [ 2042625.0, 344125.0 ], [ 2042375.0, 344125.0 ], [ 2042125.0, 344125.0 ], [ 2041875.0, 344125.0 ], [ 2041625.0, 344125.0 ], [ 2041375.0, 344125.0 ], [ 2041375.0, 343875.0 ], [ 2041125.0, 343875.0 ], [ 2041125.0, 344125.0 ], [ 2041125.0, 344375.0 ], [ 2041125.0, 344625.0 ], [ 2041125.0, 344875.0 ], [ 2041125.0, 345125.0 ], [ 2041375.0, 345125.0 ], [ 2041625.0, 345125.0 ], [ 2041875.0, 345125.0 ], [ 2042125.0, 345125.0 ], [ 2042125.0, 344875.0 ], [ 2042125.0, 344625.0 ], [ 2041875.0, 344625.0 ] ] ], [ [ [ 2042875.0, 344625.0 ], [ 2042625.0, 344625.0 ], [ 2042625.0, 344875.0 ], [ 2042875.0, 344875.0 ], [ 2042875.0, 344625.0 ] ] ], [ [ [ 2043375.0, 344625.0 ], [ 2043125.0, 344625.0 ], [ 2043125.0, 344875.0 ], [ 2043375.0, 344875.0 ], [ 2043375.0, 344625.0 ] ] ], [ [ [ 2039875.0, 345125.0 ], [ 2040125.0, 345125.0 ], [ 2040375.0, 345125.0 ], [ 2040375.0, 344875.0 ], [ 2040125.0, 344875.0 ], [ 2039875.0, 344875.0 ], [ 2039625.0, 344875.0 ], [ 2039625.0, 345125.0 ], [ 2039625.0, 345375.0 ], [ 2039875.0, 345375.0 ], [ 2039875.0, 345125.0 ] ] ], [ [ [ 2037875.0, 345375.0 ], [ 2037875.0, 345625.0 ], [ 2038125.0, 345625.0 ], [ 2038125.0, 345375.0 ], [ 2037875.0, 345375.0 ] ] ], [ [ [ 2044375.0, 345375.0 ], [ 2044125.0, 345375.0 ], [ 2044125.0, 345625.0 ], [ 2044375.0, 345625.0 ], [ 2044375.0, 345375.0 ] ] ], [ [ [ 2042625.0, 345625.0 ], [ 2042625.0, 345375.0 ], [ 2042375.0, 345375.0 ], [ 2042375.0, 345625.0 ], [ 2042625.0, 345625.0 ] ] ], [ [ [ 2042375.0, 345625.0 ], [ 2042125.0, 345625.0 ], [ 2042125.0, 345875.0 ], [ 2042375.0, 345875.0 ], [ 2042375.0, 345625.0 ] ] ], [ [ [ 2043375.0, 345625.0 ], [ 2043375.0, 345875.0 ], [ 2043625.0, 345875.0 ], [ 2043625.0, 345625.0 ], [ 2043375.0, 345625.0 ] ] ], [ [ [ 2046125.0, 348875.0 ], [ 2045875.0, 348875.0 ], [ 2045875.0, 349125.0 ], [ 2046125.0, 349125.0 ], [ 2046125.0, 348875.0 ] ] ], [ [ [ 2038875.0, 349625.0 ], [ 2038875.0, 349875.0 ], [ 2039125.0, 349875.0 ], [ 2039125.0, 349625.0 ], [ 2038875.0, 349625.0 ] ] ], [ [ [ 2036875.0, 350125.0 ], [ 2036875.0, 350492.031781814410351 ], [ 2036913.239939339924604, 350485.913391519978177 ], [ 2036980.972776822280139, 350519.027223177661654 ], [ 2037125.0, 350375.0 ], [ 2037125.0, 350125.0 ], [ 2037375.0, 350125.0 ], [ 2037375.0, 349875.0 ], [ 2037375.0, 349625.0 ], [ 2037125.0, 349625.0 ], [ 2036875.0, 349625.0 ], [ 2036625.0, 349625.0 ], [ 2036625.0, 349875.0 ], [ 2036875.0, 349875.0 ], [ 2036875.0, 350125.0 ] ] ], [ [ [ 2043625.0, 350125.0 ], [ 2043625.0, 350375.0 ], [ 2043875.0, 350375.0 ], [ 2043875.0, 350125.0 ], [ 2043625.0, 350125.0 ] ] ], [ [ [ 2038125.0, 350125.0 ], [ 2037875.0, 350125.0 ], [ 2037875.0, 350375.0 ], [ 2038125.0, 350375.0 ], [ 2038125.0, 350125.0 ] ] ], [ [ [ 2036375.0, 350375.0 ], [ 2036625.0, 350375.0 ], [ 2036625.0, 350125.0 ], [ 2036375.0, 350125.0 ], [ 2036375.0, 350375.0 ] ] ], [ [ [ 2045875.0, 350625.0 ], [ 2045875.0, 350875.0 ], [ 2046125.0, 350875.0 ], [ 2046125.0, 350625.0 ], [ 2045875.0, 350625.0 ] ] ], [ [ [ 2037463.693344051716849, 350786.306655948341358 ], [ 2037577.676959899952635, 350869.602375223999843 ], [ 2037579.476168158696964, 350875.0 ], [ 2037671.259638859890401, 351150.350412080006208 ], [ 2037875.0, 351252.220592652447522 ], [ 2038008.157283080043271, 351318.799234193982556 ], [ 2038125.0, 351292.092327469727024 ], [ 2038125.0, 350875.0 ], [ 2038125.0, 350625.0 ], [ 2037875.0, 350625.0 ], [ 2037625.0, 350625.0 ], [ 2037625.0, 350375.0 ], [ 2037375.0, 350375.0 ], [ 2037375.0, 350721.492289139947388 ], [ 2037463.693344051716849, 350786.306655948341358 ] ] ], [ [ [ 2039625.0, 350875.0 ], [ 2039375.0, 350875.0 ], [ 2039375.0, 350625.0 ], [ 2039125.0, 350625.0 ], [ 2039125.0, 350875.0 ], [ 2039125.0, 351164.751396803359967 ], [ 2039375.0, 351162.414948205288965 ], [ 2039625.0, 351160.078499607217964 ], [ 2039664.570700539974496, 351159.708679975999985 ], [ 2039875.0, 351248.169404589687474 ], [ 2039964.293289145454764, 351285.706710854603443 ], [ 2040125.0, 351125.0 ], [ 2040125.0, 350875.0 ], [ 2040125.0, 350625.0 ], [ 2039875.0, 350625.0 ], [ 2039875.0, 350875.0 ], [ 2039625.0, 350875.0 ] ] ], [ [ [ 2040492.320194975240156, 351507.679805024818052 ], [ 2040875.0, 351668.551570194249507 ], [ 2040875.0, 351375.0 ], [ 2040625.0, 351375.0 ], [ 2040492.320194975240156, 351507.679805024818052 ] ] ], [ [ [ 2042750.898281793342903, 352499.101718206715304 ], [ 2042790.232177539961413, 352516.65752478298964 ], [ 2043125.0, 352672.693374234077055 ], [ 2043262.989490407053381, 352737.010509593063034 ], [ 2043625.0, 352905.744221690867562 ], [ 2043875.0, 353022.269645419204608 ], [ 2043894.507789179915562, 353031.362259019981138 ], [ 2044125.0, 353047.258273558982182 ], [ 2044165.897558140102774, 353050.078794809989631 ], [ 2044534.408276748144999, 352784.408276748086791 ], [ 2044568.303077640011907, 352759.97249005897902 ], [ 2044625.0, 352740.272711950703524 ], [ 2044875.0, 352653.408305170363747 ], [ 2044875.0, 352375.0 ], [ 2045125.0, 352375.0 ], [ 2045125.0, 352125.0 ], [ 2044875.0, 352125.0 ], [ 2044875.0, 351875.0 ], [ 2045125.0, 351875.0 ], [ 2045375.0, 351875.0 ], [ 2045625.0, 351875.0 ], [ 2045625.0, 351625.0 ], [ 2045375.0, 351625.0 ], [ 2045125.0, 351625.0 ], [ 2045125.0, 351375.0 ], [ 2044875.0, 351375.0 ], [ 2044625.0, 351375.0 ], [ 2044375.0, 351375.0 ], [ 2044375.0, 351125.0 ], [ 2044375.0, 350875.0 ], [ 2044125.0, 350875.0 ], [ 2044125.0, 350625.0 ], [ 2043875.0, 350625.0 ], [ 2043625.0, 350625.0 ], [ 2043375.0, 350625.0 ], [ 2043125.0, 350625.0 ], [ 2042875.0, 350625.0 ], [ 2042625.0, 350625.0 ], [ 2042375.0, 350625.0 ], [ 2042125.0, 350625.0 ], [ 2042125.0, 350375.0 ], [ 2041875.0, 350375.0 ], [ 2041875.0, 350625.0 ], [ 2041875.0, 350875.0 ], [ 2041625.0, 350875.0 ], [ 2041375.0, 350875.0 ], [ 2041375.0, 351125.0 ], [ 2041125.0, 351125.0 ], [ 2041125.0, 351375.0 ], [ 2041375.0, 351375.0 ], [ 2041625.0, 351375.0 ], [ 2041625.0, 351625.0 ], [ 2041625.0, 351996.582146105298307 ], [ 2041713.788906794274226, 352036.211093205667567 ], [ 2042125.0, 352219.745987913920544 ], [ 2042232.343594293808565, 352267.656405706191435 ], [ 2042625.0, 352442.909829722542781 ], [ 2042750.898281793342903, 352499.101718206715304 ] ] ], [ [ [ 2041125.0, 351625.0 ], [ 2041020.347100805025548, 351729.65289919503266 ], [ 2041133.818760090041906, 351777.354361060017254 ], [ 2041375.0, 351885.000225200958084 ], [ 2041375.0, 351625.0 ], [ 2041125.0, 351625.0 ] ] ], [ [ [ 2045589.400186964077875, 358875.0 ], [ 2045524.329313099151477, 359024.329313099267893 ], [ 2045371.52309199119918, 359375.0 ], [ 2045262.584544504759833, 359625.0 ], [ 2045220.827367573278025, 359720.827367573219817 ], [ 2045044.707449531881139, 360125.0 ], [ 2044993.20090842875652, 360243.200908428698312 ], [ 2044826.830354559002444, 360625.0 ], [ 2044765.574449284235016, 360765.574449284176808 ], [ 2044689.960560269886628, 360939.098630482971203 ], [ 2044595.073402913520113, 361125.0 ], [ 2044520.703770205378532, 361270.70377020543674 ], [ 2044339.865069583524019, 361625.0 ], [ 2044267.255494345445186, 361767.255494345503394 ], [ 2044231.40543341008015, 361837.492348423984367 ], [ 2044147.549035033443943, 362125.0 ], [ 2044074.632368366699666, 362375.0 ], [ 2044034.881807609926909, 362511.287636880006175 ], [ 2044375.0, 362642.102326260879636 ], [ 2044625.0, 362738.256172414810862 ], [ 2044643.169220800045878, 362745.244334260991309 ], [ 2044875.0, 362766.809988140070345 ], [ 2045125.0, 362790.065802093537059 ], [ 2045202.705755531555042, 362797.294244468561374 ], [ 2045625.0, 362836.577430000528693 ], [ 2045875.0, 362859.833243953995407 ], [ 2046125.0, 362883.08905790746212 ], [ 2046125.0, 362625.0 ], [ 2045875.0, 362625.0 ], [ 2045875.0, 362375.0 ], [ 2045625.0, 362375.0 ], [ 2045375.0, 362375.0 ], [ 2045125.0, 362375.0 ], [ 2045125.0, 362125.0 ], [ 2045375.0, 362125.0 ], [ 2045375.0, 361875.0 ], [ 2045625.0, 361875.0 ], [ 2045625.0, 361625.0 ], [ 2045625.0, 361375.0 ], [ 2045625.0, 361125.0 ], [ 2045875.0, 361125.0 ], [ 2045875.0, 360875.0 ], [ 2045875.0, 360625.0 ], [ 2045875.0, 360375.0 ], [ 2046125.0, 360375.0 ], [ 2046375.0, 360375.0 ], [ 2046375.0, 360125.0 ], [ 2046375.0, 359875.0 ], [ 2046625.0, 359875.0 ], [ 2046625.0, 359625.0 ], [ 2046625.0, 359375.0 ], [ 2046875.0, 359375.0 ], [ 2046875.0, 359125.0 ], [ 2047125.0, 359125.0 ], [ 2047125.0, 358875.0 ], [ 2047375.0, 358875.0 ], [ 2047375.0, 358625.0 ], [ 2047375.0, 358375.0 ], [ 2047625.0, 358375.0 ], [ 2047625.0, 358125.0 ], [ 2047875.0, 358125.0 ], [ 2047875.0, 357875.0 ], [ 2048125.0, 357875.0 ], [ 2048125.0, 357625.0 ], [ 2048125.0, 357375.0 ], [ 2048125.0, 357125.0 ], [ 2047875.0, 357125.0 ], [ 2047625.0, 357125.0 ], [ 2047625.0, 356875.0 ], [ 2047625.0, 356625.0 ], [ 2047875.0, 356625.0 ], [ 2047875.0, 356375.0 ], [ 2047924.015269186114892, 356325.984730813885108 ], [ 2048040.994343778584152, 356290.994343778467737 ], [ 2048056.414309432962909, 356241.334207701147534 ], [ 2048059.782211526064202, 356095.699855783430394 ], [ 2047899.410151296760887, 355996.421776332135778 ], [ 2047895.280881618382409, 355895.280881618324202 ], [ 2047875.0, 355875.0 ], [ 2047625.0, 355875.0 ], [ 2047375.0, 355875.0 ], [ 2047125.0, 355875.0 ], [ 2046875.0, 355875.0 ], [ 2046875.0, 356125.0 ], [ 2046625.0, 356125.0 ], [ 2046625.0, 356375.0 ], [ 2046875.0, 356375.0 ], [ 2046875.0, 356625.0 ], [ 2046625.0, 356625.0 ], [ 2046625.0, 356875.0 ], [ 2046375.0, 356875.0 ], [ 2046375.0, 356625.0 ], [ 2046125.0, 356625.0 ], [ 2045875.0, 356625.0 ], [ 2045875.0, 356375.0 ], [ 2045625.0, 356375.0 ], [ 2045625.0, 356125.0 ], [ 2045625.0, 355875.0 ], [ 2045375.0, 355875.0 ], [ 2045375.0, 355521.513473123952281 ], [ 2045232.740098200039938, 355558.094590729975607 ], [ 2045077.972340184263885, 355875.0 ], [ 2045036.216472399886698, 355960.500110224995296 ], [ 2045224.312295054318383, 356275.687704945623409 ], [ 2045283.57930984441191, 356375.0 ], [ 2045382.47238452010788, 356540.71271972800605 ], [ 2045549.680005601840094, 356700.319994398159906 ], [ 2045805.49395908927545, 356944.50604091072455 ], [ 2046000.118065600050613, 357130.28359712701058 ], [ 2046029.557656314224005, 357220.44234368565958 ], [ 2046080.025462459772825, 357375.0 ], [ 2046149.850351929897442, 357588.83872399298707 ], [ 2046134.092924396041781, 357625.0 ], [ 2046025.154376909602433, 357875.0 ], [ 2045979.582231388427317, 357979.582231388310902 ], [ 2045807.277281936723739, 358375.0 ], [ 2045751.955772243672982, 358501.955772243789397 ], [ 2045589.400186964077875, 358875.0 ] ] ], [ [ [ 2048010.392948073800653, 327625.0 ], [ 2047625.0, 327625.0 ], [ 2047375.0, 327625.0 ], [ 2047375.0, 327875.0 ], [ 2047375.0, 328125.0 ], [ 2047375.0, 328375.0 ], [ 2047625.0, 328375.0 ], [ 2047625.0, 328125.0 ], [ 2047875.0, 328125.0 ], [ 2047875.0, 327875.0 ], [ 2048033.314358457922935, 327716.685641542135272 ], [ 2048010.392948073800653, 327625.0 ] ] ], [ [ [ 2047625.0, 333875.0 ], [ 2047875.0, 333875.0 ], [ 2048125.0, 333875.0 ], [ 2048125.0, 333491.750577958940994 ], [ 2047875.0, 333506.750577958882786 ], [ 2047625.0, 333521.750577958824579 ], [ 2047525.515732530038804, 333527.71963400702225 ], [ 2047218.655123849865049, 333718.655123849981464 ], [ 2047375.0, 333875.0 ], [ 2047625.0, 333875.0 ] ] ], [ [ [ 2049125.0, 334878.986807471257634 ], [ 2049312.944900509901345, 334959.534621976024937 ], [ 2049375.0, 334933.333579969126731 ], [ 2049767.578298415988684, 334767.578298415930476 ], [ 2049875.0, 334722.222468858293723 ], [ 2050125.0, 334616.666913302848116 ], [ 2050155.189011079957709, 334603.92044195800554 ], [ 2050504.849866310367361, 334504.849866310425568 ], [ 2050625.0, 334470.807328432041686 ], [ 2050716.685084800003096, 334444.829887738975231 ], [ 2050973.939559718128294, 334125.0 ], [ 2051062.940996919991449, 334014.34956455900101 ], [ 2050988.074853759957477, 333808.467670864018146 ], [ 2050772.407563124084845, 333727.59243687603157 ], [ 2050688.610281110042706, 333696.168456120998599 ], [ 2050375.0, 333576.06239101360552 ], [ 2050248.771690039895475, 333527.71963400702225 ], [ 2050125.0, 333526.633917427738197 ], [ 2049875.0, 333524.440934971673414 ], [ 2049625.0, 333522.247952515608631 ], [ 2049375.0, 333520.054970059543848 ], [ 2049181.929149979958311, 333518.361366112018004 ], [ 2049125.0, 333514.664668061363045 ], [ 2048875.0, 333498.430901827581692 ], [ 2048625.0, 333482.19713559380034 ], [ 2048461.342522050021216, 333471.570026636007242 ], [ 2048375.0, 333476.750577958999202 ], [ 2048375.0, 333875.0 ], [ 2048375.0, 334125.0 ], [ 2048375.0, 334375.0 ], [ 2048500.0, 334500.0 ], [ 2048500.0, 334532.85154322150629 ], [ 2048545.56693310989067, 334529.054298795992509 ], [ 2048751.448826800100505, 334716.219656699977349 ], [ 2048773.901268813991919, 334726.098731186124496 ], [ 2048985.405524180037901, 334819.16060354799265 ], [ 2049125.0, 334878.986807471257634 ] ] ], [ [ [ 2047125.0, 345125.0 ], [ 2047125.0, 345375.0 ], [ 2047375.0, 345375.0 ], [ 2047375.0, 345125.0 ], [ 2047125.0, 345125.0 ] ] ], [ [ [ 2047875.0, 345625.0 ], [ 2048125.0, 345625.0 ], [ 2048125.0, 345875.0 ], [ 2048375.0, 345875.0 ], [ 2048625.0, 345875.0 ], [ 2048625.0, 345524.746649439970497 ], [ 2048592.35827257996425, 345515.660807786975056 ], [ 2048514.520994080230594, 345485.479005919652991 ], [ 2048125.0, 345334.440253112465143 ], [ 2047875.0, 345237.50147760193795 ], [ 2047875.0, 345625.0 ] ] ], [ [ [ 2049875.0, 346125.0 ], [ 2050125.0, 346125.0 ], [ 2050267.940121002029628, 345982.05987899802858 ], [ 2049875.0, 345872.684793770487886 ], [ 2049875.0, 346125.0 ] ] ], [ [ [ 2052525.807856808882207, 346474.192143191059586 ], [ 2052125.0, 346388.429763349995483 ], [ 2051875.0, 346334.9363135684398 ], [ 2051625.0, 346281.442863786884118 ], [ 2051625.0, 346625.0 ], [ 2051875.0, 346625.0 ], [ 2052125.0, 346625.0 ], [ 2052375.0, 346625.0 ], [ 2052525.807856808882207, 346474.192143191059586 ] ] ], [ [ [ 2054125.0, 347625.0 ], [ 2053875.0, 347625.0 ], [ 2053875.0, 347875.0 ], [ 2054125.0, 347875.0 ], [ 2054125.0, 347625.0 ] ] ], [ [ [ 2055526.476974438643083, 347723.523025561473332 ], [ 2055221.476974438177422, 347528.523025561822578 ], [ 2055125.0, 347625.0 ], [ 2054875.0, 347625.0 ], [ 2054875.0, 347875.0 ], [ 2054875.0, 348125.0 ], [ 2055125.0, 348125.0 ], [ 2055375.0, 348125.0 ], [ 2055375.0, 347875.0 ], [ 2055526.476974438643083, 347723.523025561473332 ] ] ], [ [ [ 2049625.0, 347875.0 ], [ 2049625.0, 348125.0 ], [ 2049625.0, 348375.0 ], [ 2049875.0, 348375.0 ], [ 2049875.0, 348125.0 ], [ 2049875.0, 347875.0 ], [ 2049875.0, 347625.0 ], [ 2049625.0, 347625.0 ], [ 2049625.0, 347875.0 ] ] ], [ [ [ 2054375.0, 347875.0 ], [ 2054375.0, 348125.0 ], [ 2054625.0, 348125.0 ], [ 2054625.0, 347875.0 ], [ 2054375.0, 347875.0 ] ] ], [ [ [ 2056625.0, 348375.0 ], [ 2056875.0, 348375.0 ], [ 2056986.184095392236486, 348263.815904607879929 ], [ 2056818.275752479908988, 348107.90101476298878 ], [ 2056625.0, 348121.706425654585473 ], [ 2056625.0, 348375.0 ] ] ], [ [ [ 2055875.0, 348875.0 ], [ 2056125.0, 348875.0 ], [ 2056125.0, 348625.0 ], [ 2055875.0, 348625.0 ], [ 2055875.0, 348875.0 ] ] ], [ [ [ 2057125.0, 348875.0 ], [ 2057125.0, 349125.0 ], [ 2057375.0, 349125.0 ], [ 2057375.0, 348875.0 ], [ 2057125.0, 348875.0 ] ] ], [ [ [ 2059625.0, 349625.0 ], [ 2059759.905580135295168, 349490.094419864646625 ], [ 2059375.0, 349380.41702337434981 ], [ 2059125.0, 349309.180464234494139 ], [ 2059125.0, 349625.0 ], [ 2059375.0, 349625.0 ], [ 2059625.0, 349625.0 ] ] ], [ [ [ 2056625.0, 349125.0 ], [ 2056625.0, 349375.0 ], [ 2056625.0, 349625.0 ], [ 2056625.0, 349875.0 ], [ 2056875.0, 349875.0 ], [ 2056875.0, 349625.0 ], [ 2056875.0, 349375.0 ], [ 2056875.0, 349125.0 ], [ 2056625.0, 349125.0 ] ] ], [ [ [ 2056375.0, 349625.0 ], [ 2056375.0, 349375.0 ], [ 2056125.0, 349375.0 ], [ 2056125.0, 349625.0 ], [ 2056375.0, 349625.0 ] ] ], [ [ [ 2058875.0, 349875.0 ], [ 2058875.0, 349625.0 ], [ 2058625.0, 349625.0 ], [ 2058625.0, 349875.0 ], [ 2058875.0, 349875.0 ] ] ], [ [ [ 2048375.0, 350125.0 ], [ 2048375.0, 349875.0 ], [ 2048125.0, 349875.0 ], [ 2047875.0, 349875.0 ], [ 2047875.0, 350125.0 ], [ 2048125.0, 350125.0 ], [ 2048375.0, 350125.0 ] ] ], [ [ [ 2049375.0, 350875.0 ], [ 2049375.0, 350625.0 ], [ 2049625.0, 350625.0 ], [ 2049625.0, 350375.0 ], [ 2049625.0, 350125.0 ], [ 2049375.0, 350125.0 ], [ 2049375.0, 350375.0 ], [ 2049125.0, 350375.0 ], [ 2049125.0, 350625.0 ], [ 2049125.0, 350875.0 ], [ 2049375.0, 350875.0 ] ] ], [ [ [ 2047875.0, 350625.0 ], [ 2047625.0, 350625.0 ], [ 2047625.0, 350875.0 ], [ 2047875.0, 350875.0 ], [ 2047875.0, 351125.0 ], [ 2048125.0, 351125.0 ], [ 2048125.0, 350875.0 ], [ 2048125.0, 350625.0 ], [ 2047875.0, 350625.0 ] ] ], [ [ [ 2050125.0, 350875.0 ], [ 2050125.0, 351125.0 ], [ 2050375.0, 351125.0 ], [ 2050375.0, 350875.0 ], [ 2050125.0, 350875.0 ] ] ], [ [ [ 2047625.0, 350875.0 ], [ 2047375.0, 350875.0 ], [ 2047375.0, 351125.0 ], [ 2047375.0, 351375.0 ], [ 2047375.0, 351625.0 ], [ 2047625.0, 351625.0 ], [ 2047625.0, 351375.0 ], [ 2047625.0, 351125.0 ], [ 2047625.0, 350875.0 ] ] ], [ [ [ 2049125.0, 351125.0 ], [ 2049125.0, 351375.0 ], [ 2049375.0, 351375.0 ], [ 2049375.0, 351125.0 ], [ 2049125.0, 351125.0 ] ] ], [ [ [ 2048375.0, 351375.0 ], [ 2048375.0, 351625.0 ], [ 2048625.0, 351625.0 ], [ 2048625.0, 351375.0 ], [ 2048375.0, 351375.0 ] ] ], [ [ [ 2050125.0, 351875.0 ], [ 2050125.0, 351625.0 ], [ 2050125.0, 351375.0 ], [ 2049875.0, 351375.0 ], [ 2049875.0, 351625.0 ], [ 2049875.0, 351875.0 ], [ 2050125.0, 351875.0 ] ] ], [ [ [ 2047125.0, 351625.0 ], [ 2047125.0, 351875.0 ], [ 2047375.0, 351875.0 ], [ 2047375.0, 351625.0 ], [ 2047125.0, 351625.0 ] ] ], [ [ [ 2048125.0, 351625.0 ], [ 2048125.0, 351875.0 ], [ 2048375.0, 351875.0 ], [ 2048375.0, 351625.0 ], [ 2048125.0, 351625.0 ] ] ], [ [ [ 2049375.0, 352375.0 ], [ 2049375.0, 352125.0 ], [ 2049625.0, 352125.0 ], [ 2049625.0, 351875.0 ], [ 2049375.0, 351875.0 ], [ 2049375.0, 351625.0 ], [ 2049125.0, 351625.0 ], [ 2049125.0, 351875.0 ], [ 2048875.0, 351875.0 ], [ 2048875.0, 352125.0 ], [ 2048875.0, 352375.0 ], [ 2049125.0, 352375.0 ], [ 2049125.0, 352625.0 ], [ 2049125.0, 352875.0 ], [ 2049125.0, 353125.0 ], [ 2049375.0, 353125.0 ], [ 2049625.0, 353125.0 ], [ 2049625.0, 352875.0 ], [ 2049625.0, 352625.0 ], [ 2049625.0, 352375.0 ], [ 2049375.0, 352375.0 ] ] ], [ [ [ 2056625.0, 352375.0 ], [ 2056375.0, 352375.0 ], [ 2056125.0, 352375.0 ], [ 2055875.0, 352375.0 ], [ 2055875.0, 352625.0 ], [ 2056125.0, 352625.0 ], [ 2056375.0, 352625.0 ], [ 2056625.0, 352625.0 ], [ 2056625.0, 352375.0 ] ] ], [ [ [ 2057625.0, 353375.0 ], [ 2057375.0, 353375.0 ], [ 2057375.0, 353625.0 ], [ 2057625.0, 353625.0 ], [ 2057625.0, 353375.0 ] ] ], [ [ [ 2049125.0, 353625.0 ], [ 2049125.0, 353375.0 ], [ 2048875.0, 353375.0 ], [ 2048625.0, 353375.0 ], [ 2048500.0, 353500.0 ], [ 2048625.0, 353625.0 ], [ 2048875.0, 353625.0 ], [ 2048875.0, 353875.0 ], [ 2048875.0, 354125.0 ], [ 2049125.0, 354125.0 ], [ 2049125.0, 353875.0 ], [ 2049125.0, 353625.0 ] ] ], [ [ [ 2047875.0, 354625.0 ], [ 2047875.0, 354375.0 ], [ 2047727.399179421830922, 354227.39917942188913 ], [ 2047625.0, 354213.435654955392238 ], [ 2047375.0, 354179.344745864684228 ], [ 2047375.0, 354625.0 ], [ 2047125.0, 354625.0 ], [ 2047125.0, 354875.0 ], [ 2046875.0, 354875.0 ], [ 2046875.0, 355125.0 ], [ 2047125.0, 355125.0 ], [ 2047125.0, 355375.0 ], [ 2047375.0, 355375.0 ], [ 2047375.0, 355125.0 ], [ 2047625.0, 355125.0 ], [ 2047875.0, 355125.0 ], [ 2048125.0, 355125.0 ], [ 2048375.0, 355125.0 ], [ 2048375.0, 354875.0 ], [ 2048125.0, 354875.0 ], [ 2048125.0, 354625.0 ], [ 2047875.0, 354625.0 ] ] ], [ [ [ 2056375.0, 355125.0 ], [ 2056125.0, 355125.0 ], [ 2056125.0, 355375.0 ], [ 2056375.0, 355375.0 ], [ 2056375.0, 355125.0 ] ] ], [ [ [ 2046625.0, 355375.0 ], [ 2046625.0, 355625.0 ], [ 2046875.0, 355625.0 ], [ 2046875.0, 355375.0 ], [ 2046625.0, 355375.0 ] ] ], [ [ [ 2055875.0, 355744.162986650306266 ], [ 2055875.0, 355375.0 ], [ 2055625.0, 355375.0 ], [ 2055625.0, 355645.153085660480428 ], [ 2055875.0, 355744.162986650306266 ] ] ], [ [ [ 2057375.0, 355875.0 ], [ 2057375.0, 355625.0 ], [ 2057375.0, 355375.0 ], [ 2057125.0, 355375.0 ], [ 2056875.0, 355375.0 ], [ 2056875.0, 355625.0 ], [ 2056625.0, 355625.0 ], [ 2056625.0, 355375.0 ], [ 2056375.0, 355375.0 ], [ 2056375.0, 355625.0 ], [ 2056125.0, 355625.0 ], [ 2055968.720130129950121, 355781.279869870049879 ], [ 2056350.362357720034197, 355932.425306538993027 ], [ 2056375.0, 355955.766230804147199 ], [ 2056461.903827425092459, 356038.096172574907541 ], [ 2056718.660584182245657, 356281.339415817637928 ], [ 2056817.524534153053537, 356375.0 ], [ 2057125.0, 356375.0 ], [ 2057125.0, 356125.0 ], [ 2057375.0, 356125.0 ], [ 2057375.0, 355875.0 ] ] ], [ [ [ 2059375.0, 356125.0 ], [ 2059125.0, 356125.0 ], [ 2059125.0, 356375.0 ], [ 2059125.0, 356625.0 ], [ 2059375.0, 356625.0 ], [ 2059375.0, 356375.0 ], [ 2059375.0, 356125.0 ] ] ], [ [ [ 2050375.0, 356625.0 ], [ 2050375.0, 356375.0 ], [ 2050125.0, 356375.0 ], [ 2050125.0, 356625.0 ], [ 2050375.0, 356625.0 ] ] ], [ [ [ 2051895.8268395899795, 356886.968631852010731 ], [ 2052326.307162770070136, 356765.311149213986937 ], [ 2052311.916275671450421, 356625.0 ], [ 2051875.0, 356625.0 ], [ 2051875.0, 356375.0 ], [ 2051875.0, 356125.0 ], [ 2051625.0, 356125.0 ], [ 2051625.0, 356375.0 ], [ 2051554.664228707784787, 356445.335771292157006 ], [ 2051699.30321379005909, 356596.862327100010589 ], [ 2051781.620976123027503, 356718.379023877030704 ], [ 2051895.8268395899795, 356886.968631852010731 ] ] ], [ [ [ 2049875.0, 356625.0 ], [ 2049625.0, 356625.0 ], [ 2049625.0, 356875.0 ], [ 2049875.0, 356875.0 ], [ 2049875.0, 356625.0 ] ] ], [ [ [ 2051268.822890609968454, 356858.893828166008461 ], [ 2051385.769804697716609, 356625.0 ], [ 2051125.0, 356625.0 ], [ 2051125.0, 356977.336208671040367 ], [ 2051268.822890609968454, 356858.893828166008461 ] ] ], [ [ [ 2050125.0, 357375.0 ], [ 2050125.0, 357625.0 ], [ 2050125.0, 357875.0 ], [ 2050375.0, 357875.0 ], [ 2050375.0, 357625.0 ], [ 2050648.724023817339912, 357625.0 ], [ 2050679.252013219986111, 357495.256045041023754 ], [ 2050706.992838188074529, 357456.992838188132737 ], [ 2050947.687645873054862, 357125.0 ], [ 2050625.0, 357125.0 ], [ 2050375.0, 357125.0 ], [ 2050125.0, 357125.0 ], [ 2050125.0, 357375.0 ] ] ], [ [ [ 2057250.0, 357750.0 ], [ 2057375.0, 357625.0 ], [ 2057375.0, 357375.0 ], [ 2057125.0, 357375.0 ], [ 2057125.0, 357625.0 ], [ 2057250.0, 357750.0 ] ] ], [ [ [ 2056041.53951718006283, 358103.543458229978569 ], [ 2056269.222136728698388, 358230.777863271418028 ], [ 2056359.72062562010251, 358281.350548240006901 ], [ 2056625.0, 358360.369936354050878 ], [ 2056799.55921669001691, 358412.366298772976734 ], [ 2056943.242366073187441, 358125.0 ], [ 2056968.00803879997693, 358075.468654545024037 ], [ 2056999.206282081548125, 357999.206282081606332 ], [ 2056875.0, 357875.0 ], [ 2056875.0, 357625.0 ], [ 2056625.0, 357625.0 ], [ 2056511.712746385484934, 357511.712746385543142 ], [ 2056261.712746385252103, 357761.712746385193896 ], [ 2056153.838731920113787, 357869.586760849982966 ], [ 2056041.53951718006283, 358103.543458229978569 ] ] ], [ [ [ 2048125.0, 358125.0 ], [ 2048125.0, 358375.0 ], [ 2048375.0, 358375.0 ], [ 2048375.0, 358125.0 ], [ 2048375.0, 357875.0 ], [ 2048125.0, 357875.0 ], [ 2048125.0, 358125.0 ] ] ], [ [ [ 2057992.250846266048029, 357875.0 ], [ 2057625.0, 357875.0 ], [ 2057298.930689605418593, 357875.0 ], [ 2057520.495808988809586, 358229.504191011190414 ], [ 2057712.803501297254115, 358537.196498702745885 ], [ 2057767.680689608445391, 358625.0 ], [ 2057903.83482832997106, 358842.846621953009162 ], [ 2058081.641918339999393, 358880.279693534015678 ], [ 2058254.291882123332471, 358754.291882123390678 ], [ 2058427.897830459987745, 358627.606460363022052 ], [ 2058250.090740449959412, 358393.649762981978711 ], [ 2058194.191655781352893, 358305.8083442185889 ], [ 2057999.747211334994063, 358000.252788664889522 ], [ 2057988.059239380061626, 357981.885975593002513 ], [ 2057992.250846266048029, 357875.0 ] ] ], [ [ [ 2049375.0, 359875.0 ], [ 2049490.703500043135136, 359990.703500043193344 ], [ 2049750.101996283745393, 359750.101996283803601 ], [ 2050009.50049252435565, 359509.50049252435565 ], [ 2050052.248064239975065, 359469.850570933020208 ], [ 2050125.0, 359455.300183781015221 ], [ 2050286.20476161991246, 359423.059231457009446 ], [ 2050300.61904099280946, 359449.380959007306956 ], [ 2050477.542117915814742, 359772.45788208412705 ], [ 2050501.444923209957778, 359816.106483055977151 ], [ 2050534.572526490082964, 359875.0 ], [ 2050669.893745319917798, 360115.571055702981539 ], [ 2050903.850442700088024, 360096.854519912973046 ], [ 2050934.987919178325683, 359875.0 ], [ 2050970.075638475595042, 359625.0 ], [ 2050978.716585859889165, 359563.433249884983525 ], [ 2051028.77643278427422, 359528.776432784216013 ], [ 2051100.374068500008434, 359479.208838828024454 ], [ 2051228.358788205077872, 359228.358788205194287 ], [ 2051334.33076587994583, 359020.653711961989757 ], [ 2051275.166686237091199, 358724.833313762908801 ], [ 2051268.822890609968454, 358693.114335628983099 ], [ 2050903.850442700088024, 358655.681264049024321 ], [ 2050875.0, 358652.866586712363642 ], [ 2050625.0, 358628.476342809619382 ], [ 2050520.161459, 358618.248192468017805 ], [ 2050481.406756587559357, 358518.593243412440643 ], [ 2050454.653583730105311, 358449.79937035398325 ], [ 2050472.253435578430071, 358375.0 ], [ 2050531.076964991400018, 358125.0 ], [ 2050125.0, 358125.0 ], [ 2049875.0, 358125.0 ], [ 2049875.0, 357875.0 ], [ 2049625.0, 357875.0 ], [ 2049625.0, 358125.0 ], [ 2049375.0, 358125.0 ], [ 2049125.0, 358125.0 ], [ 2049125.0, 358375.0 ], [ 2048875.0, 358375.0 ], [ 2048875.0, 358625.0 ], [ 2049125.0, 358625.0 ], [ 2049375.0, 358625.0 ], [ 2049625.0, 358625.0 ], [ 2049875.0, 358625.0 ], [ 2049875.0, 358875.0 ], [ 2049875.0, 359125.0 ], [ 2049875.0, 359375.0 ], [ 2049625.0, 359375.0 ], [ 2049625.0, 359625.0 ], [ 2049375.0, 359625.0 ], [ 2049375.0, 359875.0 ] ] ], [ [ [ 2047125.0, 359625.0 ], [ 2047375.0, 359625.0 ], [ 2047375.0, 359375.0 ], [ 2047125.0, 359375.0 ], [ 2047125.0, 359625.0 ] ] ], [ [ [ 2047375.0, 360125.0 ], [ 2047625.0, 360125.0 ], [ 2047625.0, 359875.0 ], [ 2047375.0, 359875.0 ], [ 2047375.0, 360125.0 ] ] ], [ [ [ 2047375.0, 360625.0 ], [ 2047375.0, 360875.0 ], [ 2047625.0, 360875.0 ], [ 2047625.0, 360625.0 ], [ 2047375.0, 360625.0 ] ] ], [ [ [ 2047375.0, 361375.0 ], [ 2047625.0, 361375.0 ], [ 2047625.0, 361125.0 ], [ 2047375.0, 361125.0 ], [ 2047375.0, 361375.0 ] ] ], [ [ [ 2062875.0, 351375.0 ], [ 2063125.0, 351375.0 ], [ 2063125.0, 351030.452113917737734 ], [ 2063028.14763583149761, 350971.852364168618806 ], [ 2062875.0, 351125.0 ], [ 2062875.0, 351375.0 ] ] ], [ [ [ 2065034.834964490029961, 352197.464084975013975 ], [ 2064875.0, 352110.745114879333414 ], [ 2064875.0, 352375.0 ], [ 2065125.0, 352375.0 ], [ 2065220.994820896303281, 352279.005179103580303 ], [ 2065034.834964490029961, 352197.464084975013975 ] ] ], [ [ [ 2058125.0, 353625.0 ], [ 2058375.0, 353625.0 ], [ 2058375.0, 353375.0 ], [ 2058625.0, 353375.0 ], [ 2058625.0, 353125.0 ], [ 2058875.0, 353125.0 ], [ 2058875.0, 352875.0 ], [ 2058625.0, 352875.0 ], [ 2058625.0, 352625.0 ], [ 2058875.0, 352625.0 ], [ 2059125.0, 352625.0 ], [ 2059375.0, 352625.0 ], [ 2059625.0, 352625.0 ], [ 2059625.0, 352875.0 ], [ 2059625.0, 353125.0 ], [ 2059375.0, 353125.0 ], [ 2059375.0, 353375.0 ], [ 2059375.0, 353625.0 ], [ 2059625.0, 353625.0 ], [ 2059875.0, 353625.0 ], [ 2060125.0, 353625.0 ], [ 2060125.0, 353375.0 ], [ 2060125.0, 353125.0 ], [ 2060163.097459559794515, 353067.355153968965169 ], [ 2060319.55769241345115, 353000.673875383858103 ], [ 2060295.89290426322259, 352753.722286372852977 ], [ 2060594.955344538670033, 352883.180850168806501 ], [ 2060615.875600476050749, 352875.0 ], [ 2060875.0, 352875.0 ], [ 2060875.0, 352625.0 ], [ 2061125.0, 352625.0 ], [ 2061125.0, 352375.0 ], [ 2061125.0, 352125.0 ], [ 2061125.0, 351950.060636591981165 ], [ 2061172.273600102867931, 351856.060891543340404 ], [ 2061065.446651025675237, 351772.888735701038968 ], [ 2061083.122, 351764.558200000028592 ], [ 2061063.703, 351664.178100000019185 ], [ 2061061.849, 351663.458700000017416 ], [ 2061054.935050831874833, 351651.875175932247657 ], [ 2061048.719, 351641.4609 ], [ 2061044.033, 351635.098400000017136 ], [ 2061027.602, 351627.59519999998156 ], [ 2060991.953694114694372, 351624.893341886578128 ], [ 2060964.719821736216545, 351696.371154662920162 ], [ 2060857.700888885883614, 351615.397245519154239 ], [ 2060854.733, 351615.192800000018906 ], [ 2060751.325999999884516, 351607.555799999972805 ], [ 2060650.648, 351601.70150000002468 ], [ 2060635.897513984469697, 351600.890607873327099 ], [ 2060639.944290086627007, 351387.894145780184772 ], [ 2060625.0, 351374.847991493297741 ], [ 2060436.516043751034886, 351387.744993808853906 ], [ 2060375.0, 351327.830256019020453 ], [ 2060183.066451482241973, 351375.142003979417495 ], [ 2060170.242608245229349, 351571.859622314281296 ], [ 2060247.435, 351577.7048 ], [ 2060347.54, 351583.63890000001993 ], [ 2060375.690805861959234, 351585.1759505996597 ], [ 2060407.137546690180898, 351713.740684832155239 ], [ 2060386.95, 351771.946799999976065 ], [ 2060330.204857724253088, 351819.724748992884997 ], [ 2060158.335058291675523, 351754.521282917354256 ], [ 2060123.591380463680252, 351785.751374438928906 ], [ 2059919.377369914203882, 351654.814204456051812 ], [ 2059875.0, 351772.684193356079049 ], [ 2059673.995058369124308, 351652.305168879975099 ], [ 2059625.0, 351758.932045619178098 ], [ 2059427.903657301561907, 351650.729735348955728 ], [ 2059375.0, 351746.357840756012592 ], [ 2059375.0, 351875.0 ], [ 2059125.0, 351875.0 ], [ 2058875.0, 351875.0 ], [ 2058625.0, 351875.0 ], [ 2058625.0, 352125.0 ], [ 2058375.0, 352125.0 ], [ 2058375.0, 352375.0 ], [ 2058125.0, 352375.0 ], [ 2058125.0, 352625.0 ], [ 2058125.0, 352875.0 ], [ 2057875.0, 352875.0 ], [ 2057875.0, 353125.0 ], [ 2057875.0, 353375.0 ], [ 2057875.0, 353625.0 ], [ 2058125.0, 353625.0 ] ] ], [ [ [ 2062625.0, 352625.0 ], [ 2062875.0, 352625.0 ], [ 2062875.0, 352375.0 ], [ 2062625.0, 352375.0 ], [ 2062625.0, 352625.0 ] ] ], [ [ [ 2061625.0, 352875.0 ], [ 2061875.0, 352875.0 ], [ 2061875.0, 352625.0 ], [ 2062125.0, 352625.0 ], [ 2062125.0, 352375.0 ], [ 2061875.0, 352375.0 ], [ 2061625.0, 352375.0 ], [ 2061625.0, 352625.0 ], [ 2061625.0, 352875.0 ] ] ], [ [ [ 2067496.059420929988846, 353348.531036088010296 ], [ 2067375.0, 353288.853856756235473 ], [ 2067375.0, 353625.0 ], [ 2067625.0, 353625.0 ], [ 2067875.0, 353625.0 ], [ 2067988.448316165013239, 353511.551683835103177 ], [ 2067625.0, 353391.220822401810437 ], [ 2067496.059420929988846, 353348.531036088010296 ] ] ], [ [ [ 2062625.0, 353875.0 ], [ 2062625.0, 354125.0 ], [ 2062875.0, 354125.0 ], [ 2062875.0, 353875.0 ], [ 2062625.0, 353875.0 ] ] ], [ [ [ 2070375.0, 354372.352028578228783 ], [ 2070375.0, 354625.0 ], [ 2070625.0, 354625.0 ], [ 2070739.666175345424563, 354510.333824654691853 ], [ 2070375.0, 354372.352028578228783 ] ] ], [ [ [ 2061375.0, 354875.0 ], [ 2061625.0, 354875.0 ], [ 2061625.0, 354625.0 ], [ 2061375.0, 354625.0 ], [ 2061375.0, 354875.0 ] ] ], [ [ [ 2070125.0, 355125.0 ], [ 2070375.0, 355125.0 ], [ 2070375.0, 354875.0 ], [ 2070125.0, 354875.0 ], [ 2070125.0, 355125.0 ] ] ], [ [ [ 2059625.0, 355875.0 ], [ 2059625.0, 355625.0 ], [ 2059375.0, 355625.0 ], [ 2059375.0, 355875.0 ], [ 2059625.0, 355875.0 ] ] ], [ [ [ 2066625.0, 356125.0 ], [ 2066625.0, 356375.0 ], [ 2066875.0, 356375.0 ], [ 2066875.0, 356125.0 ], [ 2066625.0, 356125.0 ] ] ], [ [ [ 2068375.0, 356375.0 ], [ 2068125.0, 356375.0 ], [ 2068125.0, 356625.0 ], [ 2068375.0, 356625.0 ], [ 2068375.0, 356375.0 ] ] ], [ [ [ 2059375.0, 356625.0 ], [ 2059375.0, 356875.0 ], [ 2059625.0, 356875.0 ], [ 2059625.0, 356625.0 ], [ 2059375.0, 356625.0 ] ] ], [ [ [ 2060625.0, 357125.0 ], [ 2060625.0, 356875.0 ], [ 2060375.0, 356875.0 ], [ 2060375.0, 357125.0 ], [ 2060375.0, 357536.061457500502001 ], [ 2060451.632580644218251, 357548.367419355723541 ], [ 2060625.0, 357375.0 ], [ 2060875.0, 357375.0 ], [ 2060875.0, 357125.0 ], [ 2060625.0, 357125.0 ] ] ], [ [ [ 2066125.0, 357375.0 ], [ 2066125.0, 357125.0 ], [ 2065875.0, 357125.0 ], [ 2065875.0, 357375.0 ], [ 2065875.0, 357625.0 ], [ 2066125.0, 357625.0 ], [ 2066125.0, 357375.0 ] ] ], [ [ [ 2066125.0, 357875.0 ], [ 2066125.0, 358125.0 ], [ 2066375.0, 358125.0 ], [ 2066375.0, 357875.0 ], [ 2066375.0, 357625.0 ], [ 2066125.0, 357625.0 ], [ 2066125.0, 357875.0 ] ] ], [ [ [ 2072625.0, 358125.0 ], [ 2072375.0, 358125.0 ], [ 2072375.0, 358375.0 ], [ 2072625.0, 358375.0 ], [ 2072875.0, 358375.0 ], [ 2072875.0, 358125.0 ], [ 2072875.0, 357875.0 ], [ 2072625.0, 357875.0 ], [ 2072625.0, 358125.0 ] ] ], [ [ [ 2064625.0, 358375.0 ], [ 2064625.0, 358125.0 ], [ 2064375.0, 358125.0 ], [ 2064375.0, 358375.0 ], [ 2064265.642357896314934, 358484.357642103568651 ], [ 2064625.0, 358670.232284570811316 ], [ 2064625.0, 358375.0 ] ] ], [ [ [ 2065875.0, 358875.0 ], [ 2065875.0, 358625.0 ], [ 2065625.0, 358625.0 ], [ 2065375.0, 358625.0 ], [ 2065375.0, 358375.0 ], [ 2065125.0, 358375.0 ], [ 2065125.0, 358625.0 ], [ 2064875.0, 358625.0 ], [ 2064759.960539714666083, 358740.039460285275709 ], [ 2065125.0, 358928.852974225825164 ], [ 2065254.278721533017233, 358995.721278466982767 ], [ 2065625.0, 359187.473663880780805 ], [ 2065736.705056630074978, 359245.252141448028851 ], [ 2065748.449899390572682, 359251.550100609427318 ], [ 2066125.0, 359453.468270501471125 ], [ 2066236.657446560449898, 359513.342553439608309 ], [ 2066375.0, 359375.0 ], [ 2066375.0, 359125.0 ], [ 2066125.0, 359125.0 ], [ 2066125.0, 358875.0 ], [ 2065875.0, 358875.0 ] ] ], [ [ [ 2072375.0, 359375.0 ], [ 2072375.0, 359625.0 ], [ 2072625.0, 359625.0 ], [ 2072625.0, 359375.0 ], [ 2072375.0, 359375.0 ] ] ], [ [ [ 2066625.0, 359375.0 ], [ 2066625.0, 359625.0 ], [ 2066489.289956248365343, 359760.710043751518242 ], [ 2066494.724756140029058, 359769.315143580024596 ], [ 2066501.330059669679031, 359875.0 ], [ 2066875.0, 359875.0 ], [ 2066875.0, 359625.0 ], [ 2066875.0, 359375.0 ], [ 2066625.0, 359375.0 ] ] ], [ [ [ 2069125.0, 359875.0 ], [ 2069125.0, 360125.0 ], [ 2069375.0, 360125.0 ], [ 2069625.0, 360125.0 ], [ 2069875.0, 360125.0 ], [ 2070125.0, 360125.0 ], [ 2070375.0, 360125.0 ], [ 2070375.0, 359875.0 ], [ 2070125.0, 359875.0 ], [ 2070125.0, 359625.0 ], [ 2069875.0, 359625.0 ], [ 2069625.0, 359625.0 ], [ 2069375.0, 359625.0 ], [ 2069375.0, 359875.0 ], [ 2069125.0, 359875.0 ] ] ], [ [ [ 2067625.0, 360125.0 ], [ 2067875.0, 360125.0 ], [ 2067875.0, 359875.0 ], [ 2067625.0, 359875.0 ], [ 2067375.0, 359875.0 ], [ 2067375.0, 360125.0 ], [ 2067625.0, 360125.0 ] ] ], [ [ [ 2072375.0, 360125.0 ], [ 2072375.0, 360375.0 ], [ 2072625.0, 360375.0 ], [ 2072625.0, 360125.0 ], [ 2072375.0, 360125.0 ] ] ], [ [ [ 2072375.0, 360875.0 ], [ 2072375.0, 361125.0 ], [ 2072625.0, 361125.0 ], [ 2072625.0, 360875.0 ], [ 2072375.0, 360875.0 ] ] ], [ [ [ 2069875.0, 361375.0 ], [ 2069625.0, 361375.0 ], [ 2069375.0, 361375.0 ], [ 2069375.0, 361625.0 ], [ 2069375.0, 361875.0 ], [ 2069625.0, 361875.0 ], [ 2069625.0, 362125.0 ], [ 2069875.0, 362125.0 ], [ 2069875.0, 361875.0 ], [ 2070125.0, 361875.0 ], [ 2070125.0, 361625.0 ], [ 2070125.0, 361375.0 ], [ 2069875.0, 361375.0 ] ] ], [ [ [ 2068125.0, 362125.0 ], [ 2068125.0, 362375.0 ], [ 2068375.0, 362375.0 ], [ 2068625.0, 362375.0 ], [ 2068875.0, 362375.0 ], [ 2069125.0, 362375.0 ], [ 2069125.0, 362125.0 ], [ 2068875.0, 362125.0 ], [ 2068625.0, 362125.0 ], [ 2068375.0, 362125.0 ], [ 2068125.0, 362125.0 ] ] ], [ [ [ 2067625.0, 362284.505883899983019 ], [ 2067625.0, 362625.0 ], [ 2067875.0, 362625.0 ], [ 2067875.0, 362375.0 ], [ 2067719.184426684165373, 362219.184426684165373 ], [ 2067625.0, 362284.505883899983019 ] ] ], [ [ [ 2070625.0, 362875.0 ], [ 2070875.0, 362875.0 ], [ 2070875.0, 362625.0 ], [ 2070625.0, 362625.0 ], [ 2070625.0, 362375.0 ], [ 2070375.0, 362375.0 ], [ 2070375.0, 362625.0 ], [ 2070375.0, 362875.0 ], [ 2070125.0, 362875.0 ], [ 2070125.0, 363125.0 ], [ 2070375.0, 363125.0 ], [ 2070375.0, 363375.0 ], [ 2070625.0, 363375.0 ], [ 2070625.0, 363125.0 ], [ 2070625.0, 362875.0 ] ] ], [ [ [ 2068125.0, 363375.0 ], [ 2068375.0, 363375.0 ], [ 2068375.0, 363125.0 ], [ 2068125.0, 363125.0 ], [ 2068125.0, 363375.0 ] ] ], [ [ [ 2066625.0, 363375.0 ], [ 2066625.0, 363125.0 ], [ 2066375.0, 363125.0 ], [ 2066125.0, 363125.0 ], [ 2066125.0, 363375.0 ], [ 2066375.0, 363375.0 ], [ 2066625.0, 363375.0 ] ] ], [ [ [ 2067625.0, 363125.0 ], [ 2067375.0, 363125.0 ], [ 2067125.0, 363125.0 ], [ 2067125.0, 363375.0 ], [ 2067375.0, 363375.0 ], [ 2067625.0, 363375.0 ], [ 2067875.0, 363375.0 ], [ 2067875.0, 363125.0 ], [ 2067625.0, 363125.0 ] ] ], [ [ [ 2065875.0, 363625.0 ], [ 2065875.0, 363875.0 ], [ 2066125.0, 363875.0 ], [ 2066125.0, 363625.0 ], [ 2065875.0, 363625.0 ] ] ], [ [ [ 2067875.0, 363625.0 ], [ 2067875.0, 363875.0 ], [ 2068125.0, 363875.0 ], [ 2068125.0, 363625.0 ], [ 2067875.0, 363625.0 ] ] ], [ [ [ 2072625.0, 363625.0 ], [ 2072375.0, 363625.0 ], [ 2072375.0, 363875.0 ], [ 2072625.0, 363875.0 ], [ 2072625.0, 363625.0 ] ] ], [ [ [ 2066625.0, 364125.0 ], [ 2066875.0, 364125.0 ], [ 2066875.0, 363875.0 ], [ 2066625.0, 363875.0 ], [ 2066625.0, 364125.0 ] ] ], [ [ [ 2070625.0, 364375.0 ], [ 2070875.0, 364375.0 ], [ 2070875.0, 364625.0 ], [ 2070875.0, 364875.0 ], [ 2071125.0, 364875.0 ], [ 2071125.0, 364625.0 ], [ 2071375.0, 364625.0 ], [ 2071625.0, 364625.0 ], [ 2071875.0, 364625.0 ], [ 2071875.0, 364875.0 ], [ 2072125.0, 364875.0 ], [ 2072125.0, 364625.0 ], [ 2072125.0, 364375.0 ], [ 2072125.0, 364125.0 ], [ 2071875.0, 364125.0 ], [ 2071875.0, 363875.0 ], [ 2071875.0, 363625.0 ], [ 2071625.0, 363625.0 ], [ 2071625.0, 363375.0 ], [ 2071375.0, 363375.0 ], [ 2071375.0, 363625.0 ], [ 2071375.0, 363875.0 ], [ 2071125.0, 363875.0 ], [ 2071125.0, 363625.0 ], [ 2070875.0, 363625.0 ], [ 2070625.0, 363625.0 ], [ 2070625.0, 363875.0 ], [ 2070375.0, 363875.0 ], [ 2070375.0, 363625.0 ], [ 2070125.0, 363625.0 ], [ 2070125.0, 363875.0 ], [ 2069875.0, 363875.0 ], [ 2069875.0, 363625.0 ], [ 2069625.0, 363625.0 ], [ 2069625.0, 363375.0 ], [ 2069375.0, 363375.0 ], [ 2069125.0, 363375.0 ], [ 2068875.0, 363375.0 ], [ 2068625.0, 363375.0 ], [ 2068625.0, 363625.0 ], [ 2068875.0, 363625.0 ], [ 2068875.0, 363875.0 ], [ 2069125.0, 363875.0 ], [ 2069375.0, 363875.0 ], [ 2069625.0, 363875.0 ], [ 2069625.0, 364125.0 ], [ 2069875.0, 364125.0 ], [ 2070125.0, 364125.0 ], [ 2070125.0, 364375.0 ], [ 2070375.0, 364375.0 ], [ 2070625.0, 364375.0 ] ] ], [ [ [ 2067875.0, 364375.0 ], [ 2068125.0, 364375.0 ], [ 2068125.0, 364125.0 ], [ 2067875.0, 364125.0 ], [ 2067875.0, 364375.0 ] ] ], [ [ [ 2068625.0, 364625.0 ], [ 2068625.0, 364875.0 ], [ 2068875.0, 364875.0 ], [ 2068875.0, 364625.0 ], [ 2069125.0, 364625.0 ], [ 2069125.0, 364875.0 ], [ 2069375.0, 364875.0 ], [ 2069375.0, 364625.0 ], [ 2069375.0, 364375.0 ], [ 2069125.0, 364375.0 ], [ 2068875.0, 364375.0 ], [ 2068625.0, 364375.0 ], [ 2068625.0, 364625.0 ] ] ], [ [ [ 2066520.182177630718797, 364729.817822369164787 ], [ 2066875.0, 364864.40389292355394 ], [ 2066875.0, 364625.0 ], [ 2066625.0, 364625.0 ], [ 2066520.182177630718797, 364729.817822369164787 ] ] ], [ [ [ 2067625.0, 364875.0 ], [ 2067453.413004581583664, 365046.586995418299921 ], [ 2067875.0, 365134.984268651111051 ], [ 2067875.0, 364875.0 ], [ 2067625.0, 364875.0 ] ] ], [ [ [ 2070625.0, 365125.0 ], [ 2070625.0, 365375.0 ], [ 2070875.0, 365375.0 ], [ 2070875.0, 365125.0 ], [ 2070625.0, 365125.0 ] ] ], [ [ [ 2072375.0, 365875.0 ], [ 2072375.0, 365625.0 ], [ 2072125.0, 365625.0 ], [ 2072125.0, 365875.0 ], [ 2072375.0, 365875.0 ] ] ], [ [ [ 2071125.0, 366125.0 ], [ 2071375.0, 366125.0 ], [ 2071375.0, 365875.0 ], [ 2071125.0, 365875.0 ], [ 2071125.0, 366125.0 ] ] ], [ [ [ 2071125.0, 366375.0 ], [ 2071125.0, 366125.0 ], [ 2070875.0, 366125.0 ], [ 2070875.0, 366375.0 ], [ 2071125.0, 366375.0 ] ] ], [ [ [ 2071375.0, 366625.0 ], [ 2071375.0, 366875.0 ], [ 2071625.0, 366875.0 ], [ 2071625.0, 366625.0 ], [ 2071375.0, 366625.0 ] ] ], [ [ [ 2070875.0, 367375.0 ], [ 2070875.0, 367125.0 ], [ 2070625.0, 367125.0 ], [ 2070625.0, 366875.0 ], [ 2070625.0, 366625.0 ], [ 2070375.0, 366625.0 ], [ 2070375.0, 366875.0 ], [ 2070295.117965420009568, 366954.882034580048639 ], [ 2070504.795384774450213, 367245.204615225607995 ], [ 2070593.64609425002709, 367368.228674500016496 ], [ 2070594.4274010383524, 367375.0 ], [ 2070875.0, 367375.0 ] ] ], [ [ [ 2071125.0, 367875.0 ], [ 2070875.0, 367875.0 ], [ 2070648.246363546932116, 367875.0 ], [ 2070629.015594316646457, 368125.0 ], [ 2070875.0, 368125.0 ], [ 2071125.0, 368125.0 ], [ 2071125.0, 367875.0 ] ] ], [ [ [ 2071125.0, 368375.0 ], [ 2071125.0, 368625.0 ], [ 2071375.0, 368625.0 ], [ 2071375.0, 368375.0 ], [ 2071125.0, 368375.0 ] ] ], [ [ [ 2071875.0, 369158.764727400324773 ], [ 2071875.0, 368875.0 ], [ 2071625.0, 368875.0 ], [ 2071625.0, 368625.0 ], [ 2071375.0, 368625.0 ], [ 2071375.0, 368947.085895284602884 ], [ 2071499.996063312981278, 369000.003936686960515 ], [ 2071875.0, 369158.764727400324773 ] ] ], [ [ [ 2072625.0, 369491.622143981629051 ], [ 2072625.0, 369125.0 ], [ 2072375.0, 369125.0 ], [ 2072202.560165878152475, 369297.439834121789318 ], [ 2072530.807548559969291, 369436.405879343976267 ], [ 2072625.0, 369491.622143981629051 ] ] ], [ [ [ 2074375.0, 356875.0 ], [ 2074125.0, 356875.0 ], [ 2074125.0, 357125.0 ], [ 2074375.0, 357125.0 ], [ 2074625.0, 357125.0 ], [ 2074625.0, 357375.0 ], [ 2074375.0, 357375.0 ], [ 2074375.0, 357625.0 ], [ 2074625.0, 357625.0 ], [ 2074875.0, 357625.0 ], [ 2075125.0, 357625.0 ], [ 2075125.0, 357375.0 ], [ 2075375.0, 357375.0 ], [ 2075625.0, 357375.0 ], [ 2075875.0, 357375.0 ], [ 2076125.0, 357375.0 ], [ 2076125.0, 357037.964313182164915 ], [ 2075875.0, 356990.955766173545271 ], [ 2075777.396945019485429, 356972.603054980572779 ], [ 2075375.0, 356896.938672156305984 ], [ 2075366.362720810109749, 356895.314568376983516 ], [ 2075263.560928629944101, 356736.439071370055899 ], [ 2075191.453294214559719, 356625.0 ], [ 2074968.918071488384157, 356281.081928511557635 ], [ 2074772.489500060677528, 355977.510499939206056 ], [ 2074748.717039729934186, 355940.77124306402402 ], [ 2074375.0, 355807.89407338242745 ], [ 2074240.078142586629838, 355759.921857413311955 ], [ 2073875.0, 355630.116295604908373 ], [ 2073625.0, 355541.227406716148835 ], [ 2073502.373224553652108, 355497.626775446347892 ], [ 2073375.0, 355625.0 ], [ 2073125.0, 355625.0 ], [ 2073125.0, 355875.0 ], [ 2073125.0, 356125.0 ], [ 2073125.0, 356375.0 ], [ 2073375.0, 356375.0 ], [ 2073375.0, 356125.0 ], [ 2073625.0, 356125.0 ], [ 2073625.0, 356375.0 ], [ 2073875.0, 356375.0 ], [ 2073875.0, 356625.0 ], [ 2074125.0, 356625.0 ], [ 2074375.0, 356625.0 ], [ 2074625.0, 356625.0 ], [ 2074625.0, 356875.0 ], [ 2074375.0, 356875.0 ] ] ], [ [ [ 2072875.0, 357125.0 ], [ 2072625.0, 357125.0 ], [ 2072625.0, 357375.0 ], [ 2072875.0, 357375.0 ], [ 2073125.0, 357375.0 ], [ 2073125.0, 357125.0 ], [ 2072875.0, 357125.0 ] ] ], [ [ [ 2073625.0, 357625.0 ], [ 2073875.0, 357625.0 ], [ 2074125.0, 357625.0 ], [ 2074125.0, 357375.0 ], [ 2073875.0, 357375.0 ], [ 2073875.0, 357125.0 ], [ 2073625.0, 357125.0 ], [ 2073625.0, 357375.0 ], [ 2073625.0, 357625.0 ] ] ], [ [ [ 2076125.0, 358125.0 ], [ 2076375.0, 358125.0 ], [ 2076375.0, 357875.0 ], [ 2076125.0, 357875.0 ], [ 2076125.0, 357625.0 ], [ 2075875.0, 357625.0 ], [ 2075625.0, 357625.0 ], [ 2075625.0, 357875.0 ], [ 2075875.0, 357875.0 ], [ 2075875.0, 358125.0 ], [ 2076125.0, 358125.0 ] ] ], [ [ [ 2074875.0, 359375.0 ], [ 2075125.0, 359375.0 ], [ 2075375.0, 359375.0 ], [ 2075625.0, 359375.0 ], [ 2075625.0, 359125.0 ], [ 2075875.0, 359125.0 ], [ 2076125.0, 359125.0 ], [ 2076125.0, 359375.0 ], [ 2076375.0, 359375.0 ], [ 2076375.0, 359625.0 ], [ 2076625.0, 359625.0 ], [ 2076625.0, 359875.0 ], [ 2076875.0, 359875.0 ], [ 2077125.0, 359875.0 ], [ 2077125.0, 360125.0 ], [ 2077375.0, 360125.0 ], [ 2077375.0, 359875.0 ], [ 2077375.0, 359625.0 ], [ 2077375.0, 359375.0 ], [ 2077625.0, 359375.0 ], [ 2077625.0, 359625.0 ], [ 2077875.0, 359625.0 ], [ 2077875.0, 359375.0 ], [ 2078125.0, 359375.0 ], [ 2078375.0, 359375.0 ], [ 2078375.0, 359625.0 ], [ 2078625.0, 359625.0 ], [ 2078625.0, 359875.0 ], [ 2078875.0, 359875.0 ], [ 2078875.0, 360125.0 ], [ 2078875.0, 360375.0 ], [ 2079125.0, 360375.0 ], [ 2079375.0, 360375.0 ], [ 2079375.0, 360625.0 ], [ 2079125.0, 360625.0 ], [ 2078875.0, 360625.0 ], [ 2078875.0, 360875.0 ], [ 2079125.0, 360875.0 ], [ 2079375.0, 360875.0 ], [ 2079375.0, 361125.0 ], [ 2079625.0, 361125.0 ], [ 2079625.0, 361375.0 ], [ 2079875.0, 361375.0 ], [ 2079875.0, 361625.0 ], [ 2079625.0, 361625.0 ], [ 2079625.0, 361875.0 ], [ 2079875.0, 361875.0 ], [ 2079875.0, 362125.0 ], [ 2080125.0, 362125.0 ], [ 2080125.0, 361875.0 ], [ 2080125.0, 361625.0 ], [ 2080125.0, 361375.0 ], [ 2080375.0, 361375.0 ], [ 2080754.836753551848233, 361375.0 ], [ 2080686.3720232618507, 361125.0 ], [ 2080606.992742140078917, 360835.145352264982648 ], [ 2080505.02776063256897, 360744.972239367372822 ], [ 2080239.684800343587995, 360510.315199656412005 ], [ 2079974.341840054607019, 360275.658159945451189 ], [ 2079708.998879765626043, 360041.001120234432165 ], [ 2079443.655919476412237, 359806.344080523471348 ], [ 2079231.327361539937556, 359618.57052588602528 ], [ 2079184.910161321749911, 359565.089838678191882 ], [ 2078952.58692899858579, 359297.413071001356002 ], [ 2078720.26369667542167, 359029.736303324520122 ], [ 2078487.94046435225755, 358762.059535647684243 ], [ 2078255.617232029093429, 358494.382767970848363 ], [ 2078023.293999705929309, 358226.706000294012483 ], [ 2077790.970767382765189, 357959.029232617176604 ], [ 2077718.039735300000757, 357875.0 ], [ 2077509.406068820040673, 357634.617732099024579 ], [ 2077375.0, 357566.214643503539264 ], [ 2077248.277869393816218, 357501.722130606183782 ], [ 2077125.0, 357625.0 ], [ 2077125.0, 357875.0 ], [ 2077125.0, 358125.0 ], [ 2077125.0, 358375.0 ], [ 2077125.0, 358625.0 ], [ 2077375.0, 358625.0 ], [ 2077625.0, 358625.0 ], [ 2077625.0, 358875.0 ], [ 2077625.0, 359125.0 ], [ 2077375.0, 359125.0 ], [ 2077375.0, 358875.0 ], [ 2077125.0, 358875.0 ], [ 2077125.0, 359125.0 ], [ 2076875.0, 359125.0 ], [ 2076875.0, 359375.0 ], [ 2076625.0, 359375.0 ], [ 2076625.0, 359125.0 ], [ 2076625.0, 358875.0 ], [ 2076625.0, 358625.0 ], [ 2076625.0, 358375.0 ], [ 2076375.0, 358375.0 ], [ 2076375.0, 358625.0 ], [ 2076125.0, 358625.0 ], [ 2076125.0, 358875.0 ], [ 2075875.0, 358875.0 ], [ 2075875.0, 358625.0 ], [ 2075625.0, 358625.0 ], [ 2075625.0, 358875.0 ], [ 2075375.0, 358875.0 ], [ 2075375.0, 358625.0 ], [ 2075125.0, 358625.0 ], [ 2074875.0, 358625.0 ], [ 2074875.0, 358375.0 ], [ 2074875.0, 358125.0 ], [ 2074625.0, 358125.0 ], [ 2074625.0, 358375.0 ], [ 2074625.0, 358625.0 ], [ 2074625.0, 358875.0 ], [ 2074375.0, 358875.0 ], [ 2074375.0, 358625.0 ], [ 2074125.0, 358625.0 ], [ 2074125.0, 358875.0 ], [ 2074125.0, 359125.0 ], [ 2073875.0, 359125.0 ], [ 2073625.0, 359125.0 ], [ 2073625.0, 359375.0 ], [ 2073625.0, 359625.0 ], [ 2073875.0, 359625.0 ], [ 2073875.0, 359875.0 ], [ 2073875.0, 360125.0 ], [ 2074125.0, 360125.0 ], [ 2074375.0, 360125.0 ], [ 2074375.0, 359875.0 ], [ 2074625.0, 359875.0 ], [ 2074625.0, 359625.0 ], [ 2074875.0, 359625.0 ], [ 2074875.0, 359375.0 ] ] ], [ [ [ 2076125.0, 360375.0 ], [ 2075875.0, 360375.0 ], [ 2075875.0, 360625.0 ], [ 2076125.0, 360625.0 ], [ 2076125.0, 360375.0 ] ] ], [ [ [ 2077375.0, 360375.0 ], [ 2077375.0, 360625.0 ], [ 2077625.0, 360625.0 ], [ 2077625.0, 360375.0 ], [ 2077375.0, 360375.0 ] ] ], [ [ [ 2078125.0, 361375.0 ], [ 2078375.0, 361375.0 ], [ 2078375.0, 361125.0 ], [ 2078125.0, 361125.0 ], [ 2078125.0, 361375.0 ] ] ], [ [ [ 2074375.0, 361125.0 ], [ 2074625.0, 361125.0 ], [ 2074875.0, 361125.0 ], [ 2075125.0, 361125.0 ], [ 2075375.0, 361125.0 ], [ 2075375.0, 360875.0 ], [ 2075375.0, 360625.0 ], [ 2075125.0, 360625.0 ], [ 2074875.0, 360625.0 ], [ 2074625.0, 360625.0 ], [ 2074625.0, 360875.0 ], [ 2074375.0, 360875.0 ], [ 2074125.0, 360875.0 ], [ 2074125.0, 361125.0 ], [ 2074125.0, 361375.0 ], [ 2073875.0, 361375.0 ], [ 2073625.0, 361375.0 ], [ 2073625.0, 361625.0 ], [ 2073875.0, 361625.0 ], [ 2073875.0, 361875.0 ], [ 2073875.0, 362125.0 ], [ 2074125.0, 362125.0 ], [ 2074125.0, 361875.0 ], [ 2074375.0, 361875.0 ], [ 2074375.0, 361625.0 ], [ 2074375.0, 361375.0 ], [ 2074375.0, 361125.0 ] ] ], [ [ [ 2078125.0, 361375.0 ], [ 2077875.0, 361375.0 ], [ 2077875.0, 361625.0 ], [ 2078125.0, 361625.0 ], [ 2078125.0, 361375.0 ] ] ], [ [ [ 2080625.0, 361875.0 ], [ 2080625.0, 362125.0 ], [ 2080625.0, 362375.0 ], [ 2081028.695674711605534, 362375.0 ], [ 2080960.230944421608001, 362125.0 ], [ 2080891.766214131610468, 361875.0 ], [ 2080780.669894482009113, 361469.330105518107302 ], [ 2080625.0, 361625.0 ], [ 2080625.0, 361875.0 ] ] ], [ [ [ 2077625.0, 362125.0 ], [ 2077625.0, 361875.0 ], [ 2077375.0, 361875.0 ], [ 2077125.0, 361875.0 ], [ 2077125.0, 362125.0 ], [ 2077375.0, 362125.0 ], [ 2077625.0, 362125.0 ] ] ], [ [ [ 2074875.0, 362125.0 ], [ 2074875.0, 362375.0 ], [ 2075125.0, 362375.0 ], [ 2075125.0, 362125.0 ], [ 2074875.0, 362125.0 ] ] ], [ [ [ 2074625.0, 362875.0 ], [ 2074875.0, 362875.0 ], [ 2074875.0, 362625.0 ], [ 2074625.0, 362625.0 ], [ 2074625.0, 362875.0 ] ] ], [ [ [ 2080875.0, 362875.0 ], [ 2080875.0, 363125.0 ], [ 2081125.0, 363125.0 ], [ 2081210.637321189278737, 363039.362678810663056 ], [ 2081165.625135291367769, 362875.0 ], [ 2081049.399536174023524, 362450.600463825976476 ], [ 2080875.0, 362625.0 ], [ 2080875.0, 362875.0 ] ] ], [ [ [ 2079625.0, 362625.0 ], [ 2079625.0, 362375.0 ], [ 2079375.0, 362375.0 ], [ 2079375.0, 362625.0 ], [ 2079125.0, 362625.0 ], [ 2078875.0, 362625.0 ], [ 2078875.0, 362875.0 ], [ 2079125.0, 362875.0 ], [ 2079375.0, 362875.0 ], [ 2079375.0, 363125.0 ], [ 2079375.0, 363375.0 ], [ 2079625.0, 363375.0 ], [ 2079875.0, 363375.0 ], [ 2079875.0, 363125.0 ], [ 2079875.0, 362875.0 ], [ 2079875.0, 362625.0 ], [ 2079625.0, 362625.0 ] ] ], [ [ [ 2077625.0, 363875.0 ], [ 2077625.0, 364125.0 ], [ 2077875.0, 364125.0 ], [ 2077875.0, 363875.0 ], [ 2078125.0, 363875.0 ], [ 2078125.0, 363625.0 ], [ 2078375.0, 363625.0 ], [ 2078375.0, 363375.0 ], [ 2078125.0, 363375.0 ], [ 2077875.0, 363375.0 ], [ 2077625.0, 363375.0 ], [ 2077625.0, 363125.0 ], [ 2077625.0, 362875.0 ], [ 2077375.0, 362875.0 ], [ 2077125.0, 362875.0 ], [ 2077125.0, 362625.0 ], [ 2077375.0, 362625.0 ], [ 2077375.0, 362375.0 ], [ 2077125.0, 362375.0 ], [ 2077125.0, 362125.0 ], [ 2076875.0, 362125.0 ], [ 2076875.0, 362375.0 ], [ 2076875.0, 362625.0 ], [ 2076625.0, 362625.0 ], [ 2076375.0, 362625.0 ], [ 2076125.0, 362625.0 ], [ 2076125.0, 362875.0 ], [ 2075875.0, 362875.0 ], [ 2075875.0, 363125.0 ], [ 2076125.0, 363125.0 ], [ 2076375.0, 363125.0 ], [ 2076375.0, 363375.0 ], [ 2076625.0, 363375.0 ], [ 2076875.0, 363375.0 ], [ 2076875.0, 363625.0 ], [ 2077125.0, 363625.0 ], [ 2077125.0, 363875.0 ], [ 2077375.0, 363875.0 ], [ 2077625.0, 363875.0 ] ] ], [ [ [ 2080375.0, 363375.0 ], [ 2080125.0, 363375.0 ], [ 2080125.0, 363625.0 ], [ 2080375.0, 363625.0 ], [ 2080375.0, 363375.0 ] ] ], [ [ [ 2075875.0, 363875.0 ], [ 2075625.0, 363875.0 ], [ 2075625.0, 364125.0 ], [ 2075625.0, 364375.0 ], [ 2075875.0, 364375.0 ], [ 2075875.0, 364125.0 ], [ 2075875.0, 363875.0 ] ] ], [ [ [ 2081625.0, 364625.0 ], [ 2081875.0, 364625.0 ], [ 2081875.0, 364375.0 ], [ 2081625.0, 364375.0 ], [ 2081625.0, 364625.0 ] ] ], [ [ [ 2081625.0, 364875.0 ], [ 2081625.0, 364625.0 ], [ 2081375.0, 364625.0 ], [ 2081375.0, 364875.0 ], [ 2081375.0, 365125.0 ], [ 2081625.0, 365125.0 ], [ 2081625.0, 364875.0 ] ] ], [ [ [ 2073625.0, 364875.0 ], [ 2073375.0, 364875.0 ], [ 2073375.0, 365125.0 ], [ 2073625.0, 365125.0 ], [ 2073625.0, 364875.0 ] ] ], [ [ [ 2083875.0, 365375.0 ], [ 2084125.0, 365375.0 ], [ 2084237.027790324995294, 365262.972209675121121 ], [ 2083947.204781475709751, 365052.795218524290249 ], [ 2083625.0, 364819.135262491530739 ], [ 2083625.0, 365125.0 ], [ 2083625.0, 365375.0 ], [ 2083875.0, 365375.0 ] ] ], [ [ [ 2073875.0, 365125.0 ], [ 2073625.0, 365125.0 ], [ 2073625.0, 365375.0 ], [ 2073875.0, 365375.0 ], [ 2073875.0, 365125.0 ] ] ], [ [ [ 2080375.0, 365625.0 ], [ 2080625.0, 365625.0 ], [ 2080625.0, 365375.0 ], [ 2080375.0, 365375.0 ], [ 2080375.0, 365625.0 ] ] ], [ [ [ 2081125.0, 365625.0 ], [ 2081125.0, 365875.0 ], [ 2081375.0, 365875.0 ], [ 2081375.0, 365625.0 ], [ 2081125.0, 365625.0 ] ] ], [ [ [ 2077625.0, 366125.0 ], [ 2077625.0, 365875.0 ], [ 2077375.0, 365875.0 ], [ 2077125.0, 365875.0 ], [ 2077125.0, 366125.0 ], [ 2077375.0, 366125.0 ], [ 2077625.0, 366125.0 ] ] ], [ [ [ 2074625.0, 365875.0 ], [ 2074375.0, 365875.0 ], [ 2074375.0, 366125.0 ], [ 2074625.0, 366125.0 ], [ 2074625.0, 365875.0 ] ] ], [ [ [ 2080875.0, 365875.0 ], [ 2080875.0, 366125.0 ], [ 2081125.0, 366125.0 ], [ 2081125.0, 365875.0 ], [ 2080875.0, 365875.0 ] ] ], [ [ [ 2073875.0, 366125.0 ], [ 2073625.0, 366125.0 ], [ 2073625.0, 366375.0 ], [ 2073625.0, 366625.0 ], [ 2073875.0, 366625.0 ], [ 2074125.0, 366625.0 ], [ 2074125.0, 366375.0 ], [ 2074125.0, 366125.0 ], [ 2073875.0, 366125.0 ] ] ], [ [ [ 2082625.0, 366875.0 ], [ 2082875.0, 366875.0 ], [ 2082875.0, 366625.0 ], [ 2082625.0, 366625.0 ], [ 2082625.0, 366875.0 ] ] ], [ [ [ 2075125.0, 367125.0 ], [ 2075125.0, 367375.0 ], [ 2075375.0, 367375.0 ], [ 2075375.0, 367125.0 ], [ 2075625.0, 367125.0 ], [ 2075625.0, 366875.0 ], [ 2075375.0, 366875.0 ], [ 2075125.0, 366875.0 ], [ 2074875.0, 366875.0 ], [ 2074875.0, 367125.0 ], [ 2075125.0, 367125.0 ] ] ], [ [ [ 2075375.0, 367375.0 ], [ 2075375.0, 367625.0 ], [ 2075625.0, 367625.0 ], [ 2075625.0, 367375.0 ], [ 2075375.0, 367375.0 ] ] ], [ [ [ 2083625.0, 367375.0 ], [ 2083375.0, 367375.0 ], [ 2083375.0, 367625.0 ], [ 2083625.0, 367625.0 ], [ 2083625.0, 367875.0 ], [ 2083875.0, 367875.0 ], [ 2083875.0, 368125.0 ], [ 2084125.0, 368125.0 ], [ 2084125.0, 367875.0 ], [ 2084348.716473254840821, 367875.0 ], [ 2084355.473230011295527, 367625.0 ], [ 2084362.229986767983064, 367375.0 ], [ 2084368.98674352443777, 367125.0 ], [ 2084125.0, 367125.0 ], [ 2083875.0, 367125.0 ], [ 2083875.0, 367375.0 ], [ 2083625.0, 367375.0 ] ] ], [ [ [ 2075875.0, 367625.0 ], [ 2075625.0, 367625.0 ], [ 2075625.0, 367875.0 ], [ 2075875.0, 367875.0 ], [ 2075875.0, 367625.0 ] ] ], [ [ [ 2076125.0, 367375.0 ], [ 2076125.0, 367625.0 ], [ 2076125.0, 367875.0 ], [ 2076125.0, 368125.0 ], [ 2076375.0, 368125.0 ], [ 2076375.0, 367875.0 ], [ 2076375.0, 367625.0 ], [ 2076375.0, 367375.0 ], [ 2076125.0, 367375.0 ] ] ], [ [ [ 2073125.0, 367375.0 ], [ 2073125.0, 367625.0 ], [ 2073125.0, 367875.0 ], [ 2073125.0, 368125.0 ], [ 2073375.0, 368125.0 ], [ 2073375.0, 368375.0 ], [ 2073625.0, 368375.0 ], [ 2073625.0, 368125.0 ], [ 2073625.0, 367875.0 ], [ 2073625.0, 367625.0 ], [ 2073875.0, 367625.0 ], [ 2073875.0, 367375.0 ], [ 2073625.0, 367375.0 ], [ 2073375.0, 367375.0 ], [ 2073125.0, 367375.0 ] ] ], [ [ [ 2074375.0, 368375.0 ], [ 2074375.0, 368125.0 ], [ 2074125.0, 368125.0 ], [ 2074125.0, 368375.0 ], [ 2074375.0, 368375.0 ] ] ], [ [ [ 2077125.0, 368375.0 ], [ 2076875.0, 368375.0 ], [ 2076875.0, 368625.0 ], [ 2077125.0, 368625.0 ], [ 2077125.0, 368375.0 ] ] ], [ [ [ 2083875.0, 368625.0 ], [ 2084125.0, 368625.0 ], [ 2084125.0, 368375.0 ], [ 2083875.0, 368375.0 ], [ 2083875.0, 368625.0 ] ] ], [ [ [ 2079125.0, 368625.0 ], [ 2079125.0, 368375.0 ], [ 2078875.0, 368375.0 ], [ 2078875.0, 368625.0 ], [ 2078875.0, 368875.0 ], [ 2079125.0, 368875.0 ], [ 2079125.0, 368625.0 ] ] ], [ [ [ 2083375.0, 368875.0 ], [ 2083625.0, 368875.0 ], [ 2083625.0, 368625.0 ], [ 2083375.0, 368625.0 ], [ 2083125.0, 368625.0 ], [ 2083125.0, 368375.0 ], [ 2083125.0, 368125.0 ], [ 2083125.0, 367875.0 ], [ 2082875.0, 367875.0 ], [ 2082625.0, 367875.0 ], [ 2082625.0, 368125.0 ], [ 2082375.0, 368125.0 ], [ 2082375.0, 367875.0 ], [ 2082125.0, 367875.0 ], [ 2082125.0, 368125.0 ], [ 2082125.0, 368375.0 ], [ 2082125.0, 368625.0 ], [ 2082375.0, 368625.0 ], [ 2082625.0, 368625.0 ], [ 2082625.0, 368875.0 ], [ 2082625.0, 369125.0 ], [ 2082875.0, 369125.0 ], [ 2082875.0, 369375.0 ], [ 2083125.0, 369375.0 ], [ 2083125.0, 369125.0 ], [ 2083125.0, 368875.0 ], [ 2083375.0, 368875.0 ] ] ], [ [ [ 2084125.0, 368875.0 ], [ 2084375.0, 368875.0 ], [ 2084498.74964591441676, 368751.250354085525032 ], [ 2084464.899913297733292, 368625.0 ], [ 2084125.0, 368625.0 ], [ 2084125.0, 368875.0 ] ] ], [ [ [ 2076375.0, 369125.0 ], [ 2076375.0, 369375.0 ], [ 2076625.0, 369375.0 ], [ 2076625.0, 369125.0 ], [ 2076625.0, 368875.0 ], [ 2076375.0, 368875.0 ], [ 2076375.0, 369125.0 ] ] ], [ [ [ 2081125.0, 369375.0 ], [ 2080875.0, 369375.0 ], [ 2080875.0, 369625.0 ], [ 2080875.0, 369875.0 ], [ 2081125.0, 369875.0 ], [ 2081125.0, 369625.0 ], [ 2081125.0, 369375.0 ] ] ], [ [ [ 2080288.811633700039238, 370324.428998021001462 ], [ 2080625.0, 370457.010325575945899 ], [ 2080625.0, 370125.0 ], [ 2080625.0, 369875.0 ], [ 2080375.0, 369875.0 ], [ 2080375.0, 370125.0 ], [ 2080228.359867894090712, 370271.640132105909288 ], [ 2080288.811633700039238, 370324.428998021001462 ] ] ], [ [ [ 2084375.0, 370375.0 ], [ 2084375.0, 370625.0 ], [ 2084744.536088891560212, 370625.0 ], [ 2084790.138491299934685, 370474.161284343979787 ], [ 2084780.131389210000634, 370375.0 ], [ 2084375.0, 370375.0 ] ] ], [ [ [ 2081625.0, 370375.0 ], [ 2081625.0, 370653.635633661004249 ], [ 2081875.0, 370678.635633661004249 ], [ 2081875.0, 370375.0 ], [ 2081625.0, 370375.0 ] ] ], [ [ [ 2082375.0, 370728.635633660946041 ], [ 2082375.0, 370375.0 ], [ 2082125.0, 370375.0 ], [ 2082125.0, 370703.635633661004249 ], [ 2082375.0, 370728.635633660946041 ] ] ] ] } },
{ "type": "Feature", "properties": { "classification": "fluvial", "mesh_name": "BaldEagleCr", "cell_id": 63, "area_acres": 99601.164563350947 }, "geometry": { "type": "MultiPolygon", "coordinates": [ [ [ [ 1981875.0, 300125.0 ], [ 1981875.0, 300375.0 ], [ 1982125.0, 300375.0 ], [ 1982125.0, 300625.0 ], [ 1982375.0, 300625.0 ], [ 1982375.0, 300875.0 ], [ 1982625.0, 300875.0 ], [ 1982625.0, 301125.0 ], [ 1982625.0, 301375.0 ], [ 1982875.0, 301375.0 ], [ 1983125.0, 301375.0 ], [ 1983125.0, 301625.0 ], [ 1983375.0, 301625.0 ], [ 1983375.0, 301875.0 ], [ 1983625.0, 301875.0 ], [ 1983625.0, 302125.0 ], [ 1983875.0, 302125.0 ], [ 1983875.0, 302375.0 ], [ 1983875.0, 302625.0 ], [ 1984125.0, 302625.0 ], [ 1984375.0, 302625.0 ], [ 1984375.0, 302875.0 ], [ 1984625.0, 302875.0 ], [ 1984625.0, 303125.0 ], [ 1984375.0, 303125.0 ], [ 1984125.0, 303125.0 ], [ 1983875.0, 303125.0 ], [ 1983875.0, 302875.0 ], [ 1983625.0, 302875.0 ], [ 1983375.0, 302875.0 ], [ 1983375.0, 302625.0 ], [ 1983125.0, 302625.0 ], [ 1983125.0, 302375.0 ], [ 1982875.0, 302375.0 ], [ 1982625.0, 302375.0 ], [ 1982375.0, 302375.0 ], [ 1982375.0, 302625.0 ], [ 1982625.0, 302625.0 ], [ 1982625.0, 302875.0 ], [ 1982875.0, 302875.0 ], [ 1982875.0, 303125.0 ], [ 1983125.0, 303125.0 ], [ 1983375.0, 303125.0 ], [ 1983375.0, 303375.0 ], [ 1983625.0, 303375.0 ], [ 1983625.0, 303625.0 ], [ 1983875.0, 303625.0 ], [ 1983875.0, 303875.0 ], [ 1983875.0, 304125.0 ], [ 1983875.0, 304375.0 ], [ 1984125.0, 304375.0 ], [ 1984125.0, 304625.0 ], [ 1984375.0, 304625.0 ], [ 1984375.0, 304875.0 ], [ 1984625.0, 304875.0 ], [ 1984625.0, 305125.0 ], [ 1984875.0, 305125.0 ], [ 1985125.0, 305125.0 ], [ 1985375.0, 305125.0 ], [ 1985375.0, 305375.0 ], [ 1985125.0, 305375.0 ], [ 1985125.0, 305625.0 ], [ 1984875.0, 305625.0 ], [ 1984875.0, 305875.0 ], [ 1984875.0, 306125.0 ], [ 1985125.0, 306125.0 ], [ 1985375.0, 306125.0 ], [ 1985375.0, 305875.0 ], [ 1985625.0, 305875.0 ], [ 1985875.0, 305875.0 ], [ 1986125.0, 305875.0 ], [ 1986125.0, 306125.0 ], [ 1986125.0, 306375.0 ], [ 1986375.0, 306375.0 ], [ 1986375.0, 306625.0 ], [ 1986625.0, 306625.0 ], [ 1986625.0, 306875.0 ], [ 1986875.0, 306875.0 ], [ 1986875.0, 306625.0 ], [ 1986875.0, 306375.0 ], [ 1986625.0, 306375.0 ], [ 1986625.0, 306125.0 ], [ 1986875.0, 306125.0 ], [ 1987125.0, 306125.0 ], [ 1987125.0, 305875.0 ], [ 1987375.0, 305875.0 ], [ 1987375.0, 305625.0 ], [ 1987375.0, 305375.0 ], [ 1987625.0, 305375.0 ], [ 1987625.0, 305625.0 ], [ 1987875.0, 305625.0 ], [ 1987875.0, 305875.0 ], [ 1988125.0, 305875.0 ], [ 1988125.0, 306125.0 ], [ 1988125.0, 306375.0 ], [ 1987875.0, 306375.0 ], [ 1987875.0, 306625.0 ], [ 1987875.0, 306875.0 ], [ 1987625.0, 306875.0 ], [ 1987625.0, 307125.0 ], [ 1987875.0, 307125.0 ], [ 1988125.0, 307125.0 ], [ 1988375.0, 307125.0 ], [ 1988375.0, 306875.0 ], [ 1988625.0, 306875.0 ], [ 1988875.0, 306875.0 ], [ 1988875.0, 307125.0 ], [ 1989125.0, 307125.0 ], [ 1989125.0, 307375.0 ], [ 1989375.0, 307375.0 ], [ 1989375.0, 307625.0 ], [ 1989625.0, 307625.0 ], [ 1989625.0, 307875.0 ], [ 1989375.0, 307875.0 ], [ 1989125.0, 307875.0 ], [ 1988875.0, 307875.0 ], [ 1988625.0, 307875.0 ], [ 1988375.0, 307875.0 ], [ 1988125.0, 307875.0 ], [ 1987875.0, 307875.0 ], [ 1987875.0, 308125.0 ], [ 1987875.0, 308375.0 ], [ 1987875.0, 308625.0 ], [ 1988125.0, 308625.0 ], [ 1988125.0, 308875.0 ], [ 1987875.0, 308875.0 ], [ 1987875.0, 309125.0 ], [ 1988125.0, 309125.0 ], [ 1988375.0, 309125.0 ], [ 1988375.0, 309375.0 ], [ 1988625.0, 309375.0 ], [ 1988875.0, 309375.0 ], [ 1989125.0, 309375.0 ], [ 1989125.0, 309625.0 ], [ 1989125.0, 309875.0 ], [ 1989375.0, 309875.0 ], [ 1989375.0, 310125.0 ], [ 1989625.0, 310125.0 ], [ 1989625.0, 310375.0 ], [ 1989875.0, 310375.0 ], [ 1989875.0, 310625.0 ], [ 1989875.0, 310875.0 ], [ 1990125.0, 310875.0 ], [ 1990125.0, 311125.0 ], [ 1990375.0, 311125.0 ], [ 1990375.0, 311375.0 ], [ 1990125.0, 311375.0 ], [ 1990125.0, 311625.0 ], [ 1989875.0, 311625.0 ], [ 1989625.0, 311625.0 ], [ 1989472.232882233802229, 311777.767117766314186 ], [ 1989551.787058606510982, 311875.0 ], [ 1989809.732882233336568, 312190.267117766605224 ], [ 1990034.732882233103737, 312465.267117766779847 ], [ 1990259.732882233103737, 312740.267117767012678 ], [ 1990307.077053480083123, 312798.132215957972221 ], [ 1990545.596453194972128, 312954.40354680508608 ], [ 1990875.0, 313170.219663676631171 ], [ 1990998.721453195437789, 313251.278546804445796 ], [ 1991276.169096539961174, 313433.054588995000813 ], [ 1991288.911694810260087, 313461.088305189681705 ], [ 1991363.417010633274913, 313625.0 ], [ 1991625.0, 313625.0 ], [ 1991875.0, 313625.0 ], [ 1991875.0, 313875.0 ], [ 1991625.0, 313875.0 ], [ 1991523.286694810492918, 313976.713305189507082 ], [ 1991590.68973790621385, 314125.0 ], [ 1991757.661694810725749, 314492.338305189332459 ], [ 1991777.423601570073515, 314535.814500059990678 ], [ 1992125.0, 314700.455951947369613 ], [ 1992243.440604035742581, 314756.559395964199211 ], [ 1992375.0, 314625.0 ], [ 1992625.0, 314625.0 ], [ 1992625.0, 314375.0 ], [ 1992625.0, 314125.0 ], [ 1992875.0, 314125.0 ], [ 1993125.0, 314125.0 ], [ 1993125.0, 313875.0 ], [ 1993375.0, 313875.0 ], [ 1993375.0, 314125.0 ], [ 1993625.0, 314125.0 ], [ 1993625.0, 314375.0 ], [ 1993625.0, 314625.0 ], [ 1993875.0, 314625.0 ], [ 1993875.0, 314375.0 ], [ 1994125.0, 314375.0 ], [ 1994125.0, 314625.0 ], [ 1994125.0, 314875.0 ], [ 1994375.0, 314875.0 ], [ 1994375.0, 315125.0 ], [ 1994625.0, 315125.0 ], [ 1994875.0, 315125.0 ], [ 1994875.0, 315375.0 ], [ 1994875.0, 315625.0 ], [ 1994875.0, 315875.0 ], [ 1995125.0, 315875.0 ], [ 1995375.0, 315875.0 ], [ 1995625.0, 315875.0 ], [ 1995625.0, 316125.0 ], [ 1995875.0, 316125.0 ], [ 1995875.0, 316375.0 ], [ 1995875.0, 316625.0 ], [ 1995875.0, 316875.0 ], [ 1996125.0, 316875.0 ], [ 1996375.0, 316875.0 ], [ 1996625.0, 316875.0 ], [ 1996875.0, 316875.0 ], [ 1996875.0, 317125.0 ], [ 1997125.0, 317125.0 ], [ 1997375.0, 317125.0 ], [ 1997375.0, 316875.0 ], [ 1997625.0, 316875.0 ], [ 1997625.0, 317125.0 ], [ 1997875.0, 317125.0 ], [ 1997875.0, 317375.0 ], [ 1997625.0, 317375.0 ], [ 1997625.0, 317625.0 ], [ 1997625.0, 317875.0 ], [ 1997375.0, 317875.0 ], [ 1997375.0, 318125.0 ], [ 1997375.0, 318375.0 ], [ 1997625.0, 318375.0 ], [ 1997875.0, 318375.0 ], [ 1997875.0, 318625.0 ], [ 1997875.0, 318875.0 ], [ 1998125.0, 318875.0 ], [ 1998125.0, 319125.0 ], [ 1998375.0, 319125.0 ], [ 1998375.0, 319375.0 ], [ 1998125.0, 319375.0 ], [ 1998125.0, 319625.0 ], [ 1998125.0, 319875.0 ], [ 1998375.0, 319875.0 ], [ 1998625.0, 319875.0 ], [ 1998875.0, 319875.0 ], [ 1998875.0, 320125.0 ], [ 1999125.0, 320125.0 ], [ 1999375.0, 320125.0 ], [ 1999375.0, 320375.0 ], [ 1999125.0, 320375.0 ], [ 1998875.0, 320375.0 ], [ 1998625.0, 320375.0 ], [ 1998625.0, 320625.0 ], [ 1998480.176777553744614, 320769.823222446255386 ], [ 1998494.233968959888443, 320784.787329427024815 ], [ 1998787.831081654177979, 320962.168918345822021 ], [ 1999125.0, 321165.875139845709782 ], [ 1999255.363549187313765, 321244.636450812686235 ], [ 1999625.0, 321467.958473177917767 ], [ 1999722.896016720449552, 321527.103983279608656 ], [ 2000034.584328409051523, 321715.415671590832062 ], [ 2000098.248385060112923, 321753.879372483992483 ], [ 2000313.718596992781386, 321936.28140300733503 ], [ 2000625.0, 322199.790495347464457 ], [ 2000719.88580773328431, 322280.11419226671569 ], [ 2000875.0, 322125.0 ], [ 2001125.0, 322125.0 ], [ 2001375.0, 322125.0 ], [ 2001375.0, 322375.0 ], [ 2001625.0, 322375.0 ], [ 2001875.0, 322375.0 ], [ 2001875.0, 322625.0 ], [ 2002125.0, 322625.0 ], [ 2002125.0, 322875.0 ], [ 2002375.0, 322875.0 ], [ 2002625.0, 322875.0 ], [ 2002625.0, 323109.507223077933304 ], [ 2002853.736919600982219, 323091.999806937295943 ], [ 2002947.325, 323247.302000000025146 ], [ 2002991.896173857152462, 323380.988579147902783 ], [ 2003125.0, 323449.735253088292666 ], [ 2003125.0, 323625.0 ], [ 2003375.0, 323625.0 ], [ 2003625.0, 323625.0 ], [ 2003875.0, 323625.0 ], [ 2004125.0, 323625.0 ], [ 2004125.0, 323875.0 ], [ 2004375.0, 323875.0 ], [ 2004375.0, 324125.0 ], [ 2004625.0, 324125.0 ], [ 2004875.0, 324125.0 ], [ 2005125.0, 324125.0 ], [ 2005125.0, 324375.0 ], [ 2005125.0, 324625.0 ], [ 2005125.0, 324875.0 ], [ 2004875.0, 324875.0 ], [ 2004875.0, 325125.0 ], [ 2004625.0, 325125.0 ], [ 2004625.0, 325375.0 ], [ 2004875.0, 325375.0 ], [ 2004875.0, 325625.0 ], [ 2004875.0, 325875.0 ], [ 2005125.0, 325875.0 ], [ 2005125.0, 326125.0 ], [ 2005125.0, 326375.0 ], [ 2005375.0, 326375.0 ], [ 2005625.0, 326375.0 ], [ 2005625.0, 326125.0 ], [ 2005875.0, 326125.0 ], [ 2006125.0, 326125.0 ], [ 2006375.0, 326125.0 ], [ 2006625.0, 326125.0 ], [ 2006625.0, 326375.0 ], [ 2006875.0, 326375.0 ], [ 2007125.0, 326375.0 ], [ 2007375.0, 326375.0 ], [ 2007625.0, 326375.0 ], [ 2007875.0, 326375.0 ], [ 2008125.0, 326375.0 ], [ 2008375.0, 326375.0 ], [ 2008375.0, 326125.0 ], [ 2008625.0, 326125.0 ], [ 2008875.0, 326125.0 ], [ 2008875.0, 325875.0 ], [ 2009125.0, 325875.0 ], [ 2009375.0, 325875.0 ], [ 2009625.0, 325875.0 ], [ 2009625.0, 326125.0 ], [ 2009875.0, 326125.0 ], [ 2010125.0, 326125.0 ], [ 2010125.0, 325875.0 ], [ 2010375.0, 325875.0 ], [ 2010375.0, 326125.0 ], [ 2010625.0, 326125.0 ], [ 2010875.0, 326125.0 ], [ 2011125.0, 326125.0 ], [ 2011125.0, 326375.0 ], [ 2011375.0, 326375.0 ], [ 2011375.0, 326625.0 ], [ 2011625.0, 326625.0 ], [ 2011875.0, 326625.0 ], [ 2011875.0, 326875.0 ], [ 2012125.0, 326875.0 ], [ 2012125.0, 327125.0 ], [ 2012375.0, 327125.0 ], [ 2012375.0, 327375.0 ], [ 2012625.0, 327375.0 ], [ 2012875.0, 327375.0 ], [ 2013125.0, 327375.0 ], [ 2013375.0, 327375.0 ], [ 2013625.0, 327375.0 ], [ 2013875.0, 327375.0 ], [ 2014125.0, 327375.0 ], [ 2014125.0, 327125.0 ], [ 2014375.0, 327125.0 ], [ 2014625.0, 327125.0 ], [ 2014625.0, 326875.0 ], [ 2014875.0, 326875.0 ], [ 2015125.0, 326875.0 ], [ 2015375.0, 326875.0 ], [ 2015625.0, 326875.0 ], [ 2015625.0, 326625.0 ], [ 2015875.0, 326625.0 ], [ 2015875.0, 326375.0 ], [ 2016125.0, 326375.0 ], [ 2016375.0, 326375.0 ], [ 2016625.0, 326375.0 ], [ 2016875.0, 326375.0 ], [ 2016875.0, 326625.0 ], [ 2016625.0, 326625.0 ], [ 2016625.0, 326875.0 ], [ 2016375.0, 326875.0 ], [ 2016125.0, 326875.0 ], [ 2015875.0, 326875.0 ], [ 2015875.0, 327125.0 ], [ 2015875.0, 327375.0 ], [ 2015875.0, 327625.0 ], [ 2015625.0, 327625.0 ], [ 2015625.0, 327875.0 ], [ 2015875.0, 327875.0 ], [ 2016125.0, 327875.0 ], [ 2016375.0, 327875.0 ], [ 2016375.0, 328125.0 ], [ 2016375.0, 328375.0 ], [ 2016125.0, 328375.0 ], [ 2016125.0, 328125.0 ], [ 2015875.0, 328125.0 ], [ 2015625.0, 328125.0 ], [ 2015375.0, 328125.0 ], [ 2015125.0, 328125.0 ], [ 2015125.0, 328375.0 ], [ 2015375.0, 328375.0 ], [ 2015375.0, 328625.0 ], [ 2015625.0, 328625.0 ], [ 2015875.0, 328625.0 ], [ 2015875.0, 328875.0 ], [ 2016125.0, 328875.0 ], [ 2016125.0, 329125.0 ], [ 2016125.0, 329375.0 ], [ 2016125.0, 329625.0 ], [ 2016375.0, 329625.0 ], [ 2016625.0, 329625.0 ], [ 2016625.0, 329375.0 ], [ 2016875.0, 329375.0 ], [ 2017125.0, 329375.0 ], [ 2017125.0, 329125.0 ], [ 2017375.0, 329125.0 ], [ 2017625.0, 329125.0 ], [ 2017625.0, 328875.0 ], [ 2017875.0, 328875.0 ], [ 2017875.0, 328625.0 ], [ 2017875.0, 328375.0 ], [ 2018125.0, 328375.0 ], [ 2018125.0, 328125.0 ], [ 2018375.0, 328125.0 ], [ 2018375.0, 328375.0 ], [ 2018375.0, 328625.0 ], [ 2018125.0, 328625.0 ], [ 2018125.0, 328875.0 ], [ 2018375.0, 328875.0 ], [ 2018375.0, 329125.0 ], [ 2018375.0, 329375.0 ], [ 2018375.0, 329625.0 ], [ 2018625.0, 329625.0 ], [ 2018625.0, 329875.0 ], [ 2018875.0, 329875.0 ], [ 2018875.0, 330125.0 ], [ 2018875.0, 330375.0 ], [ 2019125.0, 330375.0 ], [ 2019125.0, 330625.0 ], [ 2019375.0, 330625.0 ], [ 2019375.0, 330875.0 ], [ 2019375.0, 331125.0 ], [ 2019625.0, 331125.0 ], [ 2019875.0, 331125.0 ], [ 2019875.0, 331375.0 ], [ 2019875.0, 331625.0 ], [ 2019875.0, 331875.0 ], [ 2020125.0, 331875.0 ], [ 2020375.0, 331875.0 ], [ 2020375.0, 332125.0 ], [ 2020625.0, 332125.0 ], [ 2020625.0, 332375.0 ], [ 2020875.0, 332375.0 ], [ 2020875.0, 332625.0 ], [ 2021125.0, 332625.0 ], [ 2021125.0, 332875.0 ], [ 2021375.0, 332875.0 ], [ 2021625.0, 332875.0 ], [ 2021625.0, 333125.0 ], [ 2021875.0, 333125.0 ], [ 2021875.0, 333375.0 ], [ 2022125.0, 333375.0 ], [ 2022375.0, 333375.0 ], [ 2022375.0, 333625.0 ], [ 2022625.0, 333625.0 ], [ 2022625.0, 333875.0 ], [ 2022875.0, 333875.0 ], [ 2022875.0, 334125.0 ], [ 2023125.0, 334125.0 ], [ 2023125.0, 334375.0 ], [ 2023375.0, 334375.0 ], [ 2023625.0, 334375.0 ], [ 2023625.0, 334625.0 ], [ 2023625.0, 334875.0 ], [ 2023875.0, 334875.0 ], [ 2024125.0, 334875.0 ], [ 2024125.0, 334625.0 ], [ 2024375.0, 334625.0 ], [ 2024625.0, 334625.0 ], [ 2024875.0, 334625.0 ], [ 2024875.0, 334875.0 ], [ 2025125.0, 334875.0 ], [ 2025125.0, 335125.0 ], [ 2025375.0, 335125.0 ], [ 2025625.0, 335125.0 ], [ 2025625.0, 335375.0 ], [ 2025875.0, 335375.0 ], [ 2026125.0, 335375.0 ], [ 2026125.0, 335625.0 ], [ 2026375.0, 335625.0 ], [ 2026625.0, 335625.0 ], [ 2026875.0, 335625.0 ], [ 2026875.0, 335875.0 ], [ 2027125.0, 335875.0 ], [ 2027125.0, 336125.0 ], [ 2027375.0, 336125.0 ], [ 2027625.0, 336125.0 ], [ 2027625.0, 336375.0 ], [ 2027625.0, 336625.0 ], [ 2027875.0, 336625.0 ], [ 2027875.0, 336375.0 ], [ 2028125.0, 336375.0 ], [ 2028375.0, 336375.0 ], [ 2028375.0, 336625.0 ], [ 2028625.0, 336625.0 ], [ 2028875.0, 336625.0 ], [ 2028875.0, 336875.0 ], [ 2029125.0, 336875.0 ], [ 2029375.0, 336875.0 ], [ 2029375.0, 337125.0 ], [ 2029625.0, 337125.0 ], [ 2029625.0, 337375.0 ], [ 2029875.0, 337375.0 ], [ 2030125.0, 337375.0 ], [ 2030125.0, 337625.0 ], [ 2030125.0, 337875.0 ], [ 2030375.0, 337875.0 ], [ 2030625.0, 337875.0 ], [ 2030625.0, 338125.0 ], [ 2030625.0, 338375.0 ], [ 2030625.0, 338625.0 ], [ 2030875.0, 338625.0 ], [ 2031125.0, 338625.0 ], [ 2031125.0, 338375.0 ], [ 2031125.0, 338125.0 ], [ 2031375.0, 338125.0 ], [ 2031375.0, 338375.0 ], [ 2031625.0, 338375.0 ], [ 2031875.0, 338375.0 ], [ 2031875.0, 338625.0 ], [ 2031625.0, 338625.0 ], [ 2031625.0, 338875.0 ], [ 2031875.0, 338875.0 ], [ 2031875.0, 339125.0 ], [ 2032125.0, 339125.0 ], [ 2032125.0, 338875.0 ], [ 2032125.0, 338625.0 ], [ 2032375.0, 338625.0 ], [ 2032375.0, 338875.0 ], [ 2032625.0, 338875.0 ], [ 2032875.0, 338875.0 ], [ 2033125.0, 338875.0 ], [ 2033125.0, 339125.0 ], [ 2033375.0, 339125.0 ], [ 2033375.0, 339375.0 ], [ 2033625.0, 339375.0 ], [ 2033625.0, 339625.0 ], [ 2033875.0, 339625.0 ], [ 2033875.0, 339875.0 ], [ 2034125.0, 339875.0 ], [ 2034125.0, 340125.0 ], [ 2034375.0, 340125.0 ], [ 2034375.0, 340375.0 ], [ 2034625.0, 340375.0 ], [ 2034625.0, 340625.0 ], [ 2034625.0, 340875.0 ], [ 2034875.0, 340875.0 ], [ 2034875.0, 341125.0 ], [ 2034875.0, 341375.0 ], [ 2035125.0, 341375.0 ], [ 2035125.0, 341625.0 ], [ 2035375.0, 341625.0 ], [ 2035375.0, 341875.0 ], [ 2035625.0, 341875.0 ], [ 2035625.0, 342125.0 ], [ 2035625.0, 342375.0 ], [ 2035875.0, 342375.0 ], [ 2035875.0, 342625.0 ], [ 2035875.0, 342875.0 ], [ 2035875.0, 343125.0 ], [ 2035875.0, 343375.0 ], [ 2036125.0, 343375.0 ], [ 2036125.0, 343625.0 ], [ 2036125.0, 343875.0 ], [ 2036125.0, 344125.0 ], [ 2036125.0, 344375.0 ], [ 2036125.0, 344625.0 ], [ 2036125.0, 344875.0 ], [ 2035875.0, 344875.0 ], [ 2035875.0, 345125.0 ], [ 2036125.0, 345125.0 ], [ 2036125.0, 345375.0 ], [ 2036125.0, 345625.0 ], [ 2036375.0, 345625.0 ], [ 2036375.0, 345875.0 ], [ 2036625.0, 345875.0 ], [ 2036625.0, 346125.0 ], [ 2036375.0, 346125.0 ], [ 2036375.0, 346375.0 ], [ 2036375.0, 346625.0 ], [ 2036375.0, 346875.0 ], [ 2036625.0, 346875.0 ], [ 2036625.0, 347125.0 ], [ 2036375.0, 347125.0 ], [ 2036375.0, 347375.0 ], [ 2036375.0, 347625.0 ], [ 2036625.0, 347625.0 ], [ 2036625.0, 347875.0 ], [ 2036625.0, 348125.0 ], [ 2036625.0, 348375.0 ], [ 2036625.0, 348625.0 ], [ 2036625.0, 348875.0 ], [ 2036875.0, 348875.0 ], [ 2037125.0, 348875.0 ], [ 2037375.0, 348875.0 ], [ 2037625.0, 348875.0 ], [ 2037625.0, 348625.0 ], [ 2037875.0, 348625.0 ], [ 2038125.0, 348625.0 ], [ 2038125.0, 348875.0 ], [ 2038375.0, 348875.0 ], [ 2038375.0, 349125.0 ], [ 2038625.0, 349125.0 ], [ 2038875.0, 349125.0 ], [ 2038875.0, 349375.0 ], [ 2039125.0, 349375.0 ], [ 2039375.0, 349375.0 ], [ 2039375.0, 349625.0 ], [ 2039625.0, 349625.0 ], [ 2039875.0, 349625.0 ], [ 2040125.0, 349625.0 ], [ 2040375.0, 349625.0 ], [ 2040625.0, 349625.0 ], [ 2040875.0, 349625.0 ], [ 2040875.0, 349875.0 ], [ 2041125.0, 349875.0 ], [ 2041375.0, 349875.0 ], [ 2041625.0, 349875.0 ], [ 2041625.0, 349625.0 ], [ 2041875.0, 349625.0 ], [ 2041875.0, 349875.0 ], [ 2042125.0, 349875.0 ], [ 2042375.0, 349875.0 ], [ 2042625.0, 349875.0 ], [ 2042875.0, 349875.0 ], [ 2043125.0, 349875.0 ], [ 2043375.0, 349875.0 ], [ 2043625.0, 349875.0 ], [ 2043625.0, 349625.0 ], [ 2043875.0, 349625.0 ], [ 2043875.0, 349375.0 ], [ 2044125.0, 349375.0 ], [ 2044125.0, 349125.0 ], [ 2044125.0, 348875.0 ], [ 2044375.0, 348875.0 ], [ 2044375.0, 348625.0 ], [ 2044625.0, 348625.0 ], [ 2044625.0, 348375.0 ], [ 2044875.0, 348375.0 ], [ 2044875.0, 348125.0 ], [ 2045125.0, 348125.0 ], [ 2045375.0, 348125.0 ], [ 2045392.826774591812864, 348142.826774591754656 ], [ 2045556.744589085225016, 348117.718749680614565 ], [ 2045704.146632959367707, 348188.635653424018528 ], [ 2045703.53, 348189.957 ], [ 2045683.655, 348230.872199999983422 ], [ 2045638.91, 348320.3714 ], [ 2045601.113818968413398, 348404.300741842540447 ], [ 2045875.0, 348438.28639834333444 ], [ 2045875.0, 348625.0 ], [ 2046125.0, 348625.0 ], [ 2046375.0, 348625.0 ], [ 2046625.0, 348625.0 ], [ 2046875.0, 348625.0 ], [ 2047125.0, 348625.0 ], [ 2047375.0, 348625.0 ], [ 2047625.0, 348625.0 ], [ 2047625.0, 348875.0 ], [ 2047875.0, 348875.0 ], [ 2047875.0, 349125.0 ], [ 2048125.0, 349125.0 ], [ 2048375.0, 349125.0 ], [ 2048375.0, 349375.0 ], [ 2048625.0, 349375.0 ], [ 2048625.0, 349625.0 ], [ 2048875.0, 349625.0 ], [ 2048875.0, 349375.0 ], [ 2048875.0, 349125.0 ], [ 2048625.0, 349125.0 ], [ 2048625.0, 348875.0 ], [ 2048375.0, 348875.0 ], [ 2048375.0, 348625.0 ], [ 2048125.0, 348625.0 ], [ 2048125.0, 348375.0 ], [ 2047875.0, 348375.0 ], [ 2047625.0, 348375.0 ], [ 2047625.0, 348125.0 ], [ 2047375.0, 348125.0 ], [ 2047125.0, 348125.0 ], [ 2046875.0, 348125.0 ], [ 2046625.0, 348125.0 ], [ 2046375.0, 348125.0 ], [ 2046125.0, 348125.0 ], [ 2046089.87689806940034, 348089.876898069458548 ], [ 2046133.482221366371959, 347904.488804459862877 ], [ 2045958.136711895698681, 347764.27390489471145 ], [ 2046003.847, 347713.34529999998631 ], [ 2046072.674000000115484, 347635.247199999983422 ], [ 2046075.053, 347630.305500000016764 ], [ 2046122.245087900198996, 347577.973078029346652 ], [ 2046139.892, 347558.403999999980442 ], [ 2046207.156999999890104, 347483.392400000011548 ], [ 2046274.705, 347407.8383 ], [ 2046287.932678144657984, 347392.772025281796232 ], [ 2046341.25, 347332.043799999984913 ], [ 2046411.049000000115484, 347255.744200000015553 ], [ 2046457.673041422618553, 347211.46309378487058 ], [ 2046485.556, 347184.981299999984913 ], [ 2046565.708000000100583, 347123.543899999989662 ], [ 2046651.968000000109896, 347068.3297 ], [ 2046660.080679264385253, 347063.249455272394698 ], [ 2046738.525, 347014.126800000027288 ], [ 2046825.058, 346961.364500000025146 ], [ 2046869.245368028059602, 346934.381285698153079 ], [ 2046911.172, 346908.778600000019651 ], [ 2046998.466, 346858.839700000011362 ], [ 2047074.832631461787969, 346796.353584631113335 ], [ 2047077.102, 346794.496700000017881 ], [ 2047131.55, 346710.1251 ], [ 2047135.395, 346703.76510000001872 ], [ 2047135.838, 346703.745500000019092 ], [ 2047138.368, 346696.5062 ], [ 2047154.071, 346669.8028 ], [ 2047159.066000000108033, 346664.3712 ], [ 2047206.933, 346605.079900000011548 ], [ 2047221.362432709662244, 346597.393960972491186 ], [ 2047376.290053668897599, 346783.877958185737953 ], [ 2047375.0, 346884.71753743494628 ], [ 2047625.0, 346875.0 ], [ 2047625.0, 347125.0 ], [ 2047875.0, 347125.0 ], [ 2048125.0, 347125.0 ], [ 2048125.0, 346875.0 ], [ 2048125.0, 346674.451247457182035 ], [ 2048169.136621022131294, 346466.638890414615162 ], [ 2048255.281999999890104, 346487.695499999972526 ], [ 2048354.399, 346511.208600000012666 ], [ 2048410.55518839834258, 346525.293504228000529 ], [ 2048451.509, 346535.565400000021327 ], [ 2048548.051, 346568.491900000022724 ], [ 2048643.018, 346606.93 ], [ 2048644.829203597269952, 346607.831182389636524 ], [ 2048734.287, 346652.3418 ], [ 2048824.444, 346701.8885 ], [ 2048862.720127717126161, 346726.940490917710122 ], [ 2048908.692, 346757.0294 ], [ 2048988.824, 346817.044600000022911 ], [ 2049059.556004673242569, 346878.25982995639788 ], [ 2049067.213, 346884.886600000027101 ], [ 2049143.874, 346950.678699999989476 ], [ 2049164.018, 346969.060999999986961 ], [ 2049246.389999999897555, 347028.6629 ], [ 2049253.963706719689071, 347032.600758539338131 ], [ 2049335.862, 347075.1828 ], [ 2049429.828, 347112.933499999984633 ], [ 2049481.395841051125899, 347132.290066618821584 ], [ 2049525.551, 347148.864200000010896 ], [ 2049620.142, 347187.714800000016112 ], [ 2049711.574092950439081, 347225.890785001625773 ], [ 2049713.02, 347226.494499999971595 ], [ 2049806.409, 347264.1264 ], [ 2049812.534, 347266.604899999976624 ], [ 2049899.589, 347301.8296 ], [ 2049942.999113350640982, 347316.274769303330686 ], [ 2049996.120000000111759, 347333.951300000015181 ], [ 2050093.68, 347358.6692 ], [ 2050184.511057119816542, 347373.972390304203145 ], [ 2050194.082, 347375.584900000016205 ], [ 2050295.031, 347380.165700000012293 ], [ 2050395.870000000111759, 347379.972400000027847 ], [ 2050432.790745197795331, 347381.637759556586388 ], [ 2050606.428, 347389.469900000025518 ], [ 2050680.816848023561761, 347396.403189528442454 ], [ 2050693.102999999886379, 347397.548300000024028 ], [ 2050792.656, 347418.07909999997355 ], [ 2050891.608, 347444.3001 ], [ 2050921.034117232076824, 347452.326108012755867 ], [ 2050955.508, 347461.728899999987334 ], [ 2050989.633, 347471.4816 ], [ 2051088.202, 347496.375 ], [ 2051162.892246897798032, 347517.011693853070028 ], [ 2051185.308, 347523.205100000021048 ], [ 2051252.842, 347541.0538 ], [ 2051284.082, 347549.719700000016019 ], [ 2051381.33, 347575.379000000015367 ], [ 2051403.036967576947063, 347581.186219056078698 ], [ 2051479.602, 347601.669500000018161 ], [ 2051577.727, 347627.338800000026822 ], [ 2051642.329075674060732, 347647.870469470217358 ], [ 2051674.817, 347658.195699999982025 ], [ 2051764.92, 347702.402 ], [ 2051855.0, 347751.324499999987893 ], [ 2051864.872799330158159, 347757.811560854723211 ], [ 2051781.506151334848255, 347891.426020058279391 ], [ 2051625.0, 347935.163654816860799 ], [ 2051625.0, 348125.0 ], [ 2051625.0, 348375.0 ], [ 2051375.0, 348375.0 ], [ 2051125.0, 348375.0 ], [ 2051125.0, 348625.0 ], [ 2051125.0, 348875.0 ], [ 2051125.0, 349125.0 ], [ 2051125.0, 349375.0 ], [ 2051375.0, 349375.0 ], [ 2051625.0, 349375.0 ], [ 2051875.0, 349375.0 ], [ 2052125.0, 349375.0 ], [ 2052125.0, 349625.0 ], [ 2052375.0, 349625.0 ], [ 2052375.0, 349875.0 ], [ 2052375.0, 350125.0 ], [ 2052375.0, 350375.0 ], [ 2052125.0, 350375.0 ], [ 2051875.0, 350375.0 ], [ 2051625.0, 350375.0 ], [ 2051375.0, 350375.0 ], [ 2051125.0, 350375.0 ], [ 2051125.0, 350625.0 ], [ 2051125.0, 350875.0 ], [ 2050875.0, 350875.0 ], [ 2050875.0, 351125.0 ], [ 2050875.0, 351375.0 ], [ 2051125.0, 351375.0 ], [ 2051125.0, 351625.0 ], [ 2051125.0, 351875.0 ], [ 2051125.0, 352125.0 ], [ 2051375.0, 352125.0 ], [ 2051625.0, 352125.0 ], [ 2051625.0, 352375.0 ], [ 2051875.0, 352375.0 ], [ 2052125.0, 352375.0 ], [ 2052375.0, 352375.0 ], [ 2052625.0, 352375.0 ], [ 2052875.0, 352375.0 ], [ 2053125.0, 352375.0 ], [ 2053125.0, 352125.0 ], [ 2053375.0, 352125.0 ], [ 2053375.0, 352375.0 ], [ 2053375.0, 352625.0 ], [ 2053625.0, 352625.0 ], [ 2053875.0, 352625.0 ], [ 2054125.0, 352625.0 ], [ 2054375.0, 352625.0 ], [ 2054625.0, 352625.0 ], [ 2054625.0, 352375.0 ], [ 2054625.0, 352125.0 ], [ 2054625.0, 351875.0 ], [ 2054625.0, 351625.0 ], [ 2054625.0, 351375.0 ], [ 2054375.0, 351375.0 ], [ 2054375.0, 351125.0 ], [ 2054625.0, 351125.0 ], [ 2054625.0, 350875.0 ], [ 2054875.0, 350875.0 ], [ 2054875.0, 351125.0 ], [ 2054875.0, 351375.0 ], [ 2054875.0, 351625.0 ], [ 2055125.0, 351625.0 ], [ 2055375.0, 351625.0 ], [ 2055625.0, 351625.0 ], [ 2055875.0, 351625.0 ], [ 2056125.0, 351625.0 ], [ 2056125.0, 351375.0 ], [ 2056350.599363238317892, 351375.0 ], [ 2056298.700922008370981, 351060.282652019988745 ], [ 2056325.352999999886379, 351070.809300000022631 ], [ 2056420.75, 351103.553599999984726 ], [ 2056519.756, 351131.548799999989569 ], [ 2056531.340559467673302, 351135.295387445541564 ], [ 2056615.531, 351162.523600000014994 ], [ 2056713.584, 351191.837999999988824 ], [ 2056765.632664176868275, 351207.582327997835819 ], [ 2056792.118906509829685, 351118.623203010298312 ], [ 2056875.0, 351003.812603708007373 ], [ 2056875.0, 350875.0 ], [ 2057125.0, 350875.0 ], [ 2057375.0, 350875.0 ], [ 2057625.0, 350875.0 ], [ 2057875.0, 350875.0 ], [ 2058125.0, 350875.0 ], [ 2058375.0, 350875.0 ], [ 2058625.0, 350875.0 ], [ 2058875.0, 350875.0 ], [ 2059125.0, 350875.0 ], [ 2059125.0, 351125.0 ], [ 2059375.0, 351125.0 ], [ 2059375.0, 350875.0 ], [ 2059625.0, 350875.0 ], [ 2059875.0, 350875.0 ], [ 2060125.0, 350875.0 ], [ 2060375.0, 350875.0 ], [ 2060625.0, 350875.0 ], [ 2060875.0, 350875.0 ], [ 2061125.0, 350875.0 ], [ 2061375.0, 350875.0 ], [ 2061625.0, 350875.0 ], [ 2061625.0, 351125.0 ], [ 2061875.0, 351125.0 ], [ 2061875.0, 351375.0 ], [ 2062125.0, 351375.0 ], [ 2062125.0, 351625.0 ], [ 2062375.0, 351625.0 ], [ 2062375.0, 351875.0 ], [ 2062625.0, 351875.0 ], [ 2062625.0, 352125.0 ], [ 2062875.0, 352125.0 ], [ 2062875.0, 352375.0 ], [ 2062875.0, 352625.0 ], [ 2062625.0, 352625.0 ], [ 2062625.0, 352875.0 ], [ 2062375.0, 352875.0 ], [ 2062125.0, 352875.0 ], [ 2061875.0, 352875.0 ], [ 2061625.0, 352875.0 ], [ 2061625.0, 352625.0 ], [ 2061375.0, 352625.0 ], [ 2061125.0, 352625.0 ], [ 2060875.0, 352625.0 ], [ 2060875.0, 352875.0 ], [ 2061125.0, 352875.0 ], [ 2061125.0, 353125.0 ], [ 2061125.0, 353375.0 ], [ 2060936.052242371952161, 353375.0 ], [ 2060859.709153409814462, 353594.106941066042054 ], [ 2060721.401, 353583.945469315221999 ], [ 2060721.401, 353603.5735 ], [ 2060689.642, 353702.8212 ], [ 2060634.063, 353788.174300000013318 ], [ 2060619.980225313687697, 353803.537643563351594 ], [ 2060612.229, 353811.9937 ], [ 2060518.936, 353873.527300000016112 ], [ 2060405.793, 353909.256500000017695 ], [ 2060400.958919165888801, 353910.302681645960547 ], [ 2060464.563876286381856, 354093.325358352158219 ], [ 2060375.0, 354183.159451701270882 ], [ 2060229.512232922716066, 354139.291739791049622 ], [ 2060086.491612219018862, 354299.844311335938983 ], [ 2059948.375940116588026, 354084.718807010853197 ], [ 2059867.871, 354141.4962 ], [ 2059770.608, 354210.969600000011269 ], [ 2059748.387329578632489, 354227.130101456714328 ], [ 2059683.27, 354274.488200000021607 ], [ 2059591.962, 354324.112000000022817 ], [ 2059531.247434145305306, 354340.55549108015839 ], [ 2059479.528306855354458, 354115.575721094384789 ], [ 2059375.0, 354072.547782423382159 ], [ 2059302.939927732106298, 354127.833226935239509 ], [ 2059125.0, 354064.438373746932484 ], [ 2059067.533013119595125, 354118.25090581382392 ], [ 2058875.0, 354043.578457493975293 ], [ 2058821.234719607280567, 354106.711431483156048 ], [ 2058625.0, 354019.287746681598946 ], [ 2058573.495666530216113, 354099.253049300634302 ], [ 2058343.532893176889047, 353971.947776140645146 ], [ 2058333.412036124849692, 353916.587963875208516 ], [ 2058125.0, 353895.644056870019995 ], [ 2057875.0, 353864.916898941854015 ], [ 2057632.892756410408765, 353845.583374651672784 ], [ 2057625.0, 353832.339210560137872 ], [ 2057389.012500252341852, 353821.701727379695512 ], [ 2057375.0, 353785.807741610682569 ], [ 2057139.27351446961984, 353807.841307688562665 ], [ 2057114.510077364509925, 353625.0 ], [ 2056916.989277934422716, 353646.347393477743026 ], [ 2056875.0, 353603.90383220568765 ], [ 2056658.175532867899165, 353622.710814676713198 ], [ 2056625.0, 353581.976500147837214 ], [ 2056402.627314012264833, 353604.969436102895997 ], [ 2056375.0, 353565.189082252269145 ], [ 2056146.247539822710678, 353594.238262819766533 ], [ 2056125.0, 353559.748845178051852 ], [ 2055875.0, 353574.024443101196084 ], [ 2055625.0, 353585.932005941634998 ], [ 2055375.0, 353601.183635311434045 ], [ 2055125.0, 353616.0711170466966 ], [ 2054875.0, 353631.366473019646946 ], [ 2054649.23748409259133, 353649.237484092533123 ], [ 2054640.49694138020277, 353726.582696879399009 ], [ 2054405.525373977608979, 353829.287263249338139 ], [ 2054375.0, 353761.103910181613173 ], [ 2054157.475470915203914, 353836.179934816900641 ], [ 2054125.0, 353783.61728427791968 ], [ 2053910.105774469673634, 353847.911656951182522 ], [ 2053875.0, 353803.9519838662236 ], [ 2053663.131689874920994, 353861.772455535479821 ], [ 2053625.0, 353823.244815596495755 ], [ 2053406.365308852866292, 353879.118521186290309 ], [ 2053375.0, 353853.129190048901364 ], [ 2053146.790609032381326, 353896.790609032323118 ], [ 2053148.391622352646664, 353994.126779674785212 ], [ 2052904.208318482618779, 354094.469294218695723 ], [ 2052875.0, 354069.359012868138961 ], [ 2052673.265417135553434, 354143.942089368880261 ], [ 2052613.944667972391471, 354125.0 ], [ 2052534.696291639702395, 354454.267284148954786 ], [ 2052463.99, 354472.983599999977741 ], [ 2052296.063303152332082, 354506.857195468794089 ], [ 2052054.392753534484655, 354555.606147173035424 ], [ 2052001.495000000111759, 354566.276499999978114 ], [ 2051813.08432263857685, 354606.194154093100224 ], [ 2051767.271, 354615.900399999984074 ], [ 2051600.534, 354653.614500000025146 ], [ 2051572.818477610126138, 354661.120849128928967 ], [ 2051505.257, 354679.418899999989662 ], [ 2051431.813, 354703.238399999972899 ], [ 2051339.851474932162091, 354741.190803439472802 ], [ 2051306.761, 354754.847200000018347 ], [ 2051211.483, 354798.516200000012759 ], [ 2051119.563230130355805, 354846.244091359432787 ], [ 2051108.266, 354852.10999999998603 ], [ 2050977.259, 354939.447999999974854 ], [ 2050910.884441662346944, 354980.747683905356098 ], [ 2050887.936, 354995.026699999987613 ], [ 2050836.327, 355024.800999999977648 ], [ 2050747.004, 355060.530199999979232 ], [ 2050723.183999999891967, 355066.4851 ], [ 2050686.458111064741388, 355073.265250973519869 ], [ 2050625.0, 355382.912428987619933 ], [ 2050875.0, 355375.0 ], [ 2050875.0, 355625.0 ], [ 2050875.0, 355875.0 ], [ 2051125.0, 355875.0 ], [ 2051375.0, 355875.0 ], [ 2051625.0, 355875.0 ], [ 2051625.0, 355625.0 ], [ 2051875.0, 355625.0 ], [ 2052125.0, 355625.0 ], [ 2052375.0, 355625.0 ], [ 2052375.0, 355375.0 ], [ 2052625.0, 355375.0 ], [ 2052790.338085040450096, 355540.338085040508304 ], [ 2052875.0, 355487.659560176252853 ], [ 2052875.0, 355125.0 ], [ 2053125.0, 355125.0 ], [ 2053375.0, 355125.0 ], [ 2053625.0, 355125.0 ], [ 2053875.0, 355125.0 ], [ 2053875.0, 354875.0 ], [ 2054125.0, 354875.0 ], [ 2054375.0, 354875.0 ], [ 2054625.0, 354875.0 ], [ 2054875.0, 354875.0 ], [ 2054875.0, 354625.0 ], [ 2055125.0, 354625.0 ], [ 2055375.0, 354625.0 ], [ 2055625.0, 354625.0 ], [ 2055625.0, 354875.0 ], [ 2055875.0, 354875.0 ], [ 2056125.0, 354875.0 ], [ 2056375.0, 354875.0 ], [ 2056625.0, 354875.0 ], [ 2056875.0, 354875.0 ], [ 2057125.0, 354875.0 ], [ 2057375.0, 354875.0 ], [ 2057625.0, 354875.0 ], [ 2057875.0, 354875.0 ], [ 2058125.0, 354875.0 ], [ 2058125.0, 355125.0 ], [ 2058375.0, 355125.0 ], [ 2058625.0, 355125.0 ], [ 2058875.0, 355125.0 ], [ 2059125.0, 355125.0 ], [ 2059125.0, 355375.0 ], [ 2059375.0, 355375.0 ], [ 2059625.0, 355375.0 ], [ 2059875.0, 355375.0 ], [ 2059875.0, 355625.0 ], [ 2060125.0, 355625.0 ], [ 2060375.0, 355625.0 ], [ 2060375.0, 355875.0 ], [ 2060375.0, 356125.0 ], [ 2060625.0, 356125.0 ], [ 2060625.0, 356375.0 ], [ 2060625.0, 356625.0 ], [ 2060875.0, 356625.0 ], [ 2060875.0, 356875.0 ], [ 2061125.0, 356875.0 ], [ 2061125.0, 357125.0 ], [ 2061375.0, 357125.0 ], [ 2061375.0, 357375.0 ], [ 2061625.0, 357375.0 ], [ 2061875.0, 357375.0 ], [ 2062125.0, 357375.0 ], [ 2062375.0, 357375.0 ], [ 2062625.0, 357375.0 ], [ 2062625.0, 357125.0 ], [ 2062875.0, 357125.0 ], [ 2063125.0, 357125.0 ], [ 2063375.0, 357125.0 ], [ 2063375.0, 356875.0 ], [ 2063625.0, 356875.0 ], [ 2063875.0, 356875.0 ], [ 2064125.0, 356875.0 ], [ 2064375.0, 356875.0 ], [ 2064625.0, 356875.0 ], [ 2064875.0, 356875.0 ], [ 2065125.0, 356875.0 ], [ 2065125.0, 356625.0 ], [ 2065375.0, 356625.0 ], [ 2065625.0, 356625.0 ], [ 2065625.0, 356375.0 ], [ 2065875.0, 356375.0 ], [ 2065875.0, 356125.0 ], [ 2066125.0, 356125.0 ], [ 2066125.0, 356375.0 ], [ 2066125.0, 356625.0 ], [ 2065875.0, 356625.0 ], [ 2065875.0, 356875.0 ], [ 2065875.0, 357125.0 ], [ 2066125.0, 357125.0 ], [ 2066125.0, 357375.0 ], [ 2066125.0, 357625.0 ], [ 2066375.0, 357625.0 ], [ 2066375.0, 357875.0 ], [ 2066375.0, 358125.0 ], [ 2066625.0, 358125.0 ], [ 2066625.0, 358375.0 ], [ 2066625.0, 358625.0 ], [ 2066875.0, 358625.0 ], [ 2066875.0, 358875.0 ], [ 2066875.0, 359125.0 ], [ 2067125.0, 359125.0 ], [ 2067125.0, 359375.0 ], [ 2067375.0, 359375.0 ], [ 2067625.0, 359375.0 ], [ 2067625.0, 359625.0 ], [ 2067625.0, 359875.0 ], [ 2067875.0, 359875.0 ], [ 2067875.0, 360125.0 ], [ 2068125.0, 360125.0 ], [ 2068375.0, 360125.0 ], [ 2068375.0, 360375.0 ], [ 2068625.0, 360375.0 ], [ 2068625.0, 360125.0 ], [ 2068875.0, 360125.0 ], [ 2069125.0, 360125.0 ], [ 2069125.0, 359875.0 ], [ 2069375.0, 359875.0 ], [ 2069375.0, 359625.0 ], [ 2069625.0, 359625.0 ], [ 2069875.0, 359625.0 ], [ 2070125.0, 359625.0 ], [ 2070125.0, 359875.0 ], [ 2070375.0, 359875.0 ], [ 2070625.0, 359875.0 ], [ 2070625.0, 360125.0 ], [ 2070625.0, 360375.0 ], [ 2070625.0, 360625.0 ], [ 2070625.0, 360875.0 ], [ 2070875.0, 360875.0 ], [ 2070875.0, 361125.0 ], [ 2070875.0, 361375.0 ], [ 2070875.0, 361625.0 ], [ 2071125.0, 361625.0 ], [ 2071125.0, 361875.0 ], [ 2071125.0, 362125.0 ], [ 2071375.0, 362125.0 ], [ 2071375.0, 362375.0 ], [ 2071625.0, 362375.0 ], [ 2071625.0, 362625.0 ], [ 2071625.0, 362875.0 ], [ 2071875.0, 362875.0 ], [ 2071875.0, 363125.0 ], [ 2072125.0, 363125.0 ], [ 2072375.0, 363125.0 ], [ 2072375.0, 363375.0 ], [ 2072375.0, 363625.0 ], [ 2072625.0, 363625.0 ], [ 2072625.0, 363875.0 ], [ 2072625.0, 364125.0 ], [ 2072625.0, 364375.0 ], [ 2072625.0, 364625.0 ], [ 2072875.0, 364625.0 ], [ 2072875.0, 364875.0 ], [ 2073125.0, 364875.0 ], [ 2073375.0, 364875.0 ], [ 2073625.0, 364875.0 ], [ 2073625.0, 365125.0 ], [ 2073875.0, 365125.0 ], [ 2073875.0, 365375.0 ], [ 2073875.0, 365625.0 ], [ 2074125.0, 365625.0 ], [ 2074125.0, 365875.0 ], [ 2074375.0, 365875.0 ], [ 2074625.0, 365875.0 ], [ 2074875.0, 365875.0 ], [ 2075125.0, 365875.0 ], [ 2075125.0, 366125.0 ], [ 2075375.0, 366125.0 ], [ 2075375.0, 366375.0 ], [ 2075625.0, 366375.0 ], [ 2075625.0, 366625.0 ], [ 2075875.0, 366625.0 ], [ 2075875.0, 366875.0 ], [ 2076125.0, 366875.0 ], [ 2076125.0, 367125.0 ], [ 2076375.0, 367125.0 ], [ 2076375.0, 367375.0 ], [ 2076625.0, 367375.0 ], [ 2076625.0, 367625.0 ], [ 2076875.0, 367625.0 ], [ 2077125.0, 367625.0 ], [ 2077375.0, 367625.0 ], [ 2077375.0, 367875.0 ], [ 2077625.0, 367875.0 ], [ 2077875.0, 367875.0 ], [ 2078125.0, 367875.0 ], [ 2078125.0, 368125.0 ], [ 2078375.0, 368125.0 ], [ 2078625.0, 368125.0 ], [ 2078875.0, 368125.0 ], [ 2079125.0, 368125.0 ], [ 2079375.0, 368125.0 ], [ 2079625.0, 368125.0 ], [ 2079875.0, 368125.0 ], [ 2080125.0, 368125.0 ], [ 2080375.0, 368125.0 ], [ 2080625.0, 368125.0 ], [ 2080625.0, 367875.0 ], [ 2080875.0, 367875.0 ], [ 2081125.0, 367875.0 ], [ 2081125.0, 367625.0 ], [ 2081375.0, 367625.0 ], [ 2081625.0, 367625.0 ], [ 2081625.0, 367375.0 ], [ 2081875.0, 367375.0 ], [ 2081875.0, 367125.0 ], [ 2081875.0, 366875.0 ], [ 2082125.0, 366875.0 ], [ 2082375.0, 366875.0 ], [ 2082375.0, 366625.0 ], [ 2082375.0, 366375.0 ], [ 2082375.0, 366125.0 ], [ 2082625.0, 366125.0 ], [ 2082625.0, 365875.0 ], [ 2082625.0, 365625.0 ], [ 2082875.0, 365625.0 ], [ 2082875.0, 365375.0 ], [ 2082875.0, 365125.0 ], [ 2082875.0, 364875.0 ], [ 2083125.0, 364875.0 ], [ 2083125.0, 364625.0 ], [ 2083222.647259352728724, 364527.352740647213068 ], [ 2083189.874681219924241, 364503.586367191979662 ], [ 2082875.0, 364379.293729868950322 ], [ 2082625.0, 364280.609519343124703 ], [ 2082513.430910659488291, 364236.569089340511709 ], [ 2082125.0, 364083.241098291473463 ], [ 2081875.0, 363984.556887765647843 ], [ 2081875.0, 364375.0 ], [ 2081875.0, 364625.0 ], [ 2081625.0, 364625.0 ], [ 2081625.0, 364375.0 ], [ 2081375.0, 364375.0 ], [ 2081125.0, 364375.0 ], [ 2080875.0, 364375.0 ], [ 2080625.0, 364375.0 ], [ 2080375.0, 364375.0 ], [ 2080125.0, 364375.0 ], [ 2079875.0, 364375.0 ], [ 2079875.0, 364625.0 ], [ 2079625.0, 364625.0 ], [ 2079375.0, 364625.0 ], [ 2079125.0, 364625.0 ], [ 2078875.0, 364625.0 ], [ 2078875.0, 364875.0 ], [ 2078625.0, 364875.0 ], [ 2078375.0, 364875.0 ], [ 2078125.0, 364875.0 ], [ 2077875.0, 364875.0 ], [ 2077625.0, 364875.0 ], [ 2077375.0, 364875.0 ], [ 2077125.0, 364875.0 ], [ 2077125.0, 365125.0 ], [ 2077125.0, 365375.0 ], [ 2077125.0, 365625.0 ], [ 2077125.0, 365875.0 ], [ 2077375.0, 365875.0 ], [ 2077625.0, 365875.0 ], [ 2077625.0, 366125.0 ], [ 2077375.0, 366125.0 ], [ 2077125.0, 366125.0 ], [ 2076875.0, 366125.0 ], [ 2076625.0, 366125.0 ], [ 2076625.0, 365875.0 ], [ 2076375.0, 365875.0 ], [ 2076375.0, 365625.0 ], [ 2076125.0, 365625.0 ], [ 2076125.0, 365375.0 ], [ 2075875.0, 365375.0 ], [ 2075875.0, 365125.0 ], [ 2075875.0, 364875.0 ], [ 2075625.0, 364875.0 ], [ 2075375.0, 364875.0 ], [ 2075375.0, 364625.0 ], [ 2075125.0, 364625.0 ], [ 2075125.0, 364375.0 ], [ 2075125.0, 364125.0 ], [ 2074875.0, 364125.0 ], [ 2074875.0, 363875.0 ], [ 2074875.0, 363625.0 ], [ 2074625.0, 363625.0 ], [ 2074625.0, 363375.0 ], [ 2074375.0, 363375.0 ], [ 2074125.0, 363375.0 ], [ 2074125.0, 363125.0 ], [ 2074125.0, 362875.0 ], [ 2073875.0, 362875.0 ], [ 2073875.0, 362625.0 ], [ 2073625.0, 362625.0 ], [ 2073625.0, 362375.0 ], [ 2073375.0, 362375.0 ], [ 2073375.0, 362125.0 ], [ 2073125.0, 362125.0 ], [ 2073125.0, 361875.0 ], [ 2072875.0, 361875.0 ], [ 2072875.0, 361625.0 ], [ 2072625.0, 361625.0 ], [ 2072375.0, 361625.0 ], [ 2072375.0, 361375.0 ], [ 2072125.0, 361375.0 ], [ 2072125.0, 361125.0 ], [ 2072125.0, 360875.0 ], [ 2071875.0, 360875.0 ], [ 2071875.0, 360625.0 ], [ 2071875.0, 360375.0 ], [ 2071875.0, 360125.0 ], [ 2071625.0, 360125.0 ], [ 2071625.0, 359875.0 ], [ 2071625.0, 359625.0 ], [ 2071875.0, 359625.0 ], [ 2071875.0, 359375.0 ], [ 2071625.0, 359375.0 ], [ 2071625.0, 359125.0 ], [ 2071375.0, 359125.0 ], [ 2071375.0, 358875.0 ], [ 2071625.0, 358875.0 ], [ 2071625.0, 358625.0 ], [ 2071625.0, 358375.0 ], [ 2071625.0, 358125.0 ], [ 2071625.0, 357875.0 ], [ 2071875.0, 357875.0 ], [ 2072125.0, 357875.0 ], [ 2072375.0, 357875.0 ], [ 2072375.0, 358125.0 ], [ 2072625.0, 358125.0 ], [ 2072625.0, 357875.0 ], [ 2072875.0, 357875.0 ], [ 2072875.0, 357625.0 ], [ 2072625.0, 357625.0 ], [ 2072625.0, 357375.0 ], [ 2072375.0, 357375.0 ], [ 2072375.0, 357625.0 ], [ 2072125.0, 357625.0 ], [ 2071875.0, 357625.0 ], [ 2071625.0, 357625.0 ], [ 2071375.0, 357625.0 ], [ 2071375.0, 357375.0 ], [ 2071375.0, 357125.0 ], [ 2071375.0, 356875.0 ], [ 2071375.0, 356625.0 ], [ 2071125.0, 356625.0 ], [ 2071125.0, 356375.0 ], [ 2071125.0, 356125.0 ], [ 2070875.0, 356125.0 ], [ 2070875.0, 355875.0 ], [ 2070625.0, 355875.0 ], [ 2070625.0, 355625.0 ], [ 2070375.0, 355625.0 ], [ 2070375.0, 355375.0 ], [ 2070125.0, 355375.0 ], [ 2069875.0, 355375.0 ], [ 2069875.0, 355125.0 ], [ 2069625.0, 355125.0 ], [ 2069625.0, 354875.0 ], [ 2069375.0, 354875.0 ], [ 2069125.0, 354875.0 ], [ 2069125.0, 354625.0 ], [ 2068875.0, 354625.0 ], [ 2068625.0, 354625.0 ], [ 2068625.0, 354375.0 ], [ 2068375.0, 354375.0 ], [ 2068375.0, 354125.0 ], [ 2068125.0, 354125.0 ], [ 2067875.0, 354125.0 ], [ 2067625.0, 354125.0 ], [ 2067625.0, 353875.0 ], [ 2067375.0, 353875.0 ], [ 2067125.0, 353875.0 ], [ 2066875.0, 353875.0 ], [ 2066875.0, 353625.0 ], [ 2066625.0, 353625.0 ], [ 2066375.0, 353625.0 ], [ 2066125.0, 353625.0 ], [ 2066125.0, 353375.0 ], [ 2065875.0, 353375.0 ], [ 2065625.0, 353375.0 ], [ 2065625.0, 353125.0 ], [ 2065375.0, 353125.0 ], [ 2065375.0, 352875.0 ], [ 2065125.0, 352875.0 ], [ 2064875.0, 352875.0 ], [ 2064875.0, 352625.0 ], [ 2064625.0, 352625.0 ], [ 2064375.0, 352625.0 ], [ 2064375.0, 352375.0 ], [ 2064125.0, 352375.0 ], [ 2063875.0, 352375.0 ], [ 2063875.0, 352125.0 ], [ 2063625.0, 352125.0 ], [ 2063625.0, 351875.0 ], [ 2063375.0, 351875.0 ], [ 2063375.0, 351625.0 ], [ 2063125.0, 351625.0 ], [ 2062875.0, 351625.0 ], [ 2062875.0, 351375.0 ], [ 2062875.0, 351125.0 ], [ 2063028.14763583149761, 350971.852364168618806 ], [ 2062716.62931122421287, 350783.37068877578713 ], [ 2062375.0, 350576.670601312071085 ], [ 2062249.351824313402176, 350500.648175686539616 ], [ 2062115.055381180020049, 350419.393184882996138 ], [ 2061875.0, 350295.319617082946934 ], [ 2061762.715215404285118, 350237.284784595656674 ], [ 2061375.0, 350036.89265079254983 ], [ 2061282.16953850002028, 349988.912861703021917 ], [ 2061266.453567439690232, 349983.546432560426183 ], [ 2060875.0, 349849.879360751598142 ], [ 2060625.0, 349764.513507093011867 ], [ 2060520.999021985335276, 349729.000978014722932 ], [ 2060375.0, 349875.0 ], [ 2060125.0, 349875.0 ], [ 2059875.0, 349875.0 ], [ 2059625.0, 349875.0 ], [ 2059375.0, 349875.0 ], [ 2059125.0, 349875.0 ], [ 2058875.0, 349875.0 ], [ 2058875.0, 350125.0 ], [ 2058625.0, 350125.0 ], [ 2058375.0, 350125.0 ], [ 2058375.0, 349875.0 ], [ 2058125.0, 349875.0 ], [ 2057875.0, 349875.0 ], [ 2057875.0, 350125.0 ], [ 2057625.0, 350125.0 ], [ 2057375.0, 350125.0 ], [ 2057375.0, 350375.0 ], [ 2057125.0, 350375.0 ], [ 2056875.0, 350375.0 ], [ 2056625.0, 350375.0 ], [ 2056625.0, 350125.0 ], [ 2056375.0, 350125.0 ], [ 2056125.0, 350125.0 ], [ 2056125.0, 349875.0 ], [ 2056125.0, 349625.0 ], [ 2055875.0, 349625.0 ], [ 2055875.0, 349375.0 ], [ 2055625.0, 349375.0 ], [ 2055625.0, 349125.0 ], [ 2055375.0, 349125.0 ], [ 2055375.0, 348875.0 ], [ 2055125.0, 348875.0 ], [ 2054875.0, 348875.0 ], [ 2054875.0, 348625.0 ], [ 2054625.0, 348625.0 ], [ 2054625.0, 348375.0 ], [ 2054375.0, 348375.0 ], [ 2054375.0, 348125.0 ], [ 2054125.0, 348125.0 ], [ 2053875.0, 348125.0 ], [ 2053625.0, 348125.0 ], [ 2053375.0, 348125.0 ], [ 2053125.0, 348125.0 ], [ 2052875.0, 348125.0 ], [ 2052875.0, 347875.0 ], [ 2052875.0, 347625.0 ], [ 2052625.0, 347625.0 ], [ 2052375.0, 347625.0 ], [ 2052153.325309976236895, 347625.0 ], [ 2052125.0, 347599.571051757840905 ], [ 2052125.0, 347375.0 ], [ 2052125.0, 347125.0 ], [ 2051875.0, 347125.0 ], [ 2051625.0, 347125.0 ], [ 2051375.0, 347125.0 ], [ 2051125.0, 347125.0 ], [ 2051125.0, 346875.0 ], [ 2050875.0, 346875.0 ], [ 2050625.0, 346875.0 ], [ 2050375.0, 346875.0 ], [ 2050375.0, 346625.0 ], [ 2050125.0, 346625.0 ], [ 2049875.0, 346625.0 ], [ 2049625.0, 346625.0 ], [ 2049625.0, 346375.0 ], [ 2049375.0, 346375.0 ], [ 2049125.0, 346375.0 ], [ 2049125.0, 346125.0 ], [ 2048875.0, 346125.0 ], [ 2048625.0, 346125.0 ], [ 2048375.0, 346125.0 ], [ 2048125.0, 346125.0 ], [ 2048125.0, 345875.0 ], [ 2047875.0, 345875.0 ], [ 2047625.0, 345875.0 ], [ 2047375.0, 345875.0 ], [ 2047125.0, 345875.0 ], [ 2046875.0, 345875.0 ], [ 2046875.0, 346125.0 ], [ 2046625.0, 346125.0 ], [ 2046375.0, 346125.0 ], [ 2046125.0, 346125.0 ], [ 2045875.0, 346125.0 ], [ 2045875.0, 346375.0 ], [ 2045625.0, 346375.0 ], [ 2045625.0, 346125.0 ], [ 2045375.0, 346125.0 ], [ 2045125.0, 346125.0 ], [ 2044875.0, 346125.0 ], [ 2044625.0, 346125.0 ], [ 2044625.0, 346375.0 ], [ 2044375.0, 346375.0 ], [ 2044375.0, 346625.0 ], [ 2044125.0, 346625.0 ], [ 2043875.0, 346625.0 ], [ 2043875.0, 346875.0 ], [ 2043625.0, 346875.0 ], [ 2043375.0, 346875.0 ], [ 2043375.0, 347125.0 ], [ 2043375.0, 347375.0 ], [ 2043125.0, 347375.0 ], [ 2043125.0, 347625.0 ], [ 2042875.0, 347625.0 ], [ 2042625.0, 347625.0 ], [ 2042375.0, 347625.0 ], [ 2042375.0, 347375.0 ], [ 2042625.0, 347375.0 ], [ 2042625.0, 347125.0 ], [ 2042625.0, 346875.0 ], [ 2042375.0, 346875.0 ], [ 2042125.0, 346875.0 ], [ 2041875.0, 346875.0 ], [ 2041625.0, 346875.0 ], [ 2041625.0, 346625.0 ], [ 2041625.0, 346375.0 ], [ 2041625.0, 346125.0 ], [ 2041375.0, 346125.0 ], [ 2041375.0, 345875.0 ], [ 2041125.0, 345875.0 ], [ 2040875.0, 345875.0 ], [ 2040875.0, 346125.0 ], [ 2040625.0, 346125.0 ], [ 2040625.0, 345875.0 ], [ 2040625.0, 345625.0 ], [ 2040375.0, 345625.0 ], [ 2040375.0, 345875.0 ], [ 2040125.0, 345875.0 ], [ 2040125.0, 346125.0 ], [ 2040125.0, 346375.0 ], [ 2039875.0, 346375.0 ], [ 2039625.0, 346375.0 ], [ 2039625.0, 346125.0 ], [ 2039375.0, 346125.0 ], [ 2039125.0, 346125.0 ], [ 2038875.0, 346125.0 ], [ 2038625.0, 346125.0 ], [ 2038375.0, 346125.0 ], [ 2038125.0, 346125.0 ], [ 2037875.0, 346125.0 ], [ 2037875.0, 345875.0 ], [ 2037875.0, 345625.0 ], [ 2037625.0, 345625.0 ], [ 2037625.0, 345375.0 ], [ 2037375.0, 345375.0 ], [ 2037375.0, 345125.0 ], [ 2037375.0, 344875.0 ], [ 2037375.0, 344625.0 ], [ 2037375.0, 344375.0 ], [ 2037125.0, 344375.0 ], [ 2037125.0, 344125.0 ], [ 2037125.0, 343875.0 ], [ 2037375.0, 343875.0 ], [ 2037375.0, 343625.0 ], [ 2037375.0, 343375.0 ], [ 2037625.0, 343375.0 ], [ 2037625.0, 343125.0 ], [ 2037625.0, 342875.0 ], [ 2037625.0, 342625.0 ], [ 2037625.0, 342375.0 ], [ 2037625.0, 342125.0 ], [ 2037625.0, 341875.0 ], [ 2037625.0, 341625.0 ], [ 2037625.0, 341375.0 ], [ 2037625.0, 341125.0 ], [ 2037625.0, 340875.0 ], [ 2037375.0, 340875.0 ], [ 2037375.0, 340625.0 ], [ 2037375.0, 340375.0 ], [ 2037125.0, 340375.0 ], [ 2037125.0, 340125.0 ], [ 2037125.0, 339875.0 ], [ 2036875.0, 339875.0 ], [ 2036875.0, 339625.0 ], [ 2036625.0, 339625.0 ], [ 2036625.0, 339375.0 ], [ 2036375.0, 339375.0 ], [ 2036125.0, 339375.0 ], [ 2036125.0, 339125.0 ], [ 2035875.0, 339125.0 ], [ 2035875.0, 338875.0 ], [ 2035625.0, 338875.0 ], [ 2035625.0, 338625.0 ], [ 2035375.0, 338625.0 ], [ 2035375.0, 338375.0 ], [ 2035125.0, 338375.0 ], [ 2035125.0, 338125.0 ], [ 2034875.0, 338125.0 ], [ 2034875.0, 337875.0 ], [ 2034625.0, 337875.0 ], [ 2034375.0, 337875.0 ], [ 2034375.0, 337625.0 ], [ 2034125.0, 337625.0 ], [ 2033875.0, 337625.0 ], [ 2033875.0, 337375.0 ], [ 2033625.0, 337375.0 ], [ 2033625.0, 337125.0 ], [ 2033375.0, 337125.0 ], [ 2033375.0, 336875.0 ], [ 2033125.0, 336875.0 ], [ 2032875.0, 336875.0 ], [ 2032875.0, 336625.0 ], [ 2032625.0, 336625.0 ], [ 2032625.0, 336375.0 ], [ 2032375.0, 336375.0 ], [ 2032375.0, 336125.0 ], [ 2032125.0, 336125.0 ], [ 2032125.0, 335875.0 ], [ 2031875.0, 335875.0 ], [ 2031625.0, 335875.0 ], [ 2031625.0, 335625.0 ], [ 2031375.0, 335625.0 ], [ 2031125.0, 335625.0 ], [ 2031125.0, 335375.0 ], [ 2030875.0, 335375.0 ], [ 2030625.0, 335375.0 ], [ 2030625.0, 335125.0 ], [ 2030375.0, 335125.0 ], [ 2030375.0, 334875.0 ], [ 2030125.0, 334875.0 ], [ 2030125.0, 334625.0 ], [ 2029875.0, 334625.0 ], [ 2029625.0, 334625.0 ], [ 2029625.0, 334375.0 ], [ 2029375.0, 334375.0 ], [ 2029125.0, 334375.0 ], [ 2028875.0, 334375.0 ], [ 2028875.0, 334125.0 ], [ 2028625.0, 334125.0 ], [ 2028375.0, 334125.0 ], [ 2028375.0, 333875.0 ], [ 2028125.0, 333875.0 ], [ 2027875.0, 333875.0 ], [ 2027625.0, 333875.0 ], [ 2027625.0, 333625.0 ], [ 2027375.0, 333625.0 ], [ 2027125.0, 333625.0 ], [ 2026875.0, 333625.0 ], [ 2026625.0, 333625.0 ], [ 2026375.0, 333625.0 ], [ 2026375.0, 333875.0 ], [ 2026125.0, 333875.0 ], [ 2025875.0, 333875.0 ], [ 2025625.0, 333875.0 ], [ 2025375.0, 333875.0 ], [ 2025375.0, 333625.0 ], [ 2025125.0, 333625.0 ], [ 2024875.0, 333625.0 ], [ 2024625.0, 333625.0 ], [ 2024375.0, 333625.0 ], [ 2024375.0, 333375.0 ], [ 2024125.0, 333375.0 ], [ 2024125.0, 333125.0 ], [ 2023875.0, 333125.0 ], [ 2023875.0, 332875.0 ], [ 2023625.0, 332875.0 ], [ 2023625.0, 332625.0 ], [ 2023375.0, 332625.0 ], [ 2023375.0, 332375.0 ], [ 2023125.0, 332375.0 ], [ 2023125.0, 332125.0 ], [ 2022875.0, 332125.0 ], [ 2022875.0, 331875.0 ], [ 2022625.0, 331875.0 ], [ 2022625.0, 331625.0 ], [ 2022625.0, 331375.0 ], [ 2022625.0, 331125.0 ], [ 2022375.0, 331125.0 ], [ 2022375.0, 330875.0 ], [ 2022125.0, 330875.0 ], [ 2022125.0, 330625.0 ], [ 2021875.0, 330625.0 ], [ 2021875.0, 330375.0 ], [ 2021625.0, 330375.0 ], [ 2021625.0, 330125.0 ], [ 2021625.0, 329875.0 ], [ 2021375.0, 329875.0 ], [ 2021375.0, 329625.0 ], [ 2021125.0, 329625.0 ], [ 2021125.0, 329375.0 ], [ 2020875.0, 329375.0 ], [ 2020875.0, 329125.0 ], [ 2020875.0, 328875.0 ], [ 2020625.0, 328875.0 ], [ 2020625.0, 328625.0 ], [ 2020375.0, 328625.0 ], [ 2020375.0, 328375.0 ], [ 2020375.0, 328125.0 ], [ 2020125.0, 328125.0 ], [ 2020125.0, 327875.0 ], [ 2019875.0, 327875.0 ], [ 2019875.0, 327625.0 ], [ 2019625.0, 327625.0 ], [ 2019375.0, 327625.0 ], [ 2019375.0, 327375.0 ], [ 2019125.0, 327375.0 ], [ 2019125.0, 327125.0 ], [ 2018875.0, 327125.0 ], [ 2018625.0, 327125.0 ], [ 2018625.0, 326875.0 ], [ 2018375.0, 326875.0 ], [ 2018375.0, 326625.0 ], [ 2018125.0, 326625.0 ], [ 2018125.0, 326375.0 ], [ 2017875.0, 326375.0 ], [ 2017875.0, 326125.0 ], [ 2017625.0, 326125.0 ], [ 2017625.0, 325875.0 ], [ 2017375.0, 325875.0 ], [ 2017375.0, 325545.715381716901902 ], [ 2017279.628278370480984, 325470.371721629460808 ], [ 2017125.0, 325625.0 ], [ 2016875.0, 325625.0 ], [ 2016875.0, 325375.0 ], [ 2016625.0, 325375.0 ], [ 2016375.0, 325375.0 ], [ 2016375.0, 325125.0 ], [ 2016125.0, 325125.0 ], [ 2015875.0, 325125.0 ], [ 2015625.0, 325125.0 ], [ 2015375.0, 325125.0 ], [ 2015125.0, 325125.0 ], [ 2015125.0, 325375.0 ], [ 2014875.0, 325375.0 ], [ 2014625.0, 325375.0 ], [ 2014375.0, 325375.0 ], [ 2014375.0, 325125.0 ], [ 2014125.0, 325125.0 ], [ 2014125.0, 325375.0 ], [ 2013875.0, 325375.0 ], [ 2013625.0, 325375.0 ], [ 2013375.0, 325375.0 ], [ 2013125.0, 325375.0 ], [ 2013125.0, 325625.0 ], [ 2012875.0, 325625.0 ], [ 2012875.0, 325375.0 ], [ 2012625.0, 325375.0 ], [ 2012375.0, 325375.0 ], [ 2012125.0, 325375.0 ], [ 2011875.0, 325375.0 ], [ 2011625.0, 325375.0 ], [ 2011625.0, 325125.0 ], [ 2011375.0, 325125.0 ], [ 2011375.0, 324875.0 ], [ 2011125.0, 324875.0 ], [ 2011125.0, 324625.0 ], [ 2011125.0, 324375.0 ], [ 2011125.0, 324125.0 ], [ 2011125.0, 323875.0 ], [ 2010875.0, 323875.0 ], [ 2010875.0, 323625.0 ], [ 2010625.0, 323625.0 ], [ 2010625.0, 323375.0 ], [ 2010625.0, 323125.0 ], [ 2010375.0, 323125.0 ], [ 2010375.0, 322875.0 ], [ 2010375.0, 322625.0 ], [ 2010125.0, 322625.0 ], [ 2009875.0, 322625.0 ], [ 2009875.0, 322375.0 ], [ 2009625.0, 322375.0 ], [ 2009625.0, 322125.0 ], [ 2009375.0, 322125.0 ], [ 2009125.0, 322125.0 ], [ 2009125.0, 322375.0 ], [ 2008875.0, 322375.0 ], [ 2008875.0, 322125.0 ], [ 2008625.0, 322125.0 ], [ 2008625.0, 321875.0 ], [ 2008875.0, 321875.0 ], [ 2009125.0, 321875.0 ], [ 2009125.0, 321625.0 ], [ 2009125.0, 321375.0 ], [ 2008875.0, 321375.0 ], [ 2008875.0, 321125.0 ], [ 2008625.0, 321125.0 ], [ 2008625.0, 320875.0 ], [ 2008375.0, 320875.0 ], [ 2008125.0, 320866.190288251324091 ], [ 2008125.0, 321125.0 ], [ 2007839.446900221286342, 321125.0 ], [ 2007771.765017242636532, 321271.765017242520116 ], [ 2007568.955295509891585, 321529.296566680888645 ], [ 2007325.133, 321253.133900000015274 ], [ 2007631.921911379788071, 320935.452957197616342 ], [ 2007802.646582176443189, 320758.666990148311015 ], [ 2007973.371252968208864, 320581.881023104069754 ], [ 2008143.762959928717464, 320405.439841219747905 ], [ 2008274.469, 320270.0933 ], [ 2008186.308506580768153, 320111.32622177497251 ], [ 2008125.0, 320081.342086733086035 ], [ 2008125.0, 319875.0 ], [ 2007875.0, 319875.0 ], [ 2007875.0, 319625.0 ], [ 2007625.0, 319625.0 ], [ 2007625.0, 319375.0 ], [ 2007375.0, 319375.0 ], [ 2007375.0, 319125.0 ], [ 2007125.0, 319125.0 ], [ 2007125.0, 318875.0 ], [ 2006875.0, 318875.0 ], [ 2006875.0, 318625.0 ], [ 2006625.0, 318625.0 ], [ 2006625.0, 318375.0 ], [ 2006375.0, 318375.0 ], [ 2006375.0, 318034.667274101113435 ], [ 2006291.591722484445199, 317958.408277515554801 ], [ 2006030.397692633559927, 317719.60230736649828 ], [ 2005769.203662782674655, 317480.79633721744176 ], [ 2005545.214006379945204, 317276.005794221011456 ], [ 2005506.366842415882275, 317243.633157584175933 ], [ 2005233.639569688821211, 317016.360430311178789 ], [ 2004960.912296961760148, 316789.087703038239852 ], [ 2004688.185024234699085, 316561.814975765242707 ], [ 2004375.0, 316300.827455569116864 ], [ 2004279.094115144107491, 316220.905884855776094 ], [ 2004141.701392299961299, 316106.411949152010493 ], [ 2004028.590898402268067, 315971.409101597848348 ], [ 2003947.815705170854926, 315875.0 ], [ 2003686.67913369461894, 315563.320866305439267 ], [ 2003458.737957222852856, 315291.262042777147144 ], [ 2003230.796780751086771, 315019.203219248913229 ], [ 2003105.77541523007676, 314869.984170079987962 ], [ 2003022.843322667758912, 314727.156677332357503 ], [ 2002963.526542281731963, 314625.0 ], [ 2002747.333118587499484, 314252.6668814123841 ], [ 2002673.203961639199406, 314125.0 ], [ 2002504.27000920008868, 313834.058193019009195 ], [ 2002460.105908090481535, 313789.894091909460258 ], [ 2002210.105908090481535, 313539.894091909634881 ], [ 2001960.105908090248704, 313289.894091909809504 ], [ 2001710.105908090015873, 313039.894091909925919 ], [ 2001460.105908090015873, 312789.894091910100542 ], [ 2001210.105908089783043, 312539.894091910216957 ], [ 2000960.105908089550212, 312289.89409191039158 ], [ 2000833.421659100102261, 312163.209842921001837 ], [ 2000708.416651226580143, 312041.583348773478065 ], [ 2000454.991993692470714, 311795.00800630741287 ], [ 2000201.567336158594117, 311548.432663841405883 ], [ 1999948.142678624717519, 311301.857321375340689 ], [ 1999694.71802109060809, 311055.281978909275495 ], [ 1999596.993880030000582, 310960.199030850024428 ], [ 1999375.0, 310836.228422521555331 ], [ 1999239.461762215243652, 310760.538237784639932 ], [ 1998875.0, 310557.007643300748896 ], [ 1998758.211762215243652, 310491.788237784639932 ], [ 1998375.0, 310277.786864079942461 ], [ 1998276.961762215476483, 310223.038237784639932 ], [ 1997875.0, 309998.566084859136026 ], [ 1997795.711762215476483, 309954.288237784581725 ], [ 1997474.878428881987929, 309775.121571117895655 ], [ 1997125.0, 309579.734916027926374 ], [ 1997023.887420879909769, 309523.26944976602681 ], [ 1996875.0, 309533.904265543096699 ], [ 1996625.0, 309551.761408400139771 ], [ 1996556.049882849911228, 309556.686416768003255 ], [ 1996512.335807487834245, 309487.664192512223963 ], [ 1996440.981818896485493, 309375.0 ], [ 1996125.0, 309375.0 ], [ 1996125.0, 309125.0 ], [ 1995875.0, 309125.0 ], [ 1995875.0, 309375.0 ], [ 1995625.0, 309375.0 ], [ 1995375.0, 309375.0 ], [ 1995125.0, 309375.0 ], [ 1994875.0, 309375.0 ], [ 1994875.0, 309625.0 ], [ 1994625.0, 309625.0 ], [ 1994375.0, 309625.0 ], [ 1994125.0, 309625.0 ], [ 1993875.0, 309625.0 ], [ 1993875.0, 309375.0 ], [ 1993625.0, 309375.0 ], [ 1993375.0, 309375.0 ], [ 1993375.0, 309125.0 ], [ 1993375.0, 308875.0 ], [ 1993375.0, 308625.0 ], [ 1993375.0, 308375.0 ], [ 1993375.0, 308125.0 ], [ 1993125.0, 308125.0 ], [ 1993125.0, 307875.0 ], [ 1993125.0, 307625.0 ], [ 1993125.0, 307375.0 ], [ 1993125.0, 307125.0 ], [ 1993125.0, 306875.0 ], [ 1992875.0, 306875.0 ], [ 1992875.0, 306625.0 ], [ 1992625.0, 306625.0 ], [ 1992625.0, 306375.0 ], [ 1992625.0, 306125.0 ], [ 1992375.0, 306125.0 ], [ 1992125.0, 306125.0 ], [ 1992125.0, 305875.0 ], [ 1992125.0, 305625.0 ], [ 1991875.0, 305625.0 ], [ 1991875.0, 305375.0 ], [ 1991625.0, 305375.0 ], [ 1991375.0, 305375.0 ], [ 1991375.0, 305125.0 ], [ 1991125.0, 305125.0 ], [ 1991125.0, 304875.0 ], [ 1990875.0, 304875.0 ], [ 1990875.0, 304625.0 ], [ 1990625.0, 304625.0 ], [ 1990625.0, 304375.0 ], [ 1990375.0, 304375.0 ], [ 1990125.0, 304375.0 ], [ 1990125.0, 304125.0 ], [ 1989875.0, 304125.0 ], [ 1989875.0, 303875.0 ], [ 1989625.0, 303875.0 ], [ 1989625.0, 303625.0 ], [ 1989375.0, 303625.0 ], [ 1989125.0, 303625.0 ], [ 1989125.0, 303375.0 ], [ 1988875.0, 303375.0 ], [ 1988875.0, 303125.0 ], [ 1988625.0, 303125.0 ], [ 1988375.0, 303125.0 ], [ 1988375.0, 302875.0 ], [ 1988125.0, 302875.0 ], [ 1988125.0, 302625.0 ], [ 1987875.0, 302625.0 ], [ 1987875.0, 302375.0 ], [ 1987625.0, 302375.0 ], [ 1987375.0, 302375.0 ], [ 1987375.0, 302125.0 ], [ 1987125.0, 302125.0 ], [ 1987125.0, 301875.0 ], [ 1986875.0, 301875.0 ], [ 1986875.0, 301625.0 ], [ 1986625.0, 301625.0 ], [ 1986625.0, 301375.0 ], [ 1986375.0, 301375.0 ], [ 1986125.0, 301375.0 ], [ 1986125.0, 301125.0 ], [ 1985875.0, 301125.0 ], [ 1985875.0, 300875.0 ], [ 1985625.0, 300875.0 ], [ 1985625.0, 300625.0 ], [ 1985375.0, 300625.0 ], [ 1985375.0, 300375.0 ], [ 1985125.0, 300375.0 ], [ 1984875.0, 300375.0 ], [ 1984875.0, 300125.0 ], [ 1984625.0, 300125.0 ], [ 1984625.0, 299875.0 ], [ 1984375.0, 299875.0 ], [ 1984375.0, 299625.0 ], [ 1984125.0, 299625.0 ], [ 1984125.0, 299375.0 ], [ 1983875.0, 299375.0 ], [ 1983875.0, 299125.0 ], [ 1983625.0, 299125.0 ], [ 1983625.0, 298875.0 ], [ 1983375.0, 298875.0 ], [ 1983375.0, 298625.0 ], [ 1983125.0, 298625.0 ], [ 1982875.0, 298625.0 ], [ 1982625.0, 298625.0 ], [ 1982625.0, 298375.0 ], [ 1982625.0, 298125.0 ], [ 1982375.0, 298125.0 ], [ 1982375.0, 297875.0 ], [ 1982125.0, 297875.0 ], [ 1981875.0, 297875.0 ], [ 1981875.0, 298125.0 ], [ 1981625.0, 298125.0 ], [ 1981625.0, 297875.0 ], [ 1981375.0, 297875.0 ], [ 1981125.0, 297875.0 ], [ 1981125.0, 297625.0 ], [ 1980875.0, 297625.0 ], [ 1980875.0, 297375.0 ], [ 1980625.0, 297375.0 ], [ 1980625.0, 297125.0 ], [ 1980625.0, 296875.0 ], [ 1980375.0, 296875.0 ], [ 1980375.0, 296625.0 ], [ 1980125.0, 296625.0 ], [ 1979875.0, 296625.0 ], [ 1979875.0, 296375.0 ], [ 1979625.0, 296375.0 ], [ 1979625.0, 296125.0 ], [ 1979375.0, 296125.0 ], [ 1979375.0, 295875.0 ], [ 1979125.0, 295875.0 ], [ 1979125.0, 295625.0 ], [ 1979125.0, 295375.0 ], [ 1978875.0, 295375.0 ], [ 1978625.0, 295375.0 ], [ 1978625.0, 295125.0 ], [ 1978375.0, 295125.0 ], [ 1978375.0, 294875.0 ], [ 1978125.0, 294875.0 ], [ 1978125.0, 294625.0 ], [ 1977875.0, 294625.0 ], [ 1977875.0, 294375.0 ], [ 1977875.0, 294125.0 ], [ 1977625.0, 294125.0 ], [ 1977375.0, 294125.0 ], [ 1977125.0, 294125.0 ], [ 1977125.0, 293875.0 ], [ 1976875.0, 293875.0 ], [ 1976625.0, 293875.0 ], [ 1976375.0, 293875.0 ], [ 1976125.0, 293875.0 ], [ 1975875.0, 293875.0 ], [ 1975875.0, 293625.0 ], [ 1975625.0, 293625.0 ], [ 1975375.0, 293625.0 ], [ 1975125.0, 293625.0 ], [ 1974875.0, 293625.0 ], [ 1974625.0, 293625.0 ], [ 1974375.0, 293625.0 ], [ 1974125.0, 293625.0 ], [ 1973875.0, 293625.0 ], [ 1973625.0, 293625.0 ], [ 1973625.0, 293375.0 ], [ 1973375.0, 293375.0 ], [ 1973125.0, 293375.0 ], [ 1973125.0, 293625.0 ], [ 1972875.0, 293625.0 ], [ 1972625.0, 293625.0 ], [ 1972375.0, 293625.0 ], [ 1972125.0, 293625.0 ], [ 1971875.0, 293625.0 ], [ 1971875.0, 293875.0 ], [ 1971625.0, 293875.0 ], [ 1971625.0, 294125.0 ], [ 1971375.0, 294125.0 ], [ 1971125.0, 294125.0 ], [ 1970875.0, 294125.0 ], [ 1970875.0, 294375.0 ], [ 1970625.0, 294375.0 ], [ 1970625.0, 294625.0 ], [ 1970375.0, 294625.0 ], [ 1970375.0, 294875.0 ], [ 1970375.0, 295125.0 ], [ 1970125.0, 295125.0 ], [ 1970125.0, 294875.0 ], [ 1969875.0, 294875.0 ], [ 1969875.0, 295125.0 ], [ 1969625.0, 295125.0 ], [ 1969375.0, 295125.0 ], [ 1969375.0, 294875.0 ], [ 1969375.0, 294625.0 ], [ 1969125.0, 294625.0 ], [ 1969125.0, 294375.0 ], [ 1968875.0, 294375.0 ], [ 1968875.0, 294125.0 ], [ 1968625.0, 294125.0 ], [ 1968625.0, 293875.0 ], [ 1968375.0, 293875.0 ], [ 1968375.0, 294125.0 ], [ 1968375.0, 294375.0 ], [ 1968375.0, 294625.0 ], [ 1968239.674949259730056, 294760.325050740211736 ], [ 1968304.760731272865087, 294875.0 ], [ 1968511.22667340002954, 295238.773326600086875 ], [ 1968588.544515060260892, 295375.0 ], [ 1968782.778397540096194, 295717.221602459903806 ], [ 1968886.801205229945481, 295900.49988267297158 ], [ 1969125.0, 295940.872559752839152 ], [ 1969375.0, 295983.245441108942032 ], [ 1969625.0, 296025.618322465044912 ], [ 1969709.978535863105208, 296040.021464136836585 ], [ 1970125.0, 296110.364085177250672 ], [ 1970125.0, 295875.0 ], [ 1969875.0, 295875.0 ], [ 1969875.0, 295625.0 ], [ 1970125.0, 295625.0 ], [ 1970125.0, 295375.0 ], [ 1970375.0, 295375.0 ], [ 1970625.0, 295375.0 ], [ 1970875.0, 295375.0 ], [ 1971125.0, 295375.0 ], [ 1971375.0, 295375.0 ], [ 1971375.0, 295625.0 ], [ 1971625.0, 295625.0 ], [ 1971875.0, 295625.0 ], [ 1971875.0, 295875.0 ], [ 1972125.0, 295875.0 ], [ 1972375.0, 295875.0 ], [ 1972375.0, 296125.0 ], [ 1972625.0, 296125.0 ], [ 1972875.0, 296125.0 ], [ 1973125.0, 296125.0 ], [ 1973125.0, 296375.0 ], [ 1973375.0, 296375.0 ], [ 1973625.0, 296375.0 ], [ 1973625.0, 296625.0 ], [ 1973875.0, 296625.0 ], [ 1973875.0, 296375.0 ], [ 1974125.0, 296375.0 ], [ 1974375.0, 296375.0 ], [ 1974625.0, 296375.0 ], [ 1974625.0, 296125.0 ], [ 1974875.0, 296125.0 ], [ 1975125.0, 296125.0 ], [ 1975375.0, 296125.0 ], [ 1975625.0, 296125.0 ], [ 1975625.0, 295875.0 ], [ 1975875.0, 295875.0 ], [ 1976125.0, 295875.0 ], [ 1976125.0, 295625.0 ], [ 1975875.0, 295625.0 ], [ 1975875.0, 295375.0 ], [ 1975875.0, 295125.0 ], [ 1976125.0, 295125.0 ], [ 1976375.0, 295125.0 ], [ 1976375.0, 295375.0 ], [ 1976625.0, 295375.0 ], [ 1976625.0, 295625.0 ], [ 1976875.0, 295625.0 ], [ 1976875.0, 295875.0 ], [ 1976875.0, 296125.0 ], [ 1977125.0, 296125.0 ], [ 1977125.0, 296375.0 ], [ 1977375.0, 296375.0 ], [ 1977625.0, 296375.0 ], [ 1977625.0, 296625.0 ], [ 1977875.0, 296625.0 ], [ 1977875.0, 296875.0 ], [ 1978125.0, 296875.0 ], [ 1978125.0, 297125.0 ], [ 1977875.0, 297125.0 ], [ 1977875.0, 297375.0 ], [ 1977875.0, 297625.0 ], [ 1977875.0, 297875.0 ], [ 1978125.0, 297875.0 ], [ 1978375.0, 297875.0 ], [ 1978375.0, 297625.0 ], [ 1978625.0, 297625.0 ], [ 1978875.0, 297625.0 ], [ 1979125.0, 297625.0 ], [ 1979125.0, 297875.0 ], [ 1979125.0, 298125.0 ], [ 1979375.0, 298125.0 ], [ 1979375.0, 298375.0 ], [ 1979625.0, 298375.0 ], [ 1979875.0, 298375.0 ], [ 1979875.0, 298625.0 ], [ 1979875.0, 298875.0 ], [ 1980125.0, 298875.0 ], [ 1980125.0, 299125.0 ], [ 1979875.0, 299125.0 ], [ 1979625.0, 299125.0 ], [ 1979375.0, 299125.0 ], [ 1979375.0, 299375.0 ], [ 1979125.0, 299375.0 ], [ 1978875.0, 299375.0 ], [ 1978875.0, 299625.0 ], [ 1978625.0, 299625.0 ], [ 1978375.0, 299625.0 ], [ 1978214.142584908287972, 299785.857415091595612 ], [ 1978234.099880037130788, 299875.0 ], [ 1978290.070029290858656, 300125.0 ], [ 1978625.0, 300125.0 ], [ 1978875.0, 300125.0 ], [ 1979125.0, 300125.0 ], [ 1979375.0, 300125.0 ], [ 1979375.0, 300375.0 ], [ 1979625.0, 300375.0 ], [ 1979875.0, 300375.0 ], [ 1980125.0, 300375.0 ], [ 1980375.0, 300375.0 ], [ 1980375.0, 300625.0 ], [ 1980625.0, 300625.0 ], [ 1980625.0, 300875.0 ], [ 1980625.0, 301125.0 ], [ 1980875.0, 301125.0 ], [ 1980875.0, 301375.0 ], [ 1981125.0, 301375.0 ], [ 1981125.0, 301625.0 ], [ 1981125.0, 301875.0 ], [ 1981375.0, 301875.0 ], [ 1981625.0, 301875.0 ], [ 1981875.0, 301875.0 ], [ 1982125.0, 301875.0 ], [ 1982125.0, 301625.0 ], [ 1982125.0, 301375.0 ], [ 1981875.0, 301375.0 ], [ 1981625.0, 301375.0 ], [ 1981625.0, 301125.0 ], [ 1981625.0, 300875.0 ], [ 1981375.0, 300875.0 ], [ 1981375.0, 300625.0 ], [ 1981375.0, 300375.0 ], [ 1981375.0, 300125.0 ], [ 1981625.0, 300125.0 ], [ 1981875.0, 300125.0 ] ], [ [ 2081625.0, 364875.0 ], [ 2081625.0, 365125.0 ], [ 2081375.0, 365125.0 ], [ 2081375.0, 364875.0 ], [ 2081375.0, 364625.0 ], [ 2081625.0, 364625.0 ], [ 2081625.0, 364875.0 ] ], [ [ 2068125.0, 356375.0 ], [ 2068375.0, 356375.0 ], [ 2068375.0, 356625.0 ], [ 2068125.0, 356625.0 ], [ 2068125.0, 356375.0 ] ], [ [ 2081125.0, 365875.0 ], [ 2081125.0, 365625.0 ], [ 2081375.0, 365625.0 ], [ 2081375.0, 365875.0 ], [ 2081125.0, 365875.0 ] ], [ [ 2081125.0, 366125.0 ], [ 2080875.0, 366125.0 ], [ 2080875.0, 365875.0 ], [ 2081125.0, 365875.0 ], [ 2081125.0, 366125.0 ] ], [ [ 2080375.0, 365375.0 ], [ 2080625.0, 365375.0 ], [ 2080625.0, 365625.0 ], [ 2080375.0, 365625.0 ], [ 2080375.0, 365375.0 ] ], [ [ 2000375.0, 317875.0 ], [ 2000125.0, 317875.0 ], [ 2000125.0, 317625.0 ], [ 1999875.0, 317625.0 ], [ 1999875.0, 317375.0 ], [ 1999875.0, 317125.0 ], [ 1999625.0, 317125.0 ], [ 1999375.0, 317125.0 ], [ 1999375.0, 316875.0 ], [ 1999125.0, 316875.0 ], [ 1998875.0, 316875.0 ], [ 1998875.0, 316625.0 ], [ 1998625.0, 316625.0 ], [ 1998625.0, 316375.0 ], [ 1998625.0, 316125.0 ], [ 1998625.0, 315875.0 ], [ 1998875.0, 315875.0 ], [ 1998875.0, 315625.0 ], [ 1998625.0, 315625.0 ], [ 1998625.0, 315375.0 ], [ 1998375.0, 315375.0 ], [ 1998375.0, 315625.0 ], [ 1998375.0, 315875.0 ], [ 1998375.0, 316125.0 ], [ 1998375.0, 316375.0 ], [ 1998125.0, 316375.0 ], [ 1997875.0, 316375.0 ], [ 1997875.0, 316125.0 ], [ 1997875.0, 315875.0 ], [ 1997625.0, 315875.0 ], [ 1997625.0, 315625.0 ], [ 1997625.0, 315375.0 ], [ 1997375.0, 315375.0 ], [ 1997375.0, 315125.0 ], [ 1997375.0, 314875.0 ], [ 1997375.0, 314625.0 ], [ 1997125.0, 314625.0 ], [ 1996875.0, 314625.0 ], [ 1996875.0, 314375.0 ], [ 1997125.0, 314375.0 ], [ 1997375.0, 314375.0 ], [ 1997625.0, 314375.0 ], [ 1997875.0, 314375.0 ], [ 1998125.0, 314375.0 ], [ 1998375.0, 314375.0 ], [ 1998625.0, 314375.0 ], [ 1998875.0, 314375.0 ], [ 1998875.0, 314625.0 ], [ 1999125.0, 314625.0 ], [ 1999125.0, 314875.0 ], [ 1999375.0, 314875.0 ], [ 1999375.0, 315125.0 ], [ 1999625.0, 315125.0 ], [ 1999625.0, 315375.0 ], [ 1999875.0, 315375.0 ], [ 1999875.0, 315625.0 ], [ 2000125.0, 315625.0 ], [ 2000125.0, 315875.0 ], [ 2000125.0, 316125.0 ], [ 2000375.0, 316125.0 ], [ 2000375.0, 316375.0 ], [ 2000625.0, 316375.0 ], [ 2000625.0, 316625.0 ], [ 2000875.0, 316625.0 ], [ 2000875.0, 316875.0 ], [ 2001125.0, 316875.0 ], [ 2001125.0, 317125.0 ], [ 2001375.0, 317125.0 ], [ 2001375.0, 317375.0 ], [ 2001375.0, 317625.0 ], [ 2001375.0, 317875.0 ], [ 2001375.0, 318125.0 ], [ 2001125.0, 318125.0 ], [ 2001125.0, 317875.0 ], [ 2000875.0, 317875.0 ], [ 2000875.0, 318125.0 ], [ 2000625.0, 318125.0 ], [ 2000625.0, 317875.0 ], [ 2000375.0, 317875.0 ] ], [ [ 2001375.0, 318625.0 ], [ 2001375.0, 318375.0 ], [ 2001625.0, 318375.0 ], [ 2001625.0, 318625.0 ], [ 2001375.0, 318625.0 ] ], [ [ 2005125.0, 321375.0 ], [ 2005375.0, 321375.0 ], [ 2005375.0, 321625.0 ], [ 2005625.0, 321625.0 ], [ 2005625.0, 321875.0 ], [ 2005875.0, 321875.0 ], [ 2005875.0, 322125.0 ], [ 2005875.0, 322361.554269636981189 ], [ 2005951.030115525936708, 322384.202294588496443 ], [ 2006132.449795989319682, 322221.120999313541688 ], [ 2006220.382, 322395.3335 ], [ 2006411.373, 322194.9804 ], [ 2006583.639, 322015.2244 ], [ 2006777.156816202681512, 321818.271713803464081 ], [ 2006900.085, 321693.1616 ], [ 2006975.777212957851589, 321614.801813868223689 ], [ 2007148.71354650054127, 321435.770793618052267 ], [ 2007289.698470945237204, 321629.262140740000177 ], [ 2007375.0, 321686.510160959267523 ], [ 2007375.0, 321875.0 ], [ 2007625.0, 321875.0 ], [ 2007625.0, 322125.0 ], [ 2007875.0, 322125.0 ], [ 2007875.0, 322375.0 ], [ 2008125.0, 322375.0 ], [ 2008375.0, 322375.0 ], [ 2008625.0, 322375.0 ], [ 2008625.0, 322625.0 ], [ 2008625.0, 322875.0 ], [ 2008875.0, 322875.0 ], [ 2008875.0, 323125.0 ], [ 2009125.0, 323125.0 ], [ 2009125.0, 323375.0 ], [ 2009125.0, 323625.0 ], [ 2009375.0, 323625.0 ], [ 2009375.0, 323875.0 ], [ 2009375.0, 324125.0 ], [ 2009125.0, 324125.0 ], [ 2009125.0, 324375.0 ], [ 2009125.0, 324625.0 ], [ 2008875.0, 324625.0 ], [ 2008875.0, 324875.0 ], [ 2008625.0, 324875.0 ], [ 2008625.0, 325125.0 ], [ 2008375.0, 325125.0 ], [ 2008125.0, 325125.0 ], [ 2008125.0, 324875.0 ], [ 2007875.0, 324875.0 ], [ 2007625.0, 324875.0 ], [ 2007625.0, 324625.0 ], [ 2007375.0, 324625.0 ], [ 2007375.0, 324375.0 ], [ 2007125.0, 324375.0 ], [ 2006875.0, 324375.0 ], [ 2006625.0, 324375.0 ], [ 2006625.0, 324125.0 ], [ 2006625.0, 323875.0 ], [ 2006375.0, 323875.0 ], [ 2006375.0, 323625.0 ], [ 2006375.0, 323375.0 ], [ 2006125.0, 323375.0 ], [ 2006125.0, 323125.0 ], [ 2006125.0, 322874.713643411407247 ], [ 2006060.660845221951604, 322854.279157310607843 ], [ 2005890.83, 322726.7586 ], [ 2005823.255498570390046, 322591.415552785329055 ], [ 2005853.200009249150753, 322375.0 ], [ 2005625.0, 322375.0 ], [ 2005625.0, 322125.0 ], [ 2005375.0, 322125.0 ], [ 2005125.0, 322125.0 ], [ 2005125.0, 321875.0 ], [ 2004875.0, 321875.0 ], [ 2004625.0, 321875.0 ], [ 2004625.0, 321625.0 ], [ 2004375.0, 321625.0 ], [ 2004375.0, 321375.0 ], [ 2004125.0, 321375.0 ], [ 2004125.0, 321125.0 ], [ 2003875.0, 321125.0 ], [ 2003875.0, 320875.0 ], [ 2003625.0, 320875.0 ], [ 2003625.0, 320625.0 ], [ 2003375.0, 320625.0 ], [ 2003375.0, 320375.0 ], [ 2003125.0, 320375.0 ], [ 2003125.0, 320125.0 ], [ 2003375.0, 320125.0 ], [ 2003625.0, 320125.0 ], [ 2003875.0, 320125.0 ], [ 2003875.0, 320375.0 ], [ 2004125.0, 320375.0 ], [ 2004125.0, 320625.0 ], [ 2004375.0, 320625.0 ], [ 2004375.0, 320875.0 ], [ 2004625.0, 320875.0 ], [ 2004875.0, 320875.0 ], [ 2004875.0, 321125.0 ], [ 2004875.0, 321375.0 ], [ 2005125.0, 321375.0 ] ], [ [ 2005156.825999999884516, 323118.102399999974295 ], [ 2005265.429, 323086.270599999988917 ], [ 2005352.183974175946787, 323250.94473008584464 ], [ 2005344.927198968362063, 323344.927198968245648 ], [ 2005125.0, 323380.201065170695074 ], [ 2005108.617624151753262, 323367.808860811113846 ], [ 2004965.835, 323153.6791 ], [ 2005156.825999999884516, 323118.102399999974295 ] ], [ [ 2061125.0, 355125.0 ], [ 2060875.0, 355125.0 ], [ 2060875.0, 354875.0 ], [ 2060875.0, 354625.0 ], [ 2060875.0, 354375.0 ], [ 2061125.0, 354375.0 ], [ 2061125.0, 354125.0 ], [ 2061125.0, 353875.0 ], [ 2061375.0, 353875.0 ], [ 2061375.0, 353625.0 ], [ 2061625.0, 353625.0 ], [ 2061875.0, 353625.0 ], [ 2062125.0, 353625.0 ], [ 2062375.0, 353625.0 ], [ 2062625.0, 353625.0 ], [ 2062875.0, 353625.0 ], [ 2062875.0, 353875.0 ], [ 2063125.0, 353875.0 ], [ 2063375.0, 353875.0 ], [ 2063625.0, 353875.0 ], [ 2063875.0, 353875.0 ], [ 2064125.0, 353875.0 ], [ 2064375.0, 353875.0 ], [ 2064625.0, 353875.0 ], [ 2064625.0, 353625.0 ], [ 2064875.0, 353625.0 ], [ 2065125.0, 353625.0 ], [ 2065125.0, 353875.0 ], [ 2065375.0, 353875.0 ], [ 2065625.0, 353875.0 ], [ 2065625.0, 354125.0 ], [ 2065875.0, 354125.0 ], [ 2065875.0, 354375.0 ], [ 2065625.0, 354375.0 ], [ 2065625.0, 354625.0 ], [ 2065625.0, 354875.0 ], [ 2065625.0, 355125.0 ], [ 2065625.0, 355375.0 ], [ 2065375.0, 355375.0 ], [ 2065375.0, 355625.0 ], [ 2065125.0, 355625.0 ], [ 2065125.0, 355875.0 ], [ 2064875.0, 355875.0 ], [ 2064875.0, 356125.0 ], [ 2064625.0, 356125.0 ], [ 2064375.0, 356125.0 ], [ 2064125.0, 356125.0 ], [ 2063875.0, 356125.0 ], [ 2063875.0, 355875.0 ], [ 2064125.0, 355875.0 ], [ 2064125.0, 355625.0 ], [ 2064375.0, 355625.0 ], [ 2064625.0, 355625.0 ], [ 2064625.0, 355375.0 ], [ 2064875.0, 355375.0 ], [ 2064875.0, 355125.0 ], [ 2065125.0, 355125.0 ], [ 2065125.0, 354875.0 ], [ 2064875.0, 354875.0 ], [ 2064625.0, 354875.0 ], [ 2064375.0, 354875.0 ], [ 2064125.0, 354875.0 ], [ 2063875.0, 354875.0 ], [ 2063625.0, 354875.0 ], [ 2063625.0, 354625.0 ], [ 2063375.0, 354625.0 ], [ 2063125.0, 354625.0 ], [ 2062875.0, 354625.0 ], [ 2062625.0, 354625.0 ], [ 2062625.0, 354875.0 ], [ 2062375.0, 354875.0 ], [ 2062375.0, 355125.0 ], [ 2062375.0, 355375.0 ], [ 2062375.0, 355625.0 ], [ 2062375.0, 355875.0 ], [ 2062375.0, 356125.0 ], [ 2062375.0, 356375.0 ], [ 2062375.0, 356625.0 ], [ 2062125.0, 356625.0 ], [ 2061875.0, 356625.0 ], [ 2061875.0, 356375.0 ], [ 2061625.0, 356375.0 ], [ 2061375.0, 356375.0 ], [ 2061375.0, 356125.0 ], [ 2061375.0, 355875.0 ], [ 2061125.0, 355875.0 ], [ 2061125.0, 355625.0 ], [ 2061125.0, 355375.0 ], [ 2061125.0, 355125.0 ] ], [ [ 2066875.0, 356375.0 ], [ 2066625.0, 356375.0 ], [ 2066625.0, 356125.0 ], [ 2066875.0, 356125.0 ], [ 2066875.0, 356375.0 ] ] ], [ [ [ 2008625.0, 333125.0 ], [ 2008625.0, 332875.0 ], [ 2008375.0, 332875.0 ], [ 2008375.0, 333125.0 ], [ 2008375.0, 333375.0 ], [ 2008625.0, 333375.0 ], [ 2008625.0, 333125.0 ] ] ], [ [ [ 2009625.0, 333625.0 ], [ 2009625.0, 333375.0 ], [ 2009375.0, 333375.0 ], [ 2009375.0, 333125.0 ], [ 2009375.0, 332875.0 ], [ 2009125.0, 332875.0 ], [ 2009125.0, 333125.0 ], [ 2009125.0, 333375.0 ], [ 2009125.0, 333625.0 ], [ 2009375.0, 333625.0 ], [ 2009625.0, 333625.0 ] ] ], [ [ [ 2004375.0, 336375.0 ], [ 2004625.0, 336375.0 ], [ 2004875.0, 336375.0 ], [ 2004875.0, 336125.0 ], [ 2004625.0, 336125.0 ], [ 2004455.95006786310114, 335955.950067863217555 ], [ 2004375.0, 335992.955813172273338 ], [ 2004375.0, 336375.0 ] ] ], [ [ [ 2003246.088069938356057, 337753.911930061702151 ], [ 2003625.0, 337902.992033692600671 ], [ 2003625.0, 337625.0 ], [ 2003625.0, 337375.0 ], [ 2003375.0, 337375.0 ], [ 2003375.0, 337625.0 ], [ 2003246.088069938356057, 337753.911930061702151 ] ] ], [ [ [ 2048125.0, 333491.750577958940994 ], [ 2048125.0, 333875.0 ], [ 2048375.0, 333875.0 ], [ 2048375.0, 333476.750577958999202 ], [ 2048125.0, 333491.750577958940994 ] ] ], [ [ [ 2046252.791298779891804, 335736.270857280003838 ], [ 2046259.751744923647493, 335740.248255076410715 ], [ 2046375.0, 335625.0 ], [ 2046375.0, 335375.0 ], [ 2046625.0, 335375.0 ], [ 2046625.0, 335625.0 ], [ 2046875.0, 335625.0 ], [ 2046875.0, 335875.0 ], [ 2047125.0, 335875.0 ], [ 2047375.0, 335875.0 ], [ 2047737.973404365358874, 335875.0 ], [ 2047722.039358329959214, 335829.853536232025363 ], [ 2047534.874000420095399, 335652.04644622298656 ], [ 2047506.799196739913896, 335521.030695690016728 ], [ 2047513.270180029096082, 335500.0 ], [ 2047581.665339899947867, 335277.715730414027348 ], [ 2047681.691049345536157, 335181.691049345594365 ], [ 2047815.622037279885262, 335053.117300927988254 ], [ 2047945.164908360224217, 334945.164908360282425 ], [ 2048217.892181086586788, 334717.892181086703204 ], [ 2048433.267718360060826, 334538.412566690996755 ], [ 2048500.0, 334532.85154322150629 ], [ 2048500.0, 334500.0 ], [ 2048375.0, 334375.0 ], [ 2048125.0, 334375.0 ], [ 2048125.0, 334625.0 ], [ 2047875.0, 334625.0 ], [ 2047625.0, 334625.0 ], [ 2047625.0, 334375.0 ], [ 2047375.0, 334375.0 ], [ 2047375.0, 334625.0 ], [ 2047375.0, 334875.0 ], [ 2047125.0, 334875.0 ], [ 2047125.0, 334625.0 ], [ 2046875.0, 334625.0 ], [ 2046875.0, 334375.0 ], [ 2047125.0, 334375.0 ], [ 2047125.0, 334125.0 ], [ 2047125.0, 333776.929423133609816 ], [ 2047104.393677240004763, 333789.751135073020123 ], [ 2046754.47435533371754, 334004.474355333659332 ], [ 2046692.629889850039035, 334042.424368244013749 ], [ 2046625.0, 334030.662648270372301 ], [ 2046492.398484806297347, 334007.601515193702653 ], [ 2046477.389728259993717, 334004.991296663996764 ], [ 2046228.662012284621596, 333771.337987715494819 ], [ 2046125.0, 333875.0 ], [ 2045875.0, 333875.0 ], [ 2045625.0, 333875.0 ], [ 2045625.0, 333549.322385145525914 ], [ 2045375.0, 333574.322385145002045 ], [ 2045373.114116620039567, 333574.510973482974805 ], [ 2045125.0, 333582.14525399467675 ], [ 2044875.0, 333589.837561687338166 ], [ 2044764.826703439932317, 333593.227509273972828 ], [ 2044625.0, 333544.825958084082231 ], [ 2044625.0, 333875.0 ], [ 2044625.0, 334125.0 ], [ 2044875.0, 334125.0 ], [ 2044875.0, 334375.0 ], [ 2045125.0, 334375.0 ], [ 2045375.0, 334375.0 ], [ 2045375.0, 334625.0 ], [ 2045625.0, 334625.0 ], [ 2045625.0, 334875.0 ], [ 2045875.0, 334875.0 ], [ 2045875.0, 335125.0 ], [ 2045875.0, 335375.0 ], [ 2046125.0, 335375.0 ], [ 2046125.0, 335625.0 ], [ 2046239.942066502058879, 335739.942066502058879 ], [ 2046252.791298779891804, 335736.270857280003838 ] ], [ [ 2046875.0, 334875.0 ], [ 2046875.0, 335125.0 ], [ 2046625.0, 335125.0 ], [ 2046625.0, 334875.0 ], [ 2046625.0, 334625.0 ], [ 2046875.0, 334625.0 ], [ 2046875.0, 334875.0 ] ], [ [ 2045625.0, 334125.0 ], [ 2045375.0, 334125.0 ], [ 2045125.0, 334125.0 ], [ 2045125.0, 333875.0 ], [ 2045375.0, 333875.0 ], [ 2045625.0, 333875.0 ], [ 2045625.0, 334125.0 ] ], [ [ 2046125.0, 335125.0 ], [ 2046125.0, 334875.0 ], [ 2046375.0, 334875.0 ], [ 2046375.0, 335125.0 ], [ 2046125.0, 335125.0 ] ] ], [ [ [ 2044025.523539710091427, 338758.991387435991783 ], [ 2044222.047165510011837, 338749.633119540987536 ], [ 2044278.1967728799209, 338646.692172694019973 ], [ 2044193.972361830063164, 338515.676422160991933 ], [ 2043838.358181810006499, 338412.735475312976632 ], [ 2043625.0, 338306.056384409370366 ], [ 2043625.0, 338665.145386123040225 ], [ 2043744.775502860080451, 338712.200047959981021 ], [ 2044025.523539710091427, 338758.991387435991783 ] ] ], [ [ [ 2024597.759389230050147, 340856.255727335985284 ], [ 2024532.251513970084488, 341005.988013660011347 ], [ 2024575.095829048892483, 341125.0 ], [ 2024616.475925019942224, 341239.94471104000695 ], [ 2024784.924747139913961, 341417.751801048987545 ], [ 2024875.0, 341459.324994679889642 ], [ 2024906.582229770021513, 341473.901408420992084 ], [ 2025018.881444520084187, 341427.110068944981322 ], [ 2025224.763338210061193, 341258.661246831004974 ], [ 2025242.967770617455244, 341242.967770617571659 ], [ 2025125.0, 341125.0 ], [ 2025125.0, 340875.0 ], [ 2024973.459618293214589, 340723.459618293156382 ], [ 2024875.0, 340770.720235074171796 ], [ 2024794.283015029970556, 340809.464387859974522 ], [ 2024597.759389230050147, 340856.255727335985284 ] ] ], [ [ [ 2039875.0, 344125.0 ], [ 2039875.0, 344375.0 ], [ 2040125.0, 344375.0 ], [ 2040125.0, 344125.0 ], [ 2039875.0, 344125.0 ] ] ], [ [ [ 2029031.68183714337647, 344375.0 ], [ 2029005.503567880019546, 344412.397527520020958 ], [ 2029033.578371569979936, 344590.204617529991083 ], [ 2029164.59412209992297, 344739.936903853027616 ], [ 2029248.818533160025254, 344796.086511224973947 ], [ 2029375.0, 344729.284558192535769 ], [ 2029407.909087379928678, 344711.862100168014877 ], [ 2029625.0, 344644.489058320061304 ], [ 2029625.0, 344375.0 ], [ 2029375.0, 344375.0 ], [ 2029375.0, 344125.0 ], [ 2029292.459090777207166, 344042.459090777207166 ], [ 2029286.251604740042239, 344047.425079606997315 ], [ 2029136.51931840996258, 344225.232169615977909 ], [ 2029031.68183714337647, 344375.0 ] ] ], [ [ [ 2047375.0, 358375.0 ], [ 2047375.0, 358625.0 ], [ 2047375.0, 358875.0 ], [ 2047125.0, 358875.0 ], [ 2047125.0, 359125.0 ], [ 2047125.0, 359375.0 ], [ 2047375.0, 359375.0 ], [ 2047375.0, 359625.0 ], [ 2047375.0, 359875.0 ], [ 2047625.0, 359875.0 ], [ 2047625.0, 360125.0 ], [ 2047625.0, 360375.0 ], [ 2047875.0, 360375.0 ], [ 2047875.0, 360625.0 ], [ 2047875.0, 360875.0 ], [ 2047875.0, 361125.0 ], [ 2047625.0, 361125.0 ], [ 2047625.0, 361375.0 ], [ 2047375.0, 361375.0 ], [ 2047375.0, 361125.0 ], [ 2047125.0, 361125.0 ], [ 2046875.0, 361125.0 ], [ 2046875.0, 361375.0 ], [ 2046625.0, 361375.0 ], [ 2046625.0, 361625.0 ], [ 2046625.0, 361875.0 ], [ 2046625.0, 362125.0 ], [ 2046625.0, 362375.0 ], [ 2046625.0, 362625.0 ], [ 2046375.0, 362625.0 ], [ 2046375.0, 362906.344871860928833 ], [ 2046625.0, 362929.600685814453755 ], [ 2046875.0, 362952.856499767920468 ], [ 2047125.0, 362976.112313721387181 ], [ 2047375.0, 362999.368127674912103 ], [ 2047460.007857260061428, 363007.275835326989181 ], [ 2047680.179382668575272, 362625.0 ], [ 2047751.378885387675837, 362501.378885387792252 ], [ 2047968.154066211078316, 362125.0 ], [ 2048025.475270928815007, 362025.475270928873215 ], [ 2048208.206194622907788, 361708.206194622965995 ], [ 2048311.610235719941556, 361528.669507881975733 ], [ 2048426.862366632325575, 361375.0 ], [ 2048511.778495218837634, 361261.778495218895841 ], [ 2048726.064209505682811, 360976.064209505624603 ], [ 2048940.349923792295158, 360690.349923792295158 ], [ 2049176.862366638379171, 360375.0 ], [ 2049261.778495222330093, 360261.778495222388301 ], [ 2049406.527579470071942, 360068.779716227028985 ], [ 2049490.703500043135136, 359990.703500043193344 ], [ 2049375.0, 359875.0 ], [ 2049375.0, 359625.0 ], [ 2049625.0, 359625.0 ], [ 2049625.0, 359375.0 ], [ 2049875.0, 359375.0 ], [ 2049875.0, 359125.0 ], [ 2049875.0, 358875.0 ], [ 2049875.0, 358625.0 ], [ 2049625.0, 358625.0 ], [ 2049375.0, 358625.0 ], [ 2049125.0, 358625.0 ], [ 2048875.0, 358625.0 ], [ 2048875.0, 358375.0 ], [ 2048875.0, 358125.0 ], [ 2048625.0, 358125.0 ], [ 2048625.0, 357875.0 ], [ 2048375.0, 357875.0 ], [ 2048375.0, 358125.0 ], [ 2048375.0, 358375.0 ], [ 2048125.0, 358375.0 ], [ 2048125.0, 358125.0 ], [ 2048125.0, 357875.0 ], [ 2047875.0, 357875.0 ], [ 2047875.0, 358125.0 ], [ 2047625.0, 358125.0 ], [ 2047625.0, 358375.0 ], [ 2047375.0, 358375.0 ] ] ], [ [ [ 2071375.0, 368947.085895284602884 ], [ 2071375.0, 368625.0 ], [ 2071125.0, 368625.0 ], [ 2071038.647045284276828, 368711.352954715664964 ], [ 2071248.724846909986809, 368893.626341421972029 ], [ 2071375.0, 368947.085895284602884 ] ] ] ] } },
{ "type": "Feature", "properties": { "classification": "pluvial", "mesh_name": "BaldEagleCr", "cell_id": 63, "area_acres": 65.757417196410884 }, "geometry": { "type": "MultiPolygon", "coordinates": [ [ [ [ 1969731.653768708230928, 289768.346231291885488 ], [ 1969588.437905200058594, 289578.114005430019461 ], [ 1969282.045979656279087, 289782.045979656279087 ], [ 1968981.852391151245683, 289981.852391151245683 ], [ 1968875.0, 290052.972473834874108 ], [ 1968531.562008393695578, 290281.562008393579163 ], [ 1968231.368419888429344, 290481.368419888545759 ], [ 1968125.0, 290552.166375235130545 ], [ 1967781.078037130879238, 290781.078037130879238 ], [ 1967480.884448625845835, 290980.884448625845835 ], [ 1967375.0, 291051.360276635328773 ], [ 1967030.594065868295729, 291280.594065868179314 ], [ 1966730.400477363029495, 291480.40047736314591 ], [ 1966625.0, 291550.55417803558521 ], [ 1966280.110094605479389, 291780.110094605479389 ], [ 1965981.774563469924033, 291978.679798224009573 ], [ 1966276.481022683903575, 292223.518977316096425 ], [ 1966549.587168198544532, 292450.412831801455468 ], [ 1966875.0, 292720.762558158952743 ], [ 1966875.0, 292375.0 ], [ 1967125.0, 292375.0 ], [ 1967125.0, 292625.0 ], [ 1967375.0, 292625.0 ], [ 1967375.0, 292875.0 ], [ 1967625.0, 292875.0 ], [ 1967625.0, 293125.0 ], [ 1967625.0, 293375.0 ], [ 1967875.0, 293375.0 ], [ 1968125.0, 293375.0 ], [ 1968375.0, 293375.0 ], [ 1968375.0, 293625.0 ], [ 1968375.0, 293875.0 ], [ 1968625.0, 293875.0 ], [ 1968625.0, 294125.0 ], [ 1968875.0, 294125.0 ], [ 1968875.0, 294375.0 ], [ 1969125.0, 294375.0 ], [ 1969125.0, 294625.0 ], [ 1969375.0, 294625.0 ], [ 1969375.0, 294875.0 ], [ 1969375.0, 295125.0 ], [ 1969625.0, 295125.0 ], [ 1969875.0, 295125.0 ], [ 1969875.0, 294875.0 ], [ 1970125.0, 294875.0 ], [ 1970125.0, 295125.0 ], [ 1970375.0, 295125.0 ], [ 1970375.0, 294875.0 ], [ 1970375.0, 294625.0 ], [ 1970625.0, 294625.0 ], [ 1970625.0, 294375.0 ], [ 1970875.0, 294375.0 ], [ 1970875.0, 294125.0 ], [ 1971125.0, 294125.0 ], [ 1971375.0, 294125.0 ], [ 1971625.0, 294125.0 ], [ 1971625.0, 293875.0 ], [ 1971875.0, 293875.0 ], [ 1971875.0, 293625.0 ], [ 1972125.0, 293625.0 ], [ 1972375.0, 293625.0 ], [ 1972625.0, 293625.0 ], [ 1972875.0, 293625.0 ], [ 1973125.0, 293625.0 ], [ 1973125.0, 293375.0 ], [ 1973375.0, 293375.0 ], [ 1973625.0, 293375.0 ], [ 1973625.0, 293625.0 ], [ 1973875.0, 293625.0 ], [ 1974125.0, 293625.0 ], [ 1974375.0, 293625.0 ], [ 1974625.0, 293625.0 ], [ 1974875.0, 293625.0 ], [ 1975125.0, 293625.0 ], [ 1975375.0, 293625.0 ], [ 1975625.0, 293625.0 ], [ 1975875.0, 293625.0 ], [ 1975875.0, 293875.0 ], [ 1976125.0, 293875.0 ], [ 1976375.0, 293875.0 ], [ 1976625.0, 293875.0 ], [ 1976875.0, 293875.0 ], [ 1977125.0, 293875.0 ], [ 1977125.0, 294125.0 ], [ 1977375.0, 294125.0 ], [ 1977625.0, 294125.0 ], [ 1977875.0, 294125.0 ], [ 1977875.0, 294375.0 ], [ 1977875.0, 294625.0 ], [ 1978125.0, 294625.0 ], [ 1978125.0, 294875.0 ], [ 1978375.0, 294875.0 ], [ 1978375.0, 295125.0 ], [ 1978625.0, 295125.0 ], [ 1978625.0, 295375.0 ], [ 1978875.0, 295375.0 ], [ 1979125.0, 295375.0 ], [ 1979125.0, 295625.0 ], [ 1979125.0, 295875.0 ], [ 1979375.0, 295875.0 ], [ 1979375.0, 296125.0 ], [ 1979625.0, 296125.0 ], [ 1979625.0, 296375.0 ], [ 1979875.0, 296375.0 ], [ 1979875.0, 296625.0 ], [ 1980125.0, 296625.0 ], [ 1980375.0, 296625.0 ], [ 1980375.0, 296375.0 ], [ 1980480.477373670320958, 296269.522626329620834 ], [ 1980196.693589886883274, 296053.306410113233142 ], [ 1979981.234249880071729, 295889.146912964992225 ], [ 1979875.0, 295790.239163076970726 ], [ 1979789.429719120729715, 295710.570280879270285 ], [ 1979530.501147691626102, 295469.498852308257483 ], [ 1979271.572576262755319, 295228.427423737244681 ], [ 1979012.644004833884537, 294987.355995166231878 ], [ 1979012.142206819960847, 294986.888803911977448 ], [ 1978745.992334429407492, 294754.0076655705343 ], [ 1978479.325667762663215, 294520.674332237278577 ], [ 1978212.659001095918939, 294287.340998904081061 ], [ 1977945.992334429174662, 294054.007665570825338 ], [ 1977675.463526739971712, 293817.294958842976484 ], [ 1977625.0, 293795.785914658743422 ], [ 1977505.253554089926183, 293744.746445910132024 ], [ 1977125.0, 293582.671160560392309 ], [ 1976979.39148512436077, 293520.608514875697438 ], [ 1976625.0, 293369.556406462041195 ], [ 1976375.0, 293262.999029412865639 ], [ 1976278.242059836862609, 293221.757940163079184 ], [ 1975875.0, 293049.884275314572733 ], [ 1975752.379990871297196, 292997.620009128586389 ], [ 1975637.02853962010704, 292948.45381679199636 ], [ 1975375.0, 292910.633214321394917 ], [ 1975375.0, 293125.0 ], [ 1975125.0, 293125.0 ], [ 1975125.0, 292874.548783856793307 ], [ 1974875.0, 292838.464353392249905 ], [ 1974875.0, 293125.0 ], [ 1974625.0, 293125.0 ], [ 1974625.0, 292802.379922927648295 ], [ 1974375.0, 292766.295492463046685 ], [ 1974125.0, 292730.211061998503283 ], [ 1974125.0, 293125.0 ], [ 1973875.0, 293125.0 ], [ 1973625.0, 293125.0 ], [ 1973625.0, 292875.0 ], [ 1973375.0, 292875.0 ], [ 1973191.671768725616857, 292691.671768725675065 ], [ 1973125.0, 292696.610418260970619 ], [ 1972875.0, 292715.128936779568903 ], [ 1972625.0, 292733.647455298167188 ], [ 1972375.0, 292752.165973816765472 ], [ 1972125.0, 292770.684492335363757 ], [ 1971875.0, 292789.203010853962041 ], [ 1971827.494301399914548, 292792.721951491024811 ], [ 1971778.426443077158183, 292721.573556922958232 ], [ 1971711.823990026721731, 292625.0 ], [ 1971472.303994097281247, 292277.696005902718753 ], [ 1971268.222361444029957, 291981.777638555911835 ], [ 1971125.0, 292125.0 ], [ 1971125.0, 292375.0 ], [ 1971125.0, 292625.0 ], [ 1970875.0, 292625.0 ], [ 1970875.0, 292875.0 ], [ 1970625.0, 292875.0 ], [ 1970625.0, 293125.0 ], [ 1970375.0, 293125.0 ], [ 1970375.0, 293375.0 ], [ 1970125.0, 293375.0 ], [ 1969875.0, 293375.0 ], [ 1969875.0, 293125.0 ], [ 1970125.0, 293125.0 ], [ 1970125.0, 292875.0 ], [ 1969875.0, 292875.0 ], [ 1969875.0, 292625.0 ], [ 1969625.0, 292625.0 ], [ 1969625.0, 292375.0 ], [ 1969625.0, 292125.0 ], [ 1969875.0, 292125.0 ], [ 1969875.0, 291875.0 ], [ 1969875.0, 291625.0 ], [ 1969875.0, 291375.0 ], [ 1969875.0, 291125.0 ], [ 1969875.0, 290875.0 ], [ 1969625.0, 290875.0 ], [ 1969375.0, 290875.0 ], [ 1969375.0, 290625.0 ], [ 1969625.0, 290625.0 ], [ 1969625.0, 290375.0 ], [ 1969625.0, 290125.0 ], [ 1969625.0, 289875.0 ], [ 1969731.653768708230928, 289768.346231291885488 ] ], [ [ 1968125.0, 292125.0 ], [ 1967875.0, 292125.0 ], [ 1967875.0, 291875.0 ], [ 1968125.0, 291875.0 ], [ 1968375.0, 291875.0 ], [ 1968375.0, 292125.0 ], [ 1968125.0, 292125.0 ] ], [ [ 1967625.0, 291625.0 ], [ 1967375.0, 291625.0 ], [ 1967375.0, 291375.0 ], [ 1967625.0, 291375.0 ], [ 1967625.0, 291625.0 ] ], [ [ 1967375.0, 291875.0 ], [ 1967125.0, 291875.0 ], [ 1967125.0, 291625.0 ], [ 1967375.0, 291625.0 ], [ 1967375.0, 291875.0 ] ], [ [ 1967125.0, 292125.0 ], [ 1966875.0, 292125.0 ], [ 1966875.0, 291875.0 ], [ 1967125.0, 291875.0 ], [ 1967125.0, 292125.0 ] ], [ [ 1971125.0, 293875.0 ], [ 1971125.0, 293625.0 ], [ 1971375.0, 293625.0 ], [ 1971375.0, 293875.0 ], [ 1971125.0, 293875.0 ] ] ], [ [ [ 1973875.0, 296625.0 ], [ 1973625.0, 296625.0 ], [ 1973625.0, 296375.0 ], [ 1973375.0, 296375.0 ], [ 1973125.0, 296375.0 ], [ 1973125.0, 296125.0 ], [ 1972875.0, 296125.0 ], [ 1972625.0, 296125.0 ], [ 1972375.0, 296125.0 ], [ 1972375.0, 295875.0 ], [ 1972125.0, 295875.0 ], [ 1971875.0, 295875.0 ], [ 1971875.0, 295625.0 ], [ 1971625.0, 295625.0 ], [ 1971375.0, 295625.0 ], [ 1971375.0, 295375.0 ], [ 1971125.0, 295375.0 ], [ 1970875.0, 295375.0 ], [ 1970625.0, 295375.0 ], [ 1970375.0, 295375.0 ], [ 1970375.0, 295625.0 ], [ 1970125.0, 295625.0 ], [ 1970125.0, 295875.0 ], [ 1970125.0, 296110.364085177250672 ], [ 1970375.0, 296152.736966533353552 ], [ 1970625.0, 296195.109847889456432 ], [ 1970858.402258340036497, 296234.669552693027072 ], [ 1970875.0, 296241.829362820833921 ], [ 1970968.037020494928584, 296281.962979504955001 ], [ 1971375.0, 296457.515637330594473 ], [ 1971492.009623234858736, 296507.990376765199471 ], [ 1971875.0, 296673.201911840413231 ], [ 1972015.982225974556059, 296734.017774025443941 ], [ 1972375.0, 296888.888186350173783 ], [ 1972562.667575439903885, 296969.842826735985 ], [ 1972625.0, 296995.635554140084423 ], [ 1972716.50168121792376, 297033.498318781959824 ], [ 1973125.0, 297202.532105863792822 ], [ 1973246.989486096426845, 297253.010513903573155 ], [ 1973531.759618500014767, 297370.846430759993382 ], [ 1973757.81749333976768, 297257.817493339825887 ], [ 1973875.0, 297199.226240009593312 ], [ 1974257.817493339534849, 297007.817493339476641 ], [ 1974375.0, 296949.226240009069443 ], [ 1974400.600760550005361, 296936.425859734008554 ], [ 1974625.0, 296976.70264630188467 ], [ 1974875.0, 297021.574441173637751 ], [ 1974962.686886831186712, 297037.313113168929704 ], [ 1975375.0, 297111.318030917085707 ], [ 1975625.0, 297156.189825788780581 ], [ 1975703.862473630113527, 297170.344628748018295 ], [ 1976019.423931503202766, 297019.423931503144559 ], [ 1976125.0, 296968.931029178202152 ], [ 1976472.452714669983834, 296802.757991725986358 ], [ 1976500.0, 296806.201402392238379 ], [ 1976875.0, 296853.076402392413002 ], [ 1977007.124186699977145, 296869.591925729997456 ], [ 1977024.736170638352633, 296975.26382936158916 ], [ 1977049.692199077922851, 297125.0 ], [ 1977091.358865744201466, 297375.0 ], [ 1977133.025532410480082, 297625.0 ], [ 1977203.307599208550528, 298046.692400791565888 ], [ 1977207.625988709973171, 298072.602737800974865 ], [ 1977486.949448623461649, 298263.050551376480144 ], [ 1977784.246745922137052, 298465.753254077921156 ], [ 1977942.799262759974226, 298573.85724282998126 ], [ 1977985.48404832277447, 298764.515951677167322 ], [ 1978010.219283021986485, 298875.0 ], [ 1978066.189432275714353, 299125.0 ], [ 1978122.159581529675052, 299375.0 ], [ 1978214.142584908287972, 299785.857415091595612 ], [ 1978375.0, 299625.0 ], [ 1978625.0, 299625.0 ], [ 1978875.0, 299625.0 ], [ 1978875.0, 299375.0 ], [ 1979125.0, 299375.0 ], [ 1979375.0, 299375.0 ], [ 1979375.0, 299125.0 ], [ 1979625.0, 299125.0 ], [ 1979875.0, 299125.0 ], [ 1980125.0, 299125.0 ], [ 1980125.0, 298875.0 ], [ 1979875.0, 298875.0 ], [ 1979875.0, 298625.0 ], [ 1979875.0, 298375.0 ], [ 1979625.0, 298375.0 ], [ 1979375.0, 298375.0 ], [ 1979375.0, 298125.0 ], [ 1979125.0, 298125.0 ], [ 1979125.0, 297875.0 ], [ 1979125.0, 297625.0 ], [ 1978875.0, 297625.0 ], [ 1978625.0, 297625.0 ], [ 1978375.0, 297625.0 ], [ 1978375.0, 297875.0 ], [ 1978125.0, 297875.0 ], [ 1977875.0, 297875.0 ], [ 1977875.0, 297625.0 ], [ 1977875.0, 297375.0 ], [ 1977875.0, 297125.0 ], [ 1978125.0, 297125.0 ], [ 1978125.0, 296875.0 ], [ 1977875.0, 296875.0 ], [ 1977875.0, 296625.0 ], [ 1977625.0, 296625.0 ], [ 1977625.0, 296375.0 ], [ 1977375.0, 296375.0 ], [ 1977125.0, 296375.0 ], [ 1977125.0, 296125.0 ], [ 1976875.0, 296125.0 ], [ 1976875.0, 295875.0 ], [ 1976875.0, 295625.0 ], [ 1976625.0, 295625.0 ], [ 1976625.0, 295375.0 ], [ 1976375.0, 295375.0 ], [ 1976375.0, 295125.0 ], [ 1976125.0, 295125.0 ], [ 1975875.0, 295125.0 ], [ 1975875.0, 295375.0 ], [ 1975875.0, 295625.0 ], [ 1976125.0, 295625.0 ], [ 1976125.0, 295875.0 ], [ 1975875.0, 295875.0 ], [ 1975625.0, 295875.0 ], [ 1975625.0, 296125.0 ], [ 1975375.0, 296125.0 ], [ 1975125.0, 296125.0 ], [ 1974875.0, 296125.0 ], [ 1974625.0, 296125.0 ], [ 1974625.0, 296375.0 ], [ 1974375.0, 296375.0 ], [ 1974125.0, 296375.0 ], [ 1973875.0, 296375.0 ], [ 1973875.0, 296625.0 ] ] ], [ [ [ 1981625.0, 301375.0 ], [ 1981875.0, 301375.0 ], [ 1982125.0, 301375.0 ], [ 1982125.0, 301625.0 ], [ 1982125.0, 301875.0 ], [ 1981875.0, 301875.0 ], [ 1981625.0, 301875.0 ], [ 1981375.0, 301875.0 ], [ 1981125.0, 301875.0 ], [ 1981125.0, 301625.0 ], [ 1981125.0, 301375.0 ], [ 1980875.0, 301375.0 ], [ 1980875.0, 301125.0 ], [ 1980625.0, 301125.0 ], [ 1980625.0, 300875.0 ], [ 1980625.0, 300625.0 ], [ 1980375.0, 300625.0 ], [ 1980375.0, 300375.0 ], [ 1980125.0, 300375.0 ], [ 1979875.0, 300375.0 ], [ 1979625.0, 300375.0 ], [ 1979375.0, 300375.0 ], [ 1979375.0, 300125.0 ], [ 1979125.0, 300125.0 ], [ 1978875.0, 300125.0 ], [ 1978625.0, 300125.0 ], [ 1978290.070029290858656, 300125.0 ], [ 1978346.040178544819355, 300375.0 ], [ 1978402.010327798547223, 300625.0 ], [ 1978444.053767790086567, 300812.794031961995643 ], [ 1978625.0, 300890.3424171951483 ], [ 1978875.0, 300997.485274338396266 ], [ 1978964.260307963006198, 301035.739692036993802 ], [ 1979375.0, 301211.770988624950405 ], [ 1979379.728843840071931, 301213.797635985014495 ], [ 1979470.505592635832727, 301279.494407364283688 ], [ 1979760.576234282227233, 301489.423765717714559 ], [ 1980050.64687592885457, 301699.35312407114543 ], [ 1980375.0, 301934.093333661789075 ], [ 1980485.752838398562744, 302014.24716160132084 ], [ 1980775.823480045190081, 302224.176519954751711 ], [ 1981125.0, 302476.881869141827337 ], [ 1981210.929442515131086, 302539.070557484927122 ], [ 1981501.000084161525592, 302748.9999158384162 ], [ 1981718.916533980052918, 302906.709983377018943 ], [ 1981776.006001422647387, 302973.993998577294406 ], [ 1982005.514198143966496, 303244.485801855975296 ], [ 1982235.022394865285605, 303514.977605134656187 ], [ 1982375.0, 303375.0 ], [ 1982625.0, 303375.0 ], [ 1982625.0, 303625.0 ], [ 1982464.530591586604714, 303785.469408413337078 ], [ 1982540.495942023815587, 303875.0 ], [ 1982808.792886668583378, 304191.207113331358414 ], [ 1983038.301083389902487, 304461.698916610039305 ], [ 1983267.809280111221597, 304732.190719888720196 ], [ 1983497.317476832540706, 305002.682523167401087 ], [ 1983726.825673553859815, 305273.174326446081977 ], [ 1983956.333870275178924, 305543.666129724762868 ], [ 1984025.344426872441545, 305625.0 ], [ 1984300.596165357157588, 305949.403834642784204 ], [ 1984530.104362078476697, 306219.895637921465095 ], [ 1984759.612558799795806, 306490.387441200145986 ], [ 1984989.120755521114916, 306760.879244478826877 ], [ 1985218.628952242434025, 307031.371047757507768 ], [ 1985298.071699599735439, 307125.0 ], [ 1985461.61683820001781, 307317.749627635988872 ], [ 1985570.665596464881673, 307429.334403535176534 ], [ 1985817.792033246019855, 307682.207966753921937 ], [ 1986064.918470027390867, 307935.08152997266734 ], [ 1986312.044906808529049, 308187.955093191354536 ], [ 1986559.171343589900061, 308440.828656410099939 ], [ 1986806.297780371038243, 308693.702219628845342 ], [ 1986898.546419280115515, 308788.096175722021144 ], [ 1987047.582258242648095, 308952.417741757293697 ], [ 1987285.387136291712523, 309214.612863708287477 ], [ 1987523.19201434077695, 309476.80798565922305 ], [ 1987625.0, 309375.0 ], [ 1987875.0, 309375.0 ], [ 1987875.0, 309625.0 ], [ 1987760.996892389841378, 309739.003107610158622 ], [ 1987998.801770438905805, 310001.198229561152402 ], [ 1988201.80813235999085, 310225.025756807008293 ], [ 1988234.732882234733552, 310265.267117765208241 ], [ 1988459.732882234500721, 310540.267117765382864 ], [ 1988529.059785881079733, 310625.0 ], [ 1988797.23288223426789, 310952.767117765673902 ], [ 1988875.0, 310875.0 ], [ 1989125.0, 310875.0 ], [ 1989125.0, 311125.0 ], [ 1989022.23288223403506, 311227.767117765906733 ], [ 1989247.232882233802229, 311502.767117766081356 ], [ 1989472.232882233802229, 311777.767117766314186 ], [ 1989625.0, 311625.0 ], [ 1989875.0, 311625.0 ], [ 1990125.0, 311625.0 ], [ 1990125.0, 311375.0 ], [ 1990375.0, 311375.0 ], [ 1990375.0, 311125.0 ], [ 1990125.0, 311125.0 ], [ 1990125.0, 310875.0 ], [ 1989875.0, 310875.0 ], [ 1989875.0, 310625.0 ], [ 1989875.0, 310375.0 ], [ 1989625.0, 310375.0 ], [ 1989625.0, 310125.0 ], [ 1989375.0, 310125.0 ], [ 1989375.0, 309875.0 ], [ 1989125.0, 309875.0 ], [ 1989125.0, 309625.0 ], [ 1989125.0, 309375.0 ], [ 1988875.0, 309375.0 ], [ 1988625.0, 309375.0 ], [ 1988375.0, 309375.0 ], [ 1988375.0, 309125.0 ], [ 1988125.0, 309125.0 ], [ 1987875.0, 309125.0 ], [ 1987875.0, 308875.0 ], [ 1988125.0, 308875.0 ], [ 1988125.0, 308625.0 ], [ 1987875.0, 308625.0 ], [ 1987875.0, 308375.0 ], [ 1987875.0, 308125.0 ], [ 1987875.0, 307875.0 ], [ 1988125.0, 307875.0 ], [ 1988375.0, 307875.0 ], [ 1988625.0, 307875.0 ], [ 1988875.0, 307875.0 ], [ 1989125.0, 307875.0 ], [ 1989375.0, 307875.0 ], [ 1989625.0, 307875.0 ], [ 1989625.0, 307625.0 ], [ 1989375.0, 307625.0 ], [ 1989375.0, 307375.0 ], [ 1989125.0, 307375.0 ], [ 1989125.0, 307125.0 ], [ 1988875.0, 307125.0 ], [ 1988875.0, 306875.0 ], [ 1988625.0, 306875.0 ], [ 1988375.0, 306875.0 ], [ 1988375.0, 307125.0 ], [ 1988125.0, 307125.0 ], [ 1987875.0, 307125.0 ], [ 1987625.0, 307125.0 ], [ 1987625.0, 306875.0 ], [ 1987875.0, 306875.0 ], [ 1987875.0, 306625.0 ], [ 1987875.0, 306375.0 ], [ 1988125.0, 306375.0 ], [ 1988125.0, 306125.0 ], [ 1988125.0, 305875.0 ], [ 1987875.0, 305875.0 ], [ 1987875.0, 305625.0 ], [ 1987625.0, 305625.0 ], [ 1987625.0, 305375.0 ], [ 1987375.0, 305375.0 ], [ 1987375.0, 305625.0 ], [ 1987375.0, 305875.0 ], [ 1987125.0, 305875.0 ], [ 1987125.0, 306125.0 ], [ 1986875.0, 306125.0 ], [ 1986625.0, 306125.0 ], [ 1986625.0, 306375.0 ], [ 1986875.0, 306375.0 ], [ 1986875.0, 306625.0 ], [ 1986875.0, 306875.0 ], [ 1986625.0, 306875.0 ], [ 1986625.0, 306625.0 ], [ 1986375.0, 306625.0 ], [ 1986375.0, 306375.0 ], [ 1986125.0, 306375.0 ], [ 1986125.0, 306125.0 ], [ 1986125.0, 305875.0 ], [ 1985875.0, 305875.0 ], [ 1985625.0, 305875.0 ], [ 1985375.0, 305875.0 ], [ 1985375.0, 306125.0 ], [ 1985125.0, 306125.0 ], [ 1984875.0, 306125.0 ], [ 1984875.0, 305875.0 ], [ 1984875.0, 305625.0 ], [ 1985125.0, 305625.0 ], [ 1985125.0, 305375.0 ], [ 1985375.0, 305375.0 ], [ 1985375.0, 305125.0 ], [ 1985125.0, 305125.0 ], [ 1984875.0, 305125.0 ], [ 1984625.0, 305125.0 ], [ 1984625.0, 304875.0 ], [ 1984375.0, 304875.0 ], [ 1984375.0, 304625.0 ], [ 1984125.0, 304625.0 ], [ 1984125.0, 304375.0 ], [ 1983875.0, 304375.0 ], [ 1983875.0, 304125.0 ], [ 1983875.0, 303875.0 ], [ 1983875.0, 303625.0 ], [ 1983625.0, 303625.0 ], [ 1983625.0, 303375.0 ], [ 1983375.0, 303375.0 ], [ 1983375.0, 303125.0 ], [ 1983125.0, 303125.0 ], [ 1982875.0, 303125.0 ], [ 1982875.0, 302875.0 ], [ 1982625.0, 302875.0 ], [ 1982625.0, 302625.0 ], [ 1982375.0, 302625.0 ], [ 1982375.0, 302375.0 ], [ 1982625.0, 302375.0 ], [ 1982875.0, 302375.0 ], [ 1983125.0, 302375.0 ], [ 1983125.0, 302625.0 ], [ 1983375.0, 302625.0 ], [ 1983375.0, 302875.0 ], [ 1983625.0, 302875.0 ], [ 1983875.0, 302875.0 ], [ 1983875.0, 303125.0 ], [ 1984125.0, 303125.0 ], [ 1984375.0, 303125.0 ], [ 1984625.0, 303125.0 ], [ 1984625.0, 302875.0 ], [ 1984375.0, 302875.0 ], [ 1984375.0, 302625.0 ], [ 1984125.0, 302625.0 ], [ 1983875.0, 302625.0 ], [ 1983875.0, 302375.0 ], [ 1983875.0, 302125.0 ], [ 1983625.0, 302125.0 ], [ 1983625.0, 301875.0 ], [ 1983375.0, 301875.0 ], [ 1983375.0, 301625.0 ], [ 1983125.0, 301625.0 ], [ 1983125.0, 301375.0 ], [ 1982875.0, 301375.0 ], [ 1982625.0, 301375.0 ], [ 1982625.0, 301125.0 ], [ 1982625.0, 300875.0 ], [ 1982375.0, 300875.0 ], [ 1982375.0, 300625.0 ], [ 1982125.0, 300625.0 ], [ 1982125.0, 300375.0 ], [ 1981875.0, 300375.0 ], [ 1981875.0, 300125.0 ], [ 1981625.0, 300125.0 ], [ 1981375.0, 300125.0 ], [ 1981375.0, 300375.0 ], [ 1981375.0, 300625.0 ], [ 1981375.0, 300875.0 ], [ 1981625.0, 300875.0 ], [ 1981625.0, 301125.0 ], [ 1981625.0, 301375.0 ] ], [ [ 1988625.0, 307125.0 ], [ 1988625.0, 307375.0 ], [ 1988375.0, 307375.0 ], [ 1988375.0, 307125.0 ], [ 1988625.0, 307125.0 ] ], [ [ 1984625.0, 305625.0 ], [ 1984625.0, 305375.0 ], [ 1984875.0, 305375.0 ], [ 1984875.0, 305625.0 ], [ 1984625.0, 305625.0 ] ], [ [ 1983625.0, 302375.0 ], [ 1983375.0, 302375.0 ], [ 1983375.0, 302125.0 ], [ 1983625.0, 302125.0 ], [ 1983625.0, 302375.0 ] ], [ [ 1982375.0, 301125.0 ], [ 1982125.0, 301125.0 ], [ 1982125.0, 300875.0 ], [ 1982375.0, 300875.0 ], [ 1982375.0, 301125.0 ] ], [ [ 1981625.0, 302375.0 ], [ 1981625.0, 302125.0 ], [ 1981875.0, 302125.0 ], [ 1982125.0, 302125.0 ], [ 1982125.0, 302375.0 ], [ 1982125.0, 302625.0 ], [ 1981875.0, 302625.0 ], [ 1981875.0, 302375.0 ], [ 1981625.0, 302375.0 ] ], [ [ 1983125.0, 302125.0 ], [ 1982875.0, 302125.0 ], [ 1982625.0, 302125.0 ], [ 1982625.0, 301875.0 ], [ 1982875.0, 301875.0 ], [ 1983125.0, 301875.0 ], [ 1983125.0, 302125.0 ] ], [ [ 1985875.0, 306625.0 ], [ 1986125.0, 306625.0 ], [ 1986125.0, 306875.0 ], [ 1985875.0, 306875.0 ], [ 1985875.0, 306625.0 ] ], [ [ 1987625.0, 308125.0 ], [ 1987625.0, 308375.0 ], [ 1987375.0, 308375.0 ], [ 1987375.0, 308125.0 ], [ 1987625.0, 308125.0 ] ] ], [ [ [ 1982625.0, 298125.0 ], [ 1982625.0, 298375.0 ], [ 1982625.0, 298625.0 ], [ 1982875.0, 298625.0 ], [ 1983125.0, 298625.0 ], [ 1983375.0, 298625.0 ], [ 1983375.0, 298875.0 ], [ 1983625.0, 298875.0 ], [ 1983625.0, 299125.0 ], [ 1983875.0, 299125.0 ], [ 1983875.0, 299375.0 ], [ 1984125.0, 299375.0 ], [ 1984125.0, 299625.0 ], [ 1984375.0, 299625.0 ], [ 1984375.0, 299875.0 ], [ 1984625.0, 299875.0 ], [ 1984625.0, 300125.0 ], [ 1984875.0, 300125.0 ], [ 1984875.0, 300375.0 ], [ 1985125.0, 300375.0 ], [ 1985375.0, 300375.0 ], [ 1985375.0, 300625.0 ], [ 1985625.0, 300625.0 ], [ 1985625.0, 300875.0 ], [ 1985875.0, 300875.0 ], [ 1985875.0, 301125.0 ], [ 1986125.0, 301125.0 ], [ 1986125.0, 301375.0 ], [ 1986375.0, 301375.0 ], [ 1986625.0, 301375.0 ], [ 1986625.0, 301625.0 ], [ 1986875.0, 301625.0 ], [ 1986875.0, 301875.0 ], [ 1987125.0, 301875.0 ], [ 1987125.0, 302125.0 ], [ 1987375.0, 302125.0 ], [ 1987375.0, 302375.0 ], [ 1987625.0, 302375.0 ], [ 1987875.0, 302375.0 ], [ 1987875.0, 302625.0 ], [ 1988125.0, 302625.0 ], [ 1988125.0, 302875.0 ], [ 1988375.0, 302875.0 ], [ 1988375.0, 303125.0 ], [ 1988625.0, 303125.0 ], [ 1988875.0, 303125.0 ], [ 1988875.0, 303375.0 ], [ 1989125.0, 303375.0 ], [ 1989125.0, 303625.0 ], [ 1989375.0, 303625.0 ], [ 1989625.0, 303625.0 ], [ 1989625.0, 303875.0 ], [ 1989875.0, 303875.0 ], [ 1989875.0, 304125.0 ], [ 1990125.0, 304125.0 ], [ 1990125.0, 304375.0 ], [ 1990375.0, 304375.0 ], [ 1990625.0, 304375.0 ], [ 1990625.0, 304625.0 ], [ 1990875.0, 304625.0 ], [ 1990875.0, 304875.0 ], [ 1991125.0, 304875.0 ], [ 1991125.0, 305125.0 ], [ 1991375.0, 305125.0 ], [ 1991375.0, 305375.0 ], [ 1991625.0, 305375.0 ], [ 1991875.0, 305375.0 ], [ 1991875.0, 305625.0 ], [ 1992125.0, 305625.0 ], [ 1992125.0, 305875.0 ], [ 1992125.0, 306125.0 ], [ 1992375.0, 306125.0 ], [ 1992625.0, 306125.0 ], [ 1992625.0, 306375.0 ], [ 1992625.0, 306625.0 ], [ 1992875.0, 306625.0 ], [ 1992875.0, 306875.0 ], [ 1993125.0, 306875.0 ], [ 1993125.0, 307125.0 ], [ 1993125.0, 307375.0 ], [ 1993125.0, 307625.0 ], [ 1993125.0, 307875.0 ], [ 1993125.0, 308125.0 ], [ 1993375.0, 308125.0 ], [ 1993375.0, 308375.0 ], [ 1993375.0, 308625.0 ], [ 1993375.0, 308875.0 ], [ 1993375.0, 309125.0 ], [ 1993375.0, 309375.0 ], [ 1993625.0, 309375.0 ], [ 1993875.0, 309375.0 ], [ 1993875.0, 309625.0 ], [ 1994125.0, 309625.0 ], [ 1994375.0, 309625.0 ], [ 1994625.0, 309625.0 ], [ 1994875.0, 309625.0 ], [ 1994875.0, 309375.0 ], [ 1995125.0, 309375.0 ], [ 1995375.0, 309375.0 ], [ 1995625.0, 309375.0 ], [ 1995625.0, 309125.0 ], [ 1995875.0, 309125.0 ], [ 1995875.0, 308875.0 ], [ 1996027.641929935663939, 308722.358070064452477 ], [ 1995965.981818894622847, 308625.0 ], [ 1995921.12750981003046, 308554.177406709000934 ], [ 1995625.0, 308311.449939651589375 ], [ 1995522.536519650835544, 308227.463480349106248 ], [ 1995247.761744876159355, 308002.238255123840645 ], [ 1995125.0, 308125.0 ], [ 1994875.0, 308125.0 ], [ 1994875.0, 307875.0 ], [ 1994625.0, 307875.0 ], [ 1994625.0, 307625.0 ], [ 1994698.212195326574147, 307551.787804673425853 ], [ 1994375.0, 307286.859775717195589 ], [ 1994286.050033164443448, 307213.949966835556552 ], [ 1994125.0, 307375.0 ], [ 1993875.0, 307375.0 ], [ 1993875.0, 307125.0 ], [ 1994011.275258389534429, 306988.724741610349156 ], [ 1993736.50048361485824, 306763.49951638514176 ], [ 1993461.725708840182051, 306538.274291159934364 ], [ 1993186.950934065273032, 306313.049065934668761 ], [ 1992875.0, 306057.351578995934688 ], [ 1992774.788771903142333, 305975.211228096857667 ], [ 1992500.013997128466144, 305749.986002871650271 ], [ 1992225.239222353557125, 305524.760777646384668 ], [ 1991950.464447578880936, 305299.535552421177272 ], [ 1991625.0, 305032.761415061540902 ], [ 1991538.302285416750237, 304961.697714583366178 ], [ 1991263.527510641841218, 304736.472489358158782 ], [ 1990988.752735867165029, 304511.247264132893179 ], [ 1990713.97796109225601, 304286.022038907685783 ], [ 1990439.203186317579821, 304060.796813682478387 ], [ 1990125.0, 303803.253218340221792 ], [ 1990027.041024155449122, 303722.958975844609085 ], [ 1989805.822548449970782, 303541.632356413989328 ], [ 1989751.42911317711696, 303498.570886822941247 ], [ 1989472.359345735283569, 303277.640654264832847 ], [ 1989193.289578293217346, 303056.710421706666239 ], [ 1988875.0, 302804.731172224448528 ], [ 1988774.684927130583674, 302725.315072869474534 ], [ 1988495.615159688750282, 302504.384840311307926 ], [ 1988216.54539224668406, 302283.454607753199525 ], [ 1987937.475624804850668, 302062.524375195091125 ], [ 1987625.0, 301815.147838891192805 ], [ 1987518.870973642216995, 301731.129026357841212 ], [ 1987239.801206200150773, 301510.198793799732812 ], [ 1986960.731438758317381, 301289.268561241624411 ], [ 1986625.0, 301023.481172224564943 ], [ 1986542.126787595683709, 300957.873212404374499 ], [ 1986263.057020153850317, 300736.942979846266098 ], [ 1985983.987252711784095, 300516.012747288157698 ], [ 1985704.917485269950703, 300295.08251472999109 ], [ 1985375.0, 300033.897838891251013 ], [ 1985286.31283410731703, 299963.687165892799385 ], [ 1985007.243066665250808, 299742.756933334690984 ], [ 1984728.173299223417416, 299521.826700776524376 ], [ 1984449.103531781584024, 299300.896468218415976 ], [ 1984191.772092120023444, 299097.175745152984746 ], [ 1984125.0, 299046.301770204328932 ], [ 1984027.774670965271071, 298972.225329034787137 ], [ 1983743.990887181600556, 298756.009112818399444 ], [ 1983460.207103397930041, 298539.792896602011751 ], [ 1983125.0, 298284.397008298314176 ], [ 1983034.531427722657099, 298215.468572277342901 ], [ 1982750.747643938986585, 297999.252356060955208 ], [ 1982466.9638601555489, 297783.036139844509307 ], [ 1982125.0, 297522.492246392357629 ], [ 1982041.288184480043128, 297458.711815519898664 ], [ 1981757.504400696605444, 297242.495599303510971 ], [ 1981473.720616912934929, 297026.279383087065071 ], [ 1981189.936833129264414, 296810.063166870677378 ], [ 1980875.0, 296570.111294009839185 ], [ 1980764.261157453991473, 296485.738842546066735 ], [ 1980625.0, 296625.0 ], [ 1980375.0, 296625.0 ], [ 1980375.0, 296875.0 ], [ 1980625.0, 296875.0 ], [ 1980625.0, 297125.0 ], [ 1980625.0, 297375.0 ], [ 1980875.0, 297375.0 ], [ 1980875.0, 297625.0 ], [ 1981125.0, 297625.0 ], [ 1981125.0, 297875.0 ], [ 1981375.0, 297875.0 ], [ 1981625.0, 297875.0 ], [ 1981625.0, 298125.0 ], [ 1981875.0, 298125.0 ], [ 1981875.0, 297875.0 ], [ 1982125.0, 297875.0 ], [ 1982375.0, 297875.0 ], [ 1982375.0, 298125.0 ], [ 1982625.0, 298125.0 ] ], [ [ 1995375.0, 309125.0 ], [ 1995375.0, 308875.0 ], [ 1995625.0, 308875.0 ], [ 1995625.0, 309125.0 ], [ 1995375.0, 309125.0 ] ] ], [ [ [ 1997625.0, 315375.0 ], [ 1997625.0, 315625.0 ], [ 1997625.0, 315875.0 ], [ 1997875.0, 315875.0 ], [ 1997875.0, 316125.0 ], [ 1997875.0, 316375.0 ], [ 1998125.0, 316375.0 ], [ 1998375.0, 316375.0 ], [ 1998375.0, 316125.0 ], [ 1998375.0, 315875.0 ], [ 1998125.0, 315875.0 ], [ 1998125.0, 315625.0 ], [ 1998125.0, 315375.0 ], [ 1998375.0, 315375.0 ], [ 1998625.0, 315375.0 ], [ 1998625.0, 315625.0 ], [ 1998875.0, 315625.0 ], [ 1998875.0, 315875.0 ], [ 1998625.0, 315875.0 ], [ 1998625.0, 316125.0 ], [ 1998625.0, 316375.0 ], [ 1998625.0, 316625.0 ], [ 1998875.0, 316625.0 ], [ 1998875.0, 316875.0 ], [ 1999125.0, 316875.0 ], [ 1999375.0, 316875.0 ], [ 1999375.0, 317125.0 ], [ 1999625.0, 317125.0 ], [ 1999875.0, 317125.0 ], [ 1999875.0, 317375.0 ], [ 1999875.0, 317625.0 ], [ 2000125.0, 317625.0 ], [ 2000125.0, 317875.0 ], [ 2000375.0, 317875.0 ], [ 2000375.0, 317625.0 ], [ 2000375.0, 317375.0 ], [ 2000625.0, 317375.0 ], [ 2000875.0, 317375.0 ], [ 2001125.0, 317375.0 ], [ 2001375.0, 317375.0 ], [ 2001375.0, 317125.0 ], [ 2001125.0, 317125.0 ], [ 2001125.0, 316875.0 ], [ 2000875.0, 316875.0 ], [ 2000875.0, 316625.0 ], [ 2000625.0, 316625.0 ], [ 2000625.0, 316375.0 ], [ 2000375.0, 316375.0 ], [ 2000375.0, 316125.0 ], [ 2000125.0, 316125.0 ], [ 2000125.0, 315875.0 ], [ 2000125.0, 315625.0 ], [ 1999875.0, 315625.0 ], [ 1999875.0, 315375.0 ], [ 1999625.0, 315375.0 ], [ 1999625.0, 315125.0 ], [ 1999375.0, 315125.0 ], [ 1999375.0, 314875.0 ], [ 1999125.0, 314875.0 ], [ 1999125.0, 314625.0 ], [ 1998875.0, 314625.0 ], [ 1998875.0, 314375.0 ], [ 1998625.0, 314375.0 ], [ 1998375.0, 314375.0 ], [ 1998125.0, 314375.0 ], [ 1997875.0, 314375.0 ], [ 1997625.0, 314375.0 ], [ 1997375.0, 314375.0 ], [ 1997125.0, 314375.0 ], [ 1996875.0, 314375.0 ], [ 1996875.0, 314625.0 ], [ 1997125.0, 314625.0 ], [ 1997375.0, 314625.0 ], [ 1997375.0, 314875.0 ], [ 1997375.0, 315125.0 ], [ 1997375.0, 315375.0 ], [ 1997625.0, 315375.0 ] ] ], [ [ [ 1995375.0, 317550.752425343205687 ], [ 1995445.872684899717569, 317554.127315100340638 ], [ 1995625.0, 317375.0 ], [ 1995875.0, 317375.0 ], [ 1995875.0, 317574.561949152790476 ], [ 1995921.12750981003046, 317576.758497238974087 ], [ 1996027.524240780156106, 317722.475759219843894 ], [ 1996238.533415092155337, 318011.46658490790287 ], [ 1996321.430829286342487, 318125.0 ], [ 1996555.047176559921354, 318444.952823439962231 ], [ 1996766.056350871920586, 318733.943649128021207 ], [ 1996977.065525183919817, 319022.934474816080183 ], [ 1997051.589559445157647, 319125.0 ], [ 1997293.579286651918665, 319456.420713348197751 ], [ 1997458.307991900015622, 319682.027418361976743 ], [ 1997511.426777554443106, 319738.573222445673309 ], [ 1997753.614277554210275, 319996.385722445789725 ], [ 1997995.801777553977445, 320254.198222445964348 ], [ 1998237.989277553977445, 320512.010722446138971 ], [ 1998480.176777553744614, 320769.823222446255386 ], [ 1998625.0, 320625.0 ], [ 1998625.0, 320375.0 ], [ 1998875.0, 320375.0 ], [ 1999125.0, 320375.0 ], [ 1999375.0, 320375.0 ], [ 1999375.0, 320125.0 ], [ 1999125.0, 320125.0 ], [ 1998875.0, 320125.0 ], [ 1998875.0, 319875.0 ], [ 1998625.0, 319875.0 ], [ 1998375.0, 319875.0 ], [ 1998125.0, 319875.0 ], [ 1998125.0, 319625.0 ], [ 1998125.0, 319375.0 ], [ 1998375.0, 319375.0 ], [ 1998375.0, 319125.0 ], [ 1998125.0, 319125.0 ], [ 1998125.0, 318875.0 ], [ 1997875.0, 318875.0 ], [ 1997875.0, 318625.0 ], [ 1997875.0, 318375.0 ], [ 1997625.0, 318375.0 ], [ 1997375.0, 318375.0 ], [ 1997375.0, 318125.0 ], [ 1997375.0, 317875.0 ], [ 1997625.0, 317875.0 ], [ 1997625.0, 317625.0 ], [ 1997625.0, 317375.0 ], [ 1997875.0, 317375.0 ], [ 1997875.0, 317125.0 ], [ 1997625.0, 317125.0 ], [ 1997625.0, 316875.0 ], [ 1997375.0, 316875.0 ], [ 1997375.0, 317125.0 ], [ 1997125.0, 317125.0 ], [ 1996875.0, 317125.0 ], [ 1996875.0, 316875.0 ], [ 1996625.0, 316875.0 ], [ 1996375.0, 316875.0 ], [ 1996125.0, 316875.0 ], [ 1995875.0, 316875.0 ], [ 1995875.0, 316625.0 ], [ 1995875.0, 316375.0 ], [ 1995875.0, 316125.0 ], [ 1995625.0, 316125.0 ], [ 1995625.0, 315875.0 ], [ 1995375.0, 315875.0 ], [ 1995125.0, 315875.0 ], [ 1994875.0, 315875.0 ], [ 1994875.0, 315625.0 ], [ 1994875.0, 315375.0 ], [ 1994875.0, 315125.0 ], [ 1994625.0, 315125.0 ], [ 1994375.0, 315125.0 ], [ 1994375.0, 314875.0 ], [ 1994125.0, 314875.0 ], [ 1994125.0, 314625.0 ], [ 1994125.0, 314375.0 ], [ 1993875.0, 314375.0 ], [ 1993875.0, 314625.0 ], [ 1993625.0, 314625.0 ], [ 1993625.0, 314375.0 ], [ 1993625.0, 314125.0 ], [ 1993375.0, 314125.0 ], [ 1993375.0, 313875.0 ], [ 1993125.0, 313875.0 ], [ 1993125.0, 314125.0 ], [ 1992875.0, 314125.0 ], [ 1992625.0, 314125.0 ], [ 1992625.0, 314375.0 ], [ 1992625.0, 314625.0 ], [ 1992375.0, 314625.0 ], [ 1992243.440604035742581, 314756.559395964199211 ], [ 1992412.345974609954283, 314836.567203078011516 ], [ 1992625.0, 314841.883553712745197 ], [ 1992875.0, 314848.133553712745197 ], [ 1992875.0, 314625.0 ], [ 1993125.0, 314625.0 ], [ 1993125.0, 314854.383553712745197 ], [ 1993375.0, 314860.633553712745197 ], [ 1993625.0, 314866.883553712745197 ], [ 1993749.024654689943418, 314869.984170079987962 ], [ 1993795.505394287640229, 314954.494605712417979 ], [ 1993972.924749125726521, 315277.075250874215271 ], [ 1994026.783361144829541, 315375.0 ], [ 1994239.053781383205205, 315760.94621861691121 ], [ 1994301.783361143665388, 315875.0 ], [ 1994505.182813640451059, 316244.817186359548941 ], [ 1994576.783361142268404, 316375.0 ], [ 1994771.311845897696912, 316728.68815410224488 ], [ 1994851.783361141104251, 316875.0 ], [ 1995037.440878155175596, 317212.559121844940819 ], [ 1995219.371202769922093, 317543.341530236997642 ], [ 1995375.0, 317550.752425343205687 ] ], [ [ 1995125.0, 316875.0 ], [ 1995125.0, 316625.0 ], [ 1995375.0, 316625.0 ], [ 1995375.0, 316875.0 ], [ 1995125.0, 316875.0 ] ] ], [ [ [ 2005125.0, 321875.0 ], [ 2005125.0, 322125.0 ], [ 2005375.0, 322125.0 ], [ 2005625.0, 322125.0 ], [ 2005625.0, 322375.0 ], [ 2005853.200009249150753, 322375.0 ], [ 2005823.255498570390046, 322591.415552785329055 ], [ 2005890.83, 322726.7586 ], [ 2006060.660845221951604, 322854.279157310607843 ], [ 2006125.0, 322874.713643411407247 ], [ 2006125.0, 323125.0 ], [ 2006125.0, 323375.0 ], [ 2006375.0, 323375.0 ], [ 2006375.0, 323625.0 ], [ 2006375.0, 323875.0 ], [ 2006625.0, 323875.0 ], [ 2006625.0, 324125.0 ], [ 2006625.0, 324375.0 ], [ 2006875.0, 324375.0 ], [ 2007125.0, 324375.0 ], [ 2007375.0, 324375.0 ], [ 2007375.0, 324625.0 ], [ 2007625.0, 324625.0 ], [ 2007625.0, 324875.0 ], [ 2007875.0, 324875.0 ], [ 2008125.0, 324875.0 ], [ 2008125.0, 325125.0 ], [ 2008375.0, 325125.0 ], [ 2008625.0, 325125.0 ], [ 2008625.0, 324875.0 ], [ 2008875.0, 324875.0 ], [ 2008875.0, 324625.0 ], [ 2009125.0, 324625.0 ], [ 2009125.0, 324375.0 ], [ 2009125.0, 324125.0 ], [ 2009375.0, 324125.0 ], [ 2009375.0, 323875.0 ], [ 2009375.0, 323625.0 ], [ 2009125.0, 323625.0 ], [ 2009125.0, 323375.0 ], [ 2009125.0, 323125.0 ], [ 2008875.0, 323125.0 ], [ 2008875.0, 322875.0 ], [ 2008625.0, 322875.0 ], [ 2008625.0, 322625.0 ], [ 2008625.0, 322375.0 ], [ 2008375.0, 322375.0 ], [ 2008125.0, 322375.0 ], [ 2007875.0, 322375.0 ], [ 2007875.0, 322125.0 ], [ 2007625.0, 322125.0 ], [ 2007625.0, 321875.0 ], [ 2007375.0, 321875.0 ], [ 2007375.0, 321686.510160959267523 ], [ 2007289.698470945237204, 321629.262140740000177 ], [ 2007148.71354650054127, 321435.770793618052267 ], [ 2006975.777212957851589, 321614.801813868223689 ], [ 2006900.085, 321693.1616 ], [ 2006777.156816202681512, 321818.271713803464081 ], [ 2006583.639, 322015.2244 ], [ 2006411.373, 322194.9804 ], [ 2006220.382, 322395.3335 ], [ 2006132.449795989319682, 322221.120999313541688 ], [ 2005951.030115525936708, 322384.202294588496443 ], [ 2005875.0, 322361.554269636981189 ], [ 2005875.0, 322125.0 ], [ 2005875.0, 321875.0 ], [ 2005625.0, 321875.0 ], [ 2005625.0, 321625.0 ], [ 2005375.0, 321625.0 ], [ 2005375.0, 321375.0 ], [ 2005125.0, 321375.0 ], [ 2005125.0, 321625.0 ], [ 2004875.0, 321625.0 ], [ 2004875.0, 321375.0 ], [ 2004625.0, 321375.0 ], [ 2004625.0, 321125.0 ], [ 2004875.0, 321125.0 ], [ 2004875.0, 320875.0 ], [ 2004625.0, 320875.0 ], [ 2004375.0, 320875.0 ], [ 2004375.0, 320625.0 ], [ 2004125.0, 320625.0 ], [ 2004125.0, 320375.0 ], [ 2003875.0, 320375.0 ], [ 2003875.0, 320125.0 ], [ 2003625.0, 320125.0 ], [ 2003375.0, 320125.0 ], [ 2003125.0, 320125.0 ], [ 2003125.0, 320375.0 ], [ 2003375.0, 320375.0 ], [ 2003375.0, 320625.0 ], [ 2003625.0, 320625.0 ], [ 2003625.0, 320875.0 ], [ 2003875.0, 320875.0 ], [ 2003875.0, 321125.0 ], [ 2004125.0, 321125.0 ], [ 2004125.0, 321375.0 ], [ 2004375.0, 321375.0 ], [ 2004375.0, 321625.0 ], [ 2004625.0, 321625.0 ], [ 2004625.0, 321875.0 ], [ 2004875.0, 321875.0 ], [ 2005125.0, 321875.0 ] ], [ [ 2007375.0, 323625.0 ], [ 2007375.0, 323875.0 ], [ 2007125.0, 323875.0 ], [ 2007125.0, 323625.0 ], [ 2007375.0, 323625.0 ] ], [ [ 2008125.0, 322875.0 ], [ 2008375.0, 322875.0 ], [ 2008375.0, 323125.0 ], [ 2008625.0, 323125.0 ], [ 2008625.0, 323375.0 ], [ 2008625.0, 323625.0 ], [ 2008375.0, 323625.0 ], [ 2008375.0, 323375.0 ], [ 2008125.0, 323375.0 ], [ 2008125.0, 323125.0 ], [ 2008125.0, 322875.0 ] ], [ [ 2007875.0, 324125.0 ], [ 2007875.0, 324375.0 ], [ 2007625.0, 324375.0 ], [ 2007625.0, 324125.0 ], [ 2007875.0, 324125.0 ] ] ], [ [ [ 2003260.908588130027056, 326425.80663290398661 ], [ 2003205.3318010433577, 326455.3318010433577 ], [ 2003125.0, 326498.008070347073954 ], [ 2002961.444015480112284, 326584.89718712202739 ], [ 2002720.589737881906331, 326720.589737881789915 ], [ 2002875.0, 326875.0 ], [ 2002875.0, 327125.0 ], [ 2002625.0, 327125.0 ], [ 2002375.0, 327125.0 ], [ 2002235.943278753664345, 326985.943278753547929 ], [ 2002125.0, 327034.480963207723107 ], [ 2001714.204148316988721, 327214.204148317105137 ], [ 2001698.07784962002188, 327221.259403996984474 ], [ 2001529.62902751006186, 327361.633422426006291 ], [ 2001463.792912004282698, 327463.792912004340906 ], [ 2001267.846966058947146, 327767.846966059005354 ], [ 2001037.681677520507947, 328125.0 ], [ 2000986.849489589920267, 328203.877532996004447 ], [ 2000974.910712822573259, 328375.0 ], [ 2000957.468852357938886, 328625.0 ], [ 2000952.09044894319959, 328702.090448943316005 ], [ 2000930.699882220011204, 329008.688571984996088 ], [ 2000933.026110779028386, 329125.0 ], [ 2000940.0581501100678, 329476.601966745976824 ], [ 2000944.225144267082214, 329555.774855732859578 ], [ 2000947.868572912644595, 329625.0 ], [ 2000958.774685899959877, 329832.216146763996221 ], [ 2000987.757296156603843, 329875.0 ], [ 2001155.298311700113118, 330122.322451515996363 ], [ 2001375.0, 330259.636006701330189 ], [ 2001379.896741189993918, 330262.69646994501818 ], [ 2001625.0, 330296.503815988078713 ], [ 2001651.286510149948299, 330300.129541526024695 ], [ 2001875.0, 330223.427773576579057 ], [ 2001978.825886480044574, 330187.830326783005148 ], [ 2002125.0, 330140.538701820769347 ], [ 2002297.006994920084253, 330084.889379934989847 ], [ 2002430.972065173555166, 329930.972065173671581 ], [ 2002663.64533249870874, 329663.645332498825155 ], [ 2002736.845585989998654, 329579.542913593002595 ], [ 2002983.229285574285313, 329483.229285574343521 ], [ 2003125.0, 329427.809824481024407 ], [ 2003522.444971849909052, 329272.444971849850845 ], [ 2003625.0, 329232.355279027775396 ], [ 2003625.0, 328875.0 ], [ 2003947.537490927381441, 328875.0 ], [ 2004014.085931829176843, 328764.085931829176843 ], [ 2004131.227502380032092, 328568.849980909028091 ], [ 2004241.455922896508127, 328375.0 ], [ 2004289.865650846390054, 328289.865650846331846 ], [ 2004471.115650845691562, 327971.115650845749769 ], [ 2004667.926511129597202, 327625.0 ], [ 2004674.007040299940854, 327614.306655596999917 ], [ 2004769.156847949139774, 327519.156847949197982 ], [ 2004625.0, 327375.0 ], [ 2004625.0, 327125.0 ], [ 2004875.0, 327125.0 ], [ 2004875.0, 327413.313695900083985 ], [ 2004936.038541370071471, 327352.275154531002045 ], [ 2005265.730908230412751, 327265.730908230296336 ], [ 2005375.0, 327237.047771640762221 ], [ 2005625.0, 327171.422771640878636 ], [ 2005684.699972989968956, 327155.751528731023427 ], [ 2005875.0, 327152.470493782660924 ], [ 2006125.0, 327148.160148955124896 ], [ 2006227.479510910110548, 327146.393260836019181 ], [ 2006375.0, 327165.551765912619885 ], [ 2006625.0, 327198.01929838018259 ], [ 2006625.0, 326875.0 ], [ 2006875.0, 326875.0 ], [ 2007125.0, 326875.0 ], [ 2007125.0, 327125.0 ], [ 2007375.0, 327125.0 ], [ 2007375.0, 327496.136256483208854 ], [ 2007455.539839698001742, 327544.46016030194005 ], [ 2007625.0, 327375.0 ], [ 2007875.0, 327375.0 ], [ 2007875.0, 327625.0 ], [ 2007768.039839698467404, 327731.960160301590804 ], [ 2008071.05828626989387, 327913.771228244004305 ], [ 2008125.0, 327961.209331098711118 ], [ 2008212.148997415555641, 328037.851002584444359 ], [ 2008478.186733264941722, 328271.81326673500007 ], [ 2008744.224469114560634, 328505.775530885555781 ], [ 2009010.262204963946715, 328739.737795036053285 ], [ 2009276.299940813332796, 328973.700059186608996 ], [ 2009390.574059499893337, 329074.196447251015343 ], [ 2009509.054002849152312, 329240.945997150847688 ], [ 2009604.302899609785527, 329375.0 ], [ 2009643.247292669955641, 329429.81062727002427 ], [ 2009738.123310459312052, 329761.876689540746156 ], [ 2009770.444256304064766, 329875.0 ], [ 2009774.263043199898675, 329888.36575413600076 ], [ 2009791.791505856206641, 330125.0 ], [ 2009792.979578990023583, 330141.038987306994386 ], [ 2009737.728454741183668, 330237.728454741125461 ], [ 2009555.910272921202704, 330555.910272921260912 ], [ 2009625.0, 330625.0 ], [ 2009625.0, 330875.0 ], [ 2009375.0, 330875.0 ], [ 2009375.0, 331125.0 ], [ 2009125.0, 331125.0 ], [ 2009040.099367298651487, 331040.099367298651487 ], [ 2008801.003182099899277, 331254.672866837994661 ], [ 2008779.566767575684935, 331279.566767575684935 ], [ 2008548.223483992973343, 331548.22348399303155 ], [ 2008316.88020041026175, 331816.880200410319958 ], [ 2008051.554817429045215, 332125.0 ], [ 2007969.865275036310777, 332219.865275036310777 ], [ 2007930.684267840115353, 332265.365799522027373 ], [ 2007771.11454053921625, 332521.114540539099835 ], [ 2007550.314211132237688, 332875.0 ], [ 2007482.956645802594721, 332982.956645802536514 ], [ 2007290.851382644847035, 333290.851382644847035 ], [ 2007247.530711489962414, 333360.283143263019156 ], [ 2007064.971143612172455, 333564.971143612230662 ], [ 2006938.707870949991047, 333706.539055385976098 ], [ 2006846.326062612701207, 333875.0 ], [ 2006779.617316730087623, 333996.64536013797624 ], [ 2006769.210183766670525, 334125.0 ], [ 2006758.394419982563704, 334258.394419982505497 ], [ 2006751.542513039894402, 334342.901272260991391 ], [ 2006657.959834089968354, 334530.06663016602397 ], [ 2006557.940748014487326, 334625.0 ], [ 2006464.084190337220207, 334714.084190337278415 ], [ 2006207.562451206380501, 334957.562451206322294 ], [ 2006125.0, 335035.926811672979966 ], [ 2006105.822028269991279, 335054.129632298019715 ], [ 2005815.71572351991199, 335297.444597574009094 ], [ 2005803.438300265232101, 335303.438300265290309 ], [ 2005467.459464286686853, 335467.459464286803268 ], [ 2005375.0, 335512.597155513591133 ], [ 2004963.49121031910181, 335713.49121031910181 ], [ 2004875.0, 335756.691643703321461 ], [ 2004627.215700830100104, 335877.657207078009378 ], [ 2004455.95006786310114, 335955.950067863217555 ], [ 2004625.0, 336125.0 ], [ 2004875.0, 336125.0 ], [ 2004875.0, 336375.0 ], [ 2004625.0, 336375.0 ], [ 2004375.0, 336375.0 ], [ 2004375.0, 335992.955813172273338 ], [ 2004299.67632450000383, 336027.389493400987703 ], [ 2004125.0, 336112.786807600059547 ], [ 2003878.554269209969789, 336233.271387096028775 ], [ 2003798.266360673354939, 336298.266360673296731 ], [ 2003521.950571199413389, 336521.950571199529804 ], [ 2003485.507017609896138, 336551.452495533972979 ], [ 2003258.463113754056394, 336758.463113754056394 ], [ 2003167.32590917008929, 336841.55880028597312 ], [ 2003091.741589248180389, 337125.0 ], [ 2003055.026694430038333, 337262.680855571001302 ], [ 2003076.086534011177719, 337375.0 ], [ 2003139.251105489907786, 337711.877714540984016 ], [ 2003246.088069938356057, 337753.911930061702151 ], [ 2003375.0, 337625.0 ], [ 2003375.0, 337375.0 ], [ 2003625.0, 337375.0 ], [ 2003625.0, 337625.0 ], [ 2003625.0, 337902.992033692600671 ], [ 2003710.105447100009769, 337936.476144027023111 ], [ 2003875.0, 337948.468475147208665 ], [ 2004125.0, 337966.650293329323176 ], [ 2004224.810181329958141, 337973.909215607971419 ], [ 2004375.0, 337901.089909587753937 ], [ 2004533.633021879941225, 337824.176929284003563 ], [ 2004702.081843989901245, 337777.385589807992801 ], [ 2004777.385589807992801, 337777.385589807992801 ], [ 2004875.0, 337777.385589807992801 ], [ 2004875.0, 337375.0 ], [ 2005125.0, 337375.0 ], [ 2005375.0, 337375.0 ], [ 2005375.0, 337727.439467867545318 ], [ 2005779.495071282377467, 337529.495071282435674 ], [ 2005875.0, 337482.758616803621408 ], [ 2006152.613367750076577, 337346.90526662801858 ], [ 2006284.5399657539092, 337284.539965753792785 ], [ 2006375.0, 337241.777040473942179 ], [ 2006625.0, 337123.595222292642575 ], [ 2006667.318101990036666, 337103.590301351970993 ], [ 2007040.623179794289172, 337040.623179794405587 ], [ 2007125.0, 337026.390945060877129 ], [ 2007375.0, 336984.222270361962728 ], [ 2007444.054337290115654, 336972.574550819001161 ], [ 2007779.622726832749322, 336779.622726832691114 ], [ 2007875.0, 336724.780794761667494 ], [ 2008192.715768910013139, 336542.09422763902694 ], [ 2008249.736196049489081, 336499.736196049605496 ], [ 2008536.621441951021552, 336286.62144195107976 ], [ 2008625.0, 336220.968798828951549 ], [ 2008966.949310803320259, 335966.949310803320259 ], [ 2009253.83455670485273, 335753.834556704794522 ], [ 2009540.719802606385201, 335540.719802606268786 ], [ 2009625.0, 335478.111655970860738 ], [ 2009625.0, 335125.0 ], [ 2009875.0, 335125.0 ], [ 2009971.047671458451077, 335221.047671458509285 ], [ 2010257.932917359983549, 335007.932917359983549 ], [ 2010375.0, 334920.968798827321734 ], [ 2010688.260786212282255, 334688.260786212224048 ], [ 2010813.03077957010828, 334595.574505431985017 ], [ 2010992.578270388534293, 334492.5782703885925 ], [ 2011125.0, 334416.615417743101716 ], [ 2011469.179255609866232, 334219.179255609866232 ], [ 2011786.913245757343248, 334036.913245757343248 ], [ 2011875.0, 333986.382859602512326 ], [ 2011875.0, 333625.0 ], [ 2011875.0, 333375.0 ], [ 2012125.0, 333375.0 ], [ 2012241.619971779873595, 333491.61997177999001 ], [ 2012453.61997177824378, 333203.619971778301988 ], [ 2012695.520784334046766, 332875.0 ], [ 2012703.400894399965182, 332864.294944815977942 ], [ 2012771.978686381364241, 332771.978686381364241 ], [ 2012946.71585967997089, 332536.755568484019022 ], [ 2013005.758291682694107, 332505.7582916826359 ], [ 2013125.0, 332443.156394816120155 ], [ 2013321.046575489919633, 332340.231942683982197 ], [ 2013375.0, 332335.857340696733445 ], [ 2013801.293040144955739, 332301.293040144839324 ], [ 2013875.0, 332295.316800156666432 ], [ 2013961.714070417685434, 332288.285929582372773 ], [ 2014013.558399739908054, 332284.082335313025396 ], [ 2014375.0, 332352.463178606063593 ], [ 2014625.0, 332399.760475903749466 ], [ 2014706.070223979884759, 332415.098085845995229 ], [ 2014778.094239746453241, 332471.905760253488552 ], [ 2015057.621798801701516, 332692.378201198414899 ], [ 2015370.50724453991279, 332939.161087977990974 ], [ 2015375.0, 332942.219079854083247 ], [ 2015483.756119152763858, 333016.243880847119726 ], [ 2015781.260144917760044, 333218.739855082239956 ], [ 2016125.0, 333452.706224644323811 ], [ 2016227.516183565137908, 333522.483816434862092 ], [ 2016525.020209329901263, 333724.979790669982322 ], [ 2016875.0, 333963.193369434506167 ], [ 2016971.276247977279127, 334028.723752022604458 ], [ 2017268.780273742275313, 334231.219726257724687 ], [ 2017625.0, 334473.68051422474673 ], [ 2017715.036312389653176, 334534.963687610405032 ], [ 2018012.540338154416531, 334737.459661845467053 ], [ 2018375.0, 334984.167659014987294 ], [ 2018458.796376801794395, 335041.203623198147397 ], [ 2018756.300402566790581, 335243.699597433209419 ], [ 2019053.804428331786767, 335446.195571668329649 ], [ 2019375.0, 335664.817185401916504 ], [ 2019500.0604669789318, 335749.939533020951785 ], [ 2019797.564492743927985, 335952.435507256072015 ], [ 2020125.0, 336175.304330192157067 ], [ 2020243.820531391305849, 336256.179468608694151 ], [ 2020541.324557156302035, 336458.67544284381438 ], [ 2020625.0, 336375.0 ], [ 2020875.0, 336375.0 ], [ 2020875.0, 336685.791474982397631 ], [ 2020987.580595803447068, 336762.419404196436517 ], [ 2021285.084621568443254, 336964.915378431556746 ], [ 2021375.0, 336875.0 ], [ 2021625.0, 336875.0 ], [ 2021875.0, 336875.0 ], [ 2021875.0, 337125.0 ], [ 2021731.340660215821117, 337268.659339784178883 ], [ 2022028.844685980817303, 337471.155314019299112 ], [ 2022125.0, 337375.0 ], [ 2022375.0, 337375.0 ], [ 2022625.0, 337375.0 ], [ 2022625.0, 337625.0 ], [ 2022478.972284559160471, 337771.027715440897737 ], [ 2022782.575888162711635, 337967.424111837288365 ], [ 2023125.0, 338188.932884479698259 ], [ 2023237.981293568154797, 338262.018706431786995 ], [ 2023375.0, 338125.0 ], [ 2023375.0, 337875.0 ], [ 2023375.0, 337625.0 ], [ 2023625.0, 337625.0 ], [ 2023625.0, 337875.0 ], [ 2023625.0, 338125.0 ], [ 2023625.0, 338375.0 ], [ 2023875.0, 338375.0 ], [ 2024125.0, 338375.0 ], [ 2024125.0, 338125.0 ], [ 2024375.0, 338125.0 ], [ 2024375.0, 338375.0 ], [ 2024375.0, 338625.0 ], [ 2024375.0, 338875.0 ], [ 2024300.593906180933118, 338949.406093819066882 ], [ 2024625.0, 339159.259293974901084 ], [ 2024755.99931158637628, 339244.000688413565513 ], [ 2025125.0, 339482.701430473360233 ], [ 2025211.404716991819441, 339538.595283008122351 ], [ 2025440.003499800106511, 339686.47224043298047 ], [ 2025485.883939142571762, 339764.116060857428238 ], [ 2025551.406266818754375, 339875.0 ], [ 2025561.660982439992949, 339892.354134128021542 ], [ 2025552.302714539924636, 340088.87775992800016 ], [ 2025535.445669173495844, 340125.0 ], [ 2025484.394774437416345, 340234.39477443729993 ], [ 2025421.286964009981602, 340369.625796784996055 ], [ 2025271.55467768991366, 340509.999815213028342 ], [ 2025265.023668463807553, 340515.023668463807553 ], [ 2025028.239712409907952, 340697.165173117013182 ], [ 2024973.459618293214589, 340723.459618293156382 ], [ 2025125.0, 340875.0 ], [ 2025125.0, 341125.0 ], [ 2025242.967770617455244, 341242.967770617571659 ], [ 2025496.153107170015574, 341024.70454945001984 ], [ 2025513.204662976320833, 341013.204662976204418 ], [ 2025625.0, 340937.807807775272522 ], [ 2025898.558626669924706, 340753.314780489017721 ], [ 2025998.553137133130804, 340748.553137133247219 ], [ 2026095.082252470077947, 340743.956512593023945 ], [ 2026125.0, 340749.467676611733623 ], [ 2026125.0, 340375.0 ], [ 2026375.0, 340375.0 ], [ 2026375.0, 340625.0 ], [ 2026231.005073083331808, 340768.994926916609984 ], [ 2026450.696432489901781, 340809.464387859974522 ], [ 2026625.0, 340918.897695617226418 ], [ 2026751.611554779810831, 340998.388445220189169 ], [ 2026875.0, 340875.0 ], [ 2027125.0, 340875.0 ], [ 2027125.0, 340625.0 ], [ 2027375.0, 340625.0 ], [ 2027375.0, 340875.0 ], [ 2027375.0, 341125.0 ], [ 2027212.347141260746866, 341287.652858739311341 ], [ 2027519.504198914626613, 341480.495801085373387 ], [ 2027875.0, 341703.687339631142095 ], [ 2027980.239785395562649, 341769.760214604495559 ], [ 2028287.396843049442396, 341962.603156950557604 ], [ 2028625.0, 342174.561126039479859 ], [ 2028748.132429530378431, 342251.867570469621569 ], [ 2029125.0, 342488.476983645057771 ], [ 2029208.868016011314467, 342541.131983988743741 ], [ 2029342.401212109951302, 342624.968359532998875 ], [ 2029508.24220726150088, 342741.757792738440912 ], [ 2029801.630637013586238, 342948.36936298647197 ], [ 2029875.0, 342875.0 ], [ 2030125.0, 342875.0 ], [ 2030125.0, 343125.0 ], [ 2030033.358234973857179, 343216.641765026201028 ], [ 2030034.913036359939724, 343223.897504827007651 ], [ 2030019.094194850884378, 343269.09419485082617 ], [ 2029969.405161089962348, 343411.062862731982023 ], [ 2029821.294835292035714, 343625.0 ], [ 2029800.956338980002329, 343654.377828006981872 ], [ 2029714.733685465529561, 343714.733685465645976 ], [ 2029625.0, 343777.547265290108044 ], [ 2029613.790981069905683, 343785.393578540999442 ], [ 2029292.459090777207166, 344042.459090777207166 ], [ 2029375.0, 344125.0 ], [ 2029375.0, 344375.0 ], [ 2029625.0, 344375.0 ], [ 2029875.0, 344375.0 ], [ 2030125.0, 344375.0 ], [ 2030125.0, 344628.064234380959533 ], [ 2030240.794930049916729, 344702.5038322720211 ], [ 2030334.377609, 344702.5038322720211 ], [ 2030390.527216369984671, 344562.129813843988813 ], [ 2030437.31855585006997, 344393.680991730012465 ], [ 2030448.096051078988239, 344375.0 ], [ 2030512.817008001729846, 344262.817008001788054 ], [ 2030577.692574280081317, 344150.366026454023086 ], [ 2030700.308486106572673, 343950.308486106514465 ], [ 2030755.499664290109649, 343860.259721702022944 ], [ 2030875.0, 343834.281387852039188 ], [ 2030970.739825879922137, 343813.468382226012181 ], [ 2031125.0, 343856.661230980011169 ], [ 2031204.696523260092363, 343878.976257493020967 ], [ 2031300.643960478482768, 343949.356039521517232 ], [ 2031625.0, 344187.279094299650751 ], [ 2031733.288588577648625, 344266.711411422467791 ], [ 2031875.0, 344125.0 ], [ 2032125.0, 344125.0 ], [ 2032375.0, 344125.0 ], [ 2032625.0, 344125.0 ], [ 2032625.0, 344375.0 ], [ 2032375.0, 344375.0 ], [ 2032375.0, 344625.0 ], [ 2032625.0, 344625.0 ], [ 2032625.0, 344920.803449601109605 ], [ 2032742.792720808647573, 345007.207279191410635 ], [ 2033031.222472874680534, 345218.777527125377674 ], [ 2033375.0, 345470.94671607716009 ], [ 2033463.86710097361356, 345536.13289902638644 ], [ 2033752.296853039646521, 345747.703146960353479 ], [ 2034040.726605105679482, 345959.273394894320518 ], [ 2034375.0, 346204.471071378618944 ], [ 2034470.732018690090626, 346274.692838669987395 ], [ 2034472.124453406548128, 346277.875546593335457 ], [ 2034514.616401774343103, 346375.0 ], [ 2034536.239893960068002, 346424.425124993023928 ], [ 2034573.672965540084988, 346611.590482897998299 ], [ 2034567.362604551017284, 346625.0 ], [ 2034505.806571095483378, 346755.806571095483378 ], [ 2034498.806822380051017, 346770.681037115980871 ], [ 2034339.716268159914762, 346985.921198707015719 ], [ 2034294.147701082052663, 347044.147701082110871 ], [ 2034171.267446039943025, 347201.16136029700283 ], [ 2034068.326499199960381, 347313.460575038974639 ], [ 2034012.941016733646393, 347375.0 ], [ 2033984.102088140090927, 347407.043253991985694 ], [ 2034021.535159720107913, 347538.059004525013734 ], [ 2034105.759570779977366, 347622.283415581972804 ], [ 2034199.342249729903415, 347631.64168347697705 ], [ 2034358.43280395003967, 347547.41727242001798 ], [ 2034508.165090270107612, 347528.700736629019957 ], [ 2034531.267692423891276, 347531.267692423949484 ], [ 2034625.0, 347541.682393265888095 ], [ 2034699.985846060561016, 347550.014153939438984 ], [ 2034760.838323439937085, 347556.775540315022226 ], [ 2034919.92887766007334, 347669.074755057983566 ], [ 2034984.327424573944882, 347765.672575426171534 ], [ 2035125.0, 347625.0 ], [ 2035375.0, 347625.0 ], [ 2035375.0, 347875.0 ], [ 2035375.0, 348125.0 ], [ 2035264.198325752746314, 348235.801674247137271 ], [ 2035331.692665050039068, 348361.586579304013867 ], [ 2035340.076052985154092, 348375.0 ], [ 2035545.816032607574016, 348704.183967392367776 ], [ 2035612.440701910061762, 348810.783438274986111 ], [ 2035630.553065473912284, 348875.0 ], [ 2035715.381648760056123, 349175.755886188999284 ], [ 2035719.413591908058152, 349280.586408092058264 ], [ 2035875.0, 349125.0 ], [ 2036125.0, 349125.0 ], [ 2036125.0, 349375.0 ], [ 2035723.044883902417496, 349375.0 ], [ 2035724.739916650112718, 349419.070851463999134 ], [ 2035865.113935079891235, 349596.877941474027466 ], [ 2035992.176342483609915, 349757.823657516506501 ], [ 2036005.487953509902582, 349774.685031483008061 ], [ 2035985.424959806958213, 349875.0 ], [ 2035968.054881929885596, 349961.850389386992902 ], [ 2035965.487505705095828, 349965.487505705154035 ], [ 2035875.0, 350093.678138779476285 ], [ 2035875.0, 350375.0 ], [ 2036125.0, 350375.0 ], [ 2036125.0, 350576.604799978144001 ], [ 2036155.220239829970524, 350551.421266785997432 ], [ 2036239.444650890072808, 350570.137802576995455 ], [ 2036375.0, 350575.35146985045867 ], [ 2036482.759616160066798, 350579.496070471999701 ], [ 2036679.283241959987208, 350523.346463100984693 ], [ 2036760.372225702274591, 350510.372225702158175 ], [ 2036875.0, 350492.031781814410351 ], [ 2036875.0, 350125.0 ], [ 2036875.0, 349875.0 ], [ 2036625.0, 349875.0 ], [ 2036625.0, 349625.0 ], [ 2036875.0, 349625.0 ], [ 2037125.0, 349625.0 ], [ 2037375.0, 349625.0 ], [ 2037375.0, 349875.0 ], [ 2037375.0, 350125.0 ], [ 2037125.0, 350125.0 ], [ 2037125.0, 350375.0 ], [ 2036980.972776822280139, 350519.027223177661654 ], [ 2037334.361994629958645, 350691.795285215019248 ], [ 2037375.0, 350721.492289139947388 ], [ 2037375.0, 350375.0 ], [ 2037625.0, 350375.0 ], [ 2037625.0, 350625.0 ], [ 2037875.0, 350625.0 ], [ 2038125.0, 350625.0 ], [ 2038125.0, 350875.0 ], [ 2038125.0, 351292.092327469727024 ], [ 2038375.0, 351234.949470327468589 ], [ 2038625.0, 351177.806613185210153 ], [ 2038663.236035750014707, 351169.066947871004231 ], [ 2038875.0, 351167.087845401430968 ], [ 2039125.0, 351164.751396803359967 ], [ 2039125.0, 350875.0 ], [ 2039125.0, 350625.0 ], [ 2039375.0, 350625.0 ], [ 2039375.0, 350875.0 ], [ 2039625.0, 350875.0 ], [ 2039875.0, 350875.0 ], [ 2039875.0, 350625.0 ], [ 2040125.0, 350625.0 ], [ 2040125.0, 350875.0 ], [ 2040125.0, 351125.0 ], [ 2039964.293289145454764, 351285.706710854603443 ], [ 2040375.0, 351458.360487391939387 ], [ 2040492.320194975240156, 351507.679805024818052 ], [ 2040625.0, 351375.0 ], [ 2040875.0, 351375.0 ], [ 2040875.0, 351668.551570194249507 ], [ 2041020.347100805025548, 351729.65289919503266 ], [ 2041125.0, 351625.0 ], [ 2041375.0, 351625.0 ], [ 2041375.0, 351885.000225200958084 ], [ 2041625.0, 351996.582146105298307 ], [ 2041625.0, 351625.0 ], [ 2041625.0, 351375.0 ], [ 2041375.0, 351375.0 ], [ 2041125.0, 351375.0 ], [ 2041125.0, 351125.0 ], [ 2041375.0, 351125.0 ], [ 2041375.0, 350875.0 ], [ 2041625.0, 350875.0 ], [ 2041875.0, 350875.0 ], [ 2041875.0, 350625.0 ], [ 2041875.0, 350375.0 ], [ 2042125.0, 350375.0 ], [ 2042125.0, 350625.0 ], [ 2042375.0, 350625.0 ], [ 2042625.0, 350625.0 ], [ 2042875.0, 350625.0 ], [ 2043125.0, 350625.0 ], [ 2043375.0, 350625.0 ], [ 2043625.0, 350625.0 ], [ 2043875.0, 350625.0 ], [ 2044125.0, 350625.0 ], [ 2044125.0, 350875.0 ], [ 2044375.0, 350875.0 ], [ 2044375.0, 351125.0 ], [ 2044375.0, 351375.0 ], [ 2044625.0, 351375.0 ], [ 2044875.0, 351375.0 ], [ 2045125.0, 351375.0 ], [ 2045125.0, 351625.0 ], [ 2045375.0, 351625.0 ], [ 2045625.0, 351625.0 ], [ 2045625.0, 351875.0 ], [ 2045375.0, 351875.0 ], [ 2045125.0, 351875.0 ], [ 2044875.0, 351875.0 ], [ 2044875.0, 352125.0 ], [ 2045125.0, 352125.0 ], [ 2045125.0, 352375.0 ], [ 2044875.0, 352375.0 ], [ 2044875.0, 352653.408305170363747 ], [ 2045267.152075534453616, 352517.152075534453616 ], [ 2045375.0, 352479.679491609684192 ], [ 2045625.0, 352392.815084829286207 ], [ 2045672.578689269954339, 352376.283506354026031 ], [ 2045875.0, 352446.413724244979676 ], [ 2046007.634251584066078, 352492.365748415933922 ], [ 2046375.0, 352619.642070702218916 ], [ 2046625.0, 352706.256243930838536 ], [ 2046750.324310062918812, 352749.67568993702298 ], [ 2046861.078711959999055, 352788.047293743991759 ], [ 2047125.0, 352871.509581414051354 ], [ 2047375.0, 352950.569410473166499 ], [ 2047507.521941394079477, 352992.478058605978731 ], [ 2047875.0, 353108.689068591454998 ], [ 2047955.996055709896609, 353134.303205867006909 ], [ 2048125.0, 353217.148276598658413 ], [ 2048230.92681438731961, 353269.073185612796806 ], [ 2048433.267718360060826, 353368.259903247992042 ], [ 2048452.087732181418687, 353500.0 ], [ 2048470.700789940077811, 353630.291404313989915 ], [ 2048330.326771510066465, 353873.606369589979295 ], [ 2048329.118958488106728, 353875.0 ], [ 2048234.34944204846397, 353984.34944204846397 ], [ 2048125.0, 354110.521875184436794 ], [ 2048087.011806240072474, 354154.35440644697519 ], [ 2047740.755894120084122, 354229.220549607998691 ], [ 2047727.399179421830922, 354227.39917942188913 ], [ 2047875.0, 354375.0 ], [ 2047875.0, 354625.0 ], [ 2048125.0, 354625.0 ], [ 2048125.0, 354875.0 ], [ 2048375.0, 354875.0 ], [ 2048375.0, 355125.0 ], [ 2048125.0, 355125.0 ], [ 2047875.0, 355125.0 ], [ 2047625.0, 355125.0 ], [ 2047375.0, 355125.0 ], [ 2047375.0, 355375.0 ], [ 2047125.0, 355375.0 ], [ 2047125.0, 355125.0 ], [ 2046875.0, 355125.0 ], [ 2046875.0, 354875.0 ], [ 2047125.0, 354875.0 ], [ 2047125.0, 354625.0 ], [ 2047375.0, 354625.0 ], [ 2047375.0, 354179.344745864684228 ], [ 2047328.992106729885563, 354173.070942236983683 ], [ 2047125.0, 354243.193228925112635 ], [ 2047029.527534079970792, 354276.011889084998984 ], [ 2046768.949744597775862, 354625.0 ], [ 2046707.413975914940238, 354707.41397591488203 ], [ 2046505.464531949954107, 354977.881981225975323 ], [ 2046487.374677254352719, 354987.374677254352719 ], [ 2046375.0, 355046.343567298550624 ], [ 2045995.491560370894149, 355245.491560370835941 ], [ 2045875.0, 355308.719804921769537 ], [ 2045560.279474529903382, 355473.870179673016537 ], [ 2045491.544808166567236, 355491.544808166567236 ], [ 2045375.0, 355521.513473123952281 ], [ 2045375.0, 355875.0 ], [ 2045625.0, 355875.0 ], [ 2045625.0, 356125.0 ], [ 2045625.0, 356375.0 ], [ 2045875.0, 356375.0 ], [ 2045875.0, 356625.0 ], [ 2046125.0, 356625.0 ], [ 2046375.0, 356625.0 ], [ 2046375.0, 356875.0 ], [ 2046625.0, 356875.0 ], [ 2046625.0, 356625.0 ], [ 2046875.0, 356625.0 ], [ 2046875.0, 356375.0 ], [ 2046625.0, 356375.0 ], [ 2046625.0, 356125.0 ], [ 2046875.0, 356125.0 ], [ 2046875.0, 355875.0 ], [ 2047125.0, 355875.0 ], [ 2047375.0, 355875.0 ], [ 2047625.0, 355875.0 ], [ 2047875.0, 355875.0 ], [ 2047895.280881618382409, 355895.280881618324202 ], [ 2047899.410151296760887, 355996.421776332135778 ], [ 2048059.782211526064202, 356095.699855783430394 ], [ 2048056.414309432962909, 356241.334207701147534 ], [ 2048040.994343778584152, 356290.994343778467737 ], [ 2047924.015269186114892, 356325.984730813885108 ], [ 2047875.0, 356375.0 ], [ 2047875.0, 356625.0 ], [ 2047625.0, 356625.0 ], [ 2047625.0, 356875.0 ], [ 2047625.0, 357125.0 ], [ 2047875.0, 357125.0 ], [ 2048125.0, 357125.0 ], [ 2048125.0, 357375.0 ], [ 2048125.0, 357625.0 ], [ 2048125.0, 357875.0 ], [ 2048375.0, 357875.0 ], [ 2048625.0, 357875.0 ], [ 2048625.0, 358125.0 ], [ 2048875.0, 358125.0 ], [ 2048875.0, 358375.0 ], [ 2049125.0, 358375.0 ], [ 2049125.0, 358125.0 ], [ 2049375.0, 358125.0 ], [ 2049625.0, 358125.0 ], [ 2049625.0, 357875.0 ], [ 2049875.0, 357875.0 ], [ 2049875.0, 358125.0 ], [ 2050125.0, 358125.0 ], [ 2050531.076964991400018, 358125.0 ], [ 2050548.967066898010671, 358048.967066898068879 ], [ 2050648.724023817339912, 357625.0 ], [ 2050375.0, 357625.0 ], [ 2050375.0, 357875.0 ], [ 2050125.0, 357875.0 ], [ 2050125.0, 357625.0 ], [ 2050125.0, 357375.0 ], [ 2050125.0, 357125.0 ], [ 2050375.0, 357125.0 ], [ 2050625.0, 357125.0 ], [ 2050947.687645873054862, 357125.0 ], [ 2050950.641782179940492, 357120.925329232006334 ], [ 2051044.023082175292075, 357044.023082175292075 ], [ 2051125.0, 356977.336208671040367 ], [ 2051125.0, 356625.0 ], [ 2051385.769804697716609, 356625.0 ], [ 2051465.513203134061769, 356465.513203133945353 ], [ 2051502.779588, 356390.980433405027725 ], [ 2051554.664228707784787, 356445.335771292157006 ], [ 2051625.0, 356375.0 ], [ 2051625.0, 356125.0 ], [ 2051875.0, 356125.0 ], [ 2051875.0, 356375.0 ], [ 2051875.0, 356625.0 ], [ 2052311.916275671450421, 356625.0 ], [ 2052288.87409119005315, 356400.338701299973764 ], [ 2052291.964176714653149, 356375.0 ], [ 2052300.989809681195766, 356300.989809681137558 ], [ 2052335.665430669905618, 356016.649717596010305 ], [ 2052406.490289468783885, 355875.0 ], [ 2052476.039449099916965, 355735.901680739014409 ], [ 2052482.118906959658489, 355732.118906959600281 ], [ 2052790.338085040450096, 355540.338085040508304 ], [ 2052625.0, 355375.0 ], [ 2052375.0, 355375.0 ], [ 2052375.0, 355625.0 ], [ 2052125.0, 355625.0 ], [ 2051875.0, 355625.0 ], [ 2051625.0, 355625.0 ], [ 2051625.0, 355875.0 ], [ 2051375.0, 355875.0 ], [ 2051125.0, 355875.0 ], [ 2050875.0, 355875.0 ], [ 2050875.0, 355625.0 ], [ 2050875.0, 355375.0 ], [ 2050625.0, 355382.912428987619933 ], [ 2050686.458111064741388, 355073.265250973519869 ], [ 2050723.183999999891967, 355066.4851 ], [ 2050747.004, 355060.530199999979232 ], [ 2050836.327, 355024.800999999977648 ], [ 2050887.936, 354995.026699999987613 ], [ 2050910.884441662346944, 354980.747683905356098 ], [ 2050977.259, 354939.447999999974854 ], [ 2051108.266, 354852.10999999998603 ], [ 2051119.563230130355805, 354846.244091359432787 ], [ 2051211.483, 354798.516200000012759 ], [ 2051306.761, 354754.847200000018347 ], [ 2051339.851474932162091, 354741.190803439472802 ], [ 2051431.813, 354703.238399999972899 ], [ 2051505.257, 354679.418899999989662 ], [ 2051572.818477610126138, 354661.120849128928967 ], [ 2051600.534, 354653.614500000025146 ], [ 2051767.271, 354615.900399999984074 ], [ 2051813.08432263857685, 354606.194154093100224 ], [ 2052001.495000000111759, 354566.276499999978114 ], [ 2052054.392753534484655, 354555.606147173035424 ], [ 2052296.063303152332082, 354506.857195468794089 ], [ 2052463.99, 354472.983599999977741 ], [ 2052534.696291639702395, 354454.267284148954786 ], [ 2052613.944667972391471, 354125.0 ], [ 2052673.265417135553434, 354143.942089368880261 ], [ 2052875.0, 354069.359012868138961 ], [ 2052904.208318482618779, 354094.469294218695723 ], [ 2053148.391622352646664, 353994.126779674785212 ], [ 2053146.790609032381326, 353896.790609032323118 ], [ 2053375.0, 353853.129190048901364 ], [ 2053406.365308852866292, 353879.118521186290309 ], [ 2053625.0, 353823.244815596495755 ], [ 2053663.131689874920994, 353861.772455535479821 ], [ 2053875.0, 353803.9519838662236 ], [ 2053910.105774469673634, 353847.911656951182522 ], [ 2054125.0, 353783.61728427791968 ], [ 2054157.475470915203914, 353836.179934816900641 ], [ 2054375.0, 353761.103910181613173 ], [ 2054405.525373977608979, 353829.287263249338139 ], [ 2054640.49694138020277, 353726.582696879399009 ], [ 2054649.23748409259133, 353649.237484092533123 ], [ 2054875.0, 353631.366473019646946 ], [ 2055125.0, 353616.0711170466966 ], [ 2055375.0, 353601.183635311434045 ], [ 2055625.0, 353585.932005941634998 ], [ 2055875.0, 353574.024443101196084 ], [ 2056125.0, 353559.748845178051852 ], [ 2056146.247539822710678, 353594.238262819766533 ], [ 2056375.0, 353565.189082252269145 ], [ 2056402.627314012264833, 353604.969436102895997 ], [ 2056625.0, 353581.976500147837214 ], [ 2056658.175532867899165, 353622.710814676713198 ], [ 2056875.0, 353603.90383220568765 ], [ 2056916.989277934422716, 353646.347393477743026 ], [ 2057114.510077364509925, 353625.0 ], [ 2057139.27351446961984, 353807.841307688562665 ], [ 2057375.0, 353785.807741610682569 ], [ 2057389.012500252341852, 353821.701727379695512 ], [ 2057625.0, 353832.339210560137872 ], [ 2057632.892756410408765, 353845.583374651672784 ], [ 2057875.0, 353864.916898941854015 ], [ 2058125.0, 353895.644056870019995 ], [ 2058333.412036124849692, 353916.587963875208516 ], [ 2058343.532893176889047, 353971.947776140645146 ], [ 2058573.495666530216113, 354099.253049300634302 ], [ 2058625.0, 354019.287746681598946 ], [ 2058821.234719607280567, 354106.711431483156048 ], [ 2058875.0, 354043.578457493975293 ], [ 2059067.533013119595125, 354118.25090581382392 ], [ 2059125.0, 354064.438373746932484 ], [ 2059302.939927732106298, 354127.833226935239509 ], [ 2059375.0, 354072.547782423382159 ], [ 2059479.528306855354458, 354115.575721094384789 ], [ 2059531.247434145305306, 354340.55549108015839 ], [ 2059591.962, 354324.112000000022817 ], [ 2059683.27, 354274.488200000021607 ], [ 2059748.387329578632489, 354227.130101456714328 ], [ 2059770.608, 354210.969600000011269 ], [ 2059867.871, 354141.4962 ], [ 2059948.375940116588026, 354084.718807010853197 ], [ 2060086.491612219018862, 354299.844311335938983 ], [ 2060229.512232922716066, 354139.291739791049622 ], [ 2060375.0, 354183.159451701270882 ], [ 2060464.563876286381856, 354093.325358352158219 ], [ 2060400.958919165888801, 353910.302681645960547 ], [ 2060405.793, 353909.256500000017695 ], [ 2060518.936, 353873.527300000016112 ], [ 2060612.229, 353811.9937 ], [ 2060619.980225313687697, 353803.537643563351594 ], [ 2060634.063, 353788.174300000013318 ], [ 2060689.642, 353702.8212 ], [ 2060721.401, 353603.5735 ], [ 2060721.401, 353583.945469315221999 ], [ 2060859.709153409814462, 353594.106941066042054 ], [ 2060936.052242371952161, 353375.0 ], [ 2061125.0, 353375.0 ], [ 2061125.0, 353125.0 ], [ 2061125.0, 352875.0 ], [ 2060875.0, 352875.0 ], [ 2060615.875600476050749, 352875.0 ], [ 2060594.955344538670033, 352883.180850168806501 ], [ 2060295.89290426322259, 352753.722286372852977 ], [ 2060319.55769241345115, 353000.673875383858103 ], [ 2060163.097459559794515, 353067.355153968965169 ], [ 2060125.0, 353125.0 ], [ 2060125.0, 353375.0 ], [ 2060125.0, 353625.0 ], [ 2059875.0, 353625.0 ], [ 2059625.0, 353625.0 ], [ 2059375.0, 353625.0 ], [ 2059375.0, 353375.0 ], [ 2059375.0, 353125.0 ], [ 2059625.0, 353125.0 ], [ 2059625.0, 352875.0 ], [ 2059625.0, 352625.0 ], [ 2059375.0, 352625.0 ], [ 2059125.0, 352625.0 ], [ 2058875.0, 352625.0 ], [ 2058625.0, 352625.0 ], [ 2058625.0, 352875.0 ], [ 2058875.0, 352875.0 ], [ 2058875.0, 353125.0 ], [ 2058625.0, 353125.0 ], [ 2058625.0, 353375.0 ], [ 2058375.0, 353375.0 ], [ 2058375.0, 353625.0 ], [ 2058125.0, 353625.0 ], [ 2057875.0, 353625.0 ], [ 2057875.0, 353375.0 ], [ 2057875.0, 353125.0 ], [ 2057875.0, 352875.0 ], [ 2058125.0, 352875.0 ], [ 2058125.0, 352625.0 ], [ 2058125.0, 352375.0 ], [ 2058375.0, 352375.0 ], [ 2058375.0, 352125.0 ], [ 2058625.0, 352125.0 ], [ 2058625.0, 351875.0 ], [ 2058875.0, 351875.0 ], [ 2059125.0, 351875.0 ], [ 2059375.0, 351875.0 ], [ 2059375.0, 351746.357840756012592 ], [ 2059427.903657301561907, 351650.729735348955728 ], [ 2059625.0, 351758.932045619178098 ], [ 2059673.995058369124308, 351652.305168879975099 ], [ 2059875.0, 351772.684193356079049 ], [ 2059919.377369914203882, 351654.814204456051812 ], [ 2060123.591380463680252, 351785.751374438928906 ], [ 2060158.335058291675523, 351754.521282917354256 ], [ 2060330.204857724253088, 351819.724748992884997 ], [ 2060386.95, 351771.946799999976065 ], [ 2060407.137546690180898, 351713.740684832155239 ], [ 2060375.690805861959234, 351585.1759505996597 ], [ 2060347.54, 351583.63890000001993 ], [ 2060247.435, 351577.7048 ], [ 2060170.242608245229349, 351571.859622314281296 ], [ 2060183.066451482241973, 351375.142003979417495 ], [ 2060375.0, 351327.830256019020453 ], [ 2060436.516043751034886, 351387.744993808853906 ], [ 2060625.0, 351374.847991493297741 ], [ 2060639.944290086627007, 351387.894145780184772 ], [ 2060635.897513984469697, 351600.890607873327099 ], [ 2060650.648, 351601.70150000002468 ], [ 2060751.325999999884516, 351607.555799999972805 ], [ 2060854.733, 351615.192800000018906 ], [ 2060857.700888885883614, 351615.397245519154239 ], [ 2060964.719821736216545, 351696.371154662920162 ], [ 2060991.953694114694372, 351624.893341886578128 ], [ 2061027.602, 351627.59519999998156 ], [ 2061044.033, 351635.098400000017136 ], [ 2061048.719, 351641.4609 ], [ 2061054.935050831874833, 351651.875175932247657 ], [ 2061061.849, 351663.458700000017416 ], [ 2061063.703, 351664.178100000019185 ], [ 2061083.122, 351764.558200000028592 ], [ 2061065.446651025675237, 351772.888735701038968 ], [ 2061172.273600102867931, 351856.060891543340404 ], [ 2061125.0, 351950.060636591981165 ], [ 2061125.0, 352125.0 ], [ 2061125.0, 352375.0 ], [ 2061125.0, 352625.0 ], [ 2061375.0, 352625.0 ], [ 2061625.0, 352625.0 ], [ 2061625.0, 352375.0 ], [ 2061875.0, 352375.0 ], [ 2062125.0, 352375.0 ], [ 2062125.0, 352625.0 ], [ 2061875.0, 352625.0 ], [ 2061875.0, 352875.0 ], [ 2062125.0, 352875.0 ], [ 2062375.0, 352875.0 ], [ 2062625.0, 352875.0 ], [ 2062625.0, 352625.0 ], [ 2062625.0, 352375.0 ], [ 2062875.0, 352375.0 ], [ 2062875.0, 352125.0 ], [ 2062625.0, 352125.0 ], [ 2062625.0, 351875.0 ], [ 2062375.0, 351875.0 ], [ 2062375.0, 351625.0 ], [ 2062125.0, 351625.0 ], [ 2062125.0, 351375.0 ], [ 2061875.0, 351375.0 ], [ 2061875.0, 351125.0 ], [ 2061625.0, 351125.0 ], [ 2061625.0, 350875.0 ], [ 2061375.0, 350875.0 ], [ 2061125.0, 350875.0 ], [ 2060875.0, 350875.0 ], [ 2060625.0, 350875.0 ], [ 2060375.0, 350875.0 ], [ 2060125.0, 350875.0 ], [ 2059875.0, 350875.0 ], [ 2059625.0, 350875.0 ], [ 2059375.0, 350875.0 ], [ 2059375.0, 351125.0 ], [ 2059125.0, 351125.0 ], [ 2059125.0, 350875.0 ], [ 2058875.0, 350875.0 ], [ 2058625.0, 350875.0 ], [ 2058375.0, 350875.0 ], [ 2058125.0, 350875.0 ], [ 2057875.0, 350875.0 ], [ 2057625.0, 350875.0 ], [ 2057375.0, 350875.0 ], [ 2057125.0, 350875.0 ], [ 2056875.0, 350875.0 ], [ 2056875.0, 351003.812603708007373 ], [ 2056792.118906509829685, 351118.623203010298312 ], [ 2056765.632664176868275, 351207.582327997835819 ], [ 2056713.584, 351191.837999999988824 ], [ 2056615.531, 351162.523600000014994 ], [ 2056531.340559467673302, 351135.295387445541564 ], [ 2056519.756, 351131.548799999989569 ], [ 2056420.75, 351103.553599999984726 ], [ 2056325.352999999886379, 351070.809300000022631 ], [ 2056298.700922008370981, 351060.282652019988745 ], [ 2056350.599363238317892, 351375.0 ], [ 2056125.0, 351375.0 ], [ 2056125.0, 351625.0 ], [ 2055875.0, 351625.0 ], [ 2055625.0, 351625.0 ], [ 2055375.0, 351625.0 ], [ 2055125.0, 351625.0 ], [ 2054875.0, 351625.0 ], [ 2054875.0, 351375.0 ], [ 2054875.0, 351125.0 ], [ 2054875.0, 350875.0 ], [ 2054625.0, 350875.0 ], [ 2054625.0, 351125.0 ], [ 2054375.0, 351125.0 ], [ 2054375.0, 351375.0 ], [ 2054625.0, 351375.0 ], [ 2054625.0, 351625.0 ], [ 2054625.0, 351875.0 ], [ 2054625.0, 352125.0 ], [ 2054625.0, 352375.0 ], [ 2054625.0, 352625.0 ], [ 2054375.0, 352625.0 ], [ 2054125.0, 352625.0 ], [ 2053875.0, 352625.0 ], [ 2053625.0, 352625.0 ], [ 2053375.0, 352625.0 ], [ 2053375.0, 352375.0 ], [ 2053375.0, 352125.0 ], [ 2053125.0, 352125.0 ], [ 2053125.0, 352375.0 ], [ 2052875.0, 352375.0 ], [ 2052625.0, 352375.0 ], [ 2052375.0, 352375.0 ], [ 2052125.0, 352375.0 ], [ 2051875.0, 352375.0 ], [ 2051625.0, 352375.0 ], [ 2051625.0, 352125.0 ], [ 2051375.0, 352125.0 ], [ 2051125.0, 352125.0 ], [ 2051125.0, 351875.0 ], [ 2051125.0, 351625.0 ], [ 2051125.0, 351375.0 ], [ 2050875.0, 351375.0 ], [ 2050875.0, 351125.0 ], [ 2050875.0, 350875.0 ], [ 2051125.0, 350875.0 ], [ 2051125.0, 350625.0 ], [ 2051125.0, 350375.0 ], [ 2051375.0, 350375.0 ], [ 2051625.0, 350375.0 ], [ 2051875.0, 350375.0 ], [ 2052125.0, 350375.0 ], [ 2052375.0, 350375.0 ], [ 2052375.0, 350125.0 ], [ 2052375.0, 349875.0 ], [ 2052375.0, 349625.0 ], [ 2052125.0, 349625.0 ], [ 2052125.0, 349375.0 ], [ 2051875.0, 349375.0 ], [ 2051625.0, 349375.0 ], [ 2051375.0, 349375.0 ], [ 2051125.0, 349375.0 ], [ 2051125.0, 349125.0 ], [ 2051125.0, 348875.0 ], [ 2051125.0, 348625.0 ], [ 2051125.0, 348375.0 ], [ 2051375.0, 348375.0 ], [ 2051625.0, 348375.0 ], [ 2051625.0, 348125.0 ], [ 2051625.0, 347935.163654816860799 ], [ 2051781.506151334848255, 347891.426020058279391 ], [ 2051864.872799330158159, 347757.811560854723211 ], [ 2051855.0, 347751.324499999987893 ], [ 2051764.92, 347702.402 ], [ 2051674.817, 347658.195699999982025 ], [ 2051642.329075674060732, 347647.870469470217358 ], [ 2051577.727, 347627.338800000026822 ], [ 2051479.602, 347601.669500000018161 ], [ 2051403.036967576947063, 347581.186219056078698 ], [ 2051381.33, 347575.379000000015367 ], [ 2051284.082, 347549.719700000016019 ], [ 2051252.842, 347541.0538 ], [ 2051185.308, 347523.205100000021048 ], [ 2051162.892246897798032, 347517.011693853070028 ], [ 2051088.202, 347496.375 ], [ 2050989.633, 347471.4816 ], [ 2050955.508, 347461.728899999987334 ], [ 2050921.034117232076824, 347452.326108012755867 ], [ 2050891.608, 347444.3001 ], [ 2050792.656, 347418.07909999997355 ], [ 2050693.102999999886379, 347397.548300000024028 ], [ 2050680.816848023561761, 347396.403189528442454 ], [ 2050606.428, 347389.469900000025518 ], [ 2050432.790745197795331, 347381.637759556586388 ], [ 2050395.870000000111759, 347379.972400000027847 ], [ 2050295.031, 347380.165700000012293 ], [ 2050194.082, 347375.584900000016205 ], [ 2050184.511057119816542, 347373.972390304203145 ], [ 2050093.68, 347358.6692 ], [ 2049996.120000000111759, 347333.951300000015181 ], [ 2049942.999113350640982, 347316.274769303330686 ], [ 2049899.589, 347301.8296 ], [ 2049812.534, 347266.604899999976624 ], [ 2049806.409, 347264.1264 ], [ 2049713.02, 347226.494499999971595 ], [ 2049711.574092950439081, 347225.890785001625773 ], [ 2049620.142, 347187.714800000016112 ], [ 2049525.551, 347148.864200000010896 ], [ 2049481.395841051125899, 347132.290066618821584 ], [ 2049429.828, 347112.933499999984633 ], [ 2049335.862, 347075.1828 ], [ 2049253.963706719689071, 347032.600758539338131 ], [ 2049246.389999999897555, 347028.6629 ], [ 2049164.018, 346969.060999999986961 ], [ 2049143.874, 346950.678699999989476 ], [ 2049067.213, 346884.886600000027101 ], [ 2049059.556004673242569, 346878.25982995639788 ], [ 2048988.824, 346817.044600000022911 ], [ 2048908.692, 346757.0294 ], [ 2048862.720127717126161, 346726.940490917710122 ], [ 2048824.444, 346701.8885 ], [ 2048734.287, 346652.3418 ], [ 2048644.829203597269952, 346607.831182389636524 ], [ 2048643.018, 346606.93 ], [ 2048548.051, 346568.491900000022724 ], [ 2048451.509, 346535.565400000021327 ], [ 2048410.55518839834258, 346525.293504228000529 ], [ 2048354.399, 346511.208600000012666 ], [ 2048255.281999999890104, 346487.695499999972526 ], [ 2048169.136621022131294, 346466.638890414615162 ], [ 2048125.0, 346674.451247457182035 ], [ 2048125.0, 346875.0 ], [ 2048125.0, 347125.0 ], [ 2047875.0, 347125.0 ], [ 2047625.0, 347125.0 ], [ 2047625.0, 346875.0 ], [ 2047375.0, 346884.71753743494628 ], [ 2047376.290053668897599, 346783.877958185737953 ], [ 2047221.362432709662244, 346597.393960972491186 ], [ 2047206.933, 346605.079900000011548 ], [ 2047159.066000000108033, 346664.3712 ], [ 2047154.071, 346669.8028 ], [ 2047138.368, 346696.5062 ], [ 2047135.838, 346703.745500000019092 ], [ 2047135.395, 346703.76510000001872 ], [ 2047131.55, 346710.1251 ], [ 2047077.102, 346794.496700000017881 ], [ 2047074.832631461787969, 346796.353584631113335 ], [ 2046998.466, 346858.839700000011362 ], [ 2046911.172, 346908.778600000019651 ], [ 2046869.245368028059602, 346934.381285698153079 ], [ 2046825.058, 346961.364500000025146 ], [ 2046738.525, 347014.126800000027288 ], [ 2046660.080679264385253, 347063.249455272394698 ], [ 2046651.968000000109896, 347068.3297 ], [ 2046565.708000000100583, 347123.543899999989662 ], [ 2046485.556, 347184.981299999984913 ], [ 2046457.673041422618553, 347211.46309378487058 ], [ 2046411.049000000115484, 347255.744200000015553 ], [ 2046341.25, 347332.043799999984913 ], [ 2046287.932678144657984, 347392.772025281796232 ], [ 2046274.705, 347407.8383 ], [ 2046207.156999999890104, 347483.392400000011548 ], [ 2046139.892, 347558.403999999980442 ], [ 2046122.245087900198996, 347577.973078029346652 ], [ 2046075.053, 347630.305500000016764 ], [ 2046072.674000000115484, 347635.247199999983422 ], [ 2046003.847, 347713.34529999998631 ], [ 2045958.136711895698681, 347764.27390489471145 ], [ 2046133.482221366371959, 347904.488804459862877 ], [ 2046089.87689806940034, 348089.876898069458548 ], [ 2046125.0, 348125.0 ], [ 2046375.0, 348125.0 ], [ 2046625.0, 348125.0 ], [ 2046875.0, 348125.0 ], [ 2047125.0, 348125.0 ], [ 2047375.0, 348125.0 ], [ 2047625.0, 348125.0 ], [ 2047625.0, 348375.0 ], [ 2047875.0, 348375.0 ], [ 2048125.0, 348375.0 ], [ 2048125.0, 348625.0 ], [ 2048375.0, 348625.0 ], [ 2048375.0, 348875.0 ], [ 2048625.0, 348875.0 ], [ 2048625.0, 349125.0 ], [ 2048875.0, 349125.0 ], [ 2048875.0, 349375.0 ], [ 2048875.0, 349625.0 ], [ 2048625.0, 349625.0 ], [ 2048625.0, 349375.0 ], [ 2048375.0, 349375.0 ], [ 2048375.0, 349125.0 ], [ 2048125.0, 349125.0 ], [ 2047875.0, 349125.0 ], [ 2047875.0, 348875.0 ], [ 2047625.0, 348875.0 ], [ 2047625.0, 348625.0 ], [ 2047375.0, 348625.0 ], [ 2047125.0, 348625.0 ], [ 2046875.0, 348625.0 ], [ 2046625.0, 348625.0 ], [ 2046375.0, 348625.0 ], [ 2046125.0, 348625.0 ], [ 2045875.0, 348625.0 ], [ 2045875.0, 348438.28639834333444 ], [ 2045601.113818968413398, 348404.300741842540447 ], [ 2045638.91, 348320.3714 ], [ 2045683.655, 348230.872199999983422 ], [ 2045703.53, 348189.957 ], [ 2045704.146632959367707, 348188.635653424018528 ], [ 2045556.744589085225016, 348117.718749680614565 ], [ 2045392.826774591812864, 348142.826774591754656 ], [ 2045375.0, 348125.0 ], [ 2045125.0, 348125.0 ], [ 2044875.0, 348125.0 ], [ 2044875.0, 348375.0 ], [ 2044625.0, 348375.0 ], [ 2044625.0, 348625.0 ], [ 2044375.0, 348625.0 ], [ 2044375.0, 348875.0 ], [ 2044125.0, 348875.0 ], [ 2044125.0, 349125.0 ], [ 2044125.0, 349375.0 ], [ 2043875.0, 349375.0 ], [ 2043875.0, 349625.0 ], [ 2043625.0, 349625.0 ], [ 2043625.0, 349875.0 ], [ 2043375.0, 349875.0 ], [ 2043125.0, 349875.0 ], [ 2042875.0, 349875.0 ], [ 2042625.0, 349875.0 ], [ 2042375.0, 349875.0 ], [ 2042125.0, 349875.0 ], [ 2041875.0, 349875.0 ], [ 2041875.0, 349625.0 ], [ 2041625.0, 349625.0 ], [ 2041625.0, 349875.0 ], [ 2041375.0, 349875.0 ], [ 2041125.0, 349875.0 ], [ 2040875.0, 349875.0 ], [ 2040875.0, 349625.0 ], [ 2040625.0, 349625.0 ], [ 2040375.0, 349625.0 ], [ 2040125.0, 349625.0 ], [ 2039875.0, 349625.0 ], [ 2039625.0, 349625.0 ], [ 2039375.0, 349625.0 ], [ 2039375.0, 349375.0 ], [ 2039125.0, 349375.0 ], [ 2038875.0, 349375.0 ], [ 2038875.0, 349125.0 ], [ 2038625.0, 349125.0 ], [ 2038375.0, 349125.0 ], [ 2038375.0, 348875.0 ], [ 2038125.0, 348875.0 ], [ 2038125.0, 348625.0 ], [ 2037875.0, 348625.0 ], [ 2037625.0, 348625.0 ], [ 2037625.0, 348875.0 ], [ 2037375.0, 348875.0 ], [ 2037125.0, 348875.0 ], [ 2036875.0, 348875.0 ], [ 2036625.0, 348875.0 ], [ 2036625.0, 348625.0 ], [ 2036625.0, 348375.0 ], [ 2036625.0, 348125.0 ], [ 2036625.0, 347875.0 ], [ 2036625.0, 347625.0 ], [ 2036375.0, 347625.0 ], [ 2036375.0, 347375.0 ], [ 2036375.0, 347125.0 ], [ 2036625.0, 347125.0 ], [ 2036625.0, 346875.0 ], [ 2036375.0, 346875.0 ], [ 2036375.0, 346625.0 ], [ 2036375.0, 346375.0 ], [ 2036375.0, 346125.0 ], [ 2036625.0, 346125.0 ], [ 2036625.0, 345875.0 ], [ 2036375.0, 345875.0 ], [ 2036375.0, 345625.0 ], [ 2036125.0, 345625.0 ], [ 2036125.0, 345375.0 ], [ 2036125.0, 345125.0 ], [ 2035875.0, 345125.0 ], [ 2035875.0, 344875.0 ], [ 2036125.0, 344875.0 ], [ 2036125.0, 344625.0 ], [ 2036125.0, 344375.0 ], [ 2036125.0, 344125.0 ], [ 2036125.0, 343875.0 ], [ 2036125.0, 343625.0 ], [ 2036125.0, 343375.0 ], [ 2035875.0, 343375.0 ], [ 2035875.0, 343125.0 ], [ 2035875.0, 342875.0 ], [ 2035875.0, 342625.0 ], [ 2035875.0, 342375.0 ], [ 2035625.0, 342375.0 ], [ 2035625.0, 342125.0 ], [ 2035625.0, 341875.0 ], [ 2035375.0, 341875.0 ], [ 2035375.0, 341625.0 ], [ 2035125.0, 341625.0 ], [ 2035125.0, 341375.0 ], [ 2034875.0, 341375.0 ], [ 2034875.0, 341125.0 ], [ 2034875.0, 340875.0 ], [ 2034625.0, 340875.0 ], [ 2034625.0, 340625.0 ], [ 2034625.0, 340375.0 ], [ 2034375.0, 340375.0 ], [ 2034375.0, 340125.0 ], [ 2034125.0, 340125.0 ], [ 2034125.0, 339875.0 ], [ 2033875.0, 339875.0 ], [ 2033875.0, 339625.0 ], [ 2033625.0, 339625.0 ], [ 2033625.0, 339375.0 ], [ 2033375.0, 339375.0 ], [ 2033375.0, 339125.0 ], [ 2033125.0, 339125.0 ], [ 2033125.0, 338875.0 ], [ 2032875.0, 338875.0 ], [ 2032625.0, 338875.0 ], [ 2032375.0, 338875.0 ], [ 2032375.0, 338625.0 ], [ 2032125.0, 338625.0 ], [ 2032125.0, 338875.0 ], [ 2032125.0, 339125.0 ], [ 2031875.0, 339125.0 ], [ 2031875.0, 338875.0 ], [ 2031625.0, 338875.0 ], [ 2031625.0, 338625.0 ], [ 2031875.0, 338625.0 ], [ 2031875.0, 338375.0 ], [ 2031625.0, 338375.0 ], [ 2031375.0, 338375.0 ], [ 2031375.0, 338125.0 ], [ 2031125.0, 338125.0 ], [ 2031125.0, 338375.0 ], [ 2031125.0, 338625.0 ], [ 2030875.0, 338625.0 ], [ 2030625.0, 338625.0 ], [ 2030625.0, 338375.0 ], [ 2030625.0, 338125.0 ], [ 2030375.0, 338125.0 ], [ 2030125.0, 338125.0 ], [ 2030125.0, 337875.0 ], [ 2029875.0, 337875.0 ], [ 2029875.0, 337625.0 ], [ 2030125.0, 337625.0 ], [ 2030125.0, 337375.0 ], [ 2029875.0, 337375.0 ], [ 2029625.0, 337375.0 ], [ 2029625.0, 337125.0 ], [ 2029375.0, 337125.0 ], [ 2029375.0, 336875.0 ], [ 2029125.0, 336875.0 ], [ 2028875.0, 336875.0 ], [ 2028875.0, 336625.0 ], [ 2028625.0, 336625.0 ], [ 2028375.0, 336625.0 ], [ 2028375.0, 336375.0 ], [ 2028125.0, 336375.0 ], [ 2027875.0, 336375.0 ], [ 2027875.0, 336625.0 ], [ 2027625.0, 336625.0 ], [ 2027625.0, 336375.0 ], [ 2027625.0, 336125.0 ], [ 2027375.0, 336125.0 ], [ 2027125.0, 336125.0 ], [ 2027125.0, 335875.0 ], [ 2026875.0, 335875.0 ], [ 2026875.0, 335625.0 ], [ 2026625.0, 335625.0 ], [ 2026375.0, 335625.0 ], [ 2026125.0, 335625.0 ], [ 2026125.0, 335375.0 ], [ 2025875.0, 335375.0 ], [ 2025625.0, 335375.0 ], [ 2025625.0, 335125.0 ], [ 2025375.0, 335125.0 ], [ 2025125.0, 335125.0 ], [ 2025125.0, 334875.0 ], [ 2024875.0, 334875.0 ], [ 2024875.0, 334625.0 ], [ 2024625.0, 334625.0 ], [ 2024375.0, 334625.0 ], [ 2024125.0, 334625.0 ], [ 2024125.0, 334875.0 ], [ 2023875.0, 334875.0 ], [ 2023625.0, 334875.0 ], [ 2023625.0, 334625.0 ], [ 2023625.0, 334375.0 ], [ 2023375.0, 334375.0 ], [ 2023125.0, 334375.0 ], [ 2023125.0, 334125.0 ], [ 2022875.0, 334125.0 ], [ 2022875.0, 333875.0 ], [ 2022625.0, 333875.0 ], [ 2022625.0, 333625.0 ], [ 2022375.0, 333625.0 ], [ 2022375.0, 333375.0 ], [ 2022125.0, 333375.0 ], [ 2021875.0, 333375.0 ], [ 2021875.0, 333125.0 ], [ 2021625.0, 333125.0 ], [ 2021625.0, 332875.0 ], [ 2021375.0, 332875.0 ], [ 2021125.0, 332875.0 ], [ 2021125.0, 332625.0 ], [ 2020875.0, 332625.0 ], [ 2020875.0, 332375.0 ], [ 2020625.0, 332375.0 ], [ 2020625.0, 332125.0 ], [ 2020375.0, 332125.0 ], [ 2020375.0, 331875.0 ], [ 2020125.0, 331875.0 ], [ 2019875.0, 331875.0 ], [ 2019875.0, 331625.0 ], [ 2019875.0, 331375.0 ], [ 2019875.0, 331125.0 ], [ 2019625.0, 331125.0 ], [ 2019375.0, 331125.0 ], [ 2019375.0, 330875.0 ], [ 2019375.0, 330625.0 ], [ 2019125.0, 330625.0 ], [ 2019125.0, 330375.0 ], [ 2018875.0, 330375.0 ], [ 2018875.0, 330125.0 ], [ 2018875.0, 329875.0 ], [ 2018625.0, 329875.0 ], [ 2018625.0, 329625.0 ], [ 2018375.0, 329625.0 ], [ 2018375.0, 329375.0 ], [ 2018375.0, 329125.0 ], [ 2018375.0, 328875.0 ], [ 2018125.0, 328875.0 ], [ 2018125.0, 328625.0 ], [ 2018375.0, 328625.0 ], [ 2018375.0, 328375.0 ], [ 2018375.0, 328125.0 ], [ 2018125.0, 328125.0 ], [ 2018125.0, 328375.0 ], [ 2017875.0, 328375.0 ], [ 2017875.0, 328625.0 ], [ 2017875.0, 328875.0 ], [ 2017625.0, 328875.0 ], [ 2017625.0, 329125.0 ], [ 2017375.0, 329125.0 ], [ 2017125.0, 329125.0 ], [ 2017125.0, 329375.0 ], [ 2016875.0, 329375.0 ], [ 2016625.0, 329375.0 ], [ 2016625.0, 329625.0 ], [ 2016375.0, 329625.0 ], [ 2016125.0, 329625.0 ], [ 2016125.0, 329375.0 ], [ 2016125.0, 329125.0 ], [ 2016125.0, 328875.0 ], [ 2015875.0, 328875.0 ], [ 2015875.0, 328625.0 ], [ 2015625.0, 328625.0 ], [ 2015375.0, 328625.0 ], [ 2015375.0, 328375.0 ], [ 2015125.0, 328375.0 ], [ 2015125.0, 328125.0 ], [ 2015375.0, 328125.0 ], [ 2015625.0, 328125.0 ], [ 2015875.0, 328125.0 ], [ 2016125.0, 328125.0 ], [ 2016125.0, 328375.0 ], [ 2016375.0, 328375.0 ], [ 2016375.0, 328125.0 ], [ 2016375.0, 327875.0 ], [ 2016125.0, 327875.0 ], [ 2015875.0, 327875.0 ], [ 2015625.0, 327875.0 ], [ 2015625.0, 327625.0 ], [ 2015875.0, 327625.0 ], [ 2015875.0, 327375.0 ], [ 2015875.0, 327125.0 ], [ 2015875.0, 326875.0 ], [ 2016125.0, 326875.0 ], [ 2016375.0, 326875.0 ], [ 2016625.0, 326875.0 ], [ 2016625.0, 326625.0 ], [ 2016875.0, 326625.0 ], [ 2016875.0, 326375.0 ], [ 2016625.0, 326375.0 ], [ 2016375.0, 326375.0 ], [ 2016125.0, 326375.0 ], [ 2015875.0, 326375.0 ], [ 2015875.0, 326625.0 ], [ 2015625.0, 326625.0 ], [ 2015625.0, 326875.0 ], [ 2015375.0, 326875.0 ], [ 2015125.0, 326875.0 ], [ 2014875.0, 326875.0 ], [ 2014625.0, 326875.0 ], [ 2014625.0, 327125.0 ], [ 2014375.0, 327125.0 ], [ 2014125.0, 327125.0 ], [ 2014125.0, 327375.0 ], [ 2013875.0, 327375.0 ], [ 2013625.0, 327375.0 ], [ 2013375.0, 327375.0 ], [ 2013125.0, 327375.0 ], [ 2012875.0, 327375.0 ], [ 2012625.0, 327375.0 ], [ 2012375.0, 327375.0 ], [ 2012375.0, 327125.0 ], [ 2012125.0, 327125.0 ], [ 2012125.0, 326875.0 ], [ 2011875.0, 326875.0 ], [ 2011875.0, 326625.0 ], [ 2011625.0, 326625.0 ], [ 2011375.0, 326625.0 ], [ 2011375.0, 326375.0 ], [ 2011125.0, 326375.0 ], [ 2011125.0, 326125.0 ], [ 2010875.0, 326125.0 ], [ 2010625.0, 326125.0 ], [ 2010375.0, 326125.0 ], [ 2010375.0, 325875.0 ], [ 2010125.0, 325875.0 ], [ 2010125.0, 326125.0 ], [ 2009875.0, 326125.0 ], [ 2009625.0, 326125.0 ], [ 2009625.0, 325875.0 ], [ 2009375.0, 325875.0 ], [ 2009125.0, 325875.0 ], [ 2008875.0, 325875.0 ], [ 2008875.0, 326125.0 ], [ 2008625.0, 326125.0 ], [ 2008375.0, 326125.0 ], [ 2008375.0, 326375.0 ], [ 2008125.0, 326375.0 ], [ 2007875.0, 326375.0 ], [ 2007625.0, 326375.0 ], [ 2007375.0, 326375.0 ], [ 2007125.0, 326375.0 ], [ 2006875.0, 326375.0 ], [ 2006625.0, 326375.0 ], [ 2006625.0, 326125.0 ], [ 2006375.0, 326125.0 ], [ 2006125.0, 326125.0 ], [ 2005875.0, 326125.0 ], [ 2005625.0, 326125.0 ], [ 2005625.0, 326375.0 ], [ 2005375.0, 326375.0 ], [ 2005125.0, 326375.0 ], [ 2005125.0, 326125.0 ], [ 2005125.0, 325875.0 ], [ 2004875.0, 325875.0 ], [ 2004875.0, 325625.0 ], [ 2004875.0, 325375.0 ], [ 2004625.0, 325375.0 ], [ 2004625.0, 325125.0 ], [ 2004875.0, 325125.0 ], [ 2004875.0, 324875.0 ], [ 2005125.0, 324875.0 ], [ 2005125.0, 324625.0 ], [ 2005125.0, 324375.0 ], [ 2005125.0, 324125.0 ], [ 2004875.0, 324125.0 ], [ 2004625.0, 324125.0 ], [ 2004375.0, 324125.0 ], [ 2004375.0, 323875.0 ], [ 2004125.0, 323875.0 ], [ 2004125.0, 323625.0 ], [ 2003875.0, 323625.0 ], [ 2003625.0, 323625.0 ], [ 2003375.0, 323625.0 ], [ 2003125.0, 323625.0 ], [ 2003125.0, 323449.735253088292666 ], [ 2002991.896173857152462, 323380.988579147902783 ], [ 2002947.325, 323247.302000000025146 ], [ 2002853.736919600982219, 323091.999806937295943 ], [ 2002625.0, 323109.507223077933304 ], [ 2002625.0, 322875.0 ], [ 2002375.0, 322875.0 ], [ 2002125.0, 322875.0 ], [ 2002125.0, 322625.0 ], [ 2001875.0, 322625.0 ], [ 2001875.0, 322375.0 ], [ 2001625.0, 322375.0 ], [ 2001375.0, 322375.0 ], [ 2001375.0, 322125.0 ], [ 2001125.0, 322125.0 ], [ 2001125.0, 322375.0 ], [ 2000990.663948226952925, 322509.336051772930659 ], [ 2001261.442088720854372, 322738.557911279203836 ], [ 2001375.0, 322625.0 ], [ 2001625.0, 322625.0 ], [ 2001625.0, 322875.0 ], [ 2001532.220229214522988, 322967.779770785477012 ], [ 2001802.998369708191603, 323197.001630291750189 ], [ 2002122.076081019360572, 323467.110532658640295 ], [ 2002382.353472185786813, 323687.443191276630387 ], [ 2002615.857058472698554, 323885.111016871524043 ], [ 2002647.081654819892719, 323911.543579056975432 ], [ 2002747.629449365194887, 324002.370550634746905 ], [ 2003010.32812323491089, 324239.671876765030902 ], [ 2003026.951890740077943, 324254.688481212011538 ], [ 2003296.335206218762323, 324453.664793781237677 ], [ 2003625.0, 324696.428561914828606 ], [ 2003727.707755238516256, 324772.292244761425536 ], [ 2003850.479465520009398, 324862.975894401024561 ], [ 2003855.823512454051524, 324875.0 ], [ 2003887.912537110038102, 324947.200305458973162 ], [ 2003836.054292866028845, 325125.0 ], [ 2003822.404661840060726, 325171.798734944022726 ], [ 2003752.335260098101571, 325375.0 ], [ 2003728.821982889901847, 325443.188503905024845 ], [ 2003724.133367233676836, 325474.133367233676836 ], [ 2003682.030643410049379, 325752.01134444802301 ], [ 2003629.321219603298232, 325875.0 ], [ 2003522.178362462902442, 326125.0 ], [ 2003513.581821300089359, 326145.058596046990715 ], [ 2003457.439240917097777, 326207.439240917214192 ], [ 2003260.908588130027056, 326425.80663290398661 ] ], [ [ 2023875.0, 338125.0 ], [ 2023875.0, 337875.0 ], [ 2024125.0, 337875.0 ], [ 2024125.0, 338125.0 ], [ 2023875.0, 338125.0 ] ], [ [ 2035375.0, 347375.0 ], [ 2035375.0, 347125.0 ], [ 2035375.0, 346875.0 ], [ 2035375.0, 346625.0 ], [ 2035625.0, 346625.0 ], [ 2035625.0, 346875.0 ], [ 2035625.0, 347125.0 ], [ 2035625.0, 347375.0 ], [ 2035625.0, 347625.0 ], [ 2035375.0, 347625.0 ], [ 2035375.0, 347375.0 ] ], [ [ 2035625.0, 346375.0 ], [ 2035875.0, 346375.0 ], [ 2035875.0, 346625.0 ], [ 2035625.0, 346625.0 ], [ 2035625.0, 346375.0 ] ], [ [ 2036125.0, 346625.0 ], [ 2036125.0, 346875.0 ], [ 2036125.0, 347125.0 ], [ 2036125.0, 347375.0 ], [ 2035875.0, 347375.0 ], [ 2035875.0, 347125.0 ], [ 2035875.0, 346875.0 ], [ 2035875.0, 346625.0 ], [ 2036125.0, 346625.0 ] ], [ [ 2036625.0, 349125.0 ], [ 2036375.0, 349125.0 ], [ 2036375.0, 348875.0 ], [ 2036625.0, 348875.0 ], [ 2036625.0, 349125.0 ] ], [ [ 2026875.0, 336125.0 ], [ 2026625.0, 336125.0 ], [ 2026625.0, 335875.0 ], [ 2026875.0, 335875.0 ], [ 2026875.0, 336125.0 ] ], [ [ 2023625.0, 335125.0 ], [ 2023375.0, 335125.0 ], [ 2023375.0, 334875.0 ], [ 2023625.0, 334875.0 ], [ 2023625.0, 335125.0 ] ], [ [ 2011375.0, 326875.0 ], [ 2011375.0, 327125.0 ], [ 2011125.0, 327125.0 ], [ 2011125.0, 326875.0 ], [ 2011125.0, 326625.0 ], [ 2011375.0, 326625.0 ], [ 2011375.0, 326875.0 ] ], [ [ 2011125.0, 327375.0 ], [ 2010875.0, 327375.0 ], [ 2010875.0, 327125.0 ], [ 2011125.0, 327125.0 ], [ 2011125.0, 327375.0 ] ], [ [ 2010625.0, 327125.0 ], [ 2010625.0, 326875.0 ], [ 2010875.0, 326875.0 ], [ 2010875.0, 327125.0 ], [ 2010625.0, 327125.0 ] ], [ [ 2008625.0, 326375.0 ], [ 2008625.0, 326625.0 ], [ 2008375.0, 326625.0 ], [ 2008375.0, 326375.0 ], [ 2008625.0, 326375.0 ] ], [ [ 2008375.0, 332875.0 ], [ 2008375.0, 332625.0 ], [ 2008625.0, 332625.0 ], [ 2008625.0, 332875.0 ], [ 2008625.0, 333125.0 ], [ 2008625.0, 333375.0 ], [ 2008375.0, 333375.0 ], [ 2008375.0, 333125.0 ], [ 2008375.0, 332875.0 ] ], [ [ 2008625.0, 332375.0 ], [ 2008875.0, 332375.0 ], [ 2008875.0, 332625.0 ], [ 2008625.0, 332625.0 ], [ 2008625.0, 332375.0 ] ], [ [ 2009375.0, 332875.0 ], [ 2009375.0, 333125.0 ], [ 2009375.0, 333375.0 ], [ 2009625.0, 333375.0 ], [ 2009625.0, 333625.0 ], [ 2009375.0, 333625.0 ], [ 2009125.0, 333625.0 ], [ 2009125.0, 333375.0 ], [ 2009125.0, 333125.0 ], [ 2009125.0, 332875.0 ], [ 2009375.0, 332875.0 ] ], [ [ 2009375.0, 326625.0 ], [ 2009125.0, 326625.0 ], [ 2009125.0, 326375.0 ], [ 2009375.0, 326375.0 ], [ 2009375.0, 326625.0 ] ], [ [ 2009625.0, 331625.0 ], [ 2009625.0, 331875.0 ], [ 2009375.0, 331875.0 ], [ 2009375.0, 331625.0 ], [ 2009625.0, 331625.0 ] ], [ [ 2010125.0, 334125.0 ], [ 2010125.0, 334375.0 ], [ 2009875.0, 334375.0 ], [ 2009875.0, 334125.0 ], [ 2009875.0, 333875.0 ], [ 2010125.0, 333875.0 ], [ 2010375.0, 333875.0 ], [ 2010375.0, 334125.0 ], [ 2010125.0, 334125.0 ] ], [ [ 2008125.0, 333875.0 ], [ 2008125.0, 333625.0 ], [ 2008375.0, 333625.0 ], [ 2008375.0, 333875.0 ], [ 2008125.0, 333875.0 ] ], [ [ 2008125.0, 334125.0 ], [ 2008375.0, 334125.0 ], [ 2008375.0, 334375.0 ], [ 2008125.0, 334375.0 ], [ 2008125.0, 334125.0 ] ], [ [ 2014875.0, 332125.0 ], [ 2015125.0, 332125.0 ], [ 2015125.0, 332375.0 ], [ 2014875.0, 332375.0 ], [ 2014875.0, 332125.0 ] ], [ [ 2026125.0, 336625.0 ], [ 2026375.0, 336625.0 ], [ 2026375.0, 336875.0 ], [ 2026625.0, 336875.0 ], [ 2026625.0, 337125.0 ], [ 2026875.0, 337125.0 ], [ 2027125.0, 337125.0 ], [ 2027375.0, 337125.0 ], [ 2027375.0, 337375.0 ], [ 2027375.0, 337625.0 ], [ 2027125.0, 337625.0 ], [ 2026875.0, 337625.0 ], [ 2026875.0, 337375.0 ], [ 2026625.0, 337375.0 ], [ 2026375.0, 337375.0 ], [ 2026375.0, 337125.0 ], [ 2026125.0, 337125.0 ], [ 2026125.0, 336875.0 ], [ 2025875.0, 336875.0 ], [ 2025875.0, 336625.0 ], [ 2026125.0, 336625.0 ] ], [ [ 2027625.0, 337625.0 ], [ 2027625.0, 337875.0 ], [ 2027375.0, 337875.0 ], [ 2027375.0, 337625.0 ], [ 2027625.0, 337625.0 ] ], [ [ 2026875.0, 340125.0 ], [ 2026625.0, 340125.0 ], [ 2026625.0, 339875.0 ], [ 2026875.0, 339875.0 ], [ 2026875.0, 340125.0 ] ], [ [ 2027125.0, 340125.0 ], [ 2027375.0, 340125.0 ], [ 2027375.0, 340375.0 ], [ 2027125.0, 340375.0 ], [ 2026875.0, 340375.0 ], [ 2026875.0, 340125.0 ], [ 2027125.0, 340125.0 ] ], [ [ 2027375.0, 339875.0 ], [ 2027625.0, 339875.0 ], [ 2027625.0, 340125.0 ], [ 2027375.0, 340125.0 ], [ 2027375.0, 339875.0 ] ], [ [ 2025625.0, 339375.0 ], [ 2025875.0, 339375.0 ], [ 2025875.0, 339625.0 ], [ 2025625.0, 339625.0 ], [ 2025625.0, 339375.0 ] ], [ [ 2025625.0, 338875.0 ], [ 2025875.0, 338875.0 ], [ 2025875.0, 339125.0 ], [ 2025625.0, 339125.0 ], [ 2025625.0, 338875.0 ] ], [ [ 2038875.0, 349875.0 ], [ 2038875.0, 349625.0 ], [ 2039125.0, 349625.0 ], [ 2039125.0, 349875.0 ], [ 2038875.0, 349875.0 ] ], [ [ 2012625.0, 331125.0 ], [ 2012375.0, 331125.0 ], [ 2012375.0, 330875.0 ], [ 2012375.0, 330625.0 ], [ 2012625.0, 330625.0 ], [ 2012625.0, 330875.0 ], [ 2012625.0, 331125.0 ] ], [ [ 2012125.0, 330625.0 ], [ 2012125.0, 330375.0 ], [ 2012375.0, 330375.0 ], [ 2012375.0, 330625.0 ], [ 2012125.0, 330625.0 ] ], [ [ 2012375.0, 330125.0 ], [ 2012625.0, 330125.0 ], [ 2012875.0, 330125.0 ], [ 2013125.0, 330125.0 ], [ 2013125.0, 330375.0 ], [ 2012875.0, 330375.0 ], [ 2012625.0, 330375.0 ], [ 2012375.0, 330375.0 ], [ 2012375.0, 330125.0 ] ], [ [ 2013125.0, 331125.0 ], [ 2012875.0, 331125.0 ], [ 2012875.0, 330875.0 ], [ 2013125.0, 330875.0 ], [ 2013375.0, 330875.0 ], [ 2013375.0, 331125.0 ], [ 2013125.0, 331125.0 ] ], [ [ 2013875.0, 328625.0 ], [ 2014125.0, 328625.0 ], [ 2014125.0, 328875.0 ], [ 2013875.0, 328875.0 ], [ 2013875.0, 328625.0 ] ], [ [ 2002640.221391269238666, 323263.745681104890537 ], [ 2002375.0, 323363.963568864972331 ], [ 2002375.0, 323125.0 ], [ 2002613.391619356349111, 323125.0 ], [ 2002640.221391269238666, 323263.745681104890537 ] ], [ [ 2001375.0, 328625.0 ], [ 2001625.0, 328625.0 ], [ 2001625.0, 328875.0 ], [ 2001375.0, 328875.0 ], [ 2001375.0, 328625.0 ] ], [ [ 2002125.0, 328375.0 ], [ 2002125.0, 328125.0 ], [ 2002375.0, 328125.0 ], [ 2002375.0, 328375.0 ], [ 2002125.0, 328375.0 ] ], [ [ 2006375.0, 326625.0 ], [ 2006125.0, 326625.0 ], [ 2006125.0, 326375.0 ], [ 2006375.0, 326375.0 ], [ 2006375.0, 326625.0 ] ], [ [ 2008125.0, 326875.0 ], [ 2008125.0, 327125.0 ], [ 2007875.0, 327125.0 ], [ 2007875.0, 326875.0 ], [ 2007875.0, 326625.0 ], [ 2008125.0, 326625.0 ], [ 2008125.0, 326875.0 ] ], [ [ 2010875.0, 330125.0 ], [ 2010875.0, 329875.0 ], [ 2011125.0, 329875.0 ], [ 2011125.0, 330125.0 ], [ 2010875.0, 330125.0 ] ], [ [ 2009875.0, 327625.0 ], [ 2009625.0, 327625.0 ], [ 2009625.0, 327375.0 ], [ 2009875.0, 327375.0 ], [ 2009875.0, 327625.0 ] ], [ [ 2010125.0, 326875.0 ], [ 2010125.0, 326625.0 ], [ 2010125.0, 326375.0 ], [ 2010375.0, 326375.0 ], [ 2010375.0, 326625.0 ], [ 2010375.0, 326875.0 ], [ 2010125.0, 326875.0 ] ], [ [ 2009625.0, 328375.0 ], [ 2009875.0, 328375.0 ], [ 2009875.0, 328625.0 ], [ 2009625.0, 328625.0 ], [ 2009625.0, 328375.0 ] ], [ [ 2010625.0, 328375.0 ], [ 2010875.0, 328375.0 ], [ 2010875.0, 328625.0 ], [ 2010625.0, 328625.0 ], [ 2010625.0, 328375.0 ] ], [ [ 2010375.0, 329375.0 ], [ 2010125.0, 329375.0 ], [ 2010125.0, 329125.0 ], [ 2010375.0, 329125.0 ], [ 2010375.0, 329375.0 ] ], [ [ 2010375.0, 329875.0 ], [ 2010375.0, 330125.0 ], [ 2010125.0, 330125.0 ], [ 2010125.0, 329875.0 ], [ 2010375.0, 329875.0 ] ], [ [ 2012375.0, 329375.0 ], [ 2012125.0, 329375.0 ], [ 2012125.0, 329125.0 ], [ 2012375.0, 329125.0 ], [ 2012625.0, 329125.0 ], [ 2012625.0, 329375.0 ], [ 2012375.0, 329375.0 ] ], [ [ 2012875.0, 329375.0 ], [ 2012875.0, 329625.0 ], [ 2012625.0, 329625.0 ], [ 2012625.0, 329375.0 ], [ 2012875.0, 329375.0 ] ], [ [ 2013125.0, 327625.0 ], [ 2013375.0, 327625.0 ], [ 2013625.0, 327625.0 ], [ 2013625.0, 327875.0 ], [ 2013375.0, 327875.0 ], [ 2013125.0, 327875.0 ], [ 2013125.0, 328125.0 ], [ 2012875.0, 328125.0 ], [ 2012625.0, 328125.0 ], [ 2012625.0, 327875.0 ], [ 2012875.0, 327875.0 ], [ 2012875.0, 327625.0 ], [ 2013125.0, 327625.0 ] ], [ [ 2012375.0, 327625.0 ], [ 2012375.0, 327875.0 ], [ 2012125.0, 327875.0 ], [ 2012125.0, 327625.0 ], [ 2012375.0, 327625.0 ] ], [ [ 2013375.0, 330125.0 ], [ 2013375.0, 329875.0 ], [ 2013625.0, 329875.0 ], [ 2013625.0, 330125.0 ], [ 2013625.0, 330375.0 ], [ 2013375.0, 330375.0 ], [ 2013375.0, 330125.0 ] ], [ [ 2028125.0, 338125.0 ], [ 2027875.0, 338125.0 ], [ 2027875.0, 337875.0 ], [ 2028125.0, 337875.0 ], [ 2028125.0, 338125.0 ] ], [ [ 2012625.0, 332125.0 ], [ 2012875.0, 332125.0 ], [ 2012875.0, 332375.0 ], [ 2012625.0, 332375.0 ], [ 2012625.0, 332125.0 ] ], [ [ 2016625.0, 330375.0 ], [ 2016375.0, 330375.0 ], [ 2016375.0, 330125.0 ], [ 2016375.0, 329875.0 ], [ 2016625.0, 329875.0 ], [ 2016625.0, 330125.0 ], [ 2016625.0, 330375.0 ] ], [ [ 2015875.0, 329375.0 ], [ 2015875.0, 329625.0 ], [ 2015625.0, 329625.0 ], [ 2015625.0, 329375.0 ], [ 2015875.0, 329375.0 ] ], [ [ 2014375.0, 330625.0 ], [ 2014375.0, 330375.0 ], [ 2014625.0, 330375.0 ], [ 2014625.0, 330625.0 ], [ 2014375.0, 330625.0 ] ], [ [ 2018125.0, 331375.0 ], [ 2017875.0, 331375.0 ], [ 2017875.0, 331125.0 ], [ 2018125.0, 331125.0 ], [ 2018125.0, 331375.0 ] ], [ [ 2019875.0, 332625.0 ], [ 2020125.0, 332625.0 ], [ 2020125.0, 332875.0 ], [ 2019875.0, 332875.0 ], [ 2019625.0, 332875.0 ], [ 2019375.0, 332875.0 ], [ 2019375.0, 332625.0 ], [ 2019625.0, 332625.0 ], [ 2019875.0, 332625.0 ] ], [ [ 2018375.0, 330875.0 ], [ 2018125.0, 330875.0 ], [ 2018125.0, 330625.0 ], [ 2018125.0, 330375.0 ], [ 2018375.0, 330375.0 ], [ 2018375.0, 330625.0 ], [ 2018375.0, 330875.0 ] ], [ [ 2025125.0, 335625.0 ], [ 2024875.0, 335625.0 ], [ 2024625.0, 335625.0 ], [ 2024375.0, 335625.0 ], [ 2024375.0, 335375.0 ], [ 2024625.0, 335375.0 ], [ 2024625.0, 335125.0 ], [ 2024875.0, 335125.0 ], [ 2024875.0, 335375.0 ], [ 2025125.0, 335375.0 ], [ 2025125.0, 335625.0 ] ], [ [ 2025625.0, 336125.0 ], [ 2025375.0, 336125.0 ], [ 2025375.0, 335875.0 ], [ 2025625.0, 335875.0 ], [ 2025625.0, 336125.0 ] ], [ [ 2025375.0, 336625.0 ], [ 2025625.0, 336625.0 ], [ 2025625.0, 336875.0 ], [ 2025375.0, 336875.0 ], [ 2025375.0, 336625.0 ] ], [ [ 2028125.0, 337125.0 ], [ 2028125.0, 337375.0 ], [ 2027875.0, 337375.0 ], [ 2027875.0, 337125.0 ], [ 2028125.0, 337125.0 ] ], [ [ 2025875.0, 337375.0 ], [ 2026125.0, 337375.0 ], [ 2026125.0, 337625.0 ], [ 2025875.0, 337625.0 ], [ 2025875.0, 337375.0 ] ], [ [ 2033625.0, 343625.0 ], [ 2033375.0, 343625.0 ], [ 2033125.0, 343625.0 ], [ 2033125.0, 343375.0 ], [ 2033125.0, 343125.0 ], [ 2033375.0, 343125.0 ], [ 2033625.0, 343125.0 ], [ 2033625.0, 343375.0 ], [ 2033625.0, 343625.0 ] ], [ [ 2033875.0, 342875.0 ], [ 2034125.0, 342875.0 ], [ 2034125.0, 343125.0 ], [ 2033875.0, 343125.0 ], [ 2033875.0, 342875.0 ] ], [ [ 2034375.0, 343125.0 ], [ 2034375.0, 343375.0 ], [ 2034125.0, 343375.0 ], [ 2034125.0, 343125.0 ], [ 2034375.0, 343125.0 ] ], [ [ 2035625.0, 343875.0 ], [ 2035375.0, 343875.0 ], [ 2035375.0, 343625.0 ], [ 2035625.0, 343625.0 ], [ 2035625.0, 343875.0 ] ], [ [ 2035125.0, 342875.0 ], [ 2035125.0, 342625.0 ], [ 2035375.0, 342625.0 ], [ 2035625.0, 342625.0 ], [ 2035625.0, 342875.0 ], [ 2035375.0, 342875.0 ], [ 2035125.0, 342875.0 ] ], [ [ 2032125.0, 340625.0 ], [ 2032375.0, 340625.0 ], [ 2032625.0, 340625.0 ], [ 2032625.0, 340875.0 ], [ 2032375.0, 340875.0 ], [ 2032125.0, 340875.0 ], [ 2032125.0, 340625.0 ] ], [ [ 2031625.0, 339875.0 ], [ 2031375.0, 339875.0 ], [ 2031375.0, 339625.0 ], [ 2031625.0, 339625.0 ], [ 2031625.0, 339875.0 ] ], [ [ 2030125.0, 342125.0 ], [ 2030125.0, 342375.0 ], [ 2030125.0, 342625.0 ], [ 2029875.0, 342625.0 ], [ 2029875.0, 342375.0 ], [ 2029875.0, 342125.0 ], [ 2030125.0, 342125.0 ] ], [ [ 2031875.0, 343625.0 ], [ 2032125.0, 343625.0 ], [ 2032125.0, 343875.0 ], [ 2031875.0, 343875.0 ], [ 2031875.0, 343625.0 ] ], [ [ 2032875.0, 341625.0 ], [ 2032875.0, 341375.0 ], [ 2032875.0, 341125.0 ], [ 2032875.0, 340875.0 ], [ 2033125.0, 340875.0 ], [ 2033125.0, 341125.0 ], [ 2033125.0, 341375.0 ], [ 2033125.0, 341625.0 ], [ 2032875.0, 341625.0 ] ], [ [ 2033375.0, 340375.0 ], [ 2033125.0, 340375.0 ], [ 2033125.0, 340125.0 ], [ 2033375.0, 340125.0 ], [ 2033625.0, 340125.0 ], [ 2033625.0, 340375.0 ], [ 2033375.0, 340375.0 ] ], [ [ 2034125.0, 340875.0 ], [ 2034125.0, 340625.0 ], [ 2034375.0, 340625.0 ], [ 2034375.0, 340875.0 ], [ 2034375.0, 341125.0 ], [ 2034125.0, 341125.0 ], [ 2034125.0, 340875.0 ] ], [ [ 2034625.0, 342375.0 ], [ 2034875.0, 342375.0 ], [ 2034875.0, 342625.0 ], [ 2034625.0, 342625.0 ], [ 2034625.0, 342375.0 ] ], [ [ 2036625.0, 350125.0 ], [ 2036625.0, 350375.0 ], [ 2036375.0, 350375.0 ], [ 2036375.0, 350125.0 ], [ 2036625.0, 350125.0 ] ], [ [ 2038125.0, 350125.0 ], [ 2038125.0, 350375.0 ], [ 2037875.0, 350375.0 ], [ 2037875.0, 350125.0 ], [ 2038125.0, 350125.0 ] ], [ [ 2047375.0, 351875.0 ], [ 2047125.0, 351875.0 ], [ 2047125.0, 351625.0 ], [ 2047375.0, 351625.0 ], [ 2047375.0, 351875.0 ] ], [ [ 2047375.0, 351375.0 ], [ 2047375.0, 351125.0 ], [ 2047375.0, 350875.0 ], [ 2047625.0, 350875.0 ], [ 2047625.0, 351125.0 ], [ 2047625.0, 351375.0 ], [ 2047625.0, 351625.0 ], [ 2047375.0, 351625.0 ], [ 2047375.0, 351375.0 ] ], [ [ 2047625.0, 350625.0 ], [ 2047875.0, 350625.0 ], [ 2048125.0, 350625.0 ], [ 2048125.0, 350875.0 ], [ 2048125.0, 351125.0 ], [ 2047875.0, 351125.0 ], [ 2047875.0, 350875.0 ], [ 2047625.0, 350875.0 ], [ 2047625.0, 350625.0 ] ], [ [ 2043625.0, 350375.0 ], [ 2043625.0, 350125.0 ], [ 2043875.0, 350125.0 ], [ 2043875.0, 350375.0 ], [ 2043625.0, 350375.0 ] ], [ [ 2046125.0, 350625.0 ], [ 2046125.0, 350875.0 ], [ 2045875.0, 350875.0 ], [ 2045875.0, 350625.0 ], [ 2046125.0, 350625.0 ] ], [ [ 2046125.0, 348875.0 ], [ 2046125.0, 349125.0 ], [ 2045875.0, 349125.0 ], [ 2045875.0, 348875.0 ], [ 2046125.0, 348875.0 ] ], [ [ 2046875.0, 355375.0 ], [ 2046875.0, 355625.0 ], [ 2046625.0, 355625.0 ], [ 2046625.0, 355375.0 ], [ 2046875.0, 355375.0 ] ], [ [ 2048875.0, 353875.0 ], [ 2048875.0, 353625.0 ], [ 2048625.0, 353625.0 ], [ 2048500.0, 353500.0 ], [ 2048625.0, 353375.0 ], [ 2048875.0, 353375.0 ], [ 2049125.0, 353375.0 ], [ 2049125.0, 353625.0 ], [ 2049125.0, 353875.0 ], [ 2049125.0, 354125.0 ], [ 2048875.0, 354125.0 ], [ 2048875.0, 353875.0 ] ], [ [ 2049375.0, 352125.0 ], [ 2049375.0, 352375.0 ], [ 2049625.0, 352375.0 ], [ 2049625.0, 352625.0 ], [ 2049625.0, 352875.0 ], [ 2049625.0, 353125.0 ], [ 2049375.0, 353125.0 ], [ 2049125.0, 353125.0 ], [ 2049125.0, 352875.0 ], [ 2049125.0, 352625.0 ], [ 2049125.0, 352375.0 ], [ 2048875.0, 352375.0 ], [ 2048875.0, 352125.0 ], [ 2048875.0, 351875.0 ], [ 2049125.0, 351875.0 ], [ 2049125.0, 351625.0 ], [ 2049375.0, 351625.0 ], [ 2049375.0, 351875.0 ], [ 2049625.0, 351875.0 ], [ 2049625.0, 352125.0 ], [ 2049375.0, 352125.0 ] ], [ [ 2049125.0, 351125.0 ], [ 2049375.0, 351125.0 ], [ 2049375.0, 351375.0 ], [ 2049125.0, 351375.0 ], [ 2049125.0, 351125.0 ] ], [ [ 2049375.0, 350375.0 ], [ 2049375.0, 350125.0 ], [ 2049625.0, 350125.0 ], [ 2049625.0, 350375.0 ], [ 2049625.0, 350625.0 ], [ 2049375.0, 350625.0 ], [ 2049375.0, 350875.0 ], [ 2049125.0, 350875.0 ], [ 2049125.0, 350625.0 ], [ 2049125.0, 350375.0 ], [ 2049375.0, 350375.0 ] ], [ [ 2048375.0, 351875.0 ], [ 2048125.0, 351875.0 ], [ 2048125.0, 351625.0 ], [ 2048375.0, 351625.0 ], [ 2048375.0, 351875.0 ] ], [ [ 2048375.0, 351375.0 ], [ 2048625.0, 351375.0 ], [ 2048625.0, 351625.0 ], [ 2048375.0, 351625.0 ], [ 2048375.0, 351375.0 ] ], [ [ 2047875.0, 349875.0 ], [ 2048125.0, 349875.0 ], [ 2048375.0, 349875.0 ], [ 2048375.0, 350125.0 ], [ 2048125.0, 350125.0 ], [ 2047875.0, 350125.0 ], [ 2047875.0, 349875.0 ] ], [ [ 2049625.0, 348375.0 ], [ 2049625.0, 348125.0 ], [ 2049625.0, 347875.0 ], [ 2049625.0, 347625.0 ], [ 2049875.0, 347625.0 ], [ 2049875.0, 347875.0 ], [ 2049875.0, 348125.0 ], [ 2049875.0, 348375.0 ], [ 2049625.0, 348375.0 ] ], [ [ 2050125.0, 351125.0 ], [ 2050125.0, 350875.0 ], [ 2050375.0, 350875.0 ], [ 2050375.0, 351125.0 ], [ 2050125.0, 351125.0 ] ], [ [ 2049875.0, 351375.0 ], [ 2050125.0, 351375.0 ], [ 2050125.0, 351625.0 ], [ 2050125.0, 351875.0 ], [ 2049875.0, 351875.0 ], [ 2049875.0, 351625.0 ], [ 2049875.0, 351375.0 ] ], [ [ 2049625.0, 356875.0 ], [ 2049625.0, 356625.0 ], [ 2049875.0, 356625.0 ], [ 2049875.0, 356875.0 ], [ 2049625.0, 356875.0 ] ], [ [ 2050125.0, 356625.0 ], [ 2050125.0, 356375.0 ], [ 2050375.0, 356375.0 ], [ 2050375.0, 356625.0 ], [ 2050125.0, 356625.0 ] ], [ [ 2055875.0, 352375.0 ], [ 2056125.0, 352375.0 ], [ 2056375.0, 352375.0 ], [ 2056625.0, 352375.0 ], [ 2056625.0, 352625.0 ], [ 2056375.0, 352625.0 ], [ 2056125.0, 352625.0 ], [ 2055875.0, 352625.0 ], [ 2055875.0, 352375.0 ] ], [ [ 2057625.0, 353375.0 ], [ 2057625.0, 353625.0 ], [ 2057375.0, 353625.0 ], [ 2057375.0, 353375.0 ], [ 2057625.0, 353375.0 ] ] ], [ [ [ 2007973.371252968208864, 320581.881023104069754 ], [ 2007802.646582176443189, 320758.666990148311015 ], [ 2007631.921911379788071, 320935.452957197616342 ], [ 2007325.133, 321253.133900000015274 ], [ 2007568.955295509891585, 321529.296566680888645 ], [ 2007771.765017242636532, 321271.765017242520116 ], [ 2007839.446900221286342, 321125.0 ], [ 2008125.0, 321125.0 ], [ 2008125.0, 320866.190288251324091 ], [ 2008375.0, 320875.0 ], [ 2008625.0, 320875.0 ], [ 2008625.0, 321125.0 ], [ 2008875.0, 321125.0 ], [ 2008875.0, 321375.0 ], [ 2009125.0, 321375.0 ], [ 2009125.0, 321625.0 ], [ 2009125.0, 321875.0 ], [ 2008875.0, 321875.0 ], [ 2008625.0, 321875.0 ], [ 2008625.0, 322125.0 ], [ 2008875.0, 322125.0 ], [ 2008875.0, 322375.0 ], [ 2009125.0, 322375.0 ], [ 2009125.0, 322125.0 ], [ 2009375.0, 322125.0 ], [ 2009625.0, 322125.0 ], [ 2009625.0, 322375.0 ], [ 2009875.0, 322375.0 ], [ 2009875.0, 322625.0 ], [ 2010125.0, 322625.0 ], [ 2010375.0, 322625.0 ], [ 2010375.0, 322875.0 ], [ 2010375.0, 323125.0 ], [ 2010625.0, 323125.0 ], [ 2010625.0, 323375.0 ], [ 2010625.0, 323625.0 ], [ 2010875.0, 323625.0 ], [ 2010875.0, 323875.0 ], [ 2011125.0, 323875.0 ], [ 2011125.0, 324125.0 ], [ 2011125.0, 324375.0 ], [ 2011125.0, 324625.0 ], [ 2011125.0, 324875.0 ], [ 2011375.0, 324875.0 ], [ 2011375.0, 325125.0 ], [ 2011625.0, 325125.0 ], [ 2011625.0, 325375.0 ], [ 2011875.0, 325375.0 ], [ 2012125.0, 325375.0 ], [ 2012375.0, 325375.0 ], [ 2012625.0, 325375.0 ], [ 2012875.0, 325375.0 ], [ 2012875.0, 325625.0 ], [ 2013125.0, 325625.0 ], [ 2013125.0, 325375.0 ], [ 2013375.0, 325375.0 ], [ 2013625.0, 325375.0 ], [ 2013875.0, 325375.0 ], [ 2014125.0, 325375.0 ], [ 2014125.0, 325125.0 ], [ 2014375.0, 325125.0 ], [ 2014375.0, 325375.0 ], [ 2014625.0, 325375.0 ], [ 2014875.0, 325375.0 ], [ 2015125.0, 325375.0 ], [ 2015125.0, 325125.0 ], [ 2015375.0, 325125.0 ], [ 2015625.0, 325125.0 ], [ 2015875.0, 325125.0 ], [ 2016125.0, 325125.0 ], [ 2016375.0, 325125.0 ], [ 2016375.0, 325375.0 ], [ 2016625.0, 325375.0 ], [ 2016875.0, 325375.0 ], [ 2016875.0, 325625.0 ], [ 2017125.0, 325625.0 ], [ 2017279.628278370480984, 325470.371721629460808 ], [ 2017101.786805160110816, 325329.876957793021575 ], [ 2016989.656413544202223, 325260.343586455914192 ], [ 2016625.0, 325034.215866325830575 ], [ 2016526.725379060953856, 324973.27462093916256 ], [ 2016218.104689405299723, 324781.895310594642069 ], [ 2015875.0, 324569.132067444443237 ], [ 2015875.0, 324875.0 ], [ 2015625.0, 324875.0 ], [ 2015625.0, 324625.0 ], [ 2015375.0, 324625.0 ], [ 2015125.0, 324625.0 ], [ 2015125.0, 324375.0 ], [ 2014875.0, 324375.0 ], [ 2014875.0, 324125.0 ], [ 2014625.0, 324125.0 ], [ 2014375.0, 324125.0 ], [ 2014375.0, 323718.316330191039015 ], [ 2014298.791663677198812, 323701.20833632274298 ], [ 2014125.0, 323875.0 ], [ 2014125.0, 324125.0 ], [ 2013875.0, 324125.0 ], [ 2013875.0, 323875.0 ], [ 2013875.0, 323614.831289860594552 ], [ 2013625.0, 323582.573225344181992 ], [ 2013386.554450759897009, 323551.806057700014208 ], [ 2013375.0, 323549.165040383464657 ], [ 2013125.0, 323492.022183241206221 ], [ 2013029.749385733623058, 323470.250614266318735 ], [ 2012731.475698089925572, 323402.073771376977675 ], [ 2012625.0, 323365.850080274394713 ], [ 2012375.0, 323280.798533883062191 ], [ 2012258.750324717955664, 323241.249675282160752 ], [ 2011875.0, 323110.695441100455355 ], [ 2011823.723712249891832, 323093.250930835027248 ], [ 2011720.56660290970467, 323029.433397090237122 ], [ 2011625.0, 323125.0 ], [ 2011375.0, 323125.0 ], [ 2011125.0, 323125.0 ], [ 2011125.0, 322875.0 ], [ 2011257.21581757068634, 322742.784182429371867 ], [ 2010875.0, 322506.328634269651957 ], [ 2010793.86503223143518, 322456.134967768448405 ], [ 2010719.4481006199494, 322410.097374483011663 ], [ 2010518.892821161542088, 322231.107178838399705 ], [ 2010375.0, 322375.0 ], [ 2010125.0, 322375.0 ], [ 2010125.0, 322125.0 ], [ 2010254.688275706255808, 321995.311724293627776 ], [ 2009990.48373025120236, 321759.516269748855848 ], [ 2009849.129186359932646, 321633.361139180022292 ], [ 2009728.139323069481179, 321521.860676930402406 ], [ 2009467.93524143868126, 321282.064758561202325 ], [ 2009371.85752371000126, 321193.522548103996087 ], [ 2009315.707916330080479, 321138.385272103012539 ], [ 2009224.726615462685004, 321025.273384537256788 ], [ 2009001.835049198707566, 320748.164950801176019 ], [ 2008969.452004210092127, 320707.904948922980111 ], [ 2008774.527210074011236, 320475.472789926046971 ], [ 2008705.315881839953363, 320392.943840646999888 ], [ 2008550.574199959635735, 320208.544073916855268 ], [ 2008342.490567919565365, 319960.578737477771938 ], [ 2008204.207694399869069, 319795.792305600130931 ], [ 2007976.070316154975444, 319523.929683844966348 ], [ 2007884.40169651992619, 319414.691682345990557 ], [ 2007728.158886664547026, 319271.841113335511182 ], [ 2007466.964856813661754, 319033.035143186454661 ], [ 2007205.770826962543651, 318794.229173037339933 ], [ 2006944.576797111658379, 318555.423202888283413 ], [ 2006683.382767260773107, 318316.617232739168685 ], [ 2006375.0, 318034.667274101113435 ], [ 2006375.0, 318375.0 ], [ 2006625.0, 318375.0 ], [ 2006625.0, 318625.0 ], [ 2006875.0, 318625.0 ], [ 2006875.0, 318875.0 ], [ 2007125.0, 318875.0 ], [ 2007125.0, 319125.0 ], [ 2007375.0, 319125.0 ], [ 2007375.0, 319375.0 ], [ 2007625.0, 319375.0 ], [ 2007625.0, 319625.0 ], [ 2007875.0, 319625.0 ], [ 2007875.0, 319875.0 ], [ 2008125.0, 319875.0 ], [ 2008125.0, 320081.342086733086035 ], [ 2008186.308506580768153, 320111.32622177497251 ], [ 2008274.469, 320270.0933 ], [ 2008143.762959928717464, 320405.439841219747905 ], [ 2007973.371252968208864, 320581.881023104069754 ] ], [ [ 2011125.0, 323375.0 ], [ 2010875.0, 323375.0 ], [ 2010875.0, 323125.0 ], [ 2011125.0, 323125.0 ], [ 2011125.0, 323375.0 ] ], [ [ 2014375.0, 324375.0 ], [ 2014375.0, 324625.0 ], [ 2014125.0, 324625.0 ], [ 2014125.0, 324375.0 ], [ 2014375.0, 324375.0 ] ], [ [ 2011875.0, 323625.0 ], [ 2012125.0, 323625.0 ], [ 2012125.0, 323875.0 ], [ 2011875.0, 323875.0 ], [ 2011875.0, 323625.0 ] ] ], [ [ [ 2018625.0, 327125.0 ], [ 2018875.0, 327125.0 ], [ 2019125.0, 327125.0 ], [ 2019125.0, 327375.0 ], [ 2019375.0, 327375.0 ], [ 2019375.0, 327625.0 ], [ 2019625.0, 327625.0 ], [ 2019875.0, 327625.0 ], [ 2019875.0, 327875.0 ], [ 2020125.0, 327875.0 ], [ 2020125.0, 328125.0 ], [ 2020375.0, 328125.0 ], [ 2020375.0, 328375.0 ], [ 2020375.0, 328625.0 ], [ 2020625.0, 328625.0 ], [ 2020625.0, 328875.0 ], [ 2020875.0, 328875.0 ], [ 2020875.0, 329125.0 ], [ 2020875.0, 329375.0 ], [ 2021125.0, 329375.0 ], [ 2021125.0, 329625.0 ], [ 2021375.0, 329625.0 ], [ 2021375.0, 329875.0 ], [ 2021625.0, 329875.0 ], [ 2021625.0, 330125.0 ], [ 2021625.0, 330375.0 ], [ 2021875.0, 330375.0 ], [ 2021875.0, 330625.0 ], [ 2022125.0, 330625.0 ], [ 2022125.0, 330875.0 ], [ 2022375.0, 330875.0 ], [ 2022375.0, 331125.0 ], [ 2022625.0, 331125.0 ], [ 2022625.0, 331375.0 ], [ 2022625.0, 331625.0 ], [ 2022625.0, 331875.0 ], [ 2022875.0, 331875.0 ], [ 2022875.0, 332125.0 ], [ 2023125.0, 332125.0 ], [ 2023125.0, 332375.0 ], [ 2023375.0, 332375.0 ], [ 2023375.0, 332625.0 ], [ 2023625.0, 332625.0 ], [ 2023625.0, 332875.0 ], [ 2023875.0, 332875.0 ], [ 2023875.0, 333125.0 ], [ 2024125.0, 333125.0 ], [ 2024125.0, 333375.0 ], [ 2024375.0, 333375.0 ], [ 2024375.0, 333625.0 ], [ 2024625.0, 333625.0 ], [ 2024875.0, 333625.0 ], [ 2025125.0, 333625.0 ], [ 2025375.0, 333625.0 ], [ 2025375.0, 333875.0 ], [ 2025625.0, 333875.0 ], [ 2025875.0, 333875.0 ], [ 2026125.0, 333875.0 ], [ 2026375.0, 333875.0 ], [ 2026375.0, 333625.0 ], [ 2026625.0, 333625.0 ], [ 2026875.0, 333625.0 ], [ 2027125.0, 333625.0 ], [ 2027375.0, 333625.0 ], [ 2027625.0, 333625.0 ], [ 2027625.0, 333875.0 ], [ 2027875.0, 333875.0 ], [ 2028125.0, 333875.0 ], [ 2028375.0, 333875.0 ], [ 2028375.0, 334125.0 ], [ 2028625.0, 334125.0 ], [ 2028875.0, 334125.0 ], [ 2028875.0, 334375.0 ], [ 2029125.0, 334375.0 ], [ 2029375.0, 334375.0 ], [ 2029625.0, 334375.0 ], [ 2029625.0, 334625.0 ], [ 2029875.0, 334625.0 ], [ 2030125.0, 334625.0 ], [ 2030125.0, 334875.0 ], [ 2030375.0, 334875.0 ], [ 2030375.0, 335125.0 ], [ 2030625.0, 335125.0 ], [ 2030625.0, 335375.0 ], [ 2030875.0, 335375.0 ], [ 2031125.0, 335375.0 ], [ 2031125.0, 335625.0 ], [ 2031375.0, 335625.0 ], [ 2031625.0, 335625.0 ], [ 2031625.0, 335875.0 ], [ 2031875.0, 335875.0 ], [ 2032125.0, 335875.0 ], [ 2032125.0, 336125.0 ], [ 2032375.0, 336125.0 ], [ 2032375.0, 336375.0 ], [ 2032625.0, 336375.0 ], [ 2032625.0, 336625.0 ], [ 2032875.0, 336625.0 ], [ 2032875.0, 336875.0 ], [ 2033125.0, 336875.0 ], [ 2033375.0, 336875.0 ], [ 2033375.0, 337125.0 ], [ 2033625.0, 337125.0 ], [ 2033625.0, 337375.0 ], [ 2033875.0, 337375.0 ], [ 2033875.0, 337625.0 ], [ 2034125.0, 337625.0 ], [ 2034375.0, 337625.0 ], [ 2034375.0, 337875.0 ], [ 2034625.0, 337875.0 ], [ 2034875.0, 337875.0 ], [ 2034875.0, 338125.0 ], [ 2035125.0, 338125.0 ], [ 2035125.0, 338375.0 ], [ 2035375.0, 338375.0 ], [ 2035375.0, 338625.0 ], [ 2035625.0, 338625.0 ], [ 2035625.0, 338875.0 ], [ 2035875.0, 338875.0 ], [ 2035875.0, 339125.0 ], [ 2036125.0, 339125.0 ], [ 2036125.0, 339375.0 ], [ 2036375.0, 339375.0 ], [ 2036625.0, 339375.0 ], [ 2036625.0, 339625.0 ], [ 2036875.0, 339625.0 ], [ 2036875.0, 339875.0 ], [ 2037125.0, 339875.0 ], [ 2037125.0, 340125.0 ], [ 2037125.0, 340375.0 ], [ 2037375.0, 340375.0 ], [ 2037375.0, 340625.0 ], [ 2037375.0, 340875.0 ], [ 2037625.0, 340875.0 ], [ 2037625.0, 341125.0 ], [ 2037625.0, 341375.0 ], [ 2037625.0, 341625.0 ], [ 2037625.0, 341875.0 ], [ 2037625.0, 342125.0 ], [ 2037625.0, 342375.0 ], [ 2037625.0, 342625.0 ], [ 2037625.0, 342875.0 ], [ 2037625.0, 343125.0 ], [ 2037625.0, 343375.0 ], [ 2037375.0, 343375.0 ], [ 2037375.0, 343625.0 ], [ 2037375.0, 343875.0 ], [ 2037125.0, 343875.0 ], [ 2037125.0, 344125.0 ], [ 2037125.0, 344375.0 ], [ 2037375.0, 344375.0 ], [ 2037375.0, 344625.0 ], [ 2037375.0, 344875.0 ], [ 2037375.0, 345125.0 ], [ 2037375.0, 345375.0 ], [ 2037625.0, 345375.0 ], [ 2037625.0, 345625.0 ], [ 2037875.0, 345625.0 ], [ 2037875.0, 345875.0 ], [ 2037875.0, 346125.0 ], [ 2038125.0, 346125.0 ], [ 2038375.0, 346125.0 ], [ 2038625.0, 346125.0 ], [ 2038875.0, 346125.0 ], [ 2039125.0, 346125.0 ], [ 2039375.0, 346125.0 ], [ 2039625.0, 346125.0 ], [ 2039625.0, 346375.0 ], [ 2039875.0, 346375.0 ], [ 2040125.0, 346375.0 ], [ 2040125.0, 346125.0 ], [ 2040125.0, 345875.0 ], [ 2040375.0, 345875.0 ], [ 2040375.0, 345625.0 ], [ 2040625.0, 345625.0 ], [ 2040625.0, 345875.0 ], [ 2040625.0, 346125.0 ], [ 2040875.0, 346125.0 ], [ 2040875.0, 345875.0 ], [ 2041125.0, 345875.0 ], [ 2041375.0, 345875.0 ], [ 2041375.0, 346125.0 ], [ 2041625.0, 346125.0 ], [ 2041625.0, 346375.0 ], [ 2041625.0, 346625.0 ], [ 2041625.0, 346875.0 ], [ 2041875.0, 346875.0 ], [ 2042125.0, 346875.0 ], [ 2042375.0, 346875.0 ], [ 2042625.0, 346875.0 ], [ 2042625.0, 347125.0 ], [ 2042625.0, 347375.0 ], [ 2042375.0, 347375.0 ], [ 2042375.0, 347625.0 ], [ 2042625.0, 347625.0 ], [ 2042875.0, 347625.0 ], [ 2043125.0, 347625.0 ], [ 2043125.0, 347375.0 ], [ 2043375.0, 347375.0 ], [ 2043375.0, 347125.0 ], [ 2043375.0, 346875.0 ], [ 2043625.0, 346875.0 ], [ 2043875.0, 346875.0 ], [ 2043875.0, 346625.0 ], [ 2044125.0, 346625.0 ], [ 2044375.0, 346625.0 ], [ 2044375.0, 346375.0 ], [ 2044625.0, 346375.0 ], [ 2044625.0, 346125.0 ], [ 2044875.0, 346125.0 ], [ 2045125.0, 346125.0 ], [ 2045375.0, 346125.0 ], [ 2045625.0, 346125.0 ], [ 2045625.0, 346375.0 ], [ 2045875.0, 346375.0 ], [ 2045875.0, 346125.0 ], [ 2046125.0, 346125.0 ], [ 2046375.0, 346125.0 ], [ 2046625.0, 346125.0 ], [ 2046875.0, 346125.0 ], [ 2046875.0, 345875.0 ], [ 2047125.0, 345875.0 ], [ 2047375.0, 345875.0 ], [ 2047625.0, 345875.0 ], [ 2047875.0, 345875.0 ], [ 2048125.0, 345875.0 ], [ 2048125.0, 345625.0 ], [ 2047875.0, 345625.0 ], [ 2047875.0, 345237.50147760193795 ], [ 2047793.932758786948398, 345206.067241213051602 ], [ 2047675.248018850106746, 345160.046627768024337 ], [ 2047459.140141147188842, 345040.859858852927573 ], [ 2047125.0, 344856.576508280762937 ], [ 2046975.741703647188842, 344774.258296352811158 ], [ 2046625.0, 344580.818932522961404 ], [ 2046492.343266147421673, 344507.656733852694742 ], [ 2046131.133816140005365, 344308.444249303021934 ], [ 2046125.0, 344304.580215346999466 ], [ 2046014.825111124897376, 344235.174888875102624 ], [ 2045708.068354367977008, 344041.931645632139407 ], [ 2045625.0, 344125.0 ], [ 2045625.0, 344375.0 ], [ 2045625.0, 344625.0 ], [ 2045375.0, 344625.0 ], [ 2045375.0, 344375.0 ], [ 2045125.0, 344375.0 ], [ 2045125.0, 344625.0 ], [ 2044875.0, 344625.0 ], [ 2044625.0, 344625.0 ], [ 2044625.0, 344875.0 ], [ 2044625.0, 345125.0 ], [ 2044375.0, 345125.0 ], [ 2044375.0, 344875.0 ], [ 2044125.0, 344875.0 ], [ 2043875.0, 344875.0 ], [ 2043875.0, 345125.0 ], [ 2043625.0, 345125.0 ], [ 2043625.0, 344875.0 ], [ 2043625.0, 344625.0 ], [ 2043875.0, 344625.0 ], [ 2044125.0, 344625.0 ], [ 2044375.0, 344625.0 ], [ 2044375.0, 344375.0 ], [ 2044125.0, 344375.0 ], [ 2043875.0, 344375.0 ], [ 2043625.0, 344375.0 ], [ 2043375.0, 344375.0 ], [ 2043125.0, 344375.0 ], [ 2043125.0, 344125.0 ], [ 2043375.0, 344125.0 ], [ 2043625.0, 344125.0 ], [ 2043625.0, 343875.0 ], [ 2043625.0, 343625.0 ], [ 2043625.0, 343375.0 ], [ 2043375.0, 343375.0 ], [ 2043375.0, 343125.0 ], [ 2043625.0, 343125.0 ], [ 2043625.0, 342875.0 ], [ 2043748.182275134138763, 342751.817724865977652 ], [ 2043477.097937785089016, 342522.902062214910984 ], [ 2043375.0, 342625.0 ], [ 2043125.0, 342625.0 ], [ 2043125.0, 342875.0 ], [ 2043125.0, 343125.0 ], [ 2043125.0, 343375.0 ], [ 2043125.0, 343625.0 ], [ 2043375.0, 343625.0 ], [ 2043375.0, 343875.0 ], [ 2043125.0, 343875.0 ], [ 2042875.0, 343875.0 ], [ 2042875.0, 343625.0 ], [ 2042875.0, 343375.0 ], [ 2042625.0, 343375.0 ], [ 2042625.0, 343125.0 ], [ 2042375.0, 343125.0 ], [ 2042375.0, 342875.0 ], [ 2042125.0, 342875.0 ], [ 2042125.0, 342625.0 ], [ 2042125.0, 342375.0 ], [ 2042125.0, 342125.0 ], [ 2042375.0, 342125.0 ], [ 2042375.0, 342375.0 ], [ 2042625.0, 342375.0 ], [ 2042875.0, 342375.0 ], [ 2043125.0, 342375.0 ], [ 2043206.013600436039269, 342293.986399563902523 ], [ 2042934.929263087222353, 342065.070736912835855 ], [ 2042875.0, 342125.0 ], [ 2042625.0, 342125.0 ], [ 2042625.0, 341875.0 ], [ 2042375.0, 341875.0 ], [ 2042125.0, 341875.0 ], [ 2041875.0, 341875.0 ], [ 2041875.0, 342125.0 ], [ 2041625.0, 342125.0 ], [ 2041625.0, 341875.0 ], [ 2041375.0, 341875.0 ], [ 2041375.0, 342125.0 ], [ 2041375.0, 342375.0 ], [ 2041125.0, 342375.0 ], [ 2041125.0, 342125.0 ], [ 2041125.0, 341875.0 ], [ 2041125.0, 341625.0 ], [ 2041375.0, 341625.0 ], [ 2041375.0, 341375.0 ], [ 2041375.0, 341125.0 ], [ 2041625.0, 341125.0 ], [ 2041875.0, 341125.0 ], [ 2042125.0, 341125.0 ], [ 2042125.0, 340875.0 ], [ 2042125.0, 340625.0 ], [ 2042375.0, 340625.0 ], [ 2042514.009862861363217, 340764.00986286130501 ], [ 2042614.399504894157872, 340375.0 ], [ 2042649.858159119961783, 340237.597714881005231 ], [ 2042689.136431750608608, 340125.0 ], [ 2042375.0, 340125.0 ], [ 2042375.0, 339875.0 ], [ 2042125.0, 339875.0 ], [ 2042125.0, 340125.0 ], [ 2041875.0, 340125.0 ], [ 2041625.0, 340125.0 ], [ 2041539.783753642113879, 340039.783753642172087 ], [ 2041423.925064839888364, 340125.298500138975214 ], [ 2041220.156350103206933, 340220.156350103148725 ], [ 2041125.0, 340264.453271703154314 ], [ 2040881.145526919979602, 340377.971733310027048 ], [ 2040875.0, 340380.206470371806063 ], [ 2040512.151411605300382, 340512.151411605242174 ], [ 2040375.0, 340562.024652188585605 ], [ 2040366.440792680019513, 340565.137091214011889 ], [ 2040125.0, 340565.137091214011889 ], [ 2040010.826612659962848, 340565.137091214011889 ], [ 2039875.0, 340508.542669272166677 ], [ 2039780.734586396254599, 340469.265413603803609 ], [ 2039449.330538949929178, 340331.180393834016286 ], [ 2039375.0, 340293.491669859387912 ], [ 2039263.197116261580959, 340236.802883738419041 ], [ 2038875.0, 340039.970543098519556 ], [ 2038784.893518389901146, 339994.282749606005382 ], [ 2038767.365848004352301, 339982.634151995647699 ], [ 2038466.990378667600453, 339783.009621332399547 ], [ 2038375.0, 339875.0 ], [ 2038125.0, 339875.0 ], [ 2038125.0, 339555.728515509690624 ], [ 2038016.42717466247268, 339483.57282533752732 ], [ 2037716.051705325720832, 339283.948294674279168 ], [ 2037375.0, 339057.291015509981662 ], [ 2037265.488501320593059, 338984.511498679406941 ], [ 2037125.0, 339125.0 ], [ 2037125.0, 339375.0 ], [ 2036875.0, 339375.0 ], [ 2036875.0, 339125.0 ], [ 2036875.0, 338875.0 ], [ 2036965.113031983841211, 338784.886968016100582 ], [ 2036625.0, 338558.853515510330908 ], [ 2036514.549827978713438, 338485.450172021228354 ], [ 2036214.174358641961589, 338285.825641357980203 ], [ 2035875.0, 338060.416015510680154 ], [ 2035763.611154636833817, 337986.388845363107976 ], [ 2035463.235685300081968, 337786.764314699859824 ], [ 2035125.0, 337561.978515510971192 ], [ 2035012.672481294954196, 337487.327518704987597 ], [ 2034712.297011958202347, 337287.702988041739445 ], [ 2034375.0, 337063.541015511320438 ], [ 2034292.924928680062294, 337008.995291029976215 ], [ 2034261.388202633941546, 336988.611797366000246 ], [ 2033957.684498930349946, 336792.315501069650054 ], [ 2033625.0, 336577.287715175421908 ], [ 2033502.128943375078961, 336497.871056625037454 ], [ 2033198.425239671487361, 336301.574760328629054 ], [ 2032875.0, 336092.531617614033166 ], [ 2032742.869684115983546, 336007.130315884016454 ], [ 2032375.0, 335769.360885906440672 ], [ 2032287.31412856047973, 335712.685871439403854 ], [ 2031983.61042485688813, 335516.389575143053662 ], [ 2031625.0, 335284.604788345051929 ], [ 2031528.054869301617146, 335221.945130698441062 ], [ 2031224.351165598025545, 335025.648834402032662 ], [ 2031223.41305904998444, 335025.042497242975514 ], [ 2030875.0, 334800.846267940942198 ], [ 2030768.003593581030145, 334731.996406419028062 ], [ 2030463.770789348287508, 334536.229210651712492 ], [ 2030147.212251099990681, 334332.530672996013891 ], [ 2030125.0, 334321.741865318908822 ], [ 2029992.577590650646016, 334257.422409349353984 ], [ 2029625.0, 334078.884722462389618 ], [ 2029487.769898342434317, 334012.230101657449268 ], [ 2029164.59412209992297, 333855.259010340028908 ], [ 2029125.0, 333828.497765758656897 ], [ 2029003.573267129948363, 333746.426732870109845 ], [ 2028705.224210526095703, 333544.775789473787881 ], [ 2028375.0, 333321.58076971094124 ], [ 2028257.700625620549545, 333242.299374379392248 ], [ 2027959.351569016929716, 333040.648430983070284 ], [ 2027625.0, 332814.663773663225584 ], [ 2027511.827984111383557, 332738.172015888616443 ], [ 2027213.478927507763728, 332536.521072492352687 ], [ 2026875.0, 332307.746777615509927 ], [ 2026765.955342601984739, 332234.044657397898845 ], [ 2026625.0, 332375.0 ], [ 2026375.0, 332375.0 ], [ 2026375.0, 332125.0 ], [ 2026467.60628599836491, 332032.39371400163509 ], [ 2026125.0, 331800.829781567794271 ], [ 2026020.082701092818752, 331729.917298907181248 ], [ 2025875.0, 331875.0 ], [ 2025625.0, 331875.0 ], [ 2025625.0, 332125.0 ], [ 2025625.0, 332375.0 ], [ 2025375.0, 332375.0 ], [ 2025375.0, 332125.0 ], [ 2025125.0, 332125.0 ], [ 2025125.0, 331875.0 ], [ 2025375.0, 331875.0 ], [ 2025375.0, 331625.0 ], [ 2025375.0, 331293.912785520078614 ], [ 2025274.210059583652765, 331225.789940416463651 ], [ 2024975.861002979800105, 331024.138997020141687 ], [ 2024875.0, 331125.0 ], [ 2024625.0, 331125.0 ], [ 2024375.0, 331125.0 ], [ 2024375.0, 330875.0 ], [ 2024528.337418074253947, 330721.662581925687846 ], [ 2024429.310567120090127, 330654.731390174012631 ], [ 2024233.723596747498959, 330516.276403252559248 ], [ 2023875.0, 330262.337857134349179 ], [ 2023794.586462694685906, 330205.413537305197679 ], [ 2023501.828373326454312, 329998.171626673662104 ], [ 2023209.070283957989886, 329790.929716042068321 ], [ 2023125.0, 329875.0 ], [ 2022875.0, 329875.0 ], [ 2022875.0, 329554.443120293261018 ], [ 2022769.933149905176833, 329480.066850094764959 ], [ 2022477.175060536712408, 329272.824939463171177 ], [ 2022125.0, 329023.522067662503105 ], [ 2022038.037926484132186, 328961.962073515867814 ], [ 2021745.27983711566776, 328754.720162884274032 ], [ 2021452.521747747203335, 328547.478252252738457 ], [ 2021125.0, 328315.627330821473151 ], [ 2021013.384613694623113, 328236.615386305376887 ], [ 2020873.168766930000857, 328137.357326359022409 ], [ 2020722.521045047324151, 328027.478954952734057 ], [ 2020375.0, 327774.006509554979857 ], [ 2020288.83783894055523, 327711.16216105944477 ], [ 2019999.715701536042616, 327500.284298463899177 ], [ 2019710.593564131530002, 327289.406435868411791 ], [ 2019375.0, 327044.633572260674555 ], [ 2019276.910358024761081, 326973.089641975122504 ], [ 2018987.788220620481297, 326762.211779379635118 ], [ 2018698.666083215968683, 326551.333916784089524 ], [ 2018375.0, 326315.260634966369253 ], [ 2018264.982877109199762, 326235.017122890800238 ], [ 2018037.613594680093229, 326069.18012151500443 ], [ 2017977.952300716424361, 326022.047699283633847 ], [ 2017698.622691778000444, 325801.377308221941348 ], [ 2017375.0, 325545.715381716901902 ], [ 2017375.0, 325875.0 ], [ 2017625.0, 325875.0 ], [ 2017625.0, 326125.0 ], [ 2017875.0, 326125.0 ], [ 2017875.0, 326375.0 ], [ 2018125.0, 326375.0 ], [ 2018125.0, 326625.0 ], [ 2018375.0, 326625.0 ], [ 2018375.0, 326875.0 ], [ 2018625.0, 326875.0 ], [ 2018625.0, 327125.0 ] ], [ [ 2026125.0, 333625.0 ], [ 2026125.0, 333375.0 ], [ 2026375.0, 333375.0 ], [ 2026375.0, 333625.0 ], [ 2026125.0, 333625.0 ] ], [ [ 2037125.0, 339625.0 ], [ 2037375.0, 339625.0 ], [ 2037625.0, 339625.0 ], [ 2037625.0, 339875.0 ], [ 2037375.0, 339875.0 ], [ 2037125.0, 339875.0 ], [ 2037125.0, 339625.0 ] ], [ [ 2037875.0, 345375.0 ], [ 2038125.0, 345375.0 ], [ 2038125.0, 345625.0 ], [ 2037875.0, 345625.0 ], [ 2037875.0, 345375.0 ] ], [ [ 2041625.0, 342375.0 ], [ 2041625.0, 342625.0 ], [ 2041375.0, 342625.0 ], [ 2041375.0, 342375.0 ], [ 2041625.0, 342375.0 ] ], [ [ 2041875.0, 342625.0 ], [ 2041875.0, 342875.0 ], [ 2041875.0, 343125.0 ], [ 2041875.0, 343375.0 ], [ 2041875.0, 343625.0 ], [ 2041875.0, 343875.0 ], [ 2041625.0, 343875.0 ], [ 2041625.0, 343625.0 ], [ 2041625.0, 343375.0 ], [ 2041625.0, 343125.0 ], [ 2041625.0, 342875.0 ], [ 2041625.0, 342625.0 ], [ 2041875.0, 342625.0 ] ], [ [ 2038375.0, 344125.0 ], [ 2038125.0, 344125.0 ], [ 2038125.0, 343875.0 ], [ 2038125.0, 343625.0 ], [ 2038375.0, 343625.0 ], [ 2038625.0, 343625.0 ], [ 2038875.0, 343625.0 ], [ 2038875.0, 343875.0 ], [ 2039125.0, 343875.0 ], [ 2039125.0, 344125.0 ], [ 2038875.0, 344125.0 ], [ 2038875.0, 344375.0 ], [ 2038875.0, 344625.0 ], [ 2038875.0, 344875.0 ], [ 2038875.0, 345125.0 ], [ 2038625.0, 345125.0 ], [ 2038375.0, 345125.0 ], [ 2038375.0, 344875.0 ], [ 2038375.0, 344625.0 ], [ 2038125.0, 344625.0 ], [ 2038125.0, 344375.0 ], [ 2038375.0, 344375.0 ], [ 2038375.0, 344125.0 ] ], [ [ 2040625.0, 343625.0 ], [ 2040625.0, 343375.0 ], [ 2040875.0, 343375.0 ], [ 2040875.0, 343625.0 ], [ 2040875.0, 343875.0 ], [ 2040625.0, 343875.0 ], [ 2040375.0, 343875.0 ], [ 2040375.0, 344125.0 ], [ 2040125.0, 344125.0 ], [ 2040125.0, 344375.0 ], [ 2039875.0, 344375.0 ], [ 2039875.0, 344125.0 ], [ 2039875.0, 343875.0 ], [ 2040125.0, 343875.0 ], [ 2040125.0, 343625.0 ], [ 2040375.0, 343625.0 ], [ 2040625.0, 343625.0 ] ], [ [ 2040375.0, 345125.0 ], [ 2040125.0, 345125.0 ], [ 2039875.0, 345125.0 ], [ 2039875.0, 345375.0 ], [ 2039625.0, 345375.0 ], [ 2039625.0, 345125.0 ], [ 2039625.0, 344875.0 ], [ 2039875.0, 344875.0 ], [ 2040125.0, 344875.0 ], [ 2040375.0, 344875.0 ], [ 2040375.0, 345125.0 ] ], [ [ 2024875.0, 333125.0 ], [ 2024875.0, 333375.0 ], [ 2024625.0, 333375.0 ], [ 2024625.0, 333125.0 ], [ 2024875.0, 333125.0 ] ], [ [ 2023375.0, 331625.0 ], [ 2023375.0, 331375.0 ], [ 2023625.0, 331375.0 ], [ 2023625.0, 331625.0 ], [ 2023375.0, 331625.0 ] ], [ [ 2023875.0, 331625.0 ], [ 2023875.0, 331375.0 ], [ 2024125.0, 331375.0 ], [ 2024125.0, 331625.0 ], [ 2023875.0, 331625.0 ] ], [ [ 2039375.0, 341625.0 ], [ 2039375.0, 341375.0 ], [ 2039625.0, 341375.0 ], [ 2039625.0, 341625.0 ], [ 2039625.0, 341875.0 ], [ 2039875.0, 341875.0 ], [ 2039875.0, 342125.0 ], [ 2040125.0, 342125.0 ], [ 2040125.0, 342375.0 ], [ 2039875.0, 342375.0 ], [ 2039625.0, 342375.0 ], [ 2039375.0, 342375.0 ], [ 2039375.0, 342125.0 ], [ 2039375.0, 341875.0 ], [ 2039125.0, 341875.0 ], [ 2038875.0, 341875.0 ], [ 2038625.0, 341875.0 ], [ 2038375.0, 341875.0 ], [ 2038125.0, 341875.0 ], [ 2037875.0, 341875.0 ], [ 2037875.0, 341625.0 ], [ 2038125.0, 341625.0 ], [ 2038375.0, 341625.0 ], [ 2038625.0, 341625.0 ], [ 2038875.0, 341625.0 ], [ 2039125.0, 341625.0 ], [ 2039375.0, 341625.0 ] ], [ [ 2040125.0, 341375.0 ], [ 2039875.0, 341375.0 ], [ 2039875.0, 341125.0 ], [ 2040125.0, 341125.0 ], [ 2040375.0, 341125.0 ], [ 2040375.0, 341375.0 ], [ 2040125.0, 341375.0 ] ], [ [ 2041875.0, 344125.0 ], [ 2042125.0, 344125.0 ], [ 2042375.0, 344125.0 ], [ 2042625.0, 344125.0 ], [ 2042625.0, 344375.0 ], [ 2042375.0, 344375.0 ], [ 2042125.0, 344375.0 ], [ 2041875.0, 344375.0 ], [ 2041875.0, 344625.0 ], [ 2042125.0, 344625.0 ], [ 2042125.0, 344875.0 ], [ 2042125.0, 345125.0 ], [ 2041875.0, 345125.0 ], [ 2041625.0, 345125.0 ], [ 2041375.0, 345125.0 ], [ 2041125.0, 345125.0 ], [ 2041125.0, 344875.0 ], [ 2041125.0, 344625.0 ], [ 2041125.0, 344375.0 ], [ 2041125.0, 344125.0 ], [ 2041125.0, 343875.0 ], [ 2041375.0, 343875.0 ], [ 2041375.0, 344125.0 ], [ 2041625.0, 344125.0 ], [ 2041875.0, 344125.0 ] ], [ [ 2042375.0, 345625.0 ], [ 2042375.0, 345875.0 ], [ 2042125.0, 345875.0 ], [ 2042125.0, 345625.0 ], [ 2042375.0, 345625.0 ] ], [ [ 2042375.0, 345375.0 ], [ 2042625.0, 345375.0 ], [ 2042625.0, 345625.0 ], [ 2042375.0, 345625.0 ], [ 2042375.0, 345375.0 ] ], [ [ 2043625.0, 345875.0 ], [ 2043375.0, 345875.0 ], [ 2043375.0, 345625.0 ], [ 2043625.0, 345625.0 ], [ 2043625.0, 345875.0 ] ], [ [ 2042875.0, 344625.0 ], [ 2042875.0, 344875.0 ], [ 2042625.0, 344875.0 ], [ 2042625.0, 344625.0 ], [ 2042875.0, 344625.0 ] ], [ [ 2043125.0, 344625.0 ], [ 2043375.0, 344625.0 ], [ 2043375.0, 344875.0 ], [ 2043125.0, 344875.0 ], [ 2043125.0, 344625.0 ] ], [ [ 2047125.0, 345375.0 ], [ 2047125.0, 345125.0 ], [ 2047375.0, 345125.0 ], [ 2047375.0, 345375.0 ], [ 2047125.0, 345375.0 ] ], [ [ 2044125.0, 345375.0 ], [ 2044375.0, 345375.0 ], [ 2044375.0, 345625.0 ], [ 2044125.0, 345625.0 ], [ 2044125.0, 345375.0 ] ] ], [ [ [ 2047944.085763221839443, 328375.0 ], [ 2047993.426009615883231, 328243.426009615825024 ], [ 2048096.370074129896238, 327968.908504244987853 ], [ 2048033.314358457922935, 327716.685641542135272 ], [ 2047875.0, 327875.0 ], [ 2047875.0, 328125.0 ], [ 2047625.0, 328125.0 ], [ 2047625.0, 328375.0 ], [ 2047375.0, 328375.0 ], [ 2047375.0, 328125.0 ], [ 2047375.0, 327875.0 ], [ 2047375.0, 327625.0 ], [ 2047625.0, 327625.0 ], [ 2048010.392948073800653, 327625.0 ], [ 2047955.996055709896609, 327407.412430531985592 ], [ 2047772.418572968337685, 327227.5814270315459 ], [ 2047497.440928840078413, 326958.215571561013348 ], [ 2047375.0, 326955.115801210689824 ], [ 2047125.0, 326948.786687286745291 ], [ 2046875.0, 326942.457573362858966 ], [ 2046758.137765120016411, 326939.499035771004856 ], [ 2046625.0, 326974.420416785869747 ], [ 2046375.0, 327039.994187277217861 ], [ 2046187.283423509914428, 327089.231322093983181 ], [ 2045982.021927803521976, 327232.021927803463768 ], [ 2045972.043261920101941, 327238.963608418009244 ], [ 2045809.501623356016353, 327625.0 ], [ 2045754.834475693991408, 327754.834475694107823 ], [ 2045747.44483242998831, 327772.384878445998766 ], [ 2045653.862153480062261, 328090.565986883011647 ], [ 2045633.676697514718398, 328125.0 ], [ 2045538.078787563135847, 328288.078787563194055 ], [ 2045340.573249236680567, 328625.0 ], [ 2045335.681045040022582, 328633.345524806005415 ], [ 2045245.162522831233218, 328745.162522831233218 ], [ 2045375.0, 328875.0 ], [ 2045625.0, 328875.0 ], [ 2045625.0, 329125.0 ], [ 2045625.0, 329375.0 ], [ 2045375.0, 329375.0 ], [ 2045125.0, 329375.0 ], [ 2044875.0, 329375.0 ], [ 2044758.410026984056458, 329258.410026984114666 ], [ 2044624.45268500992097, 329559.814046433020849 ], [ 2044606.470352991949767, 329625.0 ], [ 2044556.422709102509543, 329806.422709102567751 ], [ 2044549.586541849886999, 329831.203815394022968 ], [ 2044437.287327100057155, 329887.353422766027506 ], [ 2044375.0, 329889.844915849971585 ], [ 2044203.330629719886929, 329896.711690660973545 ], [ 2044125.0, 329891.489648679620586 ], [ 2044125.0, 330125.0 ], [ 2043875.0, 330125.0 ], [ 2043875.0, 329874.822982012876309 ], [ 2043625.0, 329858.156315346132033 ], [ 2043375.0, 329841.489648679387756 ], [ 2043220.712500720052049, 329831.203815394022968 ], [ 2043125.0, 329807.275690214126371 ], [ 2042875.0, 329744.775690214417409 ], [ 2042808.948713330086321, 329728.262868546997197 ], [ 2042625.0, 329792.89349755551666 ], [ 2042462.692801210097969, 329849.920351185020991 ], [ 2042266.169175409944728, 329924.786494346975815 ], [ 2042227.847405931679532, 329977.847405931563117 ], [ 2042144.511692770058289, 330093.23531646101037 ], [ 2042089.724115420831367, 330375.0 ], [ 2042079.003817510092631, 330430.132960689021274 ], [ 2042072.284264430403709, 330625.0 ], [ 2042063.663574775448069, 330875.0 ], [ 2042060.287281719967723, 330972.912498612015042 ], [ 2042052.868379213381559, 331125.0 ], [ 2042041.570745930075645, 331356.601482315978501 ], [ 2042029.305067472159863, 331375.0 ], [ 2041985.421138549922034, 331440.825893372995779 ], [ 2041953.567391995340586, 331453.56739199522417 ], [ 2041891.838459599995986, 331478.258964954002295 ], [ 2041875.0, 331478.258964954002295 ], [ 2041771.741035046055913, 331478.258964954002295 ], [ 2041751.464441169984639, 331478.258964954002295 ], [ 2041657.88176222005859, 331412.75108968699351 ], [ 2041480.59065587236546, 331269.409344127692748 ], [ 2041375.0, 331375.0 ], [ 2041125.0, 331375.0 ], [ 2041125.0, 331125.0 ], [ 2041204.391320103546605, 331045.608679896511603 ], [ 2040875.0, 330767.456009589717723 ], [ 2040797.764814077410847, 330702.235185922472738 ], [ 2040796.921115860110149, 330701.522729650023393 ], [ 2040375.0, 330533.910231568617746 ], [ 2040261.270128387259319, 330488.729871612857096 ], [ 2040113.767559509957209, 330430.132960689021274 ], [ 2039875.0, 330313.525547903496772 ], [ 2039711.362040020059794, 330233.609334888984449 ], [ 2039625.0, 330216.62008111452451 ], [ 2039548.440754137234762, 330201.559245862823445 ], [ 2039375.0, 330375.0 ], [ 2039125.0, 330375.0 ], [ 2038875.0, 330375.0 ], [ 2038875.0, 330625.0 ], [ 2038875.0, 330875.0 ], [ 2038595.783522840589285, 330875.0 ], [ 2038719.38564311992377, 331075.853445458982605 ], [ 2038875.0, 331185.359844745253213 ], [ 2038986.310525910230353, 331263.689474089711439 ], [ 2039224.732109460048378, 331431.467625477002002 ], [ 2039375.0, 331483.886657060240395 ], [ 2039479.618512869346887, 331520.381487130653113 ], [ 2039625.0, 331375.0 ], [ 2039625.0, 331125.0 ], [ 2039625.0, 330875.0 ], [ 2039625.0, 330625.0 ], [ 2039875.0, 330625.0 ], [ 2039875.0, 330875.0 ], [ 2040125.0, 330875.0 ], [ 2040125.0, 331125.0 ], [ 2040375.0, 331125.0 ], [ 2040625.0, 331125.0 ], [ 2040625.0, 331375.0 ], [ 2040875.0, 331375.0 ], [ 2040875.0, 331625.0 ], [ 2041125.0, 331625.0 ], [ 2041375.0, 331625.0 ], [ 2041375.0, 331875.0 ], [ 2041625.0, 331875.0 ], [ 2041875.0, 331875.0 ], [ 2041875.0, 332125.0 ], [ 2042125.0, 332125.0 ], [ 2042125.0, 332375.0 ], [ 2042125.0, 332625.0 ], [ 2041875.0, 332625.0 ], [ 2041625.0, 332625.0 ], [ 2041375.0, 332625.0 ], [ 2041375.0, 332375.0 ], [ 2041125.0, 332375.0 ], [ 2040992.545040570897982, 332507.454959429043811 ], [ 2041021.519545349990949, 332526.384969217993785 ], [ 2041090.550066895550117, 332625.0 ], [ 2041152.535295879933983, 332713.550327123026364 ], [ 2041156.04507137532346, 332875.0 ], [ 2041375.0, 332875.0 ], [ 2041375.0, 333125.0 ], [ 2041208.471752394689247, 333291.528247605252545 ], [ 2041234.831253152573481, 333375.0 ], [ 2041274.192778520053253, 333499.644830322009511 ], [ 2041482.171753969043493, 333767.828246030898299 ], [ 2041629.806958540109918, 333958.199957186996471 ], [ 2041714.274781053652987, 334035.725218946230598 ], [ 2041974.989066767739132, 334275.01093323220266 ], [ 2042125.0, 334125.0 ], [ 2042375.0, 334125.0 ], [ 2042375.0, 334375.0 ], [ 2042235.703352481825277, 334514.29664751823293 ], [ 2042312.960514890030026, 334585.203906167007517 ], [ 2042326.640422145137563, 334625.0 ], [ 2042466.918453689431772, 335033.081546310626436 ], [ 2042498.515422145137563, 335125.0 ], [ 2042584.452922145137563, 335375.0 ], [ 2042722.73240717779845, 335777.267592822259758 ], [ 2042875.0, 335625.0 ], [ 2042875.0, 335375.0 ], [ 2043125.0, 335375.0 ], [ 2043125.0, 335625.0 ], [ 2043125.0, 335875.0 ], [ 2042750.086454898351803, 335875.0 ], [ 2042819.051972138229758, 336125.0 ], [ 2042874.456588600063697, 336325.841734679008368 ], [ 2042875.502509138779715, 336375.0 ], [ 2042880.821658074855804, 336625.0 ], [ 2042886.140807010699064, 336875.0 ], [ 2042891.459955946775153, 337125.0 ], [ 2042893.173124389955774, 337205.51891682902351 ], [ 2042873.617614797316492, 337375.0 ], [ 2042865.098320710007101, 337448.83388210501289 ], [ 2042790.232177539961413, 337504.983489477017429 ], [ 2042754.983489477075636, 337504.983489477017429 ], [ 2042875.0, 337625.0 ], [ 2042875.0, 337875.0 ], [ 2042875.0, 338125.0 ], [ 2042875.0, 338375.0 ], [ 2043125.0, 338375.0 ], [ 2043125.0, 338625.0 ], [ 2043125.0, 338875.0 ], [ 2043301.428946725092828, 339051.428946725151036 ], [ 2043304.936911779921502, 339030.381156397983432 ], [ 2043407.877858629915863, 338712.200047959981021 ], [ 2043430.750359400175512, 338680.750359400233719 ], [ 2043482.744001789949834, 338609.259101113013458 ], [ 2043625.0, 338665.145386123040225 ], [ 2043625.0, 338306.056384409370366 ], [ 2043538.893609160091728, 338263.003188989998307 ], [ 2043524.739936937578022, 338225.260063062421978 ], [ 2043487.142413290217519, 338125.0 ], [ 2043482.744001789949834, 338113.270902665972244 ], [ 2043471.121030928567052, 337875.0 ], [ 2043464.027466, 337729.581918962008785 ], [ 2043497.493680067127571, 337625.0 ], [ 2043528.404303080402315, 337528.40430308051873 ], [ 2043538.893609160091728, 337495.625221581023652 ], [ 2043651.192823899909854, 337373.967738943989389 ], [ 2043787.333049761131406, 337287.333049761131406 ], [ 2043857.074717599898577, 337242.951988409971818 ], [ 2043875.0, 337239.590997960011009 ], [ 2044125.0, 337192.715997960825916 ], [ 2044156.539290250046179, 337186.802381039015017 ], [ 2044375.0, 337122.073281853401568 ], [ 2044409.212523420108482, 337111.936237877001986 ], [ 2044774.568503508344293, 337024.568503508344293 ], [ 2044839.692846599966288, 337008.995291029976215 ], [ 2044875.0, 337007.272990864061285 ], [ 2045125.0, 336995.077868912369013 ], [ 2045223.381830299971625, 336990.278755238978192 ], [ 2045373.114116620039567, 336943.487415763025638 ], [ 2045375.0, 336942.511958842398599 ], [ 2045375.0, 336625.0 ], [ 2045625.0, 336625.0 ], [ 2045625.0, 336375.0 ], [ 2045892.986534549389035, 336375.0 ], [ 2045887.818850859999657, 336250.975591516995337 ], [ 2045936.583595962263644, 336125.0 ], [ 2045989.164918018272147, 335989.164918018388562 ], [ 2046000.118065600050613, 335960.869286764995195 ], [ 2046121.775548239937052, 335773.703928861010354 ], [ 2046239.942066502058879, 335739.942066502058879 ], [ 2046125.0, 335625.0 ], [ 2046125.0, 335375.0 ], [ 2045875.0, 335375.0 ], [ 2045875.0, 335125.0 ], [ 2045875.0, 334875.0 ], [ 2045625.0, 334875.0 ], [ 2045625.0, 334625.0 ], [ 2045375.0, 334625.0 ], [ 2045375.0, 334375.0 ], [ 2045125.0, 334375.0 ], [ 2044875.0, 334375.0 ], [ 2044875.0, 334125.0 ], [ 2044625.0, 334125.0 ], [ 2044625.0, 333875.0 ], [ 2044625.0, 333544.825958084082231 ], [ 2044521.511738159926608, 333509.003098217013758 ], [ 2044502.733377312542871, 333497.266622687457129 ], [ 2044125.0, 333261.183261868485715 ], [ 2044041.19491577311419, 333208.805084226944018 ], [ 2043907.106781009119004, 333125.0 ], [ 2043625.0, 333125.0 ], [ 2043625.0, 332875.0 ], [ 2043625.0, 332625.0 ], [ 2043625.0, 332375.0 ], [ 2043625.0, 332125.0 ], [ 2043979.688020539935678, 332125.0 ], [ 2044007.247189057758078, 332007.24718905769987 ], [ 2044096.709297132911161, 331625.0 ], [ 2044100.389682869892567, 331609.274715486972127 ], [ 2044240.763701299903914, 331553.125108115025796 ], [ 2044299.595217163208872, 331549.595217163325287 ], [ 2044375.0, 331545.070930193178356 ], [ 2044625.0, 331530.070930193527602 ], [ 2044708.677096060011536, 331525.050304430013057 ], [ 2044875.0, 331467.697578934021294 ], [ 2045251.456633989932016, 331337.884946524980478 ], [ 2045287.948588171508163, 331287.948588171508163 ], [ 2045499.059699283214286, 330999.059699283330701 ], [ 2045607.07081400998868, 330851.255015973991249 ], [ 2045634.221415924606845, 330625.0 ], [ 2045664.221415922045708, 330375.0 ], [ 2045691.295225060079247, 330149.384923832025379 ], [ 2045711.48899010871537, 330125.0 ], [ 2045785.558080059709027, 330035.55808005965082 ], [ 2046012.053806556155905, 329762.053806556214113 ], [ 2046187.283423509914428, 329550.455778538016602 ], [ 2046271.33054011175409, 329521.330540111695882 ], [ 2046375.0, 329485.405479754437692 ], [ 2046625.0, 329398.771816388296429 ], [ 2047013.977598935598508, 329263.977598935656715 ], [ 2047125.0, 329225.504489655955695 ], [ 2047132.468480929965153, 329222.916402205009945 ], [ 2047488.473845226690173, 328988.473845226631965 ], [ 2047516.157464629970491, 328970.243169034016319 ], [ 2047718.521745939273387, 328718.521745939331595 ], [ 2047899.846448329975829, 328492.971506377973128 ], [ 2047944.085763221839443, 328375.0 ] ], [ [ 2042375.0, 332625.0 ], [ 2042375.0, 332875.0 ], [ 2042125.0, 332875.0 ], [ 2042125.0, 332625.0 ], [ 2042375.0, 332625.0 ] ], [ [ 2042375.0, 332375.0 ], [ 2042625.0, 332375.0 ], [ 2042625.0, 332625.0 ], [ 2042375.0, 332625.0 ], [ 2042375.0, 332375.0 ] ], [ [ 2042625.0, 332125.0 ], [ 2042375.0, 332125.0 ], [ 2042375.0, 331875.0 ], [ 2042375.0, 331625.0 ], [ 2042375.0, 331375.0 ], [ 2042375.0, 331125.0 ], [ 2042375.0, 330875.0 ], [ 2042375.0, 330625.0 ], [ 2042625.0, 330625.0 ], [ 2042625.0, 330875.0 ], [ 2042625.0, 331125.0 ], [ 2042625.0, 331375.0 ], [ 2042625.0, 331625.0 ], [ 2042875.0, 331625.0 ], [ 2042875.0, 331875.0 ], [ 2042875.0, 332125.0 ], [ 2042875.0, 332375.0 ], [ 2042625.0, 332375.0 ], [ 2042625.0, 332125.0 ] ] ], [ [ [ 2048125.0, 334375.0 ], [ 2048375.0, 334375.0 ], [ 2048375.0, 334125.0 ], [ 2048375.0, 333875.0 ], [ 2048125.0, 333875.0 ], [ 2047875.0, 333875.0 ], [ 2047625.0, 333875.0 ], [ 2047375.0, 333875.0 ], [ 2047218.655123849865049, 333718.655123849981464 ], [ 2047125.0, 333776.929423133609816 ], [ 2047125.0, 334125.0 ], [ 2047125.0, 334375.0 ], [ 2046875.0, 334375.0 ], [ 2046875.0, 334625.0 ], [ 2047125.0, 334625.0 ], [ 2047125.0, 334875.0 ], [ 2047375.0, 334875.0 ], [ 2047375.0, 334625.0 ], [ 2047375.0, 334375.0 ], [ 2047625.0, 334375.0 ], [ 2047625.0, 334625.0 ], [ 2047875.0, 334625.0 ], [ 2048125.0, 334625.0 ], [ 2048125.0, 334375.0 ] ] ], [ [ [ 2047125.0, 335875.0 ], [ 2046875.0, 335875.0 ], [ 2046875.0, 335625.0 ], [ 2046625.0, 335625.0 ], [ 2046625.0, 335375.0 ], [ 2046375.0, 335375.0 ], [ 2046375.0, 335625.0 ], [ 2046259.751744923647493, 335740.248255076410715 ], [ 2046383.807049310067669, 335811.13700044102734 ], [ 2046625.0, 335917.545655156834982 ], [ 2046701.988157750107348, 335951.511018869990949 ], [ 2046764.311606182716787, 335985.688393817225005 ], [ 2046992.094462499953806, 336110.601573089021258 ], [ 2047125.0, 336165.978880381851923 ], [ 2047216.69289198005572, 336204.184252040984575 ], [ 2047366.425178309902549, 336269.69212730700383 ], [ 2047375.0, 336269.69212730700383 ], [ 2047488.082660950021818, 336269.69212730700383 ], [ 2047619.098411479964852, 336250.975591516995337 ], [ 2047722.039358329959214, 336138.67637677397579 ], [ 2047778.188965700101107, 335988.944090450997464 ], [ 2047737.973404365358874, 335875.0 ], [ 2047375.0, 335875.0 ], [ 2047125.0, 335875.0 ] ] ], [ [ [ 2050375.0, 346875.0 ], [ 2050625.0, 346875.0 ], [ 2050875.0, 346875.0 ], [ 2051125.0, 346875.0 ], [ 2051125.0, 347125.0 ], [ 2051375.0, 347125.0 ], [ 2051625.0, 347125.0 ], [ 2051875.0, 347125.0 ], [ 2052125.0, 347125.0 ], [ 2052125.0, 347375.0 ], [ 2052125.0, 347599.571051757840905 ], [ 2052153.325309976236895, 347625.0 ], [ 2052375.0, 347625.0 ], [ 2052625.0, 347625.0 ], [ 2052875.0, 347625.0 ], [ 2052875.0, 347875.0 ], [ 2052875.0, 348125.0 ], [ 2053125.0, 348125.0 ], [ 2053375.0, 348125.0 ], [ 2053625.0, 348125.0 ], [ 2053875.0, 348125.0 ], [ 2054125.0, 348125.0 ], [ 2054375.0, 348125.0 ], [ 2054375.0, 348375.0 ], [ 2054625.0, 348375.0 ], [ 2054625.0, 348625.0 ], [ 2054875.0, 348625.0 ], [ 2054875.0, 348875.0 ], [ 2055125.0, 348875.0 ], [ 2055375.0, 348875.0 ], [ 2055375.0, 349125.0 ], [ 2055625.0, 349125.0 ], [ 2055625.0, 349375.0 ], [ 2055875.0, 349375.0 ], [ 2055875.0, 349625.0 ], [ 2056125.0, 349625.0 ], [ 2056125.0, 349875.0 ], [ 2056125.0, 350125.0 ], [ 2056375.0, 350125.0 ], [ 2056625.0, 350125.0 ], [ 2056625.0, 350375.0 ], [ 2056875.0, 350375.0 ], [ 2057125.0, 350375.0 ], [ 2057375.0, 350375.0 ], [ 2057375.0, 350125.0 ], [ 2057625.0, 350125.0 ], [ 2057875.0, 350125.0 ], [ 2057875.0, 349875.0 ], [ 2058125.0, 349875.0 ], [ 2058375.0, 349875.0 ], [ 2058375.0, 350125.0 ], [ 2058625.0, 350125.0 ], [ 2058875.0, 350125.0 ], [ 2058875.0, 349875.0 ], [ 2059125.0, 349875.0 ], [ 2059375.0, 349875.0 ], [ 2059625.0, 349875.0 ], [ 2059875.0, 349875.0 ], [ 2060125.0, 349875.0 ], [ 2060375.0, 349875.0 ], [ 2060520.999021985335276, 349729.000978014722932 ], [ 2060131.10258738999255, 349595.865610103996005 ], [ 2060125.0, 349594.126700793916825 ], [ 2059875.0, 349522.890141654061154 ], [ 2059759.905580135295168, 349490.094419864646625 ], [ 2059625.0, 349625.0 ], [ 2059375.0, 349625.0 ], [ 2059125.0, 349625.0 ], [ 2059125.0, 349309.180464234494139 ], [ 2058875.0, 349237.943905094580259 ], [ 2058787.102232855278999, 349212.897767144721001 ], [ 2058390.464758879970759, 349099.877411657012999 ], [ 2058375.0, 349093.421250182844233 ], [ 2058125.0, 348989.052318144124001 ], [ 2058044.538433090085164, 348955.461566909914836 ], [ 2057625.0, 348780.314454066683538 ], [ 2057515.428844048874453, 348734.571155951183755 ], [ 2057426.563165670027956, 348697.47189216199331 ], [ 2057239.545636754948646, 348510.454363245167769 ], [ 2057080.307253550039604, 348351.215980038978159 ], [ 2056986.184095392236486, 348263.815904607879929 ], [ 2056875.0, 348375.0 ], [ 2056625.0, 348375.0 ], [ 2056625.0, 348121.706425654585473 ], [ 2056425.228500880068168, 348135.975818448991049 ], [ 2056375.0, 348121.624818197393324 ], [ 2056125.0, 348050.196246767998673 ], [ 2055988.736252514179796, 348011.263747485820204 ], [ 2055966.673374020028859, 348004.960067916021217 ], [ 2055625.0, 347786.513156657922082 ], [ 2055526.476974438643083, 347723.523025561473332 ], [ 2055375.0, 347875.0 ], [ 2055375.0, 348125.0 ], [ 2055125.0, 348125.0 ], [ 2054875.0, 348125.0 ], [ 2054875.0, 347875.0 ], [ 2054875.0, 347625.0 ], [ 2055125.0, 347625.0 ], [ 2055221.476974438177422, 347528.523025561822578 ], [ 2054875.0, 347307.004959938058164 ], [ 2054824.964690800057724, 347275.0151720889844 ], [ 2054750.894243598682806, 347249.105756401258986 ], [ 2054375.0, 347117.620115636324044 ], [ 2054125.0, 347030.171555965614971 ], [ 2054010.040585062000901, 346989.959414937940892 ], [ 2053625.0, 346855.274436624080408 ], [ 2053375.0, 346767.825876953371335 ], [ 2053269.186926525318995, 346730.81307347456459 ], [ 2052875.0, 346592.928757611836772 ], [ 2052625.0, 346505.480197941069491 ], [ 2052550.905592259950936, 346479.562400994997006 ], [ 2052525.807856808882207, 346474.192143191059586 ], [ 2052375.0, 346625.0 ], [ 2052125.0, 346625.0 ], [ 2051875.0, 346625.0 ], [ 2051625.0, 346625.0 ], [ 2051625.0, 346281.442863786884118 ], [ 2051375.0, 346227.949414005328435 ], [ 2051290.19634601729922, 346209.803653982817195 ], [ 2050875.0, 346120.962514442158863 ], [ 2050625.0, 346067.469064660603181 ], [ 2050407.862244250020012, 346021.007274129020516 ], [ 2050375.0, 346011.860051502706483 ], [ 2050267.940121002029628, 345982.05987899802858 ], [ 2050125.0, 346125.0 ], [ 2049875.0, 346125.0 ], [ 2049875.0, 345872.684793770487886 ], [ 2049625.0, 345803.097164904407691 ], [ 2049375.0, 345733.509536038327496 ], [ 2049290.117540357168764, 345709.882459642831236 ], [ 2048875.0, 345594.334278306108899 ], [ 2048625.0, 345524.746649439970497 ], [ 2048625.0, 345875.0 ], [ 2048375.0, 345875.0 ], [ 2048125.0, 345875.0 ], [ 2048125.0, 346125.0 ], [ 2048375.0, 346125.0 ], [ 2048625.0, 346125.0 ], [ 2048875.0, 346125.0 ], [ 2049125.0, 346125.0 ], [ 2049125.0, 346375.0 ], [ 2049375.0, 346375.0 ], [ 2049625.0, 346375.0 ], [ 2049625.0, 346625.0 ], [ 2049875.0, 346625.0 ], [ 2050125.0, 346625.0 ], [ 2050375.0, 346625.0 ], [ 2050375.0, 346875.0 ] ], [ [ 2054375.0, 347875.0 ], [ 2054625.0, 347875.0 ], [ 2054625.0, 348125.0 ], [ 2054375.0, 348125.0 ], [ 2054375.0, 347875.0 ] ], [ [ 2056125.0, 349375.0 ], [ 2056375.0, 349375.0 ], [ 2056375.0, 349625.0 ], [ 2056125.0, 349625.0 ], [ 2056125.0, 349375.0 ] ], [ [ 2058625.0, 349875.0 ], [ 2058625.0, 349625.0 ], [ 2058875.0, 349625.0 ], [ 2058875.0, 349875.0 ], [ 2058625.0, 349875.0 ] ], [ [ 2054125.0, 347625.0 ], [ 2054125.0, 347875.0 ], [ 2053875.0, 347875.0 ], [ 2053875.0, 347625.0 ], [ 2054125.0, 347625.0 ] ], [ [ 2057125.0, 348875.0 ], [ 2057375.0, 348875.0 ], [ 2057375.0, 349125.0 ], [ 2057125.0, 349125.0 ], [ 2057125.0, 348875.0 ] ], [ [ 2055875.0, 348625.0 ], [ 2056125.0, 348625.0 ], [ 2056125.0, 348875.0 ], [ 2055875.0, 348875.0 ], [ 2055875.0, 348625.0 ] ], [ [ 2056625.0, 349375.0 ], [ 2056625.0, 349125.0 ], [ 2056875.0, 349125.0 ], [ 2056875.0, 349375.0 ], [ 2056875.0, 349625.0 ], [ 2056875.0, 349875.0 ], [ 2056625.0, 349875.0 ], [ 2056625.0, 349625.0 ], [ 2056625.0, 349375.0 ] ] ], [ [ [ 2061375.0, 353875.0 ], [ 2061125.0, 353875.0 ], [ 2061125.0, 354125.0 ], [ 2061125.0, 354375.0 ], [ 2060875.0, 354375.0 ], [ 2060875.0, 354625.0 ], [ 2060875.0, 354875.0 ], [ 2060875.0, 355125.0 ], [ 2061125.0, 355125.0 ], [ 2061125.0, 355375.0 ], [ 2061125.0, 355625.0 ], [ 2061125.0, 355875.0 ], [ 2061375.0, 355875.0 ], [ 2061375.0, 356125.0 ], [ 2061375.0, 356375.0 ], [ 2061625.0, 356375.0 ], [ 2061875.0, 356375.0 ], [ 2061875.0, 356625.0 ], [ 2062125.0, 356625.0 ], [ 2062375.0, 356625.0 ], [ 2062375.0, 356375.0 ], [ 2062375.0, 356125.0 ], [ 2062375.0, 355875.0 ], [ 2062375.0, 355625.0 ], [ 2062375.0, 355375.0 ], [ 2062375.0, 355125.0 ], [ 2062375.0, 354875.0 ], [ 2062625.0, 354875.0 ], [ 2062625.0, 354625.0 ], [ 2062875.0, 354625.0 ], [ 2063125.0, 354625.0 ], [ 2063375.0, 354625.0 ], [ 2063625.0, 354625.0 ], [ 2063625.0, 354875.0 ], [ 2063875.0, 354875.0 ], [ 2064125.0, 354875.0 ], [ 2064375.0, 354875.0 ], [ 2064625.0, 354875.0 ], [ 2064875.0, 354875.0 ], [ 2065125.0, 354875.0 ], [ 2065125.0, 355125.0 ], [ 2064875.0, 355125.0 ], [ 2064875.0, 355375.0 ], [ 2064625.0, 355375.0 ], [ 2064625.0, 355625.0 ], [ 2064375.0, 355625.0 ], [ 2064125.0, 355625.0 ], [ 2064125.0, 355875.0 ], [ 2063875.0, 355875.0 ], [ 2063875.0, 356125.0 ], [ 2064125.0, 356125.0 ], [ 2064375.0, 356125.0 ], [ 2064625.0, 356125.0 ], [ 2064875.0, 356125.0 ], [ 2064875.0, 355875.0 ], [ 2065125.0, 355875.0 ], [ 2065125.0, 355625.0 ], [ 2065375.0, 355625.0 ], [ 2065375.0, 355375.0 ], [ 2065625.0, 355375.0 ], [ 2065625.0, 355125.0 ], [ 2065625.0, 354875.0 ], [ 2065625.0, 354625.0 ], [ 2065625.0, 354375.0 ], [ 2065875.0, 354375.0 ], [ 2065875.0, 354125.0 ], [ 2065625.0, 354125.0 ], [ 2065625.0, 353875.0 ], [ 2065375.0, 353875.0 ], [ 2065125.0, 353875.0 ], [ 2065125.0, 353625.0 ], [ 2064875.0, 353625.0 ], [ 2064625.0, 353625.0 ], [ 2064625.0, 353875.0 ], [ 2064375.0, 353875.0 ], [ 2064125.0, 353875.0 ], [ 2063875.0, 353875.0 ], [ 2063625.0, 353875.0 ], [ 2063375.0, 353875.0 ], [ 2063125.0, 353875.0 ], [ 2062875.0, 353875.0 ], [ 2062875.0, 353625.0 ], [ 2062625.0, 353625.0 ], [ 2062375.0, 353625.0 ], [ 2062125.0, 353625.0 ], [ 2061875.0, 353625.0 ], [ 2061625.0, 353625.0 ], [ 2061375.0, 353625.0 ], [ 2061375.0, 353875.0 ] ], [ [ 2062875.0, 354125.0 ], [ 2062625.0, 354125.0 ], [ 2062625.0, 353875.0 ], [ 2062875.0, 353875.0 ], [ 2062875.0, 354125.0 ] ], [ [ 2061375.0, 354625.0 ], [ 2061625.0, 354625.0 ], [ 2061625.0, 354875.0 ], [ 2061375.0, 354875.0 ], [ 2061375.0, 354625.0 ] ] ], [ [ [ 2064375.0, 352375.0 ], [ 2064375.0, 352625.0 ], [ 2064625.0, 352625.0 ], [ 2064875.0, 352625.0 ], [ 2064875.0, 352875.0 ], [ 2065125.0, 352875.0 ], [ 2065375.0, 352875.0 ], [ 2065375.0, 353125.0 ], [ 2065625.0, 353125.0 ], [ 2065625.0, 353375.0 ], [ 2065875.0, 353375.0 ], [ 2066125.0, 353375.0 ], [ 2066125.0, 353625.0 ], [ 2066375.0, 353625.0 ], [ 2066625.0, 353625.0 ], [ 2066875.0, 353625.0 ], [ 2066875.0, 353875.0 ], [ 2067125.0, 353875.0 ], [ 2067375.0, 353875.0 ], [ 2067625.0, 353875.0 ], [ 2067625.0, 354125.0 ], [ 2067875.0, 354125.0 ], [ 2068125.0, 354125.0 ], [ 2068375.0, 354125.0 ], [ 2068375.0, 354375.0 ], [ 2068625.0, 354375.0 ], [ 2068625.0, 354625.0 ], [ 2068875.0, 354625.0 ], [ 2069125.0, 354625.0 ], [ 2069125.0, 354875.0 ], [ 2069375.0, 354875.0 ], [ 2069625.0, 354875.0 ], [ 2069625.0, 355125.0 ], [ 2069875.0, 355125.0 ], [ 2069875.0, 355375.0 ], [ 2070125.0, 355375.0 ], [ 2070375.0, 355375.0 ], [ 2070375.0, 355625.0 ], [ 2070625.0, 355625.0 ], [ 2070625.0, 355875.0 ], [ 2070875.0, 355875.0 ], [ 2070875.0, 356125.0 ], [ 2071125.0, 356125.0 ], [ 2071125.0, 356375.0 ], [ 2071125.0, 356625.0 ], [ 2071375.0, 356625.0 ], [ 2071375.0, 356875.0 ], [ 2071375.0, 357125.0 ], [ 2071375.0, 357375.0 ], [ 2071375.0, 357625.0 ], [ 2071625.0, 357625.0 ], [ 2071875.0, 357625.0 ], [ 2072125.0, 357625.0 ], [ 2072375.0, 357625.0 ], [ 2072375.0, 357375.0 ], [ 2072625.0, 357375.0 ], [ 2072625.0, 357625.0 ], [ 2072875.0, 357625.0 ], [ 2072875.0, 357875.0 ], [ 2072875.0, 358125.0 ], [ 2072875.0, 358375.0 ], [ 2072625.0, 358375.0 ], [ 2072375.0, 358375.0 ], [ 2072375.0, 358125.0 ], [ 2072375.0, 357875.0 ], [ 2072125.0, 357875.0 ], [ 2071875.0, 357875.0 ], [ 2071625.0, 357875.0 ], [ 2071625.0, 358125.0 ], [ 2071625.0, 358375.0 ], [ 2071625.0, 358625.0 ], [ 2071625.0, 358875.0 ], [ 2071375.0, 358875.0 ], [ 2071375.0, 359125.0 ], [ 2071625.0, 359125.0 ], [ 2071625.0, 359375.0 ], [ 2071875.0, 359375.0 ], [ 2071875.0, 359625.0 ], [ 2071625.0, 359625.0 ], [ 2071625.0, 359875.0 ], [ 2071625.0, 360125.0 ], [ 2071875.0, 360125.0 ], [ 2071875.0, 360375.0 ], [ 2071875.0, 360625.0 ], [ 2071875.0, 360875.0 ], [ 2072125.0, 360875.0 ], [ 2072125.0, 361125.0 ], [ 2072125.0, 361375.0 ], [ 2072375.0, 361375.0 ], [ 2072375.0, 361625.0 ], [ 2072625.0, 361625.0 ], [ 2072875.0, 361625.0 ], [ 2072875.0, 361875.0 ], [ 2073125.0, 361875.0 ], [ 2073125.0, 362125.0 ], [ 2073375.0, 362125.0 ], [ 2073375.0, 362375.0 ], [ 2073625.0, 362375.0 ], [ 2073625.0, 362625.0 ], [ 2073875.0, 362625.0 ], [ 2073875.0, 362875.0 ], [ 2074125.0, 362875.0 ], [ 2074125.0, 363125.0 ], [ 2074125.0, 363375.0 ], [ 2074375.0, 363375.0 ], [ 2074625.0, 363375.0 ], [ 2074625.0, 363625.0 ], [ 2074875.0, 363625.0 ], [ 2074875.0, 363875.0 ], [ 2074875.0, 364125.0 ], [ 2075125.0, 364125.0 ], [ 2075125.0, 364375.0 ], [ 2075125.0, 364625.0 ], [ 2075375.0, 364625.0 ], [ 2075375.0, 364875.0 ], [ 2075625.0, 364875.0 ], [ 2075875.0, 364875.0 ], [ 2075875.0, 365125.0 ], [ 2075875.0, 365375.0 ], [ 2076125.0, 365375.0 ], [ 2076125.0, 365625.0 ], [ 2076375.0, 365625.0 ], [ 2076375.0, 365875.0 ], [ 2076625.0, 365875.0 ], [ 2076625.0, 366125.0 ], [ 2076875.0, 366125.0 ], [ 2077125.0, 366125.0 ], [ 2077125.0, 365875.0 ], [ 2077125.0, 365625.0 ], [ 2077125.0, 365375.0 ], [ 2077125.0, 365125.0 ], [ 2077125.0, 364875.0 ], [ 2077375.0, 364875.0 ], [ 2077625.0, 364875.0 ], [ 2077875.0, 364875.0 ], [ 2078125.0, 364875.0 ], [ 2078375.0, 364875.0 ], [ 2078625.0, 364875.0 ], [ 2078875.0, 364875.0 ], [ 2078875.0, 364625.0 ], [ 2079125.0, 364625.0 ], [ 2079375.0, 364625.0 ], [ 2079625.0, 364625.0 ], [ 2079875.0, 364625.0 ], [ 2079875.0, 364375.0 ], [ 2080125.0, 364375.0 ], [ 2080375.0, 364375.0 ], [ 2080625.0, 364375.0 ], [ 2080875.0, 364375.0 ], [ 2081125.0, 364375.0 ], [ 2081375.0, 364375.0 ], [ 2081625.0, 364375.0 ], [ 2081875.0, 364375.0 ], [ 2081875.0, 363984.556887765647843 ], [ 2081796.449778583133593, 363953.550221416982822 ], [ 2081767.417961139930412, 363942.090293478977401 ], [ 2081724.657114746980369, 363875.0 ], [ 2081491.233539879787713, 363508.766460120212287 ], [ 2081405.975796067388728, 363375.0 ], [ 2081224.63842322002165, 363090.487915013974998 ], [ 2081210.637321189278737, 363039.362678810663056 ], [ 2081125.0, 363125.0 ], [ 2080875.0, 363125.0 ], [ 2080875.0, 362875.0 ], [ 2080875.0, 362625.0 ], [ 2081049.399536174023524, 362450.600463825976476 ], [ 2081028.695674711605534, 362375.0 ], [ 2080625.0, 362375.0 ], [ 2080625.0, 362125.0 ], [ 2080625.0, 361875.0 ], [ 2080625.0, 361625.0 ], [ 2080780.669894482009113, 361469.330105518107302 ], [ 2080754.836753551848233, 361375.0 ], [ 2080375.0, 361375.0 ], [ 2080125.0, 361375.0 ], [ 2080125.0, 361625.0 ], [ 2080125.0, 361875.0 ], [ 2080125.0, 362125.0 ], [ 2079875.0, 362125.0 ], [ 2079875.0, 361875.0 ], [ 2079625.0, 361875.0 ], [ 2079625.0, 361625.0 ], [ 2079875.0, 361625.0 ], [ 2079875.0, 361375.0 ], [ 2079625.0, 361375.0 ], [ 2079625.0, 361125.0 ], [ 2079375.0, 361125.0 ], [ 2079375.0, 360875.0 ], [ 2079125.0, 360875.0 ], [ 2078875.0, 360875.0 ], [ 2078875.0, 360625.0 ], [ 2079125.0, 360625.0 ], [ 2079375.0, 360625.0 ], [ 2079375.0, 360375.0 ], [ 2079125.0, 360375.0 ], [ 2078875.0, 360375.0 ], [ 2078875.0, 360125.0 ], [ 2078875.0, 359875.0 ], [ 2078625.0, 359875.0 ], [ 2078625.0, 359625.0 ], [ 2078375.0, 359625.0 ], [ 2078375.0, 359375.0 ], [ 2078125.0, 359375.0 ], [ 2077875.0, 359375.0 ], [ 2077875.0, 359625.0 ], [ 2077625.0, 359625.0 ], [ 2077625.0, 359375.0 ], [ 2077375.0, 359375.0 ], [ 2077375.0, 359625.0 ], [ 2077375.0, 359875.0 ], [ 2077375.0, 360125.0 ], [ 2077125.0, 360125.0 ], [ 2077125.0, 359875.0 ], [ 2076875.0, 359875.0 ], [ 2076625.0, 359875.0 ], [ 2076625.0, 359625.0 ], [ 2076375.0, 359625.0 ], [ 2076375.0, 359375.0 ], [ 2076125.0, 359375.0 ], [ 2076125.0, 359125.0 ], [ 2075875.0, 359125.0 ], [ 2075625.0, 359125.0 ], [ 2075625.0, 359375.0 ], [ 2075375.0, 359375.0 ], [ 2075125.0, 359375.0 ], [ 2074875.0, 359375.0 ], [ 2074875.0, 359625.0 ], [ 2074625.0, 359625.0 ], [ 2074625.0, 359875.0 ], [ 2074375.0, 359875.0 ], [ 2074375.0, 360125.0 ], [ 2074125.0, 360125.0 ], [ 2073875.0, 360125.0 ], [ 2073875.0, 359875.0 ], [ 2073875.0, 359625.0 ], [ 2073625.0, 359625.0 ], [ 2073625.0, 359375.0 ], [ 2073625.0, 359125.0 ], [ 2073875.0, 359125.0 ], [ 2074125.0, 359125.0 ], [ 2074125.0, 358875.0 ], [ 2074125.0, 358625.0 ], [ 2074375.0, 358625.0 ], [ 2074375.0, 358875.0 ], [ 2074625.0, 358875.0 ], [ 2074625.0, 358625.0 ], [ 2074625.0, 358375.0 ], [ 2074625.0, 358125.0 ], [ 2074875.0, 358125.0 ], [ 2074875.0, 358375.0 ], [ 2074875.0, 358625.0 ], [ 2075125.0, 358625.0 ], [ 2075375.0, 358625.0 ], [ 2075375.0, 358875.0 ], [ 2075625.0, 358875.0 ], [ 2075625.0, 358625.0 ], [ 2075875.0, 358625.0 ], [ 2075875.0, 358875.0 ], [ 2076125.0, 358875.0 ], [ 2076125.0, 358625.0 ], [ 2076375.0, 358625.0 ], [ 2076375.0, 358375.0 ], [ 2076625.0, 358375.0 ], [ 2076625.0, 358625.0 ], [ 2076625.0, 358875.0 ], [ 2076625.0, 359125.0 ], [ 2076625.0, 359375.0 ], [ 2076875.0, 359375.0 ], [ 2076875.0, 359125.0 ], [ 2077125.0, 359125.0 ], [ 2077125.0, 358875.0 ], [ 2077375.0, 358875.0 ], [ 2077375.0, 359125.0 ], [ 2077625.0, 359125.0 ], [ 2077625.0, 358875.0 ], [ 2077625.0, 358625.0 ], [ 2077375.0, 358625.0 ], [ 2077125.0, 358625.0 ], [ 2077125.0, 358375.0 ], [ 2077125.0, 358125.0 ], [ 2077125.0, 357875.0 ], [ 2077125.0, 357625.0 ], [ 2077248.277869393816218, 357501.722130606183782 ], [ 2076875.0, 357311.750357790791895 ], [ 2076751.236449274467304, 357248.763550725532696 ], [ 2076461.280064549995586, 357101.196462072024588 ], [ 2076375.0, 357084.972860190784559 ], [ 2076125.0, 357037.964313182164915 ], [ 2076125.0, 357375.0 ], [ 2075875.0, 357375.0 ], [ 2075625.0, 357375.0 ], [ 2075375.0, 357375.0 ], [ 2075125.0, 357375.0 ], [ 2075125.0, 357625.0 ], [ 2074875.0, 357625.0 ], [ 2074625.0, 357625.0 ], [ 2074375.0, 357625.0 ], [ 2074375.0, 357375.0 ], [ 2074625.0, 357375.0 ], [ 2074625.0, 357125.0 ], [ 2074375.0, 357125.0 ], [ 2074125.0, 357125.0 ], [ 2074125.0, 356875.0 ], [ 2074375.0, 356875.0 ], [ 2074625.0, 356875.0 ], [ 2074625.0, 356625.0 ], [ 2074375.0, 356625.0 ], [ 2074125.0, 356625.0 ], [ 2073875.0, 356625.0 ], [ 2073875.0, 356375.0 ], [ 2073625.0, 356375.0 ], [ 2073625.0, 356125.0 ], [ 2073375.0, 356125.0 ], [ 2073375.0, 356375.0 ], [ 2073125.0, 356375.0 ], [ 2073125.0, 356125.0 ], [ 2073125.0, 355875.0 ], [ 2073125.0, 355625.0 ], [ 2073375.0, 355625.0 ], [ 2073502.373224553652108, 355497.626775446347892 ], [ 2073125.0, 355363.44962893857155 ], [ 2072875.0, 355274.560740049812011 ], [ 2072764.668306520674378, 355235.33169347938383 ], [ 2072375.0, 355096.782962272234727 ], [ 2072125.0, 355007.894073383475188 ], [ 2072026.963388487463817, 354973.036611512419768 ], [ 2071625.0, 354830.116295605956111 ], [ 2071375.0, 354741.227406717138365 ], [ 2071289.258470454486087, 354710.741529545455705 ], [ 2070958.61854215990752, 354593.180666151980404 ], [ 2070875.0, 354561.541217767167836 ], [ 2070739.666175345424563, 354510.333824654691853 ], [ 2070625.0, 354625.0 ], [ 2070375.0, 354625.0 ], [ 2070375.0, 354372.352028578228783 ], [ 2070125.0, 354277.757433983730152 ], [ 2070014.175979266641662, 354235.824020733358338 ], [ 2069625.0, 354088.568244794732891 ], [ 2069375.0, 353993.973650200292468 ], [ 2069288.685783188091591, 353961.314216811966617 ], [ 2068881.083069419953972, 353807.086162953986786 ], [ 2068875.0, 353805.072173754160758 ], [ 2068739.717351697385311, 353760.282648302498274 ], [ 2068375.0, 353639.53163321322063 ], [ 2068125.0, 353556.761362942750566 ], [ 2067988.448316165013239, 353511.551683835103177 ], [ 2067875.0, 353625.0 ], [ 2067625.0, 353625.0 ], [ 2067375.0, 353625.0 ], [ 2067375.0, 353288.853856756235473 ], [ 2067265.248831795528531, 353234.751168204587884 ], [ 2066875.0, 353042.374983516347129 ], [ 2066762.890341229736805, 352987.109658770321403 ], [ 2066375.0, 352795.896110276458785 ], [ 2066260.531850663945079, 352739.468149336054921 ], [ 2066167.185379809932783, 352693.452283421996981 ], [ 2065875.0, 352565.470257554552518 ], [ 2065742.546545034041628, 352507.453454966016579 ], [ 2065375.0, 352346.461993091274053 ], [ 2065220.994820896303281, 352279.005179103580303 ], [ 2065125.0, 352375.0 ], [ 2064875.0, 352375.0 ], [ 2064875.0, 352110.745114879333414 ], [ 2064722.172132423147559, 352027.827867576968856 ], [ 2064375.0, 351839.468519134621602 ], [ 2064235.965235871262848, 351764.034764128620736 ], [ 2064155.157782339956611, 351720.192422319028992 ], [ 2063875.0, 351530.590690837008879 ], [ 2063782.207961488747969, 351467.792038511310238 ], [ 2063484.015190403908491, 351265.984809596033301 ], [ 2063228.689260710030794, 351093.188473339017946 ], [ 2063125.0, 351030.452113917737734 ], [ 2063125.0, 351375.0 ], [ 2062875.0, 351375.0 ], [ 2062875.0, 351625.0 ], [ 2063125.0, 351625.0 ], [ 2063375.0, 351625.0 ], [ 2063375.0, 351875.0 ], [ 2063625.0, 351875.0 ], [ 2063625.0, 352125.0 ], [ 2063875.0, 352125.0 ], [ 2063875.0, 352375.0 ], [ 2064125.0, 352375.0 ], [ 2064375.0, 352375.0 ] ], [ [ 2072625.0, 357125.0 ], [ 2072875.0, 357125.0 ], [ 2073125.0, 357125.0 ], [ 2073125.0, 357375.0 ], [ 2072875.0, 357375.0 ], [ 2072625.0, 357375.0 ], [ 2072625.0, 357125.0 ] ], [ [ 2073875.0, 357375.0 ], [ 2074125.0, 357375.0 ], [ 2074125.0, 357625.0 ], [ 2073875.0, 357625.0 ], [ 2073625.0, 357625.0 ], [ 2073625.0, 357375.0 ], [ 2073625.0, 357125.0 ], [ 2073875.0, 357125.0 ], [ 2073875.0, 357375.0 ] ], [ [ 2070125.0, 354875.0 ], [ 2070375.0, 354875.0 ], [ 2070375.0, 355125.0 ], [ 2070125.0, 355125.0 ], [ 2070125.0, 354875.0 ] ], [ [ 2072375.0, 359625.0 ], [ 2072375.0, 359375.0 ], [ 2072625.0, 359375.0 ], [ 2072625.0, 359625.0 ], [ 2072375.0, 359625.0 ] ], [ [ 2072625.0, 360125.0 ], [ 2072625.0, 360375.0 ], [ 2072375.0, 360375.0 ], [ 2072375.0, 360125.0 ], [ 2072625.0, 360125.0 ] ], [ [ 2072625.0, 361125.0 ], [ 2072375.0, 361125.0 ], [ 2072375.0, 360875.0 ], [ 2072625.0, 360875.0 ], [ 2072625.0, 361125.0 ] ], [ [ 2076125.0, 362875.0 ], [ 2076125.0, 362625.0 ], [ 2076375.0, 362625.0 ], [ 2076625.0, 362625.0 ], [ 2076875.0, 362625.0 ], [ 2076875.0, 362375.0 ], [ 2076875.0, 362125.0 ], [ 2077125.0, 362125.0 ], [ 2077125.0, 362375.0 ], [ 2077375.0, 362375.0 ], [ 2077375.0, 362625.0 ], [ 2077125.0, 362625.0 ], [ 2077125.0, 362875.0 ], [ 2077375.0, 362875.0 ], [ 2077625.0, 362875.0 ], [ 2077625.0, 363125.0 ], [ 2077625.0, 363375.0 ], [ 2077875.0, 363375.0 ], [ 2078125.0, 363375.0 ], [ 2078375.0, 363375.0 ], [ 2078375.0, 363625.0 ], [ 2078125.0, 363625.0 ], [ 2078125.0, 363875.0 ], [ 2077875.0, 363875.0 ], [ 2077875.0, 364125.0 ], [ 2077625.0, 364125.0 ], [ 2077625.0, 363875.0 ], [ 2077375.0, 363875.0 ], [ 2077125.0, 363875.0 ], [ 2077125.0, 363625.0 ], [ 2076875.0, 363625.0 ], [ 2076875.0, 363375.0 ], [ 2076625.0, 363375.0 ], [ 2076375.0, 363375.0 ], [ 2076375.0, 363125.0 ], [ 2076125.0, 363125.0 ], [ 2075875.0, 363125.0 ], [ 2075875.0, 362875.0 ], [ 2076125.0, 362875.0 ] ], [ [ 2077125.0, 361875.0 ], [ 2077375.0, 361875.0 ], [ 2077625.0, 361875.0 ], [ 2077625.0, 362125.0 ], [ 2077375.0, 362125.0 ], [ 2077125.0, 362125.0 ], [ 2077125.0, 361875.0 ] ], [ [ 2077625.0, 360625.0 ], [ 2077375.0, 360625.0 ], [ 2077375.0, 360375.0 ], [ 2077625.0, 360375.0 ], [ 2077625.0, 360625.0 ] ], [ [ 2073875.0, 361625.0 ], [ 2073625.0, 361625.0 ], [ 2073625.0, 361375.0 ], [ 2073875.0, 361375.0 ], [ 2074125.0, 361375.0 ], [ 2074125.0, 361125.0 ], [ 2074125.0, 360875.0 ], [ 2074375.0, 360875.0 ], [ 2074625.0, 360875.0 ], [ 2074625.0, 360625.0 ], [ 2074875.0, 360625.0 ], [ 2075125.0, 360625.0 ], [ 2075375.0, 360625.0 ], [ 2075375.0, 360875.0 ], [ 2075375.0, 361125.0 ], [ 2075125.0, 361125.0 ], [ 2074875.0, 361125.0 ], [ 2074625.0, 361125.0 ], [ 2074375.0, 361125.0 ], [ 2074375.0, 361375.0 ], [ 2074375.0, 361625.0 ], [ 2074375.0, 361875.0 ], [ 2074125.0, 361875.0 ], [ 2074125.0, 362125.0 ], [ 2073875.0, 362125.0 ], [ 2073875.0, 361875.0 ], [ 2073875.0, 361625.0 ] ], [ [ 2075125.0, 362125.0 ], [ 2075125.0, 362375.0 ], [ 2074875.0, 362375.0 ], [ 2074875.0, 362125.0 ], [ 2075125.0, 362125.0 ] ], [ [ 2074625.0, 362625.0 ], [ 2074875.0, 362625.0 ], [ 2074875.0, 362875.0 ], [ 2074625.0, 362875.0 ], [ 2074625.0, 362625.0 ] ], [ [ 2076125.0, 357625.0 ], [ 2076125.0, 357875.0 ], [ 2076375.0, 357875.0 ], [ 2076375.0, 358125.0 ], [ 2076125.0, 358125.0 ], [ 2075875.0, 358125.0 ], [ 2075875.0, 357875.0 ], [ 2075625.0, 357875.0 ], [ 2075625.0, 357625.0 ], [ 2075875.0, 357625.0 ], [ 2076125.0, 357625.0 ] ], [ [ 2076125.0, 360375.0 ], [ 2076125.0, 360625.0 ], [ 2075875.0, 360625.0 ], [ 2075875.0, 360375.0 ], [ 2076125.0, 360375.0 ] ], [ [ 2075875.0, 363875.0 ], [ 2075875.0, 364125.0 ], [ 2075875.0, 364375.0 ], [ 2075625.0, 364375.0 ], [ 2075625.0, 364125.0 ], [ 2075625.0, 363875.0 ], [ 2075875.0, 363875.0 ] ], [ [ 2080125.0, 363625.0 ], [ 2080125.0, 363375.0 ], [ 2080375.0, 363375.0 ], [ 2080375.0, 363625.0 ], [ 2080125.0, 363625.0 ] ], [ [ 2078125.0, 361375.0 ], [ 2078125.0, 361125.0 ], [ 2078375.0, 361125.0 ], [ 2078375.0, 361375.0 ], [ 2078125.0, 361375.0 ] ], [ [ 2078125.0, 361625.0 ], [ 2077875.0, 361625.0 ], [ 2077875.0, 361375.0 ], [ 2078125.0, 361375.0 ], [ 2078125.0, 361625.0 ] ], [ [ 2078875.0, 362875.0 ], [ 2078875.0, 362625.0 ], [ 2079125.0, 362625.0 ], [ 2079375.0, 362625.0 ], [ 2079375.0, 362375.0 ], [ 2079625.0, 362375.0 ], [ 2079625.0, 362625.0 ], [ 2079875.0, 362625.0 ], [ 2079875.0, 362875.0 ], [ 2079875.0, 363125.0 ], [ 2079875.0, 363375.0 ], [ 2079625.0, 363375.0 ], [ 2079375.0, 363375.0 ], [ 2079375.0, 363125.0 ], [ 2079375.0, 362875.0 ], [ 2079125.0, 362875.0 ], [ 2078875.0, 362875.0 ] ] ], [ [ [ 2046125.0, 362625.0 ], [ 2046125.0, 362883.08905790746212 ], [ 2046375.0, 362906.344871860928833 ], [ 2046375.0, 362625.0 ], [ 2046625.0, 362625.0 ], [ 2046625.0, 362375.0 ], [ 2046625.0, 362125.0 ], [ 2046625.0, 361875.0 ], [ 2046625.0, 361625.0 ], [ 2046625.0, 361375.0 ], [ 2046875.0, 361375.0 ], [ 2046875.0, 361125.0 ], [ 2047125.0, 361125.0 ], [ 2047375.0, 361125.0 ], [ 2047625.0, 361125.0 ], [ 2047875.0, 361125.0 ], [ 2047875.0, 360875.0 ], [ 2047875.0, 360625.0 ], [ 2047875.0, 360375.0 ], [ 2047625.0, 360375.0 ], [ 2047625.0, 360125.0 ], [ 2047375.0, 360125.0 ], [ 2047375.0, 359875.0 ], [ 2047375.0, 359625.0 ], [ 2047125.0, 359625.0 ], [ 2047125.0, 359375.0 ], [ 2047125.0, 359125.0 ], [ 2046875.0, 359125.0 ], [ 2046875.0, 359375.0 ], [ 2046625.0, 359375.0 ], [ 2046625.0, 359625.0 ], [ 2046625.0, 359875.0 ], [ 2046375.0, 359875.0 ], [ 2046375.0, 360125.0 ], [ 2046375.0, 360375.0 ], [ 2046125.0, 360375.0 ], [ 2045875.0, 360375.0 ], [ 2045875.0, 360625.0 ], [ 2045875.0, 360875.0 ], [ 2045875.0, 361125.0 ], [ 2045625.0, 361125.0 ], [ 2045625.0, 361375.0 ], [ 2045625.0, 361625.0 ], [ 2045625.0, 361875.0 ], [ 2045375.0, 361875.0 ], [ 2045375.0, 362125.0 ], [ 2045125.0, 362125.0 ], [ 2045125.0, 362375.0 ], [ 2045375.0, 362375.0 ], [ 2045625.0, 362375.0 ], [ 2045875.0, 362375.0 ], [ 2045875.0, 362625.0 ], [ 2046125.0, 362625.0 ] ], [ [ 2047625.0, 360875.0 ], [ 2047375.0, 360875.0 ], [ 2047375.0, 360625.0 ], [ 2047625.0, 360625.0 ], [ 2047625.0, 360875.0 ] ] ], [ [ [ 2054625.0, 355499.996281132742297 ], [ 2054875.0, 355518.613302409474272 ], [ 2055125.0, 355537.230323686206248 ], [ 2055206.686629440402612, 355543.313370559480973 ], [ 2055405.177300299983472, 355558.094590729975607 ], [ 2055625.0, 355645.153085660480428 ], [ 2055625.0, 355375.0 ], [ 2055875.0, 355375.0 ], [ 2055875.0, 355744.162986650306266 ], [ 2055968.720130129950121, 355781.279869870049879 ], [ 2056125.0, 355625.0 ], [ 2056375.0, 355625.0 ], [ 2056375.0, 355375.0 ], [ 2056625.0, 355375.0 ], [ 2056625.0, 355625.0 ], [ 2056875.0, 355625.0 ], [ 2056875.0, 355375.0 ], [ 2057125.0, 355375.0 ], [ 2057375.0, 355375.0 ], [ 2057375.0, 355625.0 ], [ 2057375.0, 355875.0 ], [ 2057375.0, 356125.0 ], [ 2057125.0, 356125.0 ], [ 2057125.0, 356375.0 ], [ 2056817.524534153053537, 356375.0 ], [ 2056883.783627749886364, 356437.771772880980279 ], [ 2056947.830443567596376, 356750.0 ], [ 2056958.649770909920335, 356802.744220794993453 ], [ 2056893.141895639942959, 357074.133989755995572 ], [ 2056849.058020095573738, 357125.0 ], [ 2056745.031082194764167, 357245.031082194647752 ], [ 2056528.169447730062529, 357495.256045041023754 ], [ 2056511.712746385484934, 357511.712746385543142 ], [ 2056625.0, 357625.0 ], [ 2056875.0, 357625.0 ], [ 2056875.0, 357875.0 ], [ 2056999.206282081548125, 357999.206282081606332 ], [ 2057052.232449860079214, 357869.586760849982966 ], [ 2057248.756075659999624, 357794.720617688028142 ], [ 2057250.0, 357796.710896631993819 ], [ 2057298.930689605418593, 357875.0 ], [ 2057625.0, 357875.0 ], [ 2057992.250846266048029, 357875.0 ], [ 2058002.054767834255472, 357625.0 ], [ 2058006.69421055726707, 357506.694210557383485 ], [ 2058006.775775169953704, 357504.614312936028 ], [ 2058125.0, 357398.212510592187755 ], [ 2058193.941133080050349, 357336.165490821993444 ], [ 2058375.0, 357352.625387815060094 ], [ 2058625.0, 357375.352660542761441 ], [ 2058811.586814159993082, 357392.315098193997983 ], [ 2058875.0, 357396.175031245104037 ], [ 2059125.0, 357411.39242254931014 ], [ 2059375.0, 357426.609813853574451 ], [ 2059625.0, 357441.827205157780554 ], [ 2059875.0, 357457.044596462044865 ], [ 2059887.787622109986842, 357457.822973460017238 ], [ 2060125.0, 357495.915472099150065 ], [ 2060375.0, 357536.061457500502001 ], [ 2060375.0, 357125.0 ], [ 2060375.0, 356875.0 ], [ 2060625.0, 356875.0 ], [ 2060625.0, 357125.0 ], [ 2060875.0, 357125.0 ], [ 2060875.0, 357375.0 ], [ 2060625.0, 357375.0 ], [ 2060451.632580644218251, 357548.367419355723541 ], [ 2060875.0, 357616.353428303205874 ], [ 2061125.0, 357656.499413704499602 ], [ 2061169.870323759969324, 357663.704867155000102 ], [ 2061375.0, 357675.562073874054477 ], [ 2061625.0, 357690.012940926069859 ], [ 2061875.0, 357704.46380797814345 ], [ 2062125.0, 357718.914675030158833 ], [ 2062375.0, 357733.365542082174215 ], [ 2062625.0, 357747.816409134189598 ], [ 2062745.233667867723852, 357754.766332132334355 ], [ 2062788.850669630104676, 357757.287546107021626 ], [ 2063125.0, 357921.786154585599434 ], [ 2063261.443581921281293, 357988.556418078835122 ], [ 2063625.0, 358166.467005648999475 ], [ 2063765.015010492876172, 358234.984989507123828 ], [ 2064108.366442860104144, 358403.008030877972487 ], [ 2064125.0, 358411.611594915855676 ], [ 2064265.642357896314934, 358484.357642103568651 ], [ 2064375.0, 358375.0 ], [ 2064375.0, 358125.0 ], [ 2064625.0, 358125.0 ], [ 2064625.0, 358375.0 ], [ 2064625.0, 358670.232284570811316 ], [ 2064759.960539714666083, 358740.039460285275709 ], [ 2064875.0, 358625.0 ], [ 2065125.0, 358625.0 ], [ 2065125.0, 358375.0 ], [ 2065375.0, 358375.0 ], [ 2065375.0, 358625.0 ], [ 2065625.0, 358625.0 ], [ 2065875.0, 358625.0 ], [ 2065875.0, 358875.0 ], [ 2066125.0, 358875.0 ], [ 2066125.0, 359125.0 ], [ 2066375.0, 359125.0 ], [ 2066375.0, 359375.0 ], [ 2066236.657446560449898, 359513.342553439608309 ], [ 2066382.425541399978101, 359591.508053570985794 ], [ 2066489.289956248365343, 359760.710043751518242 ], [ 2066625.0, 359625.0 ], [ 2066625.0, 359375.0 ], [ 2066875.0, 359375.0 ], [ 2066875.0, 359625.0 ], [ 2066875.0, 359875.0 ], [ 2066501.330059669679031, 359875.0 ], [ 2066504.083024040097371, 359919.047429903002921 ], [ 2066691.248381939949468, 360274.661609922011849 ], [ 2066717.802704068599269, 360282.197295931458939 ], [ 2067125.0, 360397.753285316983238 ], [ 2067375.0, 360468.699231262668036 ], [ 2067383.76020618993789, 360471.185235721990466 ], [ 2067476.595123366219923, 360523.404876633721869 ], [ 2067832.957065159920603, 360723.858468892984092 ], [ 2067875.0, 360743.378402926202398 ], [ 2067964.887919952394441, 360785.112080047605559 ], [ 2068094.988566220039502, 360845.515951530018356 ], [ 2068223.055662324884906, 361026.944337675115094 ], [ 2068292.271423967322335, 361125.0 ], [ 2068319.586995709920302, 361163.697059968020767 ], [ 2068348.93462627241388, 361375.0 ], [ 2068366.3783351900056, 361500.594704195973463 ], [ 2068310.099748992593959, 361625.0 ], [ 2068252.4457288144622, 361752.445728814403992 ], [ 2068188.571245179977268, 361893.641955794999376 ], [ 2068014.422521923435852, 362014.422521923435852 ], [ 2067719.184426684165373, 362219.184426684165373 ], [ 2067875.0, 362375.0 ], [ 2067875.0, 362625.0 ], [ 2067625.0, 362625.0 ], [ 2067625.0, 362284.505883899983019 ], [ 2067608.358635670039803, 362296.047475290019065 ], [ 2067205.192143323365599, 362455.192143323307391 ], [ 2067125.0, 362486.846936740737874 ], [ 2066897.130275639938191, 362576.795512147014961 ], [ 2066875.0, 362582.406004562682938 ], [ 2066625.0, 362645.786286251968704 ], [ 2066242.705913750454783, 362742.705913750454783 ], [ 2066232.693255069898441, 362745.244334260991309 ], [ 2066125.0, 362774.615222007560078 ], [ 2065923.870414529927075, 362829.468745318008587 ], [ 2065734.449496464570984, 362984.449496464570984 ], [ 2065459.449496466200799, 363209.449496466317214 ], [ 2065409.165680299978703, 363250.590800602978561 ], [ 2065240.716858180006966, 363465.830962193023879 ], [ 2065507.156448191963136, 363742.843551808095071 ], [ 2065752.292634962592274, 363997.707365037407726 ], [ 2065997.428821733221412, 364252.571178266778588 ], [ 2066242.565008503850549, 364507.434991496091243 ], [ 2066419.858612979995087, 364691.764056466985494 ], [ 2066520.182177630718797, 364729.817822369164787 ], [ 2066625.0, 364625.0 ], [ 2066875.0, 364625.0 ], [ 2066875.0, 364864.40389292355394 ], [ 2067125.0, 364959.231479130859952 ], [ 2067234.02791986009106, 365000.586897008994129 ], [ 2067375.0, 365030.145558973774314 ], [ 2067453.413004581583664, 365046.586995418299921 ], [ 2067625.0, 364875.0 ], [ 2067875.0, 364875.0 ], [ 2067875.0, 365134.984268651111051 ], [ 2068125.0, 365187.403623489721213 ], [ 2068375.0, 365239.822978328389581 ], [ 2068394.453138869954273, 365243.901862284983508 ], [ 2068470.574728991370648, 365279.425271008512937 ], [ 2068875.0, 365468.157064146769699 ], [ 2068955.949212579987943, 365505.933363350981381 ], [ 2068974.702985221985728, 365525.297014777956065 ], [ 2069220.70298522291705, 365779.297014777141158 ], [ 2069466.702985223848373, 366033.297014776268043 ], [ 2069712.702985224546865, 366287.297014775394928 ], [ 2069958.702985225478187, 366541.297014774521813 ], [ 2070039.769655956188217, 366625.0 ], [ 2070107.016163700027391, 366694.433386043994687 ], [ 2070295.117965420009568, 366954.882034580048639 ], [ 2070375.0, 366875.0 ], [ 2070375.0, 366625.0 ], [ 2070625.0, 366625.0 ], [ 2070625.0, 366875.0 ], [ 2070625.0, 367125.0 ], [ 2070875.0, 367125.0 ], [ 2070875.0, 367375.0 ], [ 2070594.4274010383524, 367375.0 ], [ 2070623.273554883897305, 367625.0 ], [ 2070649.795701619936153, 367854.858605051995255 ], [ 2070648.246363546932116, 367875.0 ], [ 2070875.0, 367875.0 ], [ 2071125.0, 367875.0 ], [ 2071125.0, 368125.0 ], [ 2070875.0, 368125.0 ], [ 2070629.015594316646457, 368125.0 ], [ 2070612.362630039919168, 368341.488535602984484 ], [ 2070770.930509852478281, 368479.069490147521719 ], [ 2071038.647045284276828, 368711.352954715664964 ], [ 2071125.0, 368625.0 ], [ 2071125.0, 368375.0 ], [ 2071375.0, 368375.0 ], [ 2071375.0, 368625.0 ], [ 2071625.0, 368625.0 ], [ 2071625.0, 368875.0 ], [ 2071875.0, 368875.0 ], [ 2071875.0, 369158.764727400324773 ], [ 2072125.0, 369264.604143458185717 ], [ 2072202.560165878152475, 369297.439834121789318 ], [ 2072375.0, 369125.0 ], [ 2072625.0, 369125.0 ], [ 2072625.0, 369491.622143981629051 ], [ 2072709.086039663525298, 369540.913960336416494 ], [ 2073024.303430967032909, 369725.696569032850675 ], [ 2073344.976855440065265, 369913.677542001008987 ], [ 2073375.0, 369914.783657853200566 ], [ 2073625.0, 369923.994184168870561 ], [ 2073875.0, 369933.204710484540556 ], [ 2074125.0, 369942.41523680021055 ], [ 2074375.0, 369951.625763115880545 ], [ 2074625.0, 369960.83628943155054 ], [ 2074875.0, 369970.046815747220535 ], [ 2075123.047755540115759, 369979.185417267028242 ], [ 2075125.0, 369978.518194476957433 ], [ 2075375.0, 369893.075156501901802 ], [ 2075761.112616637954488, 369761.112616637838073 ], [ 2075862.350919259944931, 369726.512184095976409 ], [ 2075875.0, 369724.624261597462464 ], [ 2076125.0, 369687.310828761721496 ], [ 2076375.0, 369649.997395926038735 ], [ 2076625.0, 369612.683963090297766 ], [ 2077049.348383468342945, 369549.348383468342945 ], [ 2077116.358817219967023, 369539.346826191991568 ], [ 2077125.0, 369538.026645489502698 ], [ 2077375.0, 369499.832201044948306 ], [ 2077625.0, 369461.637756600452121 ], [ 2077875.0, 369423.443312155897729 ], [ 2078125.0, 369385.248867711401545 ], [ 2078375.0, 369347.05442326690536 ], [ 2078463.949394129915163, 369333.464932497008704 ], [ 2078625.0, 369390.471424402145203 ], [ 2078875.0, 369478.963006903766654 ], [ 2078982.858659303281456, 369517.141340696776751 ], [ 2079375.0, 369655.946171907067765 ], [ 2079536.786761781433597, 369713.213238218508195 ], [ 2079624.374613140011206, 369744.216388517001178 ], [ 2079875.0, 369963.072360141552053 ], [ 2079961.44257466122508, 370038.55742533877492 ], [ 2080228.359867894090712, 370271.640132105909288 ], [ 2080375.0, 370125.0 ], [ 2080375.0, 369875.0 ], [ 2080625.0, 369875.0 ], [ 2080625.0, 370125.0 ], [ 2080625.0, 370457.010325575945899 ], [ 2080745.477443273877725, 370504.522556726180483 ], [ 2080953.248654260067269, 370586.460499086999334 ], [ 2081125.0, 370603.635633661004249 ], [ 2081375.0, 370628.635633661004249 ], [ 2081625.0, 370653.635633661004249 ], [ 2081625.0, 370375.0 ], [ 2081875.0, 370375.0 ], [ 2081875.0, 370678.635633661004249 ], [ 2082125.0, 370703.635633661004249 ], [ 2082125.0, 370375.0 ], [ 2082375.0, 370375.0 ], [ 2082375.0, 370728.635633660946041 ], [ 2082544.154196450021118, 370745.551053305971436 ], [ 2082625.0, 370759.279585984244477 ], [ 2082723.922289400594309, 370776.077710599405691 ], [ 2083125.0, 370844.185246361419559 ], [ 2083375.0, 370886.638076550036203 ], [ 2083536.130593339912593, 370913.999875419016462 ], [ 2083625.0, 370925.693218400643673 ], [ 2083875.0, 370958.587955242895987 ], [ 2084125.0, 370991.482692085148301 ], [ 2084247.358953380025923, 371007.582554372027516 ], [ 2084375.0, 370967.872006534249522 ], [ 2084668.481008660048246, 370876.566803838999476 ], [ 2084744.536088891560212, 370625.0 ], [ 2084375.0, 370625.0 ], [ 2084375.0, 370375.0 ], [ 2084780.131389210000634, 370375.0 ], [ 2084754.902031411183998, 370125.0 ], [ 2084729.672673612367362, 369875.0 ], [ 2084704.443315813550726, 369625.0 ], [ 2084687.197544449940324, 369454.110083765001036 ], [ 2084665.986869817599654, 369375.0 ], [ 2084598.957884310977533, 369125.0 ], [ 2084498.74964591441676, 368751.250354085525032 ], [ 2084375.0, 368875.0 ], [ 2084125.0, 368875.0 ], [ 2084125.0, 368625.0 ], [ 2084464.899913297733292, 368625.0 ], [ 2084397.870927791111171, 368375.0 ], [ 2084340.941632329951972, 368162.669114223972429 ], [ 2084341.959716498153284, 368125.0 ], [ 2084348.716473254840821, 367875.0 ], [ 2084125.0, 367875.0 ], [ 2084125.0, 368125.0 ], [ 2083875.0, 368125.0 ], [ 2083875.0, 367875.0 ], [ 2083625.0, 367875.0 ], [ 2083625.0, 367625.0 ], [ 2083375.0, 367625.0 ], [ 2083375.0, 367375.0 ], [ 2083625.0, 367375.0 ], [ 2083875.0, 367375.0 ], [ 2083875.0, 367125.0 ], [ 2084125.0, 367125.0 ], [ 2084368.98674352443777, 367125.0 ], [ 2084375.743500281125307, 366875.0 ], [ 2084382.500257037812844, 366625.0 ], [ 2084389.25701379426755, 366375.0 ], [ 2084396.013770550955087, 366125.0 ], [ 2084402.770527307642624, 365875.0 ], [ 2084409.52728406409733, 365625.0 ], [ 2084415.807775489985943, 365392.62181723798858 ], [ 2084237.027790324995294, 365262.972209675121121 ], [ 2084125.0, 365375.0 ], [ 2083875.0, 365375.0 ], [ 2083625.0, 365375.0 ], [ 2083625.0, 365125.0 ], [ 2083625.0, 364819.135262491530739 ], [ 2083512.470268202014267, 364737.52973179804394 ], [ 2083222.647259352728724, 364527.352740647213068 ], [ 2083125.0, 364625.0 ], [ 2083125.0, 364875.0 ], [ 2082875.0, 364875.0 ], [ 2082875.0, 365125.0 ], [ 2082875.0, 365375.0 ], [ 2082875.0, 365625.0 ], [ 2082625.0, 365625.0 ], [ 2082625.0, 365875.0 ], [ 2082625.0, 366125.0 ], [ 2082375.0, 366125.0 ], [ 2082375.0, 366375.0 ], [ 2082375.0, 366625.0 ], [ 2082375.0, 366875.0 ], [ 2082125.0, 366875.0 ], [ 2081875.0, 366875.0 ], [ 2081875.0, 367125.0 ], [ 2081875.0, 367375.0 ], [ 2081625.0, 367375.0 ], [ 2081625.0, 367625.0 ], [ 2081375.0, 367625.0 ], [ 2081125.0, 367625.0 ], [ 2081125.0, 367875.0 ], [ 2080875.0, 367875.0 ], [ 2080625.0, 367875.0 ], [ 2080625.0, 368125.0 ], [ 2080375.0, 368125.0 ], [ 2080125.0, 368125.0 ], [ 2079875.0, 368125.0 ], [ 2079625.0, 368125.0 ], [ 2079375.0, 368125.0 ], [ 2079125.0, 368125.0 ], [ 2078875.0, 368125.0 ], [ 2078625.0, 368125.0 ], [ 2078375.0, 368125.0 ], [ 2078125.0, 368125.0 ], [ 2078125.0, 367875.0 ], [ 2077875.0, 367875.0 ], [ 2077625.0, 367875.0 ], [ 2077375.0, 367875.0 ], [ 2077375.0, 367625.0 ], [ 2077125.0, 367625.0 ], [ 2076875.0, 367625.0 ], [ 2076625.0, 367625.0 ], [ 2076625.0, 367375.0 ], [ 2076375.0, 367375.0 ], [ 2076375.0, 367125.0 ], [ 2076125.0, 367125.0 ], [ 2076125.0, 366875.0 ], [ 2075875.0, 366875.0 ], [ 2075875.0, 366625.0 ], [ 2075625.0, 366625.0 ], [ 2075625.0, 366375.0 ], [ 2075375.0, 366375.0 ], [ 2075375.0, 366125.0 ], [ 2075125.0, 366125.0 ], [ 2075125.0, 365875.0 ], [ 2074875.0, 365875.0 ], [ 2074625.0, 365875.0 ], [ 2074625.0, 366125.0 ], [ 2074375.0, 366125.0 ], [ 2074375.0, 365875.0 ], [ 2074125.0, 365875.0 ], [ 2074125.0, 365625.0 ], [ 2073875.0, 365625.0 ], [ 2073875.0, 365375.0 ], [ 2073625.0, 365375.0 ], [ 2073625.0, 365125.0 ], [ 2073375.0, 365125.0 ], [ 2073375.0, 364875.0 ], [ 2073125.0, 364875.0 ], [ 2072875.0, 364875.0 ], [ 2072875.0, 364625.0 ], [ 2072625.0, 364625.0 ], [ 2072625.0, 364375.0 ], [ 2072625.0, 364125.0 ], [ 2072625.0, 363875.0 ], [ 2072375.0, 363875.0 ], [ 2072375.0, 363625.0 ], [ 2072375.0, 363375.0 ], [ 2072375.0, 363125.0 ], [ 2072125.0, 363125.0 ], [ 2071875.0, 363125.0 ], [ 2071875.0, 362875.0 ], [ 2071625.0, 362875.0 ], [ 2071625.0, 362625.0 ], [ 2071625.0, 362375.0 ], [ 2071375.0, 362375.0 ], [ 2071375.0, 362125.0 ], [ 2071125.0, 362125.0 ], [ 2071125.0, 361875.0 ], [ 2071125.0, 361625.0 ], [ 2070875.0, 361625.0 ], [ 2070875.0, 361375.0 ], [ 2070875.0, 361125.0 ], [ 2070875.0, 360875.0 ], [ 2070625.0, 360875.0 ], [ 2070625.0, 360625.0 ], [ 2070625.0, 360375.0 ], [ 2070625.0, 360125.0 ], [ 2070625.0, 359875.0 ], [ 2070375.0, 359875.0 ], [ 2070375.0, 360125.0 ], [ 2070125.0, 360125.0 ], [ 2069875.0, 360125.0 ], [ 2069625.0, 360125.0 ], [ 2069375.0, 360125.0 ], [ 2069125.0, 360125.0 ], [ 2068875.0, 360125.0 ], [ 2068625.0, 360125.0 ], [ 2068625.0, 360375.0 ], [ 2068375.0, 360375.0 ], [ 2068375.0, 360125.0 ], [ 2068125.0, 360125.0 ], [ 2067875.0, 360125.0 ], [ 2067625.0, 360125.0 ], [ 2067375.0, 360125.0 ], [ 2067375.0, 359875.0 ], [ 2067625.0, 359875.0 ], [ 2067625.0, 359625.0 ], [ 2067625.0, 359375.0 ], [ 2067375.0, 359375.0 ], [ 2067125.0, 359375.0 ], [ 2067125.0, 359125.0 ], [ 2066875.0, 359125.0 ], [ 2066875.0, 358875.0 ], [ 2066875.0, 358625.0 ], [ 2066625.0, 358625.0 ], [ 2066625.0, 358375.0 ], [ 2066625.0, 358125.0 ], [ 2066375.0, 358125.0 ], [ 2066125.0, 358125.0 ], [ 2066125.0, 357875.0 ], [ 2066125.0, 357625.0 ], [ 2065875.0, 357625.0 ], [ 2065875.0, 357375.0 ], [ 2065875.0, 357125.0 ], [ 2065875.0, 356875.0 ], [ 2065875.0, 356625.0 ], [ 2066125.0, 356625.0 ], [ 2066125.0, 356375.0 ], [ 2066125.0, 356125.0 ], [ 2065875.0, 356125.0 ], [ 2065875.0, 356375.0 ], [ 2065625.0, 356375.0 ], [ 2065625.0, 356625.0 ], [ 2065375.0, 356625.0 ], [ 2065125.0, 356625.0 ], [ 2065125.0, 356875.0 ], [ 2064875.0, 356875.0 ], [ 2064625.0, 356875.0 ], [ 2064375.0, 356875.0 ], [ 2064125.0, 356875.0 ], [ 2063875.0, 356875.0 ], [ 2063625.0, 356875.0 ], [ 2063375.0, 356875.0 ], [ 2063375.0, 357125.0 ], [ 2063125.0, 357125.0 ], [ 2062875.0, 357125.0 ], [ 2062625.0, 357125.0 ], [ 2062625.0, 357375.0 ], [ 2062375.0, 357375.0 ], [ 2062125.0, 357375.0 ], [ 2061875.0, 357375.0 ], [ 2061625.0, 357375.0 ], [ 2061375.0, 357375.0 ], [ 2061375.0, 357125.0 ], [ 2061125.0, 357125.0 ], [ 2061125.0, 356875.0 ], [ 2060875.0, 356875.0 ], [ 2060875.0, 356625.0 ], [ 2060625.0, 356625.0 ], [ 2060625.0, 356375.0 ], [ 2060625.0, 356125.0 ], [ 2060375.0, 356125.0 ], [ 2060375.0, 355875.0 ], [ 2060375.0, 355625.0 ], [ 2060125.0, 355625.0 ], [ 2059875.0, 355625.0 ], [ 2059875.0, 355375.0 ], [ 2059625.0, 355375.0 ], [ 2059375.0, 355375.0 ], [ 2059125.0, 355375.0 ], [ 2059125.0, 355125.0 ], [ 2058875.0, 355125.0 ], [ 2058625.0, 355125.0 ], [ 2058375.0, 355125.0 ], [ 2058125.0, 355125.0 ], [ 2058125.0, 354875.0 ], [ 2057875.0, 354875.0 ], [ 2057625.0, 354875.0 ], [ 2057375.0, 354875.0 ], [ 2057125.0, 354875.0 ], [ 2056875.0, 354875.0 ], [ 2056625.0, 354875.0 ], [ 2056375.0, 354875.0 ], [ 2056125.0, 354875.0 ], [ 2055875.0, 354875.0 ], [ 2055625.0, 354875.0 ], [ 2055625.0, 354625.0 ], [ 2055375.0, 354625.0 ], [ 2055125.0, 354625.0 ], [ 2054875.0, 354625.0 ], [ 2054875.0, 354875.0 ], [ 2054625.0, 354875.0 ], [ 2054375.0, 354875.0 ], [ 2054125.0, 354875.0 ], [ 2053875.0, 354875.0 ], [ 2053875.0, 355125.0 ], [ 2053625.0, 355125.0 ], [ 2053375.0, 355125.0 ], [ 2053125.0, 355125.0 ], [ 2052875.0, 355125.0 ], [ 2052875.0, 355487.659560176252853 ], [ 2052897.161504379939288, 355473.870179673016537 ], [ 2053125.0, 355467.06903055019211 ], [ 2053375.0, 355459.606343983206898 ], [ 2053524.165453359950334, 355455.153643883008044 ], [ 2053625.0, 355458.923159645171836 ], [ 2053875.0, 355468.268954037455842 ], [ 2054125.0, 355477.614748429739848 ], [ 2054375.0, 355486.960542822023854 ], [ 2054525.500118149910122, 355492.58671546302503 ], [ 2054625.0, 355499.996281132742297 ] ], [ [ 2056125.0, 355375.0 ], [ 2056125.0, 355125.0 ], [ 2056375.0, 355125.0 ], [ 2056375.0, 355375.0 ], [ 2056125.0, 355375.0 ] ], [ [ 2083875.0, 368625.0 ], [ 2083875.0, 368375.0 ], [ 2084125.0, 368375.0 ], [ 2084125.0, 368625.0 ], [ 2083875.0, 368625.0 ] ], [ [ 2076375.0, 367625.0 ], [ 2076375.0, 367875.0 ], [ 2076375.0, 368125.0 ], [ 2076125.0, 368125.0 ], [ 2076125.0, 367875.0 ], [ 2076125.0, 367625.0 ], [ 2076125.0, 367375.0 ], [ 2076375.0, 367375.0 ], [ 2076375.0, 367625.0 ] ], [ [ 2073875.0, 366125.0 ], [ 2074125.0, 366125.0 ], [ 2074125.0, 366375.0 ], [ 2074125.0, 366625.0 ], [ 2073875.0, 366625.0 ], [ 2073625.0, 366625.0 ], [ 2073625.0, 366375.0 ], [ 2073625.0, 366125.0 ], [ 2073875.0, 366125.0 ] ], [ [ 2071125.0, 364875.0 ], [ 2070875.0, 364875.0 ], [ 2070875.0, 364625.0 ], [ 2070875.0, 364375.0 ], [ 2070625.0, 364375.0 ], [ 2070375.0, 364375.0 ], [ 2070125.0, 364375.0 ], [ 2070125.0, 364125.0 ], [ 2069875.0, 364125.0 ], [ 2069625.0, 364125.0 ], [ 2069625.0, 363875.0 ], [ 2069375.0, 363875.0 ], [ 2069125.0, 363875.0 ], [ 2068875.0, 363875.0 ], [ 2068875.0, 363625.0 ], [ 2068625.0, 363625.0 ], [ 2068625.0, 363375.0 ], [ 2068875.0, 363375.0 ], [ 2069125.0, 363375.0 ], [ 2069375.0, 363375.0 ], [ 2069625.0, 363375.0 ], [ 2069625.0, 363625.0 ], [ 2069875.0, 363625.0 ], [ 2069875.0, 363875.0 ], [ 2070125.0, 363875.0 ], [ 2070125.0, 363625.0 ], [ 2070375.0, 363625.0 ], [ 2070375.0, 363875.0 ], [ 2070625.0, 363875.0 ], [ 2070625.0, 363625.0 ], [ 2070875.0, 363625.0 ], [ 2071125.0, 363625.0 ], [ 2071125.0, 363875.0 ], [ 2071375.0, 363875.0 ], [ 2071375.0, 363625.0 ], [ 2071375.0, 363375.0 ], [ 2071625.0, 363375.0 ], [ 2071625.0, 363625.0 ], [ 2071875.0, 363625.0 ], [ 2071875.0, 363875.0 ], [ 2071875.0, 364125.0 ], [ 2072125.0, 364125.0 ], [ 2072125.0, 364375.0 ], [ 2072125.0, 364625.0 ], [ 2072125.0, 364875.0 ], [ 2071875.0, 364875.0 ], [ 2071875.0, 364625.0 ], [ 2071625.0, 364625.0 ], [ 2071375.0, 364625.0 ], [ 2071125.0, 364625.0 ], [ 2071125.0, 364875.0 ] ], [ [ 2057375.0, 357625.0 ], [ 2057250.0, 357750.0 ], [ 2057125.0, 357625.0 ], [ 2057125.0, 357375.0 ], [ 2057375.0, 357375.0 ], [ 2057375.0, 357625.0 ] ], [ [ 2073625.0, 368125.0 ], [ 2073625.0, 368375.0 ], [ 2073375.0, 368375.0 ], [ 2073375.0, 368125.0 ], [ 2073125.0, 368125.0 ], [ 2073125.0, 367875.0 ], [ 2073125.0, 367625.0 ], [ 2073125.0, 367375.0 ], [ 2073375.0, 367375.0 ], [ 2073625.0, 367375.0 ], [ 2073875.0, 367375.0 ], [ 2073875.0, 367625.0 ], [ 2073625.0, 367625.0 ], [ 2073625.0, 367875.0 ], [ 2073625.0, 368125.0 ] ], [ [ 2059375.0, 355875.0 ], [ 2059375.0, 355625.0 ], [ 2059625.0, 355625.0 ], [ 2059625.0, 355875.0 ], [ 2059375.0, 355875.0 ] ], [ [ 2059375.0, 356625.0 ], [ 2059125.0, 356625.0 ], [ 2059125.0, 356375.0 ], [ 2059125.0, 356125.0 ], [ 2059375.0, 356125.0 ], [ 2059375.0, 356375.0 ], [ 2059375.0, 356625.0 ] ], [ [ 2059625.0, 356625.0 ], [ 2059625.0, 356875.0 ], [ 2059375.0, 356875.0 ], [ 2059375.0, 356625.0 ], [ 2059625.0, 356625.0 ] ], [ [ 2068125.0, 364125.0 ], [ 2068125.0, 364375.0 ], [ 2067875.0, 364375.0 ], [ 2067875.0, 364125.0 ], [ 2068125.0, 364125.0 ] ], [ [ 2068125.0, 363625.0 ], [ 2068125.0, 363875.0 ], [ 2067875.0, 363875.0 ], [ 2067875.0, 363625.0 ], [ 2068125.0, 363625.0 ] ], [ [ 2068375.0, 363375.0 ], [ 2068125.0, 363375.0 ], [ 2068125.0, 363125.0 ], [ 2068375.0, 363125.0 ], [ 2068375.0, 363375.0 ] ], [ [ 2068125.0, 362125.0 ], [ 2068375.0, 362125.0 ], [ 2068625.0, 362125.0 ], [ 2068875.0, 362125.0 ], [ 2069125.0, 362125.0 ], [ 2069125.0, 362375.0 ], [ 2068875.0, 362375.0 ], [ 2068625.0, 362375.0 ], [ 2068375.0, 362375.0 ], [ 2068125.0, 362375.0 ], [ 2068125.0, 362125.0 ] ], [ [ 2066125.0, 363875.0 ], [ 2065875.0, 363875.0 ], [ 2065875.0, 363625.0 ], [ 2066125.0, 363625.0 ], [ 2066125.0, 363875.0 ] ], [ [ 2066875.0, 364125.0 ], [ 2066625.0, 364125.0 ], [ 2066625.0, 363875.0 ], [ 2066875.0, 363875.0 ], [ 2066875.0, 364125.0 ] ], [ [ 2066375.0, 363375.0 ], [ 2066125.0, 363375.0 ], [ 2066125.0, 363125.0 ], [ 2066375.0, 363125.0 ], [ 2066625.0, 363125.0 ], [ 2066625.0, 363375.0 ], [ 2066375.0, 363375.0 ] ], [ [ 2067125.0, 363125.0 ], [ 2067375.0, 363125.0 ], [ 2067625.0, 363125.0 ], [ 2067875.0, 363125.0 ], [ 2067875.0, 363375.0 ], [ 2067625.0, 363375.0 ], [ 2067375.0, 363375.0 ], [ 2067125.0, 363375.0 ], [ 2067125.0, 363125.0 ] ], [ [ 2071125.0, 366375.0 ], [ 2070875.0, 366375.0 ], [ 2070875.0, 366125.0 ], [ 2071125.0, 366125.0 ], [ 2071125.0, 366375.0 ] ], [ [ 2071125.0, 365875.0 ], [ 2071375.0, 365875.0 ], [ 2071375.0, 366125.0 ], [ 2071125.0, 366125.0 ], [ 2071125.0, 365875.0 ] ], [ [ 2070875.0, 365375.0 ], [ 2070625.0, 365375.0 ], [ 2070625.0, 365125.0 ], [ 2070875.0, 365125.0 ], [ 2070875.0, 365375.0 ] ], [ [ 2070625.0, 362625.0 ], [ 2070875.0, 362625.0 ], [ 2070875.0, 362875.0 ], [ 2070625.0, 362875.0 ], [ 2070625.0, 363125.0 ], [ 2070625.0, 363375.0 ], [ 2070375.0, 363375.0 ], [ 2070375.0, 363125.0 ], [ 2070125.0, 363125.0 ], [ 2070125.0, 362875.0 ], [ 2070375.0, 362875.0 ], [ 2070375.0, 362625.0 ], [ 2070375.0, 362375.0 ], [ 2070625.0, 362375.0 ], [ 2070625.0, 362625.0 ] ], [ [ 2069625.0, 361375.0 ], [ 2069875.0, 361375.0 ], [ 2070125.0, 361375.0 ], [ 2070125.0, 361625.0 ], [ 2070125.0, 361875.0 ], [ 2069875.0, 361875.0 ], [ 2069875.0, 362125.0 ], [ 2069625.0, 362125.0 ], [ 2069625.0, 361875.0 ], [ 2069375.0, 361875.0 ], [ 2069375.0, 361625.0 ], [ 2069375.0, 361375.0 ], [ 2069625.0, 361375.0 ] ], [ [ 2069125.0, 364375.0 ], [ 2069375.0, 364375.0 ], [ 2069375.0, 364625.0 ], [ 2069375.0, 364875.0 ], [ 2069125.0, 364875.0 ], [ 2069125.0, 364625.0 ], [ 2068875.0, 364625.0 ], [ 2068875.0, 364875.0 ], [ 2068625.0, 364875.0 ], [ 2068625.0, 364625.0 ], [ 2068625.0, 364375.0 ], [ 2068875.0, 364375.0 ], [ 2069125.0, 364375.0 ] ], [ [ 2071625.0, 366625.0 ], [ 2071625.0, 366875.0 ], [ 2071375.0, 366875.0 ], [ 2071375.0, 366625.0 ], [ 2071625.0, 366625.0 ] ], [ [ 2072125.0, 365875.0 ], [ 2072125.0, 365625.0 ], [ 2072375.0, 365625.0 ], [ 2072375.0, 365875.0 ], [ 2072125.0, 365875.0 ] ], [ [ 2075375.0, 367625.0 ], [ 2075375.0, 367375.0 ], [ 2075625.0, 367375.0 ], [ 2075625.0, 367625.0 ], [ 2075375.0, 367625.0 ] ], [ [ 2075125.0, 367375.0 ], [ 2075125.0, 367125.0 ], [ 2074875.0, 367125.0 ], [ 2074875.0, 366875.0 ], [ 2075125.0, 366875.0 ], [ 2075375.0, 366875.0 ], [ 2075625.0, 366875.0 ], [ 2075625.0, 367125.0 ], [ 2075375.0, 367125.0 ], [ 2075375.0, 367375.0 ], [ 2075125.0, 367375.0 ] ], [ [ 2075875.0, 367625.0 ], [ 2075875.0, 367875.0 ], [ 2075625.0, 367875.0 ], [ 2075625.0, 367625.0 ], [ 2075875.0, 367625.0 ] ], [ [ 2074125.0, 368375.0 ], [ 2074125.0, 368125.0 ], [ 2074375.0, 368125.0 ], [ 2074375.0, 368375.0 ], [ 2074125.0, 368375.0 ] ], [ [ 2076875.0, 368625.0 ], [ 2076875.0, 368375.0 ], [ 2077125.0, 368375.0 ], [ 2077125.0, 368625.0 ], [ 2076875.0, 368625.0 ] ], [ [ 2076375.0, 369375.0 ], [ 2076375.0, 369125.0 ], [ 2076375.0, 368875.0 ], [ 2076625.0, 368875.0 ], [ 2076625.0, 369125.0 ], [ 2076625.0, 369375.0 ], [ 2076375.0, 369375.0 ] ], [ [ 2079125.0, 368625.0 ], [ 2079125.0, 368875.0 ], [ 2078875.0, 368875.0 ], [ 2078875.0, 368625.0 ], [ 2078875.0, 368375.0 ], [ 2079125.0, 368375.0 ], [ 2079125.0, 368625.0 ] ], [ [ 2083625.0, 368875.0 ], [ 2083375.0, 368875.0 ], [ 2083125.0, 368875.0 ], [ 2083125.0, 369125.0 ], [ 2083125.0, 369375.0 ], [ 2082875.0, 369375.0 ], [ 2082875.0, 369125.0 ], [ 2082625.0, 369125.0 ], [ 2082625.0, 368875.0 ], [ 2082625.0, 368625.0 ], [ 2082375.0, 368625.0 ], [ 2082125.0, 368625.0 ], [ 2082125.0, 368375.0 ], [ 2082125.0, 368125.0 ], [ 2082125.0, 367875.0 ], [ 2082375.0, 367875.0 ], [ 2082375.0, 368125.0 ], [ 2082625.0, 368125.0 ], [ 2082625.0, 367875.0 ], [ 2082875.0, 367875.0 ], [ 2083125.0, 367875.0 ], [ 2083125.0, 368125.0 ], [ 2083125.0, 368375.0 ], [ 2083125.0, 368625.0 ], [ 2083375.0, 368625.0 ], [ 2083625.0, 368625.0 ], [ 2083625.0, 368875.0 ] ], [ [ 2082625.0, 366625.0 ], [ 2082875.0, 366625.0 ], [ 2082875.0, 366875.0 ], [ 2082625.0, 366875.0 ], [ 2082625.0, 366625.0 ] ], [ [ 2081125.0, 369625.0 ], [ 2081125.0, 369875.0 ], [ 2080875.0, 369875.0 ], [ 2080875.0, 369625.0 ], [ 2080875.0, 369375.0 ], [ 2081125.0, 369375.0 ], [ 2081125.0, 369625.0 ] ] ] ] } }
]
}

==================================================

File: C:\GH\ras-commander\examples\README.md
==================================================
# RAS Commander Examples

This directory contains example notebooks demonstrating how to use the `ras-commander` library for automating HEC-RAS operations. These examples cover basic to advanced usage scenarios and provide a practical guide for hydraulic modelers looking to automate their workflows.

## Overview

HEC-RAS (Hydrologic Engineering Center's River Analysis System) is widely used for hydraulic modeling. The `ras-commander` library provides a Python interface to automate HEC-RAS operations without using the graphical user interface. This enables batch processing, sensitivity analysis, and integration with other Python tools for water resources engineering.

These example notebooks are designed to:
- Demonstrate key functionalities of the `ras-commander` library
- Provide practical use cases for automation
- Guide users from basic to advanced operations
- Serve as templates for your own automation scripts

## Examples

### [00_Using_RasExamples.ipynb](00_Using_RasExamples.ipynb)

This notebook introduces the `RasExamples` class, which provides easy access to HEC-RAS example projects for testing and demonstration purposes.

**Key contents:**
- Installing `ras-commander` from pip
- Using flexible imports for development without installation
- Extracting specific HEC-RAS example projects by folder name
- Advanced usage options for managing example projects
- Listing available example projects and categories
- Working with the new pipes and conduits examples (version 6.6)

### [01_project_initialization.ipynb](01_project_initialization.ipynb)

This notebook covers initializing and working with HEC-RAS projects using the `ras-commander` library.

**Key contents:**
- Setting up and configuring the RAS Commander environment
- Downloading and extracting example HEC-RAS projects
- Initializing HEC-RAS projects using the global `ras` object
- Initializing multiple HEC-RAS projects using custom RAS objects
- Accessing various project components (plans, geometries, flows, boundaries)
- Understanding the RAS object structure and its components
- Working with boundary conditions
- Comparing multiple projects

### [02_plan_and_geometry_operations.ipynb](02_plan_and_geometry_operations.ipynb)

This notebook demonstrates operations on HEC-RAS plan and geometry files using the RAS Commander library.

**Key contents:**
- Project initialization and understanding plan/geometry files
- Cloning plans to create new simulation scenarios
- Cloning geometry files for modified versions
- Setting geometry files for plans
- Clearing geometry preprocessor files
- Configuring simulation parameters and intervals
- Setting run flags and updating descriptions
- Cloning and configuring unsteady flow files
- Computing plans and verifying results
- Working with advanced HDF data
- Best practices for plan and geometry operations

### [03_unsteady_flow_operations.ipynb](03_unsteady_flow_operations.ipynb)

This notebook demonstrates operations on unsteady flow files using the RAS Commander library.

**Key contents:**
- Understanding unsteady flow files in HEC-RAS
- Extracting boundary conditions and tables from unsteady flow files
- Inspecting and analyzing boundary condition structures
- Working with different boundary condition types (flow hydrographs, stage hydrographs, etc.)
- Modifying flow titles in unsteady flow files
- Configuring restart settings for continuing simulations
- Extracting and working with flow tables
- Modifying flow tables and writing them back to files
- Applying updated unsteady flow to a plan and computing results

### [04_multiple_project_operations.ipynb](04_multiple_project_operations.ipynb)

This notebook demonstrates how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library.

**Key contents:**
- Initializing and managing multiple HEC-RAS projects
- Cloning and modifying plans across different projects
- Running computations for multiple projects in parallel
- Optimizing computing resources when working with multiple models
- Analyzing and comparing results from different projects
- Building comprehensive multi-project workflows
- Best practices for multiple project management
- Setting up compute folders for multiple projects
- Comparing project structures and results

### [05_single_plan_execution.ipynb](05_single_plan_execution.ipynb)

This notebook focuses specifically on executing a single HEC-RAS plan with various configuration options.

**Key contents:**
- Understanding the `RasCmdr.compute_plan` method and its parameters
- Executing a plan with a specified number of processor cores
- Creating and managing destination folders for computations
- Overwriting existing destination folders
- Verifying computation results
- Options for single plan execution (basic execution, destination folder, number of cores, etc.)
- Best practices for single plan execution

### [06_executing_plan_sets.ipynb](06_executing_plan_sets.ipynb)

This notebook demonstrates different ways to specify and execute HEC-RAS plans using the RAS Commander library.

**Key contents:**
- Understanding plan specification in HEC-RAS
- Sequential execution of specific plans
- Selective plan execution based on criteria
- Running only plans without HDF results
- Verifying execution results
- Best practices for plan specification
- Choosing appropriate execution methods based on scenario
- Understanding the importance of plan selection for efficiency

### [07_sequential_plan_execution.ipynb](07_sequential_plan_execution.ipynb)

This notebook demonstrates how to sequentially execute multiple HEC-RAS plans using the RAS Commander library.

**Key contents:**
- Understanding sequential execution in HEC-RAS
- Using the `RasCmdr.compute_test_mode` method
- Executing all plans in a project sequentially
- Analyzing the test folder after sequential execution
- Executing specific plans with geometry preprocessor clearing
- Best practices for sequential execution
- Environment setup and test folder management
- Benefits of sequential execution (controlled resource usage, dependency management, etc.)

### [08_parallel_execution.ipynb](08_parallel_execution.ipynb)

This notebook demonstrates how to execute multiple HEC-RAS plans in parallel to maximize computational efficiency.

**Key contents:**
- Understanding parallel execution in HEC-RAS
- Setting up a working environment for parallel execution
- Checking system resources for optimal parallel execution
- Executing all plans in a project in parallel
- Executing specific plans in parallel
- Dynamic worker allocation based on available resources
- Balancing workers and cores per worker
- Analyzing parallel execution results
- Performance comparison between different parallel configurations
- Best practices for parallel execution

### [09_plan_parameter_operations.ipynb](09_plan_parameter_operations.ipynb)

This notebook demonstrates how to perform key operations on HEC-RAS plan files, focusing on modifying simulation parameters.

**Key contents:**
- Understanding plan files in HEC-RAS
- Retrieving specific values from plan files
- Updating run flags to control which components will run
- Modifying computation and output time intervals
- Reading and updating plan descriptions
- Changing simulation start and end dates
- Verifying updated plan values
- Best practices for plan operations
- Automating parameter adjustments for sensitivity analysis
- Managing documentation through plan descriptions

### [10_1d_hdf_data_extraction.ipynb](10_1d_hdf_data_extraction.ipynb)

This notebook demonstrates how to extract and analyze 1D data from HEC-RAS HDF files using the RAS Commander library.

**Key contents:**
- Accessing and extracting base geometry attributes from HDF files
- Working with 1D cross-section data, including station-elevation profiles
- Visualizing cross-section properties like Manning's n values
- Extracting river centerlines, bank lines, and edge lines
- Analyzing runtime data and compute messages
- Processing and visualizing ineffective flow areas
- Extracting time series data for 1D cross sections
- Plotting cross-section elevation profiles with bank stations

### [11_2d_hdf_data_extraction.ipynb](11_2d_hdf_data_extraction.ipynb)

This notebook shows how to extract and analyze 2D data from HEC-RAS HDF files using the RAS Commander library.

**Key contents:**
- Working with 2D flow area attributes and perimeter polygons
- Extracting and visualizing mesh cell faces, polygons, and points
- Finding nearest faces and cells to specific points
- Extracting boundary condition lines and breaklines
- Analyzing maximum water surface elevations and timing
- Working with maximum face velocities and water surface errors
- Visualizing 2D model results with terrain data
- Extracting and interpreting cell and face time series data

### [12_2d_hdf_data_extraction_pipes_and_pumps.ipynb](12_2d_hdf_data_extraction_pipes_and_pumps.ipynb)

This notebook focuses on extracting and analyzing data related to pipes, conduits, and pump stations from HEC-RAS HDF files.

**Key contents:**
- Working with pipe conduits and associated geometries
- Extracting pipe node information and properties
- Analyzing pipe network connectivity and structures
- Visualizing pipe networks with node elevations
- Working with pump stations and pump groups
- Extracting pipe and node time series data
- Analyzing face flow, velocity, and water surface values
- Processing and visualizing pump station operation data

### [13_2d_detail_face_data_extraction.ipynb](13_2d_detail_face_data_extraction.ipynb)

This notebook demonstrates techniques for detailed face data extraction from 2D HEC-RAS models.

**Key contents:**
- Extracting and analyzing detailed face property tables
- Working with profile lines to identify cell faces
- Finding faces perpendicular to flow for discharge calculations
- Converting face velocities and flows to positive values
- Calculating discharge-weighted velocities for profile lines
- Comparing discharge-weighted and simple average velocities
- Visualizing time series data for selected faces
- Creating profile-specific result datasets for analysis

### [14_fluvial_pluvial_delineation.ipynb](14_fluvial_pluvial_delineation.ipynb)

This notebook demonstrates how to delineate fluvial and pluvial flooding areas based on the timing of maximum water surface elevations.

**Key contents:**
- Extracting maximum water surface elevation and timing data
- Identifying adjacent cells with dissimilar flood timing
- Calculating boundaries between fluvial and pluvial flooding
- Filtering boundaries based on length thresholds
- Visualizing the fluvial-pluvial boundary on a map
- Exporting boundaries to GeoJSON format
- Understanding the difference between river-driven and rainfall-driven flooding
- Using cell polygon geometry for spatial analysis

### [15_mannings_sensitivity_bulk_analysis.ipynb](15_mannings_sensitivity_bulk_analysis.ipynb)

This notebook provides tools for analyzing the sensitivity of HEC-RAS models to changes in Manning's n values applied *in bulk* across land cover types based on literature ranges.

**Key contents:**
- Defining Manning's n ranges (min/max) for various land cover types.
- Automating the creation of scenarios (min, max, current n values).
- Applying bulk changes to base and/or regional Manning's overrides.
- Running sensitivity scenarios in parallel.
- Extracting results at a point of interest.
- Comparing and visualizing the impact of bulk Manning's n changes on water surface elevation.

### [16_mannings_sensitivity_multi-interval.ipynb](16_mannings_sensitivity_multi-interval.ipynb)

This notebook performs a more detailed Manning's n sensitivity analysis by varying the roughness coefficient for *individual significant land uses* across a range of values.

**Key contents:**
- Analyzing land cover statistics within 2D mesh areas.
- Identifying significant land cover types based on area threshold.
- Generating multiple test plans by varying the n value for one land cover type at a time, across its literature-based range (min to max).
- Applying changes individually to base or regional overrides.
- Running sensitivity scenarios in parallel.
- Extracting and visualizing results to show sensitivity to specific land cover roughness.
- Estimating the number of plans to be generated and managing potential HEC-RAS limits.

### [101_Core_Sensitivity.ipynb](101_Core_Sensitivity.ipynb)

This notebook tests HEC-RAS performance with different CPU core configurations to optimize computational efficiency.

**Key contents:**
- Setting up a controlled testing environment
- Running the same plan with varying core counts
- Measuring execution time for each configuration
- Analyzing performance scaling with increased cores
- Creating visualization of performance metrics
- Calculating unit runtime based on single-core performance
- Understanding diminishing returns with multiple cores
- Identifying optimal core count for specific models

### [102_benchmarking_versions_6.1_to_6.6.ipynb](102_benchmarking_versions_6.1_to_6.6.ipynb)

This notebook compares performance across different versions of HEC-RAS by running the same plan across multiple software versions.

**Key contents:**
- Running the same model across multiple HEC-RAS versions
- Measuring preprocessing, computation, and postprocessing times
- Analyzing volume error changes between versions
- Creating visualizations of performance trends
- Identifying performance improvements between versions
- Understanding version-specific computational differences
- Setting up flexible testing environments for multiple versions
- Interpreting HEC-RAS version performance evolution

### [103_Generating_AEP_Events_from_Atlas_14.ipynb](103_Generating_AEP_Events_from_Atlas_14.ipynb)

This notebook demonstrates an end-to-end workflow for generating and analyzing multiple Annual Exceedance Probability events.

**Key contents:**
- Generating hyetographs from NOAA Atlas 14 precipitation frequency data
- Parsing duration strings and interpolating precipitation depths
- Applying the Alternating Block Method for hyetograph creation
- Cloning and configuring HEC-RAS plans for different AEP events
- Executing multiple plans in parallel with resource optimization
- Extracting and visualizing results for multiple AEP scenarios
- Creating a complete workflow from data to flood analysis
- Comparing results across different return period events

### [17_extracting_profiles_with_hecrascontroller and RasControl.ipynb](17_extracting_profiles_with_hecrascontroller%20and%20RasControl.ipynb)

This notebook demonstrates the **RasControl** class for working with legacy HEC-RAS versions (3.x-4.x) using the HECRASController COM interface.

**Key contents:**
- Introduction to RasControl wrapper for HECRASController
- Using ras-commander style API with plan numbers instead of file paths
- Running steady state plans and extracting profile results
- Running unsteady plans and extracting time series results
- Understanding output times and the "Max WS" special timestep
- Extracting and plotting steady state profiles across multiple profiles
- Visualizing unsteady time series at cross sections
- Supported versions: 3.1, 4.1, 5.0.x (501-507), 6.0, 6.3, 6.6
- Multi-version comparison for migration validation
- Integration with `init_ras_project()` and the global `ras` object
- Open-operate-close pattern to prevent conflicts with modern workflows


## Contributing

If you have suggestions for additional examples or improvements to existing ones, please feel free to contribute by submitting pull requests or opening issues in the repository.
==================================================

File: C:\GH\ras-commander\examples\RemoteWorkers.json
==================================================
{
  "workers": [

    {
      "name": "CLB-04",
      "worker_type": "psexec",
      "hostname": "192.168.3.8",
      "share_path": "\\\\192.168.3.8\\RasRemote",
      "worker_folder": "C:\\RasRemote",
      "username": ".\\bill",
      "password": "Katzen84!!",
      "session_id": 2,
      "process_priority": "low",
      "queue_priority": 1,
      "cores_total": 4,
      "cores_per_plan": 2,
      "enabled": true
    },
    {
      "name": "Local Compute",
      "worker_type": "local",
      "worker_folder": "C:\\RasRemote",
      "process_priority": "low",
      "queue_priority": 0,
      "cores_total": 4,
      "cores_per_plan": 2,
      "enabled": true
    },
    {
      "name": "CLB-05",
      "worker_type": "psexec",
      "hostname": "192.168.3.24",
      "share_path": "\\\\192.168.3.24\\RasRemote",
      "worker_folder": "C:\\RasRemote",
      "username": ".\\bill",
      "password": "Katzen84!!",
      "session_id": 2,
      "process_priority": "low",
      "queue_priority": 1,
      "cores_total": 4,
      "cores_per_plan": 2,
      "enabled": true
    },
    {
      "name": "CLB-03-VM1",
      "worker_type": "psexec",
      "hostname": "192.168.3.21",
      "share_path": "\\\\192.168.3.21\\RasRemote",
      "worker_folder": "C:\\RasRemote",
      "username": ".\\bill",
      "password": "CLBBill11!!",
      "session_id": 2,
      "process_priority": "low",
      "queue_priority": 2,
      "cores_total": 2,
      "cores_per_plan": 2,
      "enabled": true
    }
  ]
}

==================================================

File: C:\GH\ras-commander\examples\RemoteWorkers.json.template
==================================================
{
  "workers": [
    {
      "name": "Local Compute",
      "hostname": "localhost",
      "share_path": "C:\\Temp\\RasRemote",
      "username": "local_user",
      "password": "local_password",
      "ras_exe_path": "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\RAS.exe",
      "session_id": 2,
      "process_priority": "low",
      "queue_priority": 0,
      "cores_total": 4,
      "cores_per_plan": 2,
      "enabled": true
    },
    {
      "name": "Primary Workstation",
      "hostname": "192.168.1.100",
      "share_path": "\\\\192.168.1.100\\RasRemote",
      "username": "your_username",
      "password": "your_password",
      "ras_exe_path": "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\RAS.exe",
      "session_id": 2,
      "process_priority": "low",
      "queue_priority": 1,
      "cores_total": 16,
      "cores_per_plan": 4,
      "enabled": true
    },
    {
      "name": "Secondary Workstation",
      "hostname": "192.168.1.101",
      "share_path": "\\\\192.168.1.101\\RasRemote",
      "username": "your_username",
      "password": "your_password",
      "ras_exe_path": "C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe",
      "session_id": 2,
      "process_priority": "low",
      "queue_priority": 1,
      "cores_total": 8,
      "cores_per_plan": 4,
      "enabled": false
    }
  ]
}

==================================================

File: C:\GH\ras-commander\examples\REMOTE_WORKERS_README.md
==================================================
# RemoteWorkers.json Configuration Guide

**Purpose:** Secure credential storage for remote HEC-RAS execution workers

**Security:** This file is in `.gitignore` and will NOT be committed to the repository.

---

## Quick Setup

### 1. Copy the template:
```bash
copy RemoteWorkers.json.template RemoteWorkers.json
```

### 2. Edit `RemoteWorkers.json` with your worker details

### 3. Run the notebook - credentials load automatically!

---

## JSON File Format

```json
{
  "workers": [
    {
      "name": "Descriptive Name",
      "hostname": "IP_or_hostname",
      "share_path": "\\\\hostname\\ShareName",
      "username": "your_username",
      "password": "your_password",
      "ras_exe_path": "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\RAS.exe",
      "session_id": 2,
      "priority": "low",
      "enabled": true
    }
  ]
}
```

---

## Field Descriptions

| Field | Description | Example | Required |
|-------|-------------|---------|----------|
| `name` | Friendly name for the worker | `"Office Workstation"` | Yes |
| `hostname` | IP address or machine name | `"192.168.1.100"` or `"WORKSTATION-01"` | Yes |
| `share_path` | UNC path to network share | `"\\\\192.168.1.100\\RasRemote"` | Yes |
| `username` | Windows username | `"bill"` or `"DOMAIN\\user"` | Yes |
| `password` | Windows password | `"SecurePass123"` | Yes |
| `ras_exe_path` | Full path to RAS.exe on remote machine | `"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\RAS.exe"` | Yes |
| `session_id` | User session ID (use `query user` to find) | `2` | Yes |
| `priority` | Process priority | `"low"`, `"below normal"`, or `"normal"` | No (default: `"low"`) |
| `enabled` | Whether to use this worker | `true` or `false` | No (default: `true`) |

---

## Finding Session ID

**On the remote machine**, run:
```cmd
query user
```

Output example:
```
USERNAME    SESSIONNAME    ID  STATE
bill        console         2  Active
```

Use the **ID** value (2 in this example) for `session_id`.

**Typical values:**
- Session 2: Most common for single-user workstations
- Session 1: Older Windows or first console session
- Session 3+: Additional RDP sessions

---

## Multiple Workers Example

```json
{
  "workers": [
    {
      "name": "Office PC 1",
      "hostname": "192.168.1.10",
      "share_path": "\\\\192.168.1.10\\RasRemote",
      "username": "bill",
      "password": "pass1",
      "ras_exe_path": "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\RAS.exe",
      "session_id": 2,
      "enabled": true
    },
    {
      "name": "Office PC 2",
      "hostname": "192.168.1.11",
      "share_path": "\\\\192.168.1.11\\RasRemote",
      "username": "user2",
      "password": "pass2",
      "ras_exe_path": "C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe",
      "session_id": 2,
      "enabled": true
    },
    {
      "name": "Backup PC (disabled)",
      "hostname": "192.168.1.12",
      "share_path": "\\\\192.168.1.12\\RasRemote",
      "username": "user3",
      "password": "pass3",
      "ras_exe_path": "C:\\Program Files\\HEC\\HEC-RAS\\6.6\\RAS.exe",
      "session_id": 2,
      "enabled": false
    }
  ]
}
```

The notebook will use all workers where `enabled: true`.

---

## Security Best Practices

### ✅ DO:
- Keep `RemoteWorkers.json` local only (it's in `.gitignore`)
- Use strong passwords
- Rotate passwords regularly
- Limit file permissions (Windows: Right-click → Properties → Security)
- Use VPN when accessing remote office networks

### ❌ DON'T:
- Don't commit `RemoteWorkers.json` to git
- Don't share the file publicly
- Don't use weak passwords
- Don't email the file (credentials in plain text)
- Don't store in cloud storage (Dropbox, OneDrive, etc.)

---

## Path Format Notes

### Windows Paths - Use Double Backslashes:
```json
"share_path": "\\\\192.168.1.100\\RasRemote",
"ras_exe_path": "C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\RAS.exe"
```

**Why double backslashes?**
- JSON requires escaping backslashes
- `\\` in JSON becomes `\` in Python
- `\\\\hostname` in JSON becomes `\\hostname` (UNC path) in Python

---

## Troubleshooting

### "RemoteWorkers.json not found"
**Solution:** Copy the template:
```cmd
copy RemoteWorkers.json.template RemoteWorkers.json
```

### JSON Syntax Errors
**Common issues:**
- Missing commas between objects
- Trailing comma after last object
- Unescaped backslashes (use `\\` not `\`)
- Smart quotes instead of straight quotes

**Validate JSON:** Use https://jsonlint.com/ or VS Code's JSON validator

### Wrong Session ID
**Symptoms:** PsExec hangs or timeout

**Solution:**
1. On remote machine: `query user`
2. Find the ID column value
3. Update `session_id` in JSON
4. Restart notebook kernel and reload

---

## Example: Converting from Hardcoded to JSON

**Old notebook code (hardcoded):**
```python
REMOTE_CONFIG = {
    "hostname": "192.168.3.8",
    "password": "your_password_here",  # Credentials in notebook!
    # ...
}
```

**New notebook code (JSON):**
```python
import json
with open("RemoteWorkers.json") as f:
    worker_configs = json.load(f)
REMOTE_CONFIG = worker_configs["workers"][0]
# Credentials loaded securely from external file
```

---

## Advanced: Environment Variables

For even more security, use environment variables:

**Set environment variable (PowerShell):**
```powershell
$env:RAS_REMOTE_PASSWORD="YourPassword"
```

**Load in notebook:**
```python
import os
REMOTE_CONFIG["password"] = os.environ.get("RAS_REMOTE_PASSWORD", "")
```

---

## Worker Management

### Temporarily Disable a Worker:
```json
{
  "name": "Slow PC",
  "enabled": false,  # Set to false to skip
  // ...
}
```

### Add New Worker:
Add a new object to the `workers` array with all required fields.

### Remove Worker:
Delete the object from the `workers` array or set `enabled: false`.

---

## File Location

**Where to put RemoteWorkers.json:**
- Same directory as the notebook (`examples/`)
- The notebook looks for it in the current working directory
- Use absolute path if needed: `Path("/path/to/RemoteWorkers.json")`

---

**Template file:** `RemoteWorkers.json.template` (safe to commit)
**Your file:** `RemoteWorkers.json` (in .gitignore, never committed)

**Status:** ✅ Secure credential management implemented!

==================================================

File: C:\GH\ras-commander\ras_commander\AGENTS.md
==================================================
**Scope**
- Guidance for agents working inside `ras_commander/` (the core library). Inherits root policies; adds coding and API usage specifics.

**Module Layout (key classes)**
- `RasPrj` (`RasPrj.py`): Initialize and manage a RAS project. Exposes dataframes: `plan_df`, `geom_df`, `flow_df`, `unsteady_df`, `boundaries_df`. Helpers: `init_ras_project()`, `get_ras_exe()`.
- `RasPlan` (`RasPlan.py`): Clone/retarget plans, update intervals, cores, run flags, titles/descriptions, geometry/unsteady bindings.
- `RasCmdr` (`RasCmdr.py`): Execute plans (single/sequential/parallel). `compute_plan()`, `compute_parallel()`, `compute_test_mode()`.
- `RasControl` (`RasControl.py`): Legacy HEC-RAS support (3.x-4.x) via COM interface. ras-commander style API with plan numbers. `run_plan()`, `get_steady_results()`, `get_unsteady_results()`, `get_output_times()`, `set_current_plan()`.
- `RasMap` (`RasMap.py`): Parse `.rasmap`, post-process stored maps.
- `Hdf*` modules: Geometry and results accessors.
  - `HdfBase`, `HdfUtils`: shared helpers.
  - `HdfMesh`, `HdfBndry`, `HdfXsec`, `HdfStruc`, `HdfPlan`.
  - `HdfResultsMesh`, `HdfResultsPlan`, `HdfResultsXsec`, `HdfResultsPlot`.
  - `HdfPipe`, `HdfPump`, `HdfInfiltration`, `HdfFluvialPluvial`.
  - `HdfPlot`: convenience plotting helpers.

**Conventions**
- Static namespaces: Many classes expose only `@staticmethod`s. Do not instantiate unless design requires state.
- Plan numbers: Use two digits (e.g., "01"). Helpers accept both strings and paths.
- Logging: `from ras_commander import get_logger, log_call`; then `logger = get_logger(__name__)`; decorate public methods with `@log_call`.
- Imports: stdlib → third‑party → local; keep ≤79 chars where practical.

**Input Normalization (standardize_input)**
- Most HDF-facing functions are decorated with `@standardize_input(file_type=...)` and accept:
  - An `h5py.File`, a `Path`, a string path, or a plan/geom number (e.g., "01", "p01").
  - A `ras_object` kwarg to disambiguate when multiple projects are active.
- `file_type='plan_hdf'` or `'geom_hdf'` resolves plan/geometry HDF paths via the active `RasPrj`.
- Functions may also accept `hdf_file` as the first arg to work directly with an open `h5py.File`.

**Common Recipes**
- Initialize and compute:
  - `from ras_commander import init_ras_project, RasCmdr`
  - `init_ras_project(<project_folder>, <path_to_Ras.exe>)`
  - `RasCmdr.compute_plan("01", dest_folder="working/run01", overwrite_dest=True)`
- 2D mesh basics:
  - `from ras_commander import HdfMesh, HdfResultsMesh`
  - `cells = HdfMesh.get_mesh_cell_polygons("06")`
  - `faces = HdfMesh.get_mesh_cell_faces("06")`
  - `ts = HdfResultsMesh.get_mesh_timeseries("06", variables=["Water Surface"] )`
- 1D cross sections:
  - `from ras_commander import HdfXsec, HdfResultsXsec`
  - `xsecs = HdfXsec.get_cross_sections("01")`
  - `xs_ts = HdfResultsXsec.get_xsec_timeseries("01", river="...")`
- Pipes and pumps (HEC-RAS 6.6+):
  - `from ras_commander import HdfPipe, HdfPump`
  - `pipes = HdfPipe.get_pipe_conduits("02")`
  - `pumps = HdfPump.get_pump_stations("02")`
- Legacy version extraction (HEC-RAS 3.x-4.x):
  - `from ras_commander import init_ras_project, RasControl`
  - `init_ras_project(path, "4.1")  # Specify version`
  - `RasControl.run_plan("02")  # Use plan numbers`
  - `df_steady = RasControl.get_steady_results("02")`
  - `df_unsteady = RasControl.get_unsteady_results("01", max_times=10)`
  - Note: Uses plan numbers like HDF methods; automatically closes HEC-RAS to prevent conflicts

**Face/Cell Utilities (from examples, not library)**
- The examples include a notebook-only helper `find_nearest_cell_face(point, cell_faces_df)` that computes nearest faces to a point and plots selections. It is not part of the library API; port into scripts as needed.

**Performance & Execution**
- `RasCmdr.compute_plan(..., num_cores=N)`: choose modest N (2–8) unless models benefit from higher parallelism.
- `RasCmdr.compute_parallel(...)`: multiply `max_workers * num_cores` conservatively relative to physical cores/RAM.
- Use `dest_folder` or temporary copies to keep originals immutable.

**Testing & Examples**
- Prefer `ras_commander.RasExamples` to extract official models into a writable location. Avoid committing extracted content.
- See `examples/AGENTS.md` for a task-oriented index of notebooks and unique snippets.

**Out of Scope for Agents**
- `ai_tools/` knowledge base generation. Do not read or run those scripts; they are maintainer-only.


==================================================

File: C:\GH\ras-commander\ras_commander\Decorators.py
==================================================
from functools import wraps
from pathlib import Path
from typing import Union
import logging
import h5py
import inspect
import pandas as pd
from numbers import Number


def log_call(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        logger.debug(f"Calling {func.__name__}")
        result = func(*args, **kwargs)
        logger.debug(f"Finished {func.__name__}")
        return result
    return wrapper

def standardize_input(file_type: str = 'plan_hdf'):
    """
    Decorator to standardize input for HDF file operations.

    This decorator processes various input types and converts them to a Path object
    pointing to the correct HDF file. It handles the following input types:
    - h5py.File objects
    - pathlib.Path objects
    - Strings (file paths or plan/geom numbers)
    - Integers (interpreted as plan/geom numbers)

    The decorator also manages RAS object references and logging.

    Args:
        file_type (str): Specifies whether to look for 'plan_hdf' or 'geom_hdf' files.

    Returns:
        A decorator that wraps the function to standardize its input to a Path object.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(func.__module__)
            
            # Check if the function expects an hdf_path parameter
            sig = inspect.signature(func)
            param_names = list(sig.parameters.keys())
            
            # If first parameter is 'hdf_file', pass an h5py object
            if param_names and param_names[0] == 'hdf_file':
                if isinstance(args[0], h5py.File):
                    return func(*args, **kwargs)
                elif isinstance(args[0], (str, Path)):
                    with h5py.File(args[0], 'r') as hdf:
                        return func(hdf, *args[1:], **kwargs)
                else:
                    raise ValueError(f"Expected h5py.File or path, got {type(args[0])}")
                
            # Handle both static method calls and regular function calls
            if args and isinstance(args[0], type):
                # Static method call, remove the class argument
                args = args[1:]
            
            # Get hdf_input from kwargs if provided with hdf_path key, or take first positional arg
            hdf_input = kwargs.pop('hdf_path', None) if 'hdf_path' in kwargs else (args[0] if args else None)
            
            # Import ras here to ensure we get the most current instance
            from .RasPrj import ras as ras
            # ras_object is always keyword-only, never in args
            ras_object = kwargs.pop('ras_object', None)
            ras_obj = ras_object or ras

            # If no hdf_input provided, return the function unmodified
            if hdf_input is None:
                return func(*args, **kwargs)

            hdf_path = None

            # Clean and normalize string inputs
            if isinstance(hdf_input, str):
                # Clean the string (remove extra whitespace, normalize path separators)
                hdf_input = hdf_input.strip()
                
                # Check if it's a raw file path that exists
                try:
                    test_path = Path(hdf_input)
                    if test_path.is_file():
                        hdf_path = test_path
                        logger.info(f"Using HDF file from direct string path: {hdf_path}")
                except Exception as e:
                    logger.debug(f"Error converting string to path: {str(e)}")

            # If a valid path wasn't created from string processing, continue with normal flow
            if hdf_path is None:
                # If hdf_input is already a Path and exists, use it directly
                if isinstance(hdf_input, Path) and hdf_input.is_file():
                    hdf_path = hdf_input
                    logger.info(f"Using existing Path object HDF file: {hdf_path}")
                # If hdf_input is an h5py.File object, use its filename
                elif isinstance(hdf_input, h5py.File):
                    hdf_path = Path(hdf_input.filename)
                    logger.info(f"Using HDF file from h5py.File object: {hdf_path}")
                # Handle Path objects that might not be verified yet
                elif isinstance(hdf_input, Path):
                    if hdf_input.is_file():
                        hdf_path = hdf_input
                        logger.info(f"Using verified Path object HDF file: {hdf_path}")
                # Handle string inputs that are plan/geom numbers
                elif isinstance(hdf_input, str) and (hdf_input.isdigit() or (len(hdf_input) > 1 and hdf_input[0] == 'p' and hdf_input[1:].isdigit())):
                    try:
                        ras_obj.check_initialized()
                    except Exception as e:
                        raise ValueError(f"RAS object is not initialized: {str(e)}")

                    # Extract the number part and strip leading zeros
                    number_str = hdf_input if hdf_input.isdigit() else hdf_input[1:]
                    stripped_number = number_str.lstrip('0')
                    if stripped_number == '':  # Handle case where input was '0' or '00'
                        stripped_number = '0'

                    # Convert to integer and validate range
                    try:
                        number_int = int(stripped_number)
                        if not (1 <= number_int <= 99):
                            raise ValueError(f"Plan/geometry number must be between 1 and 99, got {number_int}")
                    except (ValueError, TypeError) as e:
                        raise ValueError(f"Cannot convert plan/geometry number '{hdf_input}' to integer") from e
                    
                    if file_type == 'plan_hdf':
                        try:
                            # Convert plan_number column to integers for comparison after stripping zeros
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].str.lstrip('0').astype(int) == number_int]
                            if not plan_info.empty:
                                # Make sure HDF_Results_Path is a string and not None
                                hdf_path_str = plan_info.iloc[0]['HDF_Results_Path']
                                if pd.notna(hdf_path_str):
                                    hdf_path = Path(str(hdf_path_str))
                        except Exception as e:
                            logger.warning(f"Error retrieving plan HDF path: {str(e)}")

                    elif file_type == 'plan':
                        try:
                            # Get plan file path (.p##)
                            from .RasUtils import RasUtils
                            plan_number_str = RasUtils.normalize_ras_number(number_int)
                            hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number_str}"
                            if not hdf_path.exists():
                                raise FileNotFoundError(f"Plan file not found: {hdf_path}")
                        except Exception as e:
                            logger.warning(f"Error retrieving plan file path: {str(e)}")

                    elif file_type == 'geom_hdf':
                        try:
                            # First try to get the geometry number from the plan
                            from ras_commander import RasPlan
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].astype(int) == number_int]
                            if not plan_info.empty:
                                # Extract the geometry number from the plan
                                geom_number = plan_info.iloc[0]['geometry_number']
                                if pd.notna(geom_number) and geom_number is not None:
                                    # Handle different types of geom_number (string or int)
                                    try:
                                        # Get the geometry path using RasPlan
                                        geom_path = RasPlan.get_geom_path(str(geom_number), ras_obj)

                                        if geom_path is not None:
                                            # Create the HDF path by adding .hdf to the geometry path
                                            hdf_path = Path(str(geom_path) + ".hdf")
                                            if hdf_path.exists():
                                                logger.info(f"Found geometry HDF file for plan {number_int}: {hdf_path}")
                                            else:
                                                # Try to find it in the geom_df if direct path doesn't exist
                                                geom_info = ras_obj.geom_df[ras_obj.geom_df['full_path'] == str(geom_path)]
                                                if not geom_info.empty and 'hdf_path' in geom_info.columns:
                                                    hdf_path_str = geom_info.iloc[0]['hdf_path']
                                                    if pd.notna(hdf_path_str):
                                                        hdf_path = Path(str(hdf_path_str))
                                                        logger.info(f"Found geometry HDF file from geom_df for plan {number_int}: {hdf_path}")
                                    except (TypeError, ValueError) as e:
                                        logger.warning(f"Error processing geometry number {geom_number}: {str(e)}")
                                else:
                                    logger.warning(f"No valid geometry number found for plan {number_int}")
                        except Exception as e:
                            logger.warning(f"Error retrieving geometry HDF path: {str(e)}")
                    else:
                        raise ValueError(f"Invalid file type: {file_type}")
                    



                # Handle numeric inputs (int, float, numpy types, etc. - assuming they're plan or geom numbers)
                elif isinstance(hdf_input, Number):
                    try:
                        ras_obj.check_initialized()
                    except Exception as e:
                        raise ValueError(f"RAS object is not initialized: {str(e)}")

                    # Convert to integer and validate range
                    try:
                        number_int = int(hdf_input)
                        if not (1 <= number_int <= 99):
                            raise ValueError(f"Plan/geometry number must be between 1 and 99, got {number_int}")
                    except (ValueError, TypeError) as e:
                        raise ValueError(f"Cannot convert plan/geometry number to integer: {hdf_input}") from e

                    if file_type == 'plan_hdf':
                        try:
                            # Convert plan_number column to integers for comparison after stripping zeros
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].str.lstrip('0').astype(int) == number_int]
                            if not plan_info.empty:
                                # Make sure HDF_Results_Path is a string and not None
                                hdf_path_str = plan_info.iloc[0]['HDF_Results_Path']
                                if pd.notna(hdf_path_str):
                                    hdf_path = Path(str(hdf_path_str))
                        except Exception as e:
                            logger.warning(f"Error retrieving plan HDF path: {str(e)}")

                    elif file_type == 'plan':
                        try:
                            # Get plan file path (.p##)
                            from .RasUtils import RasUtils
                            plan_number_str = RasUtils.normalize_ras_number(number_int)
                            hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number_str}"
                            if not hdf_path.exists():
                                raise FileNotFoundError(f"Plan file not found: {hdf_path}")
                        except Exception as e:
                            logger.warning(f"Error retrieving plan file path: {str(e)}")

                    elif file_type == 'geom_hdf':
                        try:
                            # First try finding plan info to get geometry number
                            plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'].astype(int) == number_int]
                            if not plan_info.empty:
                                # Extract the geometry number from the plan
                                geom_number = plan_info.iloc[0]['geometry_number']
                                if pd.notna(geom_number) and geom_number is not None:
                                    # Handle different types of geom_number (string or int)
                                    try:
                                        # Get the geometry path using RasPlan
                                        from ras_commander import RasPlan
                                        geom_path = RasPlan.get_geom_path(str(geom_number), ras_obj)

                                        if geom_path is not None:
                                            # Create the HDF path by adding .hdf to the geometry path
                                            hdf_path = Path(str(geom_path) + ".hdf")
                                            if hdf_path.exists():
                                                logger.info(f"Found geometry HDF file for plan {number_int}: {hdf_path}")
                                            else:
                                                # Try to find it in the geom_df if direct path doesn't exist
                                                geom_info = ras_obj.geom_df[ras_obj.geom_df['full_path'] == str(geom_path)]
                                                if not geom_info.empty and 'hdf_path' in geom_info.columns:
                                                    hdf_path_str = geom_info.iloc[0]['hdf_path']
                                                    if pd.notna(hdf_path_str):
                                                        hdf_path = Path(str(hdf_path_str))
                                                        logger.info(f"Found geometry HDF file from geom_df for plan {number_int}: {hdf_path}")
                                    except (TypeError, ValueError) as e:
                                        logger.warning(f"Error processing geometry number {geom_number}: {str(e)}")
                                else:
                                    logger.warning(f"No valid geometry number found for plan {number_int}")
                        except Exception as e:
                            logger.warning(f"Error retrieving geometry HDF path: {str(e)}")
                    else:
                        raise ValueError(f"Invalid file type: {file_type}")

            # Final verification that the path exists
            if hdf_path is None or not hdf_path.exists():
                file_type_name = "HDF file" if 'hdf' in file_type else "file"
                error_msg = f"{file_type_name} not found: {hdf_input}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)

            logger.info(f"Final validated file path: {hdf_path}")

            # Validate HDF file structure (only for HDF types)
            if 'hdf' in file_type:
                try:
                    with h5py.File(hdf_path, 'r') as test_file:
                        # Just open to verify it's a valid HDF5 file
                        logger.debug(f"Successfully opened HDF file for validation: {hdf_path}")
                except Exception as e:
                    logger.warning(f"Warning: Could not validate HDF file: {str(e)}")
                    # Continue anyway, let the function handle detailed validation
            
            # Pass all original arguments and keywords, replacing hdf_input with standardized hdf_path
            # If the original input was positional, replace the first argument
            if args and 'hdf_path' not in kwargs:
                new_args = (hdf_path,) + args[1:]
            else:
                new_args = args
                kwargs['hdf_path'] = hdf_path
                
            return func(*new_args, **kwargs)

        return wrapper
    return decorator
==================================================

File: C:\GH\ras-commander\ras_commander\LoggingConfig.py
==================================================
# logging_config.py

import logging
import logging.handlers
from pathlib import Path
import functools

# Define log levels
DEBUG = logging.DEBUG
INFO = logging.INFO
WARNING = logging.WARNING
ERROR = logging.ERROR
CRITICAL = logging.CRITICAL


_logging_setup_done = False

def setup_logging(log_file=None, log_level=logging.INFO):
    """Set up logging configuration for the ras-commander library."""
    global _logging_setup_done
    if _logging_setup_done:
        return
    
    # Define log format
    log_format = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # Configure console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_format)

    # Set up root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(console_handler)

    # Configure file handler if log_file is provided
    if log_file:
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        log_file_path = log_dir / log_file

        file_handler = logging.handlers.RotatingFileHandler(
            log_file_path, maxBytes=10*1024*1024, backupCount=5
        )
        file_handler.setFormatter(log_format)
        root_logger.addHandler(file_handler)
    
    _logging_setup_done = True

def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the specified name.
    
    Args:
        name: The name for the logger, typically __name__ or module path
        
    Returns:
        logging.Logger: Configured logger instance
    """
    logger = logging.getLogger(name)
    if not logger.handlers:  # Only add handler if none exists
        setup_logging()  # Ensure logging is configured
    return logger

def log_call(logger=None):
    """Decorator to log function calls."""
    def get_logger():
        # Check if logger is None or doesn't have a debug method
        if logger is None or not hasattr(logger, 'debug'):
            return logging.getLogger(__name__)
        return logger

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            log = get_logger()
            log.debug(f"Calling {func.__name__}")
            result = func(*args, **kwargs)
            log.debug(f"Finished {func.__name__}")
            return result
        return wrapper
    
    # Check if we're being called as @log_call or @log_call()
    if callable(logger):
        return decorator(logger)
    return decorator

# Set up logging when this module is imported
setup_logging()
==================================================

File: C:\GH\ras-commander\ras_commander\M3Model.py
==================================================
"""
M3Model - Manage and download HCFCD M3 hydraulic models

This module is part of the ras-commander library and provides access to Harris County
Flood Control District (HCFCD) M3 Models - Current FEMA effective HEC-RAS models.

This module follows the centralized logging configuration from ras-commander.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here


-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in M3Model:
- list_models()
- get_model_info()
- extract_model()
- list_channels()
- get_model_by_channel()
- query_arcgis_channels()
- is_model_extracted()
- clean_models_directory()

"""
import os
import requests
import zipfile
import pandas as pd
from pathlib import Path
import shutil
from typing import Union, List, Dict, Optional
from datetime import datetime
import logging
from tqdm import tqdm
from ras_commander import get_logger
from ras_commander.LoggingConfig import log_call

logger = get_logger(__name__)

class M3Model:
    """
    A class for downloading and managing HCFCD M3 Models (Current FEMA effective models).
    All methods are class methods, so no initialization is required.

    These models are Harris County Flood Control District's hydraulic and hydrologic (H&H)
    models representing Current FEMA effective models for major bayous and watersheds in
    the Houston, Texas region.

    Example:
        # List all available models
        models = M3Model.list_models()

        # Extract a specific model
        path = M3Model.extract_model("A")  # Clear Creek

        # Find model by channel name
        model_id = M3Model.get_model_by_channel("BRAYS BAYOU")
        path = M3Model.extract_model(model_id)
    """

    # Base URL for M3 Model downloads
    base_url = 'https://files.m3models.org/modellibrary/'

    # ArcGIS REST API for channel information
    arcgis_url = 'https://www.gis.hctx.net/arcgishcpid/rest/services/HCFCD/Channels/MapServer/0'

    # Base directory for model storage
    base_dir = Path.cwd()
    models_dir = base_dir / 'm3_models'

    # Model metadata - hardcoded since no API is available
    # Format: {model_id: {name, short_name, effective_date, size_gb, description}}
    MODELS = {
        'A': {
            'name': 'Clear Creek',
            'short_name': 'Clear',
            'effective_date': '2022-05-05',
            'size_gb': 0.03,
            'description': 'Clear Creek H&H Models',
            'primary_channels': ['CLEAR CREEK']
        },
        'B': {
            'name': 'Armand Bayou',
            'short_name': 'Armand',
            'effective_date': '2022-05-05',
            'size_gb': 0.04,
            'description': 'Armand Bayou H&H Models',
            'primary_channels': ['ARMAND BAYOU']
        },
        'C': {
            'name': 'Sims Bayou',
            'short_name': 'Sims',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Sims Bayou H&H Models',
            'primary_channels': ['SIMS BAYOU']
        },
        'D': {
            'name': 'Brays Bayou',
            'short_name': 'Brays',
            'effective_date': '2022-05-05',
            'size_gb': 0.03,
            'description': 'Brays Bayou H&H Models',
            'primary_channels': ['BRAYS BAYOU']
        },
        'E': {
            'name': 'White Oak Bayou',
            'short_name': 'WhiteOak',
            'effective_date': '2023-01-30',
            'size_gb': 0.02,
            'description': 'White Oak Bayou H&H Models',
            'primary_channels': ['WHITE OAK BAYOU']
        },
        'F': {
            'name': 'San Jacinto/Galveston Bay',
            'short_name': 'GalvBay',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'San Jacinto/Galveston Bay H&H Models',
            'primary_channels': ['BAYPORT CHANNEL', 'SHIP CHANNEL']
        },
        'G': {
            'name': 'San Jacinto River',
            'short_name': 'SanJac',
            'effective_date': '2022-05-05',
            'size_gb': 0.09,
            'description': 'San Jacinto River H&H Models',
            'primary_channels': ['SAN JACINTO RIVER', 'EAST FORK SAN JACINTO RIVER']
        },
        'H': {
            'name': 'Hunting Bayou',
            'short_name': 'Hunting',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Hunting Bayou H&H Models',
            'primary_channels': ['HUNTING BAYOU']
        },
        'I': {
            'name': 'Vince Bayou',
            'short_name': 'Vince',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Vince Bayou H&H Models',
            'primary_channels': ['VINCE BAYOU', 'LITTLE VINCE BAYOU']
        },
        'J': {
            'name': 'Spring Creek',
            'short_name': 'Spring',
            'effective_date': '2022-05-05',
            'size_gb': 0.06,
            'description': 'Spring Creek H&H Models',
            'primary_channels': ['SPRING CREEK', 'SPRING BRANCH']
        },
        'K': {
            'name': 'Cypress Creek',
            'short_name': 'Cypress',
            'effective_date': '2022-05-05',
            'size_gb': 0.04,
            'description': 'Cypress Creek H&H Models',
            'primary_channels': ['CYPRESS CREEK']
        },
        'L': {
            'name': 'Little Cypress Creek',
            'short_name': 'LttlCyp',
            'effective_date': '2022-05-05',
            'size_gb': 0.03,
            'description': 'Little Cypress Creek H&H Models',
            'primary_channels': ['LITTLE CYPRESS CREEK']
        },
        'M': {
            'name': 'Willow Creek',
            'short_name': 'Willow',
            'effective_date': '2023-01-30',
            'size_gb': 0.05,
            'description': 'Willow Creek H&H Models',
            'primary_channels': ['WILLOW CREEK', 'WILLOW WATER HOLE']
        },
        'N': {
            'name': 'Carpenters Bayou',
            'short_name': 'Carpenters',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Carpenters Bayou H&H Models',
            'primary_channels': ['CARPENTERS BAYOU']
        },
        'O': {
            'name': 'Spring Gully and Goose Creek',
            'short_name': 'SprgGully',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Spring Gully and Goose Creek H&H Models',
            'primary_channels': ['SPRING GULLY', 'GOOSE CREEK', 'E. FORK GOOSE CREEK', 'W. FORK GOOSE CREEK']
        },
        'P': {
            'name': 'Greens Bayou',
            'short_name': 'Greens',
            'effective_date': '2024-03-04',
            'size_gb': 0.02,
            'description': 'Greens Bayou H&H Models',
            'primary_channels': ['GREENS BAYOU', 'HALLS BAYOU', 'GARNERS BAYOU']
        },
        'Q': {
            'name': 'Cedar Bayou',
            'short_name': 'Cedar',
            'effective_date': '2022-05-05',
            'size_gb': 0.02,
            'description': 'Cedar Bayou H&H Models',
            'primary_channels': ['CEDAR BAYOU', 'LITTLE CEDAR BAYOU']
        },
        'R': {
            'name': 'Jackson Bayou',
            'short_name': 'Jackson',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Jackson Bayou H&H Models',
            'primary_channels': ['JACKSON BAYOU']
        },
        'S': {
            'name': 'Luce Bayou',
            'short_name': 'Luce',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Luce Bayou H&H Models',
            'primary_channels': ['LUCE BAYOU']
        },
        'T': {
            'name': 'Barker',
            'short_name': 'Barker',
            'effective_date': '2022-05-05',
            'size_gb': 0.01,
            'description': 'Barker H&H models',
            'primary_channels': ['BARKER DITCH']
        },
        'U': {
            'name': 'Addicks',
            'short_name': 'Addicks',
            'effective_date': '2022-05-05',
            'size_gb': 0.08,
            'description': 'Addicks H&H Models',
            'primary_channels': []  # Reservoir, not a specific channel
        },
        'W': {
            'name': 'Buffalo Bayou',
            'short_name': 'Buffalo',
            'effective_date': '2022-05-05',
            'size_gb': 0.03,
            'description': 'Buffalo Bayou H&H Models',
            'primary_channels': ['BUFFALO BAYOU', 'UPPER BUFFALO BAYOU/CANE']
        }
    }

    # Cache for channel data
    _channel_df = None

    @classmethod
    @log_call
    def list_models(cls, as_dataframe: bool = True) -> Union[pd.DataFrame, List[Dict]]:
        """
        List all available M3 Models.

        Args:
            as_dataframe: If True, returns a pandas DataFrame. If False, returns a list of dicts.

        Returns:
            DataFrame or list of model information dictionaries

        Example:
            >>> models_df = M3Model.list_models()
            >>> print(models_df)
            >>>
            >>> models_list = M3Model.list_models(as_dataframe=False)
            >>> for model in models_list:
            ...     print(f"{model['id']}: {model['name']}")
        """
        models_list = []
        for model_id, info in cls.MODELS.items():
            model_dict = {
                'id': model_id,
                'name': info['name'],
                'description': info['description'],
                'effective_date': info['effective_date'],
                'size_gb': info['size_gb'],
                'primary_channels': ', '.join(info['primary_channels'])
            }
            models_list.append(model_dict)

        if as_dataframe:
            df = pd.DataFrame(models_list)
            logger.info(f"Listed {len(df)} M3 Models")
            return df
        else:
            logger.info(f"Listed {len(models_list)} M3 Models")
            return models_list

    @classmethod
    @log_call
    def get_model_info(cls, model_id: str) -> Dict:
        """
        Get detailed information about a specific model.

        Args:
            model_id: Single letter model identifier (e.g., 'A', 'B', 'C')

        Returns:
            Dictionary containing model metadata

        Raises:
            ValueError: If model_id is not found

        Example:
            >>> info = M3Model.get_model_info('D')
            >>> print(f"Model: {info['name']}")
            >>> print(f"Size: {info['size_gb']} GB")
        """
        model_id = model_id.upper()

        if model_id not in cls.MODELS:
            available = ', '.join(sorted(cls.MODELS.keys()))
            error_msg = f"Model '{model_id}' not found. Available models: {available}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        info = cls.MODELS[model_id].copy()
        info['id'] = model_id
        info['download_url'] = cls._get_download_url(model_id)
        info['filename'] = cls._get_filename(model_id)

        logger.info(f"Retrieved info for model '{model_id}': {info['name']}")
        return info

    @classmethod
    def _get_filename(cls, model_id: str) -> str:
        """Generate the zip filename for a model."""
        info = cls.MODELS[model_id.upper()]
        return f"{model_id.upper()}_{info['short_name']}_FEMA_Effective.zip"

    @classmethod
    def _get_download_url(cls, model_id: str) -> str:
        """Generate the full download URL for a model."""
        info = cls.MODELS[model_id.upper()]
        filename = cls._get_filename(model_id)
        # Format effective date for URL (YYYY-MM-DD HH:MM)
        effective_date = info['effective_date'] + ' 05:00'
        return f"{cls.base_url}{filename}?effectivedate={effective_date.replace(' ', '%20')}"

    @classmethod
    @log_call
    def extract_model(cls, model_id: str, output_path: Union[str, Path] = None,
                     overwrite: bool = False) -> Path:
        """
        Download and extract a specific M3 Model.

        Args:
            model_id: Single letter model identifier (e.g., 'A', 'B', 'C')
            output_path: Optional path where the model will be extracted.
                        If None, uses default 'm3_models' folder.
            overwrite: If True, overwrites existing model directory

        Returns:
            Path to the extracted model directory

        Raises:
            ValueError: If model_id is not found

        Example:
            >>> # Extract to default location
            >>> path = M3Model.extract_model('A')
            >>>
            >>> # Extract to custom location
            >>> path = M3Model.extract_model('D', output_path='my_models')
        """
        model_id = model_id.upper()

        # Validate model ID
        if model_id not in cls.MODELS:
            available = ', '.join(sorted(cls.MODELS.keys()))
            error_msg = f"Model '{model_id}' not found. Available models: {available}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        # Determine output directory
        if output_path is None:
            base_output_path = cls.models_dir
        else:
            base_output_path = Path(output_path)
            if not base_output_path.is_absolute():
                base_output_path = Path.cwd() / base_output_path

        base_output_path.mkdir(parents=True, exist_ok=True)

        model_info = cls.MODELS[model_id]
        model_name = model_info['name']
        model_folder = base_output_path / model_name

        logger.info("----- M3Model Extracting Model -----")
        logger.info(f"Extracting model '{model_id}' - {model_name}")

        # Check if model already exists
        if model_folder.exists():
            if not overwrite:
                logger.info(f"Model '{model_name}' already exists at {model_folder}")
                logger.info("Use overwrite=True to re-download")
                return model_folder
            else:
                logger.info(f"Removing existing model '{model_name}'...")
                try:
                    shutil.rmtree(model_folder)
                    logger.info(f"Existing folder deleted")
                except Exception as e:
                    logger.error(f"Failed to delete existing folder: {e}")
                    raise

        # Download the zip file
        zip_path = base_output_path / cls._get_filename(model_id)
        url = cls._get_download_url(model_id)

        logger.info(f"Downloading from: {url}")
        logger.info(f"Size: {model_info['size_gb']} GB")

        try:
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()

            # Get file size
            total_size = int(response.headers.get('content-length', 0))

            # Download with progress bar
            with open(zip_path, 'wb') as file:
                if total_size > 0:
                    with tqdm(
                        desc=f"Downloading {model_id}",
                        total=total_size,
                        unit='iB',
                        unit_scale=True,
                        unit_divisor=1024,
                    ) as progress_bar:
                        for chunk in response.iter_content(chunk_size=8192):
                            size = file.write(chunk)
                            progress_bar.update(size)
                else:
                    for chunk in response.iter_content(chunk_size=8192):
                        file.write(chunk)

            logger.info(f"Downloaded to {zip_path}")

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to download model '{model_id}': {e}")
            if zip_path.exists():
                zip_path.unlink()
            raise

        # Extract the zip file
        logger.info(f"Extracting to {model_folder}...")
        try:
            model_folder.mkdir(parents=True, exist_ok=True)

            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(model_folder)

            logger.info(f"Successfully extracted model '{model_id}' to {model_folder}")

        except Exception as e:
            logger.error(f"Failed to extract model '{model_id}': {e}")
            if model_folder.exists():
                shutil.rmtree(model_folder)
            raise
        finally:
            # Clean up zip file
            if zip_path.exists():
                zip_path.unlink()
                logger.debug(f"Removed temporary zip file: {zip_path}")

        return model_folder

    @classmethod
    @log_call
    def list_channels(cls, refresh: bool = False) -> pd.DataFrame:
        """
        List all channels from the HCFCD ArcGIS REST API.

        Args:
            refresh: If True, forces a refresh of channel data from the API

        Returns:
            DataFrame containing channel names and associated model IDs

        Example:
            >>> channels_df = M3Model.list_channels()
            >>> print(channels_df[channels_df['model_id'].notna()])
        """
        if cls._channel_df is None or refresh:
            cls._channel_df = cls.query_arcgis_channels()

        logger.info(f"Listed {len(cls._channel_df)} channels")
        return cls._channel_df.copy()

    @classmethod
    @log_call
    def query_arcgis_channels(cls) -> pd.DataFrame:
        """
        Query the HCFCD ArcGIS REST API for channel information.

        Returns:
            DataFrame containing channel names and their associated model IDs

        Note:
            This method queries the ArcGIS REST API to get unique channel names
            and attempts to match them to M3 Models based on the primary_channels
            defined in the MODELS dictionary.
        """
        logger.info("Querying HCFCD ArcGIS REST API for channel data...")

        query_url = f"{cls.arcgis_url}/query"
        params = {
            'where': '1=1',
            'outFields': 'CHAN_NAME',
            'returnDistinctValues': 'true',
            'returnGeometry': 'false',
            'f': 'json'
        }

        try:
            response = requests.get(query_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            # Extract channel names
            channels = []
            for feature in data.get('features', []):
                chan_name = feature.get('attributes', {}).get('CHAN_NAME', '').strip()
                if chan_name and chan_name != ' ':
                    channels.append(chan_name)

            # Create DataFrame
            df = pd.DataFrame({'channel_name': sorted(set(channels))})

            # Add model_id column by matching with MODELS
            df['model_id'] = df['channel_name'].apply(cls._match_channel_to_model)

            logger.info(f"Retrieved {len(df)} unique channels from ArcGIS API")
            logger.info(f"Matched {df['model_id'].notna().sum()} channels to M3 Models")

            return df

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to query ArcGIS REST API: {e}")
            raise
        except Exception as e:
            logger.error(f"Error processing ArcGIS data: {e}")
            raise

    @classmethod
    def _match_channel_to_model(cls, channel_name: str) -> Optional[str]:
        """
        Match a channel name to a model ID based on primary_channels.

        Args:
            channel_name: Name of the channel to match

        Returns:
            Model ID if matched, None otherwise
        """
        channel_upper = channel_name.upper()

        for model_id, info in cls.MODELS.items():
            for primary_channel in info['primary_channels']:
                if channel_upper == primary_channel.upper():
                    return model_id

        return None

    @classmethod
    @log_call
    def get_model_by_channel(cls, channel_name: str) -> Optional[str]:
        """
        Find the M3 Model ID for a given channel name.

        Args:
            channel_name: Name of the channel (e.g., 'BRAYS BAYOU', 'BUFFALO BAYOU')

        Returns:
            Model ID if found, None otherwise

        Example:
            >>> model_id = M3Model.get_model_by_channel('BRAYS BAYOU')
            >>> print(f"Model ID: {model_id}")  # Output: Model ID: D
            >>>
            >>> # Extract the model
            >>> if model_id:
            ...     path = M3Model.extract_model(model_id)
        """
        model_id = cls._match_channel_to_model(channel_name)

        if model_id:
            logger.info(f"Channel '{channel_name}' matches model '{model_id}' - {cls.MODELS[model_id]['name']}")
        else:
            logger.warning(f"No model found for channel '{channel_name}'")

        return model_id

    @classmethod
    @log_call
    def is_model_extracted(cls, model_id: str, output_path: Union[str, Path] = None) -> bool:
        """
        Check if a model is already extracted.

        Args:
            model_id: Single letter model identifier
            output_path: Optional path to check. If None, uses default 'm3_models' folder.

        Returns:
            True if model directory exists, False otherwise

        Example:
            >>> if not M3Model.is_model_extracted('A'):
            ...     M3Model.extract_model('A')
        """
        model_id = model_id.upper()

        if model_id not in cls.MODELS:
            logger.warning(f"Model '{model_id}' not found")
            return False

        if output_path is None:
            base_output_path = cls.models_dir
        else:
            base_output_path = Path(output_path)
            if not base_output_path.is_absolute():
                base_output_path = Path.cwd() / base_output_path

        model_name = cls.MODELS[model_id]['name']
        model_folder = base_output_path / model_name

        is_extracted = model_folder.exists()
        logger.info(f"Model '{model_id}' extracted: {is_extracted}")
        return is_extracted

    @classmethod
    @log_call
    def clean_models_directory(cls, output_path: Union[str, Path] = None):
        """
        Remove all extracted models from the models directory.

        Args:
            output_path: Optional path to clean. If None, uses default 'm3_models' folder.

        Example:
            >>> M3Model.clean_models_directory()
        """
        if output_path is None:
            target_dir = cls.models_dir
        else:
            target_dir = Path(output_path)
            if not target_dir.is_absolute():
                target_dir = Path.cwd() / target_dir

        logger.info(f"Cleaning models directory: {target_dir}")

        if target_dir.exists():
            try:
                shutil.rmtree(target_dir)
                logger.info("All models have been removed.")
            except Exception as e:
                logger.error(f"Failed to remove models directory: {e}")
                raise
        else:
            logger.warning("Models directory does not exist.")

        target_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Models directory cleaned and recreated.")

==================================================

File: C:\GH\ras-commander\ras_commander\RasBreach.py
==================================================
"""
RasBreach: Dam breach parameter modification for HEC-RAS plan files.

This module provides methods for reading and writing breach parameters in plain text
plan files (.p##). For extracting breach RESULTS from HDF files, use HdfResultsBreach class.

Architectural Separation:
    - RasBreach: Breach PARAMETERS in plain text plan files (.p##)
    - HdfResultsBreach: Breach RESULTS from HDF files (.p##.hdf)
    - HdfStruc: Structure listings and metadata from HDF

The class follows ras-commander conventions with static methods and support for
plan numbers, integers, or file paths.

Classes:
    RasBreach: Static methods for breach parameter operations
    BreachLocation: Dataclass for breach location data
    BreachBlock: Dataclass for breach parameter blocks

Key Plan File Methods:
    - list_breach_structures_plan(): List breach structures in plan file
    - read_breach_block(): Parse breach parameters from plan
    - update_breach_block(): Modify breach parameters in plan

For HDF Results Extraction, see HdfResultsBreach:
    - HdfResultsBreach.get_breach_timeseries(): Extract time series
    - HdfResultsBreach.get_breach_summary(): Extract summary statistics
    - HdfResultsBreach.get_breaching_variables(): Breach geometry evolution
    - HdfResultsBreach.get_structure_variables(): Structure flow variables

Author: ras-commander development team
Date: 2025
"""

from typing import Dict, List, Union, Optional, Tuple
from pathlib import Path
import pandas as pd
from datetime import datetime
import re
from dataclasses import dataclass

from .Decorators import log_call
from .LoggingConfig import get_logger
from .RasPrj import ras

logger = get_logger(__name__)


class RasBreach:
    """
    Handles dam breach parameter reading and modification in plan files.

    This class provides methods for manipulating breach parameters in plain text
    plan files (.p##). For extracting breach RESULTS from HDF files, use HdfResultsBreach.

    Key Functionality:
    - List breach structures defined in plan files
    - Read breach parameters (method, geometry, timing, etc.)
    - Modify breach parameters (activation, progression, geometry)
    - Create backups before modification
    - Validate CRLF line endings for HEC-RAS compatibility

    All methods accept plan numbers, integers, or file paths.

    Examples:
        >>> from ras_commander import RasBreach, HdfResultsBreach
        >>>
        >>> # List breach structures in plan file
        >>> structures = RasBreach.list_breach_structures_plan("02")
        >>>
        >>> # Read breach parameters
        >>> params = RasBreach.read_breach_block("02", "Dam")
        >>> print(f"Active: {params['is_active']}")
        >>>
        >>> # Modify parameters
        >>> RasBreach.update_breach_block("02", "Dam", method=1)
        >>>
        >>> # For HDF results extraction, use HdfResultsBreach:
        >>> timeseries = HdfResultsBreach.get_breach_timeseries("02", "Dam")
        >>> summary = HdfResultsBreach.get_breach_summary("02")
    """

    # ==========================================================================
    # PLAN FILE PARAMETER METHODS
    # NOTE: For HDF results extraction, use HdfResultsBreach class:
    #   - HdfResultsBreach.get_breach_timeseries()
    #   - HdfResultsBreach.get_breach_summary()
    #   - HdfResultsBreach.get_breaching_variables()
    #   - HdfResultsBreach.get_structure_variables()
    # ==========================================================================

    # ==========================================================================
    # PLAN FILE PARAMETER METHODS
    # ==========================================================================

    @dataclass
    class BreachLocation:
        """Represents the structured data encoded in the `Breach Loc` line."""
        river: str
        reach: str
        station: str
        is_active: bool
        structure: str

        @classmethod
        def from_value(cls, value: str) -> "RasBreach.BreachLocation":
            parts = value.split(",")
            if len(parts) < 5:
                raise ValueError(f"Unexpected Breach Loc format: '{value}'")
            river = parts[0].strip()
            reach = parts[1].strip()
            station = parts[2].strip()
            flag = parts[3].strip()
            structure = ",".join(parts[4:]).strip()
            return cls(
                river=river,
                reach=reach,
                station=station,
                is_active=flag.strip().lower() in {"true", "1", "yes"},
                structure=structure,
            )

    @dataclass
    class BreachBlock:
        """Structured representation of a breach block within a plan file."""
        start_index: int
        end_index: int
        order: List[Tuple[str, str]]
        values: Dict[str, str]
        table_rows: Dict[str, List[List[float]]]
        table_row_lengths: Dict[str, List[int]]

        # Numeric table keys
        NUMERIC_TABLE_KEYS = {
            "Breach Progression",
            "Simplified Physical Breach Downcutting",
            "Simplified Physical Breach Widening",
        }
        DEFAULT_VALUES_PER_ROW = 10
        FIXED_WIDTH = 8

        @property
        def location(self) -> "RasBreach.BreachLocation":
            return RasBreach.BreachLocation.from_value(self.values["Breach Loc"])

        @property
        def structure_name(self) -> str:
            return self.location.structure.strip()

        @property
        def is_active(self) -> bool:
            return self.location.is_active

        def to_dict(self) -> Dict:
            """Convert breach block to dictionary for easy inspection."""
            return {
                'structure_name': self.structure_name,
                'is_active': self.is_active,
                'river': self.location.river,
                'reach': self.location.reach,
                'station': self.location.station,
                'values': self.values.copy(),
                'table_rows': self.table_rows.copy(),
            }

        def to_lines(self) -> List[str]:
            """Serialize breach block back to plan file format."""
            lines: List[str] = []
            for kind, key in self.order:
                if kind == "line":
                    lines.append(f"{key}={self.values[key]}")
                elif kind == "table":
                    rows = self.table_rows.get(key, [])
                    if rows:
                        lines.extend(RasBreach._format_numeric_rows(rows, width=self.FIXED_WIDTH))
                elif kind == "blank":
                    lines.append("")
                elif kind == "literal":
                    lines.append(key)
            return lines

    @staticmethod
    @log_call
    def list_breach_structures_plan(plan_input: Union[str, int, Path], *, ras_object=None) -> List[Dict]:
        """
        List all breach structures defined in plan file.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        List[Dict]
            List of dictionaries containing breach location information:
            - structure: Structure name
            - river: River name
            - reach: Reach name
            - station: River station
            - is_active: Boolean, True if breach is active

        Examples
        --------
        >>> # Using plan number
        >>> structures = RasBreach.list_breach_structures_plan("02")
        >>> for struct in structures:
        ...     print(f"{struct['structure']}: Active={struct['is_active']}")
        >>>
        >>> # Using plan file path
        >>> plan_path = Path("MyProject.p02")
        >>> structures = RasBreach.list_breach_structures_plan(plan_path)

        Notes
        -----
        - Returns breach structures regardless of activation status
        - Use is_active field to filter for active breaches only
        - Accepts plan number (string/int) or full plan file path
        """
        from .RasUtils import RasUtils
        
        ras_obj = ras_object or ras
        
        try:
            # Handle plan number or path input
            if isinstance(plan_input, Path):
                plan_path = plan_input
            elif isinstance(plan_input, str):
                # Check if it's a file path
                test_path = Path(plan_input)
                if test_path.exists():
                    plan_path = test_path
                else:
                    # It's a plan number
                    ras_obj.check_initialized()
                    plan_number = RasUtils.normalize_ras_number(plan_input)
                    plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            elif isinstance(plan_input, int):
                # It's a plan number
                ras_obj.check_initialized()
                plan_number = RasUtils.normalize_ras_number(plan_input)
                plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            else:
                raise ValueError(f"Invalid plan_input type: {type(plan_input)}")
            
            if not plan_path.exists():
                raise FileNotFoundError(f"Plan file not found: {plan_path}")
            
            blocks = RasBreach._read_breach_blocks_internal(plan_path)
            locations = []
            for block in blocks:
                loc = block.location
                locations.append({
                    'structure': loc.structure,
                    'river': loc.river,
                    'reach': loc.reach,
                    'station': loc.station,
                    'is_active': loc.is_active
                })
            logger.info(f"Found {len(locations)} breach structures in {plan_path.name}")
            return locations
        except Exception as e:
            logger.error(f"Error listing breach structures: {e}")
            raise

    @staticmethod
    @log_call
    def read_breach_block(plan_input: Union[str, int, Path], structure_name: str, *, ras_object=None) -> Dict:
        """
        Read breach parameters for specified structure from plan file.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        structure_name : str
            Name of breach structure to read
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        Dict
            Dictionary containing all breach parameters:
            - structure_name: Structure name
            - is_active: Boolean, breach activation status
            - river, reach, station: Location information
            - values: Dict of all breach parameter values
            - table_rows: Dict of numeric tables (progression, downcutting, etc.)

        Examples
        --------
        >>> # Using plan number
        >>> breach_data = RasBreach.read_breach_block("02", "Laxton_Dam")
        >>> print(f"Active: {breach_data['is_active']}")
        >>>
        >>> # Using plan file path
        >>> plan_path = Path("MyProject.p02")
        >>> breach_data = RasBreach.read_breach_block(plan_path, "Laxton_Dam")

        Raises
        ------
        ValueError
            If specified structure not found in plan file

        Notes
        -----
        - Accepts plan number (string/int) or full plan file path
        - Uses RasUtils.normalize_ras_number() for plan number handling
        - All values returned as strings; parse as needed
        """
        from .RasUtils import RasUtils

        ras_obj = ras_object or ras

        try:
            # Handle plan number or path input
            if isinstance(plan_input, Path):
                plan_path = plan_input
            elif isinstance(plan_input, str):
                # Check if it's a file path
                test_path = Path(plan_input)
                if test_path.exists():
                    plan_path = test_path
                else:
                    # It's a plan number
                    ras_obj.check_initialized()
                    plan_number = RasUtils.normalize_ras_number(plan_input)
                    plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            elif isinstance(plan_input, int):
                # It's a plan number
                ras_obj.check_initialized()
                plan_number = RasUtils.normalize_ras_number(plan_input)
                plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            else:
                raise ValueError(f"Invalid plan_input type: {type(plan_input)}")

            if not plan_path.exists():
                raise FileNotFoundError(f"Plan file not found: {plan_path}")

            blocks = RasBreach._read_breach_blocks_internal(plan_path)
            block = RasBreach._find_block_by_structure(blocks, structure_name)

            if block is None:
                raise ValueError(f"Structure '{structure_name}' not found in {plan_path.name}")

            logger.info(f"Read breach block for {structure_name} from {plan_path.name}")
            return block.to_dict()

        except Exception as e:
            logger.error(f"Error reading breach block: {e}")
            raise

    @staticmethod
    @log_call
    def update_breach_block(
        plan_input: Union[str, int, Path],
        structure_name: str,
        *,
        is_active: bool = None,
        method: int = None,
        geom_values: List = None,
        start_values: List = None,
        progression_mode: int = None,
        progression_pairs: List[Tuple[float, float]] = None,
        downcutting_pairs: List[Tuple[float, float]] = None,
        widening_pairs: List[Tuple[float, float]] = None,
        calculator_data: List = None,
        create_backup: bool = True,
        ras_object=None
    ) -> Dict:
        """
        Update breach parameters for specified structure in plan file.

        **CRITICAL**: Creates backup before modification. Uses CRLF line endings for HEC-RAS compatibility.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        structure_name : str
            Name of breach structure to update
        is_active : bool, optional
            Set breach activation status (True/False)
        method : int, optional
            Breach calculation method (0-7)
        geom_values : List, optional
            Breach geometry values: [center_station, final_width, final_elev,
            left_slope, right_slope, weir_coef, formation_time]
        start_values : List, optional
            Breach starting conditions
        progression_mode : int, optional
            Progression mode (0=Linear, 1=Non-linear)
        progression_pairs : List[Tuple[float, float]], optional
            Time/breach fraction pairs for non-linear progression
        downcutting_pairs : List[Tuple[float, float]], optional
            Time/elevation pairs for physical breach downcutting
        widening_pairs : List[Tuple[float, float]], optional
            Time/width pairs for physical breach widening
        calculator_data : List, optional
            Breach calculator heuristic inputs
        create_backup : bool, default True
            Create backup file before modification
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        Dict
            Updated breach block as dictionary

        Examples
        --------
        >>> # Activate breach
        >>> RasBreach.update_breach_block("02", "Laxton_Dam", is_active=True)

        >>> # Set breach geometry
        >>> geom = [150, 100, 1400, 1, 1, 2.6, 0.5]  # center, width, elev, slopes, coef, time
        >>> RasBreach.update_breach_block("02", "Laxton_Dam", geom_values=geom)

        >>> # Set non-linear progression
        >>> progression = [(0, 0), (0.5, 0.3), (1.0, 1.0)]  # time, fraction pairs
        >>> RasBreach.update_breach_block("02", "Laxton_Dam",
        ...                               progression_mode=1,
        ...                               progression_pairs=progression)

        Raises
        ------
        ValueError
            If structure not found in plan file
        RuntimeError
            If CRLF line endings not preserved (HEC-RAS incompatibility)

        Warnings
        --------
        - Modifies plan file in-place
        - Backup created in same directory with timestamp
        - HEC-RAS must be closed before modification
        - Validates CRLF line endings after write

        Notes
        -----
        Based on TNTech Dam Breach Dashboard breach_io.py implementation.
        Adapted to ras-commander conventions with plan-number support.
        """
        from .RasUtils import RasUtils

        ras_obj = ras_object or ras

        try:
            # Handle plan number or path input
            if isinstance(plan_input, Path):
                plan_path = plan_input
            elif isinstance(plan_input, str):
                # Check if it's a file path
                test_path = Path(plan_input)
                if test_path.exists():
                    plan_path = test_path
                else:
                    # It's a plan number
                    ras_obj.check_initialized()
                    plan_number = RasUtils.normalize_ras_number(plan_input)
                    plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            elif isinstance(plan_input, int):
                # It's a plan number
                ras_obj.check_initialized()
                plan_number = RasUtils.normalize_ras_number(plan_input)
                plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
            else:
                raise ValueError(f"Invalid plan_input type: {type(plan_input)}")

            if not plan_path.exists():
                raise FileNotFoundError(f"Plan file not found: {plan_path}")

            # Read all breach blocks
            lines = plan_path.read_text().splitlines()
            blocks = RasBreach._parse_breach_blocks(lines)
            block = RasBreach._find_block_by_structure(blocks, structure_name)

            if block is None:
                raise ValueError(f"Structure '{structure_name}' not found in {plan_path.name}")

            # Apply updates
            if is_active is not None:
                RasBreach._set_activation(block, is_active)
            if method is not None:
                block.values["Breach Method"] = f" {int(method)}"
            if geom_values is not None:
                block.values["Breach Geom"] = RasBreach._format_csv(geom_values)
            if start_values is not None:
                block.values["Breach Start"] = RasBreach._format_csv(start_values)
            if progression_mode is not None or progression_pairs is not None:
                mode = progression_mode if progression_mode is not None else int(block.values["Breach Progression"].strip())
                RasBreach._set_progression(block, mode, progression_pairs)
            if downcutting_pairs is not None:
                RasBreach._set_table_pairs(block, "Simplified Physical Breach Downcutting", downcutting_pairs)
            if widening_pairs is not None:
                RasBreach._set_table_pairs(block, "Simplified Physical Breach Widening", widening_pairs)
            if calculator_data is not None:
                block.values["Breach Calculator Data"] = RasBreach._format_csv(calculator_data)

            # Replace block lines in file
            new_block_lines = block.to_lines()
            lines[block.start_index:block.end_index] = new_block_lines
            block.end_index = block.start_index + len(new_block_lines)

            # Create backup
            if create_backup:
                RasBreach._create_backup(plan_path)

            # Write with CRLF line endings (CRITICAL for HEC-RAS)
            if lines and not lines[-1].endswith("\n"):
                output = "\r\n".join(lines) + "\r\n"
            else:
                output = "\r\n".join(lines)

            # Use open() with newline='' to preserve CRLF
            with open(plan_path, 'w', encoding='utf-8', newline='') as f:
                f.write(output)

            # Validate CRLF preservation
            if not RasBreach._validate_crlf(plan_path):
                raise RuntimeError(
                    f"CRITICAL: Failed to preserve CRLF line endings in {plan_path}. "
                    "HEC-RAS will not be able to open this project."
                )

            logger.info(f"Updated breach block for {structure_name} in {plan_path.name}")
            return block.to_dict()

        except Exception as e:
            logger.error(f"Error updating breach block: {e}")
            raise

    @staticmethod
    @log_call
    def set_breach_geom(
        plan_input: Union[str, int, Path],
        structure_name: str,
        *,
        centerline: Optional[float] = None,
        initial_width: Optional[float] = None,
        final_bottom_elev: Optional[float] = None,
        left_slope: Optional[float] = None,
        right_slope: Optional[float] = None,
        active: Optional[bool] = None,
        weir_coef: Optional[float] = None,
        top_elev: Optional[float] = None,
        formation_method: Optional[int] = None,
        formation_time: Optional[float] = None,
        ras_object=None
    ) -> Dict:
        """
        Update individual breach geometry parameters.

        Convenience function for modifying specific breach geometry fields without
        reconstructing the entire Breach Geom CSV. Reads current values and updates
        only the specified parameters.

        Parameters
        ----------
        plan_input : Union[str, int, Path]
            Plan number (e.g., "02", 2) or path to HEC-RAS plan file
        structure_name : str
            Name of breach structure to update
        centerline : float, optional
            Centerline/station location (ft or m)
        initial_width : float, optional
            Initial breach bottom width (ft or m)
        final_bottom_elev : float, optional
            Final breach bottom elevation (ft or m) - **Common modification**
        left_slope : float, optional
            Left side slope (H:V ratio, e.g., 0.5 = 0.5H:1V)
        right_slope : float, optional
            Right side slope (H:V ratio)
        active : bool, optional
            Breach activation flag (True/False)
        weir_coef : float, optional
            Weir discharge coefficient (dimensionless)
        top_elev : float, optional
            Top elevation (ft or m)
        formation_method : int, optional
            Formation method (1=Time-based, 2=Trigger-based)
        formation_time : float, optional
            Formation time (hrs) or trigger threshold (ft)
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        Dict
            Updated breach block dictionary

        Examples
        --------
        >>> # Update just Final Bottom Elevation
        >>> RasBreach.set_breach_geom("19", "Dam", final_bottom_elev=605)
        >>>
        >>> # Update multiple parameters
        >>> RasBreach.set_breach_geom("19", "Dam",
        ...                           initial_width=250,
        ...                           final_bottom_elev=600,
        ...                           formation_time=3.0)
        >>>
        >>> # Change breach to time-based formation
        >>> RasBreach.set_breach_geom("template_plan", "Dam",
        ...                           formation_method=1,
        ...                           formation_time=2.5)

        Notes
        -----
        - Only specified parameters are modified; others retain current values
        - Automatically reads current Breach Geom and updates selectively
        - Creates backup before modification
        - Validates CRLF line endings

        Breach Geom Field Reference:
            [0] centerline - Breach centerline/station location
            [1] initial_width - Starting breach width
            [2] final_bottom_elev - Final breach bottom elevation
            [3] left_slope - Left side slope (H:V)
            [4] right_slope - Right side slope (H:V)
            [5] active - Activation flag
            [6] weir_coef - Weir discharge coefficient
            [7] top_elev - Top elevation
            [8] formation_method - 1=Time, 2=Trigger
            [9] formation_time - Time (hrs) or threshold (ft)
        """
        try:
            # Read current breach block
            current_block = RasBreach.read_breach_block(plan_input, structure_name, ras_object=ras_object)

            # Parse current Breach Geom values
            geom_str = current_block['values'].get('Breach Geom', '')
            if not geom_str:
                raise ValueError(f"No Breach Geom found for structure '{structure_name}'")

            current_geom = [x.strip() for x in geom_str.split(',')]

            if len(current_geom) < 10:
                raise ValueError(f"Breach Geom has {len(current_geom)} fields, expected 10")

            # Update specified parameters (preserve current values for None parameters)
            new_geom = current_geom.copy()

            if centerline is not None:
                new_geom[0] = centerline
            if initial_width is not None:
                new_geom[1] = initial_width
            if final_bottom_elev is not None:
                new_geom[2] = final_bottom_elev
            if left_slope is not None:
                new_geom[3] = left_slope
            if right_slope is not None:
                new_geom[4] = right_slope
            if active is not None:
                new_geom[5] = active
            if weir_coef is not None:
                new_geom[6] = weir_coef
            if top_elev is not None:
                new_geom[7] = top_elev
            if formation_method is not None:
                new_geom[8] = formation_method
            if formation_time is not None:
                new_geom[9] = formation_time

            # Log what changed
            changes = []
            field_names = ['centerline', 'initial_width', 'final_bottom_elev', 'left_slope', 'right_slope',
                          'active', 'weir_coef', 'top_elev', 'formation_method', 'formation_time']
            for idx, (old, new, name) in enumerate(zip(current_geom, new_geom, field_names)):
                if str(old) != str(new):
                    changes.append(f"{name}: {old} → {new}")

            if changes:
                logger.info(f"Modifying breach geometry for {structure_name}: {', '.join(changes)}")
            else:
                logger.warning(f"No changes specified for {structure_name}")

            # Update using existing update_breach_block method
            return RasBreach.update_breach_block(
                plan_input,
                structure_name,
                geom_values=new_geom,
                ras_object=ras_object
            )

        except Exception as e:
            logger.error(f"Error setting breach geometry: {e}")
            raise

    # ==========================================================================
    # INTERNAL HELPER METHODS
    # ==========================================================================

    @staticmethod
    def _read_breach_blocks_internal(plan_path: Path) -> List["RasBreach.BreachBlock"]:
        """Internal method to read and parse breach blocks from plan file."""
        lines = plan_path.read_text().splitlines()
        return RasBreach._parse_breach_blocks(lines)

    @staticmethod
    def _parse_breach_blocks(lines: List[str]) -> List["RasBreach.BreachBlock"]:
        """Parse all breach blocks from plan file lines."""
        blocks: List[RasBreach.BreachBlock] = []
        idx = 0
        while idx < len(lines):
            line = lines[idx]
            if line.startswith("Breach Loc="):
                start_idx = idx
                block_lines = [line]
                idx += 1
                while idx < len(lines):
                    candidate = lines[idx]
                    if candidate.startswith("Breach Loc=") and block_lines:
                        break
                    block_lines.append(candidate)
                    idx += 1
                end_idx = start_idx + len(block_lines)
                block = RasBreach._parse_block(block_lines, start_idx, end_idx)
                blocks.append(block)
            else:
                idx += 1
        return blocks

    @staticmethod
    def _parse_block(block_lines: List[str], start_index: int, end_index: int) -> "RasBreach.BreachBlock":
        """Parse single breach block from lines."""
        values: Dict[str, str] = {}
        table_rows: Dict[str, List[List[float]]] = {}
        order: List[Tuple[str, str]] = []
        current_table_key: Optional[str] = None

        for line in block_lines:
            if "=" in line:
                key, value = line.split("=", 1)
                key = key.strip()
                value = value.rstrip()
                values[key] = value
                order.append(("line", key))
                if key in RasBreach.BreachBlock.NUMERIC_TABLE_KEYS:
                    order.append(("table", key))
                    current_table_key = key
                    table_rows.setdefault(key, [])
                else:
                    current_table_key = None
            else:
                if current_table_key:
                    stripped = line.strip()
                    if stripped:
                        numeric_row = [float(part) for part in stripped.split()]
                        table_rows.setdefault(current_table_key, []).append(numeric_row)
                else:
                    if line.strip() == "":
                        order.append(("blank", ""))
                    else:
                        order.append(("literal", line))

        table_row_lengths = {key: [len(row) for row in rows] for key, rows in table_rows.items()}
        return RasBreach.BreachBlock(
            start_index=start_index,
            end_index=end_index,
            order=order,
            values=values,
            table_rows=table_rows,
            table_row_lengths=table_row_lengths,
        )

    @staticmethod
    def _find_block_by_structure(blocks: List["RasBreach.BreachBlock"], structure_name: str) -> Optional["RasBreach.BreachBlock"]:
        """Find breach block by structure name (case-insensitive)."""
        target = structure_name.strip().lower()
        for block in blocks:
            if block.structure_name.lower() == target:
                return block
        return None

    @staticmethod
    def _set_activation(block: "RasBreach.BreachBlock", is_active: bool) -> None:
        """Set breach activation status."""
        loc = block.location
        loc.is_active = bool(is_active)
        river = (loc.river or "").rjust(16)
        reach = (loc.reach or "").rjust(16)
        station = (loc.station or "").rjust(8)
        flag = "True" if loc.is_active else "False"
        structure = (loc.structure or "").ljust(16)
        block.values["Breach Loc"] = f"{river},{reach},{station},{flag},{structure}"

    @staticmethod
    def _set_progression(block: "RasBreach.BreachBlock", mode: int, pairs: Optional[List[Tuple[float, float]]]) -> None:
        """Set breach progression mode and pairs."""
        block.values["Breach Progression"] = f" {int(mode)}"
        if pairs is not None:
            flat_values: List[float] = []
            for pair in pairs:
                if len(pair) != 2:
                    raise ValueError("Progression pairs must contain exactly two values")
                flat_values.extend([float(pair[0]), float(pair[1])])
            RasBreach._set_table_values(block, "Breach Progression", flat_values)

    @staticmethod
    def _set_table_pairs(block: "RasBreach.BreachBlock", key: str, pairs: List[Tuple[float, float]]) -> None:
        """Set table values from time/value pairs."""
        flat_values: List[float] = []
        for pair in pairs:
            if len(pair) != 2:
                raise ValueError(f"{key} pairs must contain exactly two values")
            flat_values.extend([float(pair[0]), float(pair[1])])
        RasBreach._set_table_values(block, key, flat_values)

    @staticmethod
    def _set_table_values(block: "RasBreach.BreachBlock", key: str, values: List[float]) -> None:
        """Set numeric table values for breach block."""
        lengths = block.table_row_lengths.get(key)
        if lengths and sum(lengths) == len(values):
            rows: List[List[float]] = []
            index = 0
            for length in lengths:
                rows.append(list(values[index:index + length]))
                index += length
        else:
            rows = []
            chunk = RasBreach.BreachBlock.DEFAULT_VALUES_PER_ROW
            for index in range(0, len(values), chunk):
                rows.append(list(values[index:index + chunk]))

        block.table_rows[key] = rows
        block.table_row_lengths[key] = [len(row) for row in rows]

    @staticmethod
    def _format_numeric_rows(rows: List[List[float]], width: int) -> List[str]:
        """Format numeric table rows for plan file."""
        formatted: List[str] = []
        for row in rows:
            formatted.append("".join(RasBreach._format_numeric_value(value, width=width) for value in row))
        return formatted

    @staticmethod
    def _format_numeric_value(value: float, width: int) -> str:
        """Format single numeric value with fixed width."""
        numeric = float(value)
        if numeric == 0:
            text = "0"
        elif abs(numeric) >= 10000 or (0 < abs(numeric) < 1e-4):
            text = f"{numeric:.3e}"
        else:
            text = f"{numeric:.6g}"
        if len(text) > width:
            text = f"{numeric:.6e}"
        if len(text) > width:
            text = text[:width]
        return text.rjust(width)

    @staticmethod
    def _format_csv(values: List) -> str:
        """Format values as comma-separated string."""
        formatted: List[str] = []
        for item in values:
            if item is None:
                formatted.append("")
            elif isinstance(item, bool):
                formatted.append("True" if item else "False")
            elif isinstance(item, (int, float)):
                formatted.append(str(item))
            else:
                formatted.append(str(item))
        return ",".join(formatted)

    @staticmethod
    def _create_backup(plan_path: Path) -> None:
        """Create timestamped backup of plan file."""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = plan_path.parent / f"{plan_path.stem}_backup_{timestamp}{plan_path.suffix}"
        backup_path.write_text(plan_path.read_text())
        logger.info(f"Created backup: {backup_path.name}")

    @staticmethod
    def _validate_crlf(plan_path: Path) -> bool:
        """Validate that file has CRLF line endings."""
        content = plan_path.read_bytes()
        # Check if file contains \r\n (CRLF)
        return b'\r\n' in content

==================================================

File: C:\GH\ras-commander\ras_commander\RasCmdr.py
==================================================
"""
RasCmdr - Execution operations for running HEC-RAS simulations

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function():
        
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasCmdr:
- compute_plan()
- compute_parallel()
- compute_test_mode()
        
        
        
"""
import os
import subprocess
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from .RasPrj import ras, RasPrj, init_ras_project, get_ras_exe
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasUtils import RasUtils
import logging
import time
import queue
from threading import Thread, Lock
from typing import Union, List, Optional, Dict
from pathlib import Path
import shutil
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Thread
from itertools import cycle
from ras_commander.RasPrj import RasPrj  # Ensure RasPrj is imported
from threading import Lock, Thread, current_thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import cycle
from typing import Union, List, Optional, Dict
from numbers import Number
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

# Module code starts here



class RasCmdr:
    
    @staticmethod
    @log_call
    def compute_plan(
        plan_number: Union[str, Number, Path],
        dest_folder=None,
        ras_object=None,
        clear_geompre=False,
        num_cores=None,
        overwrite_dest=False
    ):
        """
        Execute a single HEC-RAS plan in a specified location.

        This function runs a HEC-RAS plan by launching the HEC-RAS executable through command line,
        allowing for destination folder specification, core count control, and geometry preprocessor management.

        Args:
            plan_number (Union[str, Number, Path]): The plan number to execute (e.g., "01", 1, 1.0) or the full path to the plan file.
                Recommended to use two-digit strings for plan numbers for consistency (e.g., "01" instead of 1).
            dest_folder (str, Path, optional): Name of the folder or full path for computation.
                If a string is provided, it will be created in the same parent directory as the project folder.
                If a full path is provided, it will be used as is.
                If None, computation occurs in the original project folder, modifying the original project.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
                Useful when working with multiple projects simultaneously.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.
                Set to True when geometry has been modified to force recomputation of preprocessor files.
            num_cores (int, optional): Number of cores to use for the plan execution. 
                If None, the current setting in the plan file is not changed.
                Generally, 2-4 cores provides good performance for most models.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. Defaults to False.
                Set to True to replace an existing destination folder with the same name.

        Returns:
            bool: True if the execution was successful, False otherwise.

        Raises:
            ValueError: If the specified dest_folder already exists and is not empty, and overwrite_dest is False.
            FileNotFoundError: If the plan file or project file cannot be found.
            PermissionError: If there are issues accessing or writing to the destination folder.
            subprocess.CalledProcessError: If the HEC-RAS execution fails.

        Examples:
            # Run a plan in the original project folder
            RasCmdr.compute_plan("01")
            
            # Run a plan in a separate folder
            RasCmdr.compute_plan("01", dest_folder="computation_folder")
            
            # Run a plan with a specific number of cores
            RasCmdr.compute_plan("01", num_cores=4)
            
            # Run a plan in a specific folder, overwriting if it exists
            RasCmdr.compute_plan("01", dest_folder="computation_folder", overwrite_dest=True)
            
            # Run a plan in a specific folder with multiple options
            RasCmdr.compute_plan(
                "01", 
                dest_folder="computation_folder",
                num_cores=2,
                clear_geompre=True,
                overwrite_dest=True
            )
            
        Notes:
            - For executing multiple plans, consider using compute_parallel() or compute_test_mode().
            - Setting num_cores appropriately is important for performance:
              * 1-2 cores: Highest efficiency per core, good for small models
              * 3-8 cores: Good balance for most models
              * >8 cores: May have diminishing returns due to overhead
            - This function updates the RAS object's dataframes (plan_df, geom_df, etc.) after execution.
        """
        try:
            ras_obj = ras_object if ras_object is not None else ras
            logger.info(f"Using ras_object with project folder: {ras_obj.project_folder}")
            ras_obj.check_initialized()
            
            if dest_folder is not None:
                dest_folder = Path(ras_obj.project_folder).parent / dest_folder if isinstance(dest_folder, str) else Path(dest_folder)
                
                if dest_folder.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder)
                        logger.info(f"Destination folder '{dest_folder}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder.iterdir()):
                        error_msg = f"Destination folder '{dest_folder}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                
                dest_folder.mkdir(parents=True, exist_ok=True)
                shutil.copytree(ras_obj.project_folder, dest_folder, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder}")
                
                compute_ras = RasPrj()
                compute_ras.initialize(dest_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
            else:
                compute_ras = ras_obj
                compute_prj_path = ras_obj.prj_file

            # Determine the plan path
            compute_plan_path = Path(plan_number) if isinstance(plan_number, (str, Path)) and Path(plan_number).is_file() else RasPlan.get_plan_path(plan_number, compute_ras)

            if not compute_prj_path or not compute_plan_path:
                logger.error(f"Could not find project file or plan file for plan {plan_number}")
                return False

            # Clear geometry preprocessor files if requested
            if clear_geompre:
                try:
                    RasGeo.clear_geompre_files(compute_plan_path, ras_object=compute_ras)
                    logger.info(f"Cleared geometry preprocessor files for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error clearing geometry preprocessor files for plan {plan_number}: {str(e)}")

            # Set the number of cores if specified
            if num_cores is not None:
                try:
                    RasPlan.set_num_cores(compute_plan_path, num_cores=num_cores, ras_object=compute_ras)
                    logger.info(f"Set number of cores to {num_cores} for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error setting number of cores for plan {plan_number}: {str(e)}")

            # Prepare the command for HEC-RAS execution
            cmd = f'"{compute_ras.ras_exe_path}" -c "{compute_prj_path}" "{compute_plan_path}"'
            logger.info("Running HEC-RAS from the Command Line:")
            logger.info(f"Running command: {cmd}")

            # Execute the HEC-RAS command
            start_time = time.time()
            try:
                subprocess.run(cmd, check=True, shell=True, capture_output=True, text=True)
                end_time = time.time()
                run_time = end_time - start_time
                logger.info(f"HEC-RAS execution completed for plan: {plan_number}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return True
            except subprocess.CalledProcessError as e:
                end_time = time.time()
                run_time = end_time - start_time
                logger.error(f"Error running plan: {plan_number}")
                logger.error(f"Error message: {e.output}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return False
        except Exception as e:
            logger.critical(f"Error in compute_plan: {str(e)}")
            return False
        finally:
            # Update the RAS object's dataframes
            if ras_obj:
                ras_obj.plan_df = ras_obj.get_plan_entries()
                ras_obj.geom_df = ras_obj.get_geom_entries()
                ras_obj.flow_df = ras_obj.get_flow_entries()
                ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
    


    @staticmethod
    @log_call
    def compute_parallel(
        plan_number: Union[str, Number, List[Union[str, Number]], None] = None,
        max_workers: int = 2,
        num_cores: int = 2,
        clear_geompre: bool = False,
        ras_object: Optional['RasPrj'] = None,
        dest_folder: Union[str, Path, None] = None,
        overwrite_dest: bool = False
    ) -> Dict[str, bool]:
        """
        Execute multiple HEC-RAS plans in parallel using multiple worker instances.

        This method creates separate worker folders for each parallel process, runs plans
        in those folders, and then consolidates results to a final destination folder.
        It's ideal for running independent plans simultaneously to make better use of system resources.

        Args:
            plan_number (Union[str, List[str], None]): Plan number(s) to compute. 
                If None, all plans in the project are computed.
                If string, only that plan will be computed.
                If list, all specified plans will be computed.
                Recommended to use two-digit strings for plan numbers for consistency (e.g., "01" instead of 1).
            max_workers (int): Maximum number of parallel workers (separate HEC-RAS instances).
                Each worker gets a separate folder with a copy of the project.
                Optimal value depends on CPU cores and memory available.
                A good starting point is: max_workers = floor(physical_cores / num_cores).
            num_cores (int): Number of cores to use per plan computation.
                Controls computational resources allocated to each individual HEC-RAS instance.
                For parallel execution, 2-4 cores per worker often provides the best balance.
            clear_geompre (bool): Whether to clear geometry preprocessor files before computation.
                Set to True when geometry has been modified to force recomputation.
            ras_object (Optional[RasPrj]): RAS project object. If None, uses global 'ras' instance.
                Useful when working with multiple projects simultaneously.
            dest_folder (Union[str, Path, None]): Destination folder for computed results.
                If None, creates a "[Computed]" folder adjacent to the project folder.
                If string, creates folder in the project's parent directory.
                If Path, uses the exact path provided.
            overwrite_dest (bool): Whether to overwrite existing destination folder.
                Set to True to replace an existing destination folder with the same name.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.
                Keys are plan numbers and values are boolean success indicators.

        Raises:
            ValueError: If the destination folder already exists, is not empty, and overwrite_dest is False.
            FileNotFoundError: If project files cannot be found.
            PermissionError: If there are issues accessing or writing to folders.
            RuntimeError: If worker initialization fails.

        Examples:
            # Run all plans in parallel with default settings
            RasCmdr.compute_parallel()
            
            # Run all plans with 4 workers, 2 cores per worker
            RasCmdr.compute_parallel(max_workers=4, num_cores=2)
            
            # Run specific plans in parallel
            RasCmdr.compute_parallel(plan_number=["01", "03"], max_workers=2)
            
            # Run all plans with dynamic worker allocation based on system resources
            import psutil
            physical_cores = psutil.cpu_count(logical=False)
            cores_per_worker = 2
            max_workers = max(1, physical_cores // cores_per_worker)
            RasCmdr.compute_parallel(max_workers=max_workers, num_cores=cores_per_worker)
            
            # Run all plans in a specific destination folder
            RasCmdr.compute_parallel(dest_folder="parallel_results", overwrite_dest=True)

        Notes:
            - Worker Assignment: Plans are assigned to workers in a round-robin fashion.
              For example, with 3 workers and 5 plans, assignment would be:
              Worker 1: Plans 1 & 4, Worker 2: Plans 2 & 5, Worker 3: Plan 3.
            
            - Resource Management: Each HEC-RAS instance (worker) typically requires:
              * 2-4 GB of RAM
              * 2-4 cores for optimal performance
            
            - When to use parallel vs. sequential:
              * Parallel: For independent plans, faster overall completion
              * Sequential: For dependent plans, consistent resource usage, easier debugging
            
            - The function creates worker folders during execution and consolidates results
              to the destination folder upon completion.
              
            - This function updates the RAS object's dataframes (plan_df, geom_df, etc.) after execution.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()

            project_folder = Path(ras_obj.project_folder)

            if dest_folder is not None:
                dest_folder_path = Path(dest_folder)
                if dest_folder_path.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder_path)
                        logger.info(f"Destination folder '{dest_folder_path}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder_path.iterdir()):
                        error_msg = f"Destination folder '{dest_folder_path}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                dest_folder_path.mkdir(parents=True, exist_ok=True)
                shutil.copytree(project_folder, dest_folder_path, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder_path}")
                project_folder = dest_folder_path

            # Store filtered plan numbers separately to ensure only these are executed
            filtered_plan_numbers = []

            if plan_number:
                if isinstance(plan_number, (str, Number)):
                    plan_number = [plan_number]
                ras_obj.plan_df = ras_obj.plan_df[ras_obj.plan_df['plan_number'].isin(plan_number)]
                filtered_plan_numbers = list(ras_obj.plan_df['plan_number'])
                logger.info(f"Filtered plans to execute: {filtered_plan_numbers}")
            else:
                filtered_plan_numbers = list(ras_obj.plan_df['plan_number'])

            num_plans = len(ras_obj.plan_df)
            max_workers = min(max_workers, num_plans) if num_plans > 0 else 1
            logger.info(f"Adjusted max_workers to {max_workers} based on the number of plans: {num_plans}")

            worker_ras_objects = {}
            for worker_id in range(1, max_workers + 1):
                worker_folder = project_folder.parent / f"{project_folder.name} [Worker {worker_id}]"
                if worker_folder.exists():
                    shutil.rmtree(worker_folder)
                    logger.info(f"Removed existing worker folder: {worker_folder}")
                shutil.copytree(project_folder, worker_folder)
                logger.info(f"Created worker folder: {worker_folder}")

                try:
                    worker_ras = RasPrj()
                    worker_ras_object = init_ras_project(
                        ras_project_folder=worker_folder,
                        ras_version=ras_obj.ras_exe_path,
                        ras_object=worker_ras
                    )
                    worker_ras_objects[worker_id] = worker_ras_object
                except Exception as e:
                    logger.critical(f"Failed to initialize RAS project for worker {worker_id}: {str(e)}")
                    worker_ras_objects[worker_id] = None

            # Explicitly use the filtered plan numbers for assignments
            worker_cycle = cycle(range(1, max_workers + 1))
            plan_assignments = [(next(worker_cycle), plan_num) for plan_num in filtered_plan_numbers]

            execution_results: Dict[str, bool] = {}

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(
                        RasCmdr.compute_plan,
                        plan_num, 
                        ras_object=worker_ras_objects[worker_id], 
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    for worker_id, plan_num in plan_assignments
                ]

                for future, (worker_id, plan_num) in zip(as_completed(futures), plan_assignments):
                    try:
                        success = future.result()
                        execution_results[plan_num] = success
                        logger.info(f"Plan {plan_num} executed in worker {worker_id}: {'Successful' if success else 'Failed'}")
                    except Exception as e:
                        execution_results[plan_num] = False
                        logger.error(f"Plan {plan_num} failed in worker {worker_id}: {str(e)}")

            final_dest_folder = dest_folder_path if dest_folder is not None else project_folder.parent / f"{project_folder.name} [Computed]"
            final_dest_folder.mkdir(parents=True, exist_ok=True)
            logger.info(f"Final destination for computed results: {final_dest_folder}")

            for worker_ras in worker_ras_objects.values():
                if worker_ras is None:
                    continue
                worker_folder = Path(worker_ras.project_folder)
                try:
                    # First, close any open resources in the worker RAS object
                    worker_ras.close() if hasattr(worker_ras, 'close') else None
                    
                    # Add a small delay to ensure file handles are released
                    time.sleep(1)
                    
                    # Move files with retry mechanism
                    max_retries = 3
                    for retry in range(max_retries):
                        try:
                            for item in worker_folder.iterdir():
                                dest_path = final_dest_folder / item.name
                                if dest_path.exists():
                                    if dest_path.is_dir():
                                        shutil.rmtree(dest_path)
                                    else:
                                        dest_path.unlink()
                                # Use copy instead of move for more reliability
                                if item.is_dir():
                                    shutil.copytree(item, dest_path)
                                else:
                                    shutil.copy2(item, dest_path)
                            
                            # Add another small delay before removal
                            time.sleep(1)
                            
                            # Try to remove the worker folder
                            if worker_folder.exists():
                                shutil.rmtree(worker_folder)
                            break  # If successful, break the retry loop
                            
                        except PermissionError as pe:
                            if retry == max_retries - 1:  # If this was the last retry
                                logger.error(f"Failed to move/remove files after {max_retries} attempts: {str(pe)}")
                                raise
                            time.sleep(2 ** retry)  # Exponential backoff
                            continue
                            
                except Exception as e:
                    logger.error(f"Error moving results from {worker_folder} to {final_dest_folder}: {str(e)}")

            try:
                final_dest_folder_ras = RasPrj()
                final_dest_folder_ras_obj = init_ras_project(
                    ras_project_folder=final_dest_folder, 
                    ras_version=ras_obj.ras_exe_path,
                    ras_object=final_dest_folder_ras
                )
                final_dest_folder_ras_obj.check_initialized()
            except Exception as e:
                logger.critical(f"Failed to initialize RasPrj for final destination: {str(e)}")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj = ras_object or ras
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_parallel: {str(e)}")
            return {}

    @staticmethod
    @log_call
    def compute_test_mode(
        plan_number: Union[str, Number, List[Union[str, Number]], None] = None,
        dest_folder_suffix="[Test]",
        clear_geompre=False,
        num_cores=None,
        ras_object=None,
        overwrite_dest=False
    ):
        """
        Execute HEC-RAS plans sequentially in a separate test folder.

        This function creates a separate test folder, copies the project there, and executes
        the specified plans in sequential order. It's useful for batch processing plans that 
        need to be run in a specific order or when you want to ensure consistent resource usage.

        Args:
            plan_number (Union[str, Number, List[Union[str, Number]], None], optional): Plan number or list of plan numbers to execute (e.g., "01", 1, 1.0, or ["01", 2]). 
                If None, all plans will be executed. Default is None.
                Recommended to use two-digit strings for plan numbers for consistency (e.g., "01" instead of 1).
            dest_folder_suffix (str, optional): Suffix to append to the test folder name. 
                Defaults to "[Test]".
                The test folder is always created in the project folder's parent directory.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files.
                Defaults to False.
                Set to True when geometry has been modified to force recomputation.
            num_cores (int, optional): Number of cores to use for each plan.
                If None, the current setting in the plan file is not changed. Default is None.
                For sequential execution, 4-8 cores often provides good performance.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
                Useful when working with multiple projects simultaneously.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. 
                Defaults to False.
                Set to True to replace an existing test folder with the same name.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.
                Keys are plan numbers and values are boolean success indicators.

        Raises:
            ValueError: If the destination folder already exists, is not empty, and overwrite_dest is False.
            FileNotFoundError: If project files cannot be found.
            PermissionError: If there are issues accessing or writing to folders.

        Examples:
            # Run all plans sequentially
            RasCmdr.compute_test_mode()
            
            # Run a specific plan
            RasCmdr.compute_test_mode(plan_number="01")
            
            # Run multiple specific plans
            RasCmdr.compute_test_mode(plan_number=["01", "03", "05"])
            
            # Run plans with a custom folder suffix
            RasCmdr.compute_test_mode(dest_folder_suffix="[SequentialRun]")
            
            # Run plans with a specific number of cores
            RasCmdr.compute_test_mode(num_cores=4)
            
            # Run specific plans with multiple options
            RasCmdr.compute_test_mode(
                plan_number=["01", "02"],
                dest_folder_suffix="[SpecificSequential]",
                clear_geompre=True,
                num_cores=6,
                overwrite_dest=True
            )

        Notes:
            - This function was created to replicate the original HEC-RAS command line -test flag,
              which does not work in recent versions of HEC-RAS.
            
            - Key differences from other compute functions:
              * compute_plan: Runs a single plan, with option for destination folder
              * compute_parallel: Runs multiple plans simultaneously in worker folders
              * compute_test_mode: Runs multiple plans sequentially in a single test folder
            
            - Use cases:
              * Running plans in a specific order
              * Ensuring consistent resource usage
              * Easier debugging (one plan at a time)
              * Isolated test environment
            
            - Performance considerations:
              * Sequential execution is generally slower overall than parallel execution
              * Each plan gets consistent resource usage
              * Execution time scales linearly with the number of plans
            
            - This function updates the RAS object's dataframes (plan_df, geom_df, etc.) after execution.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()
            
            logger.info("Starting the compute_test_mode...")
               
            project_folder = Path(ras_obj.project_folder)

            if not project_folder.exists():
                logger.error(f"Project folder '{project_folder}' does not exist.")
                return {}

            compute_folder = project_folder.parent / f"{project_folder.name} {dest_folder_suffix}"
            logger.info(f"Creating the test folder: {compute_folder}...")

            if compute_folder.exists():
                if overwrite_dest:
                    shutil.rmtree(compute_folder)
                    logger.info(f"Compute folder '{compute_folder}' exists. Overwriting as per overwrite_dest=True.")
                elif any(compute_folder.iterdir()):
                    error_msg = (
                        f"Compute folder '{compute_folder}' exists and is not empty. "
                        "Use overwrite_dest=True to overwrite."
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg)

            try:
                shutil.copytree(project_folder, compute_folder)
                logger.info(f"Copied project folder to compute folder: {compute_folder}")
            except Exception as e:
                logger.critical(f"Error occurred while copying project folder: {str(e)}")
                return {}

            try:
                compute_ras = RasPrj()
                compute_ras.initialize(compute_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
                logger.info(f"Initialized RAS project in compute folder: {compute_prj_path}")
            except Exception as e:
                logger.critical(f"Error initializing RAS project in compute folder: {str(e)}")
                return {}

            if not compute_prj_path:
                logger.error("Project file not found.")
                return {}

            logger.info("Getting plan entries...")
            try:
                ras_compute_plan_entries = compute_ras.plan_df
                logger.info("Retrieved plan entries successfully.")
            except Exception as e:
                logger.critical(f"Error retrieving plan entries: {str(e)}")
                return {}

            if plan_number:
                if isinstance(plan_number, (str, Number)):
                    plan_number = [plan_number]
                ras_compute_plan_entries = ras_compute_plan_entries[
                    ras_compute_plan_entries['plan_number'].isin(plan_number)
                ]
                logger.info(f"Filtered plans to execute: {plan_number}")

            execution_results = {}
            logger.info("Running selected plans sequentially...")
            for _, plan in ras_compute_plan_entries.iterrows():
                plan_number = plan["plan_number"]
                start_time = time.time()
                try:
                    success = RasCmdr.compute_plan(
                        plan_number,
                        ras_object=compute_ras,
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    execution_results[plan_number] = success
                    if success:
                        logger.info(f"Successfully computed plan {plan_number}")
                    else:
                        logger.error(f"Failed to compute plan {plan_number}")
                except Exception as e:
                    execution_results[plan_number] = False
                    logger.error(f"Error computing plan {plan_number}: {str(e)}")
                finally:
                    end_time = time.time()
                    run_time = end_time - start_time
                    logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")

            logger.info("All selected plans have been executed.")
            logger.info("compute_test_mode completed.")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_test_mode: {str(e)}")
            return {}
==================================================

File: C:\GH\ras-commander\ras_commander\RasControl.py
==================================================
"""
RasControl - HECRASController API Wrapper (ras-commander style)

Provides ras-commander style API for legacy HEC-RAS versions (3.x-4.x)
that use HECRASController COM interface instead of HDF files.

Includes robust process management with session tracking, orphan detection,
and optional watchdog protection for Jupyter kernel restarts.

Public functions (HEC-RAS Operations):
- RasControl.run_plan(plan, ras_object=None, force_recompute=False, use_watchdog=True, max_runtime=3600) -> Tuple[bool, List[str]]
- RasControl.get_steady_results(plan, ras_object=None) -> pandas.DataFrame
- RasControl.get_unsteady_results(plan, max_times=None, ras_object=None) -> pandas.DataFrame
- RasControl.get_output_times(plan, ras_object=None) -> List[str]
- RasControl.get_plans(plan, ras_object=None) -> List[dict]
- RasControl.set_current_plan(plan, ras_object=None) -> bool
- RasControl.get_comp_msgs(plan, ras_object=None) -> str

Public functions (Process Management):
- RasControl.list_processes(show_all=False) -> pandas.DataFrame
- RasControl.scan_orphans() -> List[SessionLock]
- RasControl.cleanup_orphans(interactive=True, dry_run=False) -> int
- RasControl.force_cleanup_all() -> int

Private functions:
- _terminate_ras_process() -> None
- _is_ras_running() -> bool
- RasControl._normalize_version(version: str) -> str
- RasControl._get_project_info(plan, ras_object=None) -> Tuple[Path, str, Optional[str], Optional[str]]
- RasControl._com_open_close(project_path: Path, version: str, operation_func: Callable[[Any], Any]) -> Any

Session tracking infrastructure:
- SessionLock dataclass - Tracks active COM sessions with lock files
- Module-level _active_sessions dict - Tracks all active sessions
- atexit handler - Emergency cleanup on Python exit
- Watchdog support - Optional independent process for kernel restart protection

"""

import win32com.client
import psutil
import pandas as pd
from pathlib import Path
from typing import Optional, List, Tuple, Callable, Any, Union, Dict
import logging
import time
import json
import socket
import tempfile
import uuid
import atexit
import sys
import subprocess
import os
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

# Import ras-commander components
from .RasPrj import ras


# ============================================================================
# SESSION TRACKING INFRASTRUCTURE
# ============================================================================

@dataclass
class SessionLock:
    """
    Represents a tracked RasControl session for process cleanup.

    Stored as JSON in temp directory to track active COM sessions and enable
    orphan detection after crashes/kernel restarts.
    """
    python_pid: int              # Python process PID
    ras_pid: Optional[int]       # ras.exe PID (None if couldn't detect)
    project_path: str            # Absolute path to .prj file
    ras_version: str             # HEC-RAS version (e.g., "6.5")
    session_id: str              # Unique session UUID
    start_time: float            # time.time() when session started
    python_exe: str              # sys.executable
    hostname: str                # socket.gethostname()
    detection_confidence: int    # 0-100 score from PID detection

    def to_json(self) -> str:
        """Serialize to JSON string."""
        return json.dumps(asdict(self), indent=2)

    @classmethod
    def from_json(cls, data: str) -> 'SessionLock':
        """Deserialize from JSON string."""
        return cls(**json.loads(data))

    @classmethod
    def from_file(cls, path: Path) -> 'SessionLock':
        """Load from lock file."""
        return cls.from_json(path.read_text(encoding='utf-8'))


# Module-level session tracking
_active_sessions: Dict[str, SessionLock] = {}  # {session_id: SessionLock}

# Lock file directory
LOCK_DIR = Path(tempfile.gettempdir()) / "rascontrol_sessions"
LOCK_DIR.mkdir(exist_ok=True)


def _get_lock_file_path(session_id: str) -> Path:
    """Generate lock file path for a session."""
    filename = f"rasctl_{os.getpid()}_{session_id}.lock"
    return LOCK_DIR / filename


def _find_our_ras_process(project_path: Path, before_snapshot: Dict[int, Any]) -> Tuple[Optional[int], int]:
    """
    Multi-strategy detection to find the ras.exe process we just launched.

    Args:
        project_path: Path to .prj file being opened
        before_snapshot: Dict of {pid: proc_info} before COM launch

    Returns:
        Tuple of (pid, confidence_score). PID is None if detection failed.
        Confidence score is 0-100.
    """
    time.sleep(0.3)  # Give process time to appear

    candidates = {}  # {pid: confidence_score}

    try:
        after = {
            p.pid: p.info
            for p in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time'])
            if p.info['name'] and p.info['name'].lower() == 'ras.exe'
        }
    except Exception as e:
        logger.warning(f"Error scanning for ras.exe processes: {e}")
        return None, 0

    new_pids = set(after.keys()) - set(before_snapshot.keys())

    for pid in after.keys():
        proc_info = after[pid]
        score = 0

        # Criteria 1: Newly appeared (50 points)
        if pid in new_pids:
            score += 50

        # Criteria 2: Project path in cmdline (40 points)
        try:
            cmdline = ' '.join(proc_info['cmdline'] or [])
            if str(project_path) in cmdline or project_path.name in cmdline:
                score += 40
        except (TypeError, AttributeError):
            pass

        # Criteria 3: Very recent creation time (30 points)
        try:
            age = time.time() - proc_info['create_time']
            if age < 2.0:  # Created within 2 seconds
                score += 30
        except (TypeError, KeyError):
            pass

        # Criteria 4: Only one new process (20 points)
        if len(new_pids) == 1 and pid in new_pids:
            score += 20

        if score > 0:
            candidates[pid] = score

    if not candidates:
        logger.warning(f"Could not reliably identify ras.exe PID for {project_path.name}")
        return None, 0

    # Return highest confidence PID
    best_pid = max(candidates, key=candidates.get)
    confidence = candidates[best_pid]

    if confidence < 50:
        logger.warning(f"Low confidence ({confidence}/100) for PID {best_pid}")
    else:
        logger.info(f"Detected ras.exe PID {best_pid} (confidence: {confidence}/100)")

    return best_pid, confidence


def _classify_lock_file(lock: SessionLock) -> str:
    """
    Classify lock file state.

    Returns:
        'active' - Python still running, session active
        'stale_orphan' - Python dead, ras.exe still running
        'stale_clean' - Both dead, safe to delete
        'foreign_machine' - From different machine, don't touch
    """
    # Check 1: Different machine?
    if lock.hostname != socket.gethostname():
        return 'foreign_machine'

    # Check 2: Is Python process still running?
    python_alive = False
    try:
        python_proc = psutil.Process(lock.python_pid)
        if python_proc.is_running():
            # Verify it's actually Python (not PID reuse)
            if 'python' in python_proc.name().lower():
                python_alive = True
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        pass

    if python_alive:
        return 'active'

    # Check 3: Is ras.exe still running?
    if lock.ras_pid is not None:
        try:
            ras_proc = psutil.Process(lock.ras_pid)
            if ras_proc.is_running() and ras_proc.name().lower() == 'ras.exe':
                # Verify it's working on our project (if cmdline available)
                try:
                    cmdline = ' '.join(ras_proc.cmdline() or [])
                    if lock.project_path in cmdline or Path(lock.project_path).name in cmdline:
                        return 'stale_orphan'  # Orphaned process!
                except (psutil.AccessDenied, psutil.NoSuchProcess):
                    pass
                # Couldn't verify project, but ras.exe exists - assume orphan if Python dead
                return 'stale_orphan'
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass

    return 'stale_clean'


def _create_session_lock(session_id: str, lock_data: SessionLock) -> Path:
    """Create a lock file for the session."""
    lock_path = _get_lock_file_path(session_id)
    try:
        lock_path.write_text(lock_data.to_json(), encoding='utf-8')
        logger.debug(f"Created session lock: {lock_path.name}")
        return lock_path
    except Exception as e:
        logger.warning(f"Failed to create session lock file: {e}")
        return lock_path


def _remove_session_lock(session_id: str) -> None:
    """Remove a session lock file."""
    lock_path = _get_lock_file_path(session_id)
    try:
        lock_path.unlink(missing_ok=True)
        logger.debug(f"Removed session lock: {lock_path.name}")
    except Exception as e:
        logger.warning(f"Failed to remove session lock file: {e}")


def _cleanup_session(session_id: str) -> None:
    """Clean up a specific session."""
    if session_id in _active_sessions:
        lock = _active_sessions[session_id]

        # Try to terminate the ras.exe process gracefully
        if lock.ras_pid:
            try:
                proc = psutil.Process(lock.ras_pid)
                if proc.is_running() and proc.name().lower() == 'ras.exe':
                    logger.info(f"Terminating tracked ras.exe PID {lock.ras_pid}")
                    proc.terminate()
                    proc.wait(timeout=5)
            except (psutil.NoSuchProcess, psutil.TimeoutExpired, psutil.AccessDenied) as e:
                logger.debug(f"Could not terminate PID {lock.ras_pid}: {e}")

        # Remove from tracking
        del _active_sessions[session_id]

        # Remove lock file
        _remove_session_lock(session_id)


def _emergency_cleanup_all() -> None:
    """
    Emergency cleanup of all tracked sessions.
    Called by atexit handler.
    """
    if not _active_sessions:
        return

    logger.info(f"Emergency cleanup: {len(_active_sessions)} active session(s)")

    for session_id in list(_active_sessions.keys()):
        _cleanup_session(session_id)


def _spawn_watchdog(parent_pid: int, ras_pid: int, max_runtime: int,
                    lock_file_path: Path) -> int:
    """
    Spawn independent watchdog process for long-running operations.

    The watchdog monitors for:
    1. Parent Python process death (orphan detection)
    2. Runtime timeout
    3. Manual cancellation via lock file deletion

    Returns:
        Watchdog process PID
    """
    watchdog_script = f"""
import psutil
import time
import sys
from pathlib import Path

PARENT_PID = {parent_pid}
RAS_PID = {ras_pid}
MAX_RUNTIME = {max_runtime}
LOCK_FILE = Path({str(lock_file_path)!r})
CHECK_INTERVAL = 5  # seconds

start_time = time.time()

while True:
    time.sleep(CHECK_INTERVAL)

    # Check 1: Parent Python still alive?
    try:
        parent = psutil.Process(PARENT_PID)
        if not parent.is_running():
            # Parent died, orphan detected
            print(f"[Watchdog] Parent {{PARENT_PID}} died, terminating ras.exe {{RAS_PID}}", flush=True)
            try:
                ras = psutil.Process(RAS_PID)
                ras.terminate()
                ras.wait(timeout=10)
            except:
                pass
            LOCK_FILE.unlink(missing_ok=True)
            sys.exit(0)
    except psutil.NoSuchProcess:
        # Parent already gone
        try:
            ras = psutil.Process(RAS_PID)
            ras.terminate()
            ras.wait(timeout=10)
        except:
            pass
        LOCK_FILE.unlink(missing_ok=True)
        sys.exit(0)

    # Check 2: Timeout exceeded?
    if time.time() - start_time > MAX_RUNTIME:
        print(f"[Watchdog] Timeout exceeded, terminating ras.exe {{RAS_PID}}", flush=True)
        try:
            ras = psutil.Process(RAS_PID)
            ras.terminate()
            ras.wait(timeout=10)
        except:
            pass
        LOCK_FILE.unlink(missing_ok=True)
        sys.exit(0)

    # Check 3: Lock file deleted? (manual cancel signal)
    if not LOCK_FILE.exists():
        print(f"[Watchdog] Lock file deleted, assuming manual cleanup", flush=True)
        sys.exit(0)
"""

    try:
        # Launch watchdog as completely independent process
        creationflags = subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
        proc = subprocess.Popen(
            [sys.executable, '-c', watchdog_script],
            creationflags=creationflags,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )

        logger.info(f"Spawned watchdog process PID {proc.pid} (monitoring PID {ras_pid})")
        return proc.pid
    except Exception as e:
        logger.error(f"Failed to spawn watchdog process: {e}")
        return 0


def _terminate_watchdog(watchdog_pid: int) -> None:
    """Terminate a watchdog process."""
    if watchdog_pid == 0:
        return

    try:
        proc = psutil.Process(watchdog_pid)
        proc.terminate()
        proc.wait(timeout=3)
        logger.debug(f"Terminated watchdog process PID {watchdog_pid}")
    except (psutil.NoSuchProcess, psutil.TimeoutExpired, psutil.AccessDenied):
        pass


# Register atexit cleanup handler
atexit.register(_emergency_cleanup_all)


# ============================================================================
# LEGACY PROCESS TERMINATION FUNCTIONS (kept for compatibility)
# ============================================================================

def _terminate_ras_process() -> None:
    """Force terminate any running ras.exe processes."""
    for proc in psutil.process_iter(['name']):
        try:
            if proc.info['name'] and proc.info['name'].lower() == 'ras.exe':
                proc.terminate()
                proc.wait(timeout=3)
                logger.info("Terminated ras.exe process")
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
            pass


def _is_ras_running() -> bool:
    """Check if HEC-RAS is currently running"""
    for proc in psutil.process_iter(['name']):
        try:
            if proc.info['name'] and proc.info['name'].lower() == 'ras.exe':
                return True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            pass
    return False


class RasControl:
    """
    HECRASController API wrapper with ras-commander style interface.

    Works with legacy HEC-RAS versions (3.x-4.x) that use COM interface
    instead of HDF files. Integrates with ras-commander project management.

    Usage (ras-commander style):
        >>> from ras_commander import init_ras_project, RasControl
        >>>
        >>> # Initialize with version (with or without periods)
        >>> init_ras_project(path, "4.1")  # or "41"
        >>>
        >>> # Use plan numbers like HDF methods
        >>> RasControl.run_plan("02")
        >>> df = RasControl.get_steady_results("02")

    Supported Versions:
        All installed versions: 3.x, 4.x, 5.0.x, 6.0-6.7+
        Accepts formats: "4.1", "41", "5.0.6", "506", "6.6", "66", etc.
    """

    # Version mapping based on ACTUAL COM interfaces registered on system
    # Only these COM interfaces exist: RAS41, RAS503, RAS505, RAS506, RAS507,
    # RAS60, RAS631, RAS641, RAS65, RAS66, RAS67
    # Other versions use nearest available fallback
    VERSION_MAP = {
        # HEC-RAS 3.x → Use 4.1 (3.x COM not registered)
        '3.0': 'RAS41.HECRASController',
        '30': 'RAS41.HECRASController',
        '3.1': 'RAS41.HECRASController',
        '31': 'RAS41.HECRASController',
        '3.1.1': 'RAS41.HECRASController',
        '311': 'RAS41.HECRASController',
        '3.1.2': 'RAS41.HECRASController',
        '312': 'RAS41.HECRASController',
        '3.1.3': 'RAS41.HECRASController',
        '313': 'RAS41.HECRASController',

        # HEC-RAS 4.x
        '4.0': 'RAS41.HECRASController',    # Use 4.1 (4.0 COM not registered)
        '40': 'RAS41.HECRASController',
        '4.1': 'RAS41.HECRASController',    # ✓ EXISTS
        '41': 'RAS41.HECRASController',
        '4.1.0': 'RAS41.HECRASController',
        '410': 'RAS41.HECRASController',

        # HEC-RAS 5.0.x
        '5.0': 'RAS503.HECRASController',   # Use 5.0.3 (RAS50 COM not registered)
        '50': 'RAS503.HECRASController',
        '5.0.1': 'RAS501.HECRASController', # ✓ EXISTS
        '501': 'RAS501.HECRASController',
        '5.0.3': 'RAS503.HECRASController', # ✓ EXISTS
        '503': 'RAS503.HECRASController',
        '5.0.4': 'RAS504.HECRASController', # ✓ EXISTS (newly installed)
        '504': 'RAS504.HECRASController',
        '5.0.5': 'RAS505.HECRASController', # ✓ EXISTS
        '505': 'RAS505.HECRASController',
        '5.0.6': 'RAS506.HECRASController', # ✓ EXISTS
        '506': 'RAS506.HECRASController',
        '5.0.7': 'RAS507.HECRASController', # ✓ EXISTS
        '507': 'RAS507.HECRASController',

        # HEC-RAS 6.x
        '6.0': 'RAS60.HECRASController',    # ✓ EXISTS
        '60': 'RAS60.HECRASController',
        '6.1': 'RAS60.HECRASController',    # Use 6.0 (6.1 COM not registered)
        '61': 'RAS60.HECRASController',
        '6.2': 'RAS60.HECRASController',    # Use 6.0 (6.2 COM not registered)
        '62': 'RAS60.HECRASController',
        '6.3': 'RAS631.HECRASController',   # Use 6.3.1 (6.3 COM not registered)
        '63': 'RAS631.HECRASController',
        '6.3.1': 'RAS631.HECRASController', # ✓ EXISTS
        '631': 'RAS631.HECRASController',
        '6.4': 'RAS641.HECRASController',   # Use 6.4.1 (6.4 COM not registered)
        '64': 'RAS641.HECRASController',
        '6.4.1': 'RAS641.HECRASController', # ✓ EXISTS
        '641': 'RAS641.HECRASController',
        '6.5': 'RAS65.HECRASController',    # ✓ EXISTS
        '65': 'RAS65.HECRASController',
        '6.6': 'RAS66.HECRASController',    # ✓ EXISTS
        '66': 'RAS66.HECRASController',
        '6.7': 'RAS67.HECRASController',    # ✓ EXISTS
        '67': 'RAS67.HECRASController',
        '6.7 Beta 4': 'RAS67.HECRASController',
    }

    # Legacy reference (kept for backwards compatibility)
    SUPPORTED_VERSIONS = VERSION_MAP

    # Output variable codes
    WSEL = 2
    ENERGY = 3
    MAX_CHL_DPTH = 4
    MIN_CH_EL = 5
    ENERGY_SLOPE = 6
    FLOW_TOTAL = 24
    VEL_TOTAL = 25
    STA_WS_LFT = 36
    STA_WS_RGT = 37
    FROUDE_CHL = 48
    FROUDE_XS = 49
    Q_WEIR = 94
    Q_CULVERT_TOT = 242

    # ========== PRIVATE METHODS (HECRASController COM API) ==========

    @staticmethod
    def _normalize_version(version: str) -> str:
        """
        Normalize version string to match VERSION_MAP keys.

        Handles formats like:
            "6.6", "66" → "6.6"
            "4.1", "41" → "4.1"
            "5.0.6", "506" → "5.0.6"
            "6.7 Beta 4" → "6.7 Beta 4"

        Returns:
            Normalized version string that exists in VERSION_MAP

        Raises:
            ValueError: If version cannot be normalized or is not supported
        """
        version_str = str(version).strip()

        # Direct match
        if version_str in RasControl.VERSION_MAP:
            return version_str

        # Try common normalizations
        normalized_candidates = [
            version_str,
            version_str.replace('.', ''),  # "6.6" → "66"
        ]

        # Try adding periods for compact formats
        if len(version_str) == 2:  # "66" → "6.6"
            normalized_candidates.append(f"{version_str[0]}.{version_str[1]}")
        elif len(version_str) == 3 and version_str.startswith('5'):  # "506" → "5.0.6"
            normalized_candidates.append(f"5.0.{version_str[2]}")
        elif len(version_str) == 3:  # "631" → "6.3.1"
            normalized_candidates.append(f"{version_str[0]}.{version_str[1]}.{version_str[2]}")

        # Check all candidates
        for candidate in normalized_candidates:
            if candidate in RasControl.VERSION_MAP:
                logger.debug(f"Normalized version '{version}' → '{candidate}'")
                return candidate

        # Not found
        raise ValueError(
            f"Version '{version}' not supported. Supported versions:\n"
            f"  3.x: 3.0, 3.1 (3.1.1, 3.1.2, 3.1.3)\n"
            f"  4.x: 4.0, 4.1\n"
            f"  5.0.x: 5.0, 5.0.1, 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7\n"
            f"  6.x: 6.0, 6.1, 6.2, 6.3, 6.3.1, 6.4, 6.4.1, 6.5, 6.6, 6.7\n"
            f"  Formats: Can use '6.6' or '66', '5.0.6' or '506', etc."
        )

    @staticmethod
    def _get_project_info(plan: Union[str, Path], ras_object=None):
        """
        Resolve plan number/path to project path, version, and plan details.

        Returns:
            Tuple[Path, str, str, str]: (project_path, version, plan_number, plan_name)
            plan_number and plan_name are None if using direct .prj path
        """
        if ras_object is None:
            ras_object = ras

        # If it's a path to .prj file
        plan_path = Path(plan) if isinstance(plan, str) else plan
        if plan_path.exists() and plan_path.suffix == '.prj':
            # Direct path - need version from ras_object
            if not hasattr(ras_object, 'ras_version') or not ras_object.ras_version:
                raise ValueError(
                    "When using direct .prj paths, project must be initialized with version.\n"
                    "Use: init_ras_project(path, '4.1') or similar"
                )
            return plan_path, ras_object.ras_version, None, None

        # Otherwise treat as plan number
        plan_num = str(plan).zfill(2)

        # Get project path from ras_object
        if not hasattr(ras_object, 'prj_file') or not ras_object.prj_file:
            raise ValueError(
                "No project initialized. Use init_ras_project() first.\n"
                "Example: init_ras_project(path, '4.1')"
            )

        project_path = Path(ras_object.prj_file)

        # Get version
        if not hasattr(ras_object, 'ras_version') or not ras_object.ras_version:
            raise ValueError(
                "Project initialized without version. Re-initialize with:\n"
                "init_ras_project(path, '4.1')  # or '41', '501', etc."
            )

        version = ras_object.ras_version

        # Get plan name from plan_df
        plan_row = ras_object.plan_df[ras_object.plan_df['plan_number'] == plan_num]
        if plan_row.empty:
            raise ValueError(f"Plan '{plan_num}' not found in project")

        plan_name = plan_row['Plan Title'].iloc[0]

        return project_path, version, plan_num, plan_name

    @staticmethod
    def _com_open_close(project_path: Path, version: str, operation_func: Callable[[Any], Any]) -> Any:
        """
        PRIVATE: Open HEC-RAS via COM, run operation, close HEC-RAS.

        This is the core COM interface handler. All public methods use this.
        Includes session tracking for robust cleanup on crashes/kernel restarts.
        """
        # Normalize version (handles "6.6" → "6.6", "66" → "6.6", etc.)
        normalized_version = RasControl._normalize_version(version)

        if not project_path.exists():
            raise FileNotFoundError(f"Project file not found: {project_path}")

        com_rc = None
        result = None
        session_id = str(uuid.uuid4())
        lock_path = None

        # Take snapshot of ras.exe processes before COM launch
        before_snapshot = {}
        try:
            before_snapshot = {
                p.pid: p.info
                for p in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time'])
                if p.info['name'] and p.info['name'].lower() == 'ras.exe'
            }
        except Exception as e:
            logger.debug(f"Could not snapshot processes: {e}")

        try:
            # Open HEC-RAS COM interface
            com_string = RasControl.VERSION_MAP[normalized_version]
            logger.info(f"Opening HEC-RAS: {com_string} (version: {version})")
            com_rc = win32com.client.Dispatch(com_string)

            # Open project
            logger.info(f"Opening project: {project_path}")
            com_rc.Project_Open(str(project_path))

            # Detect ras.exe PID after COM launch
            ras_pid, confidence = _find_our_ras_process(project_path, before_snapshot)

            # Create session lock
            lock_data = SessionLock(
                python_pid=os.getpid(),
                ras_pid=ras_pid,
                project_path=str(project_path),
                ras_version=version,
                session_id=session_id,
                start_time=time.time(),
                python_exe=sys.executable,
                hostname=socket.gethostname(),
                detection_confidence=confidence
            )

            # Track session globally
            _active_sessions[session_id] = lock_data

            # Create lock file
            lock_path = _create_session_lock(session_id, lock_data)

            # Perform operation
            logger.info("Executing operation...")
            result = operation_func(com_rc)
            logger.info("Operation completed successfully")

            return result

        except Exception as e:
            logger.error(f"Operation failed: {e}")
            raise

        finally:
            # ALWAYS close
            logger.info("Closing HEC-RAS...")

            if com_rc is not None:
                try:
                    com_rc.QuitRas()
                    logger.info("HEC-RAS closed via QuitRas()")
                except Exception as e:
                    logger.warning(f"QuitRas() failed: {e}")

            # Clean up session tracking (terminates only our tracked PID)
            _cleanup_session(session_id)

            # Check if our specific process is still running
            if session_id in _active_sessions:
                logger.warning("Session cleanup may have failed - session still tracked")
            else:
                logger.debug("Session cleanup completed successfully")

    # ========== PUBLIC API (ras-commander style) ==========

    @staticmethod
    def run_plan(plan: Union[str, Path], ras_object=None, force_recompute: bool = False,
                 use_watchdog: bool = True, max_runtime: int = 86400) -> Tuple[bool, List[str]]:
        """
        Run a plan (steady or unsteady) and wait for completion.

        This method checks if results are current before running. If results
        are up-to-date, it skips computation (unless force_recompute=True).
        When computation is needed, it starts the computation and polls
        Compute_Complete() until the run finishes. It will block until completion.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance (uses global ras if None)
            force_recompute: If False (default), checks if results are current
                before running. If results are up-to-date, skips computation.
                If True, always runs the plan regardless of current status.
                Defaults to False.
            use_watchdog: If True, spawns independent watchdog process that will
                terminate ras.exe if Python crashes/kernel restarts. Provides
                protection against orphaned processes in Jupyter notebooks.
                Defaults to True (recommended). Set to False to disable.
            max_runtime: Maximum runtime in seconds before watchdog terminates the
                process. Only used if use_watchdog=True. Defaults to 3600 (1 hour).

        Returns:
            Tuple of (success: bool, messages: List[str])

        Example:
            >>> from ras_commander import init_ras_project, RasControl
            >>> init_ras_project(path, "4.1")
            >>> # Default: with watchdog protection (recommended)
            >>> success, msgs = RasControl.run_plan("02")
            >>> # Force recomputation even if results are current
            >>> success, msgs = RasControl.run_plan("02", force_recompute=True)
            >>> # Disable watchdog (not recommended in Jupyter)
            >>> success, msgs = RasControl.run_plan("01", use_watchdog=False)
            >>> # Long-running with extended timeout
            >>> success, msgs = RasControl.run_plan("01", max_runtime=7200)

        Note:
            Can take several minutes for large models or unsteady runs.
            Progress is logged every 30 seconds.
            If PlanOutput_IsCurrent() check fails (e.g., older HEC-RAS versions),
            the plan will be run as a safe fallback.

            Watchdog protection (use_watchdog=True):
            - Spawns independent Python process monitoring parent death
            - Survives kernel restarts and crashes
            - Automatically terminates orphaned ras.exe processes
            - Enforces max_runtime timeout
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _run_operation(com_rc):
            watchdog_pid = 0

            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            # Check if results are current (unless force_recompute=True)
            if not force_recompute:
                try:
                    is_current = com_rc.PlanOutput_IsCurrent()
                    if is_current:
                        logger.info(f"Plan {plan_num} results are current. Skipping computation.")
                        logger.info("Use force_recompute=True to recompute anyway.")
                        return True, ["Results are current - computation skipped"]
                except Exception as e:
                    logger.warning(f"Could not check PlanOutput_IsCurrent(): {e}")
                    logger.warning("Proceeding with computation...")

            # Version-specific behavior (normalize for checking)
            norm_version = RasControl._normalize_version(version)

            # Start computation (returns immediately - ASYNCHRONOUS!)
            logger.info("Starting computation...")
            if norm_version.startswith('4') or norm_version.startswith('3'):
                status, _, messages = com_rc.Compute_CurrentPlan(None, None)
            else:
                status, _, messages, _ = com_rc.Compute_CurrentPlan(None, None)

            # Spawn watchdog if requested
            if use_watchdog:
                # Find our session to get ras_pid and lock file
                current_session = None
                for session in _active_sessions.values():
                    if session.project_path == str(project_path):
                        current_session = session
                        break

                if current_session and current_session.ras_pid:
                    lock_file = _get_lock_file_path(current_session.session_id)
                    watchdog_pid = _spawn_watchdog(
                        parent_pid=os.getpid(),
                        ras_pid=current_session.ras_pid,
                        max_runtime=max_runtime,
                        lock_file_path=lock_file
                    )
                else:
                    logger.warning("Could not spawn watchdog - ras.exe PID not detected")

            try:
                # CRITICAL: Wait for computation to complete
                # Compute_CurrentPlan is ASYNCHRONOUS - it returns before computation finishes
                logger.info("Waiting for computation to complete...")
                poll_count = 0
                while True:
                    try:
                        # Check if computation is complete
                        is_complete = com_rc.Compute_Complete()

                        if is_complete:
                            logger.info(f"Computation completed (polled {poll_count} times)")
                            break

                        # Still computing - wait and poll again
                        time.sleep(1)  # Poll every second
                        poll_count += 1

                        # Log progress every 30 seconds
                        if poll_count % 30 == 0:
                            logger.info(f"Still computing... ({poll_count} seconds elapsed.  Simulation will timeout after {max_runtime} seconds.  Set max_runtime to override.)")

                    except Exception as e:
                        logger.error(f"Error checking completion status: {e}")
                        # If we can't check status, break and hope for the best
                        break

                return status, list(messages) if messages else []

            finally:
                # Always terminate watchdog on completion (even if error)
                if watchdog_pid:
                    _terminate_watchdog(watchdog_pid)

        return RasControl._com_open_close(project_path, version, _run_operation)

    @staticmethod
    def _parse_ras_datetime(time_string: str) -> pd.Timestamp:
        """
        Parse HEC-RAS COM datetime string to pandas Timestamp.

        Args:
            time_string: RAS format (e.g., "18FEB1999 0000" or "01JAN2000 0000")

        Returns:
            pandas Timestamp, or pd.NaT if string is "Max WS" or parsing fails

        Note:
            This is a private helper method for converting RAS datetime strings
            from the COM interface into proper datetime64[ns] objects. The "Max WS"
            special value is converted to pd.NaT to allow clean filtering.

            Special handling for "2400" hours: HEC-RAS uses 2400 to represent
            midnight at the end of a day (equivalent to 0000 of the next day).
        """
        time_str = time_string.strip()

        # Special case: Max WS row contains computational maximums, not a timestamp
        if time_str == 'Max WS':
            return pd.NaT

        # Special case: 2400 hours (midnight at end of day)
        # HEC-RAS uses 2400 to mean 24:00 (midnight at end of day)
        # Convert to 0000 of next day
        if ' 2400' in time_str:
            # Replace 2400 with 0000 and parse, then add 1 day
            temp_str = time_str.replace(' 2400', ' 0000')
            try:
                dt = pd.to_datetime(temp_str, format='%d%b%Y %H%M')
                # Add 1 day to get correct midnight
                return dt + pd.Timedelta(days=1)
            except (ValueError, TypeError):
                logger.warning(f"Could not parse RAS datetime with 2400: '{time_str}'")
                return pd.NaT

        try:
            # Primary format: "01JAN2000 0000" (%d%b%Y %H%M)
            return pd.to_datetime(time_str, format='%d%b%Y %H%M')
        except (ValueError, TypeError):
            try:
                # Alternate format with seconds: "01JAN2000 0000:00"
                return pd.to_datetime(time_str, format='%d%b%Y %H%M:%S')
            except (ValueError, TypeError):
                logger.warning(f"Could not parse RAS datetime: '{time_str}'")
                return pd.NaT

    @staticmethod
    def get_steady_results(plan: Union[str, Path], ras_object=None) -> pd.DataFrame:
        """
        Extract steady state profile results from HEC-RAS via COM interface.

        Opens HEC-RAS, loads the specified plan, extracts water surface elevations
        and hydraulic parameters for all profiles at all cross sections, then closes
        HEC-RAS.

        Parameters
        ----------
        plan : str or Path
            Plan number (e.g., "01", "02") or full path to .prj file
        ras_object : RasPrj, optional
            RAS project object. If None, uses global `ras` object.

        Returns
        -------
        pd.DataFrame
            Steady state results with one row per cross-section per profile

            **Schema:**

            +----------------+----------+---------------------------------------+
            | Column         | Type     | Description                           |
            +================+==========+=======================================+
            | river          | str      | River name                            |
            +----------------+----------+---------------------------------------+
            | reach          | str      | Reach name                            |
            +----------------+----------+---------------------------------------+
            | node_id        | str      | Cross section river station           |
            +----------------+----------+---------------------------------------+
            | profile        | str      | Profile name (e.g., "PF 1", "50Pct")  |
            +----------------+----------+---------------------------------------+
            | wsel           | float    | Water surface elevation (ft or m)     |
            +----------------+----------+---------------------------------------+
            | velocity       | float    | Total velocity (ft/s or m/s)          |
            +----------------+----------+---------------------------------------+
            | flow           | float    | Total flow (cfs or cms)               |
            +----------------+----------+---------------------------------------+
            | froude         | float    | Channel Froude number (dimensionless) |
            +----------------+----------+---------------------------------------+
            | energy         | float    | Energy grade elevation (ft or m)      |
            +----------------+----------+---------------------------------------+
            | max_depth      | float    | Maximum channel depth (ft or m)       |
            +----------------+----------+---------------------------------------+
            | min_ch_el      | float    | Minimum channel elevation (ft or m)   |
            +----------------+----------+---------------------------------------+

            **Note on data types:**

            - String columns (`river`, `reach`, `node_id`, `profile`) are decoded
              from COM byte strings and stripped of whitespace
            - Numeric columns are float64
            - Units depend on project settings (US customary or SI)

        Raises
        ------
        ValueError
            - If project not initialized with version
            - If plan number not found in project
        RuntimeError
            - If no steady state results found
            - If model run was not successful

        Notes
        -----
        **Comparison with HDF Methods:**

        This COM-based method returns MORE data than the HDF-based
        `HdfResultsPlan.get_steady_wse()`, which only returns WSE.
        RasControl includes velocity, flow, Froude, energy, and depths.

        **Performance Notes:**

        - HEC-RAS is opened and closed for each call (not persistent)
        - For HEC-RAS 6.0+, HDF methods may offer better performance
        - COM interface is single-threaded

        Examples
        --------
        Extract steady results for Plan 02:

        >>> from ras_commander import init_ras_project, RasControl
        >>> init_ras_project(path, "4.1")
        >>> df = RasControl.get_steady_results("02")
        >>> df.to_csv('steady_results.csv', index=False)

        Plot water surface profile:

        >>> import matplotlib.pyplot as plt
        >>> profile_data = df[df['profile'] == 'PF 1']
        >>> plt.plot(profile_data['node_id'].astype(float),
        ...          profile_data['wsel'])
        >>> plt.xlabel('Station')
        >>> plt.ylabel('Water Surface Elevation (ft)')
        >>> plt.show()

        See Also
        --------
        get_unsteady_results : Extract unsteady time series
        run_plan : Run a plan before extracting results
        HdfResultsPlan.get_steady_wse : Modern HDF-based steady extraction

        References
        ----------
        For comparison with HDF-based methods, see:
        ``feature_dev_notes/rascontrol_vs_hdf_comparison.md``
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _extract_operation(com_rc):
            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            results = []
            error_logged = False  # Track if we've already logged comp_msgs

            # Get profiles
            _, profile_names = com_rc.Output_GetProfiles(2, None)

            if profile_names is None:
                raise RuntimeError(
                    "No steady state results found. Please ensure:\n"
                    "  1. The model has been run (use RasControl.run_plan() first)\n"
                    "  2. The current plan is a steady state plan\n"
                    "  3. Results were successfully computed"
                )

            profiles = [{'name': name, 'code': i+1} for i, name in enumerate(profile_names)]
            logger.info(f"Found {len(profiles)} profiles")

            # Get rivers
            _, river_names = com_rc.Output_GetRivers(0, None)

            if river_names is None:
                raise RuntimeError("No river geometry found in model.")

            logger.info(f"Found {len(river_names)} rivers")

            # Extract data
            for riv_code, riv_name in enumerate(river_names, start=1):
                _, _, reach_names = com_rc.Geometry_GetReaches(riv_code, None, None)

                for rch_code, rch_name in enumerate(reach_names, start=1):
                    _, _, _, node_ids, node_types = com_rc.Geometry_GetNodes(
                        riv_code, rch_code, None, None, None
                    )

                    for node_code, (node_id, node_type) in enumerate(
                        zip(node_ids, node_types), start=1
                    ):
                        if node_type == '':  # Cross sections only
                            for profile in profiles:
                                try:
                                    row = {
                                        'river': riv_name.strip(),
                                        'reach': rch_name.strip(),
                                        'node_id': node_id.strip(),
                                        'profile': profile['name'].strip(),
                                    }

                                    # Extract output variables
                                    row['wsel'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.WSEL
                                    )[0]

                                    row['min_ch_el'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.MIN_CH_EL
                                    )[0]

                                    row['velocity'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.VEL_TOTAL
                                    )[0]

                                    row['flow'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.FLOW_TOTAL
                                    )[0]

                                    row['froude'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.FROUDE_CHL
                                    )[0]

                                    row['energy'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.ENERGY
                                    )[0]

                                    row['max_depth'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        profile['code'], RasControl.MAX_CHL_DPTH
                                    )[0]

                                    results.append(row)

                                except Exception as e:
                                    if not error_logged:
                                        # First error - read and log comp_msgs to diagnose issue
                                        logger.error(
                                            f"Failed to extract results at {riv_name}/{rch_name}/{node_id} "
                                            f"profile {profile['name']}: {e}"
                                        )
                                        logger.error(
                                            "This usually indicates the model run was not successful or "
                                            "results are invalid. Reading computation messages..."
                                        )

                                        # Read comp_msgs file
                                        try:
                                            project_base = project_path.stem
                                            plan_file = project_path.parent / f"{project_base}.p{plan_num}"
                                            comp_msgs_file = Path(str(plan_file) + ".comp_msgs.txt")

                                            if comp_msgs_file.exists():
                                                with open(comp_msgs_file, 'r', encoding='utf-8', errors='ignore') as f:
                                                    comp_msgs = f.read()
                                                logger.error(f"\n{'='*80}\nCOMPUTATION MESSAGES:\n{'='*80}\n{comp_msgs}\n{'='*80}")
                                            else:
                                                logger.error(f"Computation messages file not found: {comp_msgs_file}")
                                        except Exception as msg_error:
                                            logger.error(f"Could not read computation messages: {msg_error}")

                                        error_logged = True
                                        logger.info("Suppressing further extraction warnings...")

            if error_logged and len(results) == 0:
                raise RuntimeError(
                    "Failed to extract any results. The model run likely failed or produced invalid results. "
                    "Check the computation messages above for details."
                )

            logger.info(f"Extracted {len(results)} result rows")
            return pd.DataFrame(results)

        return RasControl._com_open_close(project_path, version, _extract_operation)

    @staticmethod
    def get_unsteady_results(plan: Union[str, Path], max_times: Optional[int] = None,
                            ras_object=None) -> pd.DataFrame:
        """
        Extract unsteady flow time series results from HEC-RAS via COM interface.

        Opens HEC-RAS, loads the specified plan, extracts all computed time series
        data including the critical "Max WS" row, then closes HEC-RAS.

        Parameters
        ----------
        plan : str or Path
            Plan number (e.g., "01", "02") or full path to .prj file.
        max_times : int, optional
            Maximum number of timesteps to extract. If None, extracts all timesteps.
            Note: "Max WS" row is always included and doesn't count toward this limit.
        ras_object : RasPrj, optional
            RAS project object. If None, uses global `ras` object.

        Returns
        -------
        pd.DataFrame
            Unsteady flow time series with one row per cross-section per timestep,
            plus one "Max WS" row per cross-section containing computational maximums.

            **Schema:**

            +-----------------+----------------+-------------------------------------------+
            | Column          | Type           | Description                               |
            +=================+================+===========================================+
            | river           | str            | River name                                |
            +-----------------+----------------+-------------------------------------------+
            | reach           | str            | Reach name                                |
            +-----------------+----------------+-------------------------------------------+
            | node_id         | str            | Cross section river station               |
            +-----------------+----------------+-------------------------------------------+
            | time_index      | int            | 1-based timestep index                    |
            |                 |                | 1 = "Max WS", 2+ = actual timesteps       |
            +-----------------+----------------+-------------------------------------------+
            | time_string     | str            | RAS datetime format "01JAN2000 0000"      |
            |                 |                | or "Max WS" for maximum value row         |
            +-----------------+----------------+-------------------------------------------+
            | datetime        | datetime64[ns] | Parsed timestamp                          |
            |                 |                | pd.NaT for "Max WS" rows                  |
            +-----------------+----------------+-------------------------------------------+
            | wsel            | float          | Water surface elevation (ft or m)         |
            +-----------------+----------------+-------------------------------------------+
            | velocity        | float          | Total velocity (ft/s or m/s)              |
            +-----------------+----------------+-------------------------------------------+
            | flow            | float          | Total flow (cfs or cms)                   |
            +-----------------+----------------+-------------------------------------------+
            | froude          | float          | Channel Froude number (dimensionless)     |
            +-----------------+----------------+-------------------------------------------+
            | energy          | float          | Energy grade elevation (ft or m)          |
            +-----------------+----------------+-------------------------------------------+
            | max_depth       | float          | Maximum channel depth (ft or m)           |
            +-----------------+----------------+-------------------------------------------+
            | min_ch_el       | float          | Minimum channel elevation (ft or m)       |
            +-----------------+----------------+-------------------------------------------+

            **Units depend on project settings (US Customary or SI).**

        Raises
        ------
        ValueError
            - If project not initialized with version
            - If plan not found in project
        RuntimeError
            - If no unsteady results found
            - If HEC-RAS computation was not successful

        Notes
        -----
        **Understanding "Max WS" Rows:**

        The "Max WS" row (time_index=1, time_string="Max WS") contains the maximum
        value at ANY computational timestep, not just the output intervals. This is
        critical for design applications because:

        - HEC-RAS computes at finer intervals than it outputs
        - Peak values often occur between output timesteps
        - "Max WS" captures the true computational maximum

        To separate "Max WS" from time series data:

        >>> df_max = df[df['time_string'] == 'Max WS']
        >>> df_timeseries = df[df['datetime'].notna()]  # Excludes Max WS (has NaT)

        **New in v0.81.0:**

        The `datetime` column is now included automatically as datetime64[ns] objects.
        Users no longer need to manually parse `time_string`. For backward compatibility,
        `time_string` is still included.

        **Performance Notes:**

        - HEC-RAS is opened and closed for each call (not persistent)
        - For large time series, consider using HDF-based methods for better performance
        - COM interface is single-threaded

        Examples
        --------
        Extract and plot time series at a cross section:

        >>> from ras_commander import init_ras_project, RasControl
        >>> import matplotlib.pyplot as plt
        >>>
        >>> init_ras_project(path, "4.1")
        >>> df = RasControl.get_unsteady_results("01")
        >>>
        >>> # Separate max WS from time series
        >>> df_max = df[df['time_string'] == 'Max WS']
        >>> df_ts = df[df['datetime'].notna()]
        >>>
        >>> # Plot time series for specific cross section
        >>> xs_data = df_ts[df_ts['node_id'] == '10000'].sort_values('datetime')
        >>> plt.plot(xs_data['datetime'], xs_data['wsel'])
        >>> plt.axhline(df_max[df_max['node_id'] == '10000']['wsel'].iloc[0],
        ...             color='r', linestyle='--', label='Max WS')
        >>> plt.xlabel('Date/Time')
        >>> plt.ylabel('WSE (ft)')
        >>> plt.legend()
        >>> plt.show()

        Filter to specific time range using datetime column:

        >>> import pandas as pd
        >>> start = pd.Timestamp('1999-02-18')
        >>> end = pd.Timestamp('1999-02-20')
        >>> filtered = df_ts[(df_ts['datetime'] >= start) & (df_ts['datetime'] <= end)]

        See Also
        --------
        get_steady_results : Extract steady state profile results
        get_output_times : List available timesteps before extracting
        run_plan : Run a plan before extracting results
        HdfResultsXsec.get_xsec_timeseries : Modern HDF-based extraction (returns xarray)

        References
        ----------
        For comparison with HDF-based methods, see:
        ``feature_dev_notes/rascontrol_vs_hdf_comparison.md``
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _extract_operation(com_rc):
            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            results = []
            error_logged = False  # Track if we've already logged comp_msgs

            # Get output times
            _, time_strings = com_rc.Output_GetProfiles(0, None)

            if time_strings is None:
                raise RuntimeError(
                    "No unsteady results found. Please ensure:\n"
                    "  1. The model has been run (use RasControl.run_plan() first)\n"
                    "  2. The current plan is an unsteady flow plan\n"
                    "  3. Results were successfully computed"
                )

            times = list(time_strings)
            if max_times:
                times = times[:max_times]

            logger.info(f"Extracting {len(times)} time steps")

            # Get rivers
            _, river_names = com_rc.Output_GetRivers(0, None)

            if river_names is None:
                raise RuntimeError("No river geometry found in model.")

            logger.info(f"Found {len(river_names)} rivers")

            # Extract data
            for riv_code, riv_name in enumerate(river_names, start=1):
                _, _, reach_names = com_rc.Geometry_GetReaches(riv_code, None, None)

                for rch_code, rch_name in enumerate(reach_names, start=1):
                    _, _, _, node_ids, node_types = com_rc.Geometry_GetNodes(
                        riv_code, rch_code, None, None, None
                    )

                    for node_code, (node_id, node_type) in enumerate(
                        zip(node_ids, node_types), start=1
                    ):
                        if node_type == '':  # Cross sections only
                            for time_idx, time_str in enumerate(times, start=1):
                                try:
                                    row = {
                                        'river': riv_name.strip(),
                                        'reach': rch_name.strip(),
                                        'node_id': node_id.strip(),
                                        'time_index': time_idx,
                                        'time_string': time_str.strip(),
                                        'datetime': RasControl._parse_ras_datetime(time_str),
                                    }

                                    # Extract output variables (time_idx is profile code for unsteady)
                                    row['wsel'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.WSEL
                                    )[0]

                                    row['min_ch_el'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.MIN_CH_EL
                                    )[0]

                                    row['velocity'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.VEL_TOTAL
                                    )[0]

                                    row['flow'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.FLOW_TOTAL
                                    )[0]

                                    row['froude'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.FROUDE_CHL
                                    )[0]

                                    row['energy'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.ENERGY
                                    )[0]

                                    row['max_depth'] = com_rc.Output_NodeOutput(
                                        riv_code, rch_code, node_code, 0,
                                        time_idx, RasControl.MAX_CHL_DPTH
                                    )[0]

                                    results.append(row)

                                except Exception as e:
                                    if not error_logged:
                                        # First error - read and log comp_msgs to diagnose issue
                                        logger.error(
                                            f"Failed to extract results at {riv_name}/{rch_name}/{node_id} "
                                            f"time {time_str}: {e}"
                                        )
                                        logger.error(
                                            "This usually indicates the model run was not successful or "
                                            "results are invalid. Reading computation messages..."
                                        )

                                        # Read comp_msgs file
                                        try:
                                            project_base = project_path.stem
                                            plan_file = project_path.parent / f"{project_base}.p{plan_num}"
                                            comp_msgs_file = Path(str(plan_file) + ".comp_msgs.txt")

                                            if comp_msgs_file.exists():
                                                with open(comp_msgs_file, 'r', encoding='utf-8', errors='ignore') as f:
                                                    comp_msgs = f.read()
                                                logger.error(f"\n{'='*80}\nCOMPUTATION MESSAGES:\n{'='*80}\n{comp_msgs}\n{'='*80}")
                                            else:
                                                logger.error(f"Computation messages file not found: {comp_msgs_file}")
                                        except Exception as msg_error:
                                            logger.error(f"Could not read computation messages: {msg_error}")

                                        error_logged = True
                                        logger.info("Suppressing further extraction warnings...")

            if error_logged and len(results) == 0:
                raise RuntimeError(
                    "Failed to extract any results. The model run likely failed or produced invalid results. "
                    "Check the computation messages above for details."
                )

            logger.info(f"Extracted {len(results)} result rows")
            return pd.DataFrame(results)

        return RasControl._com_open_close(project_path, version, _extract_operation)

    @staticmethod
    def get_output_times(plan: Union[str, Path], ras_object=None) -> List[str]:
        """
        Get list of output times for unsteady run.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance (uses global ras if None)

        Returns:
            List of time strings (e.g., ["01JAN2000 0000", ...])

        Example:
            >>> times = RasControl.get_output_times("01")
            >>> print(f"Found {len(times)} output times")
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        def _get_times(com_rc):
            # Set current plan if we have plan_name (using plan number)
            if plan_name:
                logger.info(f"Setting current plan to: {plan_name}")
                com_rc.Plan_SetCurrent(plan_name)

            _, time_strings = com_rc.Output_GetProfiles(0, None)

            if time_strings is None:
                raise RuntimeError(
                    "No unsteady output times found. Ensure plan has been run."
                )

            times = list(time_strings)
            logger.info(f"Found {len(times)} output times")
            return times

        return RasControl._com_open_close(project_path, version, _get_times)

    @staticmethod
    def get_plans(plan: Union[str, Path], ras_object=None) -> List[dict]:
        """
        Get list of plans in project.

        Args:
            plan: Plan number or path to .prj file
            ras_object: Optional RasPrj instance

        Returns:
            List of dicts with 'name' and 'filename' keys
        """
        project_path, version, _, _ = RasControl._get_project_info(plan, ras_object)

        def _get_plans(com_rc):
            # Don't set current plan - just getting list
            _, plan_names, _ = com_rc.Plan_Names(None, None, None)

            plans = []
            for name in plan_names:
                filename, _ = com_rc.Plan_GetFilename(name)
                plans.append({'name': name, 'filename': filename})

            logger.info(f"Found {len(plans)} plans")
            return plans

        return RasControl._com_open_close(project_path, version, _get_plans)

    @staticmethod
    def set_current_plan(plan: Union[str, Path], ras_object=None) -> bool:
        """
        Set the current/active plan by plan number.

        Note: This is rarely needed - run_plan() and get_*_results()
        automatically set the correct plan. This is provided for
        advanced use cases.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance

        Returns:
            True if successful

        Example:
            >>> RasControl.set_current_plan("02")  # Set to Plan 02
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        if not plan_name:
            raise ValueError("Cannot set current plan - plan name could not be determined")

        def _set_plan(com_rc):
            com_rc.Plan_SetCurrent(plan_name)
            logger.info(f"Set current plan to Plan {plan_num}: {plan_name}")
            return True

        return RasControl._com_open_close(project_path, version, _set_plan)

    @staticmethod
    def get_comp_msgs(plan: Union[str, Path], ras_object=None) -> str:
        """
        Read computation messages from .txt file with fallback to HDF.

        The comp_msgs file is created by HEC-RAS during plan computation
        and contains detailed messages about the computation process,
        including warnings, errors, and convergence information.

        This method checks for two .txt naming patterns (version-dependent):
        - .comp_msgs.txt (HEC-RAS 3.x-5.x)
        - .computeMsgs.txt (HEC-RAS 6.x+)

        If neither .txt file exists, falls back to HDF extraction.

        Args:
            plan: Plan number ("01", "02") or path to .prj file
            ras_object: Optional RasPrj instance (uses global ras if None)

        Returns:
            String containing computation messages, or empty string if unavailable

        Example:
            >>> from ras_commander import init_ras_project, RasControl
            >>> init_ras_project(path, "4.1")
            >>> msgs = RasControl.get_comp_msgs("08")
            >>> print(msgs)

        Note:
            File naming conventions vary by HEC-RAS version:
            - Older: {plan_file}.comp_msgs.txt
            - Newer: {plan_file}.computeMsgs.txt
            Falls back to HDF: /Results/Summary/Compute Messages (text)
        """
        project_path, version, plan_num, plan_name = RasControl._get_project_info(plan, ras_object)

        # Construct plan file path
        # e.g., "A100_00_00.prj" -> "A100_00_00"
        project_base = project_path.stem
        plan_file = project_path.parent / f"{project_base}.p{plan_num}"

        # Try both .txt file naming patterns (version-dependent)
        comp_msgs_file_old = Path(str(plan_file) + ".comp_msgs.txt")
        comp_msgs_file_new = Path(str(plan_file) + ".computeMsgs.txt")

        comp_msgs_file = None
        if comp_msgs_file_old.exists():
            comp_msgs_file = comp_msgs_file_old
        elif comp_msgs_file_new.exists():
            comp_msgs_file = comp_msgs_file_new

        # If .txt file found, read and return
        if comp_msgs_file is not None:
            logger.info(f"Reading computation messages from: {comp_msgs_file}")

            try:
                with open(comp_msgs_file, 'r', encoding='utf-8', errors='ignore') as f:
                    contents = f.read()

                logger.info(f"Read {len(contents)} characters from comp_msgs file")
                return contents
            except Exception as e:
                logger.error(f"Error reading .txt file: {e}, attempting HDF fallback")

        # If no .txt file found, try HDF fallback
        logger.warning(
            f"Computation messages .txt file not found (tried .comp_msgs.txt and .computeMsgs.txt), "
            f"falling back to HDF extraction"
        )

        try:
            # Late import to avoid circular dependency
            from .HdfResultsPlan import HdfResultsPlan

            # Construct HDF path
            hdf_file = Path(str(plan_file) + ".hdf")
            if hdf_file.exists():
                hdf_contents = HdfResultsPlan.get_compute_messages(hdf_file)
                if hdf_contents:
                    logger.info(f"Successfully retrieved {len(hdf_contents)} characters from HDF")
                    return hdf_contents
        except Exception as e:
            logger.debug(f"HDF fallback failed: {e}")

        # Both methods failed
        logger.debug(
            f"No computation messages found in .txt or HDF sources for plan {plan_num}"
        )
        return ""

    # ========== PROCESS MANAGEMENT API ==========

    @staticmethod
    def list_processes(show_all: bool = False) -> pd.DataFrame:
        """
        List ras.exe processes with tracking status.

        Args:
            show_all: If True, show all ras.exe processes. If False (default),
                     only show processes tracked by this Python session.

        Returns:
            DataFrame with columns: pid, tracked, project, age_sec, status

        Example:
            >>> # Show only tracked processes
            >>> df = RasControl.list_processes()
            >>> print(df)

            >>> # Show all ras.exe on system
            >>> df_all = RasControl.list_processes(show_all=True)
            >>> print(df_all)
        """
        tracked_pids = {lock.ras_pid for lock in _active_sessions.values() if lock.ras_pid}

        rows = []
        for proc in psutil.process_iter(['pid', 'name', 'create_time', 'cmdline']):
            try:
                if proc.info['name'] and proc.info['name'].lower() != 'ras.exe':
                    continue

                is_tracked = proc.info['pid'] in tracked_pids

                if not show_all and not is_tracked:
                    continue

                age = time.time() - proc.info['create_time']

                # Try to extract project from cmdline
                project = "Unknown"
                try:
                    cmdline = ' '.join(proc.info['cmdline'] or [])
                    for token in cmdline.split():
                        if token.endswith('.prj'):
                            project = Path(token).name
                            break
                except (TypeError, AttributeError):
                    pass

                rows.append({
                    'pid': proc.info['pid'],
                    'tracked': is_tracked,
                    'project': project,
                    'age_sec': round(age, 1),
                    'status': 'TRACKED' if is_tracked else 'UNTRACKED'
                })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue

        if not rows:
            logger.info("No ras.exe processes found")
            return pd.DataFrame(columns=['pid', 'tracked', 'project', 'age_sec', 'status'])

        return pd.DataFrame(rows)

    @staticmethod
    def scan_orphans() -> List[SessionLock]:
        """
        Scan lock files for orphaned sessions from crashed Python processes.

        Returns:
            List of SessionLock objects for orphaned processes (Python dead,
            ras.exe still running).

        Example:
            >>> orphans = RasControl.scan_orphans()
            >>> if orphans:
            >>>     print(f"Found {len(orphans)} orphaned processes")
            >>>     for orphan in orphans:
            >>>         print(f"  PID {orphan.ras_pid}: {Path(orphan.project_path).name}")
        """
        orphans = []

        if not LOCK_DIR.exists():
            return orphans

        for lock_file in LOCK_DIR.glob("rasctl_*.lock"):
            try:
                lock = SessionLock.from_file(lock_file)
                status = _classify_lock_file(lock)

                if status == 'stale_orphan':
                    orphans.append(lock)
                elif status == 'stale_clean':
                    # Clean up stale lock files
                    try:
                        lock_file.unlink()
                        logger.debug(f"Cleaned stale lock file: {lock_file.name}")
                    except Exception as e:
                        logger.debug(f"Could not clean stale lock: {e}")
            except Exception as e:
                logger.warning(f"Error reading lock file {lock_file.name}: {e}")

        return orphans

    @staticmethod
    def cleanup_orphans(interactive: bool = True, dry_run: bool = False) -> int:
        """
        Clean up orphaned ras.exe processes from crashed Python sessions.

        This method ONLY terminates processes that:
        1. Were started by RasControl (have session lock files)
        2. Have a dead parent Python process
        3. Are still running

        Args:
            interactive: If True, prompts user for confirmation before cleanup
            dry_run: If True, only reports what would be cleaned (no action)

        Returns:
            Number of processes cleaned up

        Example:
            >>> # Interactive cleanup (prompts for confirmation)
            >>> RasControl.cleanup_orphans()

            >>> # Automatic cleanup (no prompts)
            >>> count = RasControl.cleanup_orphans(interactive=False)
            >>> print(f"Cleaned {count} orphans")

            >>> # Dry run (see what would be cleaned)
            >>> RasControl.cleanup_orphans(dry_run=True)
        """
        orphans = RasControl.scan_orphans()

        if not orphans:
            print("✅ No orphaned processes found")
            logger.info("No orphaned processes found")
            return 0

        print(f"Found {len(orphans)} orphaned RAS process(es):")
        for orphan in orphans:
            age_min = (time.time() - orphan.start_time) / 60
            print(f"  • PID {orphan.ras_pid}: {Path(orphan.project_path).name} "
                  f"(running {age_min:.1f} min, Python {orphan.python_pid} crashed)")

        if dry_run:
            print("\n[Dry run - no action taken]")
            logger.info("Dry run - no orphans terminated")
            return 0

        if interactive:
            response = input("\nClean up these processes? (y/n): ")
            if response.lower() != 'y':
                print("Cancelled")
                logger.info("Cleanup cancelled by user")
                return 0

        cleaned = 0
        for orphan in orphans:
            try:
                proc = psutil.Process(orphan.ras_pid)
                proc.terminate()
                proc.wait(timeout=10)
                print(f"✅ Terminated PID {orphan.ras_pid}")
                logger.info(f"Terminated orphaned PID {orphan.ras_pid}")
                cleaned += 1

                # Remove lock file
                lock_file = _get_lock_file_path(orphan.session_id)
                lock_file.unlink(missing_ok=True)
            except psutil.TimeoutExpired:
                # Force kill if graceful termination fails
                try:
                    proc.kill()
                    print(f"⚠️  Force killed PID {orphan.ras_pid}")
                    logger.warning(f"Force killed orphaned PID {orphan.ras_pid}")
                    cleaned += 1
                except Exception as e:
                    print(f"❌ Failed to kill PID {orphan.ras_pid}: {e}")
                    logger.error(f"Failed to kill orphaned PID {orphan.ras_pid}: {e}")
            except Exception as e:
                print(f"❌ Failed to terminate PID {orphan.ras_pid}: {e}")
                logger.error(f"Failed to terminate orphaned PID {orphan.ras_pid}: {e}")

        print(f"\n✅ Cleaned up {cleaned}/{len(orphans)} processes")
        logger.info(f"Cleaned up {cleaned}/{len(orphans)} orphaned processes")
        return cleaned

    @staticmethod
    def force_cleanup_all() -> int:
        """
        NUCLEAR OPTION: Terminate ALL ras.exe processes on the system.

        ⚠️  WARNING: This will kill:
        - Your tracked processes
        - Other users' processes
        - Manual HEC-RAS GUI sessions
        - Other Python scripts' processes

        Requires explicit "YES" confirmation to prevent accidental use.

        Returns:
            Number of processes terminated

        Example:
            >>> # Prompts for "YES" confirmation
            >>> RasControl.force_cleanup_all()
        """
        all_ras = [p for p in psutil.process_iter(['pid', 'name'])
                   if p.info['name'] and p.info['name'].lower() == 'ras.exe']

        if not all_ras:
            print("No ras.exe processes found")
            logger.info("No ras.exe processes to clean up")
            return 0

        print(f"⚠️  WARNING: This will terminate ALL {len(all_ras)} ras.exe process(es)")
        print("This includes:")
        print("  • Your tracked processes")
        print("  • Other users' processes")
        print("  • Manual HEC-RAS GUI sessions")
        print("  • Other Python scripts' processes")

        response = input("\n⚠️  Type 'YES' in all caps to confirm: ")
        if response != 'YES':
            print("Cancelled")
            logger.info("Force cleanup cancelled by user")
            return 0

        terminated = 0
        for proc in all_ras:
            try:
                proc.terminate()
                proc.wait(timeout=5)
                print(f"✅ Terminated PID {proc.pid}")
                logger.info(f"Force terminated PID {proc.pid}")
                terminated += 1
            except psutil.TimeoutExpired:
                try:
                    proc.kill()
                    print(f"⚠️  Force killed PID {proc.pid}")
                    logger.warning(f"Force killed PID {proc.pid}")
                    terminated += 1
                except Exception as e:
                    print(f"❌ Failed to kill PID {proc.pid}: {e}")
                    logger.error(f"Failed to kill PID {proc.pid}: {e}")
            except Exception as e:
                print(f"❌ Failed to terminate PID {proc.pid}: {e}")
                logger.error(f"Failed to terminate PID {proc.pid}: {e}")

        print(f"\n✅ Terminated {terminated}/{len(all_ras)} processes")
        logger.info(f"Force cleanup terminated {terminated}/{len(all_ras)} processes")

        # Clean up all lock files
        if LOCK_DIR.exists():
            for lock_file in LOCK_DIR.glob("rasctl_*.lock"):
                try:
                    lock_file.unlink()
                except Exception:
                    pass

        return terminated


if __name__ == '__main__':
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("RasControl (ras-commander API) loaded successfully")
    print(f"Supported versions: {list(RasControl.SUPPORTED_VERSIONS.keys())}")
    print("\nUsage example:")
    print("  from ras_commander import init_ras_project, RasControl")
    print("  init_ras_project(path, '4.1')")
    print("  df = RasControl.get_steady_results('02')")

==================================================

File: C:\GH\ras-commander\ras_commander\RasExamples.py
==================================================
"""
RasExamples - Manage and load HEC-RAS example projects for testing and development

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasExamples:   
- get_example_projects()
- list_categories()
- list_projects()
- extract_project()
- is_project_extracted()
- clean_projects_directory()
        
"""
import os
import requests
import zipfile
import pandas as pd
from pathlib import Path
import shutil
from typing import Union, List
import csv
from datetime import datetime
import logging
import re
from tqdm import tqdm
from ras_commander import get_logger
from ras_commander.LoggingConfig import log_call

logger = get_logger(__name__)

class RasExamples:
    """
    A class for quickly loading HEC-RAS example projects for testing and development of ras-commander.
    All methods are class methods, so no initialization is required.
    """
    base_url = 'https://github.com/HydrologicEngineeringCenter/hec-downloads/releases/download/'
    valid_versions = [
            "6.6", "6.5", "6.4.1", "6.3.1", "6.3", "6.2", "6.1", "6.0",
            "5.0.7", "5.0.6", "5.0.5", "5.0.4", "5.0.3", "5.0.1", "5.0",
            "4.1", "4.0", "3.1.3", "3.1.2", "3.1.1", "3.0", "2.2"
        ]
    base_dir = Path.cwd()
    examples_dir = base_dir
    projects_dir = examples_dir / 'example_projects'
    csv_file_path = examples_dir / 'example_projects.csv'
    
    # Special projects that are not in the main zip file
    SPECIAL_PROJECTS = {
        'NewOrleansMetro': 'https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299502039/299502111/1/1747692522764/NewOrleansMetroPipesExample.zip',
        'BeaverLake': 'https://www.hec.usace.army.mil/confluence/rasdocs/hgt/files/latest/299501780/299502090/1/1747692179014/BeaverLake-SWMM-Import-Solution.zip'
    }

    _folder_df = None
    _zip_file_path = None

    def __init__(self):
        """Initialize RasExamples and ensure data is loaded"""
        self._ensure_initialized()

    @property
    def folder_df(self):
        """Access the folder DataFrame"""
        self._ensure_initialized()
        return self._folder_df

    def _ensure_initialized(self):
        """Ensure the class is initialized with required data"""
        self.projects_dir.mkdir(parents=True, exist_ok=True)
        if self._folder_df is None:
            self._load_project_data()

    def _load_project_data(self):
        """Load project data from CSV if up-to-date, otherwise extract from zip."""
        logger.debug("Loading project data")
        self._find_zip_file()
        
        if not self._zip_file_path:
            logger.info("No example projects zip file found. Downloading...")
            self.get_example_projects()
        
        try:
            zip_modified_time = os.path.getmtime(self._zip_file_path)
        except FileNotFoundError:
            logger.error(f"Zip file not found at {self._zip_file_path}.")
            return
        
        if self.csv_file_path.exists():
            csv_modified_time = os.path.getmtime(self.csv_file_path)
            
            if csv_modified_time >= zip_modified_time:
                logger.info("Loading project data from CSV...")
                try:
                    self._folder_df = pd.read_csv(self.csv_file_path)
                    logger.info(f"Loaded {len(self._folder_df)} projects from CSV.")
                    return
                except Exception as e:
                    logger.error(f"Failed to read CSV file: {e}")
                    self._folder_df = None

        logger.info("Extracting folder structure from zip file...")
        self._extract_folder_structure()
        self._save_to_csv()

    @classmethod
    def extract_project(cls, project_names: Union[str, List[str]], output_path: Union[str, Path] = None) -> Union[Path, List[Path]]:
        """Extract one or more specific HEC-RAS projects from the zip file.
        
        Args:
            project_names: Single project name as string or list of project names
            output_path: Optional path where the project folder will be placed.
                        Can be a relative path (creates subfolder in current directory)
                        or an absolute path. If None, uses default 'example_projects' folder.
            
        Returns:
            Path: Single Path object if one project extracted
            List[Path]: List of Path objects if multiple projects extracted
        """
        logger.debug(f"Extracting projects: {project_names}")
        
        # Initialize if needed
        if cls._folder_df is None:
            cls._find_zip_file()
            if not cls._zip_file_path:
                logger.info("No example projects zip file found. Downloading...")
                cls.get_example_projects()
            cls._load_project_data()
        
        if isinstance(project_names, str):
            project_names = [project_names]

        # Determine the output directory
        if output_path is None:
            # Use default 'example_projects' folder
            base_output_path = cls.projects_dir
        else:
            # Convert to Path object
            base_output_path = Path(output_path)
            # If relative path, make it relative to current working directory
            if not base_output_path.is_absolute():
                base_output_path = Path.cwd() / base_output_path
            # Create the directory if it doesn't exist
            base_output_path.mkdir(parents=True, exist_ok=True)

        extracted_paths = []

        for project_name in project_names:
            # Check if this is a special project
            if project_name in cls.SPECIAL_PROJECTS:
                try:
                    special_path = cls._extract_special_project(project_name, base_output_path)
                    extracted_paths.append(special_path)
                    continue
                except Exception as e:
                    logger.error(f"Failed to extract special project '{project_name}': {e}")
                    continue
            
            # Regular project extraction logic
            logger.info("----- RasExamples Extracting Project -----")
            logger.info(f"Extracting project '{project_name}'")
            project_path = base_output_path

            if (project_path / project_name).exists():
                logger.info(f"Project '{project_name}' already exists. Deleting existing folder...")
                try:
                    shutil.rmtree(project_path / project_name)
                    logger.info(f"Existing folder for project '{project_name}' has been deleted.")
                except Exception as e:
                    logger.error(f"Failed to delete existing project folder '{project_name}': {e}")
                    continue

            project_info = cls._folder_df[cls._folder_df['Project'] == project_name]
            if project_info.empty:
                error_msg = f"Project '{project_name}' not found in the zip file."
                logger.error(error_msg)
                raise ValueError(error_msg)

            try:
                with zipfile.ZipFile(cls._zip_file_path, 'r') as zip_ref:
                    for file in zip_ref.namelist():
                        parts = Path(file).parts
                        if len(parts) > 1 and parts[1] == project_name:
                            relative_path = Path(*parts[1:])
                            extract_path = project_path / relative_path
                            if file.endswith('/'):
                                extract_path.mkdir(parents=True, exist_ok=True)
                            else:
                                extract_path.parent.mkdir(parents=True, exist_ok=True)
                                with zip_ref.open(file) as source, open(extract_path, "wb") as target:
                                    shutil.copyfileobj(source, target)

                logger.info(f"Successfully extracted project '{project_name}' to {project_path / project_name}")
                extracted_paths.append(project_path / project_name)
            except Exception as e:
                logger.error(f"An error occurred while extracting project '{project_name}': {str(e)}")

        # Return single path if only one project was extracted, otherwise return list
        return extracted_paths[0] if len(project_names) == 1 else extracted_paths

    @classmethod
    def _find_zip_file(cls):
        """Locate the example projects zip file in the examples directory."""
        for version in cls.valid_versions:
            potential_zip = cls.examples_dir / f"Example_Projects_{version.replace('.', '_')}.zip"
            if potential_zip.exists():
                cls._zip_file_path = potential_zip
                logger.info(f"Found zip file: {cls._zip_file_path}")
                break
        else:
            logger.warning("No existing example projects zip file found.")

    @classmethod
    def get_example_projects(cls, version_number='6.6'):
        """
        Download and extract HEC-RAS example projects for a specified version.
        """
        logger.info(f"Getting example projects for version {version_number}")
        if version_number not in cls.valid_versions:
            error_msg = f"Invalid version number. Valid versions are: {', '.join(cls.valid_versions)}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        zip_url = f"{cls.base_url}1.0.33/Example_Projects_{version_number.replace('.', '_')}.zip"
        
        cls.examples_dir.mkdir(parents=True, exist_ok=True)
        
        cls._zip_file_path = cls.examples_dir / f"Example_Projects_{version_number.replace('.', '_')}.zip"

        if not cls._zip_file_path.exists():
            logger.info(f"Downloading HEC-RAS Example Projects from {zip_url}. \nThe file is over 400 MB, so it may take a few minutes to download....")
            try:
                response = requests.get(zip_url, stream=True)
                response.raise_for_status()
                with open(cls._zip_file_path, 'wb') as file:
                    shutil.copyfileobj(response.raw, file)
                logger.info(f"Downloaded to {cls._zip_file_path}")
            except requests.exceptions.RequestException as e:
                logger.error(f"Failed to download the zip file: {e}")
                raise
        else:
            logger.info("HEC-RAS Example Projects zip file already exists. Skipping download.")

        cls._load_project_data()
        return cls.projects_dir

    @classmethod
    def _load_project_data(cls):
        """Load project data from CSV if up-to-date, otherwise extract from zip."""
        logger.debug("Loading project data")
        
        try:
            zip_modified_time = os.path.getmtime(cls._zip_file_path)
        except FileNotFoundError:
            logger.error(f"Zip file not found at {cls._zip_file_path}.")
            return
        
        if cls.csv_file_path.exists():
            csv_modified_time = os.path.getmtime(cls.csv_file_path)
            
            if csv_modified_time >= zip_modified_time:
                logger.info("Loading project data from CSV...")
                try:
                    cls._folder_df = pd.read_csv(cls.csv_file_path)
                    logger.info(f"Loaded {len(cls._folder_df)} projects from CSV.")
                    return
                except Exception as e:
                    logger.error(f"Failed to read CSV file: {e}")
                    cls._folder_df = None

        logger.info("Extracting folder structure from zip file...")
        cls._extract_folder_structure()
        cls._save_to_csv()

    @classmethod
    def _extract_folder_structure(cls):
        """
        Extract folder structure from the zip file.

        Populates folder_df with category and project information.
        """
        folder_data = []
        try:
            with zipfile.ZipFile(cls._zip_file_path, 'r') as zip_ref:
                for file in zip_ref.namelist():
                    parts = Path(file).parts
                    if len(parts) > 1:
                        folder_data.append({
                            'Category': parts[0],
                            'Project': parts[1]
                        })
        
            cls._folder_df = pd.DataFrame(folder_data).drop_duplicates()
            logger.info(f"Extracted {len(cls._folder_df)} projects.")
            logger.debug(f"folder_df:\n{cls._folder_df}")
        except zipfile.BadZipFile:
            logger.error(f"The file {cls._zip_file_path} is not a valid zip file.")
            cls._folder_df = pd.DataFrame(columns=['Category', 'Project'])
        except Exception as e:
            logger.error(f"An error occurred while extracting the folder structure: {str(e)}")
            cls._folder_df = pd.DataFrame(columns=['Category', 'Project'])

    @classmethod
    def _save_to_csv(cls):
        """Save the extracted folder structure to CSV file."""
        if cls._folder_df is not None and not cls._folder_df.empty:
            try:
                cls._folder_df.to_csv(cls.csv_file_path, index=False)
                logger.info(f"Saved project data to {cls.csv_file_path}")
            except Exception as e:
                logger.error(f"Failed to save project data to CSV: {e}")
        else:
            logger.warning("No folder data to save to CSV.")

    @classmethod
    def list_categories(cls):
        """
        List all categories of example projects.
        """
        if cls._folder_df is None or 'Category' not in cls._folder_df.columns:
            logger.warning("No categories available. Make sure the zip file is properly loaded.")
            return []
        categories = cls._folder_df['Category'].unique()
        logger.info(f"Available categories: {', '.join(categories)}")
        return categories.tolist()

    @classmethod
    def list_projects(cls, category=None):
        """
        List all projects or projects in a specific category.
        
        Note: Special projects (NewOrleansMetro, BeaverLake) are also available but not listed
        in categories as they are downloaded separately.
        """
        if cls._folder_df is None:
            logger.warning("No projects available. Make sure the zip file is properly loaded.")
            return []
        if category:
            projects = cls._folder_df[cls._folder_df['Category'] == category]['Project'].unique()
            logger.info(f"Projects in category '{category}': {', '.join(projects)}")
        else:
            projects = cls._folder_df['Project'].unique()
            # Add special projects to the list
            all_projects = list(projects) + list(cls.SPECIAL_PROJECTS.keys())
            logger.info(f"All available projects: {', '.join(all_projects)}")
            return all_projects
        return projects.tolist()

    @classmethod
    def is_project_extracted(cls, project_name):
        """
        Check if a specific project is already extracted.
        """
        project_path = cls.projects_dir / project_name
        is_extracted = project_path.exists()
        logger.info(f"Project '{project_name}' extracted: {is_extracted}")
        return is_extracted

    @classmethod
    def clean_projects_directory(cls):
        """Remove all extracted projects from the example_projects directory."""
        logger.info(f"Cleaning projects directory: {cls.projects_dir}")
        if cls.projects_dir.exists():
            try:
                shutil.rmtree(cls.projects_dir)
                logger.info("All projects have been removed.")
            except Exception as e:
                logger.error(f"Failed to remove projects directory: {e}")
        else:
            logger.warning("Projects directory does not exist.")
        cls.projects_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Projects directory cleaned and recreated.")

    @classmethod
    def download_fema_ble_model(cls, huc8, output_dir=None):
        """
        Download a FEMA Base Level Engineering (BLE) model for a given HUC8.

        Args:
            huc8 (str): The 8-digit Hydrologic Unit Code (HUC) for the desired watershed.
            output_dir (str, optional): The directory to save the downloaded files. If None, uses the current working directory.

        Returns:
            str: The path to the downloaded and extracted model directory.

        Note:
            This method downloads the BLE model from the FEMA website and extracts it to the specified directory.
        """
        # Method implementation...

    @classmethod
    def _make_safe_folder_name(cls, name: str) -> str:
        """
        Convert a string to a safe folder name by replacing unsafe characters with underscores.
        """
        safe_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', name)
        logger.debug(f"Converted '{name}' to safe folder name '{safe_name}'")
        return safe_name

    @classmethod
    def _download_file_with_progress(cls, url: str, dest_folder: Path, file_size: int) -> Path:
        """
        Download a file from a URL to a specified destination folder with progress bar.
        """
        local_filename = dest_folder / url.split('/')[-1]
        try:
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                with open(local_filename, 'wb') as f, tqdm(
                    desc=local_filename.name,
                    total=file_size,
                    unit='iB',
                    unit_scale=True,
                    unit_divisor=1024,
                ) as progress_bar:
                    for chunk in r.iter_content(chunk_size=8192):
                        size = f.write(chunk)
                        progress_bar.update(size)
            logger.info(f"Successfully downloaded {url} to {local_filename}")
            return local_filename
        except requests.exceptions.RequestException as e:
            logger.error(f"Request failed for {url}: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to write file {local_filename}: {e}")
            raise

    @classmethod
    def _convert_size_to_bytes(cls, size_str: str) -> int:
        """
        Convert a human-readable file size to bytes.
        """
        units = {'B': 1, 'KB': 1024, 'MB': 1024**2, 'GB': 1024**3, 'TB': 1024**4}
        size_str = size_str.upper().replace(' ', '')
        if not re.match(r'^\d+(\.\d+)?[BKMGT]B?$', size_str):
            raise ValueError(f"Invalid size string: {size_str}")
        
        number, unit = float(re.findall(r'[\d\.]+', size_str)[0]), re.findall(r'[BKMGT]B?', size_str)[0]
        return int(number * units[unit])

    @classmethod
    def _extract_special_project(cls, project_name: str, output_path: Path = None) -> Path:
        """
        Download and extract special projects that are not in the main zip file.
        
        Args:
            project_name: Name of the special project ('NewOrleansMetro' or 'BeaverLake')
            output_path: Base output directory path. If None, uses cls.projects_dir
            
        Returns:
            Path: Path to the extracted project directory
            
        Raises:
            ValueError: If the project is not a recognized special project
        """
        if project_name not in cls.SPECIAL_PROJECTS:
            raise ValueError(f"'{project_name}' is not a recognized special project")
        
        logger.info(f"----- RasExamples Extracting Special Project -----")
        logger.info(f"Extracting special project '{project_name}'")
        
        # Use provided output_path or default
        base_path = output_path if output_path else cls.projects_dir
        
        # Create the project directory
        project_path = base_path / project_name
        
        # Check if already exists
        if project_path.exists():
            logger.info(f"Special project '{project_name}' already exists. Deleting existing folder...")
            try:
                shutil.rmtree(project_path)
                logger.info(f"Existing folder for project '{project_name}' has been deleted.")
            except Exception as e:
                logger.error(f"Failed to delete existing project folder '{project_name}': {e}")
                raise
        
        # Create the project directory
        project_path.mkdir(parents=True, exist_ok=True)
        
        # Download the zip file
        url = cls.SPECIAL_PROJECTS[project_name]
        zip_file_path = base_path / f"{project_name}_temp.zip"
        
        logger.info(f"Downloading special project from: {url}")
        logger.info("This may take a few moments...")
        
        try:
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()
            
            # Get total file size if available
            total_size = int(response.headers.get('content-length', 0))
            
            # Download with progress bar
            with open(zip_file_path, 'wb') as file:
                if total_size > 0:
                    with tqdm(
                        desc=f"Downloading {project_name}",
                        total=total_size,
                        unit='iB',
                        unit_scale=True,
                        unit_divisor=1024,
                    ) as progress_bar:
                        for chunk in response.iter_content(chunk_size=8192):
                            size = file.write(chunk)
                            progress_bar.update(size)
                else:
                    # No content length, download without progress bar
                    for chunk in response.iter_content(chunk_size=8192):
                        file.write(chunk)
            
            logger.info(f"Downloaded special project zip file to {zip_file_path}")
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to download special project '{project_name}': {e}")
            if zip_file_path.exists():
                zip_file_path.unlink()
            raise
        
        # Extract the zip file directly to the project directory
        try:
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                # Extract directly to the project directory (no internal folder structure)
                zip_ref.extractall(project_path)
            logger.info(f"Successfully extracted special project '{project_name}' to {project_path}")
            
        except Exception as e:
            logger.error(f"Failed to extract special project '{project_name}': {e}")
            if project_path.exists():
                shutil.rmtree(project_path)
            raise
        finally:
            # Clean up the temporary zip file
            if zip_file_path.exists():
                zip_file_path.unlink()
                logger.debug(f"Removed temporary zip file: {zip_file_path}")
        
        return project_path
==================================================

File: C:\GH\ras-commander\ras_commander\RasGeo.py
==================================================
"""
RasGeo - Operations for handling geometry files in HEC-RAS projects

DEPRECATION NOTICE:
    This class is deprecated and will be removed before v1.0.
    Please migrate to the new geometry subpackage classes:
    - GeomPreprocessor.clear_geompre_files() - replaces RasGeo.clear_geompre_files()
    - GeomLandCover.get_base_mannings_n() - replaces RasGeo.get_mannings_baseoverrides()
    - GeomLandCover.set_base_mannings_n() - replaces RasGeo.set_mannings_baseoverrides()
    - GeomLandCover.get_region_mannings_n() - replaces RasGeo.get_mannings_regionoverrides()
    - GeomLandCover.set_region_mannings_n() - replaces RasGeo.set_mannings_regionoverrides()

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasGeo:
- clear_geompre_files(): Clears geometry preprocessor files for specified plan files [DEPRECATED]
- get_mannings_baseoverrides(): Reads base Manning's n table from a geometry file [DEPRECATED]
- get_mannings_regionoverrides(): Reads Manning's n region overrides from a geometry file [DEPRECATED]
- set_mannings_baseoverrides(): Writes base Manning's n values to a geometry file [DEPRECATED]
- set_mannings_regionoverrides(): Writes regional Manning's n overrides to a geometry file [DEPRECATED]
"""
import warnings
from pathlib import Path
from typing import List, Union
import pandas as pd
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)


class RasGeo:
    """
    A class for operations on HEC-RAS geometry files.

    DEPRECATED: This class is deprecated and will be removed before v1.0.
    Please migrate to the new geometry subpackage classes:
    - GeomPreprocessor for geometry preprocessor operations
    - GeomLandCover for Manning's n land cover operations
    """

    @staticmethod
    @log_call
    def clear_geompre_files(
        plan_files: Union[str, Path, List[Union[str, Path]]] = None,
        ras_object=None
    ) -> None:
        """
        Clear HEC-RAS geometry preprocessor files for specified plan files.

        DEPRECATED: Use GeomPreprocessor.clear_geompre_files() instead.
        This method will be removed before v1.0.

        Parameters:
            plan_files (Union[str, Path, List[Union[str, Path]]], optional):
                Full path(s) to the HEC-RAS plan file(s) (.p*).
                If None, clears all plan files in the project directory.
            ras_object: An optional RAS object instance.

        Returns:
            None: The function deletes files and updates the ras object's geometry dataframe
        """
        warnings.warn(
            "RasGeo.clear_geompre_files() is deprecated and will be removed before v1.0. "
            "Use GeomPreprocessor.clear_geompre_files() instead.",
            DeprecationWarning,
            stacklevel=2
        )

        from .geom import GeomPreprocessor
        return GeomPreprocessor.clear_geompre_files(plan_files, ras_object)

    @staticmethod
    @log_call
    def get_mannings_baseoverrides(geom_file_path):
        """
        Reads the base Manning's n table from a HEC-RAS geometry file.

        DEPRECATED: Use GeomLandCover.get_base_mannings_n() instead.
        This method will be removed before v1.0.

        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)

        Returns:
        --------
        pandas.DataFrame
            DataFrame with Table Number, Land Cover Name, and Base Mannings n Value
        """
        warnings.warn(
            "RasGeo.get_mannings_baseoverrides() is deprecated and will be removed before v1.0. "
            "Use GeomLandCover.get_base_mannings_n() instead.",
            DeprecationWarning,
            stacklevel=2
        )

        from .geom import GeomLandCover
        return GeomLandCover.get_base_mannings_n(geom_file_path)

    @staticmethod
    @log_call
    def get_mannings_regionoverrides(geom_file_path):
        """
        Reads the Manning's n region overrides from a HEC-RAS geometry file.

        DEPRECATED: Use GeomLandCover.get_region_mannings_n() instead.
        This method will be removed before v1.0.

        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)

        Returns:
        --------
        pandas.DataFrame
            DataFrame with Table Number, Land Cover Name, MainChannel value, and Region Name
        """
        warnings.warn(
            "RasGeo.get_mannings_regionoverrides() is deprecated and will be removed before v1.0. "
            "Use GeomLandCover.get_region_mannings_n() instead.",
            DeprecationWarning,
            stacklevel=2
        )

        from .geom import GeomLandCover
        return GeomLandCover.get_region_mannings_n(geom_file_path)

    @staticmethod
    @log_call
    def set_mannings_baseoverrides(geom_file_path, mannings_data):
        """
        Writes base Manning's n values to a HEC-RAS geometry file.

        DEPRECATED: Use GeomLandCover.set_base_mannings_n() instead.
        This method will be removed before v1.0.

        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)
        mannings_data : DataFrame
            DataFrame with columns 'Table Number', 'Land Cover Name', and 'Base Mannings n Value'

        Returns:
        --------
        bool
            True if successful
        """
        warnings.warn(
            "RasGeo.set_mannings_baseoverrides() is deprecated and will be removed before v1.0. "
            "Use GeomLandCover.set_base_mannings_n() instead.",
            DeprecationWarning,
            stacklevel=2
        )

        from .geom import GeomLandCover
        return GeomLandCover.set_base_mannings_n(geom_file_path, mannings_data)

    @staticmethod
    @log_call
    def set_mannings_regionoverrides(geom_file_path, mannings_data):
        """
        Writes regional Manning's n overrides to a HEC-RAS geometry file.

        DEPRECATED: Use GeomLandCover.set_region_mannings_n() instead.
        This method will be removed before v1.0.

        Parameters:
        -----------
        geom_file_path : str or Path
            Path to the geometry file (.g##)
        mannings_data : DataFrame
            DataFrame with columns 'Table Number', 'Land Cover Name', 'MainChannel', and 'Region Name'

        Returns:
        --------
        bool
            True if successful
        """
        warnings.warn(
            "RasGeo.set_mannings_regionoverrides() is deprecated and will be removed before v1.0. "
            "Use GeomLandCover.set_region_mannings_n() instead.",
            DeprecationWarning,
            stacklevel=2
        )

        from .geom import GeomLandCover
        return GeomLandCover.set_region_mannings_n(geom_file_path, mannings_data)

==================================================

File: C:\GH\ras-commander\ras_commander\RasGeometry.py
==================================================
"""
RasGeometry - Operations for parsing and modifying HEC-RAS geometry files

DEPRECATION NOTICE:
    This class is deprecated and will be removed before v1.0.
    Please migrate to the new geometry subpackage classes:

    Cross Section Operations:
    - GeomCrossSection.get_cross_sections() - replaces RasGeometry.get_cross_sections()
    - GeomCrossSection.get_station_elevation() - replaces RasGeometry.get_station_elevation()
    - GeomCrossSection.set_station_elevation() - replaces RasGeometry.set_station_elevation()
    - GeomCrossSection.get_bank_stations() - replaces RasGeometry.get_bank_stations()
    - GeomCrossSection.get_expansion_contraction() - replaces RasGeometry.get_expansion_contraction()
    - GeomCrossSection.get_mannings_n() - replaces RasGeometry.get_mannings_n()

    Storage Area Operations:
    - GeomStorage.get_storage_areas() - replaces RasGeometry.get_storage_areas()
    - GeomStorage.get_elevation_volume() - replaces RasGeometry.get_storage_elevation_volume()

    Lateral Structure Operations:
    - GeomLateral.get_lateral_structures() - replaces RasGeometry.get_lateral_structures()
    - GeomLateral.get_weir_profile() - replaces RasGeometry.get_lateral_weir_profile()

    SA/2D Connection Operations:
    - GeomLateral.get_connections() - replaces RasGeometry.get_connections()
    - GeomLateral.get_connection_profile() - replaces RasGeometry.get_connection_weir_profile()
    - GeomLateral.get_connection_gates() - replaces RasGeometry.get_connection_gates()

This module is part of the ras-commander library and uses a centralized logging configuration.

All methods are static and designed to be used without instantiation.

List of Functions in RasGeometry (all DEPRECATED):
- get_cross_sections() - Extract all cross section metadata [DEPRECATED]
- get_station_elevation() - Read station/elevation pairs for a cross section [DEPRECATED]
- set_station_elevation() - Write station/elevation with automatic bank interpolation [DEPRECATED]
- get_bank_stations() - Read left and right bank station locations [DEPRECATED]
- get_expansion_contraction() - Read expansion and contraction coefficients [DEPRECATED]
- get_mannings_n() - Read Manning's roughness values with LOB/Channel/ROB classification [DEPRECATED]
- get_storage_areas() - List all storage area names (excluding 2D flow areas) [DEPRECATED]
- get_storage_elevation_volume() - Read elevation-volume curve for a storage area [DEPRECATED]
- get_lateral_structures() - List all lateral weir structures with metadata [DEPRECATED]
- get_lateral_weir_profile() - Read station-elevation profile for lateral weir [DEPRECATED]
- get_connections() - List all SA/2D area connections [DEPRECATED]
- get_connection_weir_profile() - Read dam/weir crest station-elevation profile [DEPRECATED]
- get_connection_gates() - Read gate definitions (CSV format, 23+ parameters) [DEPRECATED]
"""

import warnings
from pathlib import Path
from typing import Union, Optional, List, Tuple
import pandas as pd

from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)


class RasGeometry:
    """
    Operations for parsing and modifying HEC-RAS geometry files.

    DEPRECATED: This class is deprecated and will be removed before v1.0.
    Please migrate to the new geometry subpackage classes:
    - GeomCrossSection for cross section operations
    - GeomStorage for storage area operations
    - GeomLateral for lateral structure and connection operations

    All methods are static and designed to be used without instantiation.
    """

    # ========== CROSS SECTION OPERATIONS (DEPRECATED) ==========

    @staticmethod
    @log_call
    def get_cross_sections(geom_file: Union[str, Path],
                          river: Optional[str] = None,
                          reach: Optional[str] = None) -> pd.DataFrame:
        """
        Extract cross section metadata from geometry file.

        DEPRECATED: Use GeomCrossSection.get_cross_sections() instead.
        """
        warnings.warn(
            "RasGeometry.get_cross_sections() is deprecated. "
            "Use GeomCrossSection.get_cross_sections() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomCrossSection
        return GeomCrossSection.get_cross_sections(geom_file, river, reach)

    @staticmethod
    @log_call
    def get_station_elevation(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str) -> pd.DataFrame:
        """
        Extract station/elevation pairs for a cross section.

        DEPRECATED: Use GeomCrossSection.get_station_elevation() instead.
        """
        warnings.warn(
            "RasGeometry.get_station_elevation() is deprecated. "
            "Use GeomCrossSection.get_station_elevation() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomCrossSection
        return GeomCrossSection.get_station_elevation(geom_file, river, reach, rs)

    @staticmethod
    @log_call
    def set_station_elevation(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str,
                             sta_elev_df: pd.DataFrame,
                             bank_left: Optional[float] = None,
                             bank_right: Optional[float] = None):
        """
        Write station/elevation pairs to a cross section with automatic bank interpolation.

        DEPRECATED: Use GeomCrossSection.set_station_elevation() instead.
        """
        warnings.warn(
            "RasGeometry.set_station_elevation() is deprecated. "
            "Use GeomCrossSection.set_station_elevation() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomCrossSection
        return GeomCrossSection.set_station_elevation(
            geom_file, river, reach, rs, sta_elev_df, bank_left, bank_right
        )

    @staticmethod
    @log_call
    def get_bank_stations(geom_file: Union[str, Path],
                         river: str,
                         reach: str,
                         rs: str) -> Optional[Tuple[float, float]]:
        """
        Extract left and right bank station locations for a cross section.

        DEPRECATED: Use GeomCrossSection.get_bank_stations() instead.
        """
        warnings.warn(
            "RasGeometry.get_bank_stations() is deprecated. "
            "Use GeomCrossSection.get_bank_stations() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomCrossSection
        return GeomCrossSection.get_bank_stations(geom_file, river, reach, rs)

    @staticmethod
    @log_call
    def get_expansion_contraction(geom_file: Union[str, Path],
                                  river: str,
                                  reach: str,
                                  rs: str) -> Tuple[float, float]:
        """
        Extract expansion and contraction coefficients for a cross section.

        DEPRECATED: Use GeomCrossSection.get_expansion_contraction() instead.
        """
        warnings.warn(
            "RasGeometry.get_expansion_contraction() is deprecated. "
            "Use GeomCrossSection.get_expansion_contraction() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomCrossSection
        return GeomCrossSection.get_expansion_contraction(geom_file, river, reach, rs)

    @staticmethod
    @log_call
    def get_mannings_n(geom_file: Union[str, Path],
                      river: str,
                      reach: str,
                      rs: str) -> pd.DataFrame:
        """
        Extract Manning's n roughness values for a cross section.

        DEPRECATED: Use GeomCrossSection.get_mannings_n() instead.
        """
        warnings.warn(
            "RasGeometry.get_mannings_n() is deprecated. "
            "Use GeomCrossSection.get_mannings_n() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomCrossSection
        return GeomCrossSection.get_mannings_n(geom_file, river, reach, rs)

    # ========== STORAGE AREA OPERATIONS (DEPRECATED) ==========

    @staticmethod
    @log_call
    def get_storage_areas(geom_file: Union[str, Path],
                         exclude_2d: bool = True) -> List[str]:
        """
        Extract list of storage area names from geometry file.

        DEPRECATED: Use GeomStorage.get_storage_areas() instead.
        Note: The new method returns a DataFrame with more information.
        This wrapper converts it to List[str] for backward compatibility.
        """
        warnings.warn(
            "RasGeometry.get_storage_areas() is deprecated. "
            "Use GeomStorage.get_storage_areas() instead. "
            "Note: New method returns DataFrame, not List[str].",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomStorage
        df = GeomStorage.get_storage_areas(geom_file, exclude_2d)
        # Convert DataFrame to List[str] for backward compatibility
        if df.empty:
            return []
        return df['Name'].tolist()

    @staticmethod
    @log_call
    def get_storage_elevation_volume(geom_file: Union[str, Path],
                                     area_name: str) -> pd.DataFrame:
        """
        Extract storage area elevation-volume curve.

        DEPRECATED: Use GeomStorage.get_elevation_volume() instead.
        """
        warnings.warn(
            "RasGeometry.get_storage_elevation_volume() is deprecated. "
            "Use GeomStorage.get_elevation_volume() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomStorage
        return GeomStorage.get_elevation_volume(geom_file, area_name)

    # ========== LATERAL STRUCTURE OPERATIONS (DEPRECATED) ==========

    @staticmethod
    @log_call
    def get_lateral_structures(geom_file: Union[str, Path],
                               river: Optional[str] = None,
                               reach: Optional[str] = None) -> pd.DataFrame:
        """
        Extract lateral structure definitions from geometry file.

        DEPRECATED: Use GeomLateral.get_lateral_structures() instead.
        """
        warnings.warn(
            "RasGeometry.get_lateral_structures() is deprecated. "
            "Use GeomLateral.get_lateral_structures() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomLateral
        return GeomLateral.get_lateral_structures(geom_file, river, reach)

    @staticmethod
    @log_call
    def get_lateral_weir_profile(geom_file: Union[str, Path],
                                  river: str,
                                  reach: str,
                                  rs: str,
                                  position: int = 0) -> pd.DataFrame:
        """
        Extract lateral weir station-elevation profile.

        DEPRECATED: Use GeomLateral.get_weir_profile() instead.
        """
        warnings.warn(
            "RasGeometry.get_lateral_weir_profile() is deprecated. "
            "Use GeomLateral.get_weir_profile() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomLateral
        return GeomLateral.get_weir_profile(geom_file, river, reach, rs, position)

    # ========== SA/2D CONNECTION OPERATIONS (DEPRECATED) ==========

    @staticmethod
    @log_call
    def get_connections(geom_file: Union[str, Path]) -> pd.DataFrame:
        """
        Extract all SA/2D area connection definitions.

        DEPRECATED: Use GeomLateral.get_connections() instead.
        """
        warnings.warn(
            "RasGeometry.get_connections() is deprecated. "
            "Use GeomLateral.get_connections() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomLateral
        return GeomLateral.get_connections(geom_file)

    @staticmethod
    @log_call
    def get_connection_weir_profile(geom_file: Union[str, Path],
                                    connection_name: str) -> pd.DataFrame:
        """
        Extract weir/dam crest station-elevation profile for a connection.

        DEPRECATED: Use GeomLateral.get_connection_profile() instead.
        """
        warnings.warn(
            "RasGeometry.get_connection_weir_profile() is deprecated. "
            "Use GeomLateral.get_connection_profile() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomLateral
        return GeomLateral.get_connection_profile(geom_file, connection_name)

    @staticmethod
    @log_call
    def get_connection_gates(geom_file: Union[str, Path],
                            connection_name: str) -> pd.DataFrame:
        """
        Extract gate definitions for a connection.

        DEPRECATED: Use GeomLateral.get_connection_gates() instead.
        """
        warnings.warn(
            "RasGeometry.get_connection_gates() is deprecated. "
            "Use GeomLateral.get_connection_gates() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomLateral
        return GeomLateral.get_connection_gates(geom_file, connection_name)

==================================================

File: C:\GH\ras-commander\ras_commander\RasGeometryUtils.py
==================================================
"""
RasGeometryUtils - Utility functions for parsing HEC-RAS geometry files

DEPRECATION NOTICE:
    This class is deprecated and will be removed before v1.0.
    Please migrate to GeomParser from the geometry subpackage:

    from ras_commander import GeomParser

    All methods have the same names and signatures.

This module provides reusable utility functions for parsing and manipulating
HEC-RAS geometry files. These utilities handle FORTRAN-era fixed-width formats,
count interpretation, section identification, and file manipulation.

All methods are static and designed to be used without instantiation.

List of Functions:
- parse_fixed_width() - Parse fixed-width numeric data (8 or 16 char columns) [DEPRECATED]
- format_fixed_width() - Format values into fixed-width lines [DEPRECATED]
- interpret_count() - Interpret count declarations based on context [DEPRECATED]
- identify_section() - Find section boundaries by keyword marker [DEPRECATED]
- extract_keyword_value() - Extract value following keyword [DEPRECATED]
- extract_comma_list() - Extract comma-separated list [DEPRECATED]
- create_backup() - Create .bak backup before modification [DEPRECATED]
- validate_river_reach_rs() - Validate river/reach/RS exists [DEPRECATED]
"""

import warnings
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any

from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)


class RasGeometryUtils:
    """
    Utility functions for parsing HEC-RAS geometry files.

    DEPRECATED: This class is deprecated and will be removed before v1.0.
    Please migrate to GeomParser from the geometry subpackage:

    from ras_commander import GeomParser

    All methods are static and designed to be used without instantiation.
    """

    @staticmethod
    def parse_fixed_width(line: str, column_width: int = 8) -> List[float]:
        """
        Parse fixed-width numeric data from a line.

        DEPRECATED: Use GeomParser.parse_fixed_width() instead.
        """
        warnings.warn(
            "RasGeometryUtils.parse_fixed_width() is deprecated. "
            "Use GeomParser.parse_fixed_width() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.parse_fixed_width(line, column_width)

    @staticmethod
    def format_fixed_width(values: List[float],
                          column_width: int = 8,
                          values_per_line: int = 10,
                          precision: int = 2) -> List[str]:
        """
        Format values into fixed-width lines for writing to geometry files.

        DEPRECATED: Use GeomParser.format_fixed_width() instead.
        """
        warnings.warn(
            "RasGeometryUtils.format_fixed_width() is deprecated. "
            "Use GeomParser.format_fixed_width() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.format_fixed_width(values, column_width, values_per_line, precision)

    @staticmethod
    @log_call
    def interpret_count(keyword: str,
                       count_value: int,
                       additional_values: Optional[List[int]] = None) -> int:
        """
        Interpret count declarations based on keyword context.

        DEPRECATED: Use GeomParser.interpret_count() instead.
        """
        warnings.warn(
            "RasGeometryUtils.interpret_count() is deprecated. "
            "Use GeomParser.interpret_count() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.interpret_count(keyword, count_value, additional_values)

    @staticmethod
    @log_call
    def identify_section(lines: List[str],
                        keyword: str,
                        start_index: int = 0) -> Optional[Tuple[int, int]]:
        """
        Find section boundaries based on keyword marker.

        DEPRECATED: Use GeomParser.identify_section() instead.
        """
        warnings.warn(
            "RasGeometryUtils.identify_section() is deprecated. "
            "Use GeomParser.identify_section() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.identify_section(lines, keyword, start_index)

    @staticmethod
    def extract_keyword_value(line: str, keyword: str) -> str:
        """
        Extract value following keyword marker.

        DEPRECATED: Use GeomParser.extract_keyword_value() instead.
        """
        warnings.warn(
            "RasGeometryUtils.extract_keyword_value() is deprecated. "
            "Use GeomParser.extract_keyword_value() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.extract_keyword_value(line, keyword)

    @staticmethod
    def extract_comma_list(line: str, keyword: str) -> List[str]:
        """
        Extract comma-separated list following keyword.

        DEPRECATED: Use GeomParser.extract_comma_list() instead.
        """
        warnings.warn(
            "RasGeometryUtils.extract_comma_list() is deprecated. "
            "Use GeomParser.extract_comma_list() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.extract_comma_list(line, keyword)

    @staticmethod
    @log_call
    def create_backup(file_path: Path) -> Path:
        """
        Create .bak backup of file before modification.

        DEPRECATED: Use GeomParser.create_backup() instead.
        """
        warnings.warn(
            "RasGeometryUtils.create_backup() is deprecated. "
            "Use GeomParser.create_backup() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.create_backup(file_path)

    @staticmethod
    def update_timestamp(lines: List[str], keyword: str) -> List[str]:
        """
        Update timestamp for a modified section.

        DEPRECATED: Use GeomParser.update_timestamp() instead.
        """
        warnings.warn(
            "RasGeometryUtils.update_timestamp() is deprecated. "
            "Use GeomParser.update_timestamp() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.update_timestamp(lines, keyword)

    @staticmethod
    @log_call
    def validate_river_reach_rs(geom_file: Path,
                               river: str,
                               reach: str,
                               rs: str) -> bool:
        """
        Validate that river/reach/RS combination exists in geometry file.

        DEPRECATED: Use GeomParser.validate_river_reach_rs() instead.
        """
        warnings.warn(
            "RasGeometryUtils.validate_river_reach_rs() is deprecated. "
            "Use GeomParser.validate_river_reach_rs() instead.",
            DeprecationWarning,
            stacklevel=2
        )
        from .geom import GeomParser
        return GeomParser.validate_river_reach_rs(geom_file, river, reach, rs)

==================================================

File: C:\GH\ras-commander\ras_commander\RasGuiAutomation.py
==================================================
"""
RasGuiAutomation - GUI automation for HEC-RAS using win32com

This module provides functionality to automate HEC-RAS GUI operations using Windows
COM automation and win32gui. It enables programmatic control of menu items, dialogs,
and buttons for workflows that don't have API support.

Public functions:
    get_windows_by_pid(pid)                    - Return all windows for a given process ID as (hwnd, title) tuples.
    find_main_hecras_window(windows)           - Identify the main HEC-RAS window from a window list.
    enumerate_all_menus(hwnd)                  - Return all top-level menus and items for the given window handle.
    click_menu_item(hwnd, menu_id)             - Trigger a menu item by sending WM_COMMAND to the main window.
    find_dialog_by_title(pattern, exact)       - Locate a visible dialog window by title substring or exact match.
    find_button_by_text(hwnd, text)            - Find a button control in a dialog window by its text.
    click_button(button_hwnd)                  - Simulate a click on a button control.
    find_combobox_by_neighbor(hwnd, text)      - Find a combo box control near a label with specific text.
    select_combobox_item_by_text(combo, text)  - Select an item in a combo box by its text.
    set_current_plan(hwnd, plan_number, ...)   - Set the current plan in HEC-RAS by selecting from the plan dropdown.
    wait_for_window(find_window_func, ...)     - Wait for a window using a polling function and timeout.
    open_and_compute(...)                      - Open HEC-RAS, set plan, navigate via menu, optionally click Compute.
    close_window(hwnd)                         - Close the given window handle via WM_CLOSE.
    run_multiple_plans(...)                    - Automate GUI workflow for "Run Multiple Plans" in HEC-RAS.

Private functions (scoped within above):
    Various local callback functions for window and child window enumeration.

This module is part of the ras-commander library and uses a centralized logging configuration.
All public functions are static methods on RasGuiAutomation and are decorated with @log_call.
"""

import time
import ctypes
from ctypes import wintypes
import subprocess
import sys
from pathlib import Path
from typing import Optional, List, Tuple, Callable, Any

# Win32 imports
try:
    import win32gui
    import win32con
    import win32api
    import win32com.client
    import win32process
except ImportError:
    raise ImportError(
        "win32 libraries are required for GUI automation. "
        "Install with: pip install pywin32"
    )

from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

# Windows constants
WM_COMMAND = 0x0111
MF_BYPOSITION = 0x00000400


class RasGuiAutomation:
    """
    Static class for automating HEC-RAS GUI operations using win32com.

    This class provides methods to programmatically control HEC-RAS GUI elements
    including menus, dialogs, and buttons. It's designed for workflows that don't
    have programmatic API support (e.g., floodplain mapping).

    All methods are static and use the @log_call decorator for automatic logging.
    """

    @staticmethod
    @log_call
    def get_windows_by_pid(pid: int) -> List[Tuple[int, str]]:
        """
        Find all windows belonging to a specific process ID.

        Args:
            pid (int): Process ID to search for.

        Returns:
            List[Tuple[int, str]]: List of (window_handle, window_title) tuples.

        Examples:
            >>> windows = RasGuiAutomation.get_windows_by_pid(12345)
            >>> for hwnd, title in windows:
            ...     print(f"Window: {title}")
        """
        def callback(hwnd, hwnds):
            if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):
                # Get the process ID for this window
                _, window_pid = win32process.GetWindowThreadProcessId(hwnd)
                if window_pid == pid:
                    window_title = win32gui.GetWindowText(hwnd)
                    if window_title:  # Only include windows with titles
                        hwnds.append((hwnd, window_title))
            return True

        hwnds = []
        win32gui.EnumWindows(callback, hwnds)
        return hwnds

    @staticmethod
    @log_call
    def find_main_hecras_window(windows: List[Tuple[int, str]]) -> Tuple[Optional[int], Optional[str]]:
        """
        Find the main HEC-RAS window from a list of windows.

        The main window is identified by having "HEC-RAS" in the title and a menu bar.

        Args:
            windows (List[Tuple[int, str]]): List of (window_handle, window_title) tuples.

        Returns:
            Tuple[Optional[int], Optional[str]]: (window_handle, window_title) or (None, None).

        Examples:
            >>> windows = RasGuiAutomation.get_windows_by_pid(12345)
            >>> hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
        """
        for hwnd, title in windows:
            # Main window usually has "HEC-RAS" in title and has a menu bar
            if "HEC-RAS" in title and win32gui.GetMenu(hwnd):
                logger.debug(f"Found main HEC-RAS window: {title}")
                return hwnd, title
        return None, None

    @staticmethod
    @log_call
    def get_menu_string(menu_handle: int, pos: int) -> str:
        """
        Get menu item string at a specific position.

        Args:
            menu_handle (int): Handle to the menu.
            pos (int): Position index of the menu item.

        Returns:
            str: Menu item text, or empty string if not found.
        """
        # Create buffer for menu string
        buf_size = 256
        buf = ctypes.create_unicode_buffer(buf_size)

        # Get menu item info
        user32 = ctypes.windll.user32
        result = user32.GetMenuStringW(
            menu_handle,
            pos,
            buf,
            buf_size,
            MF_BYPOSITION
        )

        if result:
            return buf.value
        return ""

    @staticmethod
    @log_call
    def enumerate_all_menus(hwnd: int) -> dict:
        """
        Enumerate all menus and their items in a window.

        Args:
            hwnd (int): Handle to the window.

        Returns:
            dict: Dictionary mapping menu text to list of (item_text, menu_id) tuples.

        Examples:
            >>> hwnd = 12345
            >>> menus = RasGuiAutomation.enumerate_all_menus(hwnd)
            >>> print(menus['&Run'])
            [('&Unsteady Flow Analysis ...', 47), ...]
        """
        menu_bar = win32gui.GetMenu(hwnd)
        if not menu_bar:
            logger.warning("No menu bar found")
            return {}

        menu_count = win32gui.GetMenuItemCount(menu_bar)
        logger.debug(f"Found {menu_count} top-level menus")

        all_menus = {}

        for i in range(menu_count):
            # Get menu text
            menu_text = RasGuiAutomation.get_menu_string(menu_bar, i)

            # Get submenu handle
            submenu = win32gui.GetSubMenu(menu_bar, i)
            if submenu:
                item_count = win32gui.GetMenuItemCount(submenu)
                menu_items = []

                for j in range(item_count):
                    item_text = RasGuiAutomation.get_menu_string(submenu, j)
                    menu_id = win32gui.GetMenuItemID(submenu, j)
                    menu_items.append((item_text, menu_id))

                all_menus[menu_text] = menu_items

        return all_menus

    @staticmethod
    @log_call
    def click_menu_item(hwnd: int, menu_id: int) -> bool:
        """
        Click a menu item by sending a WM_COMMAND message.

        Args:
            hwnd (int): Handle to the main window.
            menu_id (int): Menu item ID to activate.

        Returns:
            bool: True if message was posted successfully.

        Examples:
            >>> # Click "Run > Unsteady Flow Analysis" (menu ID 47)
            >>> RasGuiAutomation.click_menu_item(hwnd, 47)
        """
        try:
            win32api.PostMessage(hwnd, WM_COMMAND, menu_id, 0)
            logger.info(f"Clicked menu item ID: {menu_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to click menu item {menu_id}: {e}")
            return False

    @staticmethod
    @log_call
    def find_dialog_by_title(title_pattern: str, exact_match: bool = False) -> Optional[int]:
        """
        Find a dialog window by title pattern.

        Args:
            title_pattern (str): Text to search for in window title.
            exact_match (bool): If True, require exact match. Default is substring match.

        Returns:
            Optional[int]: Window handle if found, None otherwise.

        Examples:
            >>> # Find "Unsteady Flow Analysis" dialog
            >>> dialog_hwnd = RasGuiAutomation.find_dialog_by_title("Unsteady Flow Analysis")
        """
        def callback(hwnd, dialogs):
            if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):
                window_title = win32gui.GetWindowText(hwnd)
                if exact_match:
                    if window_title == title_pattern:
                        dialogs.append(hwnd)
                else:
                    if title_pattern.lower() in window_title.lower():
                        dialogs.append(hwnd)
            return True

        dialogs = []
        win32gui.EnumWindows(callback, dialogs)

        if dialogs:
            logger.debug(f"Found dialog matching '{title_pattern}': {len(dialogs)} window(s)")
            return dialogs[0]

        logger.debug(f"No dialog found matching '{title_pattern}'")
        return None

    @staticmethod
    @log_call
    def find_button_by_text(dialog_hwnd: int, button_text: str) -> Optional[int]:
        """
        Find a button in a dialog by its text.

        Args:
            dialog_hwnd (int): Handle to the dialog window.
            button_text (str): Text on the button (case-insensitive).

        Returns:
            Optional[int]: Button handle if found, None otherwise.

        Examples:
            >>> button_hwnd = RasGuiAutomation.find_button_by_text(dialog_hwnd, "Compute")
        """
        def callback(child_hwnd, buttons):
            try:
                text = win32gui.GetWindowText(child_hwnd)
                class_name = win32gui.GetClassName(child_hwnd)
                if button_text.lower() in text.lower() and class_name == "Button":
                    buttons.append(child_hwnd)
            except:
                pass
            return True

        buttons = []
        win32gui.EnumChildWindows(dialog_hwnd, callback, buttons)

        if buttons:
            logger.debug(f"Found button with text '{button_text}'")
            return buttons[0]

        logger.debug(f"No button found with text '{button_text}'")
        return None

    @staticmethod
    @log_call
    def click_button(button_hwnd: int) -> bool:
        """
        Click a button by sending BN_CLICKED message.

        Args:
            button_hwnd (int): Handle to the button.

        Returns:
            bool: True if successful.
        """
        try:
            win32api.SendMessage(button_hwnd, win32con.BM_CLICK, 0, 0)
            logger.info(f"Clicked button: {win32gui.GetWindowText(button_hwnd)}")
            return True
        except Exception as e:
            logger.error(f"Failed to click button: {e}")
            return False

    @staticmethod
    @log_call
    def find_combobox_by_neighbor(hwnd: int, neighbor_text: str) -> Optional[int]:
        """
        Find a combo box control near a label with specific text.

        Args:
            hwnd (int): Handle to the parent window.
            neighbor_text (str): Text of a nearby label (case-insensitive).

        Returns:
            Optional[int]: Combo box handle if found, None otherwise.

        Examples:
            >>> combo = RasGuiAutomation.find_combobox_by_neighbor(hwnd, "Plan:")
        """
        def callback(child_hwnd, combos):
            try:
                class_name = win32gui.GetClassName(child_hwnd)
                if "ComboBox" in class_name:
                    combos.append(child_hwnd)
            except:
                pass
            return True

        combos = []
        win32gui.EnumChildWindows(hwnd, callback, combos)

        if combos:
            logger.debug(f"Found {len(combos)} combo box(es)")
            # For now, return the first combo box found
            # In a more sophisticated implementation, we could check proximity to the label
            return combos[0]

        logger.debug(f"No combo box found near '{neighbor_text}'")
        return None

    @staticmethod
    @log_call
    def select_combobox_item_by_text(combo_hwnd: int, item_text: str) -> bool:
        """
        Select an item in a combo box by its text.

        Args:
            combo_hwnd (int): Handle to the combo box.
            item_text (str): Text of the item to select (partial match, case-insensitive).

        Returns:
            bool: True if item was found and selected.

        Examples:
            >>> RasGuiAutomation.select_combobox_item_by_text(combo_hwnd, "p01")
        """
        try:
            # CB_GETCOUNT = 0x0146
            CB_GETCOUNT = 0x0146
            # CB_GETLBTEXTLEN = 0x0149
            CB_GETLBTEXTLEN = 0x0149
            # CB_GETLBTEXT = 0x0148
            CB_GETLBTEXT = 0x0148
            # CB_SETCURSEL = 0x014E
            CB_SETCURSEL = 0x014E

            # Get number of items in combo box
            count = win32api.SendMessage(combo_hwnd, CB_GETCOUNT, 0, 0)
            logger.debug(f"Combo box has {count} items")

            # Search for matching item
            for i in range(count):
                # Get length of text for this item
                text_len = win32api.SendMessage(combo_hwnd, CB_GETLBTEXTLEN, i, 0)
                if text_len > 0:
                    # Get the text
                    buffer = ctypes.create_unicode_buffer(text_len + 1)
                    win32api.SendMessage(combo_hwnd, CB_GETLBTEXT, i, buffer)
                    item = buffer.value

                    logger.debug(f"Combo box item {i}: '{item}'")

                    # Check for match (case-insensitive, partial match)
                    if item_text.lower() in item.lower():
                        # Select this item
                        win32api.SendMessage(combo_hwnd, CB_SETCURSEL, i, 0)
                        logger.info(f"Selected combo box item {i}: '{item}'")
                        return True

            logger.warning(f"Could not find item containing '{item_text}' in combo box")
            return False

        except Exception as e:
            logger.error(f"Failed to select combo box item: {e}")
            return False

## CHANGE THIS (START)

    @staticmethod
    @log_call
    def set_current_plan(hwnd: int, plan_number: str, ras_object=None) -> bool:
        """
        Set the current plan in HEC-RAS by finding and selecting from the plan dropdown.

        Args:
            hwnd (int): Handle to the main HEC-RAS window.
            plan_number (str): Plan number to select (e.g., "01", "02").
            ras_object: Optional RAS object instance.

        Returns:
            bool: True if plan was successfully selected.

        Examples:
            >>> RasGuiAutomation.set_current_plan(hwnd, "01")
        """
        ras_obj = ras_object or ras
        
        # Try to find the plan combo box
        # In HEC-RAS, the plan selector is typically a combo box near a "Plan:" label
        plan_combo = RasGuiAutomation.find_combobox_by_neighbor(hwnd, "Plan:")
        
        if not plan_combo:
            logger.warning("Could not find plan combo box")
            return False

        # Get plan details to construct the full plan text
        # Plans are typically shown as "p01 - Plan Title" or similar
        try:
            from .RasPlan import RasPlan
            plan_title = RasPlan.get_plan_title(plan_number, ras_object=ras_obj)
            plan_shortid = RasPlan.get_shortid(plan_number, ras_object=ras_obj)
            
            # Try different formats that HEC-RAS might use
            search_terms = [
                f"p{plan_number}",  # Just the plan number
                f"{plan_shortid}",  # Short ID
                f"p{plan_number} - {plan_title}",  # Full format with title
                f"p{plan_number} - {plan_shortid}",  # Format with short ID
            ]
            
            for term in search_terms:
                if RasGuiAutomation.select_combobox_item_by_text(plan_combo, term):
                    logger.info(f"Successfully set current plan to p{plan_number}")
                    return True
            
            # If none of the specific formats worked, just try the plan number
            if RasGuiAutomation.select_combobox_item_by_text(plan_combo, plan_number):
                logger.info(f"Successfully set current plan to p{plan_number}")
                return True
                
        except Exception as e:
            logger.warning(f"Could not get plan details, trying simple search: {e}")
            # Fallback to simple plan number search
            if RasGuiAutomation.select_combobox_item_by_text(plan_combo, f"p{plan_number}"):
                logger.info(f"Successfully set current plan to p{plan_number}")
                return True

        logger.error(f"Failed to set current plan to p{plan_number}")
        return False

## CHANGE THIS (START)


    @staticmethod
    @log_call
    def wait_for_window(
        find_window_func: Callable,
        timeout: int = 60,
        check_interval: int = 2
    ) -> Any:
        """
        Wait for a window to appear using a custom search function.

        Args:
            find_window_func (Callable): Function that returns window handle or None.
            timeout (int): Maximum time to wait in seconds. Default is 60.
            check_interval (int): Time between checks in seconds. Default is 2.

        Returns:
            Any: Result from find_window_func if found within timeout, None otherwise.

        Examples:
            >>> # Wait for main HEC-RAS window
            >>> def find_ras():
            ...     windows = RasGuiAutomation.get_windows_by_pid(pid)
            ...     hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
            ...     return hwnd
            >>> hwnd = RasGuiAutomation.wait_for_window(find_ras, timeout=30)
        """
        start_time = time.time()
        while time.time() - start_time < timeout:
            result = find_window_func()
            if result:
                logger.debug("Window found")
                return result
            logger.debug(f"Window not found, waiting {check_interval} seconds...")
            time.sleep(check_interval)

        logger.warning(f"Window not found after {timeout} seconds")
        return None

    @staticmethod
    @log_call
    def open_and_compute(
        plan_number: str,
        ras_object=None,
        auto_click_compute: bool = True,
        wait_for_user: bool = True
    ) -> bool:
        """
        Open HEC-RAS, set the current plan, navigate to Unsteady Flow Analysis, and optionally click Compute.

        This function automates the workflow:
        1. Open HEC-RAS with the project
        2. Wait for main window to appear
        3. Set the current plan to the specified plan_number
        4. Click "Run > Unsteady Flow Analysis" menu (ID 47)
        5. Optionally click "Compute" button in dialog
        6. Wait for user to close HEC-RAS (or return immediately)

        Args:
            plan_number (str): Plan number to run (e.g., "01", "02").
            ras_object: Optional RAS object instance.
            auto_click_compute (bool): If True, automatically click Compute button. Default True.
            wait_for_user (bool): If True, wait for user to close HEC-RAS. Default True.

        Returns:
            bool: True if successful, False otherwise.

        Examples:
            >>> # Full automation - runs plan "01"
            >>> RasGuiAutomation.open_and_compute("01", auto_click_compute=True)

            >>> # Just open dialog for plan "02", let user click Compute
            >>> RasGuiAutomation.open_and_compute("02", auto_click_compute=False)

        Notes:
            - This is designed for floodplain mapping workflows that require GUI execution
            - The function will attempt to set the current plan before running
            - Menu ID 47 is "Run > Unsteady Flow Analysis" in HEC-RAS 6.x
            - If plan selection or auto_click_compute fails, user can manually complete the workflow
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Step 1: Set current plan in .prj file BEFORE opening HEC-RAS
        # This ensures HEC-RAS opens with the correct plan active
        logger.info(f"Setting current plan to {plan_number} in project file...")
        try:
            ras_obj.set_current_plan(plan_number)
            logger.info(f"Current plan set to {plan_number} in {ras_obj.prj_file}")
        except Exception as e:
            logger.error(f"Failed to set current plan: {e}")
            return False

        # Step 2: Open HEC-RAS
        logger.info("Opening HEC-RAS...")
        ras_exe = ras_obj.ras_exe_path
        prj_path = f'"{str(ras_obj.prj_file)}"'
        command = f"{ras_exe} {prj_path}"

        try:
            if sys.platform == "win32":
                hecras_process = subprocess.Popen(command)
            else:
                hecras_process = subprocess.Popen([str(ras_exe), str(ras_obj.prj_file)])

            logger.info(f"HEC-RAS opened with Process ID: {hecras_process.pid}")
        except Exception as e:
            logger.error(f"Failed to open HEC-RAS: {e}")
            return False

        # Step 3: Wait for main window
        logger.info("Waiting for HEC-RAS main window...")
        time.sleep(3)  # Initial wait for process to start

        def find_ras_window():
            windows = RasGuiAutomation.get_windows_by_pid(hecras_process.pid)
            hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
            return hwnd

        hec_ras_hwnd = RasGuiAutomation.wait_for_window(find_ras_window, timeout=30)

        if not hec_ras_hwnd:
            logger.error("Could not find main HEC-RAS window")
            return False

        logger.info(f"Found HEC-RAS main window: {win32gui.GetWindowText(hec_ras_hwnd)}")

        # Note: Current plan was already set in .prj file before opening HEC-RAS (Step 1)
        # HEC-RAS should now have the correct plan active
        time.sleep(1)  # Let window fully load

        # Step 4: Click "Run > Unsteady Flow Analysis" (menu ID 47)
        logger.info("Clicking 'Run > Unsteady Flow Analysis' menu...")
        time.sleep(0.5)

        if not RasGuiAutomation.click_menu_item(hec_ras_hwnd, 47):
            logger.warning("Failed to click menu item, but continuing...")

        time.sleep(2)  # Wait for dialog to open

        # Step 5: Find and click Compute button (if auto_click_compute)
        if auto_click_compute:
            logger.info("Looking for Unsteady Flow Analysis dialog...")

            def find_unsteady_dialog():
                return RasGuiAutomation.find_dialog_by_title("Unsteady Flow Analysis")

            dialog_hwnd = RasGuiAutomation.wait_for_window(find_unsteady_dialog, timeout=15)

            if dialog_hwnd:
                logger.info("Found Unsteady Flow Analysis dialog")
                logger.info("Looking for Compute button...")

                # Try to find and click Compute button
                compute_button = RasGuiAutomation.find_button_by_text(dialog_hwnd, "Compute")

                if compute_button:
                    logger.info("Clicking Compute button...")
                    RasGuiAutomation.click_button(compute_button)
                else:
                    logger.warning("Could not find Compute button - user must click manually")
                    logger.info("Trying keyboard shortcut as fallback...")
                    try:
                        shell = win32com.client.Dispatch("WScript.Shell")
                        time.sleep(0.5)
                        shell.SendKeys("{ENTER}")
                        logger.info("Sent Enter key to dialog")
                    except Exception as e:
                        logger.warning(f"Keyboard fallback failed: {e}")
            else:
                logger.warning("Could not find Unsteady Flow Analysis dialog")
                logger.info("User must manually click 'Run > Unsteady Flow Analysis' and Compute")

        # Step 6: Wait for user to close HEC-RAS (or return immediately)
        if wait_for_user:
            logger.info("Waiting for user to close HEC-RAS...")
            logger.info(f"Please monitor plan {plan_number} execution and close HEC-RAS when complete")

            try:
                hecras_process.wait()
                logger.info("HEC-RAS has been closed")
            except Exception as e:
                logger.error(f"Error waiting for HEC-RAS to close: {e}")
                return False
        else:
            logger.info("Returning without waiting for HEC-RAS to close")
            logger.info(f"HEC-RAS process ID: {hecras_process.pid}")

        return True

    @staticmethod
    @log_call
    def close_window(hwnd: int) -> bool:
        """
        Close a window by sending WM_CLOSE message.

        Args:
            hwnd (int): Handle to the window to close.

        Returns:
            bool: True if successful.
        """
        try:
            win32gui.PostMessage(hwnd, win32con.WM_CLOSE, 0, 0)
            logger.info(f"Closed window: {win32gui.GetWindowText(hwnd)}")
            return True
        except Exception as e:
            logger.error(f"Failed to close window: {e}")
            return False

    @staticmethod
    @log_call
    def run_multiple_plans(
        plan_numbers: Optional[List[str]] = None,
        ras_object=None,
        check_all: bool = True,
        wait_for_user: bool = True
    ) -> bool:
        """
        Open HEC-RAS and automate "Run > Run Multiple Plans" workflow.

        This function automates the workflow:
        1. Open HEC-RAS with the project
        2. Wait for main window to appear
        3. Click "Run > Run Multiple Plans" menu (ID 52)
        4. Optionally check all plans or select specific plans
        5. Click "Compute" or "Run All Checked Plans" button
        6. Wait for user to close HEC-RAS (or return immediately)

        Args:
            plan_numbers (Optional[List[str]]): List of plan numbers to run. If None and
                check_all=True, all plans will be checked. Currently informational only -
                the function checks all plans regardless.
            ras_object: Optional RAS object instance.
            check_all (bool): If True, attempts to check all plans. Default True.
            wait_for_user (bool): If True, wait for user to close HEC-RAS. Default True.

        Returns:
            bool: True if successful, False otherwise.

        Examples:
            >>> # Run all plans
            >>> RasGuiAutomation.run_multiple_plans(check_all=True)

            >>> # Run specific plans (currently checks all, but logs which plans were requested)
            >>> RasGuiAutomation.run_multiple_plans(plan_numbers=["01", "02"])

        Notes:
            - Menu ID 52 is "Run > Run Multiple Plans" in HEC-RAS 6.x
            - This is useful for batch processing multiple plans or stored maps
            - Currently checks all plans; specific plan selection would require
              analyzing the dialog checkbox structure
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        if plan_numbers:
            logger.info(f"Requested plans: {', '.join(plan_numbers)}")
            logger.info("Note: Currently checking all plans. Specific plan selection not yet implemented.")

        # Step 1: Open HEC-RAS
        logger.info("Opening HEC-RAS...")
        ras_exe = ras_obj.ras_exe_path
        prj_path = f'"{str(ras_obj.prj_file)}"'
        command = f"{ras_exe} {prj_path}"

        try:
            if sys.platform == "win32":
                hecras_process = subprocess.Popen(command)
            else:
                hecras_process = subprocess.Popen([str(ras_exe), str(ras_obj.prj_file)])

            logger.info(f"HEC-RAS opened with Process ID: {hecras_process.pid}")
        except Exception as e:
            logger.error(f"Failed to open HEC-RAS: {e}")
            return False

        # Step 2: Wait for main window
        logger.info("Waiting for HEC-RAS main window...")
        time.sleep(3)  # Initial wait for process to start

        def find_ras_window():
            windows = RasGuiAutomation.get_windows_by_pid(hecras_process.pid)
            hwnd, title = RasGuiAutomation.find_main_hecras_window(windows)
            return hwnd

        hec_ras_hwnd = RasGuiAutomation.wait_for_window(find_ras_window, timeout=30)

        if not hec_ras_hwnd:
            logger.error("Could not find main HEC-RAS window")
            return False

        logger.info(f"Found HEC-RAS main window: {win32gui.GetWindowText(hec_ras_hwnd)}")

        # Step 3: Click "Run > Run Multiple Plans" (menu ID 52)
        logger.info("Clicking 'Run > Run Multiple Plans' menu...")
        time.sleep(1)  # Let window fully load

        if not RasGuiAutomation.click_menu_item(hec_ras_hwnd, 52):
            logger.warning("Failed to click menu item, but continuing...")

        time.sleep(2)  # Wait for dialog to open

        # Step 4: Find the Run Multiple Plans dialog
        logger.info("Looking for Run Multiple Plans dialog...")

        def find_multiple_plans_dialog():
            # Try multiple possible dialog titles
            for title_pattern in ["Run Multiple Plans", "Multiple Plans", "Compute Multiple"]:
                hwnd = RasGuiAutomation.find_dialog_by_title(title_pattern)
                if hwnd:
                    return hwnd
            return None

        dialog_hwnd = RasGuiAutomation.wait_for_window(find_multiple_plans_dialog, timeout=15)

        if dialog_hwnd:
            logger.info(f"Found dialog: {win32gui.GetWindowText(dialog_hwnd)}")

            # Step 5: Try to check all plans (if check_all)
            if check_all:
                logger.info("Attempting to check all plans...")

                # Try to find "Check All" or "Select All" button
                check_all_button = None
                for button_text in ["Check All", "Select All", "All"]:
                    check_all_button = RasGuiAutomation.find_button_by_text(dialog_hwnd, button_text)
                    if check_all_button:
                        logger.info(f"Found '{button_text}' button")
                        RasGuiAutomation.click_button(check_all_button)
                        time.sleep(0.5)
                        break

                if not check_all_button:
                    logger.warning("Could not find 'Check All' button - plans may need manual selection")

            # Step 6: Click "Compute" or "Run All Checked Plans" button
            logger.info("Looking for Compute button...")
            time.sleep(1)

            compute_button = None
            for button_text in ["Compute", "Run", "Run All Checked Plans", "Start"]:
                compute_button = RasGuiAutomation.find_button_by_text(dialog_hwnd, button_text)
                if compute_button:
                    logger.info(f"Found '{button_text}' button")
                    RasGuiAutomation.click_button(compute_button)
                    break

            if not compute_button:
                logger.warning("Could not find Compute button - trying keyboard fallback...")
                try:
                    shell = win32com.client.Dispatch("WScript.Shell")
                    time.sleep(0.5)
                    shell.SendKeys("{ENTER}")
                    logger.info("Sent Enter key to dialog")
                except Exception as e:
                    logger.warning(f"Keyboard fallback failed: {e}")
                    logger.info("User must manually click Compute button")

        else:
            logger.warning("Could not find Run Multiple Plans dialog")
            logger.info("User must manually navigate to 'Run > Run Multiple Plans' and click Compute")

        # Step 7: Wait for user to close HEC-RAS (or return immediately)
        if wait_for_user:
            logger.info("Waiting for user to close HEC-RAS...")
            if plan_numbers:
                logger.info(f"Please monitor execution of plans: {', '.join(plan_numbers)}")
            else:
                logger.info("Please monitor execution and close HEC-RAS when complete")

            try:
                hecras_process.wait()
                logger.info("HEC-RAS has been closed")
            except Exception as e:
                logger.error(f"Error waiting for HEC-RAS to close: {e}")
                return False
        else:
            logger.info("Returning without waiting for HEC-RAS to close")
            logger.info(f"HEC-RAS process ID: {hecras_process.pid}")

        return True

==================================================

File: C:\GH\ras-commander\ras_commander\RasMap.py
==================================================
"""
RasMap - Parses HEC-RAS mapper configuration files (.rasmap)

This module provides functionality to extract and organize information from 
HEC-RAS mapper configuration files, including paths to terrain, soil, and land cover data.
It also includes functions to automate the post-processing of stored maps.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL

Classes:
    RasMap: Class for parsing and accessing HEC-RAS mapper configuration.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasMap:
- parse_rasmap(): Parse a .rasmap file and extract relevant information
- get_rasmap_path(): Get the path to the .rasmap file based on the current project
- initialize_rasmap_df(): Initialize the rasmap_df as part of project initialization
- get_terrain_names(): Extracts terrain layer names from a given .rasmap file
- postprocess_stored_maps(): Automates the generation of stored floodplain map outputs (e.g., .tif files)
- get_results_folder(): Get the folder path containing raster results for a specified plan
- get_results_raster(): Get the .vrt file path for a specified plan and variable name
"""

import os
import re
import xml.etree.ElementTree as ET
from pathlib import Path
import pandas as pd
import shutil
from typing import Union, Optional, Dict, List, Any

from .RasPrj import ras
from .RasPlan import RasPlan
from .RasCmdr import RasCmdr
from .RasUtils import RasUtils
from .RasGuiAutomation import RasGuiAutomation
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasMap:
    """
    Class for parsing and accessing information from HEC-RAS mapper configuration files (.rasmap).
    
    This class provides methods to extract paths to terrain, soil, land cover data,
    and various project settings from the .rasmap file associated with a HEC-RAS project.
    It also includes functionality to automate the post-processing of stored maps.
    """
    
    @staticmethod
    @log_call
    def parse_rasmap(rasmap_path: Union[str, Path], ras_object=None) -> pd.DataFrame:
        """
        Parse a .rasmap file and extract relevant information.
        
        Args:
            rasmap_path (Union[str, Path]): Path to the .rasmap file.
            ras_object: Optional RAS object instance.
            
        Returns:
            pd.DataFrame: DataFrame containing extracted information from the .rasmap file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        rasmap_path = Path(rasmap_path)
        if not rasmap_path.exists():
            logger.error(f"RASMapper file not found: {rasmap_path}")
            # Create a single row DataFrame with all empty values
            return pd.DataFrame({
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            })
        
        try:
            # Initialize data for the DataFrame - just one row with lists
            data = {
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            }
            
            # Read the file content
            with open(rasmap_path, 'r', encoding='utf-8') as f:
                xml_content = f.read()
            
            # Check if it's a valid XML file
            if not xml_content.strip().startswith('<'):
                logger.error(f"File does not appear to be valid XML: {rasmap_path}")
                return pd.DataFrame(data)
            
            # Parse the XML file
            try:
                tree = ET.parse(rasmap_path)
                root = tree.getroot()
            except ET.ParseError as e:
                logger.error(f"Error parsing XML in {rasmap_path}: {e}")
                return pd.DataFrame(data)
            
            # Helper function to convert relative paths to absolute paths
            def to_absolute_path(relative_path: str) -> str:
                if not relative_path:
                    return None
                # Remove any leading .\ or ./
                relative_path = relative_path.lstrip('.\\').lstrip('./')
                # Convert to absolute path relative to project folder
                return str(ras_obj.project_folder / relative_path)
            
            # Extract projection path
            try:
                projection_elem = root.find(".//RASProjectionFilename")
                if projection_elem is not None and 'Filename' in projection_elem.attrib:
                    data['projection_path'][0] = to_absolute_path(projection_elem.attrib['Filename'])
            except Exception as e:
                logger.warning(f"Error extracting projection path: {e}")
            
            # Extract profile lines path
            try:
                profile_lines_elem = root.find(".//Features/Layer[@Name='Profile Lines']")
                if profile_lines_elem is not None and 'Filename' in profile_lines_elem.attrib:
                    data['profile_lines_path'][0].append(to_absolute_path(profile_lines_elem.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting profile lines path: {e}")
            
            # Extract soil layer paths
            try:
                soil_layers = root.findall(".//Layer[@Name='Hydrologic Soil Groups']")
                for layer in soil_layers:
                    if 'Filename' in layer.attrib:
                        data['soil_layer_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting soil layer paths: {e}")
            
            # Extract infiltration HDF paths
            try:
                infiltration_layers = root.findall(".//Layer[@Name='Infiltration']")
                for layer in infiltration_layers:
                    if 'Filename' in layer.attrib:
                        data['infiltration_hdf_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting infiltration HDF paths: {e}")
            
            # Extract landcover HDF paths
            try:
                landcover_layers = root.findall(".//Layer[@Name='LandCover']")
                for layer in landcover_layers:
                    if 'Filename' in layer.attrib:
                        data['landcover_hdf_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting landcover HDF paths: {e}")
            
            # Extract terrain HDF paths
            try:
                terrain_layers = root.findall(".//Terrains/Layer")
                for layer in terrain_layers:
                    if 'Filename' in layer.attrib:
                        data['terrain_hdf_path'][0].append(to_absolute_path(layer.attrib['Filename']))
            except Exception as e:
                logger.warning(f"Error extracting terrain HDF paths: {e}")
            
            # Extract current settings
            current_settings = {}
            try:
                settings_elem = root.find(".//CurrentSettings")
                if settings_elem is not None:
                    # Extract ProjectSettings
                    project_settings_elem = settings_elem.find("ProjectSettings")
                    if project_settings_elem is not None:
                        for child in project_settings_elem:
                            current_settings[child.tag] = child.text
                    
                    # Extract Folders
                    folders_elem = settings_elem.find("Folders")
                    if folders_elem is not None:
                        for child in folders_elem:
                            current_settings[child.tag] = child.text
                            
                data['current_settings'][0] = current_settings
            except Exception as e:
                logger.warning(f"Error extracting current settings: {e}")
            
            # Create DataFrame
            df = pd.DataFrame(data)
            logger.info(f"Successfully parsed RASMapper file: {rasmap_path}")
            return df
            
        except Exception as e:
            logger.error(f"Unexpected error processing RASMapper file {rasmap_path}: {e}")
            # Create a single row DataFrame with all empty values
            return pd.DataFrame({
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            })
    
    @staticmethod
    @log_call
    def get_rasmap_path(ras_object=None) -> Optional[Path]:
        """
        Get the path to the .rasmap file based on the current project.
        
        Args:
            ras_object: Optional RAS object instance.
            
        Returns:
            Optional[Path]: Path to the .rasmap file if found, None otherwise.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        project_name = ras_obj.project_name
        project_folder = ras_obj.project_folder
        rasmap_path = project_folder / f"{project_name}.rasmap"
        
        if not rasmap_path.exists():
            logger.warning(f"RASMapper file not found: {rasmap_path}")
            return None
        
        return rasmap_path
    
    @staticmethod
    @log_call
    def initialize_rasmap_df(ras_object=None) -> pd.DataFrame:
        """
        Initialize the rasmap_df as part of project initialization.
        
        Args:
            ras_object: Optional RAS object instance.
            
        Returns:
            pd.DataFrame: DataFrame containing information from the .rasmap file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        rasmap_path = RasMap.get_rasmap_path(ras_obj)
        if rasmap_path is None:
            logger.warning("No .rasmap file found for this project. Creating empty rasmap_df.")
            # Create a single row DataFrame with all empty values
            return pd.DataFrame({
                'projection_path': [None],
                'profile_lines_path': [[]],
                'soil_layer_path': [[]],
                'infiltration_hdf_path': [[]],
                'landcover_hdf_path': [[]],
                'terrain_hdf_path': [[]],
                'current_settings': [{}]
            })
        
        return RasMap.parse_rasmap(rasmap_path, ras_obj)

    @staticmethod
    @log_call
    def get_terrain_names(rasmap_path: Union[str, Path]) -> List[str]:
        """
        Extracts terrain layer names from a given .rasmap file.
        
        Args:
            rasmap_path (Union[str, Path]): Path to the .rasmap file.

        Returns:
            List[str]: A list of terrain names.
        
        Raises:
            FileNotFoundError: If the rasmap file does not exist.
            ValueError: If the file is not a valid XML or lacks a 'Terrains' section.
        """
        rasmap_path = Path(rasmap_path)
        if not rasmap_path.is_file():
            raise FileNotFoundError(f"The file '{rasmap_path}' does not exist.")

        try:
            tree = ET.parse(rasmap_path)
            root = tree.getroot()
        except ET.ParseError as e:
            raise ValueError(f"Failed to parse the RASMAP file. Ensure it is a valid XML file. Error: {e}")

        terrains_element = root.find('Terrains')
        if terrains_element is None:
            logger.warning("The RASMAP file does not contain a 'Terrains' section.")
            return []

        terrain_names = [layer.get('Name') for layer in terrains_element.findall('Layer') if layer.get('Name')]
        logger.info(f"Extracted terrain names: {terrain_names}")
        return terrain_names


    @staticmethod
    @log_call
    def postprocess_stored_maps(
        plan_number: Union[str, List[str]],
        specify_terrain: Optional[str] = None,
        layers: Union[str, List[str]] = None,
        ras_object: Optional[Any] = None,
        auto_click_compute: bool = True
    ) -> bool:
        """
        Automates the generation of stored floodplain map outputs (e.g., .tif files).

        This function modifies the plan and .rasmap files to generate floodplain maps
        for one or more plans, then restores the original files.

        Args:
            plan_number (Union[str, List[str]]): Plan number(s) to generate maps for.
            specify_terrain (Optional[str]): The name of a specific terrain to use.
            layers (Union[str, List[str]], optional): A list of map layers to generate.
                Defaults to ['WSEL', 'Velocity', 'Depth'].
            ras_object (Optional[Any]): The RAS project object.
            auto_click_compute (bool, optional): If True, uses GUI automation to automatically
                click "Run > Unsteady Flow Analysis" and "Compute" button. If False, just
                opens HEC-RAS and waits for manual execution. Defaults to True.

        Returns:
            bool: True if the process completed successfully, False otherwise.

        Notes:
            - auto_click_compute=True: Automated GUI workflow (clicks menu and Compute button)
            - auto_click_compute=False: Manual workflow (user must click Compute)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        if layers is None:
            layers = ['WSEL', 'Velocity', 'Depth']
        elif isinstance(layers, str):
            layers = [layers]

        # Convert plan_number to list if it's a string
        plan_number_list = [plan_number] if isinstance(plan_number, str) else plan_number

        rasmap_path = ras_obj.project_folder / f"{ras_obj.project_name}.rasmap"
        rasmap_backup_path = rasmap_path.with_suffix(f"{rasmap_path.suffix}.storedmap.bak")

        # Store plan paths and their backups
        plan_paths = []
        plan_backup_paths = []
        plan_results_folders = {}  # Map plan_num to results folder name

        for plan_num in plan_number_list:
            plan_path = Path(RasPlan.get_plan_path(plan_num, ras_obj))
            plan_backup_path = plan_path.with_suffix(f"{plan_path.suffix}.storedmap.bak")
            plan_paths.append(plan_path)
            plan_backup_paths.append(plan_backup_path)

            # Get the Short Identifier for this plan to determine results folder
            plan_df = ras_obj.plan_df
            plan_info = plan_df[plan_df['plan_number'] == plan_num]
            if not plan_info.empty:
                short_id = plan_info.iloc[0]['Short Identifier']
                if pd.notna(short_id) and short_id:
                    plan_results_folders[plan_num] = short_id
                else:
                    # Fallback: use plan number if no Short Identifier
                    plan_results_folders[plan_num] = f"Plan_{plan_num}"
                    logger.warning(f"Plan {plan_num} has no Short Identifier, using 'Plan_{plan_num}' as folder name")
            else:
                plan_results_folders[plan_num] = f"Plan_{plan_num}"
                logger.warning(f"Could not find plan {plan_num} in plan_df, using 'Plan_{plan_num}' as folder name")

        def _create_map_element(name, map_type, results_folder, profile_name="Max"):
            # Generate filename: "WSE (Max).vrt", "Depth (Max).vrt", etc.
            filename = f"{name} ({profile_name}).vrt"
            relative_path = f".\\{results_folder}\\{filename}"

            map_params = {
                "MapType": map_type,
                "OutputMode": "Stored Current Terrain",
                "StoredFilename": relative_path,  # Required for stored maps
                "ProfileIndex": "2147483647",
                "ProfileName": profile_name
            }

            # Create Layer element with Filename attribute
            layer_elem = ET.Element(
                'Layer',
                Name=name,
                Type="RASResultsMap",
                Checked="True",
                Filename=relative_path  # Required for stored maps
            )

            map_params_elem = ET.SubElement(layer_elem, 'MapParameters')
            for k, v in map_params.items():
                map_params_elem.set(k, str(v))
            return layer_elem

        try:
            # --- 1. Backup and Modify Plan Files ---
            for plan_num, plan_path, plan_backup_path in zip(plan_number_list, plan_paths, plan_backup_paths):
                logger.info(f"Backing up plan file {plan_path} to {plan_backup_path}")
                shutil.copy2(plan_path, plan_backup_path)
                
                logger.info(f"Updating plan run flags for floodplain mapping for plan {plan_num}...")
                RasPlan.update_run_flags(
                    plan_num,
                    geometry_preprocessor=False,
                    unsteady_flow_simulation=False,
                    post_processor=False,
                    floodplain_mapping=True, # Note: True maps to 0, which means "Run"
                    ras_object=ras_obj
                )

            # --- 2. Backup and Modify RASMAP File ---
            logger.info(f"Backing up rasmap file {rasmap_path} to {rasmap_backup_path}")
            shutil.copy2(rasmap_path, rasmap_backup_path)

            tree = ET.parse(rasmap_path)
            root = tree.getroot()
            
            results_section = root.find('Results')
            if results_section is None:
                raise ValueError(f"No <Results> section found in {rasmap_path}")

            # Process each plan's results layer
            for plan_num in plan_number_list:
                plan_hdf_part = f".p{plan_num}.hdf"
                results_layer = None
                for layer in results_section.findall("Layer[@Type='RASResults']"):
                    filename = layer.get("Filename")
                    if filename and plan_hdf_part.lower() in filename.lower():
                        results_layer = layer
                        break

                if results_layer is None:
                    logger.warning(f"Could not find RASResults layer for plan ending in '{plan_hdf_part}' in {rasmap_path}")
                    continue
                
                # Map user-provided layer names to HEC-RAS variable names and map types
                # Note: "WSE" is the correct HEC-RAS convention (not "WSEL")
                map_definitions = {
                    "WSE": "elevation",
                    "WSEL": "elevation",  # Accept both for backward compatibility, but use "WSE" in output
                    "Velocity": "velocity",
                    "Depth": "depth"
                }

                # Get the results folder for this plan
                results_folder = plan_results_folders.get(plan_num, f"Plan_{plan_num}")

                for layer_name in layers:
                    if layer_name in map_definitions:
                        map_type = map_definitions[layer_name]

                        # Convert WSEL to WSE for output (HEC-RAS convention)
                        output_name = "WSE" if layer_name == "WSEL" else layer_name

                        map_elem = _create_map_element(output_name, map_type, results_folder)
                        results_layer.append(map_elem)
                        logger.info(f"Added '{output_name}' stored map to results layer for plan {plan_num}.")

            if specify_terrain:
                terrains_elem = root.find('Terrains')
                if terrains_elem is not None:
                    for layer in list(terrains_elem):
                        if layer.get('Name') != specify_terrain:
                            terrains_elem.remove(layer)
                    logger.info(f"Filtered terrains, keeping only '{specify_terrain}'.")

            tree.write(rasmap_path, encoding='utf-8', xml_declaration=True)
            
            # --- 3. Execute HEC-RAS ---
            if auto_click_compute:
                # Use GUI automation to automatically click menu and Compute button
                logger.info("Using GUI automation to run floodplain mapping...")

                # Note: For multiple plans, we run the first plan's automation
                # The user can manually run additional plans if needed
                first_plan = plan_number_list[0]

                success = RasGuiAutomation.open_and_compute(
                    plan_number=first_plan,
                    ras_object=ras_obj,
                    auto_click_compute=True,
                    wait_for_user=True
                )

                if len(plan_number_list) > 1:
                    logger.info(f"Note: GUI automation ran plan {first_plan}. "
                               f"Please manually run remaining plans: {', '.join(plan_number_list[1:])}")

                if not success:
                    logger.error("Floodplain mapping computation failed.")
                    return False

            else:
                # Manual mode: Just open HEC-RAS and wait for user to execute
                logger.info("Opening HEC-RAS...")
                ras_exe = ras_obj.ras_exe_path
                prj_path = f'"{str(ras_obj.prj_file)}"'
                command = f"{ras_exe} {prj_path}"

                try:
                    import sys
                    import subprocess
                    if sys.platform == "win32":
                        hecras_process = subprocess.Popen(command)
                    else:
                        hecras_process = subprocess.Popen([ras_exe, prj_path])

                    logger.info(f"HEC-RAS opened with Process ID: {hecras_process.pid}")
                    logger.info(f"Please run plan(s) {', '.join(plan_number_list)} using the 'Compute Multiple' window in HEC-RAS to generate floodplain mapping results.")

                    # Wait for HEC-RAS to close
                    logger.info("Waiting for HEC-RAS to close...")
                    hecras_process.wait()
                    logger.info("HEC-RAS has closed")

                    success = True

                except Exception as e:
                    logger.error(f"Failed to launch HEC-RAS: {e}")
                    success = False

                if not success:
                    logger.error("Floodplain mapping computation failed.")
                    return False

            logger.info("Floodplain mapping computation successful.")
            return True
        
        except Exception as e:
            logger.error(f"Error in postprocess_stored_maps: {e}")
            return False

        finally:
            # --- 4. Restore Files ---
            for plan_path, plan_backup_path in zip(plan_paths, plan_backup_paths):
                if plan_backup_path.exists():
                    logger.info(f"Restoring original plan file from {plan_backup_path}")
                    shutil.move(plan_backup_path, plan_path)
            if rasmap_backup_path.exists():
                logger.info(f"Restoring original rasmap file from {rasmap_backup_path}")
                shutil.move(rasmap_backup_path, rasmap_path)

    @staticmethod
    @log_call
    def get_results_folder(plan_number: Union[str, int, float], ras_object=None) -> Path:
        """
        Get the folder path containing raster results for a specified plan.

        HEC-RAS creates output folders based on the plan's Short Identifier.
        Windows folder naming replaces special characters with underscores.

        Args:
            plan_number (Union[str, int, float]): Plan number (accepts flexible formats like 1, "01", "001").
            ras_object: Optional RAS object instance.

        Returns:
            Path: Path to the mapping output folder.

        Raises:
            ValueError: If the plan number is not found or output folder doesn't exist.

        Examples:
            >>> folder = RasMap.get_results_folder("01")
            >>> folder = RasMap.get_results_folder(1)
            >>> folder = RasMap.get_results_folder("08", ras_object=my_project)

        Notes:
            - Normalizes plan number to two-digit format ("01", "02", etc.)
            - Retrieves Short Identifier from plan_df
            - Normalizes Short ID for Windows folder naming (special chars -> underscores)
            - Searches project folder for matching output directory
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan number to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)

        # Get plan metadata from plan_df
        plan_df = ras_obj.plan_df
        plan_info = plan_df[plan_df['plan_number'] == plan_number]

        if plan_info.empty:
            raise ValueError(
                f"Plan {plan_number} not found in project. "
                f"Available plans: {list(plan_df['plan_number'])}"
            )

        short_id = plan_info.iloc[0]['Short Identifier']

        if pd.isna(short_id) or not short_id:
            raise ValueError(
                f"Plan {plan_number} does not have a Short Identifier. "
                "Check the plan file for missing metadata."
            )

        # Normalize Short ID to match Windows folder naming
        # RASMapper replaces special characters for Windows compatibility
        replacements = {
            '/': '_', '\\': '_', ':': '_', '*': '_',
            '?': '_', '"': '_', '<': '_', '>': '_',
            '|': '_', '+': '_', ' ': '_'
        }

        normalized = short_id
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)

        # Remove trailing underscores
        normalized = normalized.rstrip('_')

        # Search for output folder in project directory
        project_folder = ras_obj.project_folder

        # Try exact match with Short ID
        exact_match = project_folder / short_id
        if exact_match.exists() and exact_match.is_dir():
            logger.info(f"Found output folder (exact match): {exact_match}")
            return exact_match

        # Try normalized name
        normalized_match = project_folder / normalized
        if normalized_match.exists() and normalized_match.is_dir():
            logger.info(f"Found output folder (normalized): {normalized_match}")
            return normalized_match

        # Try partial match (contains)
        for item in project_folder.iterdir():
            if not item.is_dir():
                continue
            folder_name = item.name
            # Check if short_id is contained in folder name or vice versa
            if short_id in folder_name or folder_name in short_id:
                logger.info(f"Found output folder (partial match): {item}")
                return item
            # Check normalized version
            if normalized in folder_name or folder_name in normalized:
                logger.info(f"Found output folder (normalized partial match): {item}")
                return item

        # No folder found
        raise ValueError(
            f"Output folder not found for plan {plan_number} (Short ID: '{short_id}'). "
            f"Expected folder name: '{normalized}' in {project_folder}. "
            "Ensure the plan has been run and RASMapper has exported results."
        )

    @staticmethod
    @log_call
    def get_results_raster(
        plan_number: Union[str, int, float],
        variable_name: str,
        ras_object=None
    ) -> Path:
        """
        Get the .vrt file path for a specified plan and variable name.

        This function locates VRT (Virtual Raster) files exported by RASMapper
        for a specific hydraulic variable (e.g., WSE, Depth, Velocity).

        Args:
            plan_number (Union[str, int, float]): Plan number (accepts flexible formats).
            variable_name (str): Variable name to search for in VRT filenames (e.g., "WSE", "Depth", "Velocity").
            ras_object: Optional RAS object instance.

        Returns:
            Path: Path to the matching .vrt file.

        Raises:
            ValueError: If no matching files or multiple matching files are found.

        Examples:
            >>> vrt = RasMap.get_results_raster("01", "WSE")
            >>> vrt = RasMap.get_results_raster(1, "Depth")
            >>> vrt = RasMap.get_results_raster("08", "WSE (Max)", ras_object=my_project)

        Notes:
            - Uses get_results_folder() to locate the output directory
            - Searches for .vrt files containing the variable_name (case-insensitive)
            - If multiple files match, lists all matches and raises an error
            - User should make variable_name more specific to narrow results
            - VRT files are lightweight virtual rasters that reference underlying .tif tiles
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the mapping folder for this plan
        mapping_folder = RasMap.get_results_folder(plan_number, ras_obj)

        # List all .vrt files in the folder
        vrt_files = list(mapping_folder.glob("*.vrt"))

        if not vrt_files:
            raise ValueError(
                f"No .vrt files found in mapping folder: {mapping_folder}. "
                "Ensure RASMapper has exported raster results for this plan."
            )

        # Filter files containing variable_name (case-insensitive)
        matching_files = [
            f for f in vrt_files
            if variable_name.lower() in f.name.lower()
        ]

        # Handle results
        if len(matching_files) == 0:
            available_files = [f.name for f in vrt_files]
            raise ValueError(
                f"No .vrt files found matching variable name '{variable_name}' in {mapping_folder}. "
                f"Available files: {available_files}. "
                "Try making variable_name more specific or check for typos."
            )
        elif len(matching_files) == 1:
            logger.info(f"Found matching VRT file: {matching_files[0]}")
            return matching_files[0]
        else:
            # Multiple matches - print list and raise error
            logger.error(f"Multiple .vrt files match '{variable_name}':")
            for i, f in enumerate(matching_files, 1):
                logger.error(f"  {i}. {f.name}")

            raise ValueError(
                f"Multiple .vrt files ({len(matching_files)}) match variable name '{variable_name}'. "
                f"Matching files: {[f.name for f in matching_files]}. "
                "Please make variable_name more specific (e.g., 'WSE (Max)' instead of 'WSE')."
            )
==================================================

File: C:\GH\ras-commander\ras_commander\RasPlan.py
==================================================
"""
RasPlan - Operations for handling plan files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasPlan:
- set_geom(): Set the geometry for a specified plan
- set_steady(): Apply a steady flow file to a plan file
- set_unsteady(): Apply an unsteady flow file to a plan file
- set_num_cores(): Update the maximum number of cores to use
- set_geom_preprocessor(): Update geometry preprocessor settings
- clone_plan(): Create a new plan file based on a template
- clone_unsteady(): Copy unsteady flow files from a template
- clone_steady(): Copy steady flow files from a template
- clone_geom(): Copy geometry files from a template
- get_next_number(): Determine the next available number from a list
- get_plan_value(): Retrieve a specific value from a plan file
- get_results_path(): Get the results file path for a plan
- get_plan_path(): Get the full path for a plan number
- get_flow_path(): Get the full path for a flow number
- get_unsteady_path(): Get the full path for an unsteady number
- get_geom_path(): Get the full path for a geometry number
- update_run_flags(): Update various run flags in a plan file
- update_plan_intervals(): Update computation and output intervals
- update_plan_description(): Update the description in a plan file
- read_plan_description(): Read the description from a plan file
- update_simulation_date(): Update simulation start and end dates
- get_shortid(): Get the Short Identifier from a plan file
- set_shortid(): Set the Short Identifier in a plan file
- get_plan_title(): Get the Plan Title from a plan file
- set_plan_title(): Set the Plan Title in a plan file


        
"""
import os
import re
import logging
from pathlib import Path
import shutil
from typing import Union, Optional, List
from numbers import Number
import pandas as pd
from .RasPrj import RasPrj, ras
from .RasUtils import RasUtils
from pathlib import Path
from typing import Union, Any
from datetime import datetime

import logging
import re
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasPlan:
    """
    A class for operations on HEC-RAS plan files.
    """
    
    @staticmethod
    @log_call
    def set_geom(plan_number: Union[str, Number], new_geom: Union[str, Number], ras_object=None) -> pd.DataFrame:
        """
        Set the geometry for the specified plan by updating only the plan file.

        Parameters:
            plan_number (Union[str, Number]): The plan number to update (accepts int, float, numpy types, etc.).
            new_geom (Union[str, Number]): The new geometry number to set (accepts int, float, numpy types, etc.).
            ras_object: An optional RAS object instance.

        Returns:
            pd.DataFrame: The updated geometry DataFrame.

        Example:
            updated_geom_df = RasPlan.set_geom('02', '03')

        Note:
            This function updates the Geom File= line in the plan file and 
            updates the ras object's dataframes without modifying the PRJ file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan and geometry numbers to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        new_geom = RasUtils.normalize_ras_number(new_geom)

        # Update all dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        
        if new_geom not in ras_obj.geom_df['geom_number'].values:
            logger.error(f"Geometry {new_geom} not found in project.")
            raise ValueError(f"Geometry {new_geom} not found in project.")

        # Get the plan file path
        plan_file_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}"
        if not plan_file_path.exists():
            logger.error(f"Plan file not found: {plan_file_path}")
            raise ValueError(f"Plan file not found: {plan_file_path}")
        
        # Read the plan file and update the Geom File line
        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()
            
            for i, line in enumerate(lines):
                if line.startswith("Geom File="):
                    lines[i] = f"Geom File=g{new_geom}\n"
                    logger.info(f"Updated Geom File in plan file to g{new_geom} for plan {plan_number}")
                    break
                
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)
        except Exception as e:
            logger.error(f"Error updating plan file: {e}")
            raise
        # Update the plan_df without reinitializing
        mask = ras_obj.plan_df['plan_number'] == plan_number
        ras_obj.plan_df.loc[mask, 'geom_number'] = new_geom
        ras_obj.plan_df.loc[mask, 'geometry_number'] = new_geom  # Update geometry_number column
        ras_obj.plan_df.loc[mask, 'Geom File'] = f"g{new_geom}"
        geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom}"
        ras_obj.plan_df.loc[mask, 'Geom Path'] = str(geom_path)

        logger.info(f"Geometry for plan {plan_number} set to {new_geom}")
        logger.debug("Updated plan DataFrame:")
        logger.debug(ras_obj.plan_df)

        return ras_obj.plan_df

    @staticmethod
    @log_call
    def set_steady(plan_number: Union[str, Number], new_steady_flow_number: Union[str, Number], ras_object=None):
        """
        Apply a steady flow file to a plan file.

        Parameters:
        plan_number (Union[str, Number]): Plan number (e.g., '02', 2, or 2.0)
        new_steady_flow_number (Union[str, Number]): Steady flow number to apply (e.g., '01', 1, or 1.0)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If the specified steady flow number is not found in the project file
        FileNotFoundError: If the specified plan file is not found

        Example:
        >>> RasPlan.set_steady('02', '01')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan and flow numbers to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        new_steady_flow_number = RasUtils.normalize_ras_number(new_steady_flow_number)

        ras_obj.flow_df = ras_obj.get_flow_entries()

        if new_steady_flow_number not in ras_obj.flow_df['flow_number'].values:
            raise ValueError(f"Steady flow number {new_steady_flow_number} not found in project file.")
        
        plan_file_path = RasPlan.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}")
        
        try:
            RasUtils.update_file(plan_file_path, RasPlan._update_steady_in_file, new_steady_flow_number)
            
            # Update all dataframes
            ras_obj.plan_df = ras_obj.get_plan_entries()
            
            # Update flow-related columns
            mask = ras_obj.plan_df['plan_number'] == plan_number
            flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{new_steady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow File'] = f"f{new_steady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow Path'] = str(flow_path)
            ras_obj.plan_df.loc[mask, 'unsteady_number'] = None
            
            # Update remaining dataframes
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            
        except Exception as e:
            raise IOError(f"Failed to update steady flow file: {e}")

    @staticmethod
    def _update_steady_in_file(lines, new_steady_flow_number):
        return [f"Flow File=f{new_steady_flow_number}\n" if line.startswith("Flow File=f") else line for line in lines]

    @staticmethod
    @log_call
    def set_unsteady(plan_number: Union[str, Number], new_unsteady_flow_number: Union[str, Number], ras_object=None):
        """
        Apply an unsteady flow file to a plan file.

        Parameters:
        plan_number (Union[str, Number]): Plan number (e.g., '04', 4, or 4.0)
        new_unsteady_flow_number (Union[str, Number]): Unsteady flow number to apply (e.g., '01', 1, or 1.0)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If the specified unsteady number is not found in the project file
        FileNotFoundError: If the specified plan file is not found

        Example:
        >>> RasPlan.set_unsteady('04', '01')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan and unsteady flow numbers to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        new_unsteady_flow_number = RasUtils.normalize_ras_number(new_unsteady_flow_number)

        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        if new_unsteady_flow_number not in ras_obj.unsteady_df['unsteady_number'].values:
            raise ValueError(f"Unsteady number {new_unsteady_flow_number} not found in project file.")
        
        plan_file_path = RasPlan.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}")
        
        try:
            # Read the plan file
            with open(plan_file_path, 'r') as f:
                lines = f.readlines()

            # Update the Flow File line
            for i, line in enumerate(lines):
                if line.startswith("Flow File="):
                    lines[i] = f"Flow File=u{new_unsteady_flow_number}\n"
                    break
            
            # Write back to the plan file
            with open(plan_file_path, 'w') as f:
                f.writelines(lines)
            
            # Update all dataframes
            ras_obj.plan_df = ras_obj.get_plan_entries()
            
            # Update flow-related columns
            mask = ras_obj.plan_df['plan_number'] == plan_number
            flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow File'] = f"u{new_unsteady_flow_number}"
            ras_obj.plan_df.loc[mask, 'Flow Path'] = str(flow_path)
            ras_obj.plan_df.loc[mask, 'unsteady_number'] = new_unsteady_flow_number
            
            # Update remaining dataframes
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            
        except Exception as e:
            raise IOError(f"Failed to update unsteady flow file: {e}")

    @staticmethod
    def _update_unsteady_in_file(lines, new_unsteady_flow_number):
        return [f"Unsteady File=u{new_unsteady_flow_number}\n" if line.startswith("Unsteady File=u") else line for line in lines]
    
    @staticmethod
    @log_call
    def set_num_cores(plan_number: Union[str, Number], num_cores: int, ras_object=None):
        """
        Update the maximum number of cores to use in the HEC-RAS plan file.

        Parameters:
        plan_number (Union[str, Number]): Plan number (e.g., '02', 2, or 2.0) or full path to the plan file
        num_cores (int): Maximum number of cores to use
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Number of cores is controlled by the following parameters in the plan file corresponding to 1D, 2D, Pipe Systems and Pump Stations:
        UNET D1 Cores=  
        UNET D2 Cores=
        PS Cores=

        Where a value of "0" is used for "All Available" cores, and values of 1 or more are used to specify the number of cores to use.
        For complex 1D/2D models with pipe systems, a more complex approach may be needed to optimize performance.  (Suggest writing a custom function based on this code).
        This function simply sets the "num_cores" parameter for ALL instances of the above parameters in the plan file.


        Notes on setting num_cores in HEC-RAS:
        The recommended setting for num_cores is 2 (most efficient) to 8 (most performant)
        More details in the HEC-Commander Repository Blog "Benchmarking is All You Need"
        https://github.com/billk-FM/HEC-Commander/blob/main/Blog/7._Benchmarking_Is_All_You_Need.md
        
        Microsoft Windows has a maximum of 64 cores that can be allocated to a single Ras.exe process. 

        Example:
        >>> # Using plan number
        >>> RasPlan.set_num_cores('02', 4)
        >>> # Using full path to plan file
        >>> RasPlan.set_num_cores('/path/to/project.p02', 4)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        plan_file_path = RasUtils.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}. Please provide a valid plan number or path.")
        
        def update_num_cores(lines):
            updated_lines = []
            for line in lines:
                if any(param in line for param in ["UNET D1 Cores=", "UNET D2 Cores=", "PS Cores="]):
                    param_name = line.split("=")[0]
                    updated_lines.append(f"{param_name}= {num_cores}\n")
                else:
                    updated_lines.append(line)
            return updated_lines
        
        try:
            RasUtils.update_file(plan_file_path, update_num_cores)
        except Exception as e:
            raise IOError(f"Failed to update number of cores in plan file: {e}")
        
        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def set_geom_preprocessor(file_path, run_htab, use_ib_tables, ras_object=None):
        """
        Update the simulation plan file to modify the `Run HTab` and `UNET Use Existing IB Tables` settings.
        
        Parameters:
        file_path (str): Path to the simulation plan file (.p06 or similar) that you want to modify.
        run_htab (int): Value for the `Run HTab` setting:
            - `0` : Do not run the geometry preprocessor, use existing geometry tables.
            - `-1` : Run the geometry preprocessor, forcing a recomputation of the geometry tables.
        use_ib_tables (int): Value for the `UNET Use Existing IB Tables` setting:
            - `0` : Use existing interpolation/boundary (IB) tables without recomputing them.
            - `-1` : Do not use existing IB tables, force a recomputation.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If `run_htab` or `use_ib_tables` are not integers or not within the accepted values (`0` or `-1`).
        FileNotFoundError: If the specified file does not exist.
        IOError: If there is an error reading or writing the file.

        Example:
        >>> RasPlan.set_geom_preprocessor('/path/to/project.p06', run_htab=-1, use_ib_tables=0)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        if run_htab not in [-1, 0]:
            raise ValueError("Invalid value for `Run HTab`. Expected `0` or `-1`.")
        if use_ib_tables not in [-1, 0]:
            raise ValueError("Invalid value for `UNET Use Existing IB Tables`. Expected `0` or `-1`.")
        
        def update_geom_preprocessor(lines, run_htab, use_ib_tables):
            updated_lines = []
            for line in lines:
                if line.lstrip().startswith("Run HTab="):
                    updated_lines.append(f"Run HTab= {run_htab} \n")
                elif line.lstrip().startswith("UNET Use Existing IB Tables="):
                    updated_lines.append(f"UNET Use Existing IB Tables= {use_ib_tables} \n")
                else:
                    updated_lines.append(line)
            return updated_lines
        
        try:
            RasUtils.update_file(file_path, update_geom_preprocessor, run_htab, use_ib_tables)
        except FileNotFoundError:
            raise FileNotFoundError(f"The file '{file_path}' does not exist.")
        except IOError as e:
            raise IOError(f"An error occurred while reading or writing the file: {e}")

        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def get_results_path(plan_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Retrieve the results file path for a given HEC-RAS plan number.

        Args:
            plan_number (Union[str, Number]): The HEC-RAS plan number for which to find the results path (e.g., '02', 2, or 2.0).
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[str]: The full path to the results file if found and the file exists, or None if not found.

        Raises:
            RuntimeError: If the project is not initialized.

        Example:
            >>> ras_plan = RasPlan()
            >>> results_path = ras_plan.get_results_path('01')
            >>> if results_path:
            ...     print(f"Results file found at: {results_path}")
            ... else:
            ...     print("Results file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Update the plan dataframe in the ras instance to ensure it is current
        ras_obj.plan_df = ras_obj.get_plan_entries()

        # Normalize plan number to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        
        plan_entry = ras_obj.plan_df[ras_obj.plan_df['plan_number'] == plan_number]
        if not plan_entry.empty:
            results_path = plan_entry['HDF_Results_Path'].iloc[0]
            if results_path and Path(results_path).exists():
                return results_path
            else:
                return None
        else:
            return None

    @staticmethod
    @log_call
    def get_plan_path(plan_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given plan number.

        This method ensures that the latest plan entries are included by refreshing
        the plan dataframe before searching for the requested plan number.

        Args:
        plan_number (Union[str, Number]): The plan number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the plan file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> plan_path = ras_plan.get_plan_path('01')
        >>> if plan_path:
        ...     print(f"Plan file found at: {plan_path}")
        ... else:
        ...     print("Plan file not found.")
        >>> # Integer input also works
        >>> plan_path = ras_plan.get_plan_path(1)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan number to two-digit format
        plan_number = RasUtils.normalize_ras_number(plan_number)
        
        plan_df = ras_obj.get_plan_entries()
        
        plan_path = plan_df[plan_df['plan_number'] == plan_number]
        
        if not plan_path.empty:
            if 'full_path' in plan_path.columns and not pd.isna(plan_path['full_path'].iloc[0]):
                return plan_path['full_path'].iloc[0]
            else:
                # Fallback to constructing path
                return str(ras_obj.project_folder / f"{ras_obj.project_name}.p{plan_number}")
        return None

    @staticmethod
    @log_call
    def get_flow_path(flow_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given flow number.

        Args:
        flow_number (Union[str, Number]): The flow number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the flow file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> flow_path = ras_plan.get_flow_path('01')
        >>> if flow_path:
        ...     print(f"Flow file found at: {flow_path}")
        ... else:
        ...     print("Flow file not found.")
        >>> # Integer input also works
        >>> flow_path = ras_plan.get_flow_path(1)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize flow number to two-digit format
        flow_number = RasUtils.normalize_ras_number(flow_number)
        
        # Use updated flow dataframe
        ras_obj.flow_df = ras_obj.get_prj_entries('Flow')
        
        flow_path = ras_obj.flow_df[ras_obj.flow_df['flow_number'] == flow_number]
        if not flow_path.empty:
            full_path = flow_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_unsteady_path(unsteady_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given unsteady number.

        Args:
        unsteady_number (Union[str, Number]): The unsteady number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the unsteady file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> unsteady_path = ras_plan.get_unsteady_path('01')
        >>> if unsteady_path:
        ...     print(f"Unsteady file found at: {unsteady_path}")
        ... else:
        ...     print("Unsteady file not found.")
        >>> # Integer input also works
        >>> unsteady_path = ras_plan.get_unsteady_path(1)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize unsteady number to two-digit format
        unsteady_number = RasUtils.normalize_ras_number(unsteady_number)
        
        # Use updated unsteady dataframe
        ras_obj.unsteady_df = ras_obj.get_prj_entries('Unsteady')
        
        unsteady_path = ras_obj.unsteady_df[ras_obj.unsteady_df['unsteady_number'] == unsteady_number]
        if not unsteady_path.empty:
            full_path = unsteady_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_geom_path(geom_number: Union[str, Number], ras_object=None) -> Optional[str]:
        """
        Return the full path for a given geometry number.

        Args:
        geom_number (Union[str, Number]): The geometry number to search for (e.g., '01', 1, or 1.0).
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the geometry file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> geom_path = ras_plan.get_geom_path('01')
        >>> if geom_path:
        ...     print(f"Geometry file found at: {geom_path}")
        ... else:
        ...     print("Geometry file not found.")
        >>> # Integer input also works
        >>> geom_path = ras_plan.get_geom_path(1)
        """
        logger = get_logger(__name__)

        if geom_number is None:
            logger.warning("Provided geometry number is None")
            return None

        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()

            # Normalize geometry number to two-digit format
            geom_number = RasUtils.normalize_ras_number(geom_number)
            
            # Use updated geom dataframe
            ras_obj.geom_df = ras_obj.get_prj_entries('Geom')
            
            # Find the geometry file path
            geom_path = ras_obj.geom_df[ras_obj.geom_df['geom_number'] == geom_number]
            if not geom_path.empty:
                if 'full_path' in geom_path.columns and pd.notna(geom_path['full_path'].iloc[0]):
                    full_path = geom_path['full_path'].iloc[0]
                    logger.info(f"Found geometry path: {full_path}")
                    return full_path
                else:
                    # Fallback to constructing path
                    constructed_path = str(ras_obj.project_folder / f"{ras_obj.project_name}.g{geom_number}")
                    logger.info(f"Constructed geometry path: {constructed_path}")
                    return constructed_path
            else:
                logger.warning(f"No geometry file found with number: {geom_number}")
                return None
        except Exception as e:
            logger.error(f"Error in get_geom_path: {str(e)}")
            return None

    # Clone Functions to copy unsteady, flow, and geometry files from templates

    @staticmethod
    @log_call
    def clone_plan(template_plan: Union[str, Number], new_shortid=None, new_plan_shortid=None, new_title=None, ras_object=None):
        """
        Create a new plan file based on a template and update the project file.

        Parameters:
        template_plan (Union[str, Number]): Plan number to use as template (e.g., '01', 1, or 1.0)
        new_shortid (str, optional): New short identifier for the plan file (max 24 chars).
                                     If not provided, appends '_copy' to original.
                                     Alias: new_plan_shortid (for improved clarity)
        new_plan_shortid (str, optional): Alias for new_shortid. If both are provided,
                                          new_plan_shortid takes precedence.
        new_title (str, optional): New plan title (max 32 chars, updates "Plan Title=" line).
                                   If not provided, keeps original title.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New plan number

        Example:
        >>> # Clone with default shortid and title (string input)
        >>> new_plan = RasPlan.clone_plan('01')
        >>>
        >>> # Clone with integer input
        >>> new_plan = RasPlan.clone_plan(1)
        >>>
        >>> # Clone with custom shortid and title (either parameter name works)
        >>> new_plan = RasPlan.clone_plan('01', new_shortid='Steady_v41',
        ...                               new_title='Steady Flow - HEC-RAS 4.1')
        >>> new_plan = RasPlan.clone_plan('01', new_plan_shortid='Steady_v41',
        ...                               new_title='Steady Flow - HEC-RAS 4.1')

        Note:
            Both new_shortid and new_title are optional.
            new_plan_shortid is an alias for new_shortid for improved clarity.
            This function updates the ras object's dataframes after modifying the project structure.
        """
        # Handle parameter aliasing: new_plan_shortid takes precedence if both provided
        if new_plan_shortid is not None:
            new_shortid = new_plan_shortid

        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize plan number to two-digit format
        template_plan = RasUtils.normalize_ras_number(template_plan)

        # Validate new_title length if provided
        if new_title is not None and len(new_title) > 32:
            raise ValueError(
                f"Plan title must be 32 characters or less. "
                f"Got {len(new_title)} characters: '{new_title}'"
            )

        # Update plan entries without reinitializing the entire project
        ras_obj.plan_df = ras_obj.get_prj_entries('Plan')

        new_plan_num = RasPlan.get_next_number(ras_obj.plan_df['plan_number'])
        template_plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{template_plan}"
        new_plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{new_plan_num}"

        def update_plan_metadata(lines):
            """Update both Plan Title and Short Identifier"""
            title_pattern = re.compile(r'^Plan Title=(.*)$', re.IGNORECASE)
            shortid_pattern = re.compile(r'^Short Identifier=(.*)$', re.IGNORECASE)

            for i, line in enumerate(lines):
                # Update Plan Title if new_title provided
                title_match = title_pattern.match(line.strip())
                if title_match and new_title is not None:
                    lines[i] = f"Plan Title={new_title[:32]}\n"
                    continue

                # Update Short Identifier
                shortid_match = shortid_pattern.match(line.strip())
                if shortid_match:
                    current_shortid = shortid_match.group(1)
                    if new_shortid is None:
                        new_shortid_value = (current_shortid + "_copy")[:24]
                    else:
                        new_shortid_value = new_shortid[:24]
                    lines[i] = f"Short Identifier={new_shortid_value}\n"

            return lines

        # Use RasUtils to clone the file and update metadata
        RasUtils.clone_file(template_plan_path, new_plan_path, update_plan_metadata)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Plan', new_plan_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_plan_num

    @staticmethod
    @log_call
    def clone_unsteady(template_unsteady: Union[str, Number], new_title=None, ras_object=None):
        """
        Copy unsteady flow files from a template, find the next unsteady number,
        and update the project file accordingly.

        Parameters:
        template_unsteady (Union[str, Number]): Unsteady flow number to use as template (e.g., '01', 1, or 1.0)
        new_title (str, optional): New flow title (max 32 chars, updates "Flow Title=" line)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New unsteady flow number (e.g., '03')

        Example:
        >>> # String input
        >>> new_unsteady_num = RasPlan.clone_unsteady('01',
        ...                                           new_title='Unsteady - HEC-RAS 4.1')
        >>> print(f"New unsteady flow file created: u{new_unsteady_num}")
        >>> # Integer input also works
        >>> new_unsteady_num = RasPlan.clone_unsteady(1)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize unsteady number to two-digit format
        template_unsteady = RasUtils.normalize_ras_number(template_unsteady)

        # Validate new_title length if provided
        if new_title is not None and len(new_title) > 32:
            raise ValueError(
                f"Flow title must be 32 characters or less. "
                f"Got {len(new_title)} characters: '{new_title}'"
            )

        # Update unsteady entries without reinitializing the entire project
        ras_obj.unsteady_df = ras_obj.get_prj_entries('Unsteady')

        new_unsteady_num = RasPlan.get_next_number(ras_obj.unsteady_df['unsteady_number'])
        template_unsteady_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{template_unsteady}"
        new_unsteady_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_num}"

        def update_flow_title(lines):
            """Update Flow Title if new_title provided"""
            if new_title is None:
                return lines

            title_pattern = re.compile(r'^Flow Title=(.*)$', re.IGNORECASE)
            for i, line in enumerate(lines):
                title_match = title_pattern.match(line.strip())
                if title_match:
                    lines[i] = f"Flow Title={new_title[:32]}\n"
                    break
            return lines

        # Use RasUtils to clone the file and update flow title
        RasUtils.clone_file(template_unsteady_path, new_unsteady_path, update_flow_title)

        # Copy the corresponding .hdf file if it exists
        template_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{template_unsteady}.hdf"
        new_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_num}.hdf"
        if template_hdf_path.exists():
            shutil.copy(template_hdf_path, new_hdf_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Unsteady', new_unsteady_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_unsteady_num


    @staticmethod
    @log_call
    def clone_steady(template_flow: Union[str, Number], new_title=None, ras_object=None):
        """
        Copy steady flow files from a template, find the next flow number,
        and update the project file accordingly.

        Parameters:
        template_flow (Union[str, Number]): Flow number to use as template (e.g., '01', 1, or 1.0)
        new_title (str, optional): New flow title (max 32 chars, updates "Flow Title=" line)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New flow number (e.g., '03')

        Example:
        >>> # String input
        >>> new_flow_num = RasPlan.clone_steady('01',
        ...                                      new_title='Steady Flow - HEC-RAS 4.1')
        >>> print(f"New steady flow file created: f{new_flow_num}")
        >>> # Integer input also works
        >>> new_flow_num = RasPlan.clone_steady(1)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize flow number to two-digit format
        template_flow = RasUtils.normalize_ras_number(template_flow)

        # Validate new_title length if provided
        if new_title is not None and len(new_title) > 32:
            raise ValueError(
                f"Flow title must be 32 characters or less. "
                f"Got {len(new_title)} characters: '{new_title}'"
            )

        # Update flow entries without reinitializing the entire project
        ras_obj.flow_df = ras_obj.get_prj_entries('Flow')

        new_flow_num = RasPlan.get_next_number(ras_obj.flow_df['flow_number'])
        template_flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{template_flow}"
        new_flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{new_flow_num}"

        def update_flow_title(lines):
            """Update Flow Title if new_title provided"""
            if new_title is None:
                return lines

            title_pattern = re.compile(r'^Flow Title=(.*)$', re.IGNORECASE)
            for i, line in enumerate(lines):
                title_match = title_pattern.match(line.strip())
                if title_match:
                    lines[i] = f"Flow Title={new_title[:32]}\n"
                    break
            return lines

        # Use RasUtils to clone the file and update flow title
        RasUtils.clone_file(template_flow_path, new_flow_path, update_flow_title)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Flow', new_flow_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)
        
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        return new_flow_num

    @staticmethod
    @log_call
    def clone_geom(template_geom: Union[str, Number], ras_object=None):
        """
        Copy geometry files from a template, find the next geometry number,
        and update the project file accordingly.

        Parameters:
        template_geom (Union[str, Number]): Geometry number to use as template (e.g., '01', 1, or 1.0)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New geometry number (e.g., '03')

        Example:
        >>> # String input
        >>> new_geom_num = RasPlan.clone_geom('01')
        >>> # Integer input also works
        >>> new_geom_num = RasPlan.clone_geom(1)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Normalize geometry number to two-digit format
        template_geom = RasUtils.normalize_ras_number(template_geom)

        # Update geometry entries without reinitializing the entire project
        ras_obj.geom_df = ras_obj.get_prj_entries('Geom')

        new_geom_num = RasPlan.get_next_number(ras_obj.geom_df['geom_number'])
        template_geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{template_geom}"
        new_geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom_num}"

        # Use RasUtils to clone the file
        RasUtils.clone_file(template_geom_path, new_geom_path)

        # Handle HDF file copy
        template_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{template_geom}.hdf"
        new_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom_num}.hdf"
        if template_hdf_path.is_file():
            RasUtils.clone_file(template_hdf_path, new_hdf_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Geom', new_geom_num, ras_object=ras_obj)

        # Update all dataframes in the ras object
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_geom_num

    @staticmethod
    @log_call
    def get_next_number(existing_numbers):
        """
        Determine the next available number from a list of existing numbers.
        
        Parameters:
        existing_numbers (list): List of existing numbers as strings
        
        Returns:
        str: Next available number as a zero-padded string
        
        Example:
        >>> existing_numbers = ['01', '02', '04']
        >>> RasPlan.get_next_number(existing_numbers)
        '03'
        >>> existing_numbers = ['01', '02', '03']
        >>> RasPlan.get_next_number(existing_numbers)
        '04'
        """
        existing_numbers = sorted(int(num) for num in existing_numbers)
        next_number = 1
        for num in existing_numbers:
            if num == next_number:
                next_number += 1
            else:
                break
        return f"{next_number:02d}"

    @staticmethod
    @log_call
    def get_plan_value(
        plan_number_or_path: Union[str, Path],
        key: str,
        ras_object=None
    ) -> Any:
        """
        Retrieve a specific value from a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        key (str): The key to retrieve from the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Any: The value associated with the specified key

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading the plan file

        Available keys and their expected types:
        - 'Computation Interval' (str): Time value for computational time step (e.g., '5SEC', '2MIN')
        - 'DSS File' (str): Name of the DSS file used
        - 'Flow File' (str): Name of the flow input file
        - 'Friction Slope Method' (int): Method selection for friction slope (e.g., 1, 2)
        - 'Geom File' (str): Name of the geometry input file
        - 'Mapping Interval' (str): Time interval for mapping output
        - 'Plan File' (str): Name of the plan file
        - 'Plan Title' (str): Title of the simulation plan
        - 'Program Version' (str): Version number of HEC-RAS
        - 'Run HTab' (int): Flag to run HTab module (-1 or 1)
        - 'Run Post Process' (int): Flag to run post-processing (-1 or 1)
        - 'Run Sediment' (int): Flag to run sediment transport module (0 or 1)
        - 'Run UNET' (int): Flag to run unsteady network module (-1 or 1)
        - 'Run WQNET' (int): Flag to run water quality module (0 or 1)
        - 'Short Identifier' (str): Short name or ID for the plan
        - 'Simulation Date' (str): Start and end dates/times for simulation
        - 'UNET D1 Cores' (int): Number of cores used in 1D calculations
        - 'UNET D2 Cores' (int): Number of cores used in 2D calculations
        - 'PS Cores' (int): Number of cores used in parallel simulation
        - 'UNET Use Existing IB Tables' (int): Flag for using existing internal boundary tables (-1, 0, or 1)
        - 'UNET 1D Methodology' (str): 1D calculation methodology
        - 'UNET D2 Solver Type' (str): 2D solver type
        - 'UNET D2 Name' (str): Name of the 2D area
        - 'Run RASMapper' (int): Flag to run RASMapper for floodplain mapping (-1 for off, 0 for on)
        
        Note: 
        Writing Multi line keys like 'Description' are not supported by this function.

        Example:
        >>> computation_interval = RasPlan.get_plan_value("01", "Computation Interval")
        >>> print(f"Computation interval: {computation_interval}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        supported_plan_keys = {
            'Description', 'Computation Interval', 'DSS File', 'Flow File', 'Friction Slope Method',
            'Geom File', 'Mapping Interval', 'Plan File', 'Plan Title', 'Program Version',
            'Run HTab', 'Run Post Process', 'Run Sediment', 'Run UNET', 'Run WQNET',
            'Short Identifier', 'Simulation Date', 'UNET D1 Cores', 'UNET D2 Cores', 'PS Cores',
            'UNET Use Existing IB Tables', 'UNET 1D Methodology', 'UNET D2 Solver Type', 
            'UNET D2 Name', 'Run RASMapper', 'Run HTab', 'Run UNET'
        }

        if key not in supported_plan_keys:
            logger = logging.getLogger(__name__)
            logger.warning(f"Unknown key: {key}. Valid keys are: {', '.join(supported_plan_keys)}\n Add more keys and explanations in get_plan_value() as needed.")

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()
        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        # Handle core settings specially to convert to integers
        core_keys = {'UNET D1 Cores', 'UNET D2 Cores', 'PS Cores'}
        if key in core_keys:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                try:
                    return int(match.group(1).strip())
                except ValueError:
                    logger = logging.getLogger(__name__)
                    logger.error(f"Could not convert {key} value to integer")
                    return None
            else:
                logger = logging.getLogger(__name__)
                logger.error(f"Key '{key}' not found in the plan file.")
                return None
        elif key == 'Description':
            match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            return match.group(1).strip() if match else None
        else:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()
            else:
                logger = logging.getLogger(__name__)
                logger.error(f"Key '{key}' not found in the plan file.")
                return None





    @staticmethod
    @log_call
    def update_run_flags(
        plan_number_or_path: Union[str, Path],
        geometry_preprocessor: bool = None,
        unsteady_flow_simulation: bool = None,
        run_sediment: bool = None,
        post_processor: bool = None,
        floodplain_mapping: bool = None,
        ras_object=None
    ) -> None:
        """
        Update the run flags in a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        geometry_preprocessor (bool, optional): Set Geometry Preprocessor (Run HTab, -1 = ON, 0 = OFF)
        unsteady_flow_simulation (bool, optional): Set Unsteady Flow (Run UNet, -1 = ON, 0 = OFF)
        run_sediment (bool, optional): Set Run Sediment (Run Sediment, -1 = ON, 0 = OFF)
        post_processor (bool, optional): Set Post Processor (Run PostProcess, -1 = ON, 0 = OFF)
        floodplain_mapping (bool, optional): Set Floodplain Mapping (Run RASMapper, -1 = ON, 0 = OFF)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading or writing the plan file

        Notes:
        - -1 is ON, 0 is OFF
        - Lines affected in plan file:
            Run HTab= -1           # geometry_preprocessor
            Run UNet= -1           # unsteady_flow_simulation
            Run Sediment= 0        # run_sediment
            Run PostProcess= -1    # post_processor
            Run RASMapper= 0       # floodplain_mapping

        Example:
        >>> RasPlan.update_run_flags("01", geometry_preprocessor=True, unsteady_flow_simulation=True, run_sediment=False, post_processor=True, floodplain_mapping=False)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        # Map arguments to plan keys (string in file : argument, ON=-1, OFF=0)
        flag_map = [
            ("Run HTab", geometry_preprocessor),
            ("Run UNet", unsteady_flow_simulation),
            ("Run Sediment", run_sediment),
            ("Run PostProcess", post_processor),
            ("Run RASMapper", floodplain_mapping)
        ]

        try:
            with open(plan_file_path, 'r') as f:
                lines = f.readlines()

            # Annotate which flags got edited for logger
            updated_lines = 0

            for flag, value in flag_map:
                if value is not None:
                    # Find and update the line
                    found = False
                    for idx, line in enumerate(lines):
                        if line.strip().startswith(f"{flag}="):
                            lines[idx] = f"{flag}= {-1 if value else 0}\n"
                            updated_lines += 1
                            found = True
                            break
                    if not found:
                        # If not present, add the line at end (optional; original HEC-RAS behavior retains missing as OFF)
                        lines.append(f"{flag}= {-1 if value else 0}\n")
                        updated_lines += 1

            with open(plan_file_path, 'w') as f:
                f.writelines(lines)

            logger = get_logger(__name__)
            logger.info(
                f"Successfully updated run flags in plan file: {plan_file_path} "
                f"(flags modified: {updated_lines})"
            )

        except IOError as e:
            logger = get_logger(__name__)
            logger.error(f"Error updating run flags in plan file {plan_file_path}: {e}")
            raise


    @staticmethod
    @log_call
    def update_plan_intervals(
        plan_number_or_path: Union[str, Path],
        computation_interval: Optional[str] = None,
        output_interval: Optional[str] = None,
        instantaneous_interval: Optional[str] = None,
        mapping_interval: Optional[str] = None,
        ras_object=None
    ) -> None:
        """
        Update the computation and output intervals in a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        computation_interval (Optional[str]): The new computation interval. Valid entries include:
            '1SEC', '2SEC', '3SEC', '4SEC', '5SEC', '6SEC', '10SEC', '15SEC', '20SEC', '30SEC',
            '1MIN', '2MIN', '3MIN', '4MIN', '5MIN', '6MIN', '10MIN', '15MIN', '20MIN', '30MIN',
            '1HOUR', '2HOUR', '3HOUR', '4HOUR', '6HOUR', '8HOUR', '12HOUR', '1DAY'
        output_interval (Optional[str]): The new output interval. Valid entries are the same as computation_interval.
        instantaneous_interval (Optional[str]): The new instantaneous interval. Valid entries are the same as computation_interval.
        mapping_interval (Optional[str]): The new mapping interval. Valid entries are the same as computation_interval.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
        ValueError: If the plan file is not found or if an invalid interval is provided
        IOError: If there's an error reading or writing the plan file

        Note: This function does not check if the intervals are equal divisors. Ensure you use valid values from HEC-RAS.

        Example:
        >>> RasPlan.update_plan_intervals("01", computation_interval="5SEC", output_interval="1MIN", instantaneous_interval="1HOUR", mapping_interval="5MIN")
        >>> RasPlan.update_plan_intervals("/path/to/plan.p01", computation_interval="10SEC", output_interval="30SEC")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        valid_intervals = [
            '1SEC', '2SEC', '3SEC', '4SEC', '5SEC', '6SEC', '10SEC', '15SEC', '20SEC', '30SEC',
            '1MIN', '2MIN', '3MIN', '4MIN', '5MIN', '6MIN', '10MIN', '15MIN', '20MIN', '30MIN',
            '1HOUR', '2HOUR', '3HOUR', '4HOUR', '6HOUR', '8HOUR', '12HOUR', '1DAY'
        ]

        interval_mapping = {
            'Computation Interval': computation_interval,
            'Output Interval': output_interval,
            'Instantaneous Interval': instantaneous_interval,
            'Mapping Interval': mapping_interval
        }

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            for i, line in enumerate(lines):
                for key, value in interval_mapping.items():
                    if value is not None:
                        if value.upper() not in valid_intervals:
                            raise ValueError(f"Invalid {key}: {value}. Must be one of {valid_intervals}")
                        if line.strip().startswith(key):
                            lines[i] = f"{key}={value.upper()}\n"

            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger = logging.getLogger(__name__)
            logger.info(f"Successfully updated intervals in plan file: {plan_file_path}")

        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error updating intervals in plan file {plan_file_path}: {e}")
            raise
     
     




    @staticmethod
    @log_call
    def read_plan_description(plan_number_or_path: Union[str, Path], ras_object: Optional['RasPrj'] = None) -> str:
        """
        Read the description from the plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The description from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.
        """
        logger = logging.getLogger(__name__)

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()
        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        description_lines = []
        in_description = False
        description_found = False
        for line in lines:
            if line.strip() == "BEGIN DESCRIPTION:":
                in_description = True
                description_found = True
            elif line.strip() == "END DESCRIPTION:":
                break
            elif in_description:
                description_lines.append(line.strip())

        if not description_found:
            logger.warning(f"No description found in plan file: {plan_file_path}")
            return ""

        description = '\n'.join(description_lines)
        logger.info(f"Read description from plan file: {plan_file_path}")
        return description


    @staticmethod
    @log_call
    def update_plan_description(plan_number: Union[str, Number], description: str, ras_object=None):
        """
        Update or insert plan description in the correct location within a plan file.

        The description block will be placed after initial plan parameters
        (Plan Title, Program Version, Short Identifier, Simulation Date, Geom File,
        Flow File, and flow type) but before the Computation Interval line.

        Parameters:
        -----------
        plan_number : Union[str, Number]
            Plan number to update (e.g., '01', 1, or 1.0)
        description : str
            Description text to insert. Will be automatically wrapped in
            BEGIN DESCRIPTION/END DESCRIPTION blocks.
        ras_object : RasPrj, optional
            RAS project object. If None, uses global 'ras' object.
        
        Returns:
        --------
        bool : True if successful, False otherwise
        
        Examples:
        ---------
        >>> RasPlan.update_plan_description('02', 
        ...     'Atlas 14 Uncertainty Analysis\\n' +
        ...     'AEP: 100 years\\n' +
        ...     'Duration: 24 hours\\n' +
        ...     'Confidence Level: upper')
        True
        """
        try:
            # Get the RAS object
            if ras_object is None:
                ras_obj = ras
            else:
                ras_obj = ras_object
            
            # Get plan path
            plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_obj)
            
            # Read the plan file
            with open(plan_path, 'r') as f:
                lines = f.readlines()
            
            # Find existing description block if it exists
            desc_start_idx = None
            desc_end_idx = None
            
            for i, line in enumerate(lines):
                if line.strip().upper().startswith('BEGIN DESCRIPTION'):
                    desc_start_idx = i
                elif line.strip().upper().startswith('END DESCRIPTION'):
                    desc_end_idx = i
                    break
            
            # Find the correct insertion point (before Computation Interval)
            insertion_idx = None
            
            # Primary method: Find Computation Interval line
            for i, line in enumerate(lines):
                if line.strip().startswith('Computation Interval='):
                    insertion_idx = i
                    break
            
            # Fallback method 1: Look for common parameter lines that come after description
            if insertion_idx is None:
                fallback_markers = [
                    'K Sum by GR=',
                    'Std Step Tol=',
                    'Critical Tol=',
                    'Num of Std Step Trials=',
                    'Max Error Tol=',
                    'Flow Tol Ratio=',
                    'Split Flow NTrial=',
                    'Split Flow Tol=',
                    'Split Flow Ratio=',
                    'Log Output Level=',
                    'Friction Slope Method=',
                    'Unsteady Friction Slope Method='
                ]
                
                for i, line in enumerate(lines):
                    for marker in fallback_markers:
                        if line.strip().startswith(marker):
                            insertion_idx = i
                            break
                    if insertion_idx is not None:
                        break
            
            # Fallback method 2: Insert after initial parameters and flow type
            if insertion_idx is None:
                # Find the last of the initial parameters
                initial_params = [
                    'Plan Title=',
                    'Program Version=',
                    'Short Identifier=',
                    'Simulation Date=',
                    'Geom File=',
                    'Flow File='
                ]
                
                last_param_idx = 0
                for i, line in enumerate(lines):
                    for param in initial_params:
                        if line.strip().startswith(param):
                            last_param_idx = max(last_param_idx, i)
                
                # Check for flow type lines after Flow File
                flow_types = ['Subcritical Flow', 'Mixed Flow', 'Supercritical Flow']
                for i in range(last_param_idx + 1, min(last_param_idx + 5, len(lines))):
                    if i < len(lines) and lines[i].strip() in flow_types:
                        last_param_idx = i
                
                insertion_idx = last_param_idx + 1
            
            # Prepare the new description block
            # Ensure description doesn't have trailing newline for proper formatting
            description_clean = description.rstrip()

            description_block = [
                'Begin DESCRIPTION\n',
                description_clean + '\n',
                'END DESCRIPTION\n'
            ]
            
            # Build the new file content
            if desc_start_idx is not None and desc_end_idx is not None:
                # Replace existing description block
                # Keep it in its current location if it's already in the right place
                # Otherwise move it to the correct location
                if desc_start_idx < insertion_idx:
                    # Description is already before insertion point, replace in place
                    new_lines = lines[:desc_start_idx] + description_block + lines[desc_end_idx + 1:]
                else:
                    # Description is after insertion point, need to move it
                    # Remove old description
                    lines_without_desc = lines[:desc_start_idx] + lines[desc_end_idx + 1:]
                    # Insert at correct location
                    new_lines = lines_without_desc[:insertion_idx] + description_block + lines_without_desc[insertion_idx:]
            else:
                # No existing description, insert new one
                new_lines = lines[:insertion_idx] + description_block + lines[insertion_idx:]
            
            # Write the modified content back to the file
            with open(plan_path, 'w') as f:
                f.writelines(new_lines)
            
            # Validate the result (optional debug check)
            if __debug__:  # Only in debug mode
                with open(plan_path, 'r') as f:
                    content = f.read()
                
                # Check that description comes before Computation Interval
                if 'Begin DESCRIPTION' in content and 'Computation Interval=' in content:
                    desc_pos = content.find('Begin DESCRIPTION')
                    comp_pos = content.find('Computation Interval=')
                    if desc_pos > comp_pos:
                        print(f"Warning: Description block may be in wrong position in plan {plan_number}")
            
            return True
            
        except FileNotFoundError:
            print(f"Error: Plan file not found for plan {plan_number}")
            return False
        except IOError as e:
            print(f"Error: IO error updating plan {plan_number}: {e}")
            return False
        except Exception as e:
            print(f"Error: Unexpected error updating plan {plan_number}: {e}")
            import traceback
            traceback.print_exc()
            return False






    




    @staticmethod
    @log_call
    def update_simulation_date(plan_number_or_path: Union[str, Number, Path], start_date: datetime, end_date: datetime, ras_object: Optional['RasPrj'] = None) -> None:
        """
        Update the simulation date for a given plan.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            start_date (datetime): The start date and time for the simulation.
            end_date (datetime): The end date and time for the simulation.
            ras_object (Optional['RasPrj']): The RAS project object. Defaults to None.

        Raises:
            ValueError: If the plan file is not found or if there's an error updating the file.
        """

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        # Format the dates
        formatted_date = f"{start_date.strftime('%d%b%Y').upper()},{start_date.strftime('%H%M')},{end_date.strftime('%d%b%Y').upper()},{end_date.strftime('%H%M')}"

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Simulation Date line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Simulation Date="):
                    lines[i] = f"Simulation Date={formatted_date}\n"
                    updated = True
                    break

            # If Simulation Date line not found, add it at the end
            if not updated:
                lines.append(f"Simulation Date={formatted_date}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated simulation date in plan file: {plan_file_path}")

        except IOError as e:
            logger.error(f"Error updating simulation date in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating simulation date: {e}")

        # Refresh RasPrj dataframes
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()
            ras_object.unsteady_df = ras_object.get_unsteady_entries()

    @staticmethod
    @log_call
    def get_shortid(plan_number_or_path: Union[str, Number, Path], ras_object=None) -> str:
        """
        Get the Short Identifier from a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The Short Identifier from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.

        Example:
            >>> shortid = RasPlan.get_shortid('01')
            >>> print(f"Plan's Short Identifier: {shortid}")
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the Short Identifier using get_plan_value
        shortid = RasPlan.get_plan_value(plan_number_or_path, "Short Identifier", ras_obj)
        
        if shortid is None:
            logger.warning(f"Short Identifier not found in plan: {plan_number_or_path}")
            return ""
        
        logger.info(f"Retrieved Short Identifier: {shortid}")
        return shortid

    @staticmethod
    @log_call
    def set_shortid(plan_number_or_path: Union[str, Number, Path], new_shortid: str, ras_object=None) -> None:
        """
        Set the Short Identifier in a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            new_shortid (str): The new Short Identifier to set (max 24 characters).
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Raises:
            ValueError: If the plan file is not found or if new_shortid is too long.
            IOError: If there's an error updating the plan file.

        Example:
            >>> RasPlan.set_shortid('01', 'NewShortIdentifier')
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Ensure new_shortid is not too long (HEC-RAS limits short identifiers to 24 characters)
        if len(new_shortid) > 24:
            logger.warning(f"Short Identifier too long (24 char max). Truncating: {new_shortid}")
            new_shortid = new_shortid[:24]

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Short Identifier line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Short Identifier="):
                    lines[i] = f"Short Identifier={new_shortid}\n"
                    updated = True
                    break

            # If Short Identifier line not found, add it after Plan Title
            if not updated:
                for i, line in enumerate(lines):
                    if line.startswith("Plan Title="):
                        lines.insert(i+1, f"Short Identifier={new_shortid}\n")
                        updated = True
                        break
                
                # If Plan Title not found either, add at the beginning
                if not updated:
                    lines.insert(0, f"Short Identifier={new_shortid}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated Short Identifier in plan file to: {new_shortid}")

        except IOError as e:
            logger.error(f"Error updating Short Identifier in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating Short Identifier: {e}")

        # Refresh RasPrj dataframes if ras_object provided
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()

    @staticmethod
    @log_call
    def get_plan_title(plan_number_or_path: Union[str, Number, Path], ras_object=None) -> str:
        """
        Get the Plan Title from a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The Plan Title from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.

        Example:
            >>> title = RasPlan.get_plan_title('01')
            >>> print(f"Plan Title: {title}")
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the Plan Title using get_plan_value
        title = RasPlan.get_plan_value(plan_number_or_path, "Plan Title", ras_obj)
        
        if title is None:
            logger.warning(f"Plan Title not found in plan: {plan_number_or_path}")
            return ""
        
        logger.info(f"Retrieved Plan Title: {title}")
        return title

    @staticmethod
    @log_call
    def set_plan_title(plan_number_or_path: Union[str, Number, Path], new_title: str, ras_object=None) -> None:
        """
        Set the Plan Title in a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            new_title (str): The new Plan Title to set.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error updating the plan file.

        Example:
            >>> RasPlan.set_plan_title('01', 'Updated Plan Scenario')
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Plan Title line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Plan Title="):
                    lines[i] = f"Plan Title={new_title}\n"
                    updated = True
                    break

            # If Plan Title line not found, add it at the beginning
            if not updated:
                lines.insert(0, f"Plan Title={new_title}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated Plan Title in plan file to: {new_title}")

        except IOError as e:
            logger.error(f"Error updating Plan Title in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating Plan Title: {e}")

        # Refresh RasPrj dataframes if ras_object provided
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()

    @staticmethod
    @log_call
    def add_hdf_output_variable(
        plan_number_or_path: Union[str, Number, Path],
        variable: str,
        ras_object=None
    ) -> bool:
        """
        Add an HDF output variable to a HEC-RAS plan file.

        This enables additional output variables in the HDF results file, such as
        Face Flow, which is needed for discharge-weighted velocity calculations.

        Args:
            plan_number_or_path (Union[str, Number, Path]): The plan number or path to the plan file.
            variable (str): The variable name to add (e.g., "Face Flow", "Face Shear Stress").
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            bool: True if variable was added or already exists, False on error.

        Supported Variables:
            - "Face Flow" - Flow rate across each face (needed for discharge-weighted velocity)
            - "Face Shear Stress" - Shear stress at each face
            - "Face Cumulative Volume" - Cumulative volume through each face
            - "Cell Cumulative Precipitation" - Cumulative precipitation per cell
            - "Cell Courant" - Courant number per cell

        Example:
            >>> # Enable Face Flow output before running a plan
            >>> RasPlan.add_hdf_output_variable('02', 'Face Flow')
            >>> RasCmdr.compute_plan('02')
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path or not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_number_or_path}")
                return False

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Check if this variable already exists
            target_line = f"HDF Additional Output Variable={variable}"
            for line in lines:
                if line.strip() == target_line:
                    logger.info(f"HDF output variable '{variable}' already exists in plan")
                    return True

            # Find the best location to insert (near other HDF settings)
            insert_index = None
            for i, line in enumerate(lines):
                if line.startswith("HDF Compression="):
                    # Insert before HDF Compression
                    insert_index = i
                    break
                elif line.startswith("HDF "):
                    # Track last HDF line as fallback
                    insert_index = i + 1

            # If no HDF settings found, find Write HDF5 File or end of UNET settings
            if insert_index is None:
                for i, line in enumerate(lines):
                    if line.startswith("Write HDF5 File="):
                        insert_index = i
                        break
                    elif line.startswith("UNET "):
                        insert_index = i + 1

            # Fallback to end of file
            if insert_index is None:
                insert_index = len(lines)

            # Insert the new variable
            lines.insert(insert_index, f"{target_line}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Added HDF output variable '{variable}' to plan file: {plan_file_path.name}")
            return True

        except IOError as e:
            logger.error(f"Error adding HDF output variable to plan file {plan_file_path}: {e}")
            return False

    @staticmethod
    @log_call
    def get_hdf_output_variables(
        plan_number_or_path: Union[str, Number, Path],
        ras_object=None
    ) -> List[str]:
        """
        Get list of additional HDF output variables configured in a plan file.

        Args:
            plan_number_or_path (Union[str, Number, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            List[str]: List of variable names configured for HDF output.

        Example:
            >>> vars = RasPlan.get_hdf_output_variables('02')
            >>> print(vars)  # ['Face Flow', 'Face Shear Stress']
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path or not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_number_or_path}")
                return []

        variables = []
        try:
            with open(plan_file_path, 'r') as file:
                for line in file:
                    if line.startswith("HDF Additional Output Variable="):
                        var_name = line.split("=", 1)[1].strip()
                        variables.append(var_name)

            logger.info(f"Found {len(variables)} HDF output variables in plan")
            return variables

        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            return []

    @staticmethod
    @log_call
    def remove_hdf_output_variable(
        plan_number_or_path: Union[str, Number, Path],
        variable: str,
        ras_object=None
    ) -> bool:
        """
        Remove an HDF output variable from a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Number, Path]): The plan number or path to the plan file.
            variable (str): The variable name to remove.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            bool: True if variable was removed, False if not found or on error.

        Example:
            >>> RasPlan.remove_hdf_output_variable('02', 'Face Flow')
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_obj)
            if not plan_file_path or not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_number_or_path}")
                return False

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Find and remove the variable line
            target_line = f"HDF Additional Output Variable={variable}"
            new_lines = []
            removed = False
            for line in lines:
                if line.strip() == target_line:
                    removed = True
                else:
                    new_lines.append(line)

            if not removed:
                logger.info(f"HDF output variable '{variable}' not found in plan")
                return False

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(new_lines)

            logger.info(f"Removed HDF output variable '{variable}' from plan file")
            return True

        except IOError as e:
            logger.error(f"Error removing HDF output variable from plan file {plan_file_path}: {e}")
            return False
==================================================

File: C:\GH\ras-commander\ras_commander\RasPrj.py
==================================================
"""
RasPrj.py - Manages HEC-RAS projects within the ras-commander library

This module provides a class for managing HEC-RAS projects.

Classes:
    RasPrj: A class for managing HEC-RAS projects.

Functions:
    init_ras_project: Initialize a RAS project.
    get_ras_exe: Determine the HEC-RAS executable path based on the input.

DEVELOPER NOTE:
This class is used to initialize a RAS project and is used in conjunction with the RasCmdr class to manage the execution of RAS plans.
By default, the RasPrj class is initialized with the global 'ras' object.
However, you can create multiple RasPrj instances to manage multiple projects.
Do not mix and match global 'ras' object instances and custom instances of RasPrj - it will cause errors.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function():
        
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are class methods and are designed to be used with instances of the class.

List of Functions in RasPrj:    
- initialize()
- _load_project_data()
- _get_geom_file_for_plan()
- _parse_plan_file()
- _parse_unsteady_file()
- _get_prj_entries()
- _parse_boundary_condition()
- is_initialized (property)
- check_initialized()
- find_ras_prj()
- get_project_name()
- get_prj_entries()
- get_plan_entries()
- get_flow_entries()
- get_unsteady_entries()
- get_geom_entries()
- get_hdf_entries()
- print_data()
- get_plan_value()
- get_boundary_conditions()
        
Functions in RasPrj that are not part of the class:        
- init_ras_project()
- get_ras_exe()

        
        
        
"""
import os
import re
from pathlib import Path
import pandas as pd
from typing import Union, Any, List, Dict, Tuple
import logging
from ras_commander.LoggingConfig import get_logger
from ras_commander.Decorators import log_call

logger = get_logger(__name__)

def read_file_with_fallback_encoding(file_path, encodings=['utf-8', 'latin1', 'cp1252', 'iso-8859-1']):
    """
    Attempt to read a file using multiple encodings.
    
    Args:
        file_path (str or Path): Path to the file to read
        encodings (list): List of encodings to try, in order of preference
    
    Returns:
        tuple: (content, encoding) or (None, None) if all encodings fail
    """
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                content = file.read()
                return content, encoding
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"Error reading file {file_path} with {encoding} encoding: {e}")
            continue
    
    logger.error(f"Failed to read file {file_path} with any of the attempted encodings: {encodings}")
    return None, None

class RasPrj:
    
    def __init__(self):
        self.initialized = False
        self.boundaries_df = None  # New attribute to store boundary conditions
        self.suppress_logging = False  # Add suppress_logging as instance variable

    @log_call
    def initialize(self, project_folder, ras_exe_path, suppress_logging=True, prj_file=None):
        """
        Initialize a RasPrj instance with project folder and RAS executable path.

        IMPORTANT: External users should use init_ras_project() function instead of this method.
        This method is intended for internal use only.

        Args:
            project_folder (str or Path): Path to the HEC-RAS project folder.
            ras_exe_path (str or Path): Path to the HEC-RAS executable.
            suppress_logging (bool, default=True): If True, suppresses initialization logging messages.
            prj_file (str or Path, optional): If provided, use this specific .prj file instead of searching.
                                              This is used when user specifies a .prj file directly.

        Raises:
            ValueError: If no HEC-RAS project file is found in the specified folder,
                        or if the specified prj_file doesn't exist or is invalid.

        Note:
            This method sets up the RasPrj instance by:
            1. Finding the project file (.prj) or using the provided prj_file
            2. Loading project data (plans, geometries, flows)
            3. Extracting boundary conditions
            4. Setting the initialization flag
            5. Loading RASMapper data (.rasmap)
        """
        self.suppress_logging = suppress_logging  # Store suppress_logging state
        self.project_folder = Path(project_folder)

        # If user specified a .prj file directly, use it (Phase 2 optimization)
        if prj_file is not None:
            self.prj_file = Path(prj_file).resolve()
            if not self.prj_file.exists():
                logger.error(f"Specified .prj file does not exist: {self.prj_file}")
                raise ValueError(f"Specified .prj file does not exist: {self.prj_file}. Please check the path and try again.")
            logger.debug(f"Using specified .prj file: {self.prj_file}")
        else:
            # Search for .prj file (existing behavior)
            self.prj_file = self.find_ras_prj(self.project_folder)
            if self.prj_file is None:
                logger.error(f"No HEC-RAS project file found in {self.project_folder}")
                raise ValueError(f"No HEC-RAS project file found in {self.project_folder}. Please check the path and try again.")

        self.project_name = Path(self.prj_file).stem
        self.ras_exe_path = ras_exe_path
        
        # Set initialized to True before loading project data
        self.initialized = True
        
        # Now load the project data
        self._load_project_data()
        self.boundaries_df = self.get_boundary_conditions()
        
        # Load RASMapper data if available
        try:
            # Import here to avoid circular imports
            from .RasMap import RasMap
            self.rasmap_df = RasMap.initialize_rasmap_df(self)
        except ImportError:
            logger.warning("RasMap module not available. RASMapper data will not be loaded.")
            self.rasmap_df = pd.DataFrame(columns=['projection_path', 'profile_lines_path', 'soil_layer_path', 
                                                'infiltration_hdf_path', 'landcover_hdf_path', 'terrain_hdf_path', 
                                                'current_settings'])
        except Exception as e:
            logger.error(f"Error initializing RASMapper data: {e}")
            self.rasmap_df = pd.DataFrame(columns=['projection_path', 'profile_lines_path', 'soil_layer_path', 
                                                'infiltration_hdf_path', 'landcover_hdf_path', 'terrain_hdf_path', 
                                                'current_settings'])

        if not suppress_logging:
            logger.info(f"Initialization complete for project: {self.project_name}")
            logger.info(f"Plan entries: {len(self.plan_df)}, Flow entries: {len(self.flow_df)}, "
                         f"Unsteady entries: {len(self.unsteady_df)}, Geometry entries: {len(self.geom_df)}, "
                         f"Boundary conditions: {len(self.boundaries_df)}")
            logger.info(f"Geometry HDF files found: {self.plan_df['Geom_File'].notna().sum()}")
            logger.info(f"RASMapper data loaded: {not self.rasmap_df.empty}")

    @log_call
    def _load_project_data(self):
        """
        Load project data from the HEC-RAS project file.

        This internal method:
        1. Initializes DataFrames for plan, flow, unsteady, and geometry entries
        2. Ensures all required columns are present with appropriate default values
        3. Sets file paths for all components (geometries, flows, plans)

        Raises:
            Exception: If there's an error loading or processing project data.
        """
        try:
            # Load data frames
            self.unsteady_df = self._get_prj_entries('Unsteady')
            self.plan_df = self._get_prj_entries('Plan')
            self.flow_df = self._get_prj_entries('Flow')
            self.geom_df = self.get_geom_entries()
            
            # Ensure required columns exist
            self._ensure_required_columns()
            
            # Set paths for geometry and flow files
            self._set_file_paths()

            # Make sure all plan paths are properly set
            self._set_plan_paths()

            # Add flow_type column for deterministic steady/unsteady identification
            if not self.plan_df.empty and 'unsteady_number' in self.plan_df.columns:
                self.plan_df['flow_type'] = self.plan_df['unsteady_number'].apply(
                    lambda x: 'Unsteady' if pd.notna(x) else 'Steady'
                )
            else:
                if not self.plan_df.empty:
                    self.plan_df['flow_type'] = 'Unknown'

        except Exception as e:
            logger.error(f"Error loading project data: {e}")
            raise

    def _ensure_required_columns(self):
        """Ensure all required columns exist in plan_df."""
        required_columns = [
            'plan_number', 'unsteady_number', 'geometry_number',
            'Geom File', 'Geom Path', 'Flow File', 'Flow Path', 'full_path'
        ]
        
        for col in required_columns:
            if col not in self.plan_df.columns:
                self.plan_df[col] = None
        
        if not self.plan_df['full_path'].any():
            self.plan_df['full_path'] = self.plan_df['plan_number'].apply(
                lambda x: str(self.project_folder / f"{self.project_name}.p{x}")
            )

    def _set_file_paths(self):
        """Set geometry and flow paths in plan_df."""
        for idx, row in self.plan_df.iterrows():
            try:
                self._set_geom_path(idx, row)
                self._set_flow_path(idx, row)
                
                if not self.suppress_logging:
                    logger.info(f"Plan {row['plan_number']} paths set up")
            except Exception as e:
                logger.error(f"Error processing plan file {row['plan_number']}: {e}")

    def _set_geom_path(self, idx: int, row: pd.Series):
        """Set geometry path for a plan entry."""
        if pd.notna(row['Geom File']):
            geom_path = self.project_folder / f"{self.project_name}.g{row['Geom File']}"
            self.plan_df.at[idx, 'Geom Path'] = str(geom_path)

    def _set_flow_path(self, idx: int, row: pd.Series):
        """Set flow path for a plan entry."""
        if pd.notna(row['Flow File']):
            prefix = 'u' if pd.notna(row['unsteady_number']) else 'f'
            flow_path = self.project_folder / f"{self.project_name}.{prefix}{row['Flow File']}"
            self.plan_df.at[idx, 'Flow Path'] = str(flow_path)

    def _set_plan_paths(self):
        """Set full path information for plan files and their associated geometry and flow files."""
        if self.plan_df.empty:
            logger.debug("Plan DataFrame is empty, no paths to set")
            return
        
        # Ensure full path is set for all plan entries
        if 'full_path' not in self.plan_df.columns or self.plan_df['full_path'].isna().any():
            self.plan_df['full_path'] = self.plan_df['plan_number'].apply(
                lambda x: str(self.project_folder / f"{self.project_name}.p{x}")
            )
        
        # Create the Geom Path and Flow Path columns if they don't exist
        if 'Geom Path' not in self.plan_df.columns:
            self.plan_df['Geom Path'] = None
        if 'Flow Path' not in self.plan_df.columns:
            self.plan_df['Flow Path'] = None
        
        # Update paths for each plan entry
        for idx, row in self.plan_df.iterrows():
            try:
                # Set geometry path if Geom File exists and Geom Path is missing or invalid
                if pd.notna(row['Geom File']):
                    geom_path = self.project_folder / f"{self.project_name}.g{row['Geom File']}"
                    self.plan_df.at[idx, 'Geom Path'] = str(geom_path)
                
                # Set flow path if Flow File exists and Flow Path is missing or invalid
                if pd.notna(row['Flow File']):
                    # Determine the prefix (u for unsteady, f for steady flow)
                    prefix = 'u' if pd.notna(row['unsteady_number']) else 'f'
                    flow_path = self.project_folder / f"{self.project_name}.{prefix}{row['Flow File']}"
                    self.plan_df.at[idx, 'Flow Path'] = str(flow_path)
                
                if not self.suppress_logging:
                    logger.debug(f"Plan {row['plan_number']} paths set up")
            except Exception as e:
                logger.error(f"Error setting paths for plan {row.get('plan_number', idx)}: {e}")

    def _get_geom_file_for_plan(self, plan_number):
        """
        Get the geometry file path for a given plan number.
        
        Args:
            plan_number (str): The plan number to find the geometry file for.
        
        Returns:
            str: The full path to the geometry HDF file, or None if not found.
        """
        plan_file_path = self.project_folder / f"{self.project_name}.p{plan_number}"
        content, encoding = read_file_with_fallback_encoding(plan_file_path)
        
        if content is None:
            return None
        
        try:
            for line in content.splitlines():
                if line.startswith("Geom File="):
                    geom_file = line.strip().split('=')[1]
                    geom_hdf_path = self.project_folder / f"{self.project_name}.{geom_file}.hdf"
                    if geom_hdf_path.exists():
                        return str(geom_hdf_path)
                    else:
                        return None
        except Exception as e:
            logger.error(f"Error reading plan file for geometry: {e}")
        return None


    @staticmethod
    @log_call
    def get_plan_value(
        plan_number_or_path: Union[str, Path],
        key: str,
        ras_object=None
    ) -> Any:
        """
        Retrieve a specific value from a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        key (str): The key to retrieve from the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Any: The value associated with the specified key

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading the plan file
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # These must exactly match the keys in supported_plan_keys from _parse_plan_file
        valid_keys = {
            'Computation Interval',
            'DSS File',
            'Flow File',
            'Friction Slope Method',
            'Geom File',
            'Mapping Interval',
            'Plan Title',
            'Program Version',
            'Run HTab',
            'Run PostProcess',
            'Run Sediment',
            'Run UNet',
            'Run WQNet',
            'Short Identifier',
            'Simulation Date',
            'UNET D1 Cores',
            'UNET D2 Cores',
            'PS Cores',
            'UNET Use Existing IB Tables',
            'UNET 1D Methodology',
            'UNET D2 SolverType',
            'UNET D2 Name',
            'description'  # Special case for description block
        }

        if key not in valid_keys:
            logger.warning(f"Unknown key: {key}. Valid keys are: {', '.join(sorted(valid_keys))}")
            return None

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()
        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        if key == 'description':
            match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            return match.group(1).strip() if match else None
        else:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                value = match.group(1).strip()
                # Convert core values to integers
                if key in ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']:
                    try:
                        return int(value)
                    except ValueError:
                        logger.warning(f"Could not convert {key} value '{value}' to integer")
                        return None
                return value
            
            # Use DEBUG level for missing core values, ERROR for other missing keys
            if key in ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']:
                logger.debug(f"Core setting '{key}' not found in plan file")
            else:
                logger.error(f"Key '{key}' not found in the plan file")
            return None

    def _parse_plan_file(self, plan_file_path):
        """
        Parse a plan file and extract critical information.
        
        Args:
            plan_file_path (Path): Path to the plan file.
        
        Returns:
            dict: Dictionary containing extracted plan information.
        """
        plan_info = {}
        content, encoding = read_file_with_fallback_encoding(plan_file_path)
        
        if content is None:
            logger.error(f"Could not read plan file {plan_file_path} with any supported encoding")
            return plan_info
        
        try:
            # Extract description
            description_match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            if description_match:
                plan_info['description'] = description_match.group(1).strip()
            
            # BEGIN Exception to Style Guide, this is needed to keep the key names consistent with the plan file keys.
            
            # Extract other critical information
            supported_plan_keys = {
                'Computation Interval': r'Computation Interval=(.+)',
                'DSS File': r'DSS File=(.+)',
                'Flow File': r'Flow File=(.+)',
                'Friction Slope Method': r'Friction Slope Method=(.+)',
                'Geom File': r'Geom File=(.+)',
                'Mapping Interval': r'Mapping Interval=(.+)',
                'Plan Title': r'Plan Title=(.+)',
                'Program Version': r'Program Version=(.+)',
                'Run HTab': r'Run HTab=(.+)',
                'Run PostProcess': r'Run PostProcess=(.+)',
                'Run Sediment': r'Run Sediment=(.+)',
                'Run UNet': r'Run UNet=(.+)',
                'Run WQNet': r'Run WQNet=(.+)',
                'Short Identifier': r'Short Identifier=(.+)',
                'Simulation Date': r'Simulation Date=(.+)',
                'UNET D1 Cores': r'UNET D1 Cores=(.+)',
                'UNET D2 Cores': r'UNET D2 Cores=(.+)',
                'PS Cores': r'PS Cores=(.+)',
                'UNET Use Existing IB Tables': r'UNET Use Existing IB Tables=(.+)',
                'UNET 1D Methodology': r'UNET 1D Methodology=(.+)',
                'UNET D2 SolverType': r'UNET D2 SolverType=(.+)',
                'UNET D2 Name': r'UNET D2 Name=(.+)'
            }
            
            # END Exception to Style Guide
            
            # First, explicitly set None for core values
            core_keys = ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']
            for key in core_keys:
                plan_info[key] = None
            
            for key, pattern in supported_plan_keys.items():
                match = re.search(pattern, content)
                if match:
                    value = match.group(1).strip()
                    # Convert core values to integers if they exist
                    if key in core_keys and value:
                        try:
                            value = int(value)
                        except ValueError:
                            logger.warning(f"Could not convert {key} value '{value}' to integer in plan file {plan_file_path}")
                            value = None
                    plan_info[key] = value
                elif key in core_keys:
                    logger.debug(f"Core setting '{key}' not found in plan file {plan_file_path}")
            
            logger.debug(f"Parsed plan file: {plan_file_path} using {encoding} encoding")
        except Exception as e:
            logger.error(f"Error parsing plan file {plan_file_path}: {e}")
        
        return plan_info

    @log_call
    def _get_prj_entries(self, entry_type):
        """
        Extract entries of a specific type from the HEC-RAS project file.
        
        Args:
            entry_type (str): The type of entry to extract (e.g., 'Plan', 'Flow', 'Unsteady', 'Geom').
        
        Returns:
            pd.DataFrame: A DataFrame containing the extracted entries.
        
        Raises:
            Exception: If there's an error reading or processing the project file.
        """
        entries = []
        pattern = re.compile(rf"{entry_type} File=(\w+)")

        try:
            with open(self.prj_file, 'r', encoding='utf-8') as file:
                for line in file:
                    match = pattern.match(line.strip())
                    if match:
                        file_name = match.group(1)
                        full_path = str(self.project_folder / f"{self.project_name}.{file_name}")
                        entry_number = file_name[1:]
                        
                        entry = {
                            f'{entry_type.lower()}_number': entry_number,
                            'full_path': full_path
                        }
                        
                        # Handle Unsteady entries
                        if entry_type == 'Unsteady':
                            entry.update(self._process_unsteady_entry(entry_number, full_path))
                        else:
                            entry.update(self._process_default_entry())
                        
                        # Handle Plan entries
                        if entry_type == 'Plan':
                            entry.update(self._process_plan_entry(entry_number, full_path))
                        
                        entries.append(entry)
            
            df = pd.DataFrame(entries)
            return self._format_dataframe(df, entry_type)
        
        except Exception as e:
            logger.error(f"Error in _get_prj_entries for {entry_type}: {e}")
            raise

    def _process_unsteady_entry(self, entry_number: str, full_path: str) -> dict:
        """Process unsteady entry data."""
        entry = {'unsteady_number': entry_number}
        unsteady_info = self._parse_unsteady_file(Path(full_path))
        entry.update(unsteady_info)
        return entry

    def _process_default_entry(self) -> dict:
        """Process default entry data."""
        return {
            'unsteady_number': None
        }

    def _process_plan_entry(self, entry_number: str, full_path: str) -> dict:
        """Process plan entry data."""
        entry = {}
        plan_info = self._parse_plan_file(Path(full_path))
        
        if plan_info:
            entry.update(self._process_flow_file(plan_info))
            entry.update(self._process_geom_file(plan_info))
            
            # Add remaining plan info
            for key, value in plan_info.items():
                if key not in ['Flow File', 'Geom File']:
                    entry[key] = value
            
            # Add HDF results path
            hdf_results_path = self.project_folder / f"{self.project_name}.p{entry_number}.hdf"
            entry['HDF_Results_Path'] = str(hdf_results_path) if hdf_results_path.exists() else None
        
        return entry

    def _process_flow_file(self, plan_info: dict) -> dict:
        """Process flow file information from plan info."""
        flow_file = plan_info.get('Flow File')
        if flow_file and flow_file.startswith('u'):
            return {
                'unsteady_number': flow_file[1:],
                'Flow File': flow_file[1:]
            }
        return {
            'unsteady_number': None,
            'Flow File': flow_file[1:] if flow_file and flow_file.startswith('f') else None
        }

    def _process_geom_file(self, plan_info: dict) -> dict:
        """Process geometry file information from plan info."""
        geom_file = plan_info.get('Geom File')
        if geom_file and geom_file.startswith('g'):
            return {
                'geometry_number': geom_file[1:],
                'Geom File': geom_file[1:]
            }
        return {
            'geometry_number': None,
            'Geom File': None
        }

    def _parse_unsteady_file(self, unsteady_file_path):
        """
        Parse an unsteady flow file and extract critical information.
        
        Args:
            unsteady_file_path (Path): Path to the unsteady flow file.
        
        Returns:
            dict: Dictionary containing extracted unsteady flow information.
        """
        unsteady_info = {}
        content, encoding = read_file_with_fallback_encoding(unsteady_file_path)
        
        if content is None:
            return unsteady_info
        
        try:
            # BEGIN Exception to Style Guide, this is needed to keep the key names consistent with the unsteady file keys.
            
            supported_unsteady_keys = {
                'Flow Title': r'Flow Title=(.+)',
                'Program Version': r'Program Version=(.+)',
                'Use Restart': r'Use Restart=(.+)',
                'Precipitation Mode': r'Precipitation Mode=(.+)',
                'Wind Mode': r'Wind Mode=(.+)',
                'Met BC=Precipitation|Mode': r'Met BC=Precipitation\|Mode=(.+)',
                'Met BC=Evapotranspiration|Mode': r'Met BC=Evapotranspiration\|Mode=(.+)',
                'Met BC=Precipitation|Expanded View': r'Met BC=Precipitation\|Expanded View=(.+)',
                'Met BC=Precipitation|Constant Units': r'Met BC=Precipitation\|Constant Units=(.+)',
                'Met BC=Precipitation|Gridded Source': r'Met BC=Precipitation\|Gridded Source=(.+)'
            }
            
            # END Exception to Style Guide
            
            for key, pattern in supported_unsteady_keys.items():
                match = re.search(pattern, content)
                if match:
                    unsteady_info[key] = match.group(1).strip()
        
        except Exception as e:
            logger.error(f"Error parsing unsteady file {unsteady_file_path}: {e}")
        
        return unsteady_info

    @property
    def is_initialized(self):
        """
        Check if the RasPrj instance has been initialized.

        Returns:
            bool: True if the instance has been initialized, False otherwise.
        """
        return self.initialized

    @log_call
    def check_initialized(self):
        """
        Ensure that the RasPrj instance has been initialized before operations.

        Raises:
            RuntimeError: If the project has not been initialized with init_ras_project().

        Note:
            This method is called by other methods to validate the project state before
            performing operations. Users typically don't need to call this directly.
        """
        if not self.initialized:
            raise RuntimeError("Project not initialized. Call init_ras_project() first.")

    @staticmethod
    @log_call
    def find_ras_prj(folder_path):
        """
        Find the appropriate HEC-RAS project file (.prj) in the given folder.
        
        This method uses several strategies to locate the correct project file:
        1. If only one .prj file exists, it is selected
        2. If multiple .prj files exist, it tries to match with .rasmap file names
        3. As a last resort, it scans files for "Proj Title=" content
        
        Args:
            folder_path (str or Path): Path to the folder containing HEC-RAS files.
        
        Returns:
            Path: The full path of the selected .prj file or None if no suitable file is found.
        
        Example:
            >>> project_file = RasPrj.find_ras_prj("/path/to/ras_project")
            >>> if project_file:
            ...     print(f"Found project file: {project_file}")
            ... else:
            ...     print("No project file found")
        """
        folder_path = Path(folder_path)
        prj_files = list(folder_path.glob("*.prj"))
        rasmap_files = list(folder_path.glob("*.rasmap"))
        if len(prj_files) == 1:
            return prj_files[0].resolve()
        if len(prj_files) > 1:
            if len(rasmap_files) == 1:
                base_filename = rasmap_files[0].stem
                prj_file = folder_path / f"{base_filename}.prj"
                if prj_file.exists():
                    return prj_file.resolve()
            for prj_file in prj_files:
                try:
                    with open(prj_file, 'r') as file:
                        content = file.read()
                        if "Proj Title=" in content:
                            return prj_file.resolve()
                except Exception:
                    continue
        return None


    @log_call
    def get_project_name(self):
        """
        Get the name of the HEC-RAS project (without file extension).

        Returns:
            str: The name of the project.

        Raises:
            RuntimeError: If the project has not been initialized.

        Example:
            >>> project_name = ras.get_project_name()
            >>> print(f"Working with project: {project_name}")
        """
        self.check_initialized()
        return self.project_name

    @log_call
    def set_current_plan(self, plan_number: Union[str, int]):
        """
        Set the current plan in the HEC-RAS project file (.prj).

        This ensures that when HEC-RAS opens, it opens with the specified plan active.

        Args:
            plan_number (Union[str, int]): The plan number to set as current (e.g., "01" or 1)

        Raises:
            RuntimeError: If the project has not been initialized
            ValueError: If the plan number is invalid or not found
            IOError: If there's an error reading or writing the project file

        Example:
            >>> ras.set_current_plan("01")
            >>> # HEC-RAS will now open with plan 01 active
        """
        self.check_initialized()

        # Normalize plan number to 2-digit string with leading zero
        from .RasUtils import RasUtils
        plan_number_str = RasUtils.normalize_ras_number(plan_number)

        # Verify plan exists in plan_df
        if plan_number_str not in self.plan_df['plan_number'].values:
            available_plans = ', '.join(self.plan_df['plan_number'].values)
            raise ValueError(
                f"Plan {plan_number_str} not found in project. "
                f"Available plans: {available_plans}"
            )

        try:
            # Read the project file
            with open(self.prj_file, 'r') as f:
                lines = f.readlines()

            # Find and update the Current Plan line
            updated = False
            for i, line in enumerate(lines):
                if line.strip().startswith('Current Plan='):
                    lines[i] = f"Current Plan=p{plan_number_str}\n"
                    updated = True
                    break

            # If Current Plan line doesn't exist, add it after Proj Title
            if not updated:
                for i, line in enumerate(lines):
                    if line.strip().startswith('Proj Title='):
                        lines.insert(i + 1, f"Current Plan=p{plan_number_str}\n")
                        updated = True
                        break

            if not updated:
                raise ValueError("Could not find 'Proj Title=' or 'Current Plan=' in project file")

            # Write back to file
            with open(self.prj_file, 'w') as f:
                f.writelines(lines)

            logger.info(f"Set current plan to p{plan_number_str} in {self.prj_file}")

        except IOError as e:
            logger.error(f"Error updating current plan in {self.prj_file}: {e}")
            raise

    @log_call
    def get_prj_entries(self, entry_type):
        """
        Get entries of a specific type from the HEC-RAS project.

        This method extracts files of the specified type from the project file,
        parses their content, and returns a structured DataFrame.

        Args:
            entry_type (str): The type of entry to retrieve ('Plan', 'Flow', 'Unsteady', or 'Geom').

        Returns:
            pd.DataFrame: A DataFrame containing the requested entries with appropriate columns.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> # Get all geometry files in the project
            >>> geom_entries = ras.get_prj_entries('Geom')
            >>> print(f"Project contains {len(geom_entries)} geometry files")
        
        Note:
            This is a generic method. For specific file types, use the dedicated methods:
            get_plan_entries(), get_flow_entries(), get_unsteady_entries(), get_geom_entries()
        """
        self.check_initialized()
        return self._get_prj_entries(entry_type)

    @log_call
    def get_plan_entries(self):
        """
        Get all plan entries from the HEC-RAS project.
        
        Returns a DataFrame containing all plan files (.p*) in the project
        with their associated properties, paths and settings.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'plan_number', 'full_path',
                          'unsteady_number', 'geometry_number', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> plan_entries = ras.get_plan_entries()
            >>> print(f"Project contains {len(plan_entries)} plan files")
            >>> # Display the first plan's properties
            >>> if not plan_entries.empty:
            ...     print(plan_entries.iloc[0])
        """
        self.check_initialized()
        return self._get_prj_entries('Plan')

    @log_call
    def get_flow_entries(self):
        """
        Get all flow entries from the HEC-RAS project.
        
        Returns a DataFrame containing all flow files (.f*) in the project
        with their associated properties and paths.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'flow_number', 'full_path', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> flow_entries = ras.get_flow_entries()
            >>> print(f"Project contains {len(flow_entries)} flow files")
            >>> # Display the first flow file's properties
            >>> if not flow_entries.empty:
            ...     print(flow_entries.iloc[0])
        """
        self.check_initialized()
        return self._get_prj_entries('Flow')

    @log_call
    def get_unsteady_entries(self):
        """
        Get all unsteady flow entries from the HEC-RAS project.
        
        Returns a DataFrame containing all unsteady flow files (.u*) in the project
        with their associated properties and paths.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'unsteady_number', 'full_path', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> unsteady_entries = ras.get_unsteady_entries()
            >>> print(f"Project contains {len(unsteady_entries)} unsteady flow files")
            >>> # Display the first unsteady file's properties
            >>> if not unsteady_entries.empty:
            ...     print(unsteady_entries.iloc[0])
        """
        self.check_initialized()
        return self._get_prj_entries('Unsteady')

    @log_call
    def get_geom_entries(self):
        """
        Get all geometry entries from the HEC-RAS project.
        
        Returns a DataFrame containing all geometry files (.g*) in the project
        with their associated properties, paths and HDF links.

        Returns:
            pd.DataFrame: A DataFrame with columns including 'geom_number', 'full_path', 
                          'hdf_path', etc.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> geom_entries = ras.get_geom_entries()
            >>> print(f"Project contains {len(geom_entries)} geometry files")
            >>> # Display the first geometry file's properties
            >>> if not geom_entries.empty:
            ...     print(geom_entries.iloc[0])
        """
        self.check_initialized()
        geom_pattern = re.compile(r'Geom File=(\w+)')
        geom_entries = []

        try:
            with open(self.prj_file, 'r') as f:
                for line in f:
                    match = geom_pattern.search(line)
                    if match:
                        geom_entries.append(match.group(1))
        
            geom_df = pd.DataFrame({'geom_file': geom_entries})
            geom_df['geom_number'] = geom_df['geom_file'].str.extract(r'(\d+)$')
            geom_df['full_path'] = geom_df['geom_file'].apply(lambda x: str(self.project_folder / f"{self.project_name}.{x}"))
            geom_df['hdf_path'] = geom_df['full_path'] + ".hdf"
            
            if not self.suppress_logging:  # Only log if suppress_logging is False
                logger.info(f"Found {len(geom_df)} geometry entries")
            return geom_df
        except Exception as e:
            logger.error(f"Error reading geometry entries from project file: {e}")
            raise
    
    @log_call
    def get_hdf_entries(self):
        """
        Get all plan entries that have associated HDF results files.
        
        This method identifies which plans have been successfully computed
        and have HDF results available for further analysis.
        
        Returns:
            pd.DataFrame: A DataFrame containing plan entries with HDF results.
                          Returns an empty DataFrame if no results are found.
        
        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> hdf_entries = ras.get_hdf_entries()
            >>> if hdf_entries.empty:
            ...     print("No computed results found. Run simulations first.")
            ... else:
            ...     print(f"Found results for {len(hdf_entries)} plans")
        
        Note:
            This is useful for identifying which plans have been successfully computed
            and can be used for further results analysis.
        """
        self.check_initialized()
        
        hdf_entries = self.plan_df[self.plan_df['HDF_Results_Path'].notna()].copy()
        
        if hdf_entries.empty:
            return pd.DataFrame(columns=self.plan_df.columns)
        
        return hdf_entries
    
    
    @log_call
    def print_data(self):
        """
        Print a comprehensive summary of all RAS Object data for this instance.
        
        This method outputs:
        - Project information (name, folder, file paths)
        - Summary of plans, flows, geometries, and unsteady files
        - HDF results availability
        - Boundary conditions
        
        Useful for debugging, validation, and exploring project structure.

        Raises:
            RuntimeError: If the project has not been initialized.
        
        Example:
            >>> ras.print_data()  # Displays complete project overview
        """
        self.check_initialized()
        logger.info(f"--- Data for {self.project_name} ---")
        logger.info(f"Project folder: {self.project_folder}")
        logger.info(f"PRJ file: {self.prj_file}")
        logger.info(f"HEC-RAS executable: {self.ras_exe_path}")
        logger.info("Plan files:")
        logger.info(f"\n{self.plan_df}")
        logger.info("Flow files:")
        logger.info(f"\n{self.flow_df}")
        logger.info("Unsteady flow files:")
        logger.info(f"\n{self.unsteady_df}")
        logger.info("Geometry files:")
        logger.info(f"\n{self.geom_df}")
        logger.info("HDF entries:")
        logger.info(f"\n{self.get_hdf_entries()}")
        logger.info("Boundary conditions:")
        logger.info(f"\n{self.boundaries_df}")
        logger.info("----------------------------")

    @log_call
    def get_boundary_conditions(self) -> pd.DataFrame:
        """
        Extract boundary conditions from unsteady flow files into a structured DataFrame.

        This method:
        1. Parses all unsteady flow files to extract boundary condition information
        2. Creates a structured DataFrame with boundary locations, types and parameters
        3. Links boundary conditions to their respective unsteady flow files

        Supported boundary condition types include:
        - Flow Hydrograph
        - Stage Hydrograph
        - Precipitation Hydrograph
        - Rating Curve
        - Normal Depth
        - Lateral Inflow Hydrograph
        - Uniform Lateral Inflow Hydrograph
        - Gate Opening

        Returns:
            pd.DataFrame: A DataFrame containing detailed boundary condition information.
                              Returns an empty DataFrame if no unsteady flow files are present.
        
        Example:
            >>> boundaries = ras.get_boundary_conditions()
            >>> if not boundaries.empty:
            ...     print(f"Found {len(boundaries)} boundary conditions")
            ...     # Show flow hydrographs only
            ...     flow_hydrographs = boundaries[boundaries['bc_type'] == 'Flow Hydrograph']
            ...     print(f"Project has {len(flow_hydrographs)} flow hydrographs")
        
        Note:
            To see unparsed boundary condition lines for debugging, set logging to DEBUG:
            import logging
            logging.getLogger().setLevel(logging.DEBUG)
        """
        boundary_data = []
        
        # Check if unsteady_df is empty
        if self.unsteady_df.empty:
            logger.info("No unsteady flow files found in the project.")
            return pd.DataFrame()  # Return an empty DataFrame
        
        for _, row in self.unsteady_df.iterrows():
            unsteady_file_path = row['full_path']
            unsteady_number = row['unsteady_number']
            
            try:
                with open(unsteady_file_path, 'r') as file:
                    content = file.read()
            except IOError as e:
                logger.error(f"Error reading unsteady file {unsteady_file_path}: {e}")
                continue
                
            bc_blocks = re.split(r'(?=Boundary Location=)', content)[1:]
            
            for i, block in enumerate(bc_blocks, 1):
                bc_info, unparsed_lines = self._parse_boundary_condition(block, unsteady_number, i)
                boundary_data.append(bc_info)
                
                if unparsed_lines:
                    logger.debug(f"Unparsed lines for boundary condition {i} in unsteady file {unsteady_number}:\n{unparsed_lines}")
        
        if not boundary_data:
            logger.info("No boundary conditions found in unsteady flow files.")
            return pd.DataFrame()  # Return an empty DataFrame if no boundary conditions were found
        
        boundaries_df = pd.DataFrame(boundary_data)
        
        # Merge with unsteady_df to get relevant unsteady flow file information
        merged_df = pd.merge(boundaries_df, self.unsteady_df, 
                             left_on='unsteady_number', right_on='unsteady_number', how='left')
        
        return merged_df

    def _parse_boundary_condition(self, block: str, unsteady_number: str, bc_number: int) -> Tuple[Dict, str]:
        lines = block.split('\n')
        bc_info = {
            'unsteady_number': unsteady_number,
            'boundary_condition_number': bc_number
        }
        
        parsed_lines = set()
        
        # Parse Boundary Location
        boundary_location = lines[0].split('=')[1].strip()
        fields = [field.strip() for field in boundary_location.split(',')]
        bc_info.update({
            'river_reach_name': fields[0] if len(fields) > 0 else '',
            'river_station': fields[1] if len(fields) > 1 else '',
            'storage_area_name': fields[2] if len(fields) > 2 else '',
            'pump_station_name': fields[3] if len(fields) > 3 else ''
        })
        parsed_lines.add(0)
        
        # Determine BC Type
        bc_types = {
            'Flow Hydrograph=': 'Flow Hydrograph',
            'Lateral Inflow Hydrograph=': 'Lateral Inflow Hydrograph',
            'Uniform Lateral Inflow Hydrograph=': 'Uniform Lateral Inflow Hydrograph',
            'Stage Hydrograph=': 'Stage Hydrograph',
            'Precipitation Hydrograph=': 'Precipitation Hydrograph',
            'Rating Curve=': 'Rating Curve',
            'Friction Slope=': 'Normal Depth',
            'Gate Name=': 'Gate Opening'
        }
        
        bc_info['bc_type'] = 'Unknown'
        bc_info['hydrograph_type'] = None
        for i, line in enumerate(lines[1:], 1):
            for key, bc_type in bc_types.items():
                if line.startswith(key):
                    bc_info['bc_type'] = bc_type
                    if 'Hydrograph' in bc_type:
                        bc_info['hydrograph_type'] = bc_type
                    parsed_lines.add(i)
                    break
            if bc_info['bc_type'] != 'Unknown':
                break
        
        # Parse other fields
        known_fields = ['Interval', 'DSS Path', 'Use DSS', 'Use Fixed Start Time', 'Fixed Start Date/Time',
                        'Is Critical Boundary', 'Critical Boundary Flow', 'DSS File']
        for i, line in enumerate(lines):
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                if key in known_fields:
                    bc_info[key] = value.strip()
                    parsed_lines.add(i)
        
        # Handle hydrograph values
        bc_info['hydrograph_num_values'] = 0
        if bc_info['hydrograph_type']:
            hydrograph_key = f"{bc_info['hydrograph_type']}="
            hydrograph_line = next((line for i, line in enumerate(lines) if line.startswith(hydrograph_key)), None)
            if hydrograph_line:
                hydrograph_index = lines.index(hydrograph_line)
                values_count = int(hydrograph_line.split('=')[1].strip())
                bc_info['hydrograph_num_values'] = values_count
                if values_count > 0:
                    values = ' '.join(lines[hydrograph_index + 1:]).split()[:values_count]
                    bc_info['hydrograph_values'] = values
                    parsed_lines.update(range(hydrograph_index, hydrograph_index + (values_count // 5) + 2))
        
        # Collect unparsed lines
        unparsed_lines = '\n'.join(line for i, line in enumerate(lines) if i not in parsed_lines and line.strip())
        
        if unparsed_lines:
            logger.debug(f"Unparsed lines for boundary condition {bc_number} in unsteady file {unsteady_number}:\n{unparsed_lines}")
        
        return bc_info, unparsed_lines

    @log_call
    def _format_dataframe(self, df, entry_type):
        """
        Format the DataFrame according to the desired column structure.
        
        Args:
            df (pd.DataFrame): The DataFrame to format.
            entry_type (str): The type of entry (e.g., 'Plan', 'Flow', 'Unsteady', 'Geom').
        
        Returns:
            pd.DataFrame: The formatted DataFrame.
        """
        if df.empty:
            return df
        
        if entry_type == 'Plan':
            # Set required column order
            first_cols = ['plan_number', 'unsteady_number', 'geometry_number']
            
            # Standard plan key columns in the exact order specified
            plan_key_cols = [
                'Plan Title', 'Program Version', 'Short Identifier', 'Simulation Date',
                'Std Step Tol', 'Computation Interval', 'Output Interval', 'Instantaneous Interval',
                'Mapping Interval', 'Run HTab', 'Run UNet', 'Run Sediment', 'Run PostProcess',
                'Run WQNet', 'Run RASMapper', 'UNET Use Existing IB Tables', 'HDF_Results_Path',
                'UNET 1D Methodology', 'Write IC File', 'Write IC File at Fixed DateTime',
                'IC Time', 'Write IC File Reoccurance', 'Write IC File at Sim End'
            ]
            
            # Additional convenience columns
            file_path_cols = ['Geom File', 'Geom Path', 'Flow File', 'Flow Path']
            
            # Special columns that must be preserved
            special_cols = ['HDF_Results_Path']
            
            # Build the final column list
            all_cols = first_cols.copy()
            
            # Add plan key columns if they exist
            for col in plan_key_cols:
                if col in df.columns and col not in all_cols and col not in special_cols:
                    all_cols.append(col)
            
            # Add any remaining columns not explicitly specified
            other_cols = [col for col in df.columns if col not in all_cols + file_path_cols + special_cols + ['full_path']]
            all_cols.extend(other_cols)
            
            # Add HDF_Results_Path if it exists (ensure it comes before file paths)
            for special_col in special_cols:
                if special_col in df.columns and special_col not in all_cols:
                    all_cols.append(special_col)
            
            # Add file path columns at the end
            all_cols.extend(file_path_cols)
            
            # Rename plan_number column
            df = df.rename(columns={f'{entry_type.lower()}_number': 'plan_number'})
            
            # Fill in missing columns with None
            for col in all_cols:
                if col not in df.columns:
                    df[col] = None
            
            # Make sure full_path column is preserved and included
            if 'full_path' in df.columns and 'full_path' not in all_cols:
                all_cols.append('full_path')
            
            # Return DataFrame with specified column order
            cols_to_return = [col for col in all_cols if col in df.columns]
            return df[cols_to_return]
        
        return df


# Create a global instance named 'ras'
# Defining the global instance allows the init_ras_project function to initialize the project.
# This only happens on the library initialization, not when the user calls init_ras_project.
ras = RasPrj()

# END OF CLASS DEFINITION


# START OF FUNCTION DEFINITIONS

@log_call
def init_ras_project(ras_project_folder, ras_version=None, ras_object=None):
    """
    Initialize a RAS project for use with the ras-commander library.

    This is the primary function for setting up a HEC-RAS project. It:
    1. Finds the project file (.prj) in the specified folder OR uses the provided .prj file
    2. Validates .prj files by checking for "Proj Title=" marker
    3. Identifies the appropriate HEC-RAS executable
    4. Loads project data (plans, geometries, flows)
    5. Creates dataframes containing project components

    Args:
        ras_project_folder (str or Path): Path to the RAS project folder OR direct path to a .prj file.
                                          If a .prj file is provided:
                                          - File is validated to have .prj extension
                                          - File content is checked for "Proj Title=" marker
                                          - Parent folder is used as the project folder
        ras_version (str, optional): The version of RAS to use (e.g., "6.6") OR
                                     a full path to the Ras.exe file (e.g., "D:/Programs/HEC/HEC-RAS/6.6/Ras.exe").
                                     If None, will attempt to detect from plan files.
        ras_object (RasPrj, optional): If None, updates the global 'ras' object.
                                       If a RasPrj instance, updates that instance.
                                       If any other value, creates and returns a new RasPrj instance.

    Returns:
        RasPrj: An initialized RasPrj instance.

    Raises:
        FileNotFoundError: If the specified project folder or .prj file doesn't exist.
        ValueError: If the provided file is not a .prj file, does not contain "Proj Title=",
                    or if no HEC-RAS project file is found in the folder.

    Example:
        >>> # Initialize using project folder (existing behavior)
        >>> init_ras_project("/path/to/project", "6.6")
        >>> print(f"Initialized project: {ras.project_name}")
        >>>
        >>> # Initialize using direct .prj file path (new feature)
        >>> init_ras_project("/path/to/project/MyModel.prj", "6.6")
        >>> print(f"Initialized project: {ras.project_name}")
        >>>
        >>> # Create a new RasPrj instance with .prj file
        >>> my_project = init_ras_project("/path/to/project/MyModel.prj", "6.6", "new")
        >>> print(f"Created project instance: {my_project.project_name}")
    """
    # Convert to Path object for consistent handling
    input_path = Path(ras_project_folder).resolve()

    # Detect if input is a file or folder
    if input_path.is_file():
        # User provided a .prj file path
        if input_path.suffix.lower() != '.prj':
            error_msg = f"The provided file is not a HEC-RAS project file (.prj): {input_path}"
            logger.error(error_msg)
            raise ValueError(f"{error_msg}. Please provide either a project folder or a .prj file.")

        # Enhanced validation: Check if file contains "Proj Title=" to verify it's a HEC-RAS project file
        try:
            content, encoding = read_file_with_fallback_encoding(input_path)
            if content is None or "Proj Title=" not in content:
                error_msg = f"The file does not appear to be a valid HEC-RAS project file (missing 'Proj Title='): {input_path}"
                logger.error(error_msg)
                raise ValueError(f"{error_msg}. Please provide a valid HEC-RAS .prj file.")
            logger.debug(f"Validated .prj file contains 'Proj Title=' marker")
        except Exception as e:
            error_msg = f"Error validating .prj file: {e}"
            logger.error(error_msg)
            raise ValueError(f"{error_msg}. Please ensure the file is a valid HEC-RAS project file.")

        # Extract the parent folder to use as project_folder
        project_folder = input_path.parent
        specified_prj_file = input_path  # Store for optimization
        logger.debug(f"User provided .prj file: {input_path}")
        logger.debug(f"Using parent folder as project_folder: {project_folder}")

    elif input_path.is_dir():
        # User provided a folder path (existing behavior)
        project_folder = input_path
        specified_prj_file = None
        logger.debug(f"User provided folder path: {project_folder}")

    else:
        # Path doesn't exist
        if input_path.suffix.lower() == '.prj':
            error_msg = f"The specified .prj file does not exist: {input_path}"
            logger.error(error_msg)
            raise FileNotFoundError(f"{error_msg}. Please check the path and try again.")
        else:
            error_msg = f"The specified RAS project folder does not exist: {input_path}"
            logger.error(error_msg)
            raise FileNotFoundError(f"{error_msg}. Please check the path and try again.")

    # Determine which RasPrj instance to use
    if ras_object is None:
        # Use the global 'ras' object
        logger.debug("Initializing global 'ras' object via init_ras_project function.")
        ras_object = ras
    elif not isinstance(ras_object, RasPrj):
        # Create a new RasPrj instance
        logger.debug("Creating a new RasPrj instance.")
        ras_object = RasPrj()
    
    ras_exe_path = None
    
    # Use version specified by user if provided
    if ras_version is not None:
        ras_exe_path = get_ras_exe(ras_version)
        if ras_exe_path == "Ras.exe" and ras_version != "Ras.exe":
            logger.warning(f"HEC-RAS Version {ras_version} was not found. Running HEC-RAS will fail.")
    else:
        # No version specified, try to detect from plan files
        detected_version = None
        logger.info("No HEC-RAS Version Specified.Attempting to detect HEC-RAS version from plan files.")
        
        # Look for .pXX files in project folder
        logger.info(f"Searching for plan files in {project_folder}")
        # Search for any file with .p01 through .p99 extension, regardless of base name
        plan_files = list(project_folder.glob("*.p[0-9][0-9]"))
        
        if not plan_files:
            logger.info(f"No plan files found in {project_folder}")
        
        for plan_file in plan_files:
            logger.info(f"Found plan file: {plan_file.name}")
            content, encoding = read_file_with_fallback_encoding(plan_file)
            
            if not content:
                logger.info(f"Could not read content from {plan_file.name}")
                continue
                
            logger.info(f"Successfully read plan file with {encoding} encoding")
            
            # Look for Program Version in plan file
            for line in content.splitlines():
                if line.startswith("Program Version="):
                    version = line.split("=")[1].strip()
                    logger.info(f"Found Program Version={version} in {plan_file.name}")
                    
                    # Replace 00 in version string if present
                    if "00" in version:
                        version = version.replace("00", "0")
                    
                    # Try to get RAS executable for this version
                    test_exe_path = get_ras_exe(version)
                    logger.info(f"Checking RAS executable path: {test_exe_path}")
                    
                    if test_exe_path != "Ras.exe":
                        detected_version = version
                        ras_exe_path = test_exe_path
                        logger.debug(f"Found valid HEC-RAS version {version} in plan file {plan_file.name}")
                        break
                    else:
                        logger.info(f"Version {version} not found in default installation path")
            
            if detected_version:
                break
        
        if not detected_version:
            logger.error("No valid HEC-RAS version found in any plan files.")
            ras_exe_path = "Ras.exe"
            logger.warning("No valid HEC-RAS version was detected. Running HEC-RAS will fail.")
    
    # Initialize or re-initialize with the determined executable path
    # Pass specified_prj_file to avoid re-searching when user provided .prj file directly
    if specified_prj_file is not None:
        ras_object.initialize(project_folder, ras_exe_path, prj_file=specified_prj_file)
    else:
        ras_object.initialize(project_folder, ras_exe_path)

    # Store version for RasControl (legacy COM interface support)
    ras_object.ras_version = ras_version if ras_version else detected_version

    # Always update the global ras object as well
    if ras_object is not ras:
        if specified_prj_file is not None:
            ras.initialize(project_folder, ras_exe_path, prj_file=specified_prj_file)
        else:
            ras.initialize(project_folder, ras_exe_path)
        # Also store version in global ras object
        ras.ras_version = ras_version if ras_version else detected_version
        logger.debug("Global 'ras' object also updated to match the new project.")

    logger.debug(f"Project initialized. Project folder: {ras_object.project_folder}")
    logger.debug(f"Using HEC-RAS executable: {ras_exe_path}")
    return ras_object

@log_call
def get_ras_exe(ras_version=None):
    """
    Determine the HEC-RAS executable path based on the input.
    
    This function attempts to find the HEC-RAS executable in the following order:
    1. If ras_version is a valid file path to an .exe file, use that path directly
       (useful for non-standard installations or non-C: drive installations)
    2. If ras_version is a known version number, use default installation path (on C: drive)
    3. If global 'ras' object has ras_exe_path, use that
    4. As a fallback, return "Ras.exe" but log an error
    
    Args:
        ras_version (str, optional): Either a version number (e.g., "6.6") or 
                                     a full path to the HEC-RAS executable 
                                     (e.g., "D:/Programs/HEC/HEC-RAS/6.6/Ras.exe").
    
    Returns:
        str: The full path to the HEC-RAS executable or "Ras.exe" if not found.
    
    Note:
        - HEC-RAS version numbers include: "6.6", "6.5", "6.4.1", "6.3", etc.
        - The default installation path follows: C:/Program Files (x86)/HEC/HEC-RAS/{version}/Ras.exe
        - For non-standard installations, provide the full path to Ras.exe
        - Returns "Ras.exe" if no valid path is found, with error logged
        - Allows the library to function even without HEC-RAS installed
    """
    if ras_version is None:
        if hasattr(ras, 'ras_exe_path') and ras.ras_exe_path:
            logger.debug(f"Using HEC-RAS executable from global 'ras' object: {ras.ras_exe_path}")
            return ras.ras_exe_path
        else:
            default_path = "Ras.exe"
            logger.debug(f"No HEC-RAS version specified and global 'ras' object not initialized or missing ras_exe_path.")
            logger.warning(f"HEC-RAS is not installed or version not specified. Running HEC-RAS will fail unless a valid installed version is specified.")
            return default_path
    
    # ACTUAL folder names in C:/Program Files (x86)/HEC/HEC-RAS/
    # This list matches the exact folder names on disk (verified 2025-10-30)
    ras_version_folders = [
        "6.7 Beta 4", "6.6", "6.5", "6.4.1", "6.3.1", "6.3", "6.2", "6.1", "6.0",
        "5.0.7", "5.0.6", "5.0.5", "5.0.4", "5.0.3", "5.0.1", "5.0",
        "4.1.0", "4.0"
    ]

    # User-friendly aliases (user_input → actual_folder_name)
    # Allows users to pass "4.1" and find "4.1.0" folder, or "66" to find "6.6"
    version_aliases = {
        # 4.x aliases
        "4.1": "4.1.0",      # User passes "4.1" → finds "4.1.0" folder
        "41": "4.1.0",       # Compact format
        "410": "4.1.0",      # Full compact
        "40": "4.0",         # Compact format for 4.0

        # 5.0.x aliases
        "50": "5.0",         # Compact format
        "501": "5.0.1",      # Compact format for 5.0.1
        "503": "5.0.3",      # Compact format
        "504": "5.0.4",      # Compact format for 5.0.4
        "505": "5.0.5",
        "506": "5.0.6",
        "507": "5.0.7",

        # 6.x aliases
        "60": "6.0",
        "61": "6.1",
        "62": "6.2",
        "63": "6.3",
        "631": "6.3.1",
        "6.4": "6.4.1",      # No 6.4 folder, use 6.4.1
        "64": "6.4.1",
        "641": "6.4.1",
        "65": "6.5",
        "66": "6.6",
        "6.7": "6.7 Beta 4", # User passes "6.7" → finds "6.7 Beta 4"
        "67": "6.7 Beta 4",
    }

    # Check if input is a direct path to an executable
    hecras_path = Path(ras_version)
    if hecras_path.is_file() and hecras_path.suffix.lower() == '.exe':
        logger.debug(f"HEC-RAS executable found at specified path: {hecras_path}")
        return str(hecras_path)

    version_str = str(ras_version)

    # Check if there's an alias for this version
    if version_str in version_aliases:
        actual_folder = version_aliases[version_str]
        logger.debug(f"Mapped version '{version_str}' to folder '{actual_folder}'")
        version_str = actual_folder

    # Check if this is a known folder name
    if version_str in ras_version_folders:
        default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{version_str}/Ras.exe")
        if default_path.is_file():
            logger.debug(f"HEC-RAS executable found at default path: {default_path}")
            return str(default_path)
        else:
            error_msg = f"HEC-RAS Version {version_str} folder exists but Ras.exe not found at expected path. Running HEC-RAS will fail."
            logger.error(error_msg)
            return "Ras.exe"
    
    # Final fallback: Try to find a matching version from folder list
    try:
        # Try to find a matching version from our list
        for known_folder in ras_version_folders:
            # Check for partial matches or compact formats
            if version_str in known_folder or known_folder.replace('.', '') == version_str:
                default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{known_folder}/Ras.exe")
                if default_path.is_file():
                    logger.debug(f"HEC-RAS executable found via fuzzy match: {default_path}")
                    return str(default_path)

        # Try direct path construction for newer versions
        if '.' in version_str:
            default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{version_str}/Ras.exe")
            if default_path.is_file():
                logger.debug(f"HEC-RAS executable found at path: {default_path}")
                return str(default_path)
    except Exception as e:
        logger.error(f"Error parsing version or finding path: {e}")

    error_msg = f"HEC-RAS Version {ras_version} is not recognized or installed. Running HEC-RAS will fail unless a valid installed version is specified."
    logger.error(error_msg)
    return "Ras.exe"

==================================================

File: C:\GH\ras-commander\ras_commander\RasUnsteady.py
==================================================
"""
RasUnsteady - Operations for handling unsteady flow files in HEC-RAS projects.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function():
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasUnsteady:
- update_flow_title()
- update_restart_settings()
- extract_boundary_and_tables()
- print_boundaries_and_tables()
- identify_tables()
- parse_fixed_width_table()
- extract_tables()
- write_table_to_file()
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call
import pandas as pd
import numpy as np
import re
from typing import Union, Optional, Any, Tuple, Dict, List



logger = get_logger(__name__)

# Module code starts here

class RasUnsteady:
    """
    Class for all operations related to HEC-RAS unsteady flow files.
    """
    @staticmethod
    @log_call
    def update_flow_title(unsteady_file: str, new_title: str, ras_object: Optional[Any] = None) -> None:
        """
        Update the Flow Title in an unsteady flow file (.u*).

        The Flow Title provides a descriptive identifier for unsteady flow scenarios in HEC-RAS. 
        It appears in the HEC-RAS interface and helps differentiate between different flow files.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file or unsteady flow number
            new_title (str): New flow title (max 24 characters, will be truncated if longer)
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            None: The function modifies the file in-place and updates the ras object's unsteady dataframe

        Example:
            # Clone an existing unsteady flow file
            new_unsteady_number = RasPlan.clone_unsteady("02")
            
            # Get path to the new unsteady flow file
            new_unsteady_file = RasPlan.get_unsteady_path(new_unsteady_number)
            
            # Update the flow title
            new_title = "Modified Flow Scenario"
            RasUnsteady.update_flow_title(new_unsteady_file, new_title)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        new_title = new_title[:24]  # Truncate to 24 characters if longer
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        for i, line in enumerate(lines):
            if line.startswith("Flow Title="):
                old_title = line.strip().split('=')[1]
                lines[i] = f"Flow Title={new_title}\n"
                updated = True
                logger.info(f"Updated Flow Title from '{old_title}' to '{new_title}'")
                break
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied Flow Title modification to {unsteady_file}")
        else:
            logger.warning(f"Flow Title not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def update_restart_settings(unsteady_file: str, use_restart: bool, restart_filename: Optional[str] = None, ras_object: Optional[Any] = None) -> None:
        """
        Update the restart file settings in an unsteady flow file.

        Restart files in HEC-RAS allow simulations to continue from a previously saved state,
        which is useful for long simulations or when making downstream changes.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            use_restart (bool): Whether to use a restart file (True) or not (False)
            restart_filename (str, optional): Path to the restart file (.rst)
                                             Required if use_restart is True
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            None: The function modifies the file in-place and updates the ras object's unsteady dataframe

        Example:
            # Enable restart file for an unsteady flow
            unsteady_file = RasPlan.get_unsteady_path("03")
            RasUnsteady.update_restart_settings(
                unsteady_file, 
                use_restart=True, 
                restart_filename="model_restart.rst"
            )
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        restart_line_index = None
        for i, line in enumerate(lines):
            if line.startswith("Use Restart="):
                restart_line_index = i
                old_value = line.strip().split('=')[1]
                new_value = "-1" if use_restart else "0"
                lines[i] = f"Use Restart={new_value}\n"
                updated = True
                logger.info(f"Updated Use Restart from {old_value} to {new_value}")
                break
        
        if use_restart:
            if not restart_filename:
                logger.error("Restart filename must be specified when enabling restart.")
                raise ValueError("Restart filename must be specified when enabling restart.")
            if restart_line_index is not None:
                lines.insert(restart_line_index + 1, f"Restart Filename={restart_filename}\n")
                logger.info(f"Added Restart Filename: {restart_filename}")
            else:
                logger.warning("Could not find 'Use Restart' line to insert 'Restart Filename'")
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied restart settings modification to {unsteady_file}")
        else:
            logger.warning(f"Use Restart setting not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def extract_boundary_and_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> pd.DataFrame:
        """
        Extract boundary conditions and their associated tables from an unsteady flow file.

        Boundary conditions in HEC-RAS define time-varying inputs like flow hydrographs,
        stage hydrographs, gate operations, and lateral inflows. This function parses these
        conditions and their data tables from the unsteady flow file.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            pd.DataFrame: DataFrame containing boundary conditions with the following columns:
                - River Name, Reach Name, River Station: Location information
                - DSS File: Associated DSS file path if any
                - Tables: Dictionary containing DataFrames of time-series values

        Example:
            # Get the path to unsteady flow file "02"
            unsteady_file = RasPlan.get_unsteady_path("02")
            
            # Extract boundary conditions and tables
            boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)
            print(f"Extracted {len(boundaries_df)} boundary conditions from the file.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        table_types = [
            'Flow Hydrograph=',
            'Gate Openings=',
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=',
            'Lateral Inflow Hydrograph=',
            'Precipitation Hydrograph=',
            'Rating Curve='
        ]
        
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Initialize variables
        boundary_data = []
        current_boundary = None
        current_tables = {}
        current_table = None
        table_values = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # Check for Boundary Location line
            if line.startswith("Boundary Location="):
                # Save previous boundary if it exists
                if current_boundary is not None:
                    if current_table and table_values:
                        # Process any remaining table
                        try:
                            df = pd.DataFrame({'Value': table_values})
                            current_tables[current_table_name] = df
                        except Exception as e:
                            logger.warning(f"Error processing table {current_table_name}: {e}")
                    current_boundary['Tables'] = current_tables
                    boundary_data.append(current_boundary)
                
                # Start new boundary
                current_boundary = {
                    'Boundary Location': line.split('=', 1)[1].strip(),
                    'DSS File': '',
                    'Tables': {}
                }
                current_tables = {}
                current_table = None
                table_values = []
                
            # Check for DSS File line
            elif line.startswith("DSS File=") and current_boundary is not None:
                current_boundary['DSS File'] = line.split('=', 1)[1].strip()
                
            # Check for table headers
            elif any(line.startswith(t) for t in table_types) and current_boundary is not None:
                # If we were processing a table, save it
                if current_table and table_values:
                    try:
                        df = pd.DataFrame({'Value': table_values})
                        current_tables[current_table_name] = df
                    except Exception as e:
                        logger.warning(f"Error processing previous table: {e}")
                
                # Start new table
                try:
                    current_table = line.split('=')
                    current_table_name = current_table[0].strip()
                    num_values = int(current_table[1])
                    table_values = []
                    
                    # Read the table values
                    rows_needed = (num_values + 9) // 10  # Round up division
                    for _ in range(rows_needed):
                        i += 1
                        if i >= len(lines):
                            break
                        row = lines[i].strip()
                        # Parse fixed-width values (8 characters each)
                        j = 0
                        while j < len(row):
                            value_str = row[j:j+8].strip()
                            if value_str:
                                try:
                                    value = float(value_str)
                                    table_values.append(value)
                                except ValueError:
                                    # Try splitting merged values
                                    parts = re.findall(r'-?\d+\.?\d*', value_str)
                                    table_values.extend([float(p) for p in parts])
                            j += 8
                
                except (ValueError, IndexError) as e:
                    logger.error(f"Error processing table at line {i}: {e}")
                    current_table = None
                    
            i += 1
        
        # Add the last boundary if it exists
        if current_boundary is not None:
            if current_table and table_values:
                try:
                    df = pd.DataFrame({'Value': table_values})
                    current_tables[current_table_name] = df
                except Exception as e:
                    logger.warning(f"Error processing final table: {e}")
            current_boundary['Tables'] = current_tables
            boundary_data.append(current_boundary)
        
        # Create DataFrame
        boundaries_df = pd.DataFrame(boundary_data)
        if not boundaries_df.empty:
            # Split boundary location into components
            location_columns = ['River Name', 'Reach Name', 'River Station', 
                              'Downstream River Station', 'Storage Area Connection',
                              'Storage Area Name', 'Pump Station Name', 
                              'Blank 1', 'Blank 2']
            split_locations = boundaries_df['Boundary Location'].str.split(',', expand=True)
            # Ensure we have the right number of columns
            for i, col in enumerate(location_columns):
                if i < split_locations.shape[1]:
                    boundaries_df[col] = split_locations[i].str.strip()
                else:
                    boundaries_df[col] = ''
            boundaries_df = boundaries_df.drop(columns=['Boundary Location'])
        
        logger.info(f"Successfully extracted boundaries and tables from {unsteady_path}")
        return boundaries_df

    @staticmethod
    @log_call
    def print_boundaries_and_tables(boundaries_df: pd.DataFrame) -> None:
        """
        Print boundary conditions and their associated tables in a formatted, readable way.

        This function is useful for quickly visualizing the complex nested structure of 
        boundary conditions extracted by extract_boundary_and_tables().

        Parameters:
            boundaries_df (pd.DataFrame): DataFrame containing boundary information and 
                                         nested tables data from extract_boundary_and_tables()

        Returns:
            None: Output is printed to console

        Example:
            # Extract boundary conditions and tables
            boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)
            
            # Print in a formatted way
            print("Detailed boundary conditions and tables:")
            RasUnsteady.print_boundaries_and_tables(boundaries_df)
        """
        pd.set_option('display.max_columns', None)
        pd.set_option('display.max_rows', None)
        print("\nBoundaries and Tablesin boundaries_df:")
        for idx, row in boundaries_df.iterrows():
            print(f"\nBoundary {idx+1}:")
            print(f"River Name: {row['River Name']}")
            print(f"Reach Name: {row['Reach Name']}")
            print(f"River Station: {row['River Station']}")
            print(f"DSS File: {row['DSS File']}")
            
            if row['Tables']:
                print("\nTables for this boundary:")
                for table_name, table_df in row['Tables'].items():
                    print(f"\n{table_name}:")
                    print(table_df.to_string())
            print("-" * 80)





# Additional functions from the AWS webinar where the code was developed
# Need to add examples

    @staticmethod
    @log_call
    def identify_tables(lines: List[str]) -> List[Tuple[str, int, int]]:
        """
        Identify the start and end line numbers of tables in an unsteady flow file.

        HEC-RAS unsteady flow files contain numeric tables in a fixed-width format.
        This function locates these tables within the file and provides their positions.

        Parameters:
            lines (List[str]): List of file lines (typically from file.readlines())

        Returns:
            List[Tuple[str, int, int]]: List of tuples where each tuple contains:
                - table_name (str): The type of table (e.g., 'Flow Hydrograph=')
                - start_line (int): Line number where the table data begins
                - end_line (int): Line number where the table data ends

        Example:
            # Read the unsteady flow file
            with open(new_unsteady_file, 'r') as f:
                lines = f.readlines()
                
            # Identify tables in the file
            tables = RasUnsteady.identify_tables(lines)
            print(f"Identified {len(tables)} tables in the unsteady flow file.")
        """
        table_types = [
            'Flow Hydrograph=',
            'Gate Openings=',
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=',
            'Lateral Inflow Hydrograph=',
            'Precipitation Hydrograph=',
            'Rating Curve='
        ]
        tables = []
        current_table = None

        for i, line in enumerate(lines):
            if any(table_type in line for table_type in table_types):
                if current_table:
                    tables.append((current_table[0], current_table[1], i-1))
                table_name = line.strip().split('=')[0] + '='
                try:
                    num_values = int(line.strip().split('=')[1])
                    current_table = (table_name, i+1, num_values)
                except (ValueError, IndexError) as e:
                    logger.error(f"Error parsing table header at line {i}: {e}")
                    continue
        
        if current_table:
            tables.append((current_table[0], current_table[1], 
                          current_table[1] + (current_table[2] + 9) // 10))
        
        logger.debug(f"Identified {len(tables)} tables in the file")
        return tables

    @staticmethod
    @log_call
    def parse_fixed_width_table(lines: List[str], start: int, end: int) -> pd.DataFrame:
        """
        Parse a fixed-width table from an unsteady flow file into a pandas DataFrame.

        HEC-RAS uses a fixed-width format (8 characters per value) for numeric tables.
        This function converts this format into a DataFrame for easier manipulation.

        Parameters:
            lines (List[str]): List of file lines (from file.readlines())
            start (int): Starting line number for table data
            end (int): Ending line number for table data

        Returns:
            pd.DataFrame: DataFrame with a single column 'Value' containing the parsed numeric values

        Example:
            # Identify tables in the file
            tables = RasUnsteady.identify_tables(lines)
            
            # Parse a specific table (e.g., first flow hydrograph)
            table_name, start_line, end_line = tables[0]
            table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)
        """
        data = []
        for line in lines[start:end]:
            # Skip empty lines or lines that don't contain numeric data
            if not line.strip() or not any(c.isdigit() for c in line):
                continue
                
            # Split the line into 8-character columns and process each value
            values = []
            for i in range(0, len(line.rstrip()), 8):
                value_str = line[i:i+8].strip()
                if value_str:  # Only process non-empty strings
                    try:
                        # Handle special cases where numbers are run together
                        if len(value_str) > 8:
                            # Use regex to find all numbers in the string
                            parts = re.findall(r'-?\d+\.?\d*', value_str)
                            values.extend([float(p) for p in parts])
                        else:
                            values.append(float(value_str))
                    except ValueError:
                        # If conversion fails, try to extract any valid numbers from the string
                        parts = re.findall(r'-?\d+\.?\d*', value_str)
                        if parts:
                            values.extend([float(p) for p in parts])
                        else:
                            logger.debug(f"Skipping non-numeric value: {value_str}")
                            continue
            
            # Only add to data if we found valid numeric values
            if values:
                data.extend(values)
        
        if not data:
            logger.warning("No numeric data found in table section")
            return pd.DataFrame(columns=['Value'])
            
        return pd.DataFrame(data, columns=['Value'])
    
    @staticmethod
    @log_call
    def extract_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> Dict[str, pd.DataFrame]:
        """
        Extract all tables from an unsteady flow file and return them as DataFrames.

        This function combines identify_tables() and parse_fixed_width_table() to extract
        all tables from an unsteady flow file in a single operation.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            Dict[str, pd.DataFrame]: Dictionary where:
                - Keys are table names (e.g., 'Flow Hydrograph=')
                - Values are DataFrames with a 'Value' column containing numeric data

        Example:
            # Extract all tables from the unsteady flow file
            all_tables = RasUnsteady.extract_tables(new_unsteady_file)
            print(f"Extracted {len(all_tables)} tables from the file.")
            
            # Access a specific table
            flow_tables = [name for name in all_tables.keys() if 'Flow Hydrograph=' in name]
            if flow_tables:
                flow_df = all_tables[flow_tables[0]]
                print(f"Flow table has {len(flow_df)} values")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Fix: Use RasUnsteady.identify_tables 
        tables = RasUnsteady.identify_tables(lines)
        extracted_tables = {}
        
        for table_name, start, end in tables:
            df = RasUnsteady.parse_fixed_width_table(lines, start, end)
            extracted_tables[table_name] = df
            logger.debug(f"Extracted table '{table_name}' with {len(df)} values")
        
        return extracted_tables

    @staticmethod
    @log_call
    def write_table_to_file(unsteady_file: str, table_name: str, df: pd.DataFrame, 
                           start_line: int, ras_object: Optional[Any] = None) -> None:
        """
        Write an updated table back to an unsteady flow file in the required fixed-width format.

        This function takes a modified DataFrame and writes it back to the unsteady flow file,
        preserving the 8-character fixed-width format that HEC-RAS requires.

        Parameters:
            unsteady_file (str): Path to the unsteady flow file
            table_name (str): Name of the table to update (e.g., 'Flow Hydrograph=')
            df (pd.DataFrame): DataFrame containing the updated values with a 'Value' column
            start_line (int): Line number where the table data begins in the file
            ras_object (optional): Custom RAS object to use instead of the global one

        Returns:
            None: The function modifies the file in-place

        Example:
            # Identify tables in the unsteady flow file
            tables = RasUnsteady.identify_tables(lines)
            table_name, start_line, end_line = tables[0]
            
            # Parse and modify the table
            table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)
            table_df['Value'] = table_df['Value'] * 0.75  # Scale values to 75%
            
            # Write modified table back to the file
            RasUnsteady.write_table_to_file(new_unsteady_file, table_name, table_df, start_line)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Format values into fixed-width strings
        formatted_values = []
        for i in range(0, len(df), 10):
            row = df['Value'].iloc[i:i+10]
            formatted_row = ''.join(f'{value:8.2f}' for value in row)
            formatted_values.append(formatted_row + '\n')
        
        # Replace old table with new formatted values
        lines[start_line:start_line+len(formatted_values)] = formatted_values
        
        try:
            with open(unsteady_path, 'w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated table '{table_name}' in {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            raise
        except IOError as e:
            logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            raise








'''



Flow Title=Single 2D Area with Bridges
Program Version=6.60
Use Restart= 0 
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DSNormalDepth                   ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalD                      ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,Upstream Inflow                 ,                                
Interval=1HOUR
Flow Hydrograph= 200 
    1000    3000    6500    8000    9500   11000   12500   14000   15500   17000
   18500   20000   22000   24000   26000   28000   30000   34000   38000   42000
   46000   50000   54000   58000   62000   66000   70000   73000   76000   79000
   82000   85000   87200   89400   91600   93800   96000   96800   97600   98400
   99200  100000   99600   99200   98800   98400   98000   96400   94800   93200
   91600   90000   88500   87000   85500   84000   82500   81000   79500   78000
   76500   75000   73500   7200070666.6669333.34   6800066666.6665333.33   64000
62666.6761333.33   6000058666.6757333.33   5600054666.6753333.33   5200050666.67
49333.33   4800046666.6745333.33   4400042666.6741333.33   4000039166.6738333.33
   3750036666.6735833.33   3500034166.6733333.33   3250031666.6730833.33   30000
29166.6728333.33   2750026666.6725833.33   2500024166.6723333.33   2250021666.67
20833.33   2000019655.1719310.3518965.5218620.6918275.8617931.0417586.2117241.38
16896.5516551.72 16206.915862.0715517.2415172.4114827.5914482.7614137.93 13793.1
13448.2813103.4512758.6212413.7912068.9711724.1411379.3111034.4810689.6610344.83
   10000 9915.25 9830.51 9745.76 9661.02 9576.27 9491.53 9406.78 9322.03 9237.29
 9152.54  9067.8 8983.05 8898.31 8813.56 8728.81 8644.07 8559.32 8474.58 8389.83
 8305.09 8220.34 8135.59 8050.85  7966.1 7881.36 7796.61 7711.86 7627.12 7542.37
 7457.63 7372.88 7288.14 7203.39 7118.64  7033.9 6949.15 6864.41 6779.66 6694.92
 6610.17 6525.42 6440.68 6355.93 6271.19 6186.44  6101.7 6016.95  5932.2 5847.46
 5762.71 5677.97 5593.22 5508.48 5423.73 5338.98 5254.24 5169.49 5084.75    5000
Stage Hydrograph TW Check=0
Flow Hydrograph QMult= 0.5 
Flow Hydrograph Slope= 0.0005 
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,Sayers Dam      ,                ,                ,                                ,                                
Gate Name=Gate #1     
Gate DSS Path=
Gate Use DSS=False
Gate Time Interval=1HOUR
Gate Use Fixed Start Time=False
Gate Fixed Start Date/Time=,
Gate Openings= 100 
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalDepth                  ,                                
Friction Slope=0.0003,0
Met Point Raster Parameters=,,,,
Precipitation Mode=Disable
Wind Mode=No Wind Forces
Air Density Mode=
Wave Mode=No Wave Forcing
Met BC=Precipitation|Expanded View=0
Met BC=Precipitation|Point Interpolation=Nearest
Met BC=Precipitation|Gridded Source=DSS
Met BC=Precipitation|Gridded Interpolation=
Met BC=Evapotranspiration|Expanded View=0
Met BC=Evapotranspiration|Point Interpolation=Nearest
Met BC=Evapotranspiration|Gridded Source=DSS
Met BC=Evapotranspiration|Gridded Interpolation=
Met BC=Wind Speed|Expanded View=0
Met BC=Wind Speed|Constant Units=ft/s
Met BC=Wind Speed|Point Interpolation=Nearest
Met BC=Wind Speed|Gridded Source=DSS
Met BC=Wind Speed|Gridded Interpolation=
Met BC=Wind Direction|Expanded View=0
Met BC=Wind Direction|Point Interpolation=Nearest
Met BC=Wind Direction|Gridded Source=DSS
Met BC=Wind Direction|Gridded Interpolation=
Met BC=Wind Velocity X|Expanded View=0
Met BC=Wind Velocity X|Constant Units=ft/s
Met BC=Wind Velocity X|Point Interpolation=Nearest
Met BC=Wind Velocity X|Gridded Source=DSS
Met BC=Wind Velocity X|Gridded Interpolation=
Met BC=Wind Velocity Y|Expanded View=0
Met BC=Wind Velocity Y|Constant Units=ft/s
Met BC=Wind Velocity Y|Point Interpolation=Nearest
Met BC=Wind Velocity Y|Gridded Source=DSS
Met BC=Wind Velocity Y|Gridded Interpolation=
Met BC=Wave Forcing X|Expanded View=0
Met BC=Wave Forcing X|Point Interpolation=Nearest
Met BC=Wave Forcing X|Gridded Source=DSS
Met BC=Wave Forcing X|Gridded Interpolation=
Met BC=Wave Forcing Y|Expanded View=0
Met BC=Wave Forcing Y|Point Interpolation=Nearest
Met BC=Wave Forcing Y|Gridded Source=DSS
Met BC=Wave Forcing Y|Gridded Interpolation=
Met BC=Air Density|Mode=Constant
Met BC=Air Density|Expanded View=0
Met BC=Air Density|Constant Value=1.225
Met BC=Air Density|Constant Units=kg/m3
Met BC=Air Density|Point Interpolation=Nearest
Met BC=Air Density|Gridded Source=DSS
Met BC=Air Density|Gridded Interpolation=
Met BC=Air Temperature|Expanded View=0
Met BC=Air Temperature|Point Interpolation=Nearest
Met BC=Air Temperature|Gridded Source=DSS
Met BC=Air Temperature|Gridded Interpolation=
Met BC=Humidity|Expanded View=0
Met BC=Humidity|Point Interpolation=Nearest
Met BC=Humidity|Gridded Source=DSS
Met BC=Humidity|Gridded Interpolation=
Met BC=Air Pressure|Mode=Constant
Met BC=Air Pressure|Expanded View=0
Met BC=Air Pressure|Constant Value=1013.2
Met BC=Air Pressure|Constant Units=mb
Met BC=Air Pressure|Point Interpolation=Nearest
Met BC=Air Pressure|Gridded Source=DSS
Met BC=Air Pressure|Gridded Interpolation=
Non-Newtonian Method= 0 , 
Non-Newtonian Constant Vol Conc=0
Non-Newtonian Yield Method= 0 , 
Non-Newtonian Yield Coef=0, 0
User Yeild=   0
Non-Newtonian Sed Visc= 0 , 
Non-Newtonian Obrian B=0
User Viscosity=0
User Viscosity Ratio=0
Herschel-Bulkley Coef=0, 0
Clastic Method= 0 , 
Coulomb Phi=0
Voellmy X=0
Non-Newtonian Hindered FV= 0 
Non-Newtonian FV K=0
Non-Newtonian ds=0
Non-Newtonian Max Cv=0
Non-Newtonian Bulking Method= 0 , 
Non-Newtonian High C Transport= 0 , 
Lava Activation= 0 
Temperature=1300,15,,15,14,980
Heat Ballance=1,1200,0.5,1,70,0.95
Viscosity=1000,,,
Yield Strength=,,,
Consistency Factor=,,,
Profile Coefficient=4,1.3,
Lava Param=,2500,




'''





==================================================

File: C:\GH\ras-commander\ras_commander\RasUtils.py
==================================================
"""
RasUtils - Utility functions for the ras-commander library

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function():
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasUtils:
- create_directory()
- find_files_by_extension()
- get_file_size()
- get_file_modification_time()
- normalize_ras_number()
- get_plan_path()
- remove_with_retry()
- update_plan_file()
- check_file_access()
- convert_to_dataframe()
- save_to_excel()
- calculate_rmse()
- calculate_percent_bias()
- calculate_error_metrics()
- update_file()
- get_next_number()
- clone_file()
- update_project_file()
- decode_byte_strings()
- perform_kdtree_query()
- find_nearest_neighbors()
- consolidate_dataframe()
- find_nearest_value()
- horizontal_distance()
    
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from typing import Union, Optional, Dict, Callable, List, Tuple, Any
import pandas as pd
import numpy as np
import shutil
import re
from scipy.spatial import KDTree
import datetime
import time
import h5py
from datetime import timedelta
from numbers import Number
from .LoggingConfig import get_logger
from .Decorators import log_call


logger = get_logger(__name__)
# Module code starts here

class RasUtils:
    """
    A class containing utility functions for the ras-commander library.
    When integrating new functions that do not clearly fit into other classes, add them here.
    """

    @staticmethod
    @log_call
    def create_directory(directory_path: Path, ras_object=None) -> Path:
        """
        Ensure that a directory exists, creating it if necessary.

        Parameters:
        directory_path (Path): Path to the directory
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Path to the ensured directory

        Example:
        >>> ensured_dir = RasUtils.create_directory(Path("output"))
        >>> print(f"Directory ensured: {ensured_dir}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(directory_path)
        try:
            path.mkdir(parents=True, exist_ok=True)
            logger.info(f"Directory ensured: {path}")
        except Exception as e:
            logger.error(f"Failed to create directory {path}: {e}")
            raise
        return path

    @staticmethod
    @log_call
    def find_files_by_extension(extension: str, ras_object=None) -> list:
        """
        List all files in the project directory with a specific extension.

        Parameters:
        extension (str): File extension to filter (e.g., '.prj')
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        list: List of file paths matching the extension

        Example:
        >>> prj_files = RasUtils.find_files_by_extension('.prj')
        >>> print(f"Found {len(prj_files)} .prj files")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            files = list(ras_obj.project_folder.glob(f"*{extension}"))
            file_list = [str(file) for file in files]
            logger.info(f"Found {len(file_list)} files with extension '{extension}' in {ras_obj.project_folder}")
            return file_list
        except Exception as e:
            logger.error(f"Failed to find files with extension '{extension}': {e}")
            raise

    @staticmethod
    @log_call
    def get_file_size(file_path: Path, ras_object=None) -> Optional[int]:
        """
        Get the size of a file in bytes.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[int]: Size of the file in bytes, or None if the file does not exist

        Example:
        >>> size = RasUtils.get_file_size(Path("project.prj"))
        >>> print(f"File size: {size} bytes")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                size = path.stat().st_size
                logger.info(f"Size of {path}: {size} bytes")
                return size
            except Exception as e:
                logger.error(f"Failed to get size for {path}: {e}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def get_file_modification_time(file_path: Path, ras_object=None) -> Optional[float]:
        """
        Get the last modification time of a file.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[float]: Last modification time as a timestamp, or None if the file does not exist

        Example:
        >>> mtime = RasUtils.get_file_modification_time(Path("project.prj"))
        >>> print(f"Last modified: {mtime}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                mtime = path.stat().st_mtime
                logger.info(f"Last modification time of {path}: {mtime}")
                return mtime
            except Exception as e:
                logger.exception(f"Failed to get modification time for {path}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def normalize_ras_number(ras_number: Union[str, int, float, Path, Number]) -> str:
        """
        Normalize RAS file numbers to two-digit string format.

        HEC-RAS uses two-digit file extensions for plans (.p01), geometries (.g02),
        flows (.f03), etc. This function standardizes various input formats to ensure
        consistent file path construction.

        Parameters:
        ras_number (Union[str, int, float, Path, Number]): Input number in various formats:
            - int: 1, 2, 3, etc.
            - str: "1", "01", "001", etc.
            - float: 1.0, 2.0 (must be whole numbers)
            - Path: Path("project.p05") - extracts number from extension
            - Number: numpy.int64(1), etc.

        Returns:
        str: Normalized two-digit format ("01", "02", ..., "99")

        Raises:
        ValueError: If the number is not between 1 and 99, or cannot be converted
        TypeError: If the input type is invalid

        Examples:
        >>> RasUtils.normalize_ras_number(1)
        '01'
        >>> RasUtils.normalize_ras_number("1")
        '01'
        >>> RasUtils.normalize_ras_number("01")
        '01'
        >>> RasUtils.normalize_ras_number("001")
        '01'
        >>> RasUtils.normalize_ras_number(np.int64(5))
        '05'
        >>> RasUtils.normalize_ras_number(Path("project.p02"))
        '02'

        Notes:
        - Used for plan numbers, geometry numbers, flow file numbers, etc.
        - Ensures consistent handling across all RAS file types
        - Prevents file path construction errors from unnormalized inputs
        """
        # Handle Path objects - extract number from file extension
        if isinstance(ras_number, Path):
            # Extract from extensions like .p01, .g02, .f03, etc.
            suffix = ras_number.suffix  # e.g., ".p01"
            if len(suffix) >= 2 and suffix[0] == '.':
                # Try to extract number after the letter (e.g., "01" from ".p01")
                number_part = suffix[2:]  # Skip "." and letter
                if number_part.isdigit():
                    ras_number = number_part
                else:
                    raise ValueError(
                        f"Cannot extract RAS number from Path extension: {ras_number}. "
                        f"Expected format like 'project.p01' or 'geom.g02'"
                    )
            else:
                raise ValueError(
                    f"Cannot extract RAS number from Path: {ras_number}. "
                    f"Expected file with RAS extension like .p01, .g02, etc."
                )

        # Convert to integer for validation
        try:
            # Handle string inputs - strip leading zeros before conversion
            if isinstance(ras_number, str):
                stripped = ras_number.lstrip('0')
                if not stripped or not stripped.isdigit():
                    # Handle edge cases like "0", "00", or non-numeric strings
                    if not stripped:  # Was all zeros
                        ras_int = 0
                    else:
                        raise ValueError(f"Cannot convert '{ras_number}' to integer")
                else:
                    ras_int = int(stripped)
            else:
                # Handle numeric types (int, float, numpy types, etc.)
                ras_int = int(ras_number)

                # Check if float had decimal component
                if isinstance(ras_number, (float, np.floating)) and ras_number != ras_int:
                    raise ValueError(
                        f"RAS numbers must be integers, got float with decimals: {ras_number}"
                    )

        except (ValueError, TypeError) as e:
            raise ValueError(
                f"Cannot convert RAS number '{ras_number}' (type: {type(ras_number).__name__}) "
                f"to integer: {e}"
            ) from e

        # Validate range (1-99 for HEC-RAS files)
        if not 1 <= ras_int <= 99:
            raise ValueError(
                f"RAS file number must be between 1 and 99, got: {ras_int}"
            )

        # Return normalized two-digit format
        normalized = f"{ras_int:02d}"
        logger.debug(f"Normalized RAS number '{ras_number}' to '{normalized}'")
        return normalized

    @staticmethod
    @log_call
    def get_plan_path(current_plan_number_or_path: Union[str, Number, Path], ras_object=None) -> Path:
        """
        Get the path for a plan file with a given plan number or path.

        Parameters:
        current_plan_number_or_path (Union[str, Number, Path]): The plan number (e.g., '01', 1, or 1.0) or full path to the plan file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Full path to the plan file

        Raises:
        ValueError: If plan number is not between 1 and 99
        TypeError: If input type is invalid
        FileNotFoundError: If the plan file does not exist

        Example:
        >>> plan_path = RasUtils.get_plan_path(1)
        >>> print(f"Plan file path: {plan_path}")
        >>> plan_path = RasUtils.get_plan_path("01")
        >>> print(f"Plan file path: {plan_path}")
        >>> plan_path = RasUtils.get_plan_path("path/to/plan.p01")
        >>> print(f"Plan file path: {plan_path}")
        """
        # Validate RAS object
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Handle direct file path input
        plan_path = Path(current_plan_number_or_path)
        if plan_path.is_file():
            logger.info(f"Using provided plan file path: {plan_path}")
            return plan_path

        # Handle plan number input - use centralized normalization
        try:
            current_plan_number = RasUtils.normalize_ras_number(current_plan_number_or_path)
            logger.debug(f"Normalized plan number to: {current_plan_number}")
        except (ValueError, TypeError) as e:
            logger.error(f"Invalid plan number: {current_plan_number_or_path}. {e}")
            raise
        
        # Construct and validate plan path
        plan_name = f"{ras_obj.project_name}.p{current_plan_number}"
        full_plan_path = ras_obj.project_folder / plan_name
        
        if not full_plan_path.exists():
            logger.error(f"Plan file does not exist: {full_plan_path}")
            raise FileNotFoundError(f"Plan file does not exist: {full_plan_path}")
        
        logger.info(f"Constructed plan file path: {full_plan_path}")
        return full_plan_path

    @staticmethod
    @log_call
    def remove_with_retry(
        path: Path,
        max_attempts: int = 5,
        initial_delay: float = 1.0,
        is_folder: bool = True,
        ras_object=None
    ) -> bool:
        """
        Attempts to remove a file or folder with retry logic and exponential backoff.

        Parameters:
        path (Path): Path to the file or folder to be removed.
        max_attempts (int): Maximum number of removal attempts.
        initial_delay (float): Initial delay between attempts in seconds.
        is_folder (bool): If True, the path is treated as a folder; if False, it's treated as a file.
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        bool: True if the file or folder was successfully removed, False otherwise.

        Example:
        >>> success = RasUtils.remove_with_retry(Path("temp_folder"), is_folder=True)
        >>> print(f"Removal successful: {success}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        path = Path(path)
        for attempt in range(1, max_attempts + 1):
            try:
                if path.exists():
                    if is_folder:
                        shutil.rmtree(path)
                        logger.info(f"Folder removed: {path}")
                    else:
                        path.unlink()
                        logger.info(f"File removed: {path}")
                else:
                    logger.info(f"Path does not exist, nothing to remove: {path}")
                return True
            except PermissionError as pe:
                if attempt < max_attempts:
                    delay = initial_delay * (2 ** (attempt - 1))  # Exponential backoff
                    logger.warning(
                        f"PermissionError on attempt {attempt} to remove {path}: {pe}. "
                        f"Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(
                        f"Failed to remove {path} after {max_attempts} attempts due to PermissionError: {pe}. Skipping."
                    )
                    return False
            except Exception as e:
                logger.exception(f"Failed to remove {path} on attempt {attempt}")
                return False
        return False

    @staticmethod
    @log_call
    def update_plan_file(
        plan_number_or_path: Union[str, Path],
        file_type: str,
        entry_number: int,
        ras_object=None
    ) -> None:
        """
        Update a plan file with a new file reference.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        file_type (str): Type of file to update ('Geom', 'Flow', or 'Unsteady')
        entry_number (int): Number (from 1 to 99) to set
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Raises:
        ValueError: If an invalid file_type is provided
        FileNotFoundError: If the plan file doesn't exist

        Example:
        >>> RasUtils.update_plan_file(1, "Geom", 2)
        >>> RasUtils.update_plan_file("path/to/plan.p01", "Geom", 2)
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        valid_file_types = {'Geom': 'g', 'Flow': 'f', 'Unsteady': 'u'}
        if file_type not in valid_file_types:
            logger.error(
                f"Invalid file_type '{file_type}'. Expected one of: {', '.join(valid_file_types.keys())}"
            )
            raise ValueError(
                f"Invalid file_type. Expected one of: {', '.join(valid_file_types.keys())}"
            )

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise FileNotFoundError(f"Plan file not found: {plan_file_path}")
        
        file_prefix = valid_file_types[file_type]
        search_pattern = f"{file_type} File="
        formatted_entry_number = f"{int(entry_number):02d}"  # Ensure two-digit format

        try:
            RasUtils.check_file_access(plan_file_path, 'r')
            with plan_file_path.open('r') as file:
                lines = file.readlines()
        except Exception as e:
            logger.exception(f"Failed to read plan file {plan_file_path}")
            raise

        updated = False
        for i, line in enumerate(lines):
            if line.startswith(search_pattern):
                lines[i] = f"{search_pattern}{file_prefix}{formatted_entry_number}\n"
                logger.info(
                    f"Updated {file_type} File in {plan_file_path} to {file_prefix}{formatted_entry_number}"
                )
                updated = True
                break

        if not updated:
            logger.warning(
                f"Search pattern '{search_pattern}' not found in {plan_file_path}. No update performed."
            )

        try:
            with plan_file_path.open('w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated plan file: {plan_file_path}")
        except Exception as e:
            logger.exception(f"Failed to write updates to plan file {plan_file_path}")
            raise

        # Refresh RasPrj dataframes
        try:
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            logger.info("RAS object dataframes have been refreshed.")
        except Exception as e:
            logger.exception("Failed to refresh RasPrj dataframes")
            raise

    @staticmethod
    @log_call
    def check_file_access(file_path: Path, mode: str = 'r') -> None:
        """
        Check if the file can be accessed with the specified mode.

        Parameters:
        file_path (Path): Path to the file
        mode (str): Mode to check ('r' for read, 'w' for write, etc.)

        Raises:
        FileNotFoundError: If the file does not exist
        PermissionError: If the required permissions are not met
        """
        
        path = Path(file_path)
        if not path.exists():
            logger.error(f"File not found: {file_path}")
            raise FileNotFoundError(f"File not found: {file_path}")
        
        if mode in ('r', 'rb'):
            if not os.access(path, os.R_OK):
                logger.error(f"Read permission denied for file: {file_path}")
                raise PermissionError(f"Read permission denied for file: {file_path}")
            else:
                logger.debug(f"Read access granted for file: {file_path}")
        
        if mode in ('w', 'wb', 'a', 'ab'):
            parent_dir = path.parent
            if not os.access(parent_dir, os.W_OK):
                logger.error(f"Write permission denied for directory: {parent_dir}")
                raise PermissionError(f"Write permission denied for directory: {parent_dir}")
            else:
                logger.debug(f"Write access granted for directory: {parent_dir}")


    @staticmethod
    @log_call
    def convert_to_dataframe(data_source: Union[pd.DataFrame, Path], **kwargs) -> pd.DataFrame:
        """
        Converts input to a pandas DataFrame. Supports existing DataFrames or file paths (CSV, Excel, TSV, Parquet).

        Args:
            data_source (Union[pd.DataFrame, Path]): The input to convert to a DataFrame. Can be a file path or an existing DataFrame.
            **kwargs: Additional keyword arguments to pass to pandas read functions.

        Returns:
            pd.DataFrame: The resulting DataFrame.

        Raises:
            NotImplementedError: If the file type is unsupported or input type is invalid.

        Example:
            >>> df = RasUtils.convert_to_dataframe(Path("data.csv"))
            >>> print(type(df))
            <class 'pandas.core.frame.DataFrame'>
        """
        if isinstance(data_source, pd.DataFrame):
            logger.debug("Input is already a DataFrame, returning a copy.")
            return data_source.copy()
        elif isinstance(data_source, Path):
            ext = data_source.suffix.replace('.', '', 1)
            logger.info(f"Converting file with extension '{ext}' to DataFrame.")
            if ext == 'csv':
                return pd.read_csv(data_source, **kwargs)
            elif ext.startswith('x'):
                return pd.read_excel(data_source, **kwargs)
            elif ext == "tsv":
                return pd.read_csv(data_source, sep="\t", **kwargs)
            elif ext in ["parquet", "pq", "parq"]:
                return pd.read_parquet(data_source, **kwargs)
            else:
                logger.error(f"Unsupported file type: {ext}")
                raise NotImplementedError(f"Unsupported file type {ext}. Should be one of csv, tsv, parquet, or xlsx.")
        else:
            logger.error(f"Unsupported input type: {type(data_source)}")
            raise NotImplementedError(f"Unsupported type {type(data_source)}. Only file path / existing DataFrame supported at this time")

    @staticmethod
    @log_call
    def save_to_excel(dataframe: pd.DataFrame, excel_path: Path, **kwargs) -> None:
        """
        Saves a pandas DataFrame to an Excel file with retry functionality.

        Args:
            dataframe (pd.DataFrame): The DataFrame to save.
            excel_path (Path): The path to the Excel file where the DataFrame will be saved.
            **kwargs: Additional keyword arguments passed to `DataFrame.to_excel()`.

        Raises:
            IOError: If the file cannot be saved after multiple attempts.

        Example:
            >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
            >>> RasUtils.save_to_excel(df, Path('output.xlsx'))
        """
        saved = False
        max_attempts = 3
        attempt = 0

        while not saved and attempt < max_attempts:
            try:
                dataframe.to_excel(excel_path, **kwargs)
                logger.info(f'DataFrame successfully saved to {excel_path}')
                saved = True
            except IOError as e:
                attempt += 1
                if attempt < max_attempts:
                    logger.warning(f"Error saving file. Attempt {attempt} of {max_attempts}. Please close the Excel document if it's open.")
                else:
                    logger.error(f"Failed to save {excel_path} after {max_attempts} attempts.")
                    raise IOError(f"Failed to save {excel_path} after {max_attempts} attempts. Last error: {str(e)}")

    @staticmethod
    @log_call
    def calculate_rmse(observed_values: np.ndarray, predicted_values: np.ndarray, normalized: bool = True) -> float:
        """
        Calculate the Root Mean Squared Error (RMSE) between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            normalized (bool, optional): Whether to normalize RMSE to a percentage of observed_values. Defaults to True.

        Returns:
            float: The calculated RMSE value.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_rmse(observed, predicted)
            0.06396394
        """
        rmse = np.sqrt(np.mean((predicted_values - observed_values) ** 2))
        
        if normalized:
            rmse = rmse / np.abs(np.mean(observed_values))
        
        logger.debug(f"Calculated RMSE: {rmse}")
        return rmse

    @staticmethod
    @log_call
    def calculate_percent_bias(observed_values: np.ndarray, predicted_values: np.ndarray, as_percentage: bool = False) -> float:
        """
        Calculate the Percent Bias between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            as_percentage (bool, optional): If True, return bias as a percentage. Defaults to False.

        Returns:
            float: The calculated Percent Bias.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_percent_bias(observed, predicted, as_percentage=True)
            3.33333333
        """
        multiplier = 100 if as_percentage else 1
        
        percent_bias = multiplier * (np.mean(predicted_values) - np.mean(observed_values)) / np.mean(observed_values)
        
        logger.debug(f"Calculated Percent Bias: {percent_bias}")
        return percent_bias

    @staticmethod
    @log_call
    def calculate_error_metrics(observed_values: np.ndarray, predicted_values: np.ndarray) -> Dict[str, float]:
        """
        Compute a trio of error metrics: correlation, RMSE, and Percent Bias.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.

        Returns:
            Dict[str, float]: A dictionary containing correlation ('cor'), RMSE ('rmse'), and Percent Bias ('pb').

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_error_metrics(observed, predicted)
            {'cor': 0.9993, 'rmse': 0.06396, 'pb': 0.03333}
        """
        correlation = np.corrcoef(observed_values, predicted_values)[0, 1]
        rmse = RasUtils.calculate_rmse(observed_values, predicted_values)
        percent_bias = RasUtils.calculate_percent_bias(observed_values, predicted_values)
        
        metrics = {'cor': correlation, 'rmse': rmse, 'pb': percent_bias}
        logger.info(f"Calculated error metrics: {metrics}")
        return metrics

    
    @staticmethod
    @log_call
    def update_file(file_path: Path, update_function: Callable, *args) -> None:
        """
        Generic method to update a file.

        Parameters:
        file_path (Path): Path to the file to be updated
        update_function (Callable): Function to update the file contents
        *args: Additional arguments to pass to the update_function

        Raises:
        Exception: If there's an error updating the file

        Example:
        >>> def update_content(lines, new_value):
        ...     lines[0] = f"New value: {new_value}\\n"
        ...     return lines
        >>> RasUtils.update_file(Path("example.txt"), update_content, "Hello")
        """
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            updated_lines = update_function(lines, *args) if args else update_function(lines)
            
            with open(file_path, 'w') as f:
                f.writelines(updated_lines)
            logger.info(f"Successfully updated file: {file_path}")
        except Exception as e:
            logger.exception(f"Failed to update file {file_path}")
            raise

    @staticmethod
    @log_call
    def get_next_number(existing_numbers: list) -> str:
        """
        Determine the next available number from a list of existing numbers.

        Parameters:
        existing_numbers (list): List of existing numbers as strings

        Returns:
        str: Next available number as a zero-padded string

        Example:
        >>> RasUtils.get_next_number(["01", "02", "04"])
        "05"
        """
        existing_numbers = sorted(int(num) for num in existing_numbers)
        next_number = max(existing_numbers, default=0) + 1
        return f"{next_number:02d}"

    @staticmethod
    @log_call
    def clone_file(template_path: Path, new_path: Path, update_function: Optional[Callable] = None, *args) -> None:
        """
        Generic method to clone a file and optionally update it.

        Parameters:
        template_path (Path): Path to the template file
        new_path (Path): Path where the new file will be created
        update_function (Optional[Callable]): Function to update the cloned file
        *args: Additional arguments to pass to the update_function

        Raises:
        FileNotFoundError: If the template file doesn't exist

        Example:
        >>> def update_content(lines, new_value):
        ...     lines[0] = f"New value: {new_value}\\n"
        ...     return lines
        >>> RasUtils.clone_file(Path("template.txt"), Path("new.txt"), update_content, "Hello")
        """
        if not template_path.exists():
            logger.error(f"Template file '{template_path}' does not exist.")
            raise FileNotFoundError(f"Template file '{template_path}' does not exist.")

        shutil.copy(template_path, new_path)
        logger.info(f"File cloned from {template_path} to {new_path}")

        if update_function:
            RasUtils.update_file(new_path, update_function, *args)
    @staticmethod
    @log_call
    def update_project_file(prj_file: Path, file_type: str, new_num: str, ras_object=None) -> None:
        """
        Update the project file with a new entry.

        Parameters:
        prj_file (Path): Path to the project file
        file_type (str): Type of file being added (e.g., 'Plan', 'Geom')
        new_num (str): Number of the new file entry
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Example:
        >>> RasUtils.update_project_file(Path("project.prj"), "Plan", "02")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            with open(prj_file, 'r') as f:
                lines = f.readlines()
            
            new_line = f"{file_type} File={file_type[0].lower()}{new_num}\n"
            lines.append(new_line)
            
            with open(prj_file, 'w') as f:
                f.writelines(lines)
            logger.info(f"Project file updated with new {file_type} entry: {new_num}")
        except Exception as e:
            logger.exception(f"Failed to update project file {prj_file}")
            raise
        
  
        
        
    # From FunkShuns
        
    @staticmethod
    @log_call
    def decode_byte_strings(dataframe: pd.DataFrame) -> pd.DataFrame:
        """
        Decodes byte strings in a DataFrame to regular string objects.

        This function converts columns with byte-encoded strings (e.g., b'string') into UTF-8 decoded strings.

        Args:
            dataframe (pd.DataFrame): The DataFrame containing byte-encoded string columns.

        Returns:
            pd.DataFrame: The DataFrame with byte strings decoded to regular strings.

        Example:
            >>> df = pd.DataFrame({'A': [b'hello', b'world'], 'B': [1, 2]})
            >>> decoded_df = RasUtils.decode_byte_strings(df)
            >>> print(decoded_df)
                A  B
            0  hello  1
            1  world  2
        """
        str_df = dataframe.select_dtypes(['object'])
        str_df = str_df.stack().str.decode('utf-8').unstack()
        for col in str_df:
            dataframe[col] = str_df[col]
        return dataframe

    @staticmethod
    @log_call
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = RasUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    @log_call
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = RasUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped

    @staticmethod
    @log_call
    def consolidate_dataframe(
        dataframe: pd.DataFrame,
        group_by: Optional[Union[str, List[str]]] = None,
        pivot_columns: Optional[Union[str, List[str]]] = None,
        level: Optional[int] = None,
        n_dimensional: bool = False,
        aggregation_method: Union[str, Callable] = 'list'
    ) -> pd.DataFrame:
        """
        Consolidate rows in a DataFrame by merging duplicate values into lists or using a specified aggregation function.

        Args:
            dataframe (pd.DataFrame): The DataFrame to consolidate.
            group_by (Optional[Union[str, List[str]]]): Columns or indices to group by.
            pivot_columns (Optional[Union[str, List[str]]]): Columns to pivot.
            level (Optional[int]): Level of multi-index to group by.
            n_dimensional (bool): If True, use a pivot table for N-Dimensional consolidation.
            aggregation_method (Union[str, Callable]): Aggregation method, e.g., 'list' to aggregate into lists.

        Returns:
            pd.DataFrame: The consolidated DataFrame.

        Example:
            >>> df = pd.DataFrame({'A': [1, 1, 2], 'B': [4, 5, 6], 'C': [7, 8, 9]})
            >>> result = RasUtils.consolidate_dataframe(df, group_by='A')
            >>> print(result)
            B         C
            A            
            1  [4, 5]  [7, 8]
            2  [6]     [9]
        """
        if aggregation_method == 'list':
            agg_func = lambda x: tuple(x)
        else:
            agg_func = aggregation_method

        if n_dimensional:
            result = dataframe.pivot_table(group_by, pivot_columns, aggfunc=agg_func)
        else:
            result = dataframe.groupby(group_by, level=level).agg(agg_func).applymap(list)

        return result

    @staticmethod
    @log_call
    def find_nearest_value(array: Union[list, np.ndarray], target_value: Union[int, float]) -> Union[int, float]:
        """
        Finds the nearest value in a NumPy array to the specified target value.

        Args:
            array (Union[list, np.ndarray]): The array to search within.
            target_value (Union[int, float]): The value to find the nearest neighbor to.

        Returns:
            Union[int, float]: The nearest value in the array to the specified target value.

        Example:
            >>> arr = np.array([1, 3, 5, 7, 9])
            >>> result = RasUtils.find_nearest_value(arr, 6)
            >>> print(result)
            5
        """
        array = np.asarray(array)
        idx = (np.abs(array - target_value)).argmin()
        return array[idx]
    
    @classmethod
    @log_call
    def horizontal_distance(cls, coord1: np.ndarray, coord2: np.ndarray) -> float:
        """
        Calculate the horizontal distance between two coordinate points.
        
        Args:
            coord1 (np.ndarray): First coordinate point [X, Y].
            coord2 (np.ndarray): Second coordinate point [X, Y].
        
        Returns:
            float: Horizontal distance.
        
        Example:
            >>> distance = RasUtils.horizontal_distance(np.array([0, 0]), np.array([3, 4]))
            >>> print(distance)
            5.0
        """
        return np.linalg.norm(coord2 - coord1)
    
    
    
    
    
==================================================

File: C:\GH\ras-commander\ras_commander\__init__.py
==================================================
"""
ras-commander: A Python library for automating HEC-RAS operations
"""

from importlib.metadata import version, PackageNotFoundError
from .LoggingConfig import setup_logging, get_logger
from .Decorators import log_call, standardize_input

try:
    __version__ = version("ras-commander")
except PackageNotFoundError:
    # package is not installed
    __version__ = "0.86.0"

# Set up logging
setup_logging()

# Core functionality
from .RasPrj import RasPrj, init_ras_project, get_ras_exe, ras
from .RasPlan import RasPlan
from .RasGeo import RasGeo  # DEPRECATED - use geom subpackage
from .RasGeometry import RasGeometry  # DEPRECATED - use geom subpackage
from .RasGeometryUtils import RasGeometryUtils  # DEPRECATED - use geom subpackage
from .RasUnsteady import RasUnsteady
from .RasUtils import RasUtils
from .RasExamples import RasExamples
from .M3Model import M3Model
from .RasCmdr import RasCmdr
from .RasControl import RasControl
from .RasMap import RasMap
from .RasGuiAutomation import RasGuiAutomation
from .RasBreach import RasBreach

# Geometry handling - imported from geom subpackage
from .geom import (
    GeomParser, GeomPreprocessor, GeomLandCover,
    GeomCrossSection, GeomStorage, GeomLateral,
    GeomInlineWeir, GeomBridge, GeomCulvert,
)

# HDF handling - imported from hdf subpackage
from .hdf import (
    HdfBase, HdfUtils, HdfPlan,
    HdfMesh, HdfXsec, HdfBndry, HdfStruc, HdfHydraulicTables,
    HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfResultsBreach,
    HdfPipe, HdfPump, HdfInfiltration,
    HdfPlot, HdfResultsPlot,
    HdfFluvialPluvial,
)

# Remote execution - lazy loaded to avoid importing until needed
# This reduces import time and allows optional dependencies to be truly optional
_REMOTE_EXPORTS = {
    'RasWorker', 'PsexecWorker', 'LocalWorker', 'SshWorker', 'WinrmWorker',
    'DockerWorker', 'SlurmWorker', 'AwsEc2Worker', 'AzureFrWorker',
    'init_ras_worker', 'load_workers_from_json', 'compute_parallel_remote',
    'ExecutionResult', 'get_worker_status'
}

# DSS operations - lazy loaded to avoid importing pyjnius/Java until needed
# This keeps the Java dependency truly optional for users who don't need DSS
_DSS_EXPORTS = {'RasDss'}

def __getattr__(name):
    """Lazy load remote execution and DSS components on first access."""
    if name in _REMOTE_EXPORTS:
        from . import remote
        return getattr(remote, name)
    if name in _DSS_EXPORTS:
        from . import dss
        return getattr(dss, name)
    raise AttributeError(f"module 'ras_commander' has no attribute '{name}'")


# Define __all__ to specify what should be imported when using "from ras_commander import *"
__all__ = [
    # Core functionality
    'RasPrj', 'init_ras_project', 'get_ras_exe', 'ras',
    'RasPlan', 'RasUnsteady', 'RasUtils',
    'RasExamples', 'M3Model', 'RasCmdr', 'RasControl', 'RasMap', 'RasGuiAutomation', 'HdfFluvialPluvial',

    # Geometry handling (new in v0.86.0)
    'GeomParser', 'GeomPreprocessor', 'GeomLandCover',
    'GeomCrossSection', 'GeomStorage', 'GeomLateral',
    'GeomInlineWeir', 'GeomBridge', 'GeomCulvert',

    # Deprecated geometry classes (will be removed before v1.0)
    'RasGeo', 'RasGeometry', 'RasGeometryUtils',

    # Remote execution (lazy loaded)
    'RasWorker', 'PsexecWorker', 'LocalWorker', 'SshWorker', 'WinrmWorker',
    'DockerWorker', 'SlurmWorker', 'AwsEc2Worker', 'AzureFrWorker',
    'init_ras_worker', 'load_workers_from_json', 'compute_parallel_remote',
    'ExecutionResult', 'get_worker_status',

    # DSS operations (lazy loaded)
    'RasDss',

    # HDF handling
    'HdfBase', 'HdfBndry', 'HdfMesh', 'HdfPlan',
    'HdfResultsMesh', 'HdfResultsPlan', 'HdfResultsXsec',
    'HdfStruc', 'HdfUtils', 'HdfXsec', 'HdfPump',
    'HdfPipe', 'HdfInfiltration', 'HdfHydraulicTables', 'HdfResultsBreach', 'RasBreach',

    # Plotting functionality
    'HdfPlot', 'HdfResultsPlot',

    # Utilities
    'get_logger', 'log_call', 'standardize_input',
]

==================================================

File: C:\GH\ras-commander\troubleshooting\demo_precipitation_hydrograph_parsing.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demonstration: Precipitation Hydrograph Parsing in ras-commander\n",
        "\n",
        "This notebook demonstrates the improved boundary condition parsing in ras-commander, specifically showing how **Precipitation Hydrograph** boundary conditions are now correctly parsed by default.\n",
        "\n",
        "## What's New\n",
        "\n",
        "The following improvements have been made:\n",
        "\n",
        "1. **`geometry_number` column removed from `unsteady_df`** - This column was incorrectly appearing in unsteady flow dataframes. It's now only present in `plan_df` where it belongs.\n",
        "\n",
        "2. **`Precipitation Hydrograph` now parsed by default** - When you initialize a project, `ras.boundaries_df` automatically includes Precipitation Hydrograph boundary conditions.\n",
        "\n",
        "3. **`Rating Curve` support added** - Rating curve boundary conditions are now also recognized.\n",
        "\n",
        "4. **`RasUnsteady.extract_tables()` supports new types** - The detailed table extraction functions now parse Precipitation Hydrograph and Rating Curve data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import from local development copy of ras-commander\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_dir = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_dir.parent\n",
        "sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(f\"Loading ras-commander from: {rascmdr_directory}\")\n",
        "from ras_commander import *\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialize the Davis Storm System Project\n",
        "\n",
        "This project contains a 2D area with a **Precipitation Hydrograph** boundary condition - similar to your TCC_road project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Davis Storm System example project\n",
        "davis_path = RasExamples.extract_project(\"Davis\")\n",
        "print(f\"Project extracted to: {davis_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the project\n",
        "init_ras_project(davis_path, \"6.6\")\n",
        "print(f\"Initialized project: {ras.project_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. View unsteady_df - Note: No `geometry_number` Column\n",
        "\n",
        "The `geometry_number` column was incorrectly appearing in `unsteady_df`. This has been fixed - the column now only appears in `plan_df` where it belongs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display unsteady_df - notice there's NO geometry_number column\n",
        "print(\"Columns in unsteady_df:\")\n",
        "print(list(ras.unsteady_df.columns))\n",
        "print(\"\\nFull unsteady_df:\")\n",
        "display(ras.unsteady_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify geometry_number is NOT in unsteady_df\n",
        "if 'geometry_number' in ras.unsteady_df.columns:\n",
        "    print(\"ERROR: geometry_number should NOT be in unsteady_df!\")\n",
        "else:\n",
        "    print(\"CORRECT: geometry_number is NOT in unsteady_df (as expected)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. View boundaries_df - Precipitation Hydrograph Now Parsed!\n",
        "\n",
        "The `boundaries_df` is automatically populated during project initialization. It now correctly identifies **Precipitation Hydrograph** boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all boundary conditions\n",
        "print(f\"Found {len(ras.boundaries_df)} boundary conditions:\\n\")\n",
        "display(ras.boundaries_df[['unsteady_number', 'boundary_condition_number', 'bc_type', \n",
        "                          'storage_area_name', 'hydrograph_type', 'hydrograph_num_values', 'Interval']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to show only Precipitation Hydrograph boundary conditions\n",
        "precip_bcs = ras.boundaries_df[ras.boundaries_df['bc_type'] == 'Precipitation Hydrograph']\n",
        "print(f\"Found {len(precip_bcs)} Precipitation Hydrograph boundary condition(s):\\n\")\n",
        "\n",
        "if not precip_bcs.empty:\n",
        "    for idx, row in precip_bcs.iterrows():\n",
        "        print(f\"  Boundary #{row['boundary_condition_number']}:\")\n",
        "        print(f\"    - Storage Area: {row['storage_area_name']}\")\n",
        "        print(f\"    - BC Type: {row['bc_type']}\")\n",
        "        print(f\"    - Interval: {row['Interval']}\")\n",
        "        print(f\"    - Number of values: {row['hydrograph_num_values']}\")\n",
        "        if 'hydrograph_values' in row and row['hydrograph_values']:\n",
        "            print(f\"    - First 5 values: {row['hydrograph_values'][:5]}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using RasUnsteady.extract_tables() for Detailed Data\n",
        "\n",
        "For more detailed hydrograph data extraction, you can use `RasUnsteady.extract_tables()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to the unsteady flow file\n",
        "unsteady_path = ras.unsteady_df.iloc[0]['full_path']\n",
        "print(f\"Unsteady file: {unsteady_path}\\n\")\n",
        "\n",
        "# Extract all tables from the unsteady file\n",
        "tables = RasUnsteady.extract_tables(unsteady_path)\n",
        "print(f\"Tables found in unsteady file:\")\n",
        "for table_name, df in tables.items():\n",
        "    print(f\"  - {table_name}: {len(df)} values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the Precipitation Hydrograph values if found\n",
        "if 'Precipitation Hydrograph=' in tables:\n",
        "    precip_df = tables['Precipitation Hydrograph=']\n",
        "    print(\"Precipitation Hydrograph Values:\")\n",
        "    print(f\"  Total values: {len(precip_df)}\")\n",
        "    print(f\"  Min: {precip_df['Value'].min():.4f}\")\n",
        "    print(f\"  Max: {precip_df['Value'].max():.4f}\")\n",
        "    print(f\"  Mean: {precip_df['Value'].mean():.4f}\")\n",
        "    print(f\"\\n  All values:\")\n",
        "    display(precip_df)\n",
        "else:\n",
        "    print(\"No Precipitation Hydrograph table found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Using RasUnsteady.extract_boundary_and_tables() for Full Details\n",
        "\n",
        "This function provides complete boundary condition information with nested table data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract full boundary and table information\n",
        "boundaries_with_tables = RasUnsteady.extract_boundary_and_tables(unsteady_path)\n",
        "print(f\"Found {len(boundaries_with_tables)} boundary conditions with table data\\n\")\n",
        "\n",
        "# Show columns available\n",
        "print(\"Columns available:\")\n",
        "print(list(boundaries_with_tables.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display boundary conditions and their tables\n",
        "for idx, row in boundaries_with_tables.iterrows():\n",
        "    print(f\"\\nBoundary {idx + 1}:\")\n",
        "    print(f\"  Storage Area: {row.get('Storage Area Name', 'N/A')}\")\n",
        "    print(f\"  DSS File: {row.get('DSS File', 'N/A')}\")\n",
        "    \n",
        "    tables = row.get('Tables', {})\n",
        "    if tables:\n",
        "        print(f\"  Tables:\")\n",
        "        for table_name, table_df in tables.items():\n",
        "            print(f\"    - {table_name}: {len(table_df)} values\")\n",
        "            if len(table_df) <= 25:\n",
        "                print(f\"      Values: {table_df['Value'].tolist()}\")\n",
        "            else:\n",
        "                print(f\"      First 10: {table_df['Value'].head(10).tolist()}\")\n",
        "    else:\n",
        "        print(f\"  Tables: None\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "### Supported Boundary Condition Types\n",
        "\n",
        "The following boundary condition types are now automatically parsed:\n",
        "\n",
        "| BC Type | Parsed by Default | extract_tables() |\n",
        "|---------|------------------|------------------|\n",
        "| Flow Hydrograph | \u2705 | \u2705 |\n",
        "| Stage Hydrograph | \u2705 | \u2705 |\n",
        "| **Precipitation Hydrograph** | \u2705 **NEW** | \u2705 **NEW** |\n",
        "| **Rating Curve** | \u2705 **NEW** | \u2705 **NEW** |\n",
        "| Lateral Inflow Hydrograph | \u2705 | \u2705 |\n",
        "| Uniform Lateral Inflow Hydrograph | \u2705 | \u2705 |\n",
        "| Gate Opening | \u2705 | \u2705 |\n",
        "| Normal Depth (Friction Slope) | \u2705 | N/A |\n",
        "\n",
        "### Key Points\n",
        "\n",
        "1. **`ras.boundaries_df`** - Automatically populated during `init_ras_project()` with all boundary conditions including Precipitation Hydrograph\n",
        "\n",
        "2. **`ras.unsteady_df`** - No longer contains `geometry_number` column (only in `plan_df`)\n",
        "\n",
        "3. **`RasUnsteady.extract_tables()`** - For detailed hydrograph value extraction\n",
        "\n",
        "4. **`RasUnsteady.extract_boundary_and_tables()`** - For comprehensive boundary + table data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final verification\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n\u2705 Project initialized: {ras.project_name}\")\n",
        "print(f\"\u2705 Unsteady files found: {len(ras.unsteady_df)}\")\n",
        "print(f\"\u2705 Boundary conditions parsed: {len(ras.boundaries_df)}\")\n",
        "\n",
        "# Check for geometry_number in unsteady_df\n",
        "if 'geometry_number' not in ras.unsteady_df.columns:\n",
        "    print(f\"\u2705 geometry_number NOT in unsteady_df (correct!)\")\n",
        "else:\n",
        "    print(f\"\u274c geometry_number still in unsteady_df (bug!)\")\n",
        "\n",
        "# Check for Precipitation Hydrograph BC type\n",
        "precip_count = len(ras.boundaries_df[ras.boundaries_df['bc_type'] == 'Precipitation Hydrograph'])\n",
        "if precip_count > 0:\n",
        "    print(f\"\u2705 Precipitation Hydrograph BCs found: {precip_count}\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f  No Precipitation Hydrograph BCs in this project\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\troubleshooting\demo_precipitation_hydrograph_parsing_executed.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demonstration: Precipitation Hydrograph Parsing in ras-commander\n",
        "\n",
        "This notebook demonstrates the improved boundary condition parsing in ras-commander, specifically showing how **Precipitation Hydrograph** boundary conditions are now correctly parsed by default.\n",
        "\n",
        "## What's New\n",
        "\n",
        "The following improvements have been made:\n",
        "\n",
        "1. **`geometry_number` column removed from `unsteady_df`** - This column was incorrectly appearing in unsteady flow dataframes. It's now only present in `plan_df` where it belongs.\n",
        "\n",
        "2. **`Precipitation Hydrograph` now parsed by default** - When you initialize a project, `ras.boundaries_df` automatically includes Precipitation Hydrograph boundary conditions.\n",
        "\n",
        "3. **`Rating Curve` support added** - Rating curve boundary conditions are now also recognized.\n",
        "\n",
        "4. **`RasUnsteady.extract_tables()` supports new types** - The detailed table extraction functions now parse Precipitation Hydrograph and Rating Curve data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:13.889147Z",
          "iopub.status.busy": "2025-11-29T14:08:13.888946Z",
          "iopub.status.idle": "2025-11-29T14:08:15.267722Z",
          "shell.execute_reply": "2025-11-29T14:08:15.267135Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from: C:\\GH\\ras-commander\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:14 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:14 - ras_commander.RasRemote - INFO - RasRemote module loaded\n"
          ]
        }
      ],
      "source": [
        "# Import from local development copy of ras-commander\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_dir = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_dir.parent\n",
        "sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(f\"Loading ras-commander from: {rascmdr_directory}\")\n",
        "from ras_commander import *\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialize the Davis Storm System Project\n",
        "\n",
        "This project contains a 2D area with a **Precipitation Hydrograph** boundary condition - similar to your TCC_road project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:15.270506Z",
          "iopub.status.busy": "2025-11-29T14:08:15.270133Z",
          "iopub.status.idle": "2025-11-29T14:08:22.133937Z",
          "shell.execute_reply": "2025-11-29T14:08:22.133508Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:15 - ras_commander.RasExamples - WARNING - No existing example projects zip file found.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:15 - ras_commander.RasExamples - INFO - No example projects zip file found. Downloading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:15 - ras_commander.RasExamples - INFO - Getting example projects for version 6.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:15 - ras_commander.RasExamples - INFO - Downloading HEC-RAS Example Projects from https://github.com/HydrologicEngineeringCenter/hec-downloads/releases/download/1.0.33/Example_Projects_6_6.zip. \n",
            "The file is over 400 MB, so it may take a few minutes to download....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Downloaded to C:\\GH\\ras-commander\\troubleshooting\\Example_Projects_6_6.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Extracting folder structure from zip file...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Extracted 68 projects.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Saved project data to C:\\GH\\ras-commander\\troubleshooting\\example_projects.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Extracting project 'Davis'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasExamples - INFO - Successfully extracted project 'Davis' to C:\\GH\\ras-commander\\troubleshooting\\example_projects\\Davis\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project extracted to: C:\\GH\\ras-commander\\troubleshooting\\example_projects\\Davis\n"
          ]
        }
      ],
      "source": [
        "# Extract the Davis Storm System example project\n",
        "davis_path = RasExamples.extract_project(\"Davis\")\n",
        "print(f\"Project extracted to: {davis_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.135921Z",
          "iopub.status.busy": "2025-11-29T14:08:22.135758Z",
          "iopub.status.idle": "2025-11-29T14:08:22.176097Z",
          "shell.execute_reply": "2025-11-29T14:08:22.175466Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\troubleshooting\\example_projects\\Davis\\DavisStormSystem.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized project: DavisStormSystem\n"
          ]
        }
      ],
      "source": [
        "# Initialize the project\n",
        "init_ras_project(davis_path, \"6.6\")\n",
        "print(f\"Initialized project: {ras.project_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. View unsteady_df - Note: No `geometry_number` Column\n",
        "\n",
        "The `geometry_number` column was incorrectly appearing in `unsteady_df`. This has been fixed - the column now only appears in `plan_df` where it belongs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.178391Z",
          "iopub.status.busy": "2025-11-29T14:08:22.178195Z",
          "iopub.status.idle": "2025-11-29T14:08:22.188354Z",
          "shell.execute_reply": "2025-11-29T14:08:22.187849Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in unsteady_df:\n",
            "['unsteady_number', 'full_path', 'Flow Title', 'Program Version', 'Use Restart', 'Precipitation Mode', 'Wind Mode', 'Met BC=Precipitation|Mode', 'Met BC=Evapotranspiration|Mode', 'Met BC=Precipitation|Expanded View', 'Met BC=Precipitation|Constant Units', 'Met BC=Precipitation|Gridded Source']\n",
            "\n",
            "Full unsteady_df:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>full_path</th>\\n', '      <th>Flow Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Use Restart</th>\\n', '      <th>Precipitation Mode</th>\\n', '      <th>Wind Mode</th>\\n', '      <th>Met BC=Precipitation|Mode</th>\\n', '      <th>Met BC=Evapotranspiration|Mode</th>\\n', '      <th>Met BC=Precipitation|Expanded View</th>\\n', '      <th>Met BC=Precipitation|Constant Units</th>\\n', '      <th>Met BC=Precipitation|Gridded Source</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\troubleshooting\\\\example_pr...</td>\\n', '      <td>Full System Rain w/ Pump</td>\\n', '      <td>6.60</td>\\n', '      <td>0</td>\\n', '      <td>Disable</td>\\n', '      <td>No Wind Forces</td>\\n', '      <td>Constant</td>\\n', '      <td>None</td>\\n', '      <td>-1</td>\\n', '      <td>in/hr</td>\\n', '      <td>DSS</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  unsteady_number                                          full_path  \\\n",
              "0              01  C:\\GH\\ras-commander\\troubleshooting\\example_pr...   \n",
              "\n",
              "                 Flow Title Program Version Use Restart Precipitation Mode  \\\n",
              "0  Full System Rain w/ Pump            6.60           0            Disable   \n",
              "\n",
              "        Wind Mode Met BC=Precipitation|Mode Met BC=Evapotranspiration|Mode  \\\n",
              "0  No Wind Forces                  Constant                           None   \n",
              "\n",
              "  Met BC=Precipitation|Expanded View Met BC=Precipitation|Constant Units  \\\n",
              "0                                 -1                               in/hr   \n",
              "\n",
              "  Met BC=Precipitation|Gridded Source  \n",
              "0                                 DSS  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display unsteady_df - notice there's NO geometry_number column\n",
        "print(\"Columns in unsteady_df:\")\n",
        "print(list(ras.unsteady_df.columns))\n",
        "print(\"\\nFull unsteady_df:\")\n",
        "display(ras.unsteady_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.190059Z",
          "iopub.status.busy": "2025-11-29T14:08:22.189916Z",
          "iopub.status.idle": "2025-11-29T14:08:22.192873Z",
          "shell.execute_reply": "2025-11-29T14:08:22.192385Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CORRECT: geometry_number is NOT in unsteady_df (as expected)\n"
          ]
        }
      ],
      "source": [
        "# Verify geometry_number is NOT in unsteady_df\n",
        "if 'geometry_number' in ras.unsteady_df.columns:\n",
        "    print(\"ERROR: geometry_number should NOT be in unsteady_df!\")\n",
        "else:\n",
        "    print(\"CORRECT: geometry_number is NOT in unsteady_df (as expected)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. View boundaries_df - Precipitation Hydrograph Now Parsed!\n",
        "\n",
        "The `boundaries_df` is automatically populated during project initialization. It now correctly identifies **Precipitation Hydrograph** boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.194498Z",
          "iopub.status.busy": "2025-11-29T14:08:22.194363Z",
          "iopub.status.idle": "2025-11-29T14:08:22.200867Z",
          "shell.execute_reply": "2025-11-29T14:08:22.200433Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 boundary conditions:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>boundary_condition_number</th>\\n', '      <th>bc_type</th>\\n', '      <th>storage_area_name</th>\\n', '      <th>hydrograph_type</th>\\n', '      <th>hydrograph_num_values</th>\\n', '      <th>Interval</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>1</td>\\n', '      <td>Normal Depth</td>\\n', '      <td></td>\\n', '      <td>None</td>\\n', '      <td>0</td>\\n', '      <td>NaN</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>01</td>\\n', '      <td>2</td>\\n', '      <td>Precipitation Hydrograph</td>\\n', '      <td></td>\\n', '      <td>Precipitation Hydrograph</td>\\n', '      <td>21</td>\\n', '      <td>1HOUR</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  unsteady_number  boundary_condition_number                   bc_type  \\\n",
              "0              01                          1              Normal Depth   \n",
              "1              01                          2  Precipitation Hydrograph   \n",
              "\n",
              "  storage_area_name           hydrograph_type  hydrograph_num_values Interval  \n",
              "0                                        None                      0      NaN  \n",
              "1                    Precipitation Hydrograph                     21    1HOUR  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display all boundary conditions\n",
        "print(f\"Found {len(ras.boundaries_df)} boundary conditions:\\n\")\n",
        "display(ras.boundaries_df[['unsteady_number', 'boundary_condition_number', 'bc_type', \n",
        "                          'storage_area_name', 'hydrograph_type', 'hydrograph_num_values', 'Interval']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.202516Z",
          "iopub.status.busy": "2025-11-29T14:08:22.202346Z",
          "iopub.status.idle": "2025-11-29T14:08:22.206834Z",
          "shell.execute_reply": "2025-11-29T14:08:22.206445Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1 Precipitation Hydrograph boundary condition(s):\n",
            "\n",
            "  Boundary #2:\n",
            "    - Storage Area: \n",
            "    - BC Type: Precipitation Hydrograph\n",
            "    - Interval: 1HOUR\n",
            "    - Number of values: 21\n",
            "    - First 5 values: ['.1', '.1', '.1', '.25', '.25']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter to show only Precipitation Hydrograph boundary conditions\n",
        "precip_bcs = ras.boundaries_df[ras.boundaries_df['bc_type'] == 'Precipitation Hydrograph']\n",
        "print(f\"Found {len(precip_bcs)} Precipitation Hydrograph boundary condition(s):\\n\")\n",
        "\n",
        "if not precip_bcs.empty:\n",
        "    for idx, row in precip_bcs.iterrows():\n",
        "        print(f\"  Boundary #{row['boundary_condition_number']}:\")\n",
        "        print(f\"    - Storage Area: {row['storage_area_name']}\")\n",
        "        print(f\"    - BC Type: {row['bc_type']}\")\n",
        "        print(f\"    - Interval: {row['Interval']}\")\n",
        "        print(f\"    - Number of values: {row['hydrograph_num_values']}\")\n",
        "        if 'hydrograph_values' in row and row['hydrograph_values']:\n",
        "            print(f\"    - First 5 values: {row['hydrograph_values'][:5]}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using RasUnsteady.extract_tables() for Detailed Data\n",
        "\n",
        "For more detailed hydrograph data extraction, you can use `RasUnsteady.extract_tables()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.208522Z",
          "iopub.status.busy": "2025-11-29T14:08:22.208354Z",
          "iopub.status.idle": "2025-11-29T14:08:22.211790Z",
          "shell.execute_reply": "2025-11-29T14:08:22.211381Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsteady file: C:\\GH\\ras-commander\\troubleshooting\\example_projects\\Davis\\DavisStormSystem.u01\n",
            "\n",
            "Tables found in unsteady file:\n",
            "  - Precipitation Hydrograph=: 21 values\n"
          ]
        }
      ],
      "source": [
        "# Get the path to the unsteady flow file\n",
        "unsteady_path = ras.unsteady_df.iloc[0]['full_path']\n",
        "print(f\"Unsteady file: {unsteady_path}\\n\")\n",
        "\n",
        "# Extract all tables from the unsteady file\n",
        "tables = RasUnsteady.extract_tables(unsteady_path)\n",
        "print(f\"Tables found in unsteady file:\")\n",
        "for table_name, df in tables.items():\n",
        "    print(f\"  - {table_name}: {len(df)} values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.213375Z",
          "iopub.status.busy": "2025-11-29T14:08:22.213184Z",
          "iopub.status.idle": "2025-11-29T14:08:22.219046Z",
          "shell.execute_reply": "2025-11-29T14:08:22.218467Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precipitation Hydrograph Values:\n",
            "  Total values: 21\n",
            "  Min: 0.0000\n",
            "  Max: 0.2500\n",
            "  Mean: 0.0619\n",
            "\n",
            "  All values:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Value</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>0.10</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>0.10</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>0.10</td>\\n', '    </tr><tr>\\n', '      <th>3</th>\\n', '      <td>0.25</td>\\n', '    </tr><tr>\\n', '      <th>4</th>\\n', '      <td>0.25</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "    Value\n",
              "0    0.10\n",
              "1    0.10\n",
              "2    0.10\n",
              "3    0.25\n",
              "4    0.25\n",
              "5    0.25\n",
              "6    0.25\n",
              "7    0.00\n",
              "8    0.00\n",
              "9    0.00\n",
              "10   0.00\n",
              "11   0.00\n",
              "12   0.00\n",
              "13   0.00\n",
              "14   0.00\n",
              "15   0.00\n",
              "16   0.00\n",
              "17   0.00\n",
              "18   0.00\n",
              "19   0.00\n",
              "20   0.00"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display the Precipitation Hydrograph values if found\n",
        "if 'Precipitation Hydrograph=' in tables:\n",
        "    precip_df = tables['Precipitation Hydrograph=']\n",
        "    print(\"Precipitation Hydrograph Values:\")\n",
        "    print(f\"  Total values: {len(precip_df)}\")\n",
        "    print(f\"  Min: {precip_df['Value'].min():.4f}\")\n",
        "    print(f\"  Max: {precip_df['Value'].max():.4f}\")\n",
        "    print(f\"  Mean: {precip_df['Value'].mean():.4f}\")\n",
        "    print(f\"\\n  All values:\")\n",
        "    display(precip_df)\n",
        "else:\n",
        "    print(\"No Precipitation Hydrograph table found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Using RasUnsteady.extract_boundary_and_tables() for Full Details\n",
        "\n",
        "This function provides complete boundary condition information with nested table data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.220890Z",
          "iopub.status.busy": "2025-11-29T14:08:22.220693Z",
          "iopub.status.idle": "2025-11-29T14:08:22.227948Z",
          "shell.execute_reply": "2025-11-29T14:08:22.227507Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 09:08:22 - ras_commander.RasUnsteady - INFO - Successfully extracted boundaries and tables from C:\\GH\\ras-commander\\troubleshooting\\example_projects\\Davis\\DavisStormSystem.u01\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 boundary conditions with table data\n",
            "\n",
            "Columns available:\n",
            "['DSS File', 'Tables', 'River Name', 'Reach Name', 'River Station', 'Downstream River Station', 'Storage Area Connection', 'Storage Area Name', 'Pump Station Name', 'Blank 1', 'Blank 2']\n"
          ]
        }
      ],
      "source": [
        "# Extract full boundary and table information\n",
        "boundaries_with_tables = RasUnsteady.extract_boundary_and_tables(unsteady_path)\n",
        "print(f\"Found {len(boundaries_with_tables)} boundary conditions with table data\\n\")\n",
        "\n",
        "# Show columns available\n",
        "print(\"Columns available:\")\n",
        "print(list(boundaries_with_tables.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.229588Z",
          "iopub.status.busy": "2025-11-29T14:08:22.229415Z",
          "iopub.status.idle": "2025-11-29T14:08:22.233092Z",
          "shell.execute_reply": "2025-11-29T14:08:22.232727Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Boundary 1:\n",
            "  Storage Area: DS Channel\n",
            "  DSS File: \n",
            "  Tables: None\n",
            "\n",
            "Boundary 2:\n",
            "  Storage Area: area2\n",
            "  DSS File: \n",
            "  Tables:\n",
            "    - Precipitation Hydrograph: 21 values\n",
            "      Values: [0.1, 0.1, 1.0, 25.0, 25.0, 25.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# Display boundary conditions and their tables\n",
        "for idx, row in boundaries_with_tables.iterrows():\n",
        "    print(f\"\\nBoundary {idx + 1}:\")\n",
        "    print(f\"  Storage Area: {row.get('Storage Area Name', 'N/A')}\")\n",
        "    print(f\"  DSS File: {row.get('DSS File', 'N/A')}\")\n",
        "    \n",
        "    tables = row.get('Tables', {})\n",
        "    if tables:\n",
        "        print(f\"  Tables:\")\n",
        "        for table_name, table_df in tables.items():\n",
        "            print(f\"    - {table_name}: {len(table_df)} values\")\n",
        "            if len(table_df) <= 25:\n",
        "                print(f\"      Values: {table_df['Value'].tolist()}\")\n",
        "            else:\n",
        "                print(f\"      First 10: {table_df['Value'].head(10).tolist()}\")\n",
        "    else:\n",
        "        print(f\"  Tables: None\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "### Supported Boundary Condition Types\n",
        "\n",
        "The following boundary condition types are now automatically parsed:\n",
        "\n",
        "| BC Type | Parsed by Default | extract_tables() |\n",
        "|---------|------------------|------------------|\n",
        "| Flow Hydrograph | \u2705 | \u2705 |\n",
        "| Stage Hydrograph | \u2705 | \u2705 |\n",
        "| **Precipitation Hydrograph** | \u2705 **NEW** | \u2705 **NEW** |\n",
        "| **Rating Curve** | \u2705 **NEW** | \u2705 **NEW** |\n",
        "| Lateral Inflow Hydrograph | \u2705 | \u2705 |\n",
        "| Uniform Lateral Inflow Hydrograph | \u2705 | \u2705 |\n",
        "| Gate Opening | \u2705 | \u2705 |\n",
        "| Normal Depth (Friction Slope) | \u2705 | N/A |\n",
        "\n",
        "### Key Points\n",
        "\n",
        "1. **`ras.boundaries_df`** - Automatically populated during `init_ras_project()` with all boundary conditions including Precipitation Hydrograph\n",
        "\n",
        "2. **`ras.unsteady_df`** - No longer contains `geometry_number` column (only in `plan_df`)\n",
        "\n",
        "3. **`RasUnsteady.extract_tables()`** - For detailed hydrograph value extraction\n",
        "\n",
        "4. **`RasUnsteady.extract_boundary_and_tables()`** - For comprehensive boundary + table data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T14:08:22.234969Z",
          "iopub.status.busy": "2025-11-29T14:08:22.234718Z",
          "iopub.status.idle": "2025-11-29T14:08:22.238959Z",
          "shell.execute_reply": "2025-11-29T14:08:22.238570Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "VERIFICATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "\u2705 Project initialized: DavisStormSystem\n",
            "\u2705 Unsteady files found: 1\n",
            "\u2705 Boundary conditions parsed: 2\n",
            "\u2705 geometry_number NOT in unsteady_df (correct!)\n",
            "\u2705 Precipitation Hydrograph BCs found: 1\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Final verification\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n\u2705 Project initialized: {ras.project_name}\")\n",
        "print(f\"\u2705 Unsteady files found: {len(ras.unsteady_df)}\")\n",
        "print(f\"\u2705 Boundary conditions parsed: {len(ras.boundaries_df)}\")\n",
        "\n",
        "# Check for geometry_number in unsteady_df\n",
        "if 'geometry_number' not in ras.unsteady_df.columns:\n",
        "    print(f\"\u2705 geometry_number NOT in unsteady_df (correct!)\")\n",
        "else:\n",
        "    print(f\"\u274c geometry_number still in unsteady_df (bug!)\")\n",
        "\n",
        "# Check for Precipitation Hydrograph BC type\n",
        "precip_count = len(ras.boundaries_df[ras.boundaries_df['bc_type'] == 'Precipitation Hydrograph'])\n",
        "if precip_count > 0:\n",
        "    print(f\"\u2705 Precipitation Hydrograph BCs found: {precip_count}\")\n",
        "else:\n",
        "    print(f\"\u26a0\ufe0f  No Precipitation Hydrograph BCs in this project\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\troubleshooting\executed_02.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:18.050747Z",
          "iopub.status.busy": "2025-11-29T13:53:18.050582Z",
          "iopub.status.idle": "2025-11-29T13:53:19.443748Z",
          "shell.execute_reply": "2025-11-29T13:53:19.443184Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:44: SyntaxWarning: invalid escape sequence '\\T'\n",
            "  share_path=r\"\\\\WORKSTATION-01\\Temp\\RAS_Runs\",\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:142: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  share_path: UNC path to accessible network share (e.g., \\\\hostname\\share\\folder)\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:235: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  ras_exe_path=r\"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe\",\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:345: SyntaxWarning: invalid escape sequence '\\T'\n",
            "  share_path=r\"\\\\WORKSTATION-01\\Temp\\RAS_Runs\",\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:534: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  ras_exe_path=r\"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe\"\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:604: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  ras_exe_path=r\"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe\"\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:685: SyntaxWarning: invalid escape sequence '\\T'\n",
            "  share_path=r\"\\\\WORKSTATION-01\\Temp\\RAS_Runs\",\n",
            "C:\\GH\\ras-commander\\ras_commander\\RasRemote.py:794: SyntaxWarning: invalid escape sequence '\\R'\n",
            "  unc_path: Full UNC path (e.g., \\\\192.168.3.8\\RasRemote\\folder\\file.bat)\n",
            "2025-11-29 08:53:18 - ras_commander.RasRemote - INFO - RasRemote module loaded\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install/upgrade ras-commander from pip\n",
        "#!pip install --upgrade ras-commander\n",
        "\n",
        "#Import the ras-commander package\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Optional Code Cell For Development/Testing Mode (Local Copy)\n",
        "##### Uncomment and run this cell instead of the pip cell above\n",
        "\n",
        "```python\n",
        "# For Development Mode, add the parent directory to the Python path\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "\n",
        "# Use insert(0) instead of append() to give highest priority to local version\n",
        "if str(rascmdr_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(rascmdr_directory))\n",
        "\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "from ras_commander import *\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Plan and Geometry Operations\n",
        "\n",
        "This notebook demonstrates how to perform operations on HEC-RAS plan and geometry files using the RAS Commander library. We'll explore how to initialize projects, clone plans and geometries, configure parameters, execute plans, and analyze results.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
        "2. **Plan Operations**:\n",
        "   - Clone an existing plan to create a new one\n",
        "   - Configure simulation parameters and intervals\n",
        "   - Set run flags and update descriptions\n",
        "3. **Geometry Operations**:\n",
        "   - Clone a geometry file to create a modified version\n",
        "   - Set the geometry for a plan\n",
        "   - Clear geometry preprocessor files to ensure clean results\n",
        "4. **Flow Operations**:\n",
        "   - Clone unsteady flow files\n",
        "   - Configure flow parameters\n",
        "5. **Plan Computation**: Run the plan with specified settings\n",
        "6. **Results Verification**: Check HDF entries to confirm results were written"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.445922Z",
          "iopub.status.busy": "2025-11-29T13:53:19.445619Z",
          "iopub.status.idle": "2025-11-29T13:53:19.448303Z",
          "shell.execute_reply": "2025-11-29T13:53:19.447787Z"
        }
      },
      "outputs": [],
      "source": [
        "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.449851Z",
          "iopub.status.busy": "2025-11-29T13:53:19.449704Z",
          "iopub.status.idle": "2025-11-29T13:53:19.452894Z",
          "shell.execute_reply": "2025-11-29T13:53:19.452364Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ras-commander from local dev copy\n"
          ]
        }
      ],
      "source": [
        "# Enable this cell for local development version of ras-commander\n",
        "import os\n",
        "import sys      \n",
        "from pathlib import Path\n",
        "current_file = Path(os.getcwd()).resolve()\n",
        "rascmdr_directory = current_file.parent\n",
        "sys.path.append(str(rascmdr_directory))\n",
        "print(\"Loading ras-commander from local dev copy\")\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.454590Z",
          "iopub.status.busy": "2025-11-29T13:53:19.454427Z",
          "iopub.status.idle": "2025-11-29T13:53:19.457265Z",
          "shell.execute_reply": "2025-11-29T13:53:19.456753Z"
        }
      },
      "outputs": [],
      "source": [
        "# Import the required libraries for this notebook\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from datetime import datetime  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract an example HEC-RAS project. For this notebook, we'll use the \"Balde Eagle Creek\" project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.459313Z",
          "iopub.status.busy": "2025-11-29T13:53:19.459138Z",
          "iopub.status.idle": "2025-11-29T13:53:19.510783Z",
          "shell.execute_reply": "2025-11-29T13:53:19.510171Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Found zip file: C:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        }
      ],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(bald_eagle_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Initialization\n",
        "\n",
        "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the project folder path and HEC-RAS version as parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.512755Z",
          "iopub.status.busy": "2025-11-29T13:53:19.512601Z",
          "iopub.status.idle": "2025-11-29T13:53:19.579362Z",
          "shell.execute_reply": "2025-11-29T13:53:19.578715Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized HEC-RAS project: BaldEagle\n",
            "\n",
            "HEC-RAS Project Plan Data (plan_df):\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '      <th>flow_type</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Unsteady</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Steady</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "\n",
              "  Program Version Short Identifier                  Simulation Date  \\\n",
              "0            5.00     UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN        SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "0                 2MIN            1HOUR        1  ...     None      dss   \n",
              "1                 2MIN              NaN        1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method HDF_Results_Path Geom File  \\\n",
              "0                     2             None        01   \n",
              "1                     1             None        01   \n",
              "\n",
              "                                           Geom Path  Flow File  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...         02   \n",
              "\n",
              "                                           Flow Path  \\\n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                           full_path flow_type  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  Unsteady  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...    Steady  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan and Geometry Operations in HEC-RAS\n",
        "\n",
        "Before diving into the operations, let's understand what plan and geometry files are in HEC-RAS:\n",
        "\n",
        "- **Plan Files** (`.p*`): Define the simulation parameters including the reference to geometry and flow files, as well as computational settings.\n",
        "- **Geometry Files** (`.g*`): Define the physical characteristics of the river/channel system including cross-sections, 2D areas, and structures.\n",
        "\n",
        "The `RasPlan` and `RasGeo` classes provide methods for working with these files, including:\n",
        "\n",
        "1. Creating new plans and geometries by cloning existing ones\n",
        "2. Modifying simulation parameters and settings\n",
        "3. Associating geometries with plans\n",
        "4. Managing preprocessor files\n",
        "5. Retrieving information from plans and geometries\n",
        "\n",
        "In the following sections, we'll explore these operations in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cloning Plans and Geometries\n",
        "\n",
        "Let's start by cloning a plan to create a new simulation scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.581594Z",
          "iopub.status.busy": "2025-11-29T13:53:19.581382Z",
          "iopub.status.idle": "2025-11-29T13:53:19.646008Z",
          "shell.execute_reply": "2025-11-29T13:53:19.645583Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Project file updated with new Plan entry: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New plan created: 03\n",
            "\n",
            "Updated plan files:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>UNET D2 Cores</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>UnsteadyFlow</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>0.0</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>Steady Flow Run</td>\\n', '      <td>NaN</td>\\n', '      <td>SteadyRun</td>\\n', '      <td>02/18/1999,0000,02/24/1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>NaN</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>NaN</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>1</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>02</td>\\n', '      <td>01</td>\\n', '      <td>Unsteady with Bridges and Dam</td>\\n', '      <td>5.00</td>\\n', '      <td>Combined Test Plan</td>\\n', '      <td>18FEB1999,0000,24FEB1999,0500</td>\\n', '      <td>2MIN</td>\\n', '      <td>1HOUR</td>\\n', '      <td>1</td>\\n', '      <td>...</td>\\n', '      <td>0.0</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number                     Plan Title  \\\n",
              "0          01              02              01  Unsteady with Bridges and Dam   \n",
              "1          02            None              01                Steady Flow Run   \n",
              "2          03              02              01  Unsteady with Bridges and Dam   \n",
              "\n",
              "  Program Version    Short Identifier                  Simulation Date  \\\n",
              "0            5.00        UnsteadyFlow    18FEB1999,0000,24FEB1999,0500   \n",
              "1             NaN           SteadyRun  02/18/1999,0000,02/24/1999,0500   \n",
              "2            5.00  Combined Test Plan    18FEB1999,0000,24FEB1999,0500   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... UNET D2 Cores PS Cores  \\\n",
              "0                 2MIN            1HOUR        1  ...           0.0     None   \n",
              "1                 2MIN              NaN        1  ...           NaN     None   \n",
              "2                 2MIN            1HOUR        1  ...           0.0     None   \n",
              "\n",
              "  DSS File Friction Slope Method HDF_Results_Path  Geom File  Geom Path  \\\n",
              "0      dss                     2             None         01       None   \n",
              "1      dss                     1             None         01       None   \n",
              "2      dss                     2             None         01       None   \n",
              "\n",
              "  Flow File Flow Path                                          full_path  \n",
              "0        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "1        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "2        02      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "\n",
              "[3 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n",
            "\n",
            "New plan details:\n",
            "Plan number: 03\n",
            "Description: No description\n",
            "Short Identifier: Combined Test Plan\n",
            "Geometry file: 01\n",
            "File path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        }
      ],
      "source": [
        "# Clone plan \"01\" to create a new plan\n",
        "new_plan_number = RasPlan.clone_plan(\"1\", new_shortid=\"Combined Test Plan\")\n",
        "print(f\"New plan created: {new_plan_number}\")\n",
        "\n",
        "# Display updated plan files\n",
        "print(\"\\nUpdated plan files:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Get the path to the new plan file\n",
        "plan_path = RasPlan.get_plan_path(new_plan_number)\n",
        "print(f\"\\nNew plan file path: {plan_path}\")\n",
        "\n",
        "# Let's examine the new plan's details\n",
        "new_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
        "print(f\"\\nNew plan details:\")\n",
        "print(f\"Plan number: {new_plan_number}\")\n",
        "print(f\"Description: {new_plan.get('description', 'No description')}\")\n",
        "print(f\"Short Identifier: {new_plan.get('Short Identifier', 'Not available')}\")\n",
        "print(f\"Geometry file: {new_plan.get('Geom File', 'None')}\")\n",
        "print(f\"File path: {new_plan['full_path']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.647712Z",
          "iopub.status.busy": "2025-11-29T13:53:19.647524Z",
          "iopub.status.idle": "2025-11-29T13:53:19.659582Z",
          "shell.execute_reply": "2025-11-29T13:53:19.659031Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Retrieved Plan Title: Unsteady with Bridges and Dam\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Retrieved Short Identifier: Combined Test Plan\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current plan title: Unsteady with Bridges and Dam\n",
            "Current plan shortid: Combined Test Plan\n"
          ]
        }
      ],
      "source": [
        "# Get the current plan title and shortid\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.661475Z",
          "iopub.status.busy": "2025-11-29T13:53:19.661300Z",
          "iopub.status.idle": "2025-11-29T13:53:19.690890Z",
          "shell.execute_reply": "2025-11-29T13:53:19.690493Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Updated Plan Title in plan file to: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - WARNING - Short Identifier too long (24 char max). Truncating: Combined Test Plan clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Updated Short Identifier in plan file to: Combined Test Plan clone\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Retrieved Plan Title: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Retrieved Short Identifier: Combined Test Plan clone\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated plan title: Unsteady with Bridges and Dam clonedplan\n",
            "Updated plan shortid: Combined Test Plan clone\n"
          ]
        }
      ],
      "source": [
        "# Update the title and shortid to append \" clonedplan\"\n",
        "new_title = f\"{current_title} clonedplan\"\n",
        "new_shortid = f\"{current_shortid} clonedplan\"\n",
        "\n",
        "RasPlan.set_plan_title(new_plan_number, new_title)\n",
        "RasPlan.set_shortid(new_plan_number, new_shortid)\n",
        "\n",
        "print(f\"\\nUpdated plan title: {RasPlan.get_plan_title(new_plan_number)}\")\n",
        "print(f\"Updated plan shortid: {RasPlan.get_shortid(new_plan_number)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.692483Z",
          "iopub.status.busy": "2025-11-29T13:53:19.692338Z",
          "iopub.status.idle": "2025-11-29T13:53:19.701880Z",
          "shell.execute_reply": "2025-11-29T13:53:19.701463Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Retrieved Plan Title: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Retrieved Short Identifier: Combined Test Plan clone\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current plan title: Unsteady with Bridges and Dam clonedplan\n",
            "Current plan shortid: Combined Test Plan clone\n"
          ]
        }
      ],
      "source": [
        "# Get the current plan title and shortid again to confirm the changes\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's clone a geometry file. This allows us to make modifications to a geometry without affecting the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.703410Z",
          "iopub.status.busy": "2025-11-29T13:53:19.703271Z",
          "iopub.status.idle": "2025-11-29T13:53:19.740070Z",
          "shell.execute_reply": "2025-11-29T13:53:19.739633Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g01 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g01.hdf to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02.hdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Project file updated with new Geom entry: 02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New geometry created: 02\n",
            "\n",
            "Updated geometry files:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>geom_file</th>\\n', '      <th>geom_number</th>\\n', '      <th>full_path</th>\\n', '      <th>hdf_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>g01</td>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>g02</td>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  geom_file geom_number                                          full_path  \\\n",
              "0       g01          01  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1       g02          02  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "                                            hdf_path  \n",
              "0  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "1  C:\\GH\\ras-commander\\examples\\example_projects\\...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Found geometry path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New geometry file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n",
            "\n",
            "New geometry details:\n",
            "Geometry number: 02\n",
            "Geometry file: Not available\n",
            "File path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.g02\n",
            "HDF path: None\n"
          ]
        }
      ],
      "source": [
        "# Clone geometry \"01\" to create a new geometry file\n",
        "new_geom_number = RasPlan.clone_geom(\"01\")\n",
        "print(f\"New geometry created: {new_geom_number}\")\n",
        "\n",
        "# Display updated geometry files\n",
        "print(\"\\nUpdated geometry files:\")\n",
        "display.display(ras.geom_df)\n",
        "\n",
        "# Get the path to the new geometry file\n",
        "geom_path = RasPlan.get_geom_path(new_geom_number)\n",
        "print(f\"\\nNew geometry file path: {geom_path}\")\n",
        "\n",
        "# Examine the new geometry's details\n",
        "new_geom = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number].squeeze()\n",
        "print(f\"\\nNew geometry details:\")\n",
        "print(f\"Geometry number: {new_geom_number}\")\n",
        "print(f\"Geometry file: {new_geom.get('geom_file', 'Not available')}\")\n",
        "print(f\"File path: {new_geom.get('full_path', 'Not available')}\")\n",
        "print(f\"HDF path: {new_geom.get('hdf_path', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also clone an unsteady flow file to complete our new simulation setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.741673Z",
          "iopub.status.busy": "2025-11-29T13:53:19.741504Z",
          "iopub.status.idle": "2025-11-29T13:53:19.785467Z",
          "shell.execute_reply": "2025-11-29T13:53:19.784893Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - File cloned from C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u02 to C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Project file updated with new Unsteady entry: 01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasMap - INFO - Successfully parsed RASMapper file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.rasmap\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New unsteady flow created: 01\n",
            "\n",
            "Updated unsteady flow files:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>unsteady_number</th>\\n', '      <th>full_path</th>\\n', '      <th>Flow Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Use Restart</th>\\n', '      <th>Precipitation Mode</th>\\n', '      <th>Wind Mode</th>\\n', '      <th>Met BC=Precipitation|Mode</th>\\n', '      <th>Met BC=Evapotranspiration|Mode</th>\\n', '      <th>Met BC=Precipitation|Expanded View</th>\\n', '      <th>Met BC=Precipitation|Constant Units</th>\\n', '      <th>Met BC=Precipitation|Gridded Source</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>02</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Flow Hydrograph 2</td>\\n', '      <td>6.30</td>\\n', '      <td>0</td>\\n', '      <td>Disable</td>\\n', '      <td>No Wind Forces</td>\\n', '      <td>None</td>\\n', '      <td>None</td>\\n', '      <td>0</td>\\n', '      <td>mm/hr</td>\\n', '      <td>DSS</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>01</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>Flow Hydrograph 2</td>\\n', '      <td>6.30</td>\\n', '      <td>0</td>\\n', '      <td>Disable</td>\\n', '      <td>No Wind Forces</td>\\n', '      <td>None</td>\\n', '      <td>None</td>\\n', '      <td>0</td>\\n', '      <td>mm/hr</td>\\n', '      <td>DSS</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  unsteady_number                                          full_path  \\\n",
              "0              02  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "1              01  C:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
              "\n",
              "          Flow Title Program Version Use Restart Precipitation Mode  \\\n",
              "0  Flow Hydrograph 2            6.30           0            Disable   \n",
              "1  Flow Hydrograph 2            6.30           0            Disable   \n",
              "\n",
              "        Wind Mode Met BC=Precipitation|Mode Met BC=Evapotranspiration|Mode  \\\n",
              "0  No Wind Forces                      None                           None   \n",
              "1  No Wind Forces                      None                           None   \n",
              "\n",
              "  Met BC=Precipitation|Expanded View Met BC=Precipitation|Constant Units  \\\n",
              "0                                  0                               mm/hr   \n",
              "1                                  0                               mm/hr   \n",
              "\n",
              "  Met BC=Precipitation|Gridded Source  \n",
              "0                                 DSS  \n",
              "1                                 DSS  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New unsteady flow details:\n",
            "Unsteady number: 01\n",
            "File path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.u01\n",
            "Flow Title: Flow Hydrograph 2\n"
          ]
        }
      ],
      "source": [
        "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
        "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
        "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
        "\n",
        "# Display updated unsteady flow files\n",
        "print(\"\\nUpdated unsteady flow files:\")\n",
        "display.display(ras.unsteady_df)\n",
        "\n",
        "# Examine the new unsteady flow's details\n",
        "new_unsteady = ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number].iloc[0]\n",
        "print(f\"\\nNew unsteady flow details:\")\n",
        "print(f\"Unsteady number: {new_unsteady_number}\")\n",
        "print(f\"File path: {new_unsteady['full_path']}\")\n",
        "print(f\"Flow Title: {new_unsteady.get('Flow Title', 'Not available')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Associating Files and Setting Parameters\n",
        "\n",
        "Now that we have cloned our plan, geometry, and unsteady flow files, we need to associate them with each other and set various parameters.\n",
        "\n",
        "### Setting Geometry for a Plan\n",
        "\n",
        "Let's associate our new geometry with our new plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.787224Z",
          "iopub.status.busy": "2025-11-29T13:53:19.787064Z",
          "iopub.status.idle": "2025-11-29T13:53:19.807011Z",
          "shell.execute_reply": "2025-11-29T13:53:19.806550Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Updated Geom File in plan file to g02 for plan 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Geometry for plan 03 set to 02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated geometry for plan 03 to geometry 02\n",
            "Plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n",
            "\n",
            "Verified that plan 03 now uses geometry file: g02\n"
          ]
        }
      ],
      "source": [
        "# Set the new geometry for the cloned plan\n",
        "updated_geom_df = RasPlan.set_geom(new_plan_number, new_geom_number)\n",
        "plan_path = RasPlan.get_plan_path(new_plan_number, ras_object=ras)\n",
        "print(f\"Updated geometry for plan {new_plan_number} to geometry {new_geom_number}\")\n",
        "print(f\"Plan file path: {plan_path}\")\n",
        "\n",
        "# Let's verify the change\n",
        "updated_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
        "print(f\"\\nVerified that plan {new_plan_number} now uses geometry file: {updated_plan.get('Geom File', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Unsteady Flow for a Plan\n",
        "\n",
        "Similarly, let's associate our new unsteady flow file with our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.808793Z",
          "iopub.status.busy": "2025-11-29T13:53:19.808619Z",
          "iopub.status.idle": "2025-11-29T13:53:19.829564Z",
          "shell.execute_reply": "2025-11-29T13:53:19.829129Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated unsteady flow for plan 03 to unsteady flow 01\n"
          ]
        }
      ],
      "source": [
        "# Set unsteady flow for the cloned plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Updated unsteady flow for plan {new_plan_number} to unsteady flow {new_unsteady_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clearing Geometry Preprocessor Files\n",
        "\n",
        "When working with geometry files, it's important to clear the preprocessor files to ensure clean results. These files (with `.c*` extension) contain computed hydraulic properties that should be recomputed when the geometry changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.831418Z",
          "iopub.status.busy": "2025-11-29T13:53:19.831244Z",
          "iopub.status.idle": "2025-11-29T13:53:19.837190Z",
          "shell.execute_reply": "2025-11-29T13:53:19.836819Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleared geometry preprocessor files for plan 03\n",
            "Preprocessor file exists after clearing: False\n"
          ]
        }
      ],
      "source": [
        "# Clear geometry preprocessor files for the cloned plan\n",
        "RasGeo.clear_geompre_files(plan_path)\n",
        "print(f\"Cleared geometry preprocessor files for plan {new_plan_number}\")\n",
        "\n",
        "# Check if preprocessor file exists after clearing\n",
        "geom_preprocessor_suffix = '.c' + ''.join(Path(plan_path).suffixes[1:])\n",
        "geom_preprocessor_file = Path(plan_path).with_suffix(geom_preprocessor_suffix)\n",
        "print(f\"Preprocessor file exists after clearing: {geom_preprocessor_file.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Computation Parameters\n",
        "\n",
        "Let's set the computation parameters for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.839051Z",
          "iopub.status.busy": "2025-11-29T13:53:19.838886Z",
          "iopub.status.idle": "2025-11-29T13:53:19.877629Z",
          "shell.execute_reply": "2025-11-29T13:53:19.877205Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Constructed plan file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated number of cores for plan 03 to 2"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasUtils - INFO - Successfully updated file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Verified that UNET D1 Cores is set to: 2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated geometry preprocessor options for plan 03\n",
            "- Run HTab: -1 (Force recomputation of geometry tables)\n",
            "- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\n",
            "\n",
            "Verified setting values:\n",
            "- Run HTab: -1\n",
            "- UNET Use Existing IB Tables: -1\n"
          ]
        }
      ],
      "source": [
        "# Set the number of cores to use for the computation\n",
        "RasPlan.set_num_cores(new_plan_number, 2)\n",
        "print(f\"Updated number of cores for plan {new_plan_number} to 2\")\n",
        "\n",
        "# Verify by extracting the value from the plan file\n",
        "cores_value = RasPlan.get_plan_value(new_plan_number, \"UNET D1 Cores\")\n",
        "print(f\"\\nVerified that UNET D1 Cores is set to: {cores_value}\")\n",
        "\n",
        "# Set geometry preprocessor options\n",
        "RasPlan.set_geom_preprocessor(plan_path, run_htab=-1, use_ib_tables=-1)\n",
        "print(f\"Updated geometry preprocessor options for plan {new_plan_number}\")\n",
        "print(f\"- Run HTab: -1 (Force recomputation of geometry tables)\")\n",
        "print(f\"- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\")\n",
        "\n",
        "# Verify by extracting the values from the plan file\n",
        "run_htab_value = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
        "ib_tables_value = RasPlan.get_plan_value(new_plan_number, \"UNET Use Existing IB Tables\")\n",
        "print(f\"\\nVerified setting values:\")\n",
        "print(f\"- Run HTab: {run_htab_value}\")\n",
        "print(f\"- UNET Use Existing IB Tables: {ib_tables_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating Simulation Parameters\n",
        "\n",
        "Now, let's update various simulation parameters for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.879381Z",
          "iopub.status.busy": "2025-11-29T13:53:19.879235Z",
          "iopub.status.idle": "2025-11-29T13:53:19.889584Z",
          "shell.execute_reply": "2025-11-29T13:53:19.888712Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Updated simulation date in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated simulation date for plan 03:\n",
            "- Start Date: 2023-01-01 00:00:00\n",
            "- End Date: 2023-01-05 23:59:00\n"
          ]
        }
      ],
      "source": [
        "# 1. Update simulation date\n",
        "start_date = datetime(2023, 1, 1, 0, 0)  # January 1, 2023, 00:00\n",
        "end_date = datetime(2023, 1, 5, 23, 59)  # January 5, 2023, 23:59\n",
        "\n",
        "RasPlan.update_simulation_date(new_plan_number, start_date, end_date)\n",
        "print(f\"Updated simulation date for plan {new_plan_number}:\")\n",
        "print(f\"- Start Date: {start_date}\")\n",
        "print(f\"- End Date: {end_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.892420Z",
          "iopub.status.busy": "2025-11-29T13:53:19.892235Z",
          "iopub.status.idle": "2025-11-29T13:53:19.905093Z",
          "shell.execute_reply": "2025-11-29T13:53:19.904607Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified Simulation Date value: 01JAN2023,0000,05JAN2023,2359\n"
          ]
        }
      ],
      "source": [
        "# Verify the update\n",
        "sim_date = RasPlan.get_plan_value(new_plan_number, \"Simulation Date\")\n",
        "print(f\"Verified Simulation Date value: {sim_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.907159Z",
          "iopub.status.busy": "2025-11-29T13:53:19.906974Z",
          "iopub.status.idle": "2025-11-29T13:53:19.929486Z",
          "shell.execute_reply": "2025-11-29T13:53:19.929090Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Successfully updated intervals in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated plan intervals for plan 03:\n",
            "- Computation Interval: 1MIN\n",
            "- Output Interval: 15MIN\n",
            "- Mapping Interval: 30MIN\n",
            "Verified interval values:\n",
            "- Computation Interval: 1MIN\n",
            "- Mapping Interval: 30MIN\n"
          ]
        }
      ],
      "source": [
        "# 2. Update plan intervals\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_plan_number,\n",
        "    computation_interval=\"1MIN\",  # Computational time step\n",
        "    output_interval=\"15MIN\",      # How often results are written\n",
        "    mapping_interval=\"30MIN\"      # How often mapping outputs are created\n",
        ")\n",
        "print(f\"\\nUpdated plan intervals for plan {new_plan_number}:\")\n",
        "print(f\"- Computation Interval: 1MIN\")\n",
        "print(f\"- Output Interval: 15MIN\")\n",
        "print(f\"- Mapping Interval: 30MIN\")\n",
        "\n",
        "# Verify the updates\n",
        "comp_interval = RasPlan.get_plan_value(new_plan_number, \"Computation Interval\")\n",
        "mapping_interval = RasPlan.get_plan_value(new_plan_number, \"Mapping Interval\")\n",
        "print(f\"Verified interval values:\")\n",
        "print(f\"- Computation Interval: {comp_interval}\")\n",
        "print(f\"- Mapping Interval: {mapping_interval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.931217Z",
          "iopub.status.busy": "2025-11-29T13:53:19.931034Z",
          "iopub.status.idle": "2025-11-29T13:53:19.938884Z",
          "shell.execute_reply": "2025-11-29T13:53:19.938470Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - INFO - Successfully updated run flags in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03 (flags modified: 4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated run flags for plan 03:\n",
            "- Geometry Preprocessor: True\n",
            "- Unsteady Flow Simulation: True\n",
            "- Post Processor: True\n",
            "- Floodplain Mapping: True\n"
          ]
        }
      ],
      "source": [
        "# 3. Update run flags\n",
        "RasPlan.update_run_flags(\n",
        "    new_plan_number,\n",
        "    geometry_preprocessor=True,   # Run the geometry preprocessor\n",
        "    unsteady_flow_simulation=True, # Run unsteady flow simulation\n",
        "    post_processor=True,          # Run post-processing\n",
        "    floodplain_mapping=True       # Generate floodplain mapping outputs\n",
        ")\n",
        "print(f\"\\nUpdated run flags for plan {new_plan_number}:\")\n",
        "print(f\"- Geometry Preprocessor: True\")\n",
        "print(f\"- Unsteady Flow Simulation: True\")\n",
        "print(f\"- Post Processor: True\")\n",
        "print(f\"- Floodplain Mapping: True\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.940761Z",
          "iopub.status.busy": "2025-11-29T13:53:19.940504Z",
          "iopub.status.idle": "2025-11-29T13:53:19.972549Z",
          "shell.execute_reply": "2025-11-29T13:53:19.972157Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - WARNING - Unknown key: Run UNet. Valid keys are: UNET Use Existing IB Tables, Plan Title, Description, Friction Slope Method, Run UNET, UNET D2 Name, Run RASMapper, UNET D1 Cores, Mapping Interval, DSS File, Run Sediment, UNET 1D Methodology, UNET D2 Cores, Run WQNET, Short Identifier, Run HTab, Run Post Process, Plan File, Simulation Date, Program Version, Computation Interval, Flow File, Geom File, PS Cores, UNET D2 Solver Type\n",
            " Add more keys and explanations in get_plan_value() as needed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasPlan - WARNING - No description found in plan file: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified run flag values:\n",
            "- Run HTab (Geometry Preprocessor): -1\n",
            "- Run UNet (Unsteady Flow): -1\n",
            "\n",
            "Updated description for plan 03\n",
            "Current plan description:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify the updates\n",
        "run_htab = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
        "run_unet = RasPlan.get_plan_value(new_plan_number, \"Run UNet\")\n",
        "print(f\"Verified run flag values:\")\n",
        "print(f\"- Run HTab (Geometry Preprocessor): {run_htab}\")\n",
        "print(f\"- Run UNet (Unsteady Flow): {run_unet}\")\n",
        "\n",
        "# 4. Update plan description\n",
        "new_description = \"Combined plan with modified geometry and unsteady flow\\nJanuary 2023 simulation\\n1-minute computation interval\\nGeometry and unsteady flow from cloned files\"\n",
        "RasPlan.update_plan_description(new_plan_number, new_description)\n",
        "print(f\"\\nUpdated description for plan {new_plan_number}\")\n",
        "\n",
        "# Read back the description\n",
        "current_description = RasPlan.read_plan_description(new_plan_number)\n",
        "print(f\"Current plan description:\\n{current_description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Plan\n",
        "\n",
        "Now that we have set up all the parameters, let's compute the plan using RasCmdr.compute_plan():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:53:19.974209Z",
          "iopub.status.busy": "2025-11-29T13:53:19.974060Z",
          "iopub.status.idle": "2025-11-29T13:54:57.166505Z",
          "shell.execute_reply": "2025-11-29T13:54:57.165921Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasGeo - INFO - Clearing geometry preprocessor file for single plan: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasGeo - WARNING - No geometry preprocessor file found for: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasGeo - INFO - Geometry dataframe updated successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasCmdr - INFO - Cleared geometry preprocessor files for plan: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:53:19 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.prj\" \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing plan 03...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.RasCmdr - INFO - Total run time for plan 03: 97.18 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan 03 computed successfully\n"
          ]
        }
      ],
      "source": [
        "# Compute the plan with our configured settings\n",
        "# Note: This may take several minutes depending on the complexity of the model\n",
        "print(f\"Computing plan {new_plan_number}...\")\n",
        "success = RasCmdr.compute_plan(new_plan_number, clear_geompre=True)\n",
        "\n",
        "if success:\n",
        "    print(f\"Plan {new_plan_number} computed successfully\")\n",
        "else:\n",
        "    print(f\"Failed to compute plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verifying Results\n",
        "\n",
        "After computation, we should check if results were written correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:54:57.168606Z",
          "iopub.status.busy": "2025-11-29T13:54:57.168402Z",
          "iopub.status.idle": "2025-11-29T13:54:57.184702Z",
          "shell.execute_reply": "2025-11-29T13:54:57.184130Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HDF entries for the project:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>unsteady_number</th>\\n', '      <th>geometry_number</th>\\n', '      <th>Plan Title</th>\\n', '      <th>Program Version</th>\\n', '      <th>Short Identifier</th>\\n', '      <th>Simulation Date</th>\\n', '      <th>Computation Interval</th>\\n', '      <th>Mapping Interval</th>\\n', '      <th>Run HTab</th>\\n', '      <th>...</th>\\n', '      <th>PS Cores</th>\\n', '      <th>DSS File</th>\\n', '      <th>Friction Slope Method</th>\\n', '      <th>description</th>\\n', '      <th>HDF_Results_Path</th>\\n', '      <th>Geom File</th>\\n', '      <th>Geom Path</th>\\n', '      <th>Flow File</th>\\n', '      <th>Flow Path</th>\\n', '      <th>full_path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>01</td>\\n', '      <td>02</td>\\n', '      <td>Unsteady with Bridges and Dam clonedplan</td>\\n', '      <td>5.00</td>\\n', '      <td>Combined Test Plan clone</td>\\n', '      <td>01JAN2023,0000,05JAN2023,2359</td>\\n', '      <td>1MIN</td>\\n', '      <td>30MIN</td>\\n', '      <td>-1</td>\\n', '      <td>...</td>\\n', '      <td>None</td>\\n', '      <td>dss</td>\\n', '      <td>2</td>\\n', '      <td>Combined plan with modified geometry and unste...</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number unsteady_number geometry_number  \\\n",
              "2          03              01              02   \n",
              "\n",
              "                                 Plan Title Program Version  \\\n",
              "2  Unsteady with Bridges and Dam clonedplan            5.00   \n",
              "\n",
              "           Short Identifier                Simulation Date  \\\n",
              "2  Combined Test Plan clone  01JAN2023,0000,05JAN2023,2359   \n",
              "\n",
              "  Computation Interval Mapping Interval Run HTab  ... PS Cores DSS File  \\\n",
              "2                 1MIN            30MIN       -1  ...     None      dss   \n",
              "\n",
              "  Friction Slope Method                                        description  \\\n",
              "2                     2  Combined plan with modified geometry and unste...   \n",
              "\n",
              "                                    HDF_Results_Path  Geom File  Geom Path  \\\n",
              "2  C:\\GH\\ras-commander\\examples\\example_projects\\...         02       None   \n",
              "\n",
              "  Flow File Flow Path                                          full_path  \n",
              "2        01      None  C:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
              "\n",
              "[1 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Plan 03 has a valid HDF results file:\n",
            "HDF Path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03.hdf\n",
            "\n",
            "All plan entries with their HDF paths:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>plan_number</th>\\n', '      <th>HDF_Results_Path</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>01</td>\\n', '      <td>None</td>\\n', '    </tr><tr>\\n', '      <th>1</th>\\n', '      <td>02</td>\\n', '      <td>None</td>\\n', '    </tr><tr>\\n', '      <th>2</th>\\n', '      <td>03</td>\\n', '      <td>C:\\\\GH\\\\ras-commander\\\\examples\\\\example_projects\\\\...</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "  plan_number                                   HDF_Results_Path\n",
              "0          01                                               None\n",
              "1          02                                               None\n",
              "2          03  C:\\GH\\ras-commander\\examples\\example_projects\\..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Refresh the plan entries to ensure we have the latest data\n",
        "ras.plan_df = ras.get_plan_entries()\n",
        "hdf_entries = ras.get_hdf_entries()\n",
        "\n",
        "if not hdf_entries.empty:\n",
        "    print(\"HDF entries for the project:\")\n",
        "    display.display(hdf_entries)\n",
        "    \n",
        "    # Check if our new plan has an HDF file\n",
        "    new_plan_hdf = hdf_entries[hdf_entries['plan_number'] == new_plan_number]\n",
        "    if not new_plan_hdf.empty:\n",
        "        print(f\"\\nPlan {new_plan_number} has a valid HDF results file:\")\n",
        "        print(f\"HDF Path: {new_plan_hdf.iloc[0]['HDF_Results_Path']}\")\n",
        "    else:\n",
        "        print(f\"\\nNo HDF entry found for plan {new_plan_number}\")\n",
        "else:\n",
        "    print(\"No HDF entries found. This could mean the plan hasn't been computed successfully or the results haven't been written yet.\")\n",
        "\n",
        "# Display all plan entries to see their HDF paths\n",
        "print(\"\\nAll plan entries with their HDF paths:\")\n",
        "plan_hdf_info = ras.plan_df[['plan_number', 'HDF_Results_Path']]\n",
        "display.display(plan_hdf_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the plan was computed successfully, we can examine the runtime data and volume accounting from the HDF results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-29T13:54:57.186832Z",
          "iopub.status.busy": "2025-11-29T13:54:57.186673Z",
          "iopub.status.idle": "2025-11-29T13:54:57.217478Z",
          "shell.execute_reply": "2025-11-29T13:54:57.216792Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03.hdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.HdfResultsPlan - INFO - Extracting Plan Information from: BaldEagle.p03.hdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.HdfResultsPlan - INFO - Plan Name: Unsteady with Bridges and Dam clonedplan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.HdfResultsPlan - INFO - Simulation Duration (hours): 119.98333333333333\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking computation runtime data...\n",
            "\n",
            "Simulation Runtime Statistics:\n"
          ]
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Plan Name</th>\\n', '      <th>File Name</th>\\n', '      <th>Simulation Start Time</th>\\n', '      <th>Simulation End Time</th>\\n', '      <th>Simulation Duration (s)</th>\\n', '      <th>Simulation Time (hr)</th>\\n', '      <th>Completing Geometry (hr)</th>\\n', '      <th>Preprocessing Geometry (hr)</th>\\n', '      <th>Completing Event Conditions (hr)</th>\\n', '      <th>Unsteady Flow Computations (hr)</th>\\n', '      <th>Complete Process (hr)</th>\\n', '      <th>Unsteady Flow Speed (hr/hr)</th>\\n', '      <th>Complete Process Speed (hr/hr)</th>\\n', '    </tr>\\n', '  </thead>\n  <tbody>\n    <tr>\\n', '      <th>0</th>\\n', '      <td>Unsteady with Bridges and Dam clonedplan</td>\\n', '      <td>BaldEagle.p03.hdf</td>\\n', '      <td>2023-01-01</td>\\n', '      <td>2023-01-05 23:59:00</td>\\n', '      <td>431940.0</td>\\n', '      <td>119.983333</td>\\n', '      <td>N/A</td>\\n', '      <td>0.022352</td>\\n', '      <td>N/A</td>\\n', '      <td>0.001228</td>\\n', '      <td>0.026133</td>\\n', '      <td>97679.782904</td>\\n', '      <td>4591.296584</td>\\n', '    </tr>\n    <tr><td colspan=\"100%\" style=\"text-align:center\">[... additional rows truncated ...]</td></tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": [
              "                                  Plan Name          File Name  \\\n",
              "0  Unsteady with Bridges and Dam clonedplan  BaldEagle.p03.hdf   \n",
              "\n",
              "  Simulation Start Time Simulation End Time  Simulation Duration (s)  \\\n",
              "0            2023-01-01 2023-01-05 23:59:00                 431940.0   \n",
              "\n",
              "   Simulation Time (hr) Completing Geometry (hr)  Preprocessing Geometry (hr)  \\\n",
              "0            119.983333                      N/A                     0.022352   \n",
              "\n",
              "  Completing Event Conditions (hr)  Unsteady Flow Computations (hr)  \\\n",
              "0                              N/A                         0.001228   \n",
              "\n",
              "   Complete Process (hr)  Unsteady Flow Speed (hr/hr)  \\\n",
              "0               0.026133                 97679.782904   \n",
              "\n",
              "   Complete Process Speed (hr/hr)  \n",
              "0                     4591.296584  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 08:54:57 - ras_commander.HdfResultsPlan - INFO - Final validated file path: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p03.hdf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simulation Duration: 431940.00 seconds\n",
            "Computation Time: 0.02613 hours\n",
            "Computation Speed: 4591.30 (simulation hours/compute hours)\n",
            "\n",
            "Checking volume accounting...\n"
          ]
        }
      ],
      "source": [
        "# Get computation runtime data from HDF\n",
        "print(\"Checking computation runtime data...\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(new_plan_number)\n",
        "\n",
        "if runtime_df is not None and not runtime_df.empty:\n",
        "    print(\"\\nSimulation Runtime Statistics:\")\n",
        "    display.display(runtime_df)\n",
        "    \n",
        "    # Extract key metrics\n",
        "    sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
        "    compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
        "    compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
        "    \n",
        "    print(f\"\\nSimulation Duration: {sim_duration:.2f} seconds\")\n",
        "    print(f\"Computation Time: {compute_time:.5f} hours\")\n",
        "    print(f\"Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
        "else:\n",
        "    print(\"No runtime data found. This may indicate the simulation didn't complete successfully.\")\n",
        "\n",
        "# Get volume accounting data\n",
        "print(\"\\nChecking volume accounting...\")\n",
        "volume_df = HdfResultsPlan.get_volume_accounting(new_plan_number)\n",
        "\n",
        "if volume_df is not None and not isinstance(volume_df, bool):\n",
        "    # Handle volume_df as a dictionary\n",
        "    if isinstance(volume_df, dict):\n",
        "        error_percent = volume_df.get('Error Percent')\n",
        "        if error_percent is not None:\n",
        "            print(f\"\\nFinal Volume Balance Error: {float(error_percent):.8f}%\")\n",
        "            \n",
        "        # Print other key statistics\n",
        "        print(\"\\nDetailed Volume Statistics:\")\n",
        "        print(f\"Volume Starting: {float(volume_df['Volume Starting']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Volume Ending: {float(volume_df['Volume Ending']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Total Inflow: {float(volume_df['Total Boundary Flux of Water In']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Total Outflow: {float(volume_df['Total Boundary Flux of Water Out']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "else:\n",
        "    print(\"No volume accounting data found. This may indicate the simulation didn't complete successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan and Geometry Operations\n",
        "\n",
        "In this notebook, we've covered a comprehensive range of operations on HEC-RAS plan and geometry files using the RAS Commander library:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
        "2. **Plan Operations**:\n",
        "   - Created a new plan by cloning an existing one\n",
        "   - Updated simulation parameters (dates, intervals, etc.)\n",
        "   - Set run flags for different components\n",
        "   - Updated the plan description\n",
        "3. **Geometry Operations**:\n",
        "   - Created a new geometry by cloning an existing one\n",
        "   - Associated the new geometry with our plan\n",
        "   - Cleared geometry preprocessor files\n",
        "4. **Unsteady Flow Operations**:\n",
        "   - Created a new unsteady flow file by cloning an existing one\n",
        "   - Associated it with our plan\n",
        "5. **Computation and Verification**:\n",
        "   - Computed our plan with the specified settings\n",
        "   - Verified the results using HDF entries\n",
        "   - Analyzed runtime statistics and volume accounting\n",
        "\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasPlan`: For plan operations (cloning, setting components, and modifying parameters)\n",
        "- `RasGeo`: For geometry operations (cloning, clearing preprocessor files)\n",
        "- `RasCmdr`: For executing HEC-RAS simulations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further enhance your HEC-RAS automation, consider exploring:\n",
        "\n",
        "1. **Parameter Sweeps**: Create and run multiple plans with varying parameters\n",
        "2. **Parallel Computations**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
        "3. **Advanced Results Analysis**: Use the HDF classes to extract and analyze specific model results\n",
        "4. **Spatial Visualization**: Create maps and plots of simulation results\n",
        "5. **Model Calibration**: Automate comparison between model results and observations\n",
        "\n",
        "The RAS Commander library provides a powerful framework for automating and streamlining your HEC-RAS workflows, enabling more efficient hydraulic modeling and analyses."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rascmdr_local",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: C:\GH\ras-commander\troubleshooting\execute_notebook.py
==================================================
"""
Execute notebook and extract key information about unsteady_df columns.
"""
import sys
import json
from pathlib import Path

# Try importing required packages
try:
    import nbformat
    from nbconvert.preprocessors import ExecutePreprocessor
except ImportError:
    print("ERROR: nbconvert not found. Installing...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "nbconvert"])
    import nbformat
    from nbconvert.preprocessors import ExecutePreprocessor

# Paths
notebook_path = Path(r"C:\GH\ras-commander\examples\02_plan_and_geometry_operations.ipynb")
output_path = Path(r"C:\GH\ras-commander\troubleshooting\executed_02.ipynb")
results_path = Path(r"C:\GH\ras-commander\troubleshooting\execution_results.json")

print(f"Loading notebook: {notebook_path}")
with open(notebook_path, 'r', encoding='utf-8') as f:
    nb = nbformat.read(f, as_version=4)

print("Executing notebook...")
ep = ExecutePreprocessor(timeout=600, kernel_name='python3')

try:
    ep.preprocess(nb, {'metadata': {'path': notebook_path.parent}})
    print("✓ Notebook executed successfully!")
    execution_success = True
    error_message = None
except Exception as e:
    print(f"✗ Notebook execution failed: {e}")
    execution_success = False
    error_message = str(e)

# Save executed notebook
print(f"Saving executed notebook to: {output_path}")
with open(output_path, 'w', encoding='utf-8') as f:
    nbformat.write(nb, f)

# Extract information about unsteady_df
unsteady_df_info = {
    "found": False,
    "columns": None,
    "has_geometry_number": None,
    "cell_index": None,
    "output_text": None
}

print("\nSearching for unsteady_df output...")
for i, cell in enumerate(nb.cells):
    if cell.cell_type == 'code':
        # Look for cells that display unsteady_df
        if 'unsteady_df' in cell.source or 'ras.unsteady_df' in cell.source:
            if cell.get('outputs'):
                for output in cell.outputs:
                    output_text = None

                    # Check different output types
                    if output.output_type == 'execute_result' and 'text/plain' in output.data:
                        output_text = output.data['text/plain']
                    elif output.output_type == 'stream':
                        output_text = output.text
                    elif output.output_type == 'display_data' and 'text/plain' in output.data:
                        output_text = output.data['text/plain']

                    if output_text:
                        # Look for column information in DataFrame repr
                        if 'unsteady_file' in output_text or 'boundary_file' in output_text:
                            unsteady_df_info["found"] = True
                            unsteady_df_info["cell_index"] = i
                            unsteady_df_info["output_text"] = output_text
                            unsteady_df_info["has_geometry_number"] = "geometry_number" in output_text

                            # Try to extract column names
                            lines = output_text.split('\n')
                            for line in lines:
                                if 'Index' in line or 'Columns' in line:
                                    unsteady_df_info["columns"] = line

                            print(f"✓ Found unsteady_df output in cell {i}")
                            print(f"  Has 'geometry_number' column: {unsteady_df_info['has_geometry_number']}")
                            break

    if unsteady_df_info["found"]:
        break

# Compile results
results = {
    "execution_success": execution_success,
    "error_message": error_message,
    "unsteady_df_info": unsteady_df_info
}

# Save results to JSON
print(f"\nSaving results to: {results_path}")
with open(results_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2)

# Print summary
print("\n" + "="*60)
print("EXECUTION SUMMARY")
print("="*60)
print(f"Notebook executed: {'✓ YES' if execution_success else '✗ NO'}")
if error_message:
    print(f"Error: {error_message}")
print(f"\nunsteady_df found: {'✓ YES' if unsteady_df_info['found'] else '✗ NO'}")
if unsteady_df_info["found"]:
    print(f"Has 'geometry_number' column: {'✗ YES (UNEXPECTED!)' if unsteady_df_info['has_geometry_number'] else '✓ NO (EXPECTED)'}")
    if unsteady_df_info["output_text"]:
        print(f"\nDataFrame output preview (first 500 chars):")
        print("-" * 60)
        print(unsteady_df_info["output_text"][:500])
        print("-" * 60)
print("="*60)

==================================================

File: C:\GH\ras-commander\troubleshooting\execution_results.json
==================================================
{
  "execution_success": true,
  "error_message": null,
  "unsteady_df_info": {
    "found": false,
    "columns": null,
    "has_geometry_number": null,
    "cell_index": null,
    "output_text": null
  }
}
==================================================

File: C:\GH\ras-commander\troubleshooting\TCC_road.u01
==================================================
Flow Title=T100+10%
Program Version=6.50
Use Restart= 0 
Boundary Location=                ,                ,        ,        ,                ,Perimeter 1     ,                ,Boundary_cond                   ,                                
Friction Slope=0,0
Boundary Location=                ,                ,        ,        ,                ,Perimeter 1     ,                ,                                ,                                
Interval=6MIN
Precipitation Hydrograph= 240 
 .144519 .144519 .147381 .147381 .150242 .150242 .153104 .153104 .155966 .155966
 .158828 .158828 .161689 .161689 .164551 .164551 .167413 .167413 .170275 .170275
 .173136 .173136 .175998 .175998  .17886  .17886 .181722 .181722 .184584 .184584
 .187445 .187445 .190307 .190307 .193169 .193169 .196031 .196031 .198892 .198892
 .201754 .204616 .207478 .210339 .213201 .216063 .218925 .221786 .224648  .22751
 .230372 .233233 .236095 .238957 .241819  .24468 .247542 .250404 .253266 .256128
 .258989 .261851 .264713 .267575 .270436 .273298  .27616 .279022 .281883 .284745
 .287607 .290469  .29333 .296192 .299054 .301916 .304777 .307639 .310501 .313363
 .321948 .336257 .350566 .364874 .379183 .393492 .407801  .42211 .436418 .450727
 .457882 .457882 .457882 .457882 .457882 .469329 .492223 .515117 .538011 .560905
 .589523 .623864 .658205 .692546 .726887 .772675  .82991 .887146 .9443811.001616
1.098916 1.236281.3736451.5110091.6483743.4112186.79954210.9362219.6130713.61339
2.7129482.3867082.0604671.7342271.4079861.2076631.1332571.058851 .984445  .91004
 .852804  .81274 .772675 .732611 .692546 .658205 .629587  .60097 .572352 .543734
 .525133 .513686 .505101 .493654 .485068 .473621 .465036 .453589 .445004 .433557
 .424971 .413524 .404939 .393492 .384907  .37346 .364874 .353427 .344842 .333395
 .327672 .323379 .320517 .316224 .313363  .30907 .306208 .301916 .299054 .294761
   .2919 .287607 .284745 .280452 .277591 .273298 .270436 .266144 .263282 .258989
 .256128 .251835 .248973  .24468 .241819 .237526 .234664 .230372  .22751 .223217
 .220356 .216063 .213201 .208908 .206047 .201754 .198892   .1946 .191738 .187445
 .186014 .184584 .184584 .183153 .183153 .181722 .181722 .180291 .180291  .17886
  .17886 .177429 .177429 .175998 .175998 .174567 .174567 .173136 .173136 .171706
 .171706 .170275 .170275 .168844 .168844 .167413 .167413 .165982 .165982 .164551
 .164551  .16312  .16312 .161689 .161689 .160259 .160259 .158828 .158828 .157397
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,                ,Perimeter 1     ,                ,BC Line 1                       ,                                
Friction Slope=0.01,0
Boundary Location=                ,                ,        ,        ,                ,Perimeter 1     ,                ,BC Line 2                       ,                                
Friction Slope=0,0
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,Boundary_cond                   ,                                
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,BC Line 1                       ,                                
Friction Slope=0.0075,0
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,BC Line 2                       ,                                
Friction Slope=0.004,0
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,port_boundary                   ,                                
Interval=1DAY
Stage Hydrograph= 2 
       0       0
Stage Hydrograph Use Initial Stage=-1
Stage Hydrograph TW Check=0
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,                                ,                                
Interval=6MIN
Precipitation Hydrograph= 240 
 .144519 .144519 .147381 .147381 .150242 .150242 .153104 .153104 .155966 .155966
 .158828 .158828 .161689 .161689 .164551 .164551 .167413 .167413 .170275 .170275
 .173136 .173136 .175998 .175998  .17886  .17886 .181722 .181722 .184584 .184584
 .187445 .187445 .190307 .190307 .193169 .193169 .196031 .196031 .198892 .198892
 .201754 .204616 .207478 .210339 .213201 .216063 .218925 .221786 .224648  .22751
 .230372 .233233 .236095 .238957 .241819  .24468 .247542 .250404 .253266 .256128
 .258989 .261851 .264713 .267575 .270436 .273298  .27616 .279022 .281883 .284745
 .287607 .290469  .29333 .296192 .299054 .301916 .304777 .307639 .310501 .313363
 .321948 .336257 .350566 .364874 .379183 .393492 .407801  .42211 .436418 .450727
 .457882 .457882 .457882 .457882 .457882 .469329 .492223 .515117 .538011 .560905
 .589523 .623864 .658205 .692546 .726887 .772675  .82991 .887146 .9443811.001616
1.098916 1.236281.3736451.5110091.6483743.4112186.79954210.9362219.6130713.61339
2.7129482.3867082.0604671.7342271.4079861.2076631.1332571.058851 .984445  .91004
 .852804  .81274 .772675 .732611 .692546 .658205 .629587  .60097 .572352 .543734
 .525133 .513686 .505101 .493654 .485068 .473621 .465036 .453589 .445004 .433557
 .424971 .413524 .404939 .393492 .384907  .37346 .364874 .353427 .344842 .333395
 .327672 .323379 .320517 .316224 .313363  .30907 .306208 .301916 .299054 .294761
   .2919 .287607 .284745 .280452 .277591 .273298 .270436 .266144 .263282 .258989
 .256128 .251835 .248973  .24468 .241819 .237526 .234664 .230372  .22751 .223217
 .220356 .216063 .213201 .208908 .206047 .201754 .198892   .1946 .191738 .187445
 .186014 .184584 .184584 .183153 .183153 .181722 .181722 .180291 .180291  .17886
  .17886 .177429 .177429 .175998 .175998 .174567 .174567 .173136 .173136 .171706
 .171706 .170275 .170275 .168844 .168844 .167413 .167413 .165982 .165982 .164551
 .164551  .16312  .16312 .161689 .161689 .160259 .160259 .158828 .158828 .157397
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,sea_outlet                      ,                                
Interval=1WEEK
Stage Hydrograph= 2 
       0       0
Stage Hydrograph Use Initial Stage=-1
Stage Hydrograph TW Check=0
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,outlet                          ,                                
Friction Slope=0.002,0
Boundary Location=                ,                ,        ,        ,                ,Perimeter 2     ,                ,Inflet                          ,                                
Interval=1HOUR
Flow Hydrograph= 28 
       1   4.667   8.333      12  15.667  19.333      23  26.667  30.333      34
  37.667  41.333      45  48.667  52.333      56  59.667  63.333      67  70.667
  74.333      78  81.667  85.333      89  92.667  96.333     100
Stage Hydrograph TW Check=0
Flow Hydrograph Slope= 0.003 
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Met Point Raster Parameters=,,,,
Precipitation Mode=Disable
Wind Mode=No Wind Forces
Air Density Mode=
Met BC=Precipitation|Expanded View=0
Met BC=Precipitation|Point Interpolation=Nearest
Met BC=Precipitation|Gridded Source=DSS
Met BC=Evapotranspiration|Expanded View=0
Met BC=Evapotranspiration|Point Interpolation=Nearest
Met BC=Evapotranspiration|Gridded Source=DSS
Met BC=Wind Speed|Expanded View=0
Met BC=Wind Speed|Point Interpolation=Nearest
Met BC=Wind Speed|Gridded Source=DSS
Met BC=Wind Direction|Expanded View=0
Met BC=Wind Direction|Point Interpolation=Nearest
Met BC=Wind Direction|Gridded Source=DSS
Met BC=Wind Velocity X|Expanded View=0
Met BC=Wind Velocity X|Point Interpolation=Nearest
Met BC=Wind Velocity X|Gridded Source=DSS
Met BC=Wind Velocity Y|Expanded View=0
Met BC=Wind Velocity Y|Point Interpolation=Nearest
Met BC=Wind Velocity Y|Gridded Source=DSS
Met BC=Air Density|Mode=Constant
Met BC=Air Density|Expanded View=0
Met BC=Air Density|Constant Value=1.225
Met BC=Air Density|Constant Units=kg/m3
Met BC=Air Density|Point Interpolation=Nearest
Met BC=Air Density|Gridded Source=DSS
Met BC=Air Temperature|Expanded View=0
Met BC=Air Temperature|Point Interpolation=Nearest
Met BC=Air Temperature|Gridded Source=DSS
Met BC=Humidity|Expanded View=0
Met BC=Humidity|Point Interpolation=Nearest
Met BC=Humidity|Gridded Source=DSS
Met BC=Air Pressure|Mode=Constant
Met BC=Air Pressure|Expanded View=0
Met BC=Air Pressure|Constant Value=1013.2
Met BC=Air Pressure|Constant Units=mb
Met BC=Air Pressure|Point Interpolation=Inv Distance
Met BC=Air Pressure|Gridded Source=DSS
Non-Newtonian Method= 0 ,      
Non-Newtonian Constant Vol Conc=0
Non-Newtonian Yield Method= 0 ,      
Non-Newtonian Yield Coef=0, 0
User Yeild=   0
Non-Newtonian Sed Visc= 0 ,      
Non-Newtonian Obrian B=0
User Viscosity=0
User Viscosity Ratio=0
Herschel-Bulkley Coef=0, 0
Clastic Method= 0 ,      
Coulomb Phi=0
Voellmy X=0
Non-Newtonian Hindered FV= 0 
Non-Newtonian FV K=0
Non-Newtonian ds=0
Non-Newtonian Max Cv=0
Non-Newtonian Bulking Method= 0 , 
Non-Newtonian High C Transport= 0 , 
Lava Activation= 0 
Temperature=1300,15,,15,14,980
Heat Ballance=1,1200,0.5,1,70,0.95
Viscosity=1000,,,
Yield Strength=,,,
Consistency Factor=,,,
Profile Coefficient=4,1.3,
Lava Param=,2500,

==================================================

File: C:\GH\ras-commander\troubleshooting\test_precip_hydrograph.py
==================================================
"""
Test script to verify Precipitation Hydrograph parsing works correctly.
"""
import sys
from pathlib import Path

# Add parent directory to path for local import
sys.path.insert(0, str(Path(__file__).parent.parent))

from ras_commander.RasUnsteady import RasUnsteady

# Test file path
test_file = Path(__file__).parent / "TCC_road.u01"

print(f"Testing file: {test_file}")
print(f"File exists: {test_file.exists()}")
print("=" * 60)

# Read the file lines for identify_tables
with open(test_file, 'r') as f:
    lines = f.readlines()

# Test 1: identify_tables - should find Precipitation Hydrograph
print("\n1. Testing identify_tables():")
print("-" * 40)
tables = RasUnsteady.identify_tables(lines)
print(f"Found {len(tables)} tables:")
for table_name, start, end in tables:
    print(f"  - {table_name} (lines {start}-{end})")

# Check if Precipitation Hydrograph is found
precip_tables = [t for t in tables if 'Precipitation' in t[0]]
print(f"\nPrecipitation Hydrograph tables found: {len(precip_tables)}")

# Test 2: Check bc_types in _parse_boundary_condition
print("\n2. Testing bc_types (checking code):")
print("-" * 40)
bc_types = {
    'Flow Hydrograph=': 'Flow Hydrograph',
    'Lateral Inflow Hydrograph=': 'Lateral Inflow Hydrograph',
    'Uniform Lateral Inflow Hydrograph=': 'Uniform Lateral Inflow Hydrograph',
    'Stage Hydrograph=': 'Stage Hydrograph',
    'Precipitation Hydrograph=': 'Precipitation Hydrograph',
    'Rating Curve=': 'Rating Curve',
    'Friction Slope=': 'Normal Depth',
    'Gate Name=': 'Gate Opening'
}
print("Current bc_types includes:")
for key in bc_types:
    print(f"  - {key}")

# Test 3: Parse first few values of a precipitation hydrograph
print("\n3. Testing value parsing:")
print("-" * 40)
if precip_tables:
    table_name, start, end = precip_tables[0]
    values = RasUnsteady.parse_fixed_width_table(lines, start, end)
    print(f"First precipitation hydrograph:")
    print(f"  Number of values: {len(values)}")
    print(f"  First 10 values: {values['Value'].head(10).tolist()}")
    print(f"  Last 5 values: {values['Value'].tail(5).tolist()}")

print("\n" + "=" * 60)
print("TEST COMPLETE")

==================================================

File: C:\GH\ras-commander\troubleshooting\VERIFICATION_SUMMARY.md
==================================================
# Notebook Execution Verification Summary

## Test Objective
Verify that the `geometry_number` column has been successfully removed from `unsteady_df` after the RasPrj changes.

## Execution Details
- **Notebook**: `C:\GH\ras-commander\examples\02_plan_and_geometry_operations.ipynb`
- **Environment**: `rascmdr_local` conda environment
- **Execution Date**: 2025-11-29 08:53:19
- **Execution Status**: ✅ **SUCCESS**

## Key Findings

### 1. Notebook Execution Status
✅ **The notebook executed successfully without errors**

### 2. unsteady_df Column Verification
✅ **The `geometry_number` column is NO LONGER PRESENT in unsteady_df**

### 3. Columns Present in unsteady_df
The following columns are now present in `ras.unsteady_df`:
- `unsteady_number`
- `full_path`
- `Flow Title`
- `Program Version`
- `Use Restart`
- `Precipitation Mode`
- `Wind Mode`
- `Met BC=Precipitation|Mode`
- `Met BC=Evapotranspiration|Mode`
- `Met BC=Precipitation|Expanded View`
- `Met BC=Precipitation|Constant Units`
- `Met BC=Precipitation|Gridded Source`

### 4. DataFrame Output (Cell 20)
```
  unsteady_number                                          full_path  \
0              02  C:\GH\ras-commander\examples\example_projects\...
1              01  C:\GH\ras-commander\examples\example_projects\...

          Flow Title Program Version Use Restart Precipitation Mode  \
0  Flow Hydrograph 2            6.30           0            Disable
1  Flow Hydrograph 2            6.30           0            Disable

        Wind Mode Met BC=Precipitation|Mode Met BC=Evapotranspiration|Mode  \
0  No Wind Forces                      None                           None
1  No Wind Forces                      None                           None

  Met BC=Precipitation|Expanded View Met BC=Precipitation|Constant Units  \
0                                  0                               mm/hr
1                                  0                               mm/hr

  Met BC=Precipitation|Gridded Source
0                                 DSS
1                                 DSS
```

## Verification Result
✅ **PASSED** - The `geometry_number` column has been successfully removed from `unsteady_df`.

The change made to `RasPrj._process_default_entry()` method (removing the `geometry_number` assignment) is working correctly. The unsteady flow DataFrame no longer includes this column, which was the intended outcome.

## Additional Notes
- All 41 notebook cells executed successfully
- No errors or warnings related to the column removal
- The project initialization and all operations completed as expected
- Runtime for full notebook execution: ~1.5 minutes

==================================================

File: C:\GH\ras-commander\ras_commander\dss\AGENTS.md
==================================================
# DSS Subpackage - Developer Guidance

This document provides guidance for AI agents and developers working with the `ras_commander.dss` subpackage.

## Overview

The DSS subpackage provides HEC-DSS file operations using HEC Monolith Java libraries via pyjnius. All dependencies are lazy-loaded to minimize import overhead.

## Module Structure

```
ras_commander/dss/
├── __init__.py          # Public API exports
├── AGENTS.md            # This file
├── RasDss.py            # Main DSS class with static methods
└── _hec_monolith.py     # HEC Monolith library downloader (private)
```

## Lazy Loading Architecture

### Three-Level Lazy Loading

1. **Parent Package Level** (`ras_commander/__init__.py`):
   - Uses `__getattr__` to defer `dss` subpackage import
   - `from ras_commander import RasDss` triggers lazy load
   - Users who don't use DSS never load the subpackage

2. **Subpackage Level** (`dss/__init__.py`):
   - Imports `RasDss` class eagerly (lightweight)
   - No Java/pyjnius imports at this level

3. **Method Level** (`RasDss.py`):
   - `_configure_jvm()` called on first DSS method use
   - `jnius_config` imported only when needed
   - `jnius.autoclass` imported only after JVM configured
   - HEC Monolith downloaded automatically on first use

### Import Timeline

```
import ras_commander              # Fast - no DSS code loaded
from ras_commander import RasDss  # Loads dss/__init__.py (fast)
RasDss.get_catalog("file.dss")    # Loads pyjnius, starts JVM, downloads Monolith
```

## Public API

### RasDss Class (Static Methods)

| Method | Purpose | Returns |
|--------|---------|---------|
| `get_catalog(dss_file)` | List all paths in DSS file | `List[str]` |
| `read_timeseries(dss_file, pathname)` | Read single time series | `DataFrame` |
| `read_multiple_timeseries(dss_file, pathnames)` | Read multiple time series | `Dict[str, DataFrame]` |
| `get_info(dss_file)` | Get file metadata | `Dict` |
| `extract_boundary_timeseries(boundaries_df, ...)` | Extract BC data | `DataFrame` |
| `shutdown_jvm()` | Placeholder (no-op) | `None` |

### DataFrame Metadata

Time series DataFrames include metadata in `df.attrs`:
- `pathname`: Original DSS path
- `units`: Data units (e.g., "CFS")
- `type`: Data type (e.g., "INST-VAL")
- `interval`: Data interval in minutes
- `dss_file`: Absolute path to source file

## Dependencies

### Required (Lazy Loaded)
- **pyjnius**: `pip install pyjnius`
- **Java JRE/JDK 8+**: Must be installed, JAVA_HOME set

### Auto-Downloaded
- **HEC Monolith** (~20 MB): Downloaded to `~/.ras-commander/dss/`
  - 7 JAR files from HEC Nexus/Maven
  - Platform-specific native library (javaHeclib.dll/.so/.dylib)

### Always Available
- `pandas`, `numpy`: Core dependencies
- `requests`, `tqdm`: For downloading Monolith

## Adding New DSS Methods

When adding new methods to `RasDss`:

1. **Configure JVM First**:
   ```python
   @staticmethod
   @log_call
   def new_method(dss_file: Union[str, Path]) -> ...:
       # Must be called before any jnius imports
       RasDss._configure_jvm()

       # Now safe to import Java classes
       from jnius import autoclass
       HecDss = autoclass('hec.heclib.dss.HecDss')
       ...
   ```

2. **Use Decorator**: Always use `@log_call` for traceability

3. **Handle Cleanup**: Use try/finally to close DSS files:
   ```python
   dss = HecDss.open(dss_file)
   try:
       # operations
   finally:
       dss.done()
   ```

4. **Path Resolution**: Always resolve to absolute paths:
   ```python
   dss_file = str(Path(dss_file).resolve())
   ```

## Testing

Run module directly for basic test:
```bash
python -m ras_commander.dss.RasDss
```

This tests:
- JVM configuration
- Monolith download (if needed)
- Catalog reading
- Time series extraction

## Common Issues

### Java Not Found
- Set `JAVA_HOME` environment variable
- Or install Java and ensure it's in PATH

### pyjnius Import Error
- Install with: `pip install pyjnius`
- Ensure JAVA_HOME is set before importing pyjnius

### JVM Already Started
- JVM can only be configured once per process
- If using in notebooks, restart kernel to reconfigure

### DSS File Not Found
- Use absolute paths or resolve relative to project directory
- Check file exists before calling DSS methods

## Integration with Parent Package

### Import Pattern
```python
# Preferred - lazy loads DSS only when used
from ras_commander import RasDss

# Also works - direct import
from ras_commander.dss import RasDss
```

### With RasPrj
```python
from ras_commander import init_ras_project, RasDss

ras = init_ras_project("project_path", "6.6")
enhanced = RasDss.extract_boundary_timeseries(
    ras.boundaries_df,
    ras_object=ras
)
```

## Version History

- **v0.82.0**: Initial RasDss implementation
- **v0.86.0**: Moved to `dss/` subpackage with lazy loading

==================================================

File: C:\GH\ras-commander\ras_commander\dss\RasDss.py
==================================================
"""
RasDss - DSS File Operations for ras-commander

Summary:
    Provides static methods for interacting with HEC-DSS files (versions 6 and 7),
    enabling reading of time series, extracting catalogs, extracting boundary time
    series, and fetching file metadata, all using HEC Monolith libraries accessed
    via pyjnius. JVM setup and dependency downloads are handled automatically at
    runtime.

Functions:
    _ensure_monolith():
        Ensures HEC Monolith Java libraries are installed (downloads if needed).
    _configure_jvm():
        Configures the JVM and sets classpath/library paths for pyjnius.
    get_catalog(dss_file):
        Returns a list of all data pathnames in a DSS file.
    read_timeseries(dss_file, pathname, start_date=None, end_date=None):
        Reads a DSS time series by pathname and returns it as a pandas DataFrame.
    read_multiple_timeseries(dss_file, pathnames):
        Reads multiple DSS time series, returning a dict of pathname to DataFrame
        (or None on failure).
    get_info(dss_file):
        Returns summary information and statistics for a DSS file, including
        partial catalog.
    extract_boundary_timeseries(boundaries_df, project_dir=None, ras_object=None):
        Extracts DSS time series for DSS-defined boundary conditions in a
        DataFrame and appends results as a new column.
    shutdown_jvm():
        Placeholder for JVM lifecycle management (not typically required with
        pyjnius).

Lazy Loading:
    This module implements lazy loading for all heavy dependencies:
    - pyjnius: Only imported when DSS methods are actually called
    - jnius_config: Only imported during JVM configuration
    - HecMonolithDownloader: Only imported when ensuring monolith installation
    - Java classes: Only loaded after JVM is configured

    This ensures that importing RasDss has minimal overhead and users who don't
    use DSS functionality don't pay the cost of loading Java/pyjnius.
"""

import sys
import os
from pathlib import Path
from typing import List, Dict, Optional, Union
import logging

# Lazy imports - these are always needed for type hints and basic operations
import pandas as pd
import numpy as np

# Import decorator from parent package
from ..Decorators import log_call

logger = logging.getLogger(__name__)


class RasDss:
    """
    Static class for DSS file operations.

    Uses HEC Monolith libraries (auto-downloaded on first use).
    Supports both DSS V6 and V7 formats.

    All heavy dependencies (pyjnius, Java) are lazy-loaded on first use.

    Usage:
        from ras_commander import RasDss

        # Read time series
        df = RasDss.read_timeseries("file.dss", "/BASIN/LOC/FLOW//1HOUR/OBS/")

        # Get catalog
        paths = RasDss.get_catalog("file.dss")
    """

    _jvm_configured = False
    _monolith = None

    @staticmethod
    def _ensure_monolith():
        """Ensure HEC Monolith is downloaded and available."""
        if RasDss._monolith is not None:
            return RasDss._monolith

        # Lazy import from same subpackage
        from ._hec_monolith import HecMonolithDownloader

        RasDss._monolith = HecMonolithDownloader()

        if not RasDss._monolith.is_installed():
            print("\n" + "="*80)
            print("HEC Monolith libraries not found")
            print("Installing automatically (one-time download, ~20 MB)...")
            print("="*80)
            RasDss._monolith.install()

        return RasDss._monolith

    @staticmethod
    def _configure_jvm():
        """Configure JVM classpath for pyjnius (must be done before first import)."""
        if RasDss._jvm_configured:
            return

        # Ensure monolith is installed
        monolith = RasDss._ensure_monolith()

        # Lazy import pyjnius config
        try:
            import jnius_config
        except ImportError:
            raise ImportError(
                "pyjnius is required for DSS file operations.\n"
                "Install with: pip install pyjnius"
            )

        # Check if JVM already started
        try:
            from jnius import autoclass
            # If this succeeds, JVM already started
            RasDss._jvm_configured = True
            return
        except:
            pass

        # Get classpath and library path
        classpath = monolith.get_classpath()
        library_path = monolith.get_library_path()

        print("Configuring Java VM for DSS operations...")

        # Set JAVA_HOME if not already set
        if 'JAVA_HOME' not in os.environ:
            # Try to find Java
            java_candidates = [
                Path("C:/Program Files/Java/jre1.8.0_471"),
                Path("C:/Program Files/Java/jdk-11"),
                Path("C:/Program Files/Java/jdk-17"),
                Path("C:/Program Files (x86)/Java/jre1.8.0_471"),
            ]
            for java_home in java_candidates:
                if java_home.exists():
                    os.environ['JAVA_HOME'] = str(java_home)
                    print(f"  Found Java: {java_home}")
                    break
            else:
                raise RuntimeError(
                    "Java not found. Please set JAVA_HOME environment variable "
                    "or install Java JDK/JRE.\n"
                    "Download from: https://www.oracle.com/java/technologies/downloads/"
                )

        # Set classpath (must be done before first import from jnius)
        jnius_config.add_classpath(*classpath)

        # Set library path for native libraries
        if 'LD_LIBRARY_PATH' in os.environ:
            os.environ['LD_LIBRARY_PATH'] = (
                library_path + ':' + os.environ['LD_LIBRARY_PATH']
            )
        else:
            os.environ['LD_LIBRARY_PATH'] = library_path

        # Windows: Add to PATH for native DLLs
        if os.name == 'nt':
            os.environ['PATH'] = (
                library_path + os.pathsep + os.environ.get('PATH', '')
            )

        RasDss._jvm_configured = True
        print("[OK] Java VM configured")

    @staticmethod
    @log_call
    def get_catalog(dss_file: Union[str, Path]) -> List[str]:
        """
        Get list of all data paths in DSS file.

        Args:
            dss_file: Path to DSS file

        Returns:
            List of DSS path strings

        Example:
            paths = RasDss.get_catalog("sample.dss")
            for path in paths:
                print(path)
        """
        # Configure JVM (must be before first jnius import)
        RasDss._configure_jvm()

        # Import Java classes via pyjnius (lazy)
        from jnius import autoclass

        HecDss = autoclass('hec.heclib.dss.HecDss')

        dss_file = str(Path(dss_file).resolve())

        # Open DSS file
        dss = HecDss.open(dss_file)

        try:
            # Get catalog (returns Java Vector of pathname strings)
            catalog_vector = dss.getCatalogedPathnames()

            # Convert Java Vector to Python list
            paths = []
            for i in range(catalog_vector.size()):
                paths.append(str(catalog_vector.get(i)))

            return paths

        finally:
            dss.done()

    @staticmethod
    @log_call
    def read_timeseries(
        dss_file: Union[str, Path],
        pathname: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Read time series from DSS file.

        Args:
            dss_file: Path to DSS file
            pathname: DSS pathname (e.g., "/BASIN/LOC/FLOW//1HOUR/OBS/")
            start_date: Optional start date filter
            end_date: Optional end date filter

        Returns:
            pandas DataFrame with DatetimeIndex and 'value' column

        Example:
            df = RasDss.read_timeseries("file.dss", "/BASIN/LOC/FLOW//1HOUR/OBS/")
            print(df.head())
        """
        # Configure JVM (must be before first jnius import)
        RasDss._configure_jvm()

        # Import Java classes via pyjnius (lazy)
        from jnius import autoclass, cast

        HecDss = autoclass('hec.heclib.dss.HecDss')
        TimeSeriesContainer = autoclass('hec.io.TimeSeriesContainer')

        dss_file = str(Path(dss_file).resolve())

        # Open DSS file
        dss = HecDss.open(dss_file)

        try:
            # Read time series
            # True = ignore D-part (date) for wildcards
            container = dss.get(pathname, True)

            if container is None:
                raise ValueError(f"No data found for pathname: {pathname}")

            # Cast to TimeSeriesContainer to access fields
            tsc = cast('hec.io.TimeSeriesContainer', container)

            # Extract values and times from Java container
            # pyjnius automatically converts Java arrays to Python lists
            values = np.array(tsc.values)  # Java double[] -> numpy array
            times = np.array(tsc.times)    # Java int[] -> numpy array (minutes since 1899-12-31)

            # Convert HEC time to numpy datetime64
            # HEC epoch: December 31, 1899 00:00
            HEC_EPOCH = np.datetime64('1899-12-31T00:00:00')
            datetimes = HEC_EPOCH + times.astype('timedelta64[m]')

            # Create DataFrame
            df = pd.DataFrame({
                'value': values
            }, index=pd.DatetimeIndex(datetimes, name='datetime'))

            # Add metadata as attributes
            df.attrs['pathname'] = pathname
            df.attrs['units'] = str(tsc.units) if tsc.units else ""
            df.attrs['type'] = str(tsc.type) if tsc.type else ""
            df.attrs['interval'] = (
                int(tsc.interval) if hasattr(tsc, 'interval') else None
            )
            df.attrs['dss_file'] = dss_file

            return df

        finally:
            dss.done()

    @staticmethod
    @log_call
    def read_multiple_timeseries(
        dss_file: Union[str, Path],
        pathnames: List[str]
    ) -> Dict[str, pd.DataFrame]:
        """
        Read multiple time series from DSS file.

        Args:
            dss_file: Path to DSS file
            pathnames: List of DSS pathnames

        Returns:
            Dictionary mapping pathnames to DataFrames

        Example:
            paths = ["/BASIN/LOC1/FLOW//1HOUR/OBS/", "/BASIN/LOC2/FLOW//1HOUR/OBS/"]
            data = RasDss.read_multiple_timeseries("file.dss", paths)
            for path, df in data.items():
                print(f"{path}: {len(df)} points")
        """
        results = {}
        for pathname in pathnames:
            try:
                results[pathname] = RasDss.read_timeseries(dss_file, pathname)
            except Exception as e:
                print(f"Warning: Could not read {pathname}: {e}")
                results[pathname] = None

        return results

    @staticmethod
    @log_call
    def get_info(dss_file: Union[str, Path]) -> Dict:
        """
        Get summary information about DSS file.

        Args:
            dss_file: Path to DSS file

        Returns:
            Dictionary with file information

        Example:
            info = RasDss.get_info("sample.dss")
            print(f"Total paths: {info['total_paths']}")
            print(f"File size: {info['file_size_mb']:.2f} MB")
        """
        dss_path = Path(dss_file)

        catalog = RasDss.get_catalog(dss_file)

        return {
            'filepath': str(dss_path.resolve()),
            'filename': dss_path.name,
            'file_size_mb': dss_path.stat().st_size / (1024 * 1024),
            'total_paths': len(catalog),
            'first_5_paths': catalog[:5] if len(catalog) > 5 else catalog,
        }

    @staticmethod
    @log_call
    def extract_boundary_timeseries(
        boundaries_df: pd.DataFrame,
        project_dir: Optional[Union[str, Path]] = None,
        ras_object=None
    ) -> pd.DataFrame:
        """
        Extract DSS time series data for all DSS-defined boundaries.

        Reads boundaries_df and extracts time series for any boundary condition
        defined by a DSS file. Adds the extracted data to the dataframe.

        Args:
            boundaries_df: DataFrame from ras.boundaries_df
            project_dir: Project directory (for resolving relative DSS paths)
            ras_object: RasPrj object (alternative to project_dir)

        Returns:
            Enhanced DataFrame with 'dss_timeseries' column containing extracted data

        Example:
            from ras_commander import init_ras_project, RasDss

            ras = init_ras_project("project_path", "6.6")

            # Extract all DSS boundary data
            enhanced_boundaries = RasDss.extract_boundary_timeseries(
                ras.boundaries_df, ras_object=ras
            )

            # Now enhanced_boundaries has a 'dss_timeseries' column with DataFrames
            for idx, row in enhanced_boundaries.iterrows():
                if row['Use DSS']:
                    print(f"{row['bc_type']}: {len(row['dss_timeseries'])} points")
        """
        # Get project directory
        if ras_object is not None:
            project_dir = ras_object.project_folder
        elif project_dir is None:
            raise ValueError("Must provide either project_dir or ras_object")

        project_dir = Path(project_dir)

        # Create a copy to avoid modifying original
        result_df = boundaries_df.copy()

        # Add column for time series data
        result_df['dss_timeseries'] = None

        # Find DSS-defined boundaries
        # Note: 'Use DSS' column may be string 'True'/'False' or boolean True/False
        dss_boundaries = result_df[
            (result_df['Use DSS'] == True) | (result_df['Use DSS'] == 'True')
        ]

        if len(dss_boundaries) == 0:
            logger.info("No DSS-defined boundaries found")
            return result_df

        logger.info(f"Found {len(dss_boundaries)} DSS-defined boundaries")

        # Extract time series for each DSS boundary
        success_count = 0
        fail_count = 0

        for idx, row in dss_boundaries.iterrows():
            dss_file = row['DSS File']
            dss_path = row['DSS Path']

            if pd.isna(dss_file) or pd.isna(dss_path):
                logger.warning(f"Row {idx}: Missing DSS File or DSS Path")
                continue

            # Resolve DSS file path (may be relative to project directory)
            dss_file_path = Path(dss_file)
            if not dss_file_path.is_absolute():
                dss_file_path = project_dir / dss_file

            if not dss_file_path.exists():
                logger.warning(f"Row {idx}: DSS file not found: {dss_file_path}")
                fail_count += 1
                continue

            try:
                # Read time series
                df_ts = RasDss.read_timeseries(dss_file_path, dss_path)

                # Store in result
                result_df.at[idx, 'dss_timeseries'] = df_ts

                success_count += 1
                logger.info(
                    f"Row {idx}: Extracted {len(df_ts)} points from "
                    f"{dss_file_path.name}"
                )

            except Exception as e:
                logger.warning(f"Row {idx}: Failed to read DSS data: {e}")
                fail_count += 1

        logger.info(
            f"Extraction complete: {success_count} success, {fail_count} failed"
        )

        return result_df

    @staticmethod
    def shutdown_jvm():
        """
        Shutdown Java Virtual Machine.

        Note: With pyjnius, JVM shutdown is typically not needed.
        This is a placeholder for API compatibility.
        """
        logger.info("pyjnius handles JVM lifecycle automatically")
        pass


if __name__ == "__main__":
    """Test RasDss class"""
    import sys

    print("="*80)
    print("RasDss Test")
    print("="*80)

    # Test file (from TestData)
    test_data_dir = Path(__file__).parent.parent.parent / "TestData"

    # Find a DSS file to test with
    dss_files = list(test_data_dir.glob("*.dss"))

    if not dss_files:
        print("No DSS files found in TestData/")
        sys.exit(1)

    # Use BaldEagleDamBrk.dss (V7 file that we know works)
    test_file = test_data_dir / "BaldEagleDamBrk.dss"

    if not test_file.exists():
        # Use first available file
        test_file = dss_files[0]

    print(f"\nTest file: {test_file.name}")
    print(f"Size: {test_file.stat().st_size / 1024:.2f} KB")

    # Get file info
    print("\n" + "-"*80)
    print("Getting file info...")
    print("-"*80)
    info = RasDss.get_info(test_file)
    for key, value in info.items():
        if key == 'first_5_paths':
            print(f"{key}:")
            for path in value:
                print(f"  - {path}")
        else:
            print(f"{key}: {value}")

    # Get full catalog
    print("\n" + "-"*80)
    print("Getting catalog...")
    print("-"*80)
    catalog = RasDss.get_catalog(test_file)
    print(f"Total paths: {len(catalog)}")

    if len(catalog) > 0:
        # Read first time series
        print("\n" + "-"*80)
        print(f"Reading time series: {catalog[0]}")
        print("-"*80)
        df = RasDss.read_timeseries(test_file, catalog[0])

        print(f"\nDataFrame shape: {df.shape}")
        print(f"Date range: {df.index.min()} to {df.index.max()}")
        print(f"Value range: {df['value'].min():.2f} to {df['value'].max():.2f}")
        print(f"Units: {df.attrs.get('units', 'N/A')}")

        print("\nFirst 10 rows:")
        print(df.head(10))

        print("\nLast 10 rows:")
        print(df.tail(10))

    print("\n" + "="*80)
    print("Test complete!")
    print("="*80)

==================================================

File: C:\GH\ras-commander\ras_commander\dss\_hec_monolith.py
==================================================
"""
HEC Monolith Downloader

Replicates dssrip2's monolith installation approach.
Downloads HEC Monolith JARs and native libraries from HEC Nexus repository.

Based on: https://github.com/mkoohafkan/dssrip2
"""

import os
import sys
import platform
import hashlib
import zipfile
from pathlib import Path
from typing import List, Dict, Optional
import requests
from tqdm import tqdm


class HecMonolithDownloader:
    """
    Download and manage HEC Monolith libraries.

    Replicates dssrip2's requirements.yaml and download logic.
    """

    # HEC Nexus repository
    NEXUS_BASE = "https://www.hec.usace.army.mil/nexus/repository/maven-public"
    MAVEN_BASE = "https://repo.maven.apache.org/maven2"

    # Requirements (from dssrip2's requirements.yaml)
    COMMON_JARS = [
        {"artifactId": "hec-monolith", "group": "mil.army.usace.hec", "version": "3.3.27"},
        {"artifactId": "hec-monolith-compat", "group": "mil.army.usace.hec", "version": "3.3.27"},
        {"artifactId": "hec-nucleus-data", "group": "mil.army.usace.hec", "version": "2.0.1"},
        {"artifactId": "hec-nucleus-metadata", "group": "mil.army.usace.hec", "version": "2.0.1"},
        {"artifactId": "hecnf", "group": "mil.army.usace.hec.hecnf", "version": "7.2.0"},
        {"artifactId": "flogger", "group": "com.google.flogger", "version": "0.5.1", "source": "maven"},
        {"artifactId": "flogger-system-backend", "group": "com.google.flogger", "version": "0.5.1", "source": "maven"},
    ]

    # Platform-specific native libraries
    NATIVE_LIBS = {
        "Windows": {"artifactId": "javaHeclib", "group": "mil.army.usace.hec",
                    "version": "7-IU-8-win-x86_64", "extension": "zip"},
        "Linux": {"artifactId": "javaHeclib", "group": "mil.army.usace.hec",
                  "version": "7-IU-8-linux-x86_64", "extension": "zip"},
        "Darwin": {"artifactId": "javaHeclib", "group": "mil.army.usace.hec",
                   "version": "7-IU-8-macOS-x86_64", "extension": "zip"},
    }

    def __init__(self, cache_dir: Optional[Path] = None):
        """
        Initialize downloader.

        Args:
            cache_dir: Directory to cache downloads.
                      Defaults to ~/.ras-commander/dss/
        """
        if cache_dir is None:
            cache_dir = Path.home() / ".ras-commander" / "dss"

        self.cache_dir = Path(cache_dir)
        self.jar_dir = self.cache_dir / "jar"
        self.lib_dir = self.cache_dir / "lib"

        # Create directories
        self.jar_dir.mkdir(parents=True, exist_ok=True)
        self.lib_dir.mkdir(parents=True, exist_ok=True)

    def is_installed(self) -> bool:
        """Check if HEC Monolith is already installed."""
        # Check for key JARs
        required_jars = ["hec-monolith", "hec-nucleus-data"]
        for jar_name in required_jars:
            if not list(self.jar_dir.glob(f"{jar_name}*.jar")):
                return False

        # Check for native library
        platform_name = platform.system()
        if platform_name == "Windows":
            lib_file = "javaHeclib.dll"
        elif platform_name == "Linux":
            lib_file = "libjavaHeclib.so"
        elif platform_name == "Darwin":
            lib_file = "libjavaHeclib.dylib"
        else:
            return False

        return (self.lib_dir / lib_file).exists()

    def get_download_url(self, artifact: Dict, source: str = "nexus") -> str:
        """
        Construct Maven download URL.

        Args:
            artifact: Artifact specification dict
            source: "nexus" or "maven"

        Returns:
            Download URL
        """
        group = artifact["group"].replace(".", "/")
        artifact_id = artifact["artifactId"]
        version = artifact["version"]
        extension = artifact.get("extension", "jar")

        filename = f"{artifact_id}-{version}.{extension}"

        if source == "maven":
            base = self.MAVEN_BASE
        else:
            base = self.NEXUS_BASE

        url = f"{base}/{group}/{artifact_id}/{version}/{filename}"
        return url

    def download_file(self, url: str, dest: Path, description: str = "") -> Path:
        """
        Download file with progress bar.

        Args:
            url: URL to download
            dest: Destination file path
            description: Description for progress bar

        Returns:
            Path to downloaded file
        """
        if dest.exists():
            print(f"  Using cached: {dest.name}")
            return dest

        print(f"  Downloading: {description or dest.name}")

        response = requests.get(url, stream=True)
        response.raise_for_status()

        total_size = int(response.headers.get('content-length', 0))

        with open(dest, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    pbar.update(len(chunk))

        return dest

    def verify_sha1(self, filepath: Path, expected_sha1: Optional[str] = None) -> bool:
        """
        Verify SHA1 checksum of file.

        Args:
            filepath: File to verify
            expected_sha1: Expected SHA1 hash (if None, tries to download .sha1 file)

        Returns:
            True if checksum matches
        """
        if expected_sha1 is None:
            # Try to download .sha1 file
            return True  # Skip verification if no checksum available

        sha1 = hashlib.sha1()
        with open(filepath, 'rb') as f:
            while chunk := f.read(8192):
                sha1.update(chunk)

        computed = sha1.hexdigest()
        return computed == expected_sha1

    def download_jars(self):
        """Download all required JAR files."""
        print("\nDownloading HEC Monolith JAR files...")

        for artifact in self.COMMON_JARS:
            source = artifact.get("source", "nexus")
            url = self.get_download_url(artifact, source)
            filename = f"{artifact['artifactId']}-{artifact['version']}.jar"
            dest = self.jar_dir / filename

            self.download_file(url, dest, artifact['artifactId'])

    def download_native_library(self):
        """Download platform-specific native library."""
        platform_name = platform.system()

        if platform_name not in self.NATIVE_LIBS:
            raise RuntimeError(f"Unsupported platform: {platform_name}")

        print(f"\nDownloading native library for {platform_name}...")

        artifact = self.NATIVE_LIBS[platform_name]
        url = self.get_download_url(artifact)
        filename = f"{artifact['artifactId']}-{artifact['version']}.zip"
        dest = self.lib_dir / filename

        # Download ZIP
        self.download_file(url, dest, f"Native library ({platform_name})")

        # Extract ZIP
        print(f"  Extracting native library...")
        with zipfile.ZipFile(dest, 'r') as zip_ref:
            zip_ref.extractall(self.lib_dir)

        # Remove ZIP file
        dest.unlink()

        print(f"  [OK] Native library installed")

    def install(self, force: bool = False):
        """
        Download and install HEC Monolith.

        Args:
            force: Force re-download even if already installed
        """
        if self.is_installed() and not force:
            print("HEC Monolith already installed")
            return

        if force:
            print("Forcing re-download of HEC Monolith...")

        print("="*80)
        print("Installing HEC Monolith Libraries")
        print("="*80)
        print(f"Install location: {self.cache_dir}")

        # Download JARs
        self.download_jars()

        # Download native library
        self.download_native_library()

        print("\n" + "="*80)
        print("[SUCCESS] HEC Monolith installation complete!")
        print("="*80)

    def get_classpath(self) -> List[str]:
        """
        Get list of JAR paths for JVM classpath.

        Returns:
            List of absolute JAR file paths
        """
        if not self.is_installed():
            raise RuntimeError("HEC Monolith not installed. Call install() first.")

        jar_files = list(self.jar_dir.glob("*.jar"))
        return [str(jar.resolve()) for jar in sorted(jar_files)]

    def get_library_path(self) -> str:
        """
        Get path to native library directory.

        Returns:
            Absolute path to lib directory containing native libraries
        """
        if not self.is_installed():
            raise RuntimeError("HEC Monolith not installed. Call install() first.")

        return str(self.lib_dir.resolve())

    def get_info(self) -> Dict:
        """
        Get information about installation.

        Returns:
            Dict with installation details
        """
        jars = list(self.jar_dir.glob("*.jar"))
        libs = list(self.lib_dir.glob("*"))

        total_size = sum(f.stat().st_size for f in jars + libs)

        return {
            "installed": self.is_installed(),
            "cache_dir": str(self.cache_dir),
            "num_jars": len(jars),
            "jar_files": [j.name for j in sorted(jars)],
            "lib_files": [l.name for l in sorted(libs)],
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "platform": platform.system(),
        }


if __name__ == "__main__":
    """Test installation"""
    downloader = HecMonolithDownloader()

    print("HEC Monolith Downloader Test")
    print("="*80)

    # Check current status
    info = downloader.get_info()
    print(f"\nInstalled: {info['installed']}")
    print(f"Cache dir: {info['cache_dir']}")

    if not info['installed']:
        # Install
        downloader.install()

        # Show info
        info = downloader.get_info()
        print(f"\nInstallation complete:")
        print(f"  JARs: {info['num_jars']}")
        print(f"  Total size: {info['total_size_mb']} MB")
        print(f"\nClasspath:")
        for jar in downloader.get_classpath():
            print(f"  - {Path(jar).name}")
        print(f"\nLibrary path: {downloader.get_library_path()}")
    else:
        print("\nAlready installed!")
        print(f"  JARs: {info['num_jars']}")
        print(f"  Total size: {info['total_size_mb']} MB")

==================================================

File: C:\GH\ras-commander\ras_commander\dss\__init__.py
==================================================
"""
ras-commander DSS subpackage: HEC-DSS file operations.

This subpackage provides DSS file reading capabilities using HEC Monolith
Java libraries via pyjnius. All dependencies are lazy-loaded to minimize
import time and keep optional dependencies truly optional.

Lazy Loading Behavior:
    - `import ras_commander` - DSS not loaded (fast startup)
    - `from ras_commander import RasDss` - DSS subpackage loaded
    - `RasDss.get_catalog(...)` - pyjnius/Java loaded on first call
    - HEC Monolith libraries downloaded automatically on first use (~20 MB)

Dependencies:
    Required at runtime (lazy loaded):
        - pyjnius: pip install pyjnius
        - Java JRE/JDK 8+: Must be installed and JAVA_HOME set

    Auto-downloaded:
        - HEC Monolith libraries (~20 MB, cached in ~/.ras-commander/dss/)

Usage:
    from ras_commander import RasDss

    # Get catalog of all paths in DSS file
    paths = RasDss.get_catalog("file.dss")

    # Read time series
    df = RasDss.read_timeseries("file.dss", paths[0])
    print(df.attrs['units'])  # Access metadata

    # Extract all DSS boundary conditions
    from ras_commander import init_ras_project
    ras = init_ras_project("project_path", "6.6")
    enhanced = RasDss.extract_boundary_timeseries(ras.boundaries_df, ras_object=ras)

See Also:
    - examples/22_dss_boundary_extraction.ipynb for complete workflow
    - CLAUDE.md for API documentation
"""

from .RasDss import RasDss

__all__ = ['RasDss']

==================================================

File: C:\GH\ras-commander\ras_commander\geom\AGENTS.md
==================================================
# AGENTS.md for ras_commander/geom

This file provides guidance for AI agents working within the `geom` subpackage.

## Subpackage Overview

The `geom` subpackage provides comprehensive functionality for parsing and modifying HEC-RAS plain text geometry files (.g##). It handles 1D cross sections, storage areas, lateral structures, connections, inline weirs, bridges, and culverts.

## Module Organization

| Module | Class | Description |
|--------|-------|-------------|
| GeomParser.py | `GeomParser` | Core parsing utilities for fixed-width formats |
| GeomPreprocessor.py | `GeomPreprocessor` | Geometry preprocessor file management |
| GeomLandCover.py | `GeomLandCover` | 2D Manning's n land cover tables |
| GeomCrossSection.py | `GeomCrossSection` | 1D cross section operations |
| GeomStorage.py | `GeomStorage` | Storage area elevation-volume curves |
| GeomLateral.py | `GeomLateral` | Lateral structures and SA/2D connections |
| GeomInlineWeir.py | `GeomInlineWeir` | Inline weir structures |
| GeomBridge.py | `GeomBridge` | Bridge/culvert structure geometry |
| GeomCulvert.py | `GeomCulvert` | Culvert data extraction |

## Technical Patterns

### Fixed-Width Parsing
All geometry files use FORTRAN-era fixed-width format:
- **8-character columns** for numeric data
- **10 values per line** (80 characters total)
- **Count interpretation**: "#Sta/Elev= 40" means 40 PAIRS (80 total values)

Use `GeomParser.parse_fixed_width()` for reading and `GeomParser.format_fixed_width()` for writing.

### Import Patterns
Within this subpackage, use relative imports:
```python
from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser  # For inter-module dependencies
```

### Static Method Pattern
All classes use static methods with `@log_call` decorators:
```python
@staticmethod
@log_call
def get_something(geom_file: Union[str, Path], ...) -> pd.DataFrame:
    ...
```

### Error Handling
Standard exception hierarchy:
- `FileNotFoundError`: Geometry file doesn't exist
- `ValueError`: Invalid parameters or data not found
- `IOError`: File read/write failures

## API Reference

### GeomParser (Parsing Utilities)
- `parse_fixed_width(line, column_width=8)` - Parse fixed-width numeric data
- `format_fixed_width(values, column_width=8, values_per_line=10, precision=2)` - Format for writing
- `interpret_count(keyword, count_value)` - Interpret count declarations
- `identify_section(lines, keyword, start_index=0)` - Find section boundaries
- `extract_keyword_value(line, keyword)` - Extract value following keyword
- `extract_comma_list(line, keyword)` - Extract comma-separated list
- `create_backup(file_path)` - Create .bak backup

### GeomCrossSection (Cross Sections)
- `get_cross_sections(geom_file, river=None, reach=None)` - List all XS metadata
- `get_station_elevation(geom_file, river, reach, rs)` - Get XS geometry
- `set_station_elevation(geom_file, river, reach, rs, df, bank_left=None, bank_right=None)` - Modify XS
- `get_bank_stations(geom_file, river, reach, rs)` - Get bank locations
- `get_expansion_contraction(geom_file, river, reach, rs)` - Get coefficients
- `get_mannings_n(geom_file, river, reach, rs)` - Get roughness values

### GeomStorage (Storage Areas)
- `get_storage_areas(geom_file, exclude_2d=True)` - List storage area names
- `get_elevation_volume(geom_file, area_name)` - Get elevation-volume curve

### GeomLateral (Laterals & Connections)
- `get_lateral_structures(geom_file, river=None, reach=None)` - List lateral structures
- `get_weir_profile(geom_file, river, reach, rs, position=0)` - Get weir profile
- `get_connections(geom_file)` - List SA/2D connections
- `get_connection_profile(geom_file, connection_name)` - Get connection weir profile
- `get_connection_gates(geom_file, connection_name)` - Get gate definitions

### GeomInlineWeir (Inline Weirs)
- `get_weirs(geom_file, river=None, reach=None)` - List inline weirs
- `get_profile(geom_file, river, reach, rs)` - Get weir crest profile
- `get_gates(geom_file, river, reach, rs)` - Get gate definitions

### GeomBridge (Bridges)
- `get_bridges(geom_file, river=None, reach=None)` - List bridge structures
- `get_deck(geom_file, river, reach, rs)` - Get deck geometry
- `get_piers(geom_file, river, reach, rs)` - Get pier data
- `get_abutment(geom_file, river, reach, rs)` - Get abutment data
- `get_approach_sections(geom_file, river, reach, rs)` - Get approach XS data
- `get_coefficients(geom_file, river, reach, rs)` - Get loss coefficients
- `get_htab(geom_file, river, reach, rs)` - Get HTAB parameters

### GeomCulvert (Culverts)
- `get_culverts(geom_file, river, reach, rs)` - Get culverts at a location
- `get_all(geom_file, river=None, reach=None)` - Get all culverts in file

### GeomLandCover (2D Manning's n)
- `get_base_mannings_n(geom_file)` - Read base Manning's n table
- `set_base_mannings_n(geom_file, df)` - Write base Manning's n table
- `get_region_mannings_n(geom_file)` - Read region overrides
- `set_region_mannings_n(geom_file, df)` - Write region overrides

### GeomPreprocessor
- `clear_geompre_files(plan_files=None, ras_object=None)` - Clear preprocessor files

## Culvert Shape Codes

| Code | Shape |
|------|-------|
| 1 | Circular |
| 2 | Box |
| 3 | Pipe Arch |
| 4 | Ellipse |
| 5 | Arch |
| 6 | Semi-Circle |
| 7 | Low Profile Arch |
| 8 | High Profile Arch |
| 9 | Con Span |

## Critical Implementation Notes

1. **Bank Station Interpolation**: When modifying cross sections with `set_station_elevation()`, bank stations MUST appear as exact points in the station/elevation data. The method handles this automatically.

2. **450 Point Limit**: HEC-RAS enforces a maximum of 450 points per cross section. Validate before writing.

3. **Backup Files**: All write operations create `.bak` backup files automatically.

4. **Case Sensitivity**: River, reach, and RS parameters are case-sensitive and must match exactly.

## Deprecated Classes (Backward Compatibility)

The following classes provide backward compatibility and will be removed before v1.0:
- `RasGeo` → Use `GeomPreprocessor` and `GeomLandCover`
- `RasGeometry` → Use `GeomCrossSection`, `GeomStorage`, `GeomLateral`
- `RasGeometryUtils` → Use `GeomParser`

**Note**: `RasStruct` has been removed without backward compatibility.

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomBridge.py
==================================================
"""
GeomBridge - Bridge operations for HEC-RAS geometry files

This module provides functionality for reading bridge structure data
from HEC-RAS plain text geometry files (.g##).

All methods are static and designed to be used without instantiation.

List of Functions:
- get_bridges() - List all bridges with metadata
- get_deck() - Read deck geometry (stations, elevations, lowchord)
- get_piers() - Read pier definitions (widths, elevations)
- get_abutment() - Read abutment geometry
- get_approach_sections() - Read BR U/BR D approach sections
- get_coefficients() - Read hydraulic coefficients
- get_htab() - Read hydraulic table parameters

Example Usage:
    >>> from ras_commander import GeomBridge
    >>> from pathlib import Path
    >>>
    >>> # List all bridges
    >>> geom_file = Path("model.g08")
    >>> bridges_df = GeomBridge.get_bridges(geom_file)
    >>> print(f"Found {len(bridges_df)} bridges")
    >>>
    >>> # Get deck geometry
    >>> deck_df = GeomBridge.get_deck(geom_file, "River", "Reach", "25548")
    >>> print(deck_df)

Technical Notes:
    - Uses FORTRAN-era fixed-width format (8-char columns for numeric data)
    - Return types are all DataFrames (breaking change from RasStruct Dict returns)
"""

from pathlib import Path
from typing import Union, Optional, List, Dict, Any
import pandas as pd
import numpy as np

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser

logger = get_logger(__name__)


class GeomBridge:
    """
    Operations for parsing HEC-RAS bridges in geometry files.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8
    VALUES_PER_LINE = 10
    DEFAULT_SEARCH_RANGE = 100
    MAX_PARSE_LINES = 200

    @staticmethod
    def _find_bridge(lines: List[str], river: str, reach: str, rs: str) -> Optional[int]:
        """Find bridge/culvert section and return line index of 'Bridge Culvert-' marker."""
        current_river = None
        current_reach = None
        last_rs = None

        for i, line in enumerate(lines):
            if line.startswith("River Reach="):
                values = GeomParser.extract_comma_list(line, "River Reach")
                if len(values) >= 2:
                    current_river = values[0]
                    current_reach = values[1]

            elif line.startswith("Type RM Length L Ch R ="):
                value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                values = [v.strip() for v in value_str.split(',')]
                if len(values) > 1:
                    last_rs = values[1]

            elif line.startswith("Bridge Culvert-"):
                if (current_river == river and
                    current_reach == reach and
                    last_rs == rs):
                    logger.debug(f"Found bridge at line {i}: {river}/{reach}/RS {rs}")
                    return i

        logger.debug(f"Bridge not found: {river}/{reach}/RS {rs}")
        return None

    @staticmethod
    def _parse_bridge_header(line: str) -> Dict[str, Any]:
        """Parse 'Bridge Culvert-' header line into dict of flags."""
        value_part = line.replace("Bridge Culvert-", "").strip()
        parts = [p.strip() for p in value_part.split(',')]

        flags = {}
        flag_names = ['flag1', 'flag2', 'flag3', 'flag4', 'flag5']
        for i, name in enumerate(flag_names):
            if i < len(parts) and parts[i]:
                try:
                    flags[name] = int(parts[i])
                except ValueError:
                    flags[name] = None
            else:
                flags[name] = None

        return flags

    @staticmethod
    @log_call
    def get_bridges(geom_file: Union[str, Path],
                   river: Optional[str] = None,
                   reach: Optional[str] = None) -> pd.DataFrame:
        """
        List all bridges/culverts in geometry file with metadata.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: Optional filter by river name (case-sensitive)
            reach: Optional filter by reach name (case-sensitive)

        Returns:
            pd.DataFrame with columns:
            - River, Reach, RS: Location identifiers
            - NodeName: Bridge name/description
            - NumDecks: Number of deck spans
            - DeckWidth: Bridge deck width
            - WeirCoefficient: Weir flow coefficient
            - Skew: Bridge skew angle
            - NumPiers: Count of pier definitions
            - HasAbutment: Boolean indicating abutment presence
            - HTabHWMax: Maximum headwater elevation

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> bridges = GeomBridge.get_bridges("model.g08")
            >>> print(f"Found {len(bridges)} bridges")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridges = []
            current_river = None
            current_reach = None
            last_rs = None
            last_node_name = None
            last_edited = None

            i = 0
            while i < len(lines):
                line = lines[i]

                if line.startswith("River Reach="):
                    values = GeomParser.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                elif line.startswith("Type RM Length L Ch R ="):
                    value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]
                    if len(values) > 1:
                        last_rs = values[1]

                elif line.startswith("Node Name="):
                    last_node_name = GeomParser.extract_keyword_value(line, "Node Name")

                elif line.startswith("Node Last Edited Time="):
                    last_edited = GeomParser.extract_keyword_value(line, "Node Last Edited Time")

                elif line.startswith("Bridge Culvert-"):
                    if river is not None and current_river != river:
                        i += 1
                        continue
                    if reach is not None and current_reach != reach:
                        i += 1
                        continue

                    bridge_flags = GeomBridge._parse_bridge_header(line)

                    bridge_data = {
                        'River': current_river,
                        'Reach': current_reach,
                        'RS': last_rs,
                        'NodeName': last_node_name,
                        'NumDecks': None,
                        'DeckWidth': None,
                        'WeirCoefficient': None,
                        'Skew': None,
                        'MaxSubmergence': None,
                        'NumPiers': 0,
                        'HasAbutment': False,
                        'HTabHWMax': None,
                        'NodeLastEdited': last_edited
                    }

                    pier_count = 0
                    for j in range(i + 1, min(i + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines))):
                        search_line = lines[j]

                        if search_line.startswith("Deck Dist Width WeirC"):
                            if j + 1 < len(lines):
                                param_line = lines[j + 1]
                                parts = [p.strip() for p in param_line.split(',')]

                                if len(parts) > 0 and parts[0]:
                                    try: bridge_data['NumDecks'] = int(parts[0])
                                    except: pass
                                if len(parts) > 2 and parts[2]:
                                    try: bridge_data['DeckWidth'] = float(parts[2])
                                    except: pass
                                if len(parts) > 3 and parts[3]:
                                    try: bridge_data['WeirCoefficient'] = float(parts[3])
                                    except: pass
                                if len(parts) > 4 and parts[4]:
                                    try: bridge_data['Skew'] = float(parts[4])
                                    except: pass
                                if len(parts) > 9 and parts[9]:
                                    try: bridge_data['MaxSubmergence'] = float(parts[9])
                                    except: pass

                        elif search_line.startswith("Pier Skew, UpSta & Num"):
                            pier_count += 1

                        elif search_line.startswith("Abutment Skew #Up #Dn="):
                            bridge_data['HasAbutment'] = True

                        elif search_line.startswith("BC HTab HWMax="):
                            val = GeomParser.extract_keyword_value(search_line, "BC HTab HWMax")
                            if val:
                                try: bridge_data['HTabHWMax'] = float(val)
                                except: pass

                        elif search_line.startswith("Type RM Length L Ch R ="):
                            break

                    bridge_data['NumPiers'] = pier_count
                    bridges.append(bridge_data)
                    last_node_name = None
                    last_edited = None

                i += 1

            df = pd.DataFrame(bridges)
            logger.info(f"Found {len(df)} bridges in {geom_file.name}")
            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading bridges: {str(e)}")
            raise IOError(f"Failed to read bridges: {str(e)}")

    @staticmethod
    @log_call
    def get_deck(geom_file: Union[str, Path],
                river: str,
                reach: str,
                rs: str) -> pd.DataFrame:
        """
        Extract complete deck geometry for a bridge.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - Location: 'upstream' or 'downstream'
            - Station: Station values
            - Elevation: Deck elevation values
            - LowChord: Low chord elevation values

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge not found

        Example:
            >>> deck = GeomBridge.get_deck("model.g08", "River", "Reach", "25548")
            >>> upstream = deck[deck['Location'] == 'upstream']
            >>> print(f"Upstream deck has {len(upstream)} points")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomBridge._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge not found: {river}/{reach}/RS {rs}")

            deck_data = []

            for j in range(bridge_idx, min(bridge_idx + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines))):
                line = lines[j]

                if line.startswith("Deck Dist Width WeirC"):
                    if j + 1 < len(lines):
                        param_line = lines[j + 1]
                        parts = [p.strip() for p in param_line.split(',')]

                        num_up = 0
                        num_dn = 0
                        if len(parts) > 5 and parts[5]:
                            try: num_up = int(parts[5])
                            except: pass
                        if len(parts) > 6 and parts[6]:
                            try: num_dn = int(parts[6])
                            except: pass

                        # Read upstream data
                        if num_up > 0:
                            data_start = j + 2
                            all_up_values = []

                            for k in range(data_start, min(data_start + 10, len(lines))):
                                data_line = lines[k]
                                if '=' in data_line:
                                    break
                                values = GeomParser.parse_fixed_width(data_line, 8)
                                all_up_values.extend(values)
                                if len(all_up_values) >= num_up * 3:
                                    break

                            if len(all_up_values) >= num_up * 3:
                                stations = all_up_values[:num_up]
                                elevations = all_up_values[num_up:num_up*2]
                                lowchords = all_up_values[num_up*2:num_up*3]

                                for idx in range(min(len(stations), len(elevations), len(lowchords))):
                                    deck_data.append({
                                        'Location': 'upstream',
                                        'Station': stations[idx],
                                        'Elevation': elevations[idx],
                                        'LowChord': lowchords[idx]
                                    })

                        # Read downstream data
                        if num_dn > 0 and num_up > 0:
                            expected_up_lines = (num_up * 3 + 9) // 10 + 1
                            dn_start = j + 2 + expected_up_lines

                            all_dn_values = []
                            for k in range(dn_start, min(dn_start + 10, len(lines))):
                                if k >= len(lines):
                                    break
                                data_line = lines[k]
                                if '=' in data_line or data_line.startswith("Pier"):
                                    break
                                values = GeomParser.parse_fixed_width(data_line, 8)
                                all_dn_values.extend(values)
                                if len(all_dn_values) >= num_dn * 3:
                                    break

                            if len(all_dn_values) >= num_dn * 3:
                                stations = all_dn_values[:num_dn]
                                elevations = all_dn_values[num_dn:num_dn*2]
                                lowchords = all_dn_values[num_dn*2:num_dn*3]

                                for idx in range(min(len(stations), len(elevations), len(lowchords))):
                                    deck_data.append({
                                        'Location': 'downstream',
                                        'Station': stations[idx],
                                        'Elevation': elevations[idx],
                                        'LowChord': lowchords[idx]
                                    })

                    break

            df = pd.DataFrame(deck_data)
            logger.info(f"Extracted deck geometry for {river}/{reach}/RS {rs}: {len(df)} points")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bridge deck: {str(e)}")
            raise IOError(f"Failed to read bridge deck: {str(e)}")

    @staticmethod
    @log_call
    def get_piers(geom_file: Union[str, Path],
                 river: str,
                 reach: str,
                 rs: str) -> pd.DataFrame:
        """
        Extract all pier definitions for a bridge.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - PierIndex: Pier number (1, 2, 3...)
            - UpstreamStation, DownstreamStation: Pier locations
            - NumUpstreamPoints, NumDownstreamPoints: Point counts
            - UpstreamWidths, UpstreamElevations: Lists
            - DownstreamWidths, DownstreamElevations: Lists

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge not found or has no piers

        Example:
            >>> piers = GeomBridge.get_piers("model.g08", "River", "Reach", "25548")
            >>> print(f"Found {len(piers)} piers")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomBridge._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge not found: {river}/{reach}/RS {rs}")

            piers = []
            pier_index = 0

            i = bridge_idx
            while i < min(bridge_idx + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines)):
                line = lines[i]

                if line.startswith("Type RM Length L Ch R =") and i > bridge_idx + 5:
                    break

                if line.startswith("Pier Skew, UpSta & Num, DnSta & Num="):
                    pier_index += 1

                    value_str = GeomParser.extract_keyword_value(line, "Pier Skew, UpSta & Num, DnSta & Num")
                    parts = [p.strip() for p in value_str.split(',')]

                    pier_data = {
                        'PierIndex': pier_index,
                        'UpstreamStation': None,
                        'NumUpstreamPoints': 0,
                        'DownstreamStation': None,
                        'NumDownstreamPoints': 0,
                        'UpstreamWidths': [],
                        'UpstreamElevations': [],
                        'DownstreamWidths': [],
                        'DownstreamElevations': []
                    }

                    if len(parts) > 1 and parts[1]:
                        try: pier_data['UpstreamStation'] = float(parts[1])
                        except: pass
                    if len(parts) > 2 and parts[2]:
                        try: pier_data['NumUpstreamPoints'] = int(parts[2])
                        except: pass
                    if len(parts) > 3 and parts[3]:
                        try: pier_data['DownstreamStation'] = float(parts[3])
                        except: pass
                    if len(parts) > 4 and parts[4]:
                        try: pier_data['NumDownstreamPoints'] = int(parts[4])
                        except: pass

                    num_up = pier_data['NumUpstreamPoints']
                    num_dn = pier_data['NumDownstreamPoints']

                    if num_up > 0 and i + 2 < len(lines):
                        widths_line = lines[i + 1]
                        if '=' not in widths_line:
                            pier_data['UpstreamWidths'] = GeomParser.parse_fixed_width(widths_line, 8)[:num_up]

                        if i + 2 < len(lines):
                            elev_line = lines[i + 2]
                            if '=' not in elev_line:
                                pier_data['UpstreamElevations'] = GeomParser.parse_fixed_width(elev_line, 8)[:num_up]

                    if num_dn > 0 and i + 4 < len(lines):
                        widths_line = lines[i + 3]
                        if '=' not in widths_line:
                            pier_data['DownstreamWidths'] = GeomParser.parse_fixed_width(widths_line, 8)[:num_dn]

                        if i + 4 < len(lines):
                            elev_line = lines[i + 4]
                            if '=' not in elev_line:
                                pier_data['DownstreamElevations'] = GeomParser.parse_fixed_width(elev_line, 8)[:num_dn]

                    piers.append(pier_data)
                    i += 4

                i += 1

            if not piers:
                raise ValueError(f"No piers found for bridge: {river}/{reach}/RS {rs}")

            df = pd.DataFrame(piers)
            logger.info(f"Extracted {len(df)} piers for {river}/{reach}/RS {rs}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bridge piers: {str(e)}")
            raise IOError(f"Failed to read bridge piers: {str(e)}")

    @staticmethod
    @log_call
    def get_abutment(geom_file: Union[str, Path],
                    river: str,
                    reach: str,
                    rs: str) -> pd.DataFrame:
        """
        Extract abutment geometry for a bridge (if present).

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - Location: 'upstream' or 'downstream'
            - Station: Abutment station values
            - Parameter: Abutment parameter values

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge not found or has no abutment

        Example:
            >>> abutment = GeomBridge.get_abutment("model.g08", "River", "Reach", "25548")
            >>> print(f"Abutment has {len(abutment)} points")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomBridge._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge not found: {river}/{reach}/RS {rs}")

            abutment_data = []

            for j in range(bridge_idx, min(bridge_idx + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines))):
                line = lines[j]

                if line.startswith("Type RM Length L Ch R =") and j > bridge_idx + 5:
                    break

                if line.startswith("Abutment Skew #Up #Dn="):
                    value_str = GeomParser.extract_keyword_value(line, "Abutment Skew #Up #Dn")
                    parts = [p.strip() for p in value_str.split(',')]

                    num_up = 0
                    num_dn = 0
                    if len(parts) > 0 and parts[0]:
                        try: num_up = int(parts[0])
                        except: pass
                    if len(parts) > 1 and parts[1]:
                        try: num_dn = int(parts[1])
                        except: pass

                    if num_up > 0 and j + 2 < len(lines):
                        sta_line = lines[j + 1]
                        if '=' not in sta_line:
                            stations = GeomParser.parse_fixed_width(sta_line, 8)[:num_up]

                        param_line = lines[j + 2]
                        if '=' not in param_line:
                            params = GeomParser.parse_fixed_width(param_line, 8)[:num_up]

                        for idx in range(min(len(stations), len(params))):
                            abutment_data.append({
                                'Location': 'upstream',
                                'Station': stations[idx],
                                'Parameter': params[idx]
                            })

                    if num_dn > 0 and j + 4 < len(lines):
                        sta_line = lines[j + 3]
                        if '=' not in sta_line:
                            stations = GeomParser.parse_fixed_width(sta_line, 8)[:num_dn]

                        param_line = lines[j + 4]
                        if '=' not in param_line:
                            params = GeomParser.parse_fixed_width(param_line, 8)[:num_dn]

                        for idx in range(min(len(stations), len(params))):
                            abutment_data.append({
                                'Location': 'downstream',
                                'Station': stations[idx],
                                'Parameter': params[idx]
                            })

                    break

            if not abutment_data:
                raise ValueError(f"No abutment found for bridge: {river}/{reach}/RS {rs}")

            df = pd.DataFrame(abutment_data)
            logger.info(f"Extracted abutment for {river}/{reach}/RS {rs}: {len(df)} points")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bridge abutment: {str(e)}")
            raise IOError(f"Failed to read bridge abutment: {str(e)}")

    @staticmethod
    @log_call
    def get_approach_sections(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str) -> pd.DataFrame:
        """
        Extract BR U (upstream) and BR D (downstream) approach section geometry.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - Location: 'upstream' or 'downstream'
            - DataType: 'station_elevation', 'mannings_n', or 'banks'
            - Station, Elevation, N_Value, LeftBank, RightBank: Data values

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge not found

        Example:
            >>> approach = GeomBridge.get_approach_sections("model.g08", "River", "Reach", "25548")
            >>> upstream_xs = approach[(approach['Location'] == 'upstream') & (approach['DataType'] == 'station_elevation')]
            >>> print(f"Upstream XS has {len(upstream_xs)} points")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomBridge._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge not found: {river}/{reach}/RS {rs}")

            approach_data = []

            for j in range(bridge_idx, min(bridge_idx + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines))):
                line = lines[j]

                if line.startswith("Type RM Length L Ch R =") and j > bridge_idx + 5:
                    break

                # Upstream station/elevation
                if line.startswith("BR U #Sta/Elev="):
                    count_str = GeomParser.extract_keyword_value(line, "BR U #Sta/Elev")
                    count = int(count_str.strip())

                    values = []
                    k = j + 1
                    while len(values) < count * 2 and k < len(lines):
                        if '=' in lines[k]:
                            break
                        values.extend(GeomParser.parse_fixed_width(lines[k], 8))
                        k += 1

                    stations = values[0::2]
                    elevations = values[1::2]

                    for idx in range(min(len(stations), len(elevations))):
                        approach_data.append({
                            'Location': 'upstream',
                            'DataType': 'station_elevation',
                            'Station': stations[idx],
                            'Elevation': elevations[idx],
                            'N_Value': None,
                            'LeftBank': None,
                            'RightBank': None
                        })

                # Downstream station/elevation
                elif line.startswith("BR D #Sta/Elev="):
                    count_str = GeomParser.extract_keyword_value(line, "BR D #Sta/Elev")
                    count = int(count_str.strip())

                    values = []
                    k = j + 1
                    while len(values) < count * 2 and k < len(lines):
                        if '=' in lines[k]:
                            break
                        values.extend(GeomParser.parse_fixed_width(lines[k], 8))
                        k += 1

                    stations = values[0::2]
                    elevations = values[1::2]

                    for idx in range(min(len(stations), len(elevations))):
                        approach_data.append({
                            'Location': 'downstream',
                            'DataType': 'station_elevation',
                            'Station': stations[idx],
                            'Elevation': elevations[idx],
                            'N_Value': None,
                            'LeftBank': None,
                            'RightBank': None
                        })

                # Upstream banks
                elif line.startswith("BR U Banks="):
                    val = GeomParser.extract_keyword_value(line, "BR U Banks")
                    parts = [p.strip() for p in val.split(',')]
                    left = float(parts[0]) if len(parts) > 0 and parts[0] else None
                    right = float(parts[1]) if len(parts) > 1 and parts[1] else None
                    approach_data.append({
                        'Location': 'upstream',
                        'DataType': 'banks',
                        'Station': None,
                        'Elevation': None,
                        'N_Value': None,
                        'LeftBank': left,
                        'RightBank': right
                    })

                # Downstream banks
                elif line.startswith("BR D Banks="):
                    val = GeomParser.extract_keyword_value(line, "BR D Banks")
                    parts = [p.strip() for p in val.split(',')]
                    left = float(parts[0]) if len(parts) > 0 and parts[0] else None
                    right = float(parts[1]) if len(parts) > 1 and parts[1] else None
                    approach_data.append({
                        'Location': 'downstream',
                        'DataType': 'banks',
                        'Station': None,
                        'Elevation': None,
                        'N_Value': None,
                        'LeftBank': left,
                        'RightBank': right
                    })

            df = pd.DataFrame(approach_data)
            logger.info(f"Extracted approach sections for {river}/{reach}/RS {rs}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading approach sections: {str(e)}")
            raise IOError(f"Failed to read approach sections: {str(e)}")

    @staticmethod
    @log_call
    def get_coefficients(geom_file: Union[str, Path],
                        river: str,
                        reach: str,
                        rs: str) -> pd.DataFrame:
        """
        Extract bridge hydraulic coefficients and parameters.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - ParameterType: 'br_coef', 'wspro', or 'bc_design'
            - Index: Parameter index
            - Value: Parameter value

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge not found

        Example:
            >>> coef = GeomBridge.get_coefficients("model.g08", "River", "Reach", "25548")
            >>> br_coefs = coef[coef['ParameterType'] == 'br_coef']
            >>> print(br_coefs)
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomBridge._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge not found: {river}/{reach}/RS {rs}")

            coef_data = []

            for j in range(bridge_idx, min(bridge_idx + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines))):
                line = lines[j]

                if line.startswith("Type RM Length L Ch R =") and j > bridge_idx + 5:
                    break

                if line.startswith("BR Coef="):
                    val = GeomParser.extract_keyword_value(line, "BR Coef")
                    parts = [p.strip() for p in val.split(',')]
                    for idx, p in enumerate(parts):
                        if p:
                            try:
                                coef_data.append({
                                    'ParameterType': 'br_coef',
                                    'Index': idx,
                                    'Value': float(p)
                                })
                            except ValueError:
                                coef_data.append({
                                    'ParameterType': 'br_coef',
                                    'Index': idx,
                                    'Value': p
                                })

                elif line.startswith("WSPro="):
                    val = GeomParser.extract_keyword_value(line, "WSPro")
                    parts = [p.strip() for p in val.split(',')]
                    for idx, p in enumerate(parts):
                        if p:
                            try:
                                coef_data.append({
                                    'ParameterType': 'wspro',
                                    'Index': idx,
                                    'Value': float(p)
                                })
                            except ValueError:
                                coef_data.append({
                                    'ParameterType': 'wspro',
                                    'Index': idx,
                                    'Value': p
                                })

                elif line.startswith("BC Design="):
                    val = GeomParser.extract_keyword_value(line, "BC Design")
                    parts = [p.strip() for p in val.split(',')]
                    for idx, p in enumerate(parts):
                        if p:
                            coef_data.append({
                                'ParameterType': 'bc_design',
                                'Index': idx,
                                'Value': p
                            })

            df = pd.DataFrame(coef_data)
            logger.info(f"Extracted coefficients for {river}/{reach}/RS {rs}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bridge coefficients: {str(e)}")
            raise IOError(f"Failed to read bridge coefficients: {str(e)}")

    @staticmethod
    @log_call
    def get_htab(geom_file: Union[str, Path],
                river: str,
                reach: str,
                rs: str) -> pd.DataFrame:
        """
        Extract bridge hydraulic table (HTab) parameters.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - Parameter: Parameter name (HWMax, TWMax, MaxFlow, etc.)
            - Value: Parameter value

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge not found

        Example:
            >>> htab = GeomBridge.get_htab("model.g08", "River", "Reach", "25548")
            >>> hw_max = htab[htab['Parameter'] == 'HWMax']['Value'].values[0]
            >>> print(f"HW Max: {hw_max}")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomBridge._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge not found: {river}/{reach}/RS {rs}")

            htab_data = []

            for j in range(bridge_idx, min(bridge_idx + GeomBridge.DEFAULT_SEARCH_RANGE, len(lines))):
                line = lines[j]

                if line.startswith("Type RM Length L Ch R =") and j > bridge_idx + 5:
                    break

                if line.startswith("BC HTab HWMax="):
                    val = GeomParser.extract_keyword_value(line, "BC HTab HWMax")
                    if val:
                        try:
                            htab_data.append({'Parameter': 'HWMax', 'Value': float(val)})
                        except: pass

                elif line.startswith("BC HTab TWMax="):
                    val = GeomParser.extract_keyword_value(line, "BC HTab TWMax")
                    if val:
                        try:
                            htab_data.append({'Parameter': 'TWMax', 'Value': float(val)})
                        except: pass

                elif line.startswith("BC HTab MaxFlow="):
                    val = GeomParser.extract_keyword_value(line, "BC HTab MaxFlow")
                    if val:
                        try:
                            htab_data.append({'Parameter': 'MaxFlow', 'Value': float(val)})
                        except: pass

                elif line.startswith("BC Use User HTab Curves="):
                    val = GeomParser.extract_keyword_value(line, "BC Use User HTab Curves")
                    if val:
                        try:
                            htab_data.append({'Parameter': 'UseCurves', 'Value': int(val)})
                        except: pass

                elif line.startswith("BC User HTab FreeFlow(D)="):
                    val = GeomParser.extract_keyword_value(line, "BC User HTab FreeFlow(D)")
                    if val:
                        try:
                            htab_data.append({'Parameter': 'FreeFlowCurves', 'Value': int(val.strip())})
                        except: pass

                elif line.startswith("BC User HTab Sub Curve(D)="):
                    val = GeomParser.extract_keyword_value(line, "BC User HTab Sub Curve(D)")
                    if val:
                        try:
                            htab_data.append({'Parameter': 'SubmergedCurves', 'Value': int(val.strip())})
                        except: pass

            df = pd.DataFrame(htab_data)
            logger.info(f"Extracted HTab parameters for {river}/{reach}/RS {rs}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bridge HTab: {str(e)}")
            raise IOError(f"Failed to read bridge HTab: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomCrossSection.py
==================================================
"""
GeomCrossSection - 1D Cross section operations for HEC-RAS geometry files

This module provides comprehensive functionality for reading and modifying
HEC-RAS 1D cross section data in plain text geometry files (.g##).

All methods are static and designed to be used without instantiation.

List of Functions:
- get_cross_sections() - Extract all cross section metadata
- get_station_elevation() - Read station/elevation pairs for a cross section
- set_station_elevation() - Write station/elevation with automatic bank interpolation
- get_bank_stations() - Read left and right bank station locations
- get_expansion_contraction() - Read expansion and contraction coefficients
- get_mannings_n() - Read Manning's roughness values with LOB/Channel/ROB classification

Example Usage:
    >>> from ras_commander import GeomCrossSection
    >>> from pathlib import Path
    >>>
    >>> # List all cross sections
    >>> geom_file = Path("BaldEagle.g01")
    >>> xs_df = GeomCrossSection.get_cross_sections(geom_file)
    >>> print(f"Found {len(xs_df)} cross sections")
    >>>
    >>> # Get station/elevation for specific XS
    >>> sta_elev = GeomCrossSection.get_station_elevation(
    ...     geom_file, "Bald Eagle Creek", "Reach 1", "138154.4"
    ... )
    >>> print(sta_elev.head())
    >>>
    >>> # Modify and write back
    >>> sta_elev['Elevation'] += 1.0  # Raise XS by 1 foot
    >>> GeomCrossSection.set_station_elevation(
    ...     geom_file, "Bald Eagle Creek", "Reach 1", "138154.4", sta_elev
    ... )

Technical Notes:
    - Uses FORTRAN-era fixed-width format (8-char columns for numeric data)
    - Count interpretation: "#Sta/Elev= 40" means 40 PAIRS (80 total values)
    - Always creates .bak backup before modification
"""

from pathlib import Path
from typing import Union, Optional, List, Tuple
import pandas as pd
import numpy as np

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser

logger = get_logger(__name__)


class GeomCrossSection:
    """
    Operations for parsing and modifying HEC-RAS 1D cross sections.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8      # Character width for numeric data in geometry files
    VALUES_PER_LINE = 10        # Number of values per line in fixed-width format
    MAX_XS_POINTS = 450         # HEC-RAS hard limit on cross section points

    # Parsing constants
    DEFAULT_SEARCH_RANGE = 50   # Default number of lines to search for keywords after XS header
    MAX_PARSE_LINES = 100       # Safety limit on lines to parse for data blocks

    # ========== PRIVATE HELPER METHODS ==========

    @staticmethod
    def _find_cross_section(lines: List[str], river: str, reach: str, rs: str) -> Optional[int]:
        """
        Find cross section in geometry file and return starting line index.

        Args:
            lines: File lines (from readlines())
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string, e.g., "138154.4")

        Returns:
            Line index where "Type RM Length L Ch R =" for matching XS starts,
            or None if not found
        """
        current_river = None
        current_reach = None

        for i, line in enumerate(lines):
            # Track current river/reach
            if line.startswith("River Reach="):
                values = GeomParser.extract_comma_list(line, "River Reach")
                if len(values) >= 2:
                    current_river = values[0]
                    current_reach = values[1]

            # Find matching cross section
            elif line.startswith("Type RM Length L Ch R ="):
                value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                values = [v.strip() for v in value_str.split(',')]

                if len(values) > 1:
                    # Format: Type, RS, Length_L, Length_Ch, Length_R
                    xs_rs = values[1]  # RS is second value

                    if (current_river == river and
                        current_reach == reach and
                        xs_rs == rs):
                        logger.debug(f"Found XS at line {i}: {river}/{reach}/RS {rs}")
                        return i

        logger.debug(f"XS not found: {river}/{reach}/RS {rs}")
        return None

    @staticmethod
    def _read_bank_stations(lines: List[str], start_idx: int,
                           search_range: Optional[int] = None) -> Optional[Tuple[float, float]]:
        """
        Read bank stations from XS block starting at start_idx.

        Args:
            lines: File lines (from readlines())
            start_idx: Index to start searching (typically from _find_cross_section)
            search_range: Number of lines to search ahead (default: DEFAULT_SEARCH_RANGE)

        Returns:
            (left_bank, right_bank) tuple or None if no banks defined
        """
        if search_range is None:
            search_range = GeomCrossSection.DEFAULT_SEARCH_RANGE

        for k in range(start_idx, min(start_idx + search_range, len(lines))):
            if lines[k].startswith("Bank Sta="):
                bank_str = GeomParser.extract_keyword_value(lines[k], "Bank Sta")
                bank_values = [v.strip() for v in bank_str.split(',')]
                if len(bank_values) >= 2:
                    left_bank = float(bank_values[0])
                    right_bank = float(bank_values[1])
                    logger.debug(f"Read bank stations: {left_bank}, {right_bank}")
                    return (left_bank, right_bank)

        return None

    @staticmethod
    def _parse_data_block(lines: List[str], start_idx: int, expected_count: int,
                         column_width: Optional[int] = None,
                         max_lines: Optional[int] = None) -> List[float]:
        """
        Parse fixed-width numeric data block following a count keyword.

        Args:
            lines: File lines (from readlines())
            start_idx: Index to start parsing (typically count_line + 1)
            expected_count: Number of values to read
            column_width: Character width of each column (default: FIXED_WIDTH_COLUMN)
            max_lines: Safety limit on lines to read (default: MAX_PARSE_LINES)

        Returns:
            List of parsed float values
        """
        if column_width is None:
            column_width = GeomCrossSection.FIXED_WIDTH_COLUMN
        if max_lines is None:
            max_lines = GeomCrossSection.MAX_PARSE_LINES

        values = []
        line_idx = start_idx

        while len(values) < expected_count and line_idx < len(lines):
            # Stop if hit next keyword
            if lines[line_idx].strip() and lines[line_idx].strip()[0].isupper():
                if '=' in lines[line_idx]:
                    break

            parsed = GeomParser.parse_fixed_width(lines[line_idx], column_width=column_width)
            values.extend(parsed)
            line_idx += 1

            # Safety check
            if line_idx > start_idx + max_lines:
                logger.warning(f"Exceeded max lines ({max_lines}) while parsing data block")
                break

        return values

    @staticmethod
    def _parse_paired_data(lines: List[str], start_idx: int, count: int,
                          col1_name: str = 'Station',
                          col2_name: str = 'Elevation') -> pd.DataFrame:
        """
        Parse paired data (station/elevation, elevation/volume, etc.) into DataFrame.

        Args:
            lines: File lines (from readlines())
            start_idx: Index to start parsing (typically count_line + 1)
            count: Number of PAIRS (not total values)
            col1_name: Name for first column (default: 'Station')
            col2_name: Name for second column (default: 'Elevation')

        Returns:
            DataFrame with two columns
        """
        total_values = count * 2
        values = GeomCrossSection._parse_data_block(lines, start_idx, total_values)

        if len(values) != total_values:
            logger.warning(f"Expected {total_values} values, got {len(values)}")

        # Split into pairs
        col1_data = values[0::2]  # Every other value starting at 0
        col2_data = values[1::2]  # Every other value starting at 1

        return pd.DataFrame({col1_name: col1_data, col2_name: col2_data})

    @staticmethod
    def _interpolate_at_banks(sta_elev_df: pd.DataFrame,
                             bank_left: Optional[float] = None,
                             bank_right: Optional[float] = None) -> pd.DataFrame:
        """
        Interpolate elevation at bank stations and insert into station/elevation data.

        HEC-RAS REQUIRES that bank station values appear as exact points in the
        station/elevation data. This method ensures banks are interpolated and inserted.

        Args:
            sta_elev_df: Station/elevation data
            bank_left: Left bank station
            bank_right: Right bank station

        Returns:
            Modified DataFrame with banks interpolated and inserted
        """
        result_df = sta_elev_df.copy()

        # Interpolate and insert left bank if needed
        if bank_left is not None:
            stations = result_df['Station'].values
            elevations = result_df['Elevation'].values

            if bank_left not in stations:
                # Interpolate elevation at left bank
                bank_left_elev = np.interp(bank_left, stations, elevations)

                # Insert into DataFrame
                new_row = pd.DataFrame({'Station': [bank_left], 'Elevation': [bank_left_elev]})
                result_df = pd.concat([result_df, new_row], ignore_index=True)
                result_df = result_df.sort_values('Station').reset_index(drop=True)

                logger.debug(f"Interpolated left bank at station {bank_left:.2f}, elevation {bank_left_elev:.2f}")

        # Interpolate and insert right bank if needed
        if bank_right is not None:
            stations = result_df['Station'].values
            elevations = result_df['Elevation'].values

            if bank_right not in stations:
                # Interpolate elevation at right bank
                bank_right_elev = np.interp(bank_right, stations, elevations)

                # Insert into DataFrame
                new_row = pd.DataFrame({'Station': [bank_right], 'Elevation': [bank_right_elev]})
                result_df = pd.concat([result_df, new_row], ignore_index=True)
                result_df = result_df.sort_values('Station').reset_index(drop=True)

                logger.debug(f"Interpolated right bank at station {bank_right:.2f}, elevation {bank_right_elev:.2f}")

        return result_df

    # ========== PUBLIC API METHODS ==========

    @staticmethod
    @log_call
    def get_cross_sections(geom_file: Union[str, Path],
                          river: Optional[str] = None,
                          reach: Optional[str] = None) -> pd.DataFrame:
        """
        Extract cross section metadata from geometry file.

        Parses all cross sections and returns their metadata including
        river, reach, river station, type, and reach lengths.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (Optional[str]): Filter by specific river name. If None, returns all rivers.
            reach (Optional[str]): Filter by specific reach name. If None, returns all reaches.
                                  Note: If reach is specified, river must also be specified.

        Returns:
            pd.DataFrame: DataFrame with columns:
                - River (str): River name
                - Reach (str): Reach name
                - RS (str): River station
                - Type (int): Cross section type (1=natural, etc.)
                - Length_Left (float): Left overbank length to next XS
                - Length_Channel (float): Channel length to next XS
                - Length_Right (float): Right overbank length to next XS
                - NodeName (str): Node name (if specified)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If reach specified without river

        Example:
            >>> # Get all cross sections
            >>> xs_df = GeomCrossSection.get_cross_sections("BaldEagle.g01")
            >>> print(f"Total XS: {len(xs_df)}")
            >>>
            >>> # Filter by river
            >>> xs_df = GeomCrossSection.get_cross_sections("BaldEagle.g01", river="Bald Eagle Creek")
            >>>
            >>> # Filter by river and reach
            >>> xs_df = GeomCrossSection.get_cross_sections("BaldEagle.g01",
            ...                                        river="Bald Eagle Creek",
            ...                                        reach="Reach 1")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        if reach is not None and river is None:
            raise ValueError("If reach is specified, river must also be specified")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            cross_sections = []
            current_river = None
            current_reach = None

            i = 0
            while i < len(lines):
                line = lines[i].strip()

                # Track current river/reach
                if line.startswith("River Reach="):
                    values = GeomParser.extract_comma_list(lines[i], "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]
                        logger.debug(f"Parsing {current_river} / {current_reach}")

                # Parse cross section metadata
                elif line.startswith("Type RM Length L Ch R ="):
                    if current_river is None or current_reach is None:
                        logger.warning(f"Found XS without river/reach at line {i}")
                        i += 1
                        continue

                    # Parse the metadata line
                    # Format: "Type RM Length L Ch R = TYPE, RS, Length_L, Length_Ch, Length_R"
                    value_str = GeomParser.extract_keyword_value(lines[i], "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]

                    if len(values) >= 4:
                        xs_type_code = int(values[0]) if values[0] else 1
                        rs = values[1]  # RS is second value, not first
                        try:
                            node_name = ""

                            # Look ahead for Node Name
                            j = i + 1
                            while j < len(lines) and j < i + 10:  # Look ahead max 10 lines
                                next_line = lines[j].strip()
                                if next_line.startswith("Node Name="):
                                    node_name = GeomParser.extract_keyword_value(lines[j], "Node Name")
                                if next_line.startswith("Type RM Length") or next_line.startswith("River Reach="):
                                    break
                                j += 1

                            # Use the type code we already extracted
                            xs_type = xs_type_code

                            # Lengths are values[2], values[3], values[4]
                            length_left = float(values[2]) if len(values) > 2 and values[2] else 0.0
                            length_channel = float(values[3]) if len(values) > 3 and values[3] else 0.0
                            length_right = float(values[4]) if len(values) > 4 and values[4] else 0.0

                            # Apply filters
                            if river is not None and current_river != river:
                                i += 1
                                continue
                            if reach is not None and current_reach != reach:
                                i += 1
                                continue

                            cross_sections.append({
                                'River': current_river,
                                'Reach': current_reach,
                                'RS': rs,
                                'Type': xs_type,
                                'Length_Left': length_left,
                                'Length_Channel': length_channel,
                                'Length_Right': length_right,
                                'NodeName': node_name
                            })

                        except (ValueError, IndexError) as e:
                            logger.warning(f"Error parsing XS at line {i}: {e}")

                i += 1

            df = pd.DataFrame(cross_sections)
            logger.info(f"Extracted {len(df)} cross sections from {geom_file.name}")

            if river is not None:
                logger.debug(f"Filtered to river '{river}': {len(df)} cross sections")
            if reach is not None:
                logger.debug(f"Filtered to reach '{reach}': {len(df)} cross sections")

            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error extracting cross sections: {str(e)}")
            raise IOError(f"Failed to extract cross sections: {str(e)}")

    @staticmethod
    @log_call
    def get_station_elevation(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str) -> pd.DataFrame:
        """
        Extract station/elevation pairs for a cross section.

        Reads the cross section geometry data from the plain text geometry file.
        Uses fixed-width parsing (8-character columns) following FORTRAN conventions.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name (case-sensitive)
            reach (str): Reach name (case-sensitive)
            rs (str): River station (as string, e.g., "138154.4")

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station along cross section (ft or m)
                - Elevation (float): Ground elevation at station (ft or m)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If cross section not found

        Example:
            >>> sta_elev = GeomCrossSection.get_station_elevation(
            ...     "BaldEagle.g01", "Bald Eagle Creek", "Reach 1", "138154.4"
            ... )
            >>> print(f"XS has {len(sta_elev)} points")
            >>> print(f"Station range: {sta_elev['Station'].min():.1f} to {sta_elev['Station'].max():.1f}")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = GeomCrossSection._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(
                    f"Cross section not found: {river}/{reach}/RS {rs} in {geom_file.name}"
                )

            # Find #Sta/Elev= line within search range
            for j in range(xs_idx, min(xs_idx + GeomCrossSection.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Sta/Elev="):
                    # Extract count
                    count_str = GeomParser.extract_keyword_value(lines[j], "#Sta/Elev")
                    count = int(count_str.strip())

                    logger.debug(f"#Sta/Elev= {count} (means {count} pairs)")

                    # Parse paired data using helper
                    df = GeomCrossSection._parse_paired_data(
                        lines, j + 1, count, 'Station', 'Elevation'
                    )

                    logger.info(
                        f"Extracted {len(df)} station/elevation pairs for "
                        f"{river}/{reach}/RS {rs}"
                    )

                    return df

            # If we get here, #Sta/Elev not found for this XS
            raise ValueError(
                f"#Sta/Elev data not found for {river}/{reach}/RS {rs}"
            )

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading station/elevation: {str(e)}")
            raise IOError(f"Failed to read station/elevation: {str(e)}")

    @staticmethod
    @log_call
    def set_station_elevation(geom_file: Union[str, Path],
                             river: str,
                             reach: str,
                             rs: str,
                             sta_elev_df: pd.DataFrame,
                             bank_left: Optional[float] = None,
                             bank_right: Optional[float] = None):
        """
        Write station/elevation pairs to a cross section with automatic bank interpolation.

        Modifies the geometry file in-place, replacing the station/elevation data and
        optionally updating bank stations. Creates a .bak backup automatically.

        CRITICAL REQUIREMENTS (HEC-RAS compatibility):
        - Bank stations MUST appear as exact points in station/elevation data
        - This method automatically interpolates elevations at bank locations
        - Maximum 450 points per cross section (HEC-RAS hard limit)

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station
            sta_elev_df (pd.DataFrame): DataFrame with 'Station' and 'Elevation' columns
            bank_left (Optional[float]): Left bank station. If provided, updates bank in file.
                                         If None, reads existing banks and interpolates them.
            bank_right (Optional[float]): Right bank station. If provided, updates bank in file.

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If cross section not found, DataFrame invalid, or >450 points
            IOError: If file write fails

        Example:
            >>> # Simple elevation modification (banks auto-interpolated)
            >>> sta_elev = GeomCrossSection.get_station_elevation(geom_file, river, reach, rs)
            >>> sta_elev['Elevation'] += 1.0
            >>> GeomCrossSection.set_station_elevation(geom_file, river, reach, rs, sta_elev)
            >>>
            >>> # Modify geometry AND change bank stations
            >>> GeomCrossSection.set_station_elevation(geom_file, river, reach, rs, sta_elev,
            ...                                   bank_left=200.0, bank_right=400.0)
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        # Validate DataFrame
        if not isinstance(sta_elev_df, pd.DataFrame):
            raise ValueError("sta_elev_df must be a pandas DataFrame")

        if 'Station' not in sta_elev_df.columns or 'Elevation' not in sta_elev_df.columns:
            raise ValueError("DataFrame must have 'Station' and 'Elevation' columns")

        if len(sta_elev_df) == 0:
            raise ValueError("DataFrame cannot be empty")

        # Validate banks if provided
        if bank_left is not None and bank_right is not None:
            if bank_left >= bank_right:
                raise ValueError(f"Left bank ({bank_left}) must be < right bank ({bank_right})")

        # Validate initial point count (before interpolation)
        if len(sta_elev_df) > GeomCrossSection.MAX_XS_POINTS:
            raise ValueError(
                f"Cross section has {len(sta_elev_df)} points, exceeds HEC-RAS limit of {GeomCrossSection.MAX_XS_POINTS} points.\n"
                f"Reduce point count by decimating or simplifying the cross section geometry."
            )

        try:
            # Create backup
            backup_path = GeomParser.create_backup(geom_file)
            logger.info(f"Created backup: {backup_path}")

            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            i = GeomCrossSection._find_cross_section(lines, river, reach, rs)

            if i is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            modified_lines = lines.copy()

            # Read existing bank stations if not provided (using helper)
            existing_banks = None
            if bank_left is None or bank_right is None:
                existing_banks = GeomCrossSection._read_bank_stations(lines, i)

            # Use provided banks or existing banks
            if existing_banks:
                existing_bank_left, existing_bank_right = existing_banks
            else:
                existing_bank_left = existing_bank_right = None

            final_bank_left = bank_left if bank_left is not None else existing_bank_left
            final_bank_right = bank_right if bank_right is not None else existing_bank_right

            # Interpolate at bank stations (HEC-RAS requirement)
            sta_elev_with_banks = GeomCrossSection._interpolate_at_banks(
                sta_elev_df, final_bank_left, final_bank_right
            )

            # Validate point count AFTER interpolation (HEC-RAS limit)
            if len(sta_elev_with_banks) > GeomCrossSection.MAX_XS_POINTS:
                raise ValueError(
                    f"Cross section would have {len(sta_elev_with_banks)} points after bank interpolation, "
                    f"exceeds HEC-RAS limit of {GeomCrossSection.MAX_XS_POINTS} points.\n"
                    f"Original points: {len(sta_elev_df)}, added by interpolation: "
                    f"{len(sta_elev_with_banks) - len(sta_elev_df)}.\n"
                    f"Reduce point count before writing."
                )

            # Validate stations are in ascending order
            if not sta_elev_with_banks['Station'].is_monotonic_increasing:
                raise ValueError("Stations must be in ascending order")

            logger.info(
                f"Prepared geometry: {len(sta_elev_with_banks)} points "
                f"(original: {len(sta_elev_df)}, interpolated: "
                f"{len(sta_elev_with_banks) - len(sta_elev_df)})"
            )

            # Find #Sta/Elev= line
            for j in range(i, min(i + GeomCrossSection.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Sta/Elev="):
                    # Extract old count
                    old_count_str = GeomParser.extract_keyword_value(lines[j], "#Sta/Elev")
                    old_count = int(old_count_str.strip())
                    old_total_values = GeomParser.interpret_count("#Sta/Elev", old_count)

                    # Calculate old data line count
                    old_data_lines = (old_total_values + GeomCrossSection.VALUES_PER_LINE - 1) // GeomCrossSection.VALUES_PER_LINE

                    # Prepare new data (using bank-interpolated DataFrame)
                    new_count = len(sta_elev_with_banks)

                    # Interleave station and elevation
                    new_values = []
                    for _, row in sta_elev_with_banks.iterrows():
                        new_values.append(row['Station'])
                        new_values.append(row['Elevation'])

                    # Format new data lines using constants
                    new_data_lines = GeomParser.format_fixed_width(
                        new_values,
                        column_width=GeomCrossSection.FIXED_WIDTH_COLUMN,
                        values_per_line=GeomCrossSection.VALUES_PER_LINE,
                        precision=2
                    )

                    # Update count line
                    modified_lines[j] = f"#Sta/Elev= {new_count}\n"

                    # Replace data lines
                    # Remove old data lines
                    for k in range(old_data_lines):
                        if j + 1 + k < len(modified_lines):
                            modified_lines[j + 1 + k] = None  # Mark for deletion

                    # Insert new data lines
                    for k, data_line in enumerate(new_data_lines):
                        if j + 1 + k < len(modified_lines):
                            modified_lines[j + 1 + k] = data_line
                        else:
                            # Append if needed
                            modified_lines.append(data_line)

                    # Clean up None entries
                    modified_lines = [line for line in modified_lines if line is not None]

                    # Update Bank Sta= line if new banks provided
                    if bank_left is not None and bank_right is not None:
                        # Find Bank Sta= line in the modified lines
                        bank_sta_updated = False
                        for k in range(i, min(i + GeomCrossSection.DEFAULT_SEARCH_RANGE, len(modified_lines))):
                            if modified_lines[k].startswith("Bank Sta="):
                                # Update with new bank stations (format: no spaces after comma)
                                modified_lines[k] = f"Bank Sta={bank_left:g},{bank_right:g}\n"
                                bank_sta_updated = True
                                logger.debug(f"Updated Bank Sta= line: {bank_left:g},{bank_right:g}")
                                break

                        if not bank_sta_updated:
                            logger.warning(f"Bank Sta= line not found for XS {rs}, banks not updated in file")

                    # Write modified file
                    with open(geom_file, 'w') as f:
                        f.writelines(modified_lines)

                    logger.info(
                        f"Updated station/elevation for {river}/{reach}/RS {rs}: "
                        f"{new_count} pairs written"
                    )

                    if bank_left is not None and bank_right is not None:
                        logger.info(f"Updated bank stations: {bank_left:g}, {bank_right:g}")

                    return

            raise ValueError(
                f"#Sta/Elev data not found for {river}/{reach}/RS {rs}"
            )

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error writing station/elevation: {str(e)}")
            # Attempt to restore from backup if write failed
            if backup_path and backup_path.exists():
                logger.info(f"Restoring from backup: {backup_path}")
                import shutil
                shutil.copy2(backup_path, geom_file)
            raise IOError(f"Failed to write station/elevation: {str(e)}")

    @staticmethod
    @log_call
    def get_bank_stations(geom_file: Union[str, Path],
                         river: str,
                         reach: str,
                         rs: str) -> Optional[Tuple[float, float]]:
        """
        Extract left and right bank station locations for a cross section.

        Bank stations define the boundary between overbank areas and the main channel,
        used for subsection conveyance calculations.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            Optional[Tuple[float, float]]: (left_bank, right_bank) or None if no banks defined

        Example:
            >>> banks = GeomCrossSection.get_bank_stations("BaldEagle.g01", "Bald Eagle", "Loc Hav", "138154.4")
            >>> if banks:
            ...     left, right = banks
            ...     print(f"Bank stations: Left={left}, Right={right}")
            ...     print(f"Main channel width: {right - left} ft")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = GeomCrossSection._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            # Read bank stations using helper
            banks = GeomCrossSection._read_bank_stations(lines, xs_idx)

            if banks:
                left_bank, right_bank = banks
                logger.info(f"Extracted bank stations for {river}/{reach}/RS {rs}: {left_bank}, {right_bank}")
                return banks
            else:
                logger.info(f"No bank stations found for {river}/{reach}/RS {rs}")
                return None

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading bank stations: {str(e)}")
            raise IOError(f"Failed to read bank stations: {str(e)}")

    @staticmethod
    @log_call
    def get_expansion_contraction(geom_file: Union[str, Path],
                                  river: str,
                                  reach: str,
                                  rs: str) -> Tuple[float, float]:
        """
        Extract expansion and contraction coefficients for a cross section.

        These coefficients account for energy losses due to flow expansion
        (downstream) and contraction (upstream) at cross sections.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            Tuple[float, float]: (expansion, contraction) coefficients

        Example:
            >>> exp, cntr = GeomCrossSection.get_expansion_contraction(
            ...     "BaldEagle.g01", "Bald Eagle", "Loc Hav", "138154.4"
            ... )
            >>> print(f"Expansion: {exp}, Contraction: {cntr}")
            >>> # Typical values: expansion=0.3, contraction=0.1
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = GeomCrossSection._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            # Find Exp/Cntr= line within search range
            for j in range(xs_idx, min(xs_idx + GeomCrossSection.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("Exp/Cntr="):
                    exp_cntr_str = GeomParser.extract_keyword_value(lines[j], "Exp/Cntr")
                    values = [v.strip() for v in exp_cntr_str.split(',')]

                    if len(values) >= 2:
                        expansion = float(values[0])
                        contraction = float(values[1])

                        logger.info(
                            f"Extracted expansion/contraction for {river}/{reach}/RS {rs}: "
                            f"{expansion}, {contraction}"
                        )
                        return (expansion, contraction)

            # XS found but no Exp/Cntr= (use defaults)
            logger.info(f"No Exp/Cntr found for {river}/{reach}/RS {rs}, using defaults")
            return (0.3, 0.1)  # HEC-RAS defaults

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading expansion/contraction: {str(e)}")
            raise IOError(f"Failed to read expansion/contraction: {str(e)}")

    @staticmethod
    @log_call
    def get_mannings_n(geom_file: Union[str, Path],
                      river: str,
                      reach: str,
                      rs: str) -> pd.DataFrame:
        """
        Extract Manning's n roughness values for a cross section.

        Manning's n values define channel roughness and are organized by subsections
        (Left Overbank, Main Channel, Right Overbank) based on bank station locations.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station where this Manning's n value starts
                - n_value (float): Manning's roughness coefficient
                - Subsection (str): 'LOB' (Left Overbank), 'Channel', or 'ROB' (Right Overbank)

        Example:
            >>> mann = GeomCrossSection.get_mannings_n("BaldEagle.g01", "Bald Eagle", "Loc Hav", "138154.4")
            >>> print(mann)
               Station  n_value Subsection
            0      0.0     0.06        LOB
            1    190.0     0.04    Channel
            2    375.0     0.10        ROB
            >>>
            >>> # Calculate average channel Manning's n
            >>> channel_n = mann[mann['Subsection'] == 'Channel']['n_value'].mean()
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the cross section using helper
            xs_idx = GeomCrossSection._find_cross_section(lines, river, reach, rs)

            if xs_idx is None:
                raise ValueError(f"Cross section not found: {river}/{reach}/RS {rs}")

            # Get bank stations using helper (for subsection classification)
            banks = GeomCrossSection._read_bank_stations(lines, xs_idx)
            bank_left = bank_right = None
            if banks:
                bank_left, bank_right = banks

            # Find #Mann= line
            for j in range(xs_idx, min(xs_idx + GeomCrossSection.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Mann="):
                    # Extract count
                    mann_str = GeomParser.extract_keyword_value(lines[j], "#Mann")
                    count_values = [v.strip() for v in mann_str.split(',')]

                    num_segments = int(count_values[0]) if count_values[0] else 0
                    format_flag = int(count_values[1]) if len(count_values) > 1 and count_values[1] else 0

                    logger.debug(f"Manning's n: {num_segments} segments, format={format_flag}")

                    # Calculate total values to read (triplets)
                    total_values = num_segments * 3

                    # Parse Manning's n data using helper (note: max_lines=20 for Manning's n)
                    values = GeomCrossSection._parse_data_block(
                        lines, j + 1, total_values,
                        column_width=GeomCrossSection.FIXED_WIDTH_COLUMN,
                        max_lines=20
                    )

                    # Convert triplets to DataFrame
                    segments = []
                    for seg_idx in range(0, len(values), 3):
                        if seg_idx + 2 < len(values):
                            station = values[seg_idx]
                            n_value = values[seg_idx + 1]
                            # values[seg_idx + 2] is always 0, ignore

                            # Classify subsection based on bank stations
                            if bank_left is not None and bank_right is not None:
                                if station < bank_left:
                                    subsection = 'LOB'
                                elif station < bank_right:
                                    subsection = 'Channel'
                                else:
                                    subsection = 'ROB'
                            else:
                                subsection = 'Unknown'

                            segments.append({
                                'Station': station,
                                'n_value': n_value,
                                'Subsection': subsection
                            })

                    df = pd.DataFrame(segments)

                    logger.info(
                        f"Extracted {len(df)} Manning's n segments for {river}/{reach}/RS {rs}"
                    )

                    return df

            # XS found but no Manning's n
            raise ValueError(f"No Manning's n data found for {river}/{reach}/RS {rs}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading Manning's n: {str(e)}")
            raise IOError(f"Failed to read Manning's n: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomCulvert.py
==================================================
"""
GeomCulvert - Culvert operations for HEC-RAS geometry files

This module provides functionality for reading culvert structure data
from HEC-RAS plain text geometry files (.g##).

All methods are static and designed to be used without instantiation.

List of Functions:
- get_culverts() - List all culverts at a bridge/culvert structure
- get_all() - List all culverts across entire geometry file

Example Usage:
    >>> from ras_commander import GeomCulvert
    >>> from pathlib import Path
    >>>
    >>> # List all culverts at a specific structure
    >>> geom_file = Path("model.g08")
    >>> culverts_df = GeomCulvert.get_culverts(geom_file, "River", "Reach", "23367")
    >>> print(f"Found {len(culverts_df)} culverts")
    >>>
    >>> # List all culverts in geometry file
    >>> all_culverts = GeomCulvert.get_all(geom_file)
    >>> print(all_culverts.groupby('ShapeName').size())

Technical Notes:
    - Culvert shape codes: 1=Circular, 2=Box, 3=Pipe Arch, 4=Ellipse, 5=Arch,
      6=Semi-Circle, 7=Low Profile Arch, 8=High Profile Arch, 9=Con Span
"""

from pathlib import Path
from typing import Union, Optional, List
import pandas as pd

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser

logger = get_logger(__name__)


class GeomCulvert:
    """
    Operations for parsing HEC-RAS culverts in geometry files.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8
    DEFAULT_SEARCH_RANGE = 200

    # Culvert shape codes
    CULVERT_SHAPES = {
        1: 'Circular',
        2: 'Box',
        3: 'Pipe Arch',
        4: 'Ellipse',
        5: 'Arch',
        6: 'Semi-Circle',
        7: 'Low Profile Arch',
        8: 'High Profile Arch',
        9: 'Con Span'
    }

    @staticmethod
    def _find_bridge(lines: List[str], river: str, reach: str, rs: str) -> Optional[int]:
        """Find bridge/culvert section and return line index."""
        current_river = None
        current_reach = None
        last_rs = None

        for i, line in enumerate(lines):
            if line.startswith("River Reach="):
                values = GeomParser.extract_comma_list(line, "River Reach")
                if len(values) >= 2:
                    current_river = values[0]
                    current_reach = values[1]

            elif line.startswith("Type RM Length L Ch R ="):
                value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                values = [v.strip() for v in value_str.split(',')]
                if len(values) > 1:
                    last_rs = values[1]

            elif line.startswith("Bridge Culvert-"):
                if (current_river == river and
                    current_reach == reach and
                    last_rs == rs):
                    return i

        return None

    @staticmethod
    @log_call
    def get_culverts(geom_file: Union[str, Path],
                    river: str,
                    reach: str,
                    rs: str) -> pd.DataFrame:
        """
        List all culverts at a bridge/culvert structure.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - CulvertName: Culvert identifier (e.g., "Culvert #1")
            - Shape: Shape code (1=Circular, 2=Box, etc.)
            - ShapeName: Human-readable shape name
            - Span: Width/diameter (feet or meters)
            - Rise: Height (feet or meters)
            - Length: Culvert length
            - ManningsN: Manning's roughness coefficient
            - EntranceLoss: Entrance loss coefficient (Ke)
            - ExitLoss: Exit loss coefficient
            - InletType: Inlet control type code
            - OutletType: Outlet control type code
            - UpstreamInvert: Upstream invert elevation
            - UpstreamStation: Upstream station location
            - DownstreamInvert: Downstream invert elevation
            - DownstreamStation: Downstream station location
            - ChartNumber: Inlet control chart number
            - BottomN: Bottom Manning's n (if different)
            - NumBarrels: Number of barrels

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If bridge/culvert structure not found

        Example:
            >>> culverts = GeomCulvert.get_culverts(
            ...     "model.g08", "River", "Reach", "23367"
            ... )
            >>> print(f"Found {len(culverts)} culverts")
            >>> print(culverts[['CulvertName', 'ShapeName', 'Span', 'Rise', 'Length']])
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            bridge_idx = GeomCulvert._find_bridge(lines, river, reach, rs)

            if bridge_idx is None:
                raise ValueError(f"Bridge/culvert not found: {river}/{reach}/RS {rs}")

            culverts = []

            i = bridge_idx
            while i < min(bridge_idx + GeomCulvert.DEFAULT_SEARCH_RANGE * 2, len(lines)):
                line = lines[i]

                if line.startswith("Type RM Length L Ch R =") and i > bridge_idx + 5:
                    break

                if line.startswith("Culvert="):
                    val = GeomParser.extract_keyword_value(line, "Culvert")
                    parts = [p.strip() for p in val.split(',')]

                    culvert_data = {
                        'CulvertName': None,
                        'Shape': None,
                        'ShapeName': None,
                        'Span': None,
                        'Rise': None,
                        'Length': None,
                        'ManningsN': None,
                        'EntranceLoss': None,
                        'ExitLoss': None,
                        'InletType': None,
                        'OutletType': None,
                        'UpstreamInvert': None,
                        'UpstreamStation': None,
                        'DownstreamInvert': None,
                        'DownstreamStation': None,
                        'ChartNumber': None,
                        'BottomN': None,
                        'NumBarrels': 1
                    }

                    if len(parts) > 0 and parts[0]:
                        try:
                            shape = int(parts[0])
                            culvert_data['Shape'] = shape
                            culvert_data['ShapeName'] = GeomCulvert.CULVERT_SHAPES.get(shape, f'Unknown ({shape})')
                        except: pass
                    if len(parts) > 1 and parts[1]:
                        try: culvert_data['Span'] = float(parts[1])
                        except: pass
                    if len(parts) > 2 and parts[2]:
                        try: culvert_data['Rise'] = float(parts[2])
                        except: pass
                    if len(parts) > 3 and parts[3]:
                        try: culvert_data['Length'] = float(parts[3])
                        except: pass
                    if len(parts) > 4 and parts[4]:
                        try: culvert_data['ManningsN'] = float(parts[4])
                        except: pass
                    if len(parts) > 5 and parts[5]:
                        try: culvert_data['EntranceLoss'] = float(parts[5])
                        except: pass
                    if len(parts) > 6 and parts[6]:
                        try: culvert_data['ExitLoss'] = float(parts[6])
                        except: pass
                    if len(parts) > 7 and parts[7]:
                        try: culvert_data['InletType'] = int(parts[7])
                        except: pass
                    if len(parts) > 8 and parts[8]:
                        try: culvert_data['OutletType'] = int(parts[8])
                        except: pass
                    if len(parts) > 9 and parts[9]:
                        try: culvert_data['UpstreamInvert'] = float(parts[9])
                        except: pass
                    if len(parts) > 10 and parts[10]:
                        try: culvert_data['UpstreamStation'] = float(parts[10])
                        except: pass
                    if len(parts) > 11 and parts[11]:
                        try: culvert_data['DownstreamInvert'] = float(parts[11])
                        except: pass
                    if len(parts) > 12 and parts[12]:
                        try: culvert_data['DownstreamStation'] = float(parts[12])
                        except: pass
                    if len(parts) > 13 and parts[13]:
                        culvert_data['CulvertName'] = parts[13].strip()
                    if len(parts) > 15 and parts[15]:
                        try: culvert_data['ChartNumber'] = int(parts[15])
                        except: pass

                    # Look for additional culvert parameters
                    for k in range(i + 1, min(i + 5, len(lines))):
                        next_line = lines[k]

                        if next_line.startswith("BC Culvert Barrel="):
                            barrel_val = GeomParser.extract_keyword_value(next_line, "BC Culvert Barrel")
                            barrel_parts = [p.strip() for p in barrel_val.split(',')]
                            if len(barrel_parts) > 0 and barrel_parts[0]:
                                try: culvert_data['NumBarrels'] = int(barrel_parts[0])
                                except: pass

                        elif next_line.startswith("Culvert Bottom n="):
                            bottom_n = GeomParser.extract_keyword_value(next_line, "Culvert Bottom n")
                            if bottom_n:
                                try: culvert_data['BottomN'] = float(bottom_n)
                                except: pass

                        elif next_line.startswith("Culvert="):
                            break

                    culverts.append(culvert_data)

                i += 1

            if not culverts:
                logger.info(f"No culverts found at {river}/{reach}/RS {rs}")
                return pd.DataFrame()

            df = pd.DataFrame(culverts)
            logger.info(f"Found {len(df)} culverts at {river}/{reach}/RS {rs}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading culverts: {str(e)}")
            raise IOError(f"Failed to read culverts: {str(e)}")

    @staticmethod
    @log_call
    def get_all(geom_file: Union[str, Path],
               river: Optional[str] = None,
               reach: Optional[str] = None) -> pd.DataFrame:
        """
        List all culverts in geometry file across all bridge/culvert structures.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: Optional filter by river name (case-sensitive)
            reach: Optional filter by reach name (case-sensitive)

        Returns:
            pd.DataFrame with all culvert data plus River, Reach, RS columns

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> all_culverts = GeomCulvert.get_all("model.g08")
            >>> print(f"Found {len(all_culverts)} total culverts")
            >>> # Group by shape
            >>> print(all_culverts.groupby('ShapeName').size())
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            all_culverts = []
            current_river = None
            current_reach = None
            last_rs = None

            i = 0
            while i < len(lines):
                line = lines[i]

                if line.startswith("River Reach="):
                    values = GeomParser.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                elif line.startswith("Type RM Length L Ch R ="):
                    value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]
                    if len(values) > 1:
                        last_rs = values[1]

                elif line.startswith("Culvert="):
                    if river is not None and current_river != river:
                        i += 1
                        continue
                    if reach is not None and current_reach != reach:
                        i += 1
                        continue

                    val = GeomParser.extract_keyword_value(line, "Culvert")
                    parts = [p.strip() for p in val.split(',')]

                    culvert_data = {
                        'River': current_river,
                        'Reach': current_reach,
                        'RS': last_rs,
                        'CulvertName': None,
                        'Shape': None,
                        'ShapeName': None,
                        'Span': None,
                        'Rise': None,
                        'Length': None,
                        'ManningsN': None,
                        'EntranceLoss': None,
                        'UpstreamInvert': None,
                        'DownstreamInvert': None
                    }

                    if len(parts) > 0 and parts[0]:
                        try:
                            shape = int(parts[0])
                            culvert_data['Shape'] = shape
                            culvert_data['ShapeName'] = GeomCulvert.CULVERT_SHAPES.get(shape, f'Unknown ({shape})')
                        except: pass
                    if len(parts) > 1 and parts[1]:
                        try: culvert_data['Span'] = float(parts[1])
                        except: pass
                    if len(parts) > 2 and parts[2]:
                        try: culvert_data['Rise'] = float(parts[2])
                        except: pass
                    if len(parts) > 3 and parts[3]:
                        try: culvert_data['Length'] = float(parts[3])
                        except: pass
                    if len(parts) > 4 and parts[4]:
                        try: culvert_data['ManningsN'] = float(parts[4])
                        except: pass
                    if len(parts) > 5 and parts[5]:
                        try: culvert_data['EntranceLoss'] = float(parts[5])
                        except: pass
                    if len(parts) > 9 and parts[9]:
                        try: culvert_data['UpstreamInvert'] = float(parts[9])
                        except: pass
                    if len(parts) > 11 and parts[11]:
                        try: culvert_data['DownstreamInvert'] = float(parts[11])
                        except: pass
                    if len(parts) > 13 and parts[13]:
                        culvert_data['CulvertName'] = parts[13].strip()

                    all_culverts.append(culvert_data)

                i += 1

            df = pd.DataFrame(all_culverts)
            logger.info(f"Found {len(df)} total culverts in {geom_file.name}")
            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading all culverts: {str(e)}")
            raise IOError(f"Failed to read all culverts: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomInlineWeir.py
==================================================
"""
GeomInlineWeir - Inline weir operations for HEC-RAS geometry files

This module provides functionality for reading inline weir structure data
from HEC-RAS plain text geometry files (.g##).

All methods are static and designed to be used without instantiation.

List of Functions:
- get_weirs() - List all inline weirs with metadata
- get_profile() - Read station/elevation profile for weir crest
- get_gates() - Read gate parameters and opening definitions

Example Usage:
    >>> from ras_commander import GeomInlineWeir
    >>> from pathlib import Path
    >>>
    >>> # List all inline weirs
    >>> geom_file = Path("BaldEagle.g01")
    >>> weirs_df = GeomInlineWeir.get_weirs(geom_file)
    >>> print(f"Found {len(weirs_df)} inline weirs")
    >>>
    >>> # Get weir profile for specific inline weir
    >>> profile = GeomInlineWeir.get_profile(
    ...     geom_file, "Bald Eagle Creek", "Reach 1", "81084.18"
    ... )
    >>> print(profile.head())

Technical Notes:
    - Uses FORTRAN-era fixed-width format (8-char columns for numeric data)
    - Count interpretation: "#Inline Weir SE= 6" means 6 PAIRS (12 total values)
"""

from pathlib import Path
from typing import Union, Optional, List
import pandas as pd

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser

logger = get_logger(__name__)


class GeomInlineWeir:
    """
    Operations for parsing HEC-RAS inline weirs in geometry files.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8      # Character width for numeric data in geometry files
    VALUES_PER_LINE = 10        # Number of values per line in fixed-width format
    DEFAULT_SEARCH_RANGE = 100  # Lines to search for keywords after structure header

    @staticmethod
    def _find_inline_weir(lines: List[str], river: str, reach: str, rs: str) -> Optional[int]:
        """
        Find inline weir section and return line index of 'IW Pilot Flow=' marker.

        Args:
            lines: File lines (from readlines())
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string, e.g., "81084.18")

        Returns:
            Line index where "IW Pilot Flow=" appears for matching inline weir,
            or None if not found
        """
        current_river = None
        current_reach = None
        last_rs = None

        for i, line in enumerate(lines):
            # Track current river/reach
            if line.startswith("River Reach="):
                values = GeomParser.extract_comma_list(line, "River Reach")
                if len(values) >= 2:
                    current_river = values[0]
                    current_reach = values[1]

            # Track most recent Type RM Length line (contains RS)
            elif line.startswith("Type RM Length L Ch R ="):
                value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                values = [v.strip() for v in value_str.split(',')]
                if len(values) > 1:
                    last_rs = values[1]  # RS is second value

            # Find IW Pilot Flow marker (start of inline weir)
            elif line.startswith("IW Pilot Flow="):
                if (current_river == river and
                    current_reach == reach and
                    last_rs == rs):
                    logger.debug(f"Found inline weir at line {i}: {river}/{reach}/RS {rs}")
                    return i

        logger.debug(f"Inline weir not found: {river}/{reach}/RS {rs}")
        return None

    @staticmethod
    def _parse_paired_data(lines: List[str], start_idx: int, num_pairs: int,
                          col1_name: str, col2_name: str) -> pd.DataFrame:
        """Parse fixed-width paired data into DataFrame."""
        total_values = num_pairs * 2
        values = []

        i = start_idx
        while len(values) < total_values and i < len(lines):
            line = lines[i]
            if '=' in line and not line.strip().startswith('-'):
                break
            parsed = GeomParser.parse_fixed_width(line, GeomInlineWeir.FIXED_WIDTH_COLUMN)
            values.extend(parsed)
            i += 1

        col1_data = []
        col2_data = []
        for j in range(0, min(len(values), total_values), 2):
            if j + 1 < len(values):
                col1_data.append(values[j])
                col2_data.append(values[j + 1])

        return pd.DataFrame({col1_name: col1_data, col2_name: col2_data})

    @staticmethod
    @log_call
    def get_weirs(geom_file: Union[str, Path],
                 river: Optional[str] = None,
                 reach: Optional[str] = None) -> pd.DataFrame:
        """
        List all inline weirs in geometry file with metadata.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: Optional filter by river name (case-sensitive)
            reach: Optional filter by reach name (case-sensitive)

        Returns:
            pd.DataFrame with columns:
            - River, Reach, RS: Location identifiers
            - NodeName: Descriptive name (if available)
            - PilotFlow: Pilot flow flag (0/1)
            - Distance, Width, Coefficient, Skew: Weir parameters
            - MaxSubmergence, MinElevation, IsOgee: Additional parameters
            - SpillwayHeight, DesignHead: Design parameters
            - HasGate: Boolean indicating if gates are present
            - NumOpenings: Number of gate openings (if gates present)

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> weirs = GeomInlineWeir.get_weirs("BaldEagle.g01")
            >>> print(f"Found {len(weirs)} inline weirs")
            >>> print(weirs[['River', 'Reach', 'RS', 'Coefficient']])
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            inline_weirs = []
            current_river = None
            current_reach = None
            last_rs = None
            last_node_name = None

            i = 0
            while i < len(lines):
                line = lines[i]

                # Track current river/reach
                if line.startswith("River Reach="):
                    values = GeomParser.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                # Track RS from Type line
                elif line.startswith("Type RM Length L Ch R ="):
                    value_str = GeomParser.extract_keyword_value(line, "Type RM Length L Ch R")
                    values = [v.strip() for v in value_str.split(',')]
                    if len(values) > 1:
                        last_rs = values[1]

                # Track node name
                elif line.startswith("Node Name="):
                    last_node_name = GeomParser.extract_keyword_value(line, "Node Name")

                # Found inline weir
                elif line.startswith("IW Pilot Flow="):
                    # Apply filters
                    if river is not None and current_river != river:
                        i += 1
                        continue
                    if reach is not None and current_reach != reach:
                        i += 1
                        continue

                    # Extract pilot flow
                    pilot_flow_str = GeomParser.extract_keyword_value(line, "IW Pilot Flow")
                    pilot_flow = int(pilot_flow_str.strip()) if pilot_flow_str.strip() else 0

                    weir_data = {
                        'River': current_river,
                        'Reach': current_reach,
                        'RS': last_rs,
                        'NodeName': last_node_name,
                        'PilotFlow': pilot_flow,
                        'Distance': None,
                        'Width': None,
                        'Coefficient': None,
                        'Skew': None,
                        'MaxSubmergence': None,
                        'MinElevation': None,
                        'IsOgee': None,
                        'SpillwayHeight': None,
                        'DesignHead': None,
                        'HasGate': False,
                        'NumOpenings': 0
                    }

                    # Search for weir parameters in next ~50 lines
                    for j in range(i + 1, min(i + 50, len(lines))):
                        search_line = lines[j]

                        # Parse weir parameters (line after header)
                        if search_line.startswith("IW Dist,WD,Coef,"):
                            if j + 1 < len(lines):
                                param_line = lines[j + 1]
                                parts = [p.strip() for p in param_line.split(',')]

                                if len(parts) > 0 and parts[0]:
                                    try: weir_data['Distance'] = float(parts[0])
                                    except: pass
                                if len(parts) > 1 and parts[1]:
                                    try: weir_data['Width'] = float(parts[1])
                                    except: pass
                                if len(parts) > 2 and parts[2]:
                                    try: weir_data['Coefficient'] = float(parts[2])
                                    except: pass
                                if len(parts) > 3 and parts[3]:
                                    try: weir_data['Skew'] = float(parts[3])
                                    except: pass
                                if len(parts) > 4 and parts[4]:
                                    try: weir_data['MaxSubmergence'] = float(parts[4])
                                    except: pass
                                if len(parts) > 5 and parts[5]:
                                    try: weir_data['MinElevation'] = float(parts[5])
                                    except: pass
                                if len(parts) > 6 and parts[6]:
                                    try: weir_data['IsOgee'] = int(parts[6])
                                    except: pass
                                if len(parts) > 7 and parts[7]:
                                    try: weir_data['SpillwayHeight'] = float(parts[7])
                                    except: pass
                                if len(parts) > 8 and parts[8]:
                                    try: weir_data['DesignHead'] = float(parts[8])
                                    except: pass

                        # Check for gate presence
                        elif search_line.startswith("IW Gate Name Wd,"):
                            weir_data['HasGate'] = True
                            if j + 1 < len(lines):
                                gate_line = lines[j + 1]
                                parts = [p.strip() for p in gate_line.split(',')]
                                if len(parts) > 13 and parts[13]:
                                    try:
                                        weir_data['NumOpenings'] = int(parts[13])
                                    except:
                                        pass

                        # Stop at next structure
                        elif search_line.startswith("Type RM Length L Ch R ="):
                            break

                    inline_weirs.append(weir_data)
                    last_node_name = None

                i += 1

            df = pd.DataFrame(inline_weirs)
            logger.info(f"Found {len(df)} inline weirs in {geom_file.name}")
            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading inline weirs: {str(e)}")
            raise IOError(f"Failed to read inline weirs: {str(e)}")

    @staticmethod
    @log_call
    def get_profile(geom_file: Union[str, Path],
                   river: str,
                   reach: str,
                   rs: str) -> pd.DataFrame:
        """
        Extract weir crest station/elevation profile for an inline weir.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - Station: Station values along weir crest
            - Elevation: Elevation values at each station

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If inline weir not found

        Example:
            >>> profile = GeomInlineWeir.get_profile(
            ...     "BaldEagle.g01", "Bald Eagle Creek", "Reach 1", "81084.18"
            ... )
            >>> print(f"Profile has {len(profile)} points")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            weir_idx = GeomInlineWeir._find_inline_weir(lines, river, reach, rs)

            if weir_idx is None:
                raise ValueError(f"Inline weir not found: {river}/{reach}/RS {rs}")

            for j in range(weir_idx, min(weir_idx + GeomInlineWeir.DEFAULT_SEARCH_RANGE, len(lines))):
                line = lines[j]

                if line.startswith("#Inline Weir SE="):
                    count_str = GeomParser.extract_keyword_value(line, "#Inline Weir SE")
                    count = int(count_str.strip())

                    df = GeomInlineWeir._parse_paired_data(
                        lines, j + 1, count, 'Station', 'Elevation'
                    )

                    logger.info(f"Extracted {len(df)} profile points for {river}/{reach}/RS {rs}")
                    return df

            raise ValueError(f"#Inline Weir SE= not found for {river}/{reach}/RS {rs}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading inline weir profile: {str(e)}")
            raise IOError(f"Failed to read inline weir profile: {str(e)}")

    @staticmethod
    @log_call
    def get_gates(geom_file: Union[str, Path],
                 river: str,
                 reach: str,
                 rs: str) -> pd.DataFrame:
        """
        Extract gate parameters and opening definitions for an inline weir.

        Parameters:
            geom_file: Path to geometry file (.g##)
            river: River name (case-sensitive)
            reach: Reach name (case-sensitive)
            rs: River station (as string)

        Returns:
            pd.DataFrame with columns:
            - GateName: Gate identifier (e.g., "Gate #1")
            - Width, Height, InvertElevation: Gate dimensions
            - GateCoefficient: Flow coefficient
            - ExpansionTop, ExpansionOrifice, ExpansionHydraulic: Expansion coefficients
            - GateType: Gate type code
            - WeirCoefficient, IsOgee: Weir parameters
            - SpillwayHeight, DesignHead: Design parameters
            - NumOpenings: Number of gate openings
            - OpeningStations: List of station values for each opening

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If inline weir not found or has no gates

        Example:
            >>> gates = GeomInlineWeir.get_gates(
            ...     "BaldEagle.g01", "Bald Eagle Creek", "Reach 1", "81084.18"
            ... )
            >>> print(f"Found {len(gates)} gates")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            weir_idx = GeomInlineWeir._find_inline_weir(lines, river, reach, rs)

            if weir_idx is None:
                raise ValueError(f"Inline weir not found: {river}/{reach}/RS {rs}")

            gates = []

            i = weir_idx
            while i < min(weir_idx + GeomInlineWeir.DEFAULT_SEARCH_RANGE, len(lines)):
                line = lines[i]

                if line.startswith("Type RM Length L Ch R =") and i > weir_idx + 5:
                    break

                if line.startswith("IW Gate Name Wd,"):
                    if i + 1 < len(lines):
                        gate_line = lines[i + 1]
                        parts = [p.strip() for p in gate_line.split(',')]

                        gate_data = {
                            'GateName': parts[0] if len(parts) > 0 else None,
                            'Width': None,
                            'Height': None,
                            'InvertElevation': None,
                            'GateCoefficient': None,
                            'ExpansionTop': None,
                            'ExpansionOrifice': None,
                            'ExpansionHydraulic': None,
                            'GateType': None,
                            'WeirCoefficient': None,
                            'IsOgee': None,
                            'SpillwayHeight': None,
                            'DesignHead': None,
                            'NumOpenings': 0,
                            'OpeningStations': []
                        }

                        if len(parts) > 1 and parts[1]:
                            try: gate_data['Width'] = float(parts[1])
                            except: pass
                        if len(parts) > 2 and parts[2]:
                            try: gate_data['Height'] = float(parts[2])
                            except: pass
                        if len(parts) > 3 and parts[3]:
                            try: gate_data['InvertElevation'] = float(parts[3])
                            except: pass
                        if len(parts) > 4 and parts[4]:
                            try: gate_data['GateCoefficient'] = float(parts[4])
                            except: pass
                        if len(parts) > 5 and parts[5]:
                            try: gate_data['ExpansionTop'] = float(parts[5])
                            except: pass
                        if len(parts) > 6 and parts[6]:
                            try: gate_data['ExpansionOrifice'] = float(parts[6])
                            except: pass
                        if len(parts) > 7 and parts[7]:
                            try: gate_data['ExpansionHydraulic'] = float(parts[7])
                            except: pass
                        if len(parts) > 8 and parts[8]:
                            try: gate_data['GateType'] = float(parts[8])
                            except: pass
                        if len(parts) > 9 and parts[9]:
                            try: gate_data['WeirCoefficient'] = float(parts[9])
                            except: pass
                        if len(parts) > 10 and parts[10]:
                            try: gate_data['IsOgee'] = int(parts[10])
                            except: pass
                        if len(parts) > 11 and parts[11]:
                            try: gate_data['SpillwayHeight'] = float(parts[11])
                            except: pass
                        if len(parts) > 12 and parts[12]:
                            try: gate_data['DesignHead'] = float(parts[12])
                            except: pass
                        if len(parts) > 13 and parts[13]:
                            try: gate_data['NumOpenings'] = int(parts[13])
                            except: pass

                        num_openings = gate_data['NumOpenings']
                        if num_openings > 0 and i + 2 < len(lines):
                            station_line = lines[i + 2]
                            if '=' not in station_line:
                                stations = GeomParser.parse_fixed_width(station_line, 8)
                                gate_data['OpeningStations'] = stations[:num_openings]

                        gates.append(gate_data)
                        i += 2

                i += 1

            if not gates:
                raise ValueError(f"No gates found for inline weir: {river}/{reach}/RS {rs}")

            df = pd.DataFrame(gates)
            logger.info(f"Extracted {len(df)} gates for {river}/{reach}/RS {rs}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading inline weir gates: {str(e)}")
            raise IOError(f"Failed to read inline weir gates: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomLandCover.py
==================================================
"""
GeomLandCover - 2D Manning's n land cover operations

This module provides functionality for reading and modifying Manning's n
roughness values for 2D flow areas in HEC-RAS geometry files. These values
are associated with land cover classifications.

All methods are static and designed to be used without instantiation.

List of Functions:
- get_base_mannings_n() - Read base Manning's n table from geometry file
- set_base_mannings_n() - Write base Manning's n values to geometry file
- get_region_mannings_n() - Read Manning's n region overrides
- set_region_mannings_n() - Write regional Manning's n overrides

Example Usage:
    >>> from ras_commander import GeomLandCover, RasPlan
    >>>
    >>> # Get base Manning's n values
    >>> geom_path = RasPlan.get_geom_path("01")
    >>> mannings_df = GeomLandCover.get_base_mannings_n(geom_path)
    >>> print(mannings_df)
    >>>
    >>> # Modify and write back
    >>> mannings_df['Base Mannings n Value'] *= 1.1  # Increase by 10%
    >>> GeomLandCover.set_base_mannings_n(geom_path, mannings_df)
"""

from pathlib import Path
from typing import Union
import pandas as pd

from ..LoggingConfig import get_logger
from ..Decorators import log_call

logger = get_logger(__name__)


class GeomLandCover:
    """
    A class for 2D Manning's n land cover operations in HEC-RAS geometry files.

    All methods are static and designed to be used without instantiation.
    """

    @staticmethod
    @log_call
    def get_base_mannings_n(geom_file_path: Union[str, Path]) -> pd.DataFrame:
        """
        Reads the base Manning's n table from a HEC-RAS geometry file.

        Parameters:
            geom_file_path (Union[str, Path]): Path to the geometry file (.g##)

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Table Number (str): Manning's n table identifier
                - Land Cover Name (str): Name of the land cover type
                - Base Mannings n Value (float): Manning's n roughness coefficient

        Example:
            >>> geom_path = RasPlan.get_geom_path("01")
            >>> mannings_df = GeomLandCover.get_base_mannings_n(geom_path)
            >>> print(mannings_df)
        """
        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)

        base_table_rows = []
        table_number = None

        # Read the geometry file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()

        # Parse the file
        reading_base_table = False
        for line in lines:
            line = line.strip()

            # Find the table number
            if line.startswith('LCMann Table='):
                table_number = line.split('=')[1]
                reading_base_table = True
                continue

            # Stop reading when we hit a line without a comma or starting with LCMann
            if reading_base_table and (not ',' in line or line.startswith('LCMann')):
                reading_base_table = False
                continue

            # Parse data rows in base table
            if reading_base_table and ',' in line:
                # Check if there are multiple commas in the line
                parts = line.split(',')
                if len(parts) > 2:
                    # Handle case where land cover name contains commas
                    name = ','.join(parts[:-1])
                    value = parts[-1]
                else:
                    name, value = parts

                try:
                    base_table_rows.append([table_number, name, float(value)])
                except ValueError:
                    # Log the error and continue
                    logger.warning(f"Error parsing line: {line}")
                    continue

        # Create DataFrame
        # Note: Column uses "Mannings" (no apostrophe) for simplicity in DataFrame operations,
        # though HEC-RAS HDF files use "Manning's n" (with apostrophe) as the proper technical term.
        if base_table_rows:
            df = pd.DataFrame(base_table_rows, columns=['Table Number', 'Land Cover Name', 'Base Mannings n Value'])
            return df
        else:
            return pd.DataFrame(columns=['Table Number', 'Land Cover Name', 'Base Mannings n Value'])

    @staticmethod
    @log_call
    def set_base_mannings_n(geom_file_path: Union[str, Path], mannings_data: pd.DataFrame) -> bool:
        """
        Writes base Manning's n values to a HEC-RAS geometry file.

        Parameters:
            geom_file_path (Union[str, Path]): Path to the geometry file (.g##)
            mannings_data (pd.DataFrame): DataFrame with columns:
                - Table Number (str): Manning's n table identifier
                - Land Cover Name (str): Name of the land cover type
                - Base Mannings n Value (float): Manning's n roughness coefficient

        Returns:
            bool: True if successful

        Raises:
            ValueError: If land cover names don't match between file and DataFrame
        """
        import shutil
        import datetime

        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)

        # Create backup
        backup_path = geom_file_path.with_suffix(geom_file_path.suffix + '.bak')
        shutil.copy2(geom_file_path, backup_path)

        # Read the entire file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()

        # Find the Manning's table section
        table_number = str(mannings_data['Table Number'].iloc[0])
        start_idx = None
        end_idx = None

        for i, line in enumerate(lines):
            if line.strip() == f"LCMann Table={table_number}":
                start_idx = i
                # Find the end of this table (next LCMann directive or end of file)
                for j in range(i+1, len(lines)):
                    if lines[j].strip().startswith('LCMann'):
                        end_idx = j
                        break
                if end_idx is None:  # If we reached the end of the file
                    end_idx = len(lines)
                break

        if start_idx is None:
            raise ValueError(f"Manning's table {table_number} not found in the geometry file")

        # Extract existing land cover names from the file
        existing_landcover = []
        for i in range(start_idx+1, end_idx):
            line = lines[i].strip()
            if ',' in line:
                parts = line.split(',')
                if len(parts) > 2:
                    # Handle case where land cover name contains commas
                    name = ','.join(parts[:-1])
                else:
                    name = parts[0]
                existing_landcover.append(name)

        # Check if all land cover names in the dataframe match the file
        df_landcover = mannings_data['Land Cover Name'].tolist()
        if set(df_landcover) != set(existing_landcover):
            missing = set(existing_landcover) - set(df_landcover)
            extra = set(df_landcover) - set(existing_landcover)
            error_msg = "Land cover names don't match between file and dataframe.\n"
            if missing:
                error_msg += f"Missing in dataframe: {missing}\n"
            if extra:
                error_msg += f"Extra in dataframe: {extra}"
            raise ValueError(error_msg)

        # Create new content for the table
        new_content = [f"LCMann Table={table_number}\n"]

        # Add base table entries
        for _, row in mannings_data.iterrows():
            new_content.append(f"{row['Land Cover Name']},{row['Base Mannings n Value']}\n")

        # Replace the section in the original file
        updated_lines = lines[:start_idx] + new_content + lines[end_idx:]

        # Update the time stamp
        current_time = datetime.datetime.now().strftime("%b/%d/%Y %H:%M:%S")
        for i, line in enumerate(updated_lines):
            if line.strip().startswith("LCMann Time="):
                updated_lines[i] = f"LCMann Time={current_time}\n"
                break

        # Write the updated file
        with open(geom_file_path, 'w') as f:
            f.writelines(updated_lines)

        return True

    @staticmethod
    @log_call
    def get_region_mannings_n(geom_file_path: Union[str, Path]) -> pd.DataFrame:
        """
        Reads the Manning's n region overrides from a HEC-RAS geometry file.

        Parameters:
            geom_file_path (Union[str, Path]): Path to the geometry file (.g##)

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Table Number (str): Region table identifier
                - Land Cover Name (str): Name of the land cover type
                - MainChannel (float): Manning's n value for main channel
                - Region Name (str): Name of the region

        Example:
            >>> geom_path = RasPlan.get_geom_path("01")
            >>> region_overrides_df = GeomLandCover.get_region_mannings_n(geom_path)
            >>> print(region_overrides_df)
        """
        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)

        region_rows = []
        current_region = None
        current_table = None

        # Read the geometry file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()

        # Parse the file
        reading_region_table = False
        for line in lines:
            line = line.strip()

            # Find region name
            if line.startswith('LCMann Region Name='):
                current_region = line.split('=')[1]
                continue

            # Find region table number
            if line.startswith('LCMann Region Table='):
                current_table = line.split('=')[1]
                reading_region_table = True
                continue

            # Stop reading when we hit a line without a comma or starting with LCMann
            if reading_region_table and (not ',' in line or line.startswith('LCMann')):
                reading_region_table = False
                continue

            # Parse data rows in region table
            if reading_region_table and ',' in line and current_region is not None:
                # Check if there are multiple commas in the line
                parts = line.split(',')
                if len(parts) > 2:
                    # Handle case where land cover name contains commas
                    name = ','.join(parts[:-1])
                    value = parts[-1]
                else:
                    name, value = parts

                try:
                    region_rows.append([current_table, name, float(value), current_region])
                except ValueError:
                    # Log the error and continue
                    logger.warning(f"Error parsing line: {line}")
                    continue

        # Create DataFrame
        if region_rows:
            return pd.DataFrame(region_rows, columns=['Table Number', 'Land Cover Name', 'MainChannel', 'Region Name'])
        else:
            return pd.DataFrame(columns=['Table Number', 'Land Cover Name', 'MainChannel', 'Region Name'])

    @staticmethod
    @log_call
    def set_region_mannings_n(geom_file_path: Union[str, Path], mannings_data: pd.DataFrame) -> bool:
        """
        Writes regional Manning's n overrides to a HEC-RAS geometry file.

        Parameters:
            geom_file_path (Union[str, Path]): Path to the geometry file (.g##)
            mannings_data (pd.DataFrame): DataFrame with columns:
                - Table Number (str): Region table identifier
                - Land Cover Name (str): Name of the land cover type
                - MainChannel (float): Manning's n value
                - Region Name (str): Name of the region

        Returns:
            bool: True if successful

        Raises:
            ValueError: If region or land cover names don't match
        """
        import shutil
        import datetime

        # Convert to Path object if it's a string
        if isinstance(geom_file_path, str):
            geom_file_path = Path(geom_file_path)

        # Create backup
        backup_path = geom_file_path.with_suffix(geom_file_path.suffix + '.bak')
        shutil.copy2(geom_file_path, backup_path)

        # Read the entire file
        with open(geom_file_path, 'r') as f:
            lines = f.readlines()

        # Group data by region
        regions = mannings_data.groupby('Region Name')

        # Find the Manning's region sections
        for region_name, region_data in regions:
            table_number = str(region_data['Table Number'].iloc[0])

            # Find the region section
            region_start_idx = None
            region_table_idx = None
            region_end_idx = None
            region_polygon_line = None

            for i, line in enumerate(lines):
                if line.strip() == f"LCMann Region Name={region_name}":
                    region_start_idx = i

                if region_start_idx is not None and line.strip() == f"LCMann Region Table={table_number}":
                    region_table_idx = i

                    # Find the end of this region (next LCMann Region or end of file)
                    for j in range(i+1, len(lines)):
                        if lines[j].strip().startswith('LCMann Region Name=') or lines[j].strip().startswith('LCMann Region Polygon='):
                            if lines[j].strip().startswith('LCMann Region Polygon='):
                                region_polygon_line = lines[j]
                            region_end_idx = j
                            break
                    if region_end_idx is None:  # If we reached the end of the file
                        region_end_idx = len(lines)
                    break

            if region_start_idx is None or region_table_idx is None:
                raise ValueError(f"Region {region_name} with table {table_number} not found in the geometry file")

            # Extract existing land cover names from the file
            existing_landcover = []
            for i in range(region_table_idx+1, region_end_idx):
                line = lines[i].strip()
                if ',' in line and not line.startswith('LCMann'):
                    parts = line.split(',')
                    if len(parts) > 2:
                        # Handle case where land cover name contains commas
                        name = ','.join(parts[:-1])
                    else:
                        name = parts[0]
                    existing_landcover.append(name)

            # Check if all land cover names in the dataframe match the file
            df_landcover = region_data['Land Cover Name'].tolist()
            if set(df_landcover) != set(existing_landcover):
                missing = set(existing_landcover) - set(df_landcover)
                extra = set(df_landcover) - set(existing_landcover)
                error_msg = f"Land cover names for region {region_name} don't match between file and dataframe.\n"
                if missing:
                    error_msg += f"Missing in dataframe: {missing}\n"
                if extra:
                    error_msg += f"Extra in dataframe: {extra}"
                raise ValueError(error_msg)

            # Create new content for the region
            new_content = [
                f"LCMann Region Name={region_name}\n",
                f"LCMann Region Table={table_number}\n"
            ]

            # Add region table entries
            for _, row in region_data.iterrows():
                new_content.append(f"{row['Land Cover Name']},{row['MainChannel']}\n")

            # Add the region polygon line if it exists
            if region_polygon_line:
                new_content.append(region_polygon_line)

            # Replace the section in the original file
            if region_polygon_line:
                # If we have a polygon line, include it in the replacement
                updated_lines = lines[:region_start_idx] + new_content + lines[region_end_idx+1:]
            else:
                # If no polygon line, just replace up to the end index
                updated_lines = lines[:region_start_idx] + new_content + lines[region_end_idx:]

            # Update the lines for the next region
            lines = updated_lines

        # Update the time stamp
        current_time = datetime.datetime.now().strftime("%b/%d/%Y %H:%M:%S")
        for i, line in enumerate(lines):
            if line.strip().startswith("LCMann Region Time="):
                lines[i] = f"LCMann Region Time={current_time}\n"
                break

        # Write the updated file
        with open(geom_file_path, 'w') as f:
            f.writelines(lines)

        return True

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomLateral.py
==================================================
"""
GeomLateral - Lateral structures and SA/2D connections for HEC-RAS geometry files

This module provides functionality for reading lateral weir structures and
storage area / 2D area connections from HEC-RAS plain text geometry files (.g##).

All methods are static and designed to be used without instantiation.

List of Functions:
- get_lateral_structures() - List all lateral weir structures
- get_weir_profile() - Read station/elevation profile for lateral weir
- get_connections() - List all SA/2D area connections
- get_connection_profile() - Read dam/weir crest profile for connection
- get_connection_gates() - Read gate definitions for connection

Example Usage:
    >>> from ras_commander import GeomLateral
    >>> from pathlib import Path
    >>>
    >>> # List all lateral structures
    >>> geom_file = Path("model.g01")
    >>> laterals_df = GeomLateral.get_lateral_structures(geom_file)
    >>> print(f"Found {len(laterals_df)} lateral structures")
    >>>
    >>> # List SA/2D connections
    >>> connections_df = GeomLateral.get_connections(geom_file)
    >>> print(connections_df)
"""

from pathlib import Path
from typing import Union, Optional, List, Dict, Any
import pandas as pd

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser

logger = get_logger(__name__)


class GeomLateral:
    """
    Operations for lateral structures and SA/2D connections in geometry files.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8      # Character width for numeric data in geometry files
    VALUES_PER_LINE = 10        # Number of values per line in fixed-width format
    DEFAULT_SEARCH_RANGE = 100  # Lines to search for keywords after structure header

    @staticmethod
    @log_call
    def get_lateral_structures(geom_file: Union[str, Path],
                               river: Optional[str] = None) -> pd.DataFrame:
        """
        Extract lateral weir structure metadata from geometry file.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            river (Optional[str]): Filter by specific river name

        Returns:
            pd.DataFrame: DataFrame with columns:
                - River (str): River name
                - Reach (str): Reach name
                - Name (str): Lateral weir name
                - StartRS (str): Starting river station
                - EndRS (str): Ending river station
                - NumPoints (int): Number of station/elevation points

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> laterals = GeomLateral.get_lateral_structures("model.g01")
            >>> for _, row in laterals.iterrows():
            ...     print(f"Lateral: {row['Name']} from RS {row['StartRS']} to {row['EndRS']}")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            laterals = []
            current_river = None
            current_reach = None
            i = 0

            while i < len(lines):
                line = lines[i]

                # Track current river/reach
                if line.startswith("River Reach="):
                    values = GeomParser.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                # Find lateral weir definition
                elif line.startswith("Lat Struct="):
                    if river is not None and current_river != river:
                        i += 1
                        continue

                    lat_values = GeomParser.extract_comma_list(line, "Lat Struct")
                    lat_name = lat_values[0] if lat_values else ""

                    # Look for additional data
                    start_rs = None
                    end_rs = None
                    num_points = 0

                    for j in range(i+1, min(i+30, len(lines))):
                        if lines[j].startswith("Lat Struct RS="):
                            rs_values = GeomParser.extract_comma_list(lines[j], "Lat Struct RS")
                            if len(rs_values) >= 2:
                                start_rs = rs_values[0]
                                end_rs = rs_values[1]

                        elif lines[j].startswith("#Lat Struct Sta/Elev="):
                            count_str = GeomParser.extract_keyword_value(lines[j], "#Lat Struct Sta/Elev")
                            try:
                                num_points = int(count_str.strip())
                            except ValueError:
                                pass
                            break

                        # Stop at next structure
                        if lines[j].startswith("Lat Struct=") or lines[j].startswith("River Reach="):
                            break

                    laterals.append({
                        'River': current_river,
                        'Reach': current_reach,
                        'Name': lat_name,
                        'StartRS': start_rs,
                        'EndRS': end_rs,
                        'NumPoints': num_points
                    })

                i += 1

            df = pd.DataFrame(laterals)
            logger.info(f"Found {len(df)} lateral structures in {geom_file.name}")
            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading lateral structures: {str(e)}")
            raise IOError(f"Failed to read lateral structures: {str(e)}")

    @staticmethod
    @log_call
    def get_weir_profile(geom_file: Union[str, Path],
                        lateral_name: str) -> pd.DataFrame:
        """
        Extract station/elevation profile for a lateral weir.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            lateral_name (str): Lateral weir name

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station along weir
                - Elevation (float): Weir crest elevation

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If lateral weir not found

        Example:
            >>> profile = GeomLateral.get_weir_profile("model.g01", "Spillway")
            >>> print(f"Weir profile has {len(profile)} points")
            >>> print(f"Crest elevation range: {profile['Elevation'].min():.1f} to {profile['Elevation'].max():.1f}")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the lateral weir
            lat_idx = None
            for i, line in enumerate(lines):
                if line.startswith("Lat Struct="):
                    lat_values = GeomParser.extract_comma_list(line, "Lat Struct")
                    if lat_values and lat_values[0] == lateral_name:
                        lat_idx = i
                        break

            if lat_idx is None:
                raise ValueError(f"Lateral weir not found: {lateral_name}")

            # Find station/elevation data
            for j in range(lat_idx+1, min(lat_idx+GeomLateral.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Lat Struct Sta/Elev="):
                    count_str = GeomParser.extract_keyword_value(lines[j], "#Lat Struct Sta/Elev")
                    count = int(count_str.strip())

                    # Parse paired data
                    total_values = count * 2
                    values = []
                    k = j + 1
                    while len(values) < total_values and k < len(lines):
                        if '=' in lines[k]:
                            break
                        parsed = GeomParser.parse_fixed_width(lines[k], GeomLateral.FIXED_WIDTH_COLUMN)
                        values.extend(parsed)
                        k += 1

                    # Split into stations and elevations
                    stations = values[0::2]
                    elevations = values[1::2]

                    df = pd.DataFrame({
                        'Station': stations[:count],
                        'Elevation': elevations[:count]
                    })

                    logger.info(f"Extracted {len(df)} profile points for lateral {lateral_name}")
                    return df

                # Stop at next structure
                if lines[j].startswith("Lat Struct="):
                    break

            raise ValueError(f"Station/elevation data not found for lateral {lateral_name}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading lateral weir profile: {str(e)}")
            raise IOError(f"Failed to read lateral weir profile: {str(e)}")

    @staticmethod
    @log_call
    def get_connections(geom_file: Union[str, Path]) -> pd.DataFrame:
        """
        Extract SA/2D area connection metadata from geometry file.

        Connections include storage area to storage area connections, storage area
        to 2D flow area connections, and 2D to 2D flow area connections.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Name (str): Connection name
                - Type (str): Connection type (SA to SA, SA to 2D, etc.)
                - From (str): Upstream area name
                - To (str): Downstream area name
                - NumPoints (int): Number of station/elevation points in weir profile

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> connections = GeomLateral.get_connections("model.g01")
            >>> print(f"Found {len(connections)} connections")
            >>> for _, row in connections.iterrows():
            ...     print(f"{row['Name']}: {row['From']} -> {row['To']}")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            connections = []
            i = 0

            while i < len(lines):
                line = lines[i]

                # Find SA/2D Connection definition
                if line.startswith("SA/2D Area Conn="):
                    conn_values = GeomParser.extract_comma_list(line, "SA/2D Area Conn")
                    conn_name = conn_values[0] if conn_values else ""

                    # Parse connection parameters
                    from_area = None
                    to_area = None
                    conn_type = "Unknown"
                    num_points = 0

                    for j in range(i+1, min(i+50, len(lines))):
                        if lines[j].startswith("From Storage Area="):
                            from_area = GeomParser.extract_keyword_value(lines[j], "From Storage Area")
                        elif lines[j].startswith("To Storage Area="):
                            to_area = GeomParser.extract_keyword_value(lines[j], "To Storage Area")
                        elif lines[j].startswith("From 2D Area="):
                            from_area = GeomParser.extract_keyword_value(lines[j], "From 2D Area")
                            conn_type = "2D to SA" if to_area else "2D to 2D"
                        elif lines[j].startswith("To 2D Area="):
                            to_area = GeomParser.extract_keyword_value(lines[j], "To 2D Area")
                            conn_type = "SA to 2D" if from_area and "2D" not in str(from_area) else conn_type
                        elif lines[j].startswith("#Conn Weir Sta/Elev="):
                            count_str = GeomParser.extract_keyword_value(lines[j], "#Conn Weir Sta/Elev")
                            try:
                                num_points = int(count_str.strip())
                            except ValueError:
                                pass
                            break

                        # Stop at next structure
                        if lines[j].startswith("SA/2D Area Conn=") or lines[j].startswith("Storage Area="):
                            break

                    # Determine type
                    if from_area and to_area:
                        if "2D" in conn_type:
                            pass  # Already set
                        else:
                            conn_type = "SA to SA"

                    connections.append({
                        'Name': conn_name,
                        'Type': conn_type,
                        'From': from_area,
                        'To': to_area,
                        'NumPoints': num_points
                    })

                i += 1

            df = pd.DataFrame(connections)
            logger.info(f"Found {len(df)} SA/2D connections in {geom_file.name}")
            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading connections: {str(e)}")
            raise IOError(f"Failed to read connections: {str(e)}")

    @staticmethod
    @log_call
    def get_connection_profile(geom_file: Union[str, Path],
                              connection_name: str) -> pd.DataFrame:
        """
        Extract dam/weir crest profile for a SA/2D connection.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            connection_name (str): Connection name

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Station (float): Station along weir
                - Elevation (float): Weir crest elevation

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If connection not found

        Example:
            >>> profile = GeomLateral.get_connection_profile("model.g01", "Dam Embankment")
            >>> print(f"Weir crest has {len(profile)} points")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the connection
            conn_idx = None
            for i, line in enumerate(lines):
                if line.startswith("SA/2D Area Conn="):
                    conn_values = GeomParser.extract_comma_list(line, "SA/2D Area Conn")
                    if conn_values and conn_values[0] == connection_name:
                        conn_idx = i
                        break

            if conn_idx is None:
                raise ValueError(f"Connection not found: {connection_name}")

            # Find weir profile data
            for j in range(conn_idx+1, min(conn_idx+GeomLateral.DEFAULT_SEARCH_RANGE, len(lines))):
                if lines[j].startswith("#Conn Weir Sta/Elev="):
                    count_str = GeomParser.extract_keyword_value(lines[j], "#Conn Weir Sta/Elev")
                    count = int(count_str.strip())

                    # Parse paired data
                    total_values = count * 2
                    values = []
                    k = j + 1
                    while len(values) < total_values and k < len(lines):
                        if '=' in lines[k]:
                            break
                        parsed = GeomParser.parse_fixed_width(lines[k], GeomLateral.FIXED_WIDTH_COLUMN)
                        values.extend(parsed)
                        k += 1

                    # Split into stations and elevations
                    stations = values[0::2]
                    elevations = values[1::2]

                    df = pd.DataFrame({
                        'Station': stations[:count],
                        'Elevation': elevations[:count]
                    })

                    logger.info(f"Extracted {len(df)} weir profile points for connection {connection_name}")
                    return df

                # Stop at next structure
                if lines[j].startswith("SA/2D Area Conn="):
                    break

            raise ValueError(f"Weir profile not found for connection {connection_name}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading connection profile: {str(e)}")
            raise IOError(f"Failed to read connection profile: {str(e)}")

    @staticmethod
    @log_call
    def get_connection_gates(geom_file: Union[str, Path],
                            connection_name: str) -> pd.DataFrame:
        """
        Extract gate definitions for a SA/2D connection.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            connection_name (str): Connection name

        Returns:
            pd.DataFrame: DataFrame with gate parameters including:
                - GateName, Width, Height, InvertElevation, etc.

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If connection not found or has no gates

        Example:
            >>> gates = GeomLateral.get_connection_gates("model.g01", "Dam Outlet")
            >>> print(f"Found {len(gates)} gates")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the connection
            conn_idx = None
            for i, line in enumerate(lines):
                if line.startswith("SA/2D Area Conn="):
                    conn_values = GeomParser.extract_comma_list(line, "SA/2D Area Conn")
                    if conn_values and conn_values[0] == connection_name:
                        conn_idx = i
                        break

            if conn_idx is None:
                raise ValueError(f"Connection not found: {connection_name}")

            # Find gate definitions
            gates = []
            i = conn_idx + 1
            while i < min(conn_idx + GeomLateral.DEFAULT_SEARCH_RANGE, len(lines)):
                line = lines[i]

                # Stop at next structure
                if line.startswith("SA/2D Area Conn=") or line.startswith("Storage Area="):
                    break

                # Found gate header (simplified parsing - actual format varies)
                if line.startswith("Conn Gate Name"):
                    if i + 1 < len(lines):
                        gate_line = lines[i + 1]
                        parts = [p.strip() for p in gate_line.split(',')]

                        gate_data = {
                            'GateName': parts[0] if len(parts) > 0 else None,
                            'Width': float(parts[1]) if len(parts) > 1 and parts[1] else None,
                            'Height': float(parts[2]) if len(parts) > 2 and parts[2] else None,
                            'InvertElevation': float(parts[3]) if len(parts) > 3 and parts[3] else None,
                        }
                        gates.append(gate_data)
                        i += 2
                        continue

                i += 1

            if not gates:
                raise ValueError(f"No gates found for connection {connection_name}")

            df = pd.DataFrame(gates)
            logger.info(f"Extracted {len(df)} gates for connection {connection_name}")
            return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading connection gates: {str(e)}")
            raise IOError(f"Failed to read connection gates: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomParser.py
==================================================
"""
GeomParser - Utility functions for parsing HEC-RAS geometry files

This module provides reusable utility functions for parsing and manipulating
HEC-RAS geometry files. These utilities handle FORTRAN-era fixed-width formats,
count interpretation, section identification, and file manipulation.

All methods are static and designed to be used without instantiation.

List of Functions:
- parse_fixed_width() - Parse fixed-width numeric data (8 or 16 char columns)
- format_fixed_width() - Format values into fixed-width lines
- interpret_count() - Interpret count declarations based on context
- identify_section() - Find section boundaries by keyword marker
- extract_keyword_value() - Extract value following keyword
- extract_comma_list() - Extract comma-separated list
- create_backup() - Create .bak backup before modification
- validate_river_reach_rs() - Validate river/reach/RS exists

Example Usage:
    >>> from ras_commander import GeomParser
    >>> # Parse fixed-width line (8-char columns)
    >>> line = "       0  963.04    27.2  963.04"
    >>> values = GeomParser.parse_fixed_width(line, column_width=8)
    >>> print(values)
    [0.0, 963.04, 27.2, 963.04]

    >>> # Interpret count declaration
    >>> total_values = GeomParser.interpret_count("#Sta/Elev", 40)
    >>> print(f"40 pairs = {total_values} total values")
    40 pairs = 80 total values
"""

import re
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
from datetime import datetime

from ..LoggingConfig import get_logger
from ..Decorators import log_call

logger = get_logger(__name__)


class GeomParser:
    """
    Utility functions for parsing HEC-RAS geometry files.

    All methods are static and designed to be used without instantiation.
    """

    @staticmethod
    def parse_fixed_width(line: str, column_width: int = 8) -> List[float]:
        """
        Parse fixed-width numeric data from a line.

        HEC-RAS uses FORTRAN-era fixed-width columns for numeric data:
        - 8-character columns: Station/elevation, Manning's n, elevation-volume
        - 16-character columns: 2D coordinates (X, Y pairs)

        Values are right-aligned and left-padded with spaces within each column.
        This function MUST parse by column position, NOT by whitespace splitting.

        Parameters:
            line (str): Line containing fixed-width values
            column_width (int): Width of each column in characters. Defaults to 8.
                               Use 16 for 2D coordinate data.

        Returns:
            List[float]: Parsed numeric values

        Raises:
            ValueError: If a column contains non-numeric data that can't be parsed

        Example:
            >>> # 8-character columns (station/elevation)
            >>> line = "       0  963.04    27.2  963.04   32.64  963.02"
            >>> values = GeomParser.parse_fixed_width(line, 8)
            >>> print(values)
            [0.0, 963.04, 27.2, 963.04, 32.64, 963.02]

            >>> # 16-character columns (2D coordinates)
            >>> line = "   648224.43125   4551425.84375   648229.43125   4551425.84375"
            >>> coords = GeomParser.parse_fixed_width(line, 16)
            >>> print(coords)
            [648224.43125, 4551425.84375, 648229.43125, 4551425.84375]

        Notes:
            - Based on successful RasUnsteady.parse_fixed_width_table() pattern
            - Handles merged values (adjacent numbers without spaces) using regex
            - Skips empty columns
            - Strips line before parsing to remove trailing newlines
        """
        values = []
        line_stripped = line.rstrip('\n\r')

        # Parse by column position (CRITICAL: do NOT use .split())
        for i in range(0, len(line_stripped), column_width):
            column = line_stripped[i:i+column_width].strip()

            if not column:
                continue  # Skip empty columns

            try:
                # Try direct conversion first
                values.append(float(column))
            except ValueError:
                # Handle merged values (e.g., "123.45678.90" without space)
                # Use regex to split merged numeric values
                merged_values = re.findall(r'-?\d+\.?\d*', column)
                if merged_values:
                    for val_str in merged_values:
                        try:
                            values.append(float(val_str))
                        except ValueError:
                            logger.warning(f"Could not parse value '{val_str}' from merged column '{column}'")
                else:
                    logger.warning(f"Could not parse column '{column}' as numeric")

        return values

    @staticmethod
    def format_fixed_width(values: List[float],
                          column_width: int = 8,
                          values_per_line: int = 10,
                          precision: int = 2) -> List[str]:
        """
        Format values into fixed-width lines for writing to geometry files.

        Creates properly formatted lines with right-aligned values, left-padded
        with spaces to fill the column width. Follows HEC-RAS conventions:
        - 8-char columns: Typically 10 values per line (80 chars total)
        - 16-char columns: Typically 4 values per line (64 chars total)

        Parameters:
            values (List[float]): List of numeric values to format
            column_width (int): Width of each column in characters. Defaults to 8.
            values_per_line (int): Number of values per line. Defaults to 10.
            precision (int): Decimal places for formatting. Defaults to 2.

        Returns:
            List[str]: Lines with fixed-width formatted values (with newlines)

        Example:
            >>> values = [0.0, 963.04, 27.2, 963.04]
            >>> lines = GeomParser.format_fixed_width(values, 8, 10, 2)
            >>> print(lines[0])
            '    0.00  963.04   27.20  963.04\\n'

            >>> # 16-char columns for coordinates
            >>> coords = [648224.43125, 4551425.84375]
            >>> lines = GeomParser.format_fixed_width(coords, 16, 4, 5)
            >>> print(lines[0])
            '  648224.43125  4551425.84375\\n'

        Notes:
            - Based on RasUnsteady.write_table_to_file() pattern
            - Values are formatted as f'{value:{column_width}.{precision}f}'
            - Right-aligned within column, left-padded with spaces
            - Last line may have fewer than values_per_line values
        """
        lines = []

        for i in range(0, len(values), values_per_line):
            row_values = values[i:i+values_per_line]
            # Format each value with specified width and precision
            formatted_row = ''.join(f'{value:{column_width}.{precision}f}' for value in row_values)
            lines.append(formatted_row + '\n')

        return lines

    @staticmethod
    @log_call
    def interpret_count(keyword: str,
                       count_value: int,
                       additional_values: Optional[List[int]] = None) -> int:
        """
        Interpret count declarations based on keyword context.

        CRITICAL: Different keywords use counts differently. This is a common
        source of parsing bugs if not handled correctly.

        Count Interpretation Rules:
        - "#Sta/Elev= 40" -> 40 PAIRS -> 80 total values (station + elevation)
        - "#Mann= 3 , 0 , 0" -> 3 SEGMENTS -> 9 total values (3 left + 3 channel + 3 right)
        - "Reach XY= 591" -> 591 PAIRS -> 1182 total values (591 X + 591 Y)
        - "Storage Area Elev Volume= 53" -> 53 PAIRS -> 106 total values
        - "Levee= 12 , 0" -> 12 + 0 = 12 values (left side only)

        Parameters:
            keyword (str): Section keyword (e.g., "#Sta/Elev", "#Mann", "Reach XY")
            count_value (int): First count value after keyword
            additional_values (Optional[List[int]]): Additional count values if comma-separated

        Returns:
            int: Total number of values to read from the file

        Example:
            >>> # Station/elevation: 40 pairs = 80 values
            >>> GeomParser.interpret_count("#Sta/Elev", 40)
            80

            >>> # Manning's n: 3 segments x 3 positions = 9 values
            >>> GeomParser.interpret_count("#Mann", 3, [0, 0])
            9

            >>> # Reach coordinates: 591 pairs = 1182 values
            >>> GeomParser.interpret_count("Reach XY", 591)
            1182

            >>> # Levees: 12 left + 0 right = 12 values
            >>> GeomParser.interpret_count("Levee", 12, [0])
            12

        Notes:
            - See _PARSING_PATTERNS_REFERENCE.md for complete count interpretation guide
            - This is based on extensive validation against HDF files
        """
        keyword_lower = keyword.lower()

        # Station/Elevation pairs (most common)
        if 'sta' in keyword_lower and 'elev' in keyword_lower:
            return count_value * 2  # Pairs: station + elevation

        # Manning's n segments (triplets: left, channel, right)
        if 'mann' in keyword_lower:
            # #Mann= 3 , 0 , 0 means 3 segments with left/channel/right values each
            return count_value * 3

        # Coordinate pairs (X, Y)
        if 'xy' in keyword_lower or ('x' in keyword_lower and 'y' in keyword_lower):
            return count_value * 2  # Pairs: X + Y

        # Elevation-Volume pairs (storage areas)
        if 'elev' in keyword_lower and 'volume' in keyword_lower:
            return count_value * 2  # Pairs: elevation + volume

        # Levees (can have left and right counts)
        if 'levee' in keyword_lower:
            if additional_values:
                return count_value + sum(additional_values)
            return count_value

        # Default: count is total values (not pairs)
        logger.debug(f"Using default count interpretation for keyword '{keyword}': {count_value} values")
        return count_value

    @staticmethod
    @log_call
    def identify_section(lines: List[str],
                        keyword: str,
                        start_index: int = 0) -> Optional[Tuple[int, int]]:
        """
        Find section boundaries based on keyword marker.

        Searches for a line starting with the specified keyword and determines
        where the section ends (either at the next keyword or end of file).

        Parameters:
            lines (List[str]): All lines from geometry file
            keyword (str): Section marker keyword to search for
            start_index (int): Line index to start searching from. Defaults to 0.

        Returns:
            Optional[Tuple[int, int]]: (start_line, end_line) or None if not found
                                       start_line: Index of line with keyword
                                       end_line: Index of last line in section (exclusive)

        Example:
            >>> with open("geometry.g01") as f:
            ...     lines = f.readlines()
            >>> section = GeomParser.identify_section(lines, "River Reach=")
            >>> if section:
            ...     start, end = section
            ...     print(f"River Reach section: lines {start} to {end}")

        Notes:
            - Keyword matching is case-insensitive
            - Returns None if keyword not found
            - Section ends at next keyword starting with capital letter or "=" sign
        """
        start_line = None

        # Find the start of the section
        for i in range(start_index, len(lines)):
            if lines[i].strip().lower().startswith(keyword.lower()):
                start_line = i
                break

        if start_line is None:
            logger.debug(f"Keyword '{keyword}' not found starting from line {start_index}")
            return None

        # Find the end of the section (next keyword or end of file)
        end_line = len(lines)
        for i in range(start_line + 1, len(lines)):
            line_stripped = lines[i].strip()
            # Section ends at next keyword (starts with capital or contains "=")
            if line_stripped and (line_stripped[0].isupper() or '=' in line_stripped):
                # Check if it looks like a keyword (not just data with "=")
                if '=' in line_stripped:
                    end_line = i
                    break

        logger.debug(f"Section '{keyword}' found: lines {start_line} to {end_line}")
        return (start_line, end_line)

    @staticmethod
    def extract_keyword_value(line: str, keyword: str) -> str:
        """
        Extract value following keyword marker.

        Finds keyword followed by "=" and returns everything after the "=".

        Parameters:
            line (str): Line containing keyword
            keyword (str): Keyword to search for

        Returns:
            str: Value after "=" (stripped of leading/trailing whitespace)

        Example:
            >>> line = "Geom Title=White Lick Creek Geometry"
            >>> title = GeomParser.extract_keyword_value(line, "Geom Title")
            >>> print(title)
            'White Lick Creek Geometry'

            >>> line = "Program Version=6.30"
            >>> version = GeomParser.extract_keyword_value(line, "Program Version")
            >>> print(version)
            '6.30'

        Notes:
            - Keyword matching is case-insensitive
            - Returns empty string if keyword not found or no value after "="
        """
        # Pattern: keyword (case-insensitive) followed by = and value
        pattern = rf'{re.escape(keyword)}\s*=\s*(.+)'
        match = re.search(pattern, line, re.IGNORECASE)

        if match:
            return match.group(1).strip()
        return ""

    @staticmethod
    def extract_comma_list(line: str, keyword: str) -> List[str]:
        """
        Extract comma-separated list following keyword.

        Handles embedded commas in quoted strings properly.

        Parameters:
            line (str): Line containing keyword and comma-separated values
            keyword (str): Keyword before the list

        Returns:
            List[str]: List of values (stripped of whitespace)

        Example:
            >>> line = "River Reach=White Lick,Reach 1"
            >>> values = GeomParser.extract_comma_list(line, "River Reach")
            >>> print(values)
            ['White Lick', 'Reach 1']

            >>> line = "Storage Area=Res Pool 1"
            >>> values = GeomParser.extract_comma_list(line, "Storage Area")
            >>> print(values)
            ['Res Pool 1']

        Notes:
            - Handles cases with or without commas
            - Handles quoted strings with embedded commas
        """
        value_str = GeomParser.extract_keyword_value(line, keyword)

        if not value_str:
            return []

        # Split by comma, handling quoted strings
        # Simple approach: split by comma and strip
        values = [v.strip().strip('"\'') for v in value_str.split(',')]

        return values

    @staticmethod
    @log_call
    def create_backup(file_path: Path) -> Path:
        """
        Create .bak backup of file before modification.

        Creates a backup copy with .bak extension. If .bak already exists,
        creates .bak1, .bak2, etc.

        Parameters:
            file_path (Path): Path to file to backup

        Returns:
            Path: Path to backup file

        Raises:
            FileNotFoundError: If original file doesn't exist
            IOError: If backup creation fails

        Example:
            >>> from pathlib import Path
            >>> geom_file = Path("MyProject.g01")
            >>> backup = GeomParser.create_backup(geom_file)
            >>> print(f"Backup created: {backup}")
            Backup created: MyProject.g01.bak

        Notes:
            - Based on RasGeo.set_mannings_baseoverrides() pattern
            - Always creates backup before file modification
            - Finds next available .bakN filename if .bak exists
        """
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"Cannot create backup: file not found: {file_path}")

        # Find next available backup filename
        backup_path = file_path.with_suffix(file_path.suffix + '.bak')
        counter = 1

        while backup_path.exists():
            backup_path = file_path.with_suffix(f'{file_path.suffix}.bak{counter}')
            counter += 1

        try:
            # Copy file to backup
            import shutil
            shutil.copy2(file_path, backup_path)
            logger.info(f"Created backup: {backup_path}")
            return backup_path

        except Exception as e:
            logger.error(f"Failed to create backup of {file_path}: {str(e)}")
            raise IOError(f"Backup creation failed: {str(e)}")

    @staticmethod
    def update_timestamp(lines: List[str], keyword: str) -> List[str]:
        """
        Update timestamp for a modified section.

        Finds lines with timestamp keywords and updates them to current time.

        Parameters:
            lines (List[str]): File lines to modify
            keyword (str): Timestamp keyword to search for

        Returns:
            List[str]: Modified lines with updated timestamp

        Example:
            >>> lines = ["LCMann Time=01Jan2023 14:30:45\\n"]
            >>> updated = GeomParser.update_timestamp(lines, "LCMann Time")
            >>> print(updated[0])
            'LCMann Time=11Nov2025 10:45:30\\n'

        Notes:
            - Timestamp format: DDMmmYYYY HH:MM:SS
            - Only updates lines matching the specified keyword
            - Preserves all other lines unchanged
        """
        current_time = datetime.now()
        timestamp_str = current_time.strftime("%d%b%Y %H:%M:%S")

        updated_lines = []
        for line in lines:
            if keyword in line and '=' in line:
                # Replace the timestamp after the "="
                parts = line.split('=')
                updated_line = f"{parts[0]}={timestamp_str}\n"
                updated_lines.append(updated_line)
            else:
                updated_lines.append(line)

        return updated_lines

    @staticmethod
    @log_call
    def validate_river_reach_rs(geom_file: Path,
                               river: str,
                               reach: str,
                               rs: str) -> bool:
        """
        Validate that river/reach/RS combination exists in geometry file.

        Parameters:
            geom_file (Path): Path to geometry file
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            bool: True if combination exists

        Raises:
            ValueError: If river/reach/RS not found in geometry file

        Example:
            >>> from pathlib import Path
            >>> geom_file = Path("BaldEagle.g01")
            >>> valid = GeomParser.validate_river_reach_rs(
            ...     geom_file, "Bald Eagle Creek", "Reach 1", "138154.4"
            ... )
            >>> print(valid)
            True

        Notes:
            - Used before modification operations to ensure valid target
            - Searches for "Type RM Length L Ch R =" line with matching RS
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find River Reach line
            current_river = None
            current_reach = None

            for i, line in enumerate(lines):
                # Check for River Reach definition
                if line.startswith("River Reach="):
                    values = GeomParser.extract_comma_list(line, "River Reach")
                    if len(values) >= 2:
                        current_river = values[0]
                        current_reach = values[1]

                # Check for cross section with matching RS
                if line.startswith("Type RM Length L Ch R ="):
                    # Next line should have river station
                    if i + 1 < len(lines):
                        parts = lines[i].split('=')
                        if len(parts) > 1:
                            values = parts[1].strip().split(',')
                            if len(values) > 0:
                                xs_rs = values[0].strip()
                                if (current_river == river and
                                    current_reach == reach and
                                    xs_rs == rs):
                                    logger.debug(f"Found XS: {river}/{reach}/RS {rs}")
                                    return True

            raise ValueError(f"Cross section not found: {river}, {reach}, RS {rs}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error validating river/reach/RS: {str(e)}")
            raise ValueError(f"Validation failed: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomPreprocessor.py
==================================================
"""
GeomPreprocessor - Geometry preprocessor file operations

This module provides functionality for managing HEC-RAS geometry preprocessor
files. Geometry preprocessor files contain computed hydraulic properties
derived from the geometry.

All methods are static and designed to be used without instantiation.

List of Functions:
- clear_geompre_files() - Clear geometry preprocessor files for plan files

Example Usage:
    >>> from ras_commander import GeomPreprocessor, RasPlan
    >>>
    >>> # Clone a plan and geometry
    >>> new_plan_number = RasPlan.clone_plan("01")
    >>> new_geom_number = RasPlan.clone_geom("01")
    >>>
    >>> # Set the new geometry for the cloned plan
    >>> RasPlan.set_geom(new_plan_number, new_geom_number)
    >>> plan_path = RasPlan.get_plan_path(new_plan_number)
    >>>
    >>> # Clear geometry preprocessor files to ensure clean results
    >>> GeomPreprocessor.clear_geompre_files(plan_path)
"""

from pathlib import Path
from typing import List, Union

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from ..RasPrj import ras

logger = get_logger(__name__)


class GeomPreprocessor:
    """
    A class for managing HEC-RAS geometry preprocessor files.

    All methods are static and designed to be used without instantiation.
    """

    @staticmethod
    @log_call
    def clear_geompre_files(
        plan_files: Union[str, Path, List[Union[str, Path]]] = None,
        ras_object=None
    ) -> None:
        """
        Clear HEC-RAS geometry preprocessor files for specified plan files.

        Geometry preprocessor files (.c* extension) contain computed hydraulic properties derived
        from the geometry. These should be cleared when the geometry changes to ensure that
        HEC-RAS recomputes all hydraulic tables with updated geometry information.

        Limitations/Future Work:
        - This function only deletes the geometry preprocessor file.
        - It does not clear the IB tables.
        - It also does not clear geometry preprocessor tables from the geometry HDF.
        - All of these features will need to be added to reliably remove geometry preprocessor
          files for 1D and 2D projects.

        Parameters:
            plan_files (Union[str, Path, List[Union[str, Path]]], optional):
                Full path(s) to the HEC-RAS plan file(s) (.p*).
                If None, clears all plan files in the project directory.
            ras_object: An optional RAS object instance.

        Returns:
            None: The function deletes files and updates the ras object's geometry dataframe

        Example:
            >>> # Clone a plan and geometry
            >>> new_plan_number = RasPlan.clone_plan("01")
            >>> new_geom_number = RasPlan.clone_geom("01")
            >>>
            >>> # Set the new geometry for the cloned plan
            >>> RasPlan.set_geom(new_plan_number, new_geom_number)
            >>> plan_path = RasPlan.get_plan_path(new_plan_number)
            >>>
            >>> # Clear geometry preprocessor files to ensure clean results
            >>> GeomPreprocessor.clear_geompre_files(plan_path)
            >>> print(f"Cleared geometry preprocessor files for plan {new_plan_number}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        def clear_single_file(plan_file: Union[str, Path], ras_obj) -> None:
            plan_path = Path(plan_file)
            geom_preprocessor_suffix = '.c' + ''.join(plan_path.suffixes[1:]) if plan_path.suffixes else '.c'
            geom_preprocessor_file = plan_path.with_suffix(geom_preprocessor_suffix)
            if geom_preprocessor_file.exists():
                try:
                    geom_preprocessor_file.unlink()
                    logger.info(f"Deleted geometry preprocessor file: {geom_preprocessor_file}")
                except PermissionError:
                    logger.error(f"Permission denied: Unable to delete geometry preprocessor file: {geom_preprocessor_file}")
                    raise PermissionError(f"Unable to delete geometry preprocessor file: {geom_preprocessor_file}. Permission denied.")
                except OSError as e:
                    logger.error(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
                    raise OSError(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
            else:
                logger.warning(f"No geometry preprocessor file found for: {plan_file}")

        if plan_files is None:
            logger.info("Clearing all geometry preprocessor files in the project directory.")
            plan_files_to_clear = list(ras_obj.project_folder.glob(r'*.p*'))
        elif isinstance(plan_files, (str, Path)):
            plan_files_to_clear = [plan_files]
            logger.info(f"Clearing geometry preprocessor file for single plan: {plan_files}")
        elif isinstance(plan_files, list):
            plan_files_to_clear = plan_files
            logger.info(f"Clearing geometry preprocessor files for multiple plans: {plan_files}")
        else:
            logger.error("Invalid input type for plan_files.")
            raise ValueError("Invalid input. Please provide a string, Path, list of paths, or None.")

        for plan_file in plan_files_to_clear:
            clear_single_file(plan_file, ras_obj)

        try:
            ras_obj.geom_df = ras_obj.get_geom_entries()
            logger.info("Geometry dataframe updated successfully.")
        except Exception as e:
            logger.error(f"Failed to update geometry dataframe: {str(e)}")
            raise

==================================================

File: C:\GH\ras-commander\ras_commander\geom\GeomStorage.py
==================================================
"""
GeomStorage - Storage area operations for HEC-RAS geometry files

This module provides functionality for reading storage area data from
HEC-RAS plain text geometry files (.g##).

All methods are static and designed to be used without instantiation.

List of Functions:
- get_storage_areas() - List all storage areas with metadata
- get_elevation_volume() - Read elevation-volume curve for a storage area

Example Usage:
    >>> from ras_commander import GeomStorage
    >>> from pathlib import Path
    >>>
    >>> # List all storage areas
    >>> geom_file = Path("model.g01")
    >>> storage_df = GeomStorage.get_storage_areas(geom_file)
    >>> print(f"Found {len(storage_df)} storage areas")
    >>>
    >>> # Get elevation-volume curve
    >>> elev_vol = GeomStorage.get_elevation_volume(geom_file, "Reservoir Pool 1")
    >>> print(elev_vol)
"""

from pathlib import Path
from typing import Union, Optional, List
import pandas as pd

from ..LoggingConfig import get_logger
from ..Decorators import log_call
from .GeomParser import GeomParser

logger = get_logger(__name__)


class GeomStorage:
    """
    Operations for parsing HEC-RAS storage areas in geometry files.

    All methods are static and designed to be used without instantiation.
    """

    # HEC-RAS format constants
    FIXED_WIDTH_COLUMN = 8      # Character width for numeric data in geometry files
    VALUES_PER_LINE = 10        # Number of values per line in fixed-width format

    @staticmethod
    @log_call
    def get_storage_areas(geom_file: Union[str, Path],
                         exclude_2d: bool = True) -> pd.DataFrame:
        """
        Extract storage area metadata from geometry file.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            exclude_2d (bool): If True, exclude 2D flow areas (default True).
                2D flow areas are identified by having "Storage Area Is2D=" set to -1.

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Name (str): Storage area name
                - NumPoints (int): Number of elevation-volume points
                - MinElev (float): Minimum elevation in storage curve (if available)
                - MaxElev (float): Maximum elevation in storage curve (if available)
                - Is2D (bool): Whether this is a 2D flow area

        Raises:
            FileNotFoundError: If geometry file doesn't exist

        Example:
            >>> # Get only traditional storage areas (exclude 2D)
            >>> storage_df = GeomStorage.get_storage_areas("model.g01", exclude_2d=True)
            >>> print(f"Found {len(storage_df)} storage areas")
            >>>
            >>> # Get all storage areas including 2D flow areas
            >>> all_storage = GeomStorage.get_storage_areas("model.g01", exclude_2d=False)
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            storage_areas = []
            i = 0

            while i < len(lines):
                line = lines[i]

                # Find Storage Area definition
                if line.startswith("Storage Area="):
                    value_str = GeomParser.extract_keyword_value(line, "Storage Area")
                    # Storage Area format: Name,X,Y - extract just the name
                    parts = [p.strip() for p in value_str.split(',')]
                    sa_name = parts[0] if parts else value_str

                    # Look for elevation-volume count and 2D flag
                    num_points = 0
                    min_elev = None
                    max_elev = None
                    is_2d = False

                    for j in range(i+1, min(i+50, len(lines))):
                        # Check if this is a 2D flow area
                        if lines[j].startswith("Storage Area Is2D="):
                            is2d_str = GeomParser.extract_keyword_value(lines[j], "Storage Area Is2D")
                            try:
                                is_2d = int(is2d_str.strip()) == -1
                            except ValueError:
                                pass

                        if lines[j].startswith("Storage Area Elev Volume="):
                            count_str = GeomParser.extract_keyword_value(lines[j], "Storage Area Elev Volume")
                            try:
                                num_points = int(count_str.strip())
                            except ValueError:
                                pass

                            # Parse first and last elevation values
                            if num_points > 0:
                                values = []
                                k = j + 1
                                total_needed = num_points * 2
                                while len(values) < total_needed and k < len(lines):
                                    if '=' in lines[k]:
                                        break
                                    parsed = GeomParser.parse_fixed_width(lines[k], GeomStorage.FIXED_WIDTH_COLUMN)
                                    values.extend(parsed)
                                    k += 1

                                if len(values) >= 2:
                                    # Elevations are at even indices (0, 2, 4, ...)
                                    elevations = values[0::2]
                                    if elevations:
                                        min_elev = elevations[0]
                                        max_elev = elevations[-1] if len(elevations) > 1 else elevations[0]
                            break

                        # Stop at next storage area or section
                        if lines[j].startswith("Storage Area=") or lines[j].startswith("River Reach="):
                            break

                    storage_areas.append({
                        'Name': sa_name,
                        'NumPoints': num_points,
                        'MinElev': min_elev,
                        'MaxElev': max_elev,
                        'Is2D': is_2d
                    })

                i += 1

            df = pd.DataFrame(storage_areas)

            # Filter out 2D flow areas if requested
            if exclude_2d and not df.empty and 'Is2D' in df.columns:
                original_count = len(df)
                df = df[~df['Is2D']].reset_index(drop=True)
                if original_count != len(df):
                    logger.debug(f"Excluded {original_count - len(df)} 2D flow areas")

            logger.info(f"Found {len(df)} storage areas in {geom_file.name}")
            return df

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading storage areas: {str(e)}")
            raise IOError(f"Failed to read storage areas: {str(e)}")

    @staticmethod
    @log_call
    def get_elevation_volume(geom_file: Union[str, Path],
                            storage_name: str) -> pd.DataFrame:
        """
        Extract elevation-volume curve for a storage area.

        Parameters:
            geom_file (Union[str, Path]): Path to geometry file
            storage_name (str): Storage area name (case-sensitive)

        Returns:
            pd.DataFrame: DataFrame with columns:
                - Elevation (float): Storage elevation (ft or m)
                - Volume (float): Storage volume at elevation (acre-ft or m³)

        Raises:
            FileNotFoundError: If geometry file doesn't exist
            ValueError: If storage area not found

        Example:
            >>> elev_vol = GeomStorage.get_elevation_volume("model.g01", "Reservoir Pool 1")
            >>> print(f"Storage curve has {len(elev_vol)} points")
            >>> print(f"Elevation range: {elev_vol['Elevation'].min():.1f} to {elev_vol['Elevation'].max():.1f}")
            >>> print(f"Max volume: {elev_vol['Volume'].max():,.0f} acre-ft")
        """
        geom_file = Path(geom_file)

        if not geom_file.exists():
            raise FileNotFoundError(f"Geometry file not found: {geom_file}")

        try:
            with open(geom_file, 'r') as f:
                lines = f.readlines()

            # Find the storage area
            sa_idx = None
            for i, line in enumerate(lines):
                if line.startswith("Storage Area="):
                    value_str = GeomParser.extract_keyword_value(line, "Storage Area")
                    # Storage Area format: Name,X,Y - extract just the name
                    parts = [p.strip() for p in value_str.split(',')]
                    sa_name = parts[0] if parts else value_str
                    if sa_name == storage_name:
                        sa_idx = i
                        break

            if sa_idx is None:
                raise ValueError(f"Storage area not found: {storage_name}")

            # Find elevation-volume data
            for j in range(sa_idx+1, min(sa_idx+50, len(lines))):
                if lines[j].startswith("Storage Area Elev Volume="):
                    count_str = GeomParser.extract_keyword_value(lines[j], "Storage Area Elev Volume")
                    count = int(count_str.strip())

                    # Parse elevation-volume pairs
                    total_values = count * 2
                    values = []
                    k = j + 1
                    while len(values) < total_values and k < len(lines):
                        if '=' in lines[k]:
                            break
                        parsed = GeomParser.parse_fixed_width(lines[k], GeomStorage.FIXED_WIDTH_COLUMN)
                        values.extend(parsed)
                        k += 1

                    # Split into elevations and volumes
                    elevations = values[0::2]
                    volumes = values[1::2]

                    df = pd.DataFrame({
                        'Elevation': elevations[:count],
                        'Volume': volumes[:count]
                    })

                    logger.info(f"Extracted {len(df)} elevation-volume points for {storage_name}")
                    return df

                # Stop at next storage area
                if lines[j].startswith("Storage Area="):
                    break

            raise ValueError(f"Elevation-volume data not found for {storage_name}")

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading elevation-volume: {str(e)}")
            raise IOError(f"Failed to read elevation-volume: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\geom\__init__.py
==================================================
"""
Geometry Subpackage - HEC-RAS geometry file operations

This subpackage provides comprehensive functionality for reading and modifying
HEC-RAS plain text geometry files (.g##). It handles 1D cross sections, 2D flow
areas, storage areas, connections, inline structures, bridges, and culverts.

Classes:
    GeomParser - Utility functions for parsing geometry files
    GeomPreprocessor - Geometry preprocessor file operations
    GeomLandCover - 2D Manning's n land cover operations
    GeomCrossSection - 1D cross section operations
    GeomStorage - Storage area operations
    GeomLateral - Lateral structures and SA/2D connections
    GeomInlineWeir - Inline weir operations
    GeomBridge - Bridge operations
    GeomCulvert - Culvert operations

Example:
    >>> from ras_commander import GeomCrossSection, GeomBridge
    >>>
    >>> # Get cross section data
    >>> xs_df = GeomCrossSection.get_cross_sections("model.g01")
    >>>
    >>> # Get bridge deck geometry
    >>> deck_df = GeomBridge.get_deck("model.g01", "River", "Reach", "1000")
"""

from .GeomParser import GeomParser
from .GeomPreprocessor import GeomPreprocessor
from .GeomLandCover import GeomLandCover
from .GeomCrossSection import GeomCrossSection
from .GeomStorage import GeomStorage
from .GeomLateral import GeomLateral
from .GeomInlineWeir import GeomInlineWeir
from .GeomBridge import GeomBridge
from .GeomCulvert import GeomCulvert

__all__ = [
    'GeomParser',
    'GeomPreprocessor',
    'GeomLandCover',
    'GeomCrossSection',
    'GeomStorage',
    'GeomLateral',
    'GeomInlineWeir',
    'GeomBridge',
    'GeomCulvert',
]

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\AGENTS.md
==================================================
# HDF Subpackage - Developer Guidance

This document provides guidance for AI agents and developers working with the `ras_commander.hdf` subpackage.

## Overview

The HDF subpackage provides comprehensive HDF5 file operations for HEC-RAS plan files (.p##.hdf) and geometry files (.g##.hdf). It contains 18 classes organized by function.

## Module Structure

```
ras_commander/hdf/
├── __init__.py              # Public API exports
├── AGENTS.md                # This file
│
├── # Core
├── HdfBase.py               # Foundation class
├── HdfUtils.py              # Utility functions
├── HdfPlan.py               # Plan file info
│
├── # Geometry
├── HdfMesh.py               # 2D mesh operations
├── HdfXsec.py               # Cross-section geometry
├── HdfBndry.py              # Boundary features
├── HdfStruc.py              # Structure geometry (2D)
├── HdfHydraulicTables.py    # HTAB extraction
│
├── # Results
├── HdfResultsPlan.py        # Plan results (steady/unsteady)
├── HdfResultsMesh.py        # Mesh results
├── HdfResultsXsec.py        # XS results
├── HdfResultsBreach.py      # Breach results
│
├── # Infrastructure
├── HdfPipe.py               # Pipe networks
├── HdfPump.py               # Pump stations
├── HdfInfiltration.py       # Infiltration parameters
│
├── # Visualization
├── HdfPlot.py               # General plotting
├── HdfResultsPlot.py        # Results visualization
│
└── # Analysis
    └── HdfFluvialPluvial.py # Fluvial-pluvial analysis
```

## Lazy Loading Pattern

Heavy dependencies are lazy-loaded inside methods to reduce import overhead:

### Dependencies by Category

| Dependency | Import Time | Used In |
|------------|-------------|---------|
| **Core (always loaded)** | | |
| h5py | ~20ms | All classes |
| numpy | ~50ms | All classes |
| pandas | ~100ms | All classes |
| **Lazy Loaded** | | |
| geopandas | ~200ms | HdfMesh, HdfXsec, HdfBndry, HdfStruc, HdfPipe, HdfPump |
| shapely | ~50ms | HdfMesh, HdfXsec, HdfBndry, HdfBase |
| xarray | ~100ms | HdfResultsMesh, HdfResultsXsec, HdfPipe, HdfPump |
| matplotlib | ~300ms | HdfPlot, HdfResultsPlot |
| scipy | ~150ms | HdfUtils (KDTree only) |

### Implementation Pattern

```python
# At module level - only core dependencies
import h5py
import numpy as np
import pandas as pd
from typing import TYPE_CHECKING

# Type hints only - not imported at runtime
if TYPE_CHECKING:
    from geopandas import GeoDataFrame

# Inside methods - lazy load heavy dependencies
@staticmethod
def get_mesh_cell_polygons(hdf_path: Path) -> 'GeoDataFrame':
    # Lazy imports for heavy dependencies
    from geopandas import GeoDataFrame
    from shapely.geometry import Polygon
    from shapely.ops import polygonize

    # Method implementation...
    return GeoDataFrame(...)
```

## Class Hierarchy

```
HdfBase (foundation)
  ├── HdfMesh (uses HdfBase)
  ├── HdfPlan (uses HdfBase)
  ├── HdfXsec (uses HdfBase)
  ├── HdfBndry (uses HdfBase)
  ├── HdfStruc (uses HdfBase, HdfXsec)
  ├── HdfResultsMesh (uses HdfBase, HdfMesh)
  ├── HdfResultsPlan (uses HdfBase)
  ├── HdfResultsXsec (uses HdfBase)
  ├── HdfResultsBreach (uses HdfBase)
  ├── HdfFluvialPluvial (uses HdfMesh, HdfResultsMesh)
  └── HdfUtils (standalone utilities)

Specialized:
  ├── HdfPipe (standalone)
  ├── HdfPump (standalone)
  ├── HdfInfiltration (standalone)
  ├── HdfHydraulicTables (standalone)
  ├── HdfPlot (visualization)
  └── HdfResultsPlot (visualization)
```

## Import Patterns

### From Parent Package (Recommended)
```python
from ras_commander import HdfResultsPlan, HdfMesh

wse = HdfResultsPlan.get_steady_wse("plan.hdf")
cells = HdfMesh.get_mesh_cell_polygons("plan.hdf")
```

### From Subpackage (Direct)
```python
from ras_commander.hdf import HdfResultsPlan, HdfMesh
```

## Decorator Usage

All public methods use consistent decorators:

1. **`@staticmethod`** - All methods are static
2. **`@log_call`** - Automatic function call logging
3. **`@standardize_input(file_type='plan_hdf'|'geom_hdf')`** - Input path standardization

### File Type Expectations

| file_type | Extension | Classes |
|-----------|-----------|---------|
| `plan_hdf` | .p##.hdf | HdfResultsPlan, HdfResultsMesh, HdfResultsXsec, HdfResultsBreach |
| `geom_hdf` | .g##.hdf | HdfMesh, HdfXsec, HdfStruc, HdfBndry, HdfHydraulicTables |

## Adding New HDF Methods

When adding new methods:

1. **Use Decorators**:
   ```python
   @staticmethod
   @log_call
   @standardize_input(file_type='plan_hdf')
   def new_method(hdf_path: Path) -> pd.DataFrame:
   ```

2. **Lazy Load Heavy Dependencies**:
   ```python
   def get_something_with_geometry(hdf_path: Path) -> 'GeoDataFrame':
       from geopandas import GeoDataFrame
       from shapely.geometry import Polygon
       # ... method body
   ```

3. **Use h5py Context Manager**:
   ```python
   with h5py.File(hdf_path, 'r') as hdf_file:
       # Read data
       data = hdf_file["/some/path"][()]
   ```

4. **Handle Errors Gracefully**:
   ```python
   try:
       # HDF operations
   except Exception as e:
       logger.error(f"Error reading from {hdf_path}: {str(e)}")
       return pd.DataFrame()  # or GeoDataFrame() or {}
   ```

## Common HDF Paths

### Plan HDF (.p##.hdf)
```
/Results/Unsteady/Output/Output Blocks/...
/Results/Summary/...
/Plan Data/Plan Parameters
```

### Geometry HDF (.g##.hdf)
```
/Geometry/2D Flow Areas/{mesh_name}/...
/Geometry/Cross Sections/...
/Geometry/Structures/...
```

## Testing

Test import and basic functionality:
```python
from ras_commander import HdfMesh, HdfResultsPlan

# Check methods exist
assert hasattr(HdfMesh, 'get_mesh_cell_polygons')
assert hasattr(HdfResultsPlan, 'get_steady_wse')
```

## Version History

- **v0.80.0**: Initial HDF implementation
- **v0.80.3**: Added steady flow support
- **v0.81.0**: Added HdfHydraulicTables
- **v0.86.0**: Moved to `hdf/` subpackage with lazy loading

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfBase.py
==================================================
"""
HdfBase: Core HDF File Operations for HEC-RAS

This module provides fundamental methods for interacting with HEC-RAS HDF files.
It serves as a foundation for more specialized HDF classes.

Attribution:
    Derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    Copyright (c) 2024 fema-ffrd - MIT License

Features:
    - Time parsing and conversion utilities
    - HDF attribute and dataset access
    - Geometric data extraction
    - 2D flow area information retrieval

Classes:
    HdfBase: Base class containing static methods for HDF operations

Key Methods:
    Time Operations:
        - get_simulation_start_time(): Get simulation start datetime
        - get_unsteady_timestamps(): Get unsteady output timestamps
        - parse_ras_datetime(): Parse RAS datetime strings
    
    Data Access:
        - get_2d_flow_area_names_and_counts(): Get 2D flow area info
        - get_projection(): Get spatial projection
        - get_attrs(): Access HDF attributes
        - get_dataset_info(): Explore HDF structure
        - get_polylines_from_parts(): Extract geometric polylines

Example:
    ```python
    from ras_commander import HdfBase
    
    with h5py.File('model.hdf', 'r') as hdf:
        start_time = HdfBase.get_simulation_start_time(hdf)
        timestamps = HdfBase.get_unsteady_timestamps(hdf)
    ```
"""
import re
from datetime import datetime, timedelta
import h5py
import numpy as np
import pandas as pd
import xarray as xr
from typing import List, Tuple, Union, Optional, Dict, Any
from pathlib import Path
import logging
from shapely.geometry import LineString, MultiLineString

from .HdfUtils import HdfUtils
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfBase:
    """
    Base class for HEC-RAS HDF file operations.

    This class provides static methods for fundamental HDF file operations,
    including time parsing, attribute access, and geometric data extraction.
    All methods are designed to work with h5py.File objects or pathlib.Path
    inputs.

    Note:
        This class is not meant to be instantiated. All methods are static
        and should be called directly from the class.
    """

    @staticmethod
    def get_simulation_start_time(hdf_file: h5py.File) -> datetime:
        """
        Extract the simulation start time from the HDF file.

        Args:
            hdf_file: Open HDF file object containing RAS simulation data.

        Returns:
            datetime: Simulation start time as a datetime object.

        Raises:
            ValueError: If Plan Information is not found or start time cannot be parsed.
        
        Note:
            Expects 'Plan Data/Plan Information' group with 'Simulation Start Time' attribute.
        """
        plan_info = hdf_file.get("Plan Data/Plan Information")
        if plan_info is None:
            raise ValueError("Plan Information not found in HDF file")
        time_str = plan_info.attrs.get('Simulation Start Time')
        return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))

    @staticmethod
    def get_unsteady_timestamps(hdf_file: h5py.File) -> List[datetime]:
        """
        Extract the list of unsteady timestamps from the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.

        Returns:
            List[datetime]: A list of datetime objects representing the unsteady timestamps.
        """
        group_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
        raw_datetimes = hdf_file[group_path][:]
        return [HdfUtils.parse_ras_datetime_ms(x.decode("utf-8")) for x in raw_datetimes]

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_2d_flow_area_names_and_counts(hdf_path: Path) -> List[Tuple[str, int]]:
        """
        Get the names and cell counts of 2D flow areas from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            List[Tuple[str, int]]: A list of tuples containing the name and cell count of each 2D flow area.

        Raises:
            ValueError: If there's an error reading the HDF file or accessing the required data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                flow_area_2d_path = "Geometry/2D Flow Areas"
                if flow_area_2d_path not in hdf_file:
                    return []
                
                attributes = hdf_file[f"{flow_area_2d_path}/Attributes"][()]
                names = [HdfUtils.convert_ras_string(name) for name in attributes["Name"]]
                
                cell_info = hdf_file[f"{flow_area_2d_path}/Cell Info"][()]
                cell_counts = [info[1] for info in cell_info]
                
                return list(zip(names, cell_counts))
        except Exception as e:
            logger.error(f"Error reading 2D flow area names and counts from {hdf_path}: {str(e)}")
            raise ValueError(f"Failed to get 2D flow area names and counts: {str(e)}")


    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_projection(hdf_path: Path) -> Optional[str]:
        """
        Get projection information from HDF file or RASMapper project file.
        Converts WKT projection to EPSG code for GeoDataFrame compatibility.
        
        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            Optional[str]: The projection as EPSG code (e.g. "EPSG:6556"), or None if not found.
        """
        from pyproj import CRS

        project_folder = hdf_path.parent
        wkt = None
        proj_file = None  # Initialize proj_file variable
        
        # Try HDF file
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                proj_wkt = hdf_file.attrs.get("Projection")
                if proj_wkt is not None:
                    if isinstance(proj_wkt, (bytes, np.bytes_)):
                        wkt = proj_wkt.decode("utf-8")
                        logger.info(f"Found projection in HDF file: {hdf_path}")
                        return wkt
        except Exception as e:
            logger.error(f"Error reading projection from HDF file {hdf_path}: {str(e)}")

        # Try RASMapper file if no HDF projection
        if not wkt:
            try:
                rasmap_files = list(project_folder.glob("*.rasmap"))
                if rasmap_files:
                    with open(rasmap_files[0], 'r') as f:
                        content = f.read()
                        
                    proj_match = re.search(r'<RASProjectionFilename Filename="(.*?)"', content)
                    if proj_match:
                        proj_file = project_folder / proj_match.group(1).replace('.\\', '')
                        if proj_file.exists():
                            with open(proj_file, 'r') as f:
                                wkt = f.read().strip()
                                logger.info(f"Found projection in RASMapper file: {proj_file}")
                                return wkt
            except Exception as e:
                logger.error(f"Error reading RASMapper projection file: {str(e)}")
        
        # Customize error message based on whether proj_file was found
        if proj_file:
            error_msg = (
                "No valid projection found. Checked:\n"
                f"1. HDF file projection attribute: {hdf_path}\n"
                f"2. RASMapper projection file {proj_file} found in RASMapper file, but was invalid"
            )
        else:
            error_msg = (
                "No valid projection found. Checked:\n"
                f"1. HDF file projection attribute: {hdf_path}\n was checked and no projection attribute found"
                "2. No RASMapper projection file found"
            )

        error_msg += (
            "\nTo fix this:\n"
            "1. Open RASMapper\n"
            "2. Click Map > Set Projection\n" 
            "3. Select an appropriate projection file or coordinate system\n"
            "4. Save the RASMapper project"
        )
        
        logger.critical(error_msg)
        return None

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_attrs(hdf_file: h5py.File, attr_path: str) -> Dict[str, Any]:
        """
        Get attributes from an HDF file at a specified path.

        Args:
            hdf_file (h5py.File): The opened HDF file.
            attr_path (str): Path to the attributes in the HDF file.

        Returns:
            Dict[str, Any]: Dictionary of attributes.
        """
        try:
            if attr_path not in hdf_file:
                logger.warning(f"Path {attr_path} not found in HDF file")
                return {}
            
            return HdfUtils.convert_hdf5_attrs_to_dict(hdf_file[attr_path].attrs)
        except Exception as e:
            logger.error(f"Error getting attributes from {attr_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_dataset_info(file_path: Path, group_path: str = '/') -> None:
        """
        Recursively explore and print the structure of an HDF5 file.

        Displays detailed information about groups, datasets, and their attributes
        in a hierarchical format.

        Args:
            file_path: Path to the HDF5 file.
            group_path: Starting group path to explore (default: root '/').

        Prints:
            - Group and dataset names with hierarchical indentation
            - Dataset shapes and data types
            - All attributes for groups and datasets
        """
        def recurse(name, obj, indent=0):
            spacer = "    " * indent
            if isinstance(obj, h5py.Group):
                print(f"{spacer}Group: {name}")
                HdfBase.print_attrs(name, obj)
                for key in obj:
                    recurse(f"{name}/{key}", obj[key], indent+1)
            elif isinstance(obj, h5py.Dataset):
                print(f"{spacer}Dataset: {name}")
                print(f"{spacer}    Shape: {obj.shape}")
                print(f"{spacer}    Dtype: {obj.dtype}")
                HdfBase.print_attrs(name, obj)
            else:
                print(f"{spacer}Unknown object: {name}")

        try:
            with h5py.File(file_path, 'r') as hdf_file:
                if group_path in hdf_file:
                    print("")
                    print(f"Exploring group: {group_path}\n")
                    group = hdf_file[group_path]
                    for key in group:
                        print("")
                        recurse(f"{group_path}/{key}", group[key], indent=1)
                else:
                    print(f"Group path '{group_path}' not found in the HDF5 file.")
        except Exception as e:
            print(f"Error exploring HDF5 file: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_polylines_from_parts(hdf_path: Path, path: str, info_name: str = "Polyline Info", 
                              parts_name: str = "Polyline Parts", 
                              points_name: str = "Polyline Points") -> List[LineString]:
        """
        Extract polylines from HDF file parts data.

        Args:
            hdf_path: Path to the HDF file.
            path: Internal HDF path to polyline data.
            info_name: Name of polyline info dataset.
            parts_name: Name of polyline parts dataset.
            points_name: Name of polyline points dataset.

        Returns:
            List of Shapely LineString/MultiLineString geometries.

        Note:
            Expects HDF datasets containing:
            - Polyline information (start points and counts)
            - Parts information for multi-part lines
            - Point coordinates
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                polyline_info_path = f"{path}/{info_name}"
                polyline_parts_path = f"{path}/{parts_name}"
                polyline_points_path = f"{path}/{points_name}"

                polyline_info = hdf_file[polyline_info_path][()]
                polyline_parts = hdf_file[polyline_parts_path][()]
                polyline_points = hdf_file[polyline_points_path][()]

                geoms = []
                for pnt_start, pnt_cnt, part_start, part_cnt in polyline_info:
                    points = polyline_points[pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(LineString(points))
                    else:
                        parts = polyline_parts[part_start : part_start + part_cnt]
                        geoms.append(
                            MultiLineString(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return geoms
        except Exception as e:
            logger.error(f"Error getting polylines: {str(e)}")
            return []

    @staticmethod
    def print_attrs(name: str, obj: Union[h5py.Dataset, h5py.Group]) -> None:
        """
        Print the attributes of an HDF5 object (Dataset or Group).

        Args:
            name (str): Name of the object
            obj (Union[h5py.Dataset, h5py.Group]): HDF5 object whose attributes are to be printed
        """
        if len(obj.attrs) > 0:
            print(f"    Attributes for {name}:")
            for key, value in obj.attrs.items():
                print(f"        {key}: {value}")




==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfBndry.py
==================================================
"""
Class: HdfBndry

A utility class for extracting and processing boundary-related features from HEC-RAS HDF files,
including boundary conditions, breaklines, refinement regions, and reference features.

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfBndry:
- get_bc_lines()           # Returns boundary condition lines as a GeoDataFrame.
- get_breaklines()         # Returns 2D mesh area breaklines as a GeoDataFrame.
- get_refinement_regions() # Returns refinement regions as a GeoDataFrame.
- get_reference_lines()    # Returns reference lines as a GeoDataFrame.
- get_reference_points()   # Returns reference points as a GeoDataFrame.



"""
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .HdfMesh import HdfMesh
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfBndry:
    """
    A class for handling boundary-related data from HEC-RAS HDF files.

    This class provides methods to extract and process various boundary elements
    such as boundary condition lines, breaklines, refinement regions, and reference
    lines/points from HEC-RAS geometry HDF files.

    Methods in this class return data primarily as GeoDataFrames, making it easy
    to work with spatial data in a geospatial context.

    Note:
        This class relies on the HdfBase and HdfUtils classes for some of its
        functionality. Ensure these classes are available in the same package.
    """
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_bc_lines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area boundary condition lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the boundary condition lines and their attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                bc_lines_path = "Geometry/Boundary Condition Lines"
                if bc_lines_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                # Get geometries
                bc_line_data = hdf_file[bc_lines_path]
                geoms = HdfBase.get_polylines_from_parts(hdf_path, bc_lines_path)
                
                # Get attributes
                attributes = pd.DataFrame(bc_line_data["Attributes"][()])
                
                # Convert string columns
                str_columns = ['Name', 'SA-2D', 'Type']
                for col in str_columns:
                    if col in attributes.columns:
                        attributes[col] = attributes[col].apply(HdfUtils.convert_ras_string)
                
                # Create GeoDataFrame with all attributes
                gdf = gpd.GeoDataFrame(
                    attributes,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_file)
                )
                
                # Add ID column if not present
                if 'bc_line_id' not in gdf.columns:
                    gdf['bc_line_id'] = range(len(gdf))
                    
                return gdf

        except Exception as e:
            logger.error(f"Error reading boundary condition lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_breaklines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area breaklines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the breaklines.

        Notes
        -----
        - Zero-length breaklines are logged and skipped. 
        - Single-point breaklines are logged and skipped.
        - These invalid breaklines should be removed in RASMapper to prevent potential issues.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                breaklines_path = "Geometry/2D Flow Area Break Lines"
                if breaklines_path not in hdf_file:
                    logger.warning(f"Breaklines path '{breaklines_path}' not found in HDF file.")
                    return gpd.GeoDataFrame()

                bl_line_data = hdf_file[breaklines_path]
                attributes = bl_line_data["Attributes"][()]
                
                # Initialize lists to store valid breakline data
                valid_ids = []
                valid_names = []
                valid_geoms = []

                # Track invalid breaklines for summary
                zero_length_count = 0
                single_point_count = 0
                other_error_count = 0

                # Process each breakline
                for idx, (pnt_start, pnt_cnt, part_start, part_cnt) in enumerate(bl_line_data["Polyline Info"][()]):
                    name = HdfUtils.convert_ras_string(attributes["Name"][idx])

                    # Check for zero-length breaklines
                    if pnt_cnt == 0:
                        zero_length_count += 1
                        logger.debug(f"Zero-length breakline found (FID: {idx}, Name: {name})")
                        continue

                    # Check for single-point breaklines
                    if pnt_cnt == 1:
                        single_point_count += 1
                        logger.debug(f"Single-point breakline found (FID: {idx}, Name: {name})")
                        continue

                    try:
                        points = bl_line_data["Polyline Points"][()][pnt_start:pnt_start + pnt_cnt]
                        
                        # Additional validation of points array
                        if len(points) < 2:
                            single_point_count += 1
                            logger.debug(f"Invalid point count in breakline (FID: {idx}, Name: {name})")
                            continue

                        if part_cnt == 1:
                            geom = LineString(points)
                        else:
                            parts = bl_line_data["Polyline Parts"][()][part_start:part_start + part_cnt]
                            geom = MultiLineString([
                                points[part_pnt_start:part_pnt_start + part_pnt_cnt]
                                for part_pnt_start, part_pnt_cnt in parts
                                if part_pnt_cnt > 1  # Skip single-point parts
                            ])
                            # Skip if no valid parts remain
                            if len(geom.geoms) == 0:
                                other_error_count += 1
                                logger.debug(f"No valid parts in multipart breakline (FID: {idx}, Name: {name})")
                                continue

                        valid_ids.append(idx)
                        valid_names.append(name)
                        valid_geoms.append(geom)

                    except Exception as e:
                        other_error_count += 1
                        logger.debug(f"Error processing breakline {idx}: {str(e)}")
                        continue

                # Log summary of invalid breaklines
                total_invalid = zero_length_count + single_point_count + other_error_count
                if total_invalid > 0:
                    logger.info(
                        f"Breakline processing summary:\n"
                        f"- Zero-length breaklines: {zero_length_count}\n"
                        f"- Single-point breaklines: {single_point_count}\n"
                        f"- Other invalid breaklines: {other_error_count}\n"
                        f"Consider removing these invalid breaklines using RASMapper."
                    )

                # Create GeoDataFrame with valid breaklines
                if not valid_ids:
                    logger.warning("No valid breaklines found in the HDF file.")
                    return gpd.GeoDataFrame()

                return gpd.GeoDataFrame(
                    {
                        "bl_id": valid_ids,
                        "Name": valid_names,
                        "geometry": valid_geoms
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading breaklines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_refinement_regions(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area refinement regions.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the refinement regions.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                refinement_regions_path = "/Geometry/2D Flow Area Refinement Regions"
                if refinement_regions_path not in hdf_file:
                    return gpd.GeoDataFrame()
                rr_data = hdf_file[refinement_regions_path]
                rr_ids = range(rr_data["Attributes"][()].shape[0])
                names = np.vectorize(HdfUtils.convert_ras_string)(rr_data["Attributes"][()]["Name"])
                geoms = list()
                for pnt_start, pnt_cnt, part_start, part_cnt in rr_data["Polygon Info"][()]:
                    points = rr_data["Polygon Points"][()][pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(Polygon(points))
                    else:
                        parts = rr_data["Polygon Parts"][()][part_start : part_start + part_cnt]
                        geoms.append(
                            MultiPolygon(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return gpd.GeoDataFrame(
                    {"rr_id": rr_ids, "Name": names, "geometry": geoms},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading refinement regions: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_lines(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference lines geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference lines. If mesh_name is provided,
            returns only lines for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_lines_path = "Geometry/Reference Lines"
                attributes_path = f"{reference_lines_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                attributes = hdf_file[attributes_path][()]
                refline_ids = range(attributes.shape[0])
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA-2D"])
                
                try:
                    types = v_conv_str(attributes["Type"])
                except ValueError:
                    types = np.array([""] * attributes.shape[0])
                
                geoms = HdfBase.get_polylines_from_parts(hdf_path, reference_lines_path)
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refln_id": refline_ids,
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Type": types,
                        "geometry": geoms,
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_points(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference points geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference points. If mesh_name is provided,
            returns only points for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_points_path = "Geometry/Reference Points"
                attributes_path = f"{reference_points_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                ref_points_group = hdf_file[reference_points_path]
                attributes = ref_points_group["Attributes"][:]
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA/2D"])
                cell_id = attributes["Cell Index"]
                points = ref_points_group["Points"][()]
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refpt_id": range(attributes.shape[0]),
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Cell Index": cell_id,
                        "geometry": list(map(Point, points)),
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference points: {str(e)}")
            return gpd.GeoDataFrame()

    

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfFluvialPluvial.py
==================================================
"""
Class: HdfFluvialPluvial

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfFluvialPluvial:
- calculate_fluvial_pluvial_boundary(): Returns LineStrings representing the boundary.
- generate_fluvial_pluvial_polygons(): Returns dissolved Polygons for fluvial, pluvial, and ambiguous zones.
- _process_cell_adjacencies()
- _get_boundary_cell_pairs()
- _identify_boundary_edges()

"""

from typing import Dict, List, Tuple, Set, Optional
import pandas as pd
import geopandas as gpd
from collections import defaultdict
from shapely.geometry import LineString, MultiLineString
from tqdm import tqdm
from .HdfMesh import HdfMesh
from .HdfUtils import HdfUtils
from ..Decorators import standardize_input
from .HdfResultsMesh import HdfResultsMesh
from ..LoggingConfig import get_logger
from pathlib import Path

logger = get_logger(__name__)

class HdfFluvialPluvial:
    """
    A class for analyzing and visualizing fluvial-pluvial boundaries in HEC-RAS 2D model results.

    This class provides methods to process and visualize HEC-RAS 2D model outputs,
    specifically focusing on the delineation of fluvial and pluvial flood areas.
    It includes functionality for calculating fluvial-pluvial boundaries based on
    the timing of maximum water surface elevations.

    Key Concepts:
    - Fluvial flooding: Flooding from rivers/streams
    - Pluvial flooding: Flooding from rainfall/surface water
    - delta_t: Time threshold (in hours) used to distinguish between fluvial and pluvial cells.
               Cells with max WSE time differences greater than delta_t are considered boundaries.

    Data Requirements:
    - HEC-RAS plan HDF file containing:
        - 2D mesh cell geometry (accessed via HdfMesh)
        - Maximum water surface elevation times (accessed via HdfResultsMesh)

    Usage Example:
        >>> from ras_commander import HdfFluvialPluvial
        >>> hdf_path = Path("path/to/plan.hdf")
        
        # To get just the boundary lines
        >>> boundary_lines_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(
        ...     hdf_path, 
        ...     delta_t=12
        ... )
        
        # To get classified flood polygons
        >>> flood_polygons_gdf = HdfFluvialPluvial.generate_fluvial_pluvial_polygons(
        ...     hdf_path,
        ...     delta_t=12,
        ...     temporal_tolerance_hours=1.0
        ... )
    """
    def __init__(self):
        self.logger = get_logger(__name__)  # Initialize logger with module name
    
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def calculate_fluvial_pluvial_boundary(
        hdf_path: Path, 
        delta_t: float = 12,
        min_line_length: Optional[float] = None
    ) -> gpd.GeoDataFrame:
        """
        Calculate the fluvial-pluvial boundary lines based on cell polygons and maximum water surface elevation times.

        This function is useful for visualizing the line of transition between flooding mechanisms.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            delta_t (float): Threshold time difference in hours. Cells with time differences
                             greater than this value are considered boundaries. Default is 12 hours.
            min_line_length (float, optional): Minimum length (in CRS units) for boundary lines to be included.
                                               Lines shorter than this will be dropped. Default is None (no filtering).

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing the fluvial-pluvial boundary lines.
        """
        try:
            logger.info("Getting cell polygons from HDF file...")
            cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(hdf_path)
            if cell_polygons_gdf.empty:
                raise ValueError("No cell polygons found in HDF file")

            logger.info("Getting maximum water surface data from HDF file...")
            max_ws_df = HdfResultsMesh.get_mesh_max_ws(hdf_path)
            if max_ws_df.empty:
                raise ValueError("No maximum water surface data found in HDF file")

            logger.info("Converting maximum water surface timestamps...")
            max_ws_df['maximum_water_surface_time'] = max_ws_df['maximum_water_surface_time'].apply(
                lambda x: HdfUtils.parse_ras_datetime(x) if isinstance(x, str) else x
            )

            logger.info("Processing cell adjacencies...")
            cell_adjacency, common_edges = HdfFluvialPluvial._process_cell_adjacencies(cell_polygons_gdf)
            
            logger.info("Extracting cell times from maximum water surface data...")
            cell_times = max_ws_df.set_index('cell_id')['maximum_water_surface_time'].to_dict()
            
            logger.info("Identifying boundary edges...")
            boundary_edges = HdfFluvialPluvial._identify_boundary_edges(
                cell_adjacency, common_edges, cell_times, delta_t, min_line_length=min_line_length
            )

            logger.info("Creating final GeoDataFrame for boundaries...")
            boundary_gdf = gpd.GeoDataFrame(
                geometry=boundary_edges, 
                crs=cell_polygons_gdf.crs
            )

            logger.info("Boundary line calculation completed successfully.")
            return boundary_gdf

        except Exception as e:
            logger.error(f"Error calculating fluvial-pluvial boundary lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def generate_fluvial_pluvial_polygons(
        hdf_path: Path, 
        delta_t: float = 12, 
        temporal_tolerance_hours: float = 1.0,
        min_polygon_area_acres: Optional[float] = None
    ) -> gpd.GeoDataFrame:
        """
        Generates dissolved polygons representing fluvial, pluvial, and ambiguous flood zones.

        This function classifies each wetted cell and merges them into three distinct regions
        based on the timing of maximum water surface elevation.

        Optionally, for polygons classified as fluvial or pluvial, if their area is less than
        min_polygon_area_acres, they are reclassified to the opposite type and merged with
        adjacent polygons of that type. Ambiguous polygons are exempt from this logic.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            delta_t (float): The time difference (in hours) between adjacent cells that defines
                             the initial boundary between fluvial and pluvial zones. Default is 12.
            temporal_tolerance_hours (float): The maximum time difference (in hours) for a cell
                                              to be considered part of an expanding region. 
                                              Default is 1.0.
            min_polygon_area_acres (float, optional): Minimum polygon area (in acres). For fluvial or pluvial
                                                      polygons smaller than this, reclassify to the opposite
                                                      type and merge with adjacent polygons of that type.
                                                      Ambiguous polygons are not affected.

        Returns:
            gpd.GeoDataFrame: A GeoDataFrame with dissolved polygons for 'fluvial', 'pluvial',
                              and 'ambiguous' zones.
        """
        try:
            # --- 1. Data Loading and Preparation ---
            logger.info("Loading mesh and results data...")
            cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(hdf_path)
            max_ws_df = HdfResultsMesh.get_mesh_max_ws(hdf_path)
            max_ws_df['maximum_water_surface_time'] = max_ws_df['maximum_water_surface_time'].apply(
                lambda x: HdfUtils.parse_ras_datetime(x) if isinstance(x, str) else x
            )
            cell_times = max_ws_df.set_index('cell_id')['maximum_water_surface_time'].to_dict()
            
            logger.info("Processing cell adjacencies...")
            cell_adjacency, _ = HdfFluvialPluvial._process_cell_adjacencies(cell_polygons_gdf)

            # --- 2. Seeding the Classifications ---
            logger.info(f"Identifying initial boundary seeds with delta_t = {delta_t} hours...")
            boundary_pairs = HdfFluvialPluvial._get_boundary_cell_pairs(cell_adjacency, cell_times, delta_t)

            classifications = pd.Series('unclassified', index=cell_polygons_gdf['cell_id'], name='classification')
            
            for cell1, cell2 in boundary_pairs:
                if cell_times.get(cell1) > cell_times.get(cell2):
                    classifications.loc[cell1] = 'fluvial'
                    classifications.loc[cell2] = 'pluvial'
                else:
                    classifications.loc[cell1] = 'pluvial'
                    classifications.loc[cell2] = 'fluvial'
            
            # --- 3. Iterative Region Growth ---
            logger.info(f"Starting iterative region growth with tolerance = {temporal_tolerance_hours} hours...")
            fluvial_frontier = set(classifications[classifications == 'fluvial'].index)
            pluvial_frontier = set(classifications[classifications == 'pluvial'].index)
            
            iteration = 0
            with tqdm(desc="Region Growing", unit="iter") as pbar:
                while fluvial_frontier or pluvial_frontier:
                    iteration += 1
                    
                    next_fluvial_candidates = set()
                    for cell_id in fluvial_frontier:
                        for neighbor_id in cell_adjacency.get(cell_id, []):
                            if classifications.loc[neighbor_id] == 'unclassified' and pd.notna(cell_times.get(neighbor_id)):
                                time_diff_seconds = abs((cell_times[cell_id] - cell_times[neighbor_id]).total_seconds())
                                if time_diff_seconds <= temporal_tolerance_hours * 3600:
                                    next_fluvial_candidates.add(neighbor_id)
                    
                    next_pluvial_candidates = set()
                    for cell_id in pluvial_frontier:
                        for neighbor_id in cell_adjacency.get(cell_id, []):
                            if classifications.loc[neighbor_id] == 'unclassified' and pd.notna(cell_times.get(neighbor_id)):
                                time_diff_seconds = abs((cell_times[cell_id] - cell_times[neighbor_id]).total_seconds())
                                if time_diff_seconds <= temporal_tolerance_hours * 3600:
                                    next_pluvial_candidates.add(neighbor_id)
                    
                    # Resolve conflicts
                    ambiguous_cells = next_fluvial_candidates.intersection(next_pluvial_candidates)
                    if ambiguous_cells:
                        classifications.loc[list(ambiguous_cells)] = 'ambiguous'
                        
                    # Classify non-conflicted cells
                    newly_fluvial = next_fluvial_candidates - ambiguous_cells
                    if newly_fluvial:
                        classifications.loc[list(newly_fluvial)] = 'fluvial'

                    newly_pluvial = next_pluvial_candidates - ambiguous_cells
                    if newly_pluvial:
                        classifications.loc[list(newly_pluvial)] = 'pluvial'
                    
                    # Update frontiers for the next iteration
                    fluvial_frontier = newly_fluvial
                    pluvial_frontier = newly_pluvial
                                        
                    pbar.update(1)
                    pbar.set_postfix({
                        "Fluvial": len(fluvial_frontier), 
                        "Pluvial": len(pluvial_frontier),
                        "Ambiguous": len(ambiguous_cells)
                    })
            
            logger.info(f"Region growing completed in {iteration} iterations.")
            
            # --- 4. Finalization and Dissolving ---
            # Classify any remaining unclassified (likely isolated) cells as ambiguous
            classifications[classifications == 'unclassified'] = 'ambiguous'

            logger.info("Merging classifications with cell polygons...")
            classified_gdf = cell_polygons_gdf.merge(classifications.to_frame(), left_on='cell_id', right_index=True)
            
            logger.info("Dissolving polygons by classification...")
            final_regions_gdf = classified_gdf.dissolve(by='classification', aggfunc='first').reset_index()

            # --- 5. Minimum Polygon Area Filtering and Merging (if requested) ---
            if min_polygon_area_acres is not None:
                logger.info(f"Applying minimum polygon area filter: {min_polygon_area_acres} acres")
                # Calculate area in acres (1 acre = 4046.8564224 m^2)
                # If CRS is not projected, warn and skip area filtering
                if not final_regions_gdf.crs or not final_regions_gdf.crs.is_projected:
                    logger.warning("CRS is not projected. Area-based filtering skipped.")
                else:
                    # Explode to individual polygons for area filtering
                    exploded = final_regions_gdf.explode(index_parts=False, ignore_index=True)
                    exploded['area_acres'] = exploded.geometry.area / 4046.8564224

                    # Only consider fluvial and pluvial polygons for area filtering
                    mask_fluvial = (exploded['classification'] == 'fluvial') & (exploded['area_acres'] < min_polygon_area_acres)
                    mask_pluvial = (exploded['classification'] == 'pluvial') & (exploded['area_acres'] < min_polygon_area_acres)

                    n_fluvial = mask_fluvial.sum()
                    n_pluvial = mask_pluvial.sum()
                    logger.info(f"Found {n_fluvial} small fluvial and {n_pluvial} small pluvial polygons to reclassify.")

                    # Reclassify small fluvial polygons as pluvial, and small pluvial polygons as fluvial
                    exploded.loc[mask_fluvial, 'classification'] = 'pluvial'
                    exploded.loc[mask_pluvial, 'classification'] = 'fluvial'
                    # Ambiguous polygons are not changed

                    # Redissolve by classification to merge with adjacent polygons of the same type
                    final_regions_gdf = exploded.dissolve(by='classification', aggfunc='first').reset_index()
                    logger.info("Redissolved polygons after reclassification of small areas.")

            logger.info("Polygon generation completed successfully.")
            return final_regions_gdf
            
        except Exception as e:
            logger.error(f"Error generating fluvial-pluvial polygons: {str(e)}", exc_info=True)
            return gpd.GeoDataFrame()
        
        
    @staticmethod
    def _process_cell_adjacencies(cell_polygons_gdf: gpd.GeoDataFrame) -> Tuple[Dict[int, List[int]], Dict[int, Dict[int, LineString]]]:
        """
        Optimized method to process cell adjacencies by extracting shared edges directly.
        """
        cell_adjacency = defaultdict(list)
        common_edges = defaultdict(dict)
        edge_to_cells = defaultdict(set)

        def edge_key(coords1, coords2, precision=8):
            coords1 = tuple(round(coord, precision) for coord in coords1)
            coords2 = tuple(round(coord, precision) for coord in coords2)
            return tuple(sorted([coords1, coords2]))

        for _, row in cell_polygons_gdf.iterrows():
            cell_id = row['cell_id']
            geom = row['geometry']
            if geom.is_empty or not geom.is_valid:
                continue
            coords = list(geom.exterior.coords)
            for i in range(len(coords) - 1):
                key = edge_key(coords[i], coords[i + 1])
                edge_to_cells[key].add(cell_id)

        for edge, cells in edge_to_cells.items():
            cell_list = list(cells)
            if len(cell_list) >= 2:
                for i in range(len(cell_list)):
                    for j in range(i + 1, len(cell_list)):
                        cell1, cell2 = cell_list[i], cell_list[j]
                        cell_adjacency[cell1].append(cell2)
                        cell_adjacency[cell2].append(cell1)
                        common_edge = LineString([edge[0], edge[1]])
                        common_edges[cell1][cell2] = common_edge
                        common_edges[cell2][cell1] = common_edge

        return cell_adjacency, common_edges
    
    @staticmethod
    def _get_boundary_cell_pairs(
        cell_adjacency: Dict[int, List[int]], 
        cell_times: Dict[int, pd.Timestamp], 
        delta_t: float
    ) -> List[Tuple[int, int]]:
        """
        Identifies pairs of adjacent cell IDs that form a boundary.

        A boundary is defined where the difference in max water surface time
        between two adjacent cells is greater than delta_t.
        
        Args:
            cell_adjacency (Dict[int, List[int]]): Dictionary of cell adjacencies.
            cell_times (Dict[int, pd.Timestamp]): Dictionary mapping cell IDs to their max WSE times.
            delta_t (float): Time threshold in hours.

        Returns:
            List[Tuple[int, int]]: A list of tuples, where each tuple contains a pair of
                                   cell IDs forming a boundary.
        """
        boundary_cell_pairs = []
        processed_pairs = set()
        delta_t_seconds = delta_t * 3600

        for cell_id, neighbors in cell_adjacency.items():
            time1 = cell_times.get(cell_id)
            if not pd.notna(time1):
                continue

            for neighbor_id in neighbors:
                pair = tuple(sorted((cell_id, neighbor_id)))
                if pair in processed_pairs:
                    continue

                time2 = cell_times.get(neighbor_id)
                if not pd.notna(time2):
                    continue
                
                time_diff = abs((time1 - time2).total_seconds())

                if time_diff >= delta_t_seconds:
                    boundary_cell_pairs.append(pair)
                
                processed_pairs.add(pair)
        
        return boundary_cell_pairs

    @staticmethod
    def _identify_boundary_edges(
        cell_adjacency: Dict[int, List[int]], 
        common_edges: Dict[int, Dict[int, LineString]], 
        cell_times: Dict[int, pd.Timestamp], 
        delta_t: float,
        min_line_length: Optional[float] = None
    ) -> List[LineString]:
        """
        Identify boundary edges between cells with significant time differences.
        
        This function now uses the helper `_get_boundary_cell_pairs`.

        Args:
            cell_adjacency (Dict[int, List[int]]): Dictionary of cell adjacencies.
            common_edges (Dict[int, Dict[int, LineString]]): Dictionary of shared edges between cells.
            cell_times (Dict[int, pd.Timestamp]): Dictionary mapping cell IDs to their max WSE times.
            delta_t (float): Time threshold in hours.
            min_line_length (float, optional): Minimum length (in CRS units) for boundary lines to be included.
                                               Lines shorter than this will be dropped. Default is None (no filtering).

        Returns:
            List[LineString]: List of LineString geometries representing boundaries.
        """
        boundary_pairs = HdfFluvialPluvial._get_boundary_cell_pairs(cell_adjacency, cell_times, delta_t)
        
        boundary_edges = [common_edges[c1][c2] for c1, c2 in boundary_pairs]
        
        logger.info(f"Identified {len(boundary_edges)} boundary edges using delta_t of {delta_t} hours.")

        if min_line_length is not None:
            filtered_edges = [edge for edge in boundary_edges if edge.length >= min_line_length]
            num_dropped = len(boundary_edges) - len(filtered_edges)
            if num_dropped > 0:
                logger.info(f"{num_dropped} boundary line(s) shorter than {min_line_length} units were dropped after filtering.")
            boundary_edges = filtered_edges

        return boundary_edges

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfHydraulicTables.py
==================================================
"""
HdfHydraulicTables - Extract hydraulic property tables (HTAB) from HEC-RAS geometry HDF files

All methods are static and designed to be used without instantiation.

Hydraulic property tables contain preprocessed hydraulic properties computed during
geometry preprocessing. These tables enable hydraulic analysis without re-running HEC-RAS,
including:
- Area vs elevation curves
- Conveyance vs elevation
- Wetted perimeter vs elevation
- Top width vs elevation
- Rating curves and stage-discharge relationships

Available Functions:
- get_xs_htab() - Extract property table for a single cross section
- get_all_xs_htabs() - Extract property tables for all cross sections

Technical Notes:
    Property tables are stored in geometry HDF files (.g##.hdf), NOT plan HDF files.
    Path: /Geometry/Cross Sections/Property Tables/

    Data Structure:
        - XSEC Info: Index array mapping XS to property table rows [start_index, count, ds_cell]
        - XSEC Value: Property table data (N rows × 23 columns)
        - Variables attribute: Column names and units

    23 Hydraulic Properties Available:
        1. Elevation
        2-4. Area (LOB, Channel, ROB)
        5-7. Area Ineffective (LOB, Channel, ROB)
        8-10. Conveyance (LOB, Channel, ROB)
        11-13. Wetted Perimeter (LOB, Channel, ROB)
        14-16. Manning's n (LOB, Channel, ROB)
        17-20. Top Width (Total, LOB, Channel, ROB)
        21. Alpha (velocity distribution coefficient)
        22. Storage Area
        23. Beta (momentum coefficient)

Example Usage:
    >>> from ras_commander import HdfHydraulicTables
    >>> from pathlib import Path
    >>>
    >>> # Get property table for specific cross section
    >>> hdf_file = Path("BaldEagle.g01.hdf")
    >>> htab = HdfHydraulicTables.get_xs_htab(hdf_file, "Bald Eagle", "Loc Hav", "1")
    >>>
    >>> # Plot area-elevation curve
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(htab['Elevation'], htab['Area_Total'])
    >>> plt.xlabel('Elevation (ft)')
    >>> plt.ylabel('Area (sq ft)')
    >>> plt.title('Cross Section Area-Elevation Curve')
    >>> plt.show()
    >>>
    >>> # Calculate flow at specific stage
    >>> target_elev = 665.0
    >>> idx = (htab['Elevation'] - target_elev).abs().idxmin()
    >>> flow_area = htab.loc[idx, 'Area_Total']
    >>> conveyance = htab.loc[idx, 'Conveyance_Total']
    >>> print(f"At elevation {target_elev} ft:")
    >>> print(f"  Flow area: {flow_area:.1f} sq ft")
    >>> print(f"  Conveyance: {conveyance:.1f} cfs")

References:
    - See HdfXsec for cross section geometry extraction
    - See RasGeometry for plain text geometry operations
    - Property tables computed during geometry preprocessing in HEC-RAS
"""

from pathlib import Path
from typing import Union, Optional, Dict, Tuple
import h5py
import pandas as pd
import numpy as np

from ..LoggingConfig import get_logger
from ..Decorators import log_call, standardize_input

logger = get_logger(__name__)


class HdfHydraulicTables:
    """
    Extract hydraulic property tables (HTAB) from HEC-RAS geometry HDF files.

    All methods are static and designed to be used without instantiation.

    Property tables provide preprocessed hydraulic properties (area, conveyance,
    wetted perimeter, etc.) as functions of elevation for cross sections and structures.
    """

    @staticmethod
    def _get_xs_index(hdf_file: h5py.File, river: str, reach: str, rs: str) -> Optional[int]:
        """
        Find cross section index from river/reach/RS identifiers.

        Parameters:
            hdf_file (h5py.File): Open HDF file handle
            river (str): River name
            reach (str): Reach name
            rs (str): River station

        Returns:
            Optional[int]: Cross section index, or None if not found

        Notes:
            - Uses /Geometry/Cross Sections/Attributes to map names to indices
            - Case-sensitive matching
        """
        try:
            attrs_path = '/Geometry/Cross Sections/Attributes'
            if attrs_path not in hdf_file:
                logger.error(f"Attributes path not found: {attrs_path}")
                return None

            attrs = hdf_file[attrs_path][:]

            # Check if required fields exist
            required_fields = ['River', 'Reach', 'RS']
            for field in required_fields:
                if field not in attrs.dtype.names:
                    logger.error(f"Required field '{field}' not found in Attributes")
                    return None

            # Search for matching cross section
            for i, attr in enumerate(attrs):
                attr_river = attr['River'].decode('utf-8').strip()
                attr_reach = attr['Reach'].decode('utf-8').strip()
                attr_rs = attr['RS'].decode('utf-8').strip()

                if attr_river == river and attr_reach == reach and attr_rs == rs:
                    logger.debug(f"Found XS at index {i}: {river}/{reach}/RS {rs}")
                    return i

            logger.warning(f"Cross section not found: {river}/{reach}/RS {rs}")
            return None

        except Exception as e:
            logger.error(f"Error finding XS index: {str(e)}")
            return None

    @staticmethod
    def _extract_property_table(hdf_file: h5py.File, xs_index: int) -> Optional[pd.DataFrame]:
        """
        Extract property table for a cross section index.

        Parameters:
            hdf_file (h5py.File): Open HDF file handle
            xs_index (int): Cross section index

        Returns:
            Optional[pd.DataFrame]: Property table with all 23 hydraulic properties

        Notes:
            - Reads from /Geometry/Cross Sections/Property Tables/
            - Returns DataFrame with elevation + 22 other properties
        """
        try:
            prop_path = '/Geometry/Cross Sections/Property Tables'
            if prop_path not in hdf_file:
                logger.error(f"Property Tables path not found: {prop_path}")
                return None

            prop_tables = hdf_file[prop_path]

            # Read index info
            if 'XSEC Info' not in prop_tables:
                logger.error("XSEC Info not found in Property Tables")
                return None

            xsec_info = prop_tables['XSEC Info'][:]

            if xs_index >= len(xsec_info):
                logger.error(f"XS index {xs_index} out of range (max: {len(xsec_info)-1})")
                return None

            # Get start index and count for this XS
            start_idx = xsec_info[xs_index][0]
            count = xsec_info[xs_index][1]

            logger.debug(f"XS {xs_index}: start={start_idx}, count={count}")

            # Read property table values
            if 'XSEC Value' not in prop_tables:
                logger.error("XSEC Value not found in Property Tables")
                return None

            xsec_value = prop_tables['XSEC Value']

            # Extract data for this XS
            data = xsec_value[start_idx:start_idx + count, :]

            # Get column names from Variables attribute
            if 'Variables' in xsec_value.attrs:
                variables = xsec_value.attrs['Variables']
                # Variables is Nx2 array: [name, units]
                col_names = [var[0].decode('utf-8').strip() for var in variables]

                # Create friendly column names
                friendly_names = []
                for name in col_names:
                    # Convert names like "Area LOB" to "Area_LOB"
                    friendly = name.replace(' ', '_')
                    # Special handling for total values
                    if friendly == 'Area_Chan' and 'Area_LOB' in friendly_names:
                        # Calculate total area
                        pass  # Will compute after DataFrame creation
                    friendly_names.append(friendly)

            else:
                # Fallback column names
                col_names = [f'Property_{i}' for i in range(data.shape[1])]
                friendly_names = col_names

            # Create DataFrame
            df = pd.DataFrame(data, columns=friendly_names)

            # Calculate total values from LOB + Chan + ROB
            if 'Area_LOB' in df.columns and 'Area_Chan' in df.columns and 'Area_ROB' in df.columns:
                df['Area_Total'] = df['Area_LOB'] + df['Area_Chan'] + df['Area_ROB']

            if 'Conv_LOB' in df.columns and 'Conv_Chan' in df.columns and 'Conv_ROB' in df.columns:
                df['Conveyance_Total'] = df['Conv_LOB'] + df['Conv_Chan'] + df['Conv_ROB']

            if 'WP_LOB' in df.columns and 'WP_Chan' in df.columns and 'WP_ROB' in df.columns:
                df['Wetted_Perimeter_Total'] = df['WP_LOB'] + df['WP_Chan'] + df['WP_ROB']

            logger.info(f"Extracted property table: {len(df)} elevations × {len(df.columns)} properties")

            return df

        except Exception as e:
            logger.error(f"Error extracting property table: {str(e)}")
            return None

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_xs_htab(hdf_path: Union[str, Path],
                    river: str,
                    reach: str,
                    rs: str) -> pd.DataFrame:
        """
        Extract hydraulic property table (HTAB) for a cross section.

        Reads preprocessed hydraulic properties from geometry HDF file, including
        area, conveyance, wetted perimeter, top width, and other properties as
        functions of elevation.

        Parameters:
            hdf_path (Union[str, Path]): Path to geometry HDF file (.g##.hdf)
            river (str): River name (case-sensitive)
            reach (str): Reach name (case-sensitive)
            rs (str): River station (as string, e.g., "1")

        Returns:
            pd.DataFrame: Property table with columns:
                - Elevation: Water surface elevation (ft or m)
                - Area_LOB: Left overbank area (sq ft or sq m)
                - Area_Chan: Channel area
                - Area_ROB: Right overbank area
                - Area_Total: Total flow area (computed)
                - Area_Ineff_LOB: Ineffective area left overbank
                - Area_Ineff_Chan: Ineffective area channel
                - Area_Ineff_ROB: Ineffective area right overbank
                - Conv_LOB: Conveyance left overbank (cfs or cms)
                - Conv_Chan: Conveyance channel
                - Conv_ROB: Conveyance right overbank
                - Conveyance_Total: Total conveyance (computed)
                - WP_LOB: Wetted perimeter left overbank (ft or m)
                - WP_Chan: Wetted perimeter channel
                - WP_ROB: Wetted perimeter right overbank
                - Wetted_Perimeter_Total: Total wetted perimeter (computed)
                - Mann_N_LOB: Manning's n left overbank
                - Mann_N_Chan: Manning's n channel
                - Mann_N_ROB: Manning's n right overbank
                - Top_Width: Total top width (ft or m)
                - Top_Width_LOB: Top width left overbank
                - Top_Width_Chan: Top width channel
                - Top_Width_ROB: Top width right overbank
                - Alpha: Velocity distribution coefficient
                - Storage_Area: Storage area (sq ft or sq m)
                - Beta: Momentum coefficient

        Raises:
            FileNotFoundError: If HDF file doesn't exist
            ValueError: If cross section not found
            IOError: If HDF read fails

        Example:
            >>> from ras_commander import HdfHydraulicTables
            >>> from pathlib import Path
            >>> import matplotlib.pyplot as plt
            >>>
            >>> # Extract property table
            >>> hdf_file = Path("BaldEagle.g01.hdf")
            >>> htab = HdfHydraulicTables.get_xs_htab(hdf_file, "Bald Eagle", "Loc Hav", "1")
            >>>
            >>> print(f"Property table: {len(htab)} elevations")
            >>> print(f"Elevation range: {htab['Elevation'].min():.2f} to {htab['Elevation'].max():.2f}")
            >>>
            >>> # Plot area-elevation curve
            >>> plt.figure(figsize=(10, 6))
            >>> plt.plot(htab['Area_Total'], htab['Elevation'], 'b-', linewidth=2)
            >>> plt.xlabel('Flow Area (sq ft)')
            >>> plt.ylabel('Elevation (ft)')
            >>> plt.title('Cross Section Area-Elevation Curve')
            >>> plt.grid(True, alpha=0.3)
            >>> plt.show()
            >>>
            >>> # Calculate hydraulic radius
            >>> htab['Hydraulic_Radius'] = htab['Area_Total'] / htab['Wetted_Perimeter_Total']
            >>> print(f"Max hydraulic radius: {htab['Hydraulic_Radius'].max():.2f} ft")

        Notes:
            - Property tables are in GEOMETRY HDF (.g##.hdf), not plan HDF
            - Tables computed during geometry preprocessing in HEC-RAS
            - Use for rating curves, stage-discharge, hydraulic analysis
            - Total values (area, conveyance, WP) computed from LOB + Chan + ROB
            - See HdfXsec.get_cross_sections() for XS geometry
        """
        hdf_path = Path(hdf_path)

        if not hdf_path.exists():
            raise FileNotFoundError(f"Geometry HDF file not found: {hdf_path}")

        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Find cross section index
                xs_index = HdfHydraulicTables._get_xs_index(hdf, river, reach, rs)

                if xs_index is None:
                    raise ValueError(
                        f"Cross section not found in HDF: {river}/{reach}/RS {rs}\n"
                        f"Check that river, reach, and RS names match exactly (case-sensitive)"
                    )

                # Extract property table
                df = HdfHydraulicTables._extract_property_table(hdf, xs_index)

                if df is None:
                    raise IOError(f"Failed to extract property table for {river}/{reach}/RS {rs}")

                logger.info(
                    f"Extracted HTAB for {river}/{reach}/RS {rs}: "
                    f"{len(df)} elevations, {len(df.columns)} properties"
                )

                return df

        except FileNotFoundError:
            raise
        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Error reading property table from HDF: {str(e)}")
            raise IOError(f"Failed to read property table: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_all_xs_htabs(hdf_path: Union[str, Path]) -> Dict[Tuple[str, str, str], pd.DataFrame]:
        """
        Extract hydraulic property tables for ALL cross sections in geometry.

        Batch extraction of property tables for all cross sections, returned as
        a dictionary keyed by (river, reach, rs) tuples.

        Parameters:
            hdf_path (Union[str, Path]): Path to geometry HDF file (.g##.hdf)

        Returns:
            Dict[Tuple[str, str, str], pd.DataFrame]: Dictionary mapping
                (river, reach, rs) tuples to property table DataFrames

        Raises:
            FileNotFoundError: If HDF file doesn't exist
            IOError: If HDF read fails

        Example:
            >>> from ras_commander import HdfHydraulicTables
            >>> from pathlib import Path
            >>>
            >>> # Extract all property tables
            >>> hdf_file = Path("BaldEagle.g01.hdf")
            >>> all_htabs = HdfHydraulicTables.get_all_xs_htabs(hdf_file)
            >>>
            >>> print(f"Extracted {len(all_htabs)} property tables")
            >>>
            >>> # Access specific cross section
            >>> htab = all_htabs[("Bald Eagle", "Loc Hav", "1")]
            >>>
            >>> # Calculate statistics across all cross sections
            >>> max_areas = {}
            >>> for (river, reach, rs), htab in all_htabs.items():
            ...     max_area = htab['Area_Total'].max()
            ...     max_areas[rs] = max_area
            >>>
            >>> # Find cross section with largest area
            >>> largest_rs = max(max_areas, key=max_areas.get)
            >>> print(f"Largest XS: RS {largest_rs} with area {max_areas[largest_rs]:.1f} sq ft")

        Notes:
            - More efficient than calling get_xs_htab() repeatedly
            - Returns all cross sections in single HDF file read
            - Dictionary keys are (river, reach, rs) tuples for easy lookup
        """
        hdf_path = Path(hdf_path)

        if not hdf_path.exists():
            raise FileNotFoundError(f"Geometry HDF file not found: {hdf_path}")

        try:
            all_htabs = {}

            with h5py.File(hdf_path, 'r') as hdf:
                # Read attributes to get all river/reach/RS combinations
                attrs_path = '/Geometry/Cross Sections/Attributes'
                if attrs_path not in hdf:
                    logger.error(f"Attributes path not found: {attrs_path}")
                    return all_htabs

                attrs = hdf[attrs_path][:]

                # Extract property table for each cross section
                for i, attr in enumerate(attrs):
                    river = attr['River'].decode('utf-8').strip()
                    reach = attr['Reach'].decode('utf-8').strip()
                    rs = attr['RS'].decode('utf-8').strip()

                    # Extract property table
                    df = HdfHydraulicTables._extract_property_table(hdf, i)

                    if df is not None:
                        all_htabs[(river, reach, rs)] = df
                    else:
                        logger.warning(f"Failed to extract HTAB for {river}/{reach}/RS {rs}")

            logger.info(f"Extracted {len(all_htabs)} property tables from {hdf_path.name}")

            return all_htabs

        except FileNotFoundError:
            raise
        except Exception as e:
            logger.error(f"Error reading property tables from HDF: {str(e)}")
            raise IOError(f"Failed to read property tables: {str(e)}")

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfInfiltration.py
==================================================
"""
Class: HdfInfiltration

A comprehensive class for handling infiltration-related operations in HEC-RAS HDF geometry files.
This class provides methods for managing infiltration parameters, soil statistics, and raster data processing.

Key Features:
- Infiltration parameter management (scaling, setting, retrieving)
- Soil statistics calculation and analysis
- Raster data processing and mapping
- Weighted parameter calculations
- Data export and file management

Methods:
1. Geometry File Base Override Management:
   - scale_infiltration_data(): Updates infiltration parameters with scaling factors in geometry file
   - get_infiltration_data(): Retrieves current infiltration parameters from geometry file
   - set_infiltration_table(): Sets infiltration parameters directly in geometry file

2. Raster and Mapping Operations (uses rasmap_df HDF files):
   - get_infiltration_map(): Reads infiltration raster map from rasmap_df HDF file
   - calculate_soil_statistics(): Processes zonal statistics for soil analysis

3. Soil Analysis (uses rasmap_df HDF files):
   - get_significant_mukeys(): Identifies mukeys above percentage threshold
   - calculate_total_significant_percentage(): Computes total coverage of significant mukeys
   - get_infiltration_parameters(): Retrieves parameters for specific mukey
   - calculate_weighted_parameters(): Computes weighted average parameters

4. Data Management (uses rasmap_df HDF files):
   - save_statistics(): Exports soil statistics to CSV

Constants:
- SQM_TO_ACRE: Conversion factor from square meters to acres (0.000247105)
- SQM_TO_SQMILE: Conversion factor from square meters to square miles (3.861e-7)

Dependencies:
- pathlib: Path handling
- pandas: Data manipulation
- geopandas: Geospatial data processing
- h5py: HDF file operations
- rasterstats: Zonal statistics calculation (optional)

Note:
- Methods in section 1 work with base overrides in geometry files
- Methods in sections 2-4 work with HDF files from rasmap_df by default
- All methods are static and decorated with @standardize_input and @log_call
- The class is designed to work with both HEC-RAS geometry files and rasmap_df HDF files
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any, List, Tuple
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)
        
from pathlib import Path
import pandas as pd
import geopandas as gpd
import h5py

from ..Decorators import log_call, standardize_input

class HdfInfiltration:
        
    """
    A class for handling infiltration-related operations on HEC-RAS HDF geometry files.

    This class provides methods to extract and modify infiltration data from HEC-RAS HDF geometry files,
    including base overrides of infiltration parameters.
    """

    # Constants for unit conversion
    SQM_TO_ACRE = 0.000247105
    SQM_TO_SQMILE = 3.861e-7
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)


    @staticmethod
    @log_call 
    def get_infiltration_baseoverrides(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Retrieve current infiltration parameters from a HEC-RAS geometry HDF file.
        Dynamically reads whatever columns are present in the table.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        Optional[pd.DataFrame]
            DataFrame containing infiltration parameters if successful, None if operation fails
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                table_path = '/Geometry/Infiltration/Base Overrides'
                if table_path not in hdf_file:
                    logger.warning(f"No infiltration data found in {hdf_path}")
                    return None

                # Get column info
                col_names, _, _ = HdfInfiltration._get_table_info(hdf_file, table_path)
                if not col_names:
                    logger.error(f"No columns found in infiltration table")
                    return None
                    
                # Read data
                data = hdf_file[table_path][()]
                
                # Convert to DataFrame
                df_dict = {}
                for col in col_names:
                    values = data[col]
                    # Convert byte strings to regular strings if needed
                    if values.dtype.kind == 'S':
                        values = [v.decode('utf-8').strip() for v in values]
                    df_dict[col] = values
                
                return pd.DataFrame(df_dict)

        except Exception as e:
            logger.error(f"Error reading infiltration data from {hdf_path}: {str(e)}")
            return None
        


    # set_infiltration_baseoverrides goes here, once finalized tested and fixed. 



    # Since the infiltration base overrides are in the geometry file, the above functions work on the geometry files
    # The below functions work on the infiltration layer HDF files.  Changes only take effect if no base overrides are present. 
           
    @staticmethod
    @log_call 
    def get_infiltration_layer_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Retrieve current infiltration parameters from a HEC-RAS infiltration layer HDF file.
        Extracts the Variables dataset which contains the layer data.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS infiltration layer HDF file

        Returns
        -------
        Optional[pd.DataFrame]
            DataFrame containing infiltration parameters if successful, None if operation fails
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                variables_path = '//Variables'
                if variables_path not in hdf_file:
                    logger.warning(f"No Variables dataset found in {hdf_path}")
                    return None
                
                # Read data from Variables dataset
                data = hdf_file[variables_path][()]
                
                # Convert to DataFrame
                df_dict = {}
                for field_name in data.dtype.names:
                    values = data[field_name]
                    # Convert byte strings to regular strings if needed
                    if values.dtype.kind == 'S':
                        values = [v.decode('utf-8').strip() for v in values]
                    df_dict[field_name] = values
                
                return pd.DataFrame(df_dict)

        except Exception as e:
            logger.error(f"Error reading infiltration layer data from {hdf_path}: {str(e)}")
            return None
        

    @staticmethod
    @log_call
    def set_infiltration_layer_data(
        hdf_path: Path,
        infiltration_df: pd.DataFrame
    ) -> Optional[pd.DataFrame]:
        """
        Set infiltration layer data in the infiltration layer HDF file directly from the provided DataFrame.
        # NOTE: This will not work if there are base overrides present in the Geometry HDF file. 
        Updates the Variables dataset with the provided data.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS infiltration layer HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters with columns:
            - Name (string)
            - Curve Number (float)
            - Abstraction Ratio (float)
            - Minimum Infiltration Rate (float)

        Returns
        -------
        Optional[pd.DataFrame]
            The infiltration DataFrame if successful, None if operation fails
        """
        try:
            variables_path = '//Variables'
            
            # Validate required columns
            required_columns = ['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate']
            missing_columns = [col for col in required_columns if col not in infiltration_df.columns]
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            with h5py.File(hdf_path, 'a') as hdf_file:
                # Delete existing dataset if it exists
                if variables_path in hdf_file:
                    del hdf_file[variables_path]

                # Create dtype for structured array
                dt = np.dtype([
                    ('Name', f'S{infiltration_df["Name"].str.len().max()}'),
                    ('Curve Number', 'f4'),
                    ('Abstraction Ratio', 'f4'),
                    ('Minimum Infiltration Rate', 'f4')
                ])

                # Create structured array
                structured_array = np.zeros(infiltration_df.shape[0], dtype=dt)
                
                # Fill structured array
                structured_array['Name'] = infiltration_df['Name'].values.astype(f'|S{dt["Name"].itemsize}')
                structured_array['Curve Number'] = infiltration_df['Curve Number'].values
                structured_array['Abstraction Ratio'] = infiltration_df['Abstraction Ratio'].values
                structured_array['Minimum Infiltration Rate'] = infiltration_df['Minimum Infiltration Rate'].values

                # Create new dataset
                hdf_file.create_dataset(
                    variables_path,
                    data=structured_array,
                    dtype=dt,
                    compression='gzip',
                    compression_opts=1,
                    chunks=(100,),
                    maxshape=(None,)
                )

            return infiltration_df

        except Exception as e:
            logger.error(f"Error setting infiltration layer data in {hdf_path}: {str(e)}")
            return None
        



    @staticmethod
    @standardize_input(file_type='geom_hdf')
    @log_call
    def scale_infiltration_data(
        hdf_path: Path,
        infiltration_df: pd.DataFrame,
        scale_factors: Dict[str, float]
    ) -> Optional[pd.DataFrame]:
        """
        Update infiltration parameters in the HDF file with scaling factors.
        Supports any numeric columns present in the DataFrame.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters
        scale_factors : Dict[str, float]
            Dictionary mapping column names to their scaling factors

        Returns
        -------
        Optional[pd.DataFrame]
            The updated infiltration DataFrame if successful, None if operation fails
        """
        try:
            # Make a copy to avoid modifying the input DataFrame
            infiltration_df = infiltration_df.copy()
            
            # Apply scaling factors to specified columns
            for col, factor in scale_factors.items():
                if col in infiltration_df.columns and pd.api.types.is_numeric_dtype(infiltration_df[col]):
                    infiltration_df[col] *= factor
                else:
                    logger.warning(f"Column {col} not found or not numeric - skipping scaling")

            # Use set_infiltration_table to write the scaled data
            return HdfInfiltration.set_infiltration_table(hdf_path, infiltration_df)

        except Exception as e:
            logger.error(f"Error scaling infiltration data in {hdf_path}: {str(e)}")
            return None



    # Need to reorganize these soil staatistics functions so they are more straightforward.  


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_soils_raster_stats(
        geom_hdf_path: Path,
        soil_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate soil group statistics for each 2D flow area using the area's perimeter.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        soil_hdf_path : Path, optional
            Path to the soil HDF file. If None, uses soil_layer_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with soil statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - mukey: Soil mukey identifier
            - percentage: Percentage of 2D flow area covered by this soil type
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get the soil HDF path
        if soil_hdf_path is None:
            if ras_object is None:
                from ..RasPrj import ras
                ras_object = ras
            
            # Try to get soil_layer_path from rasmap_df
            try:
                soil_hdf_path = Path(ras_object.rasmap_df.loc[0, 'soil_layer_path'][0])
                if not soil_hdf_path.exists():
                    logger.warning(f"Soil HDF path from rasmap_df does not exist: {soil_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving soil_layer_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get infiltration map - pass as hdf_path to ensure standardize_input works correctly
        try:
            raster_map = HdfInfiltration.get_infiltration_map(hdf_path=soil_hdf_path, ras_object=ras_object)
            if not raster_map:
                logger.error(f"No infiltration map found in {soil_hdf_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error getting infiltration map: {str(e)}")
            return pd.DataFrame()
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Extract the raster data for analysis
        tif_path = soil_hdf_path.with_suffix('.tif')
        if not tif_path.exists():
            logger.error(f"No raster file found at {tif_path}")
            return pd.DataFrame()
            
        # Read the raster data and info
        import rasterio
        with rasterio.open(tif_path) as src:
            grid_data = src.read(1)
            
            # Get transform directly from rasterio
            transform = src.transform
            no_data = src.nodata if src.nodata is not None else -9999
            
            # List to store all results
            all_results = []
            
            # Calculate zonal statistics for each 2D flow area
            for _, mesh_row in mesh_areas.iterrows():
                mesh_name = mesh_row['mesh_name']
                mesh_geom = mesh_row['geometry']
                
                # Get zonal statistics directly using numpy array
                try:
                    stats = zonal_stats(
                        mesh_geom,
                        grid_data,
                        affine=transform,
                        categorical=True,
                        nodata=no_data
                    )[0]
                    
                    # Skip if no stats
                    if not stats:
                        logger.warning(f"No soil data found for 2D flow area: {mesh_name}")
                        continue
                    
                    # Calculate total area and percentages
                    total_area_sqm = sum(stats.values())
                    
                    # Process each mukey
                    for raster_val, area_sqm in stats.items():
                        # Skip NoData values
                        if raster_val is None or raster_val == no_data:
                            continue
                            
                        try:
                            mukey = raster_map.get(int(raster_val), f"Unknown-{raster_val}")
                        except (ValueError, TypeError):
                            mukey = f"Unknown-{raster_val}"
                            
                        percentage = (area_sqm / total_area_sqm) * 100 if total_area_sqm > 0 else 0
                        
                        all_results.append({
                            'mesh_name': mesh_name,
                            'mukey': mukey,
                            'percentage': percentage,
                            'area_sqm': area_sqm,
                            'area_acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                            'area_sqmiles': area_sqm * HdfInfiltration.SQM_TO_SQMILE
                        })
                except Exception as e:
                    logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                    continue
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name and percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df






    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_soil_raster_stats(
        geom_hdf_path: Path,
        landcover_hdf_path: Path = None,
        soil_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate combined land cover and soil infiltration statistics for each 2D flow area.
        
        This function processes both land cover and soil data to calculate statistics
        for each combination (Land Cover : Soil Type) within each 2D flow area.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        landcover_hdf_path : Path, optional
            Path to the land cover HDF file. If None, uses landcover_hdf_path from rasmap_df
        soil_hdf_path : Path, optional
            Path to the soil HDF file. If None, uses soil_layer_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with combined statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - combined_type: Combined land cover and soil type (e.g. "Mixed Forest : B")
            - percentage: Percentage of 2D flow area covered by this combination
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
            - curve_number: Curve number for this combination
            - abstraction_ratio: Abstraction ratio for this combination
            - min_infiltration_rate: Minimum infiltration rate for this combination
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
            import rasterio
            from rasterio.merge import merge
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas rasterio'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get RAS object
        if ras_object is None:
            from ..RasPrj import ras
            ras_object = ras
        
        # Get the landcover HDF path
        if landcover_hdf_path is None:
            try:
                landcover_hdf_path = Path(ras_object.rasmap_df.loc[0, 'landcover_hdf_path'][0])
                if not landcover_hdf_path.exists():
                    logger.warning(f"Land cover HDF path from rasmap_df does not exist: {landcover_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving landcover_hdf_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get the soil HDF path
        if soil_hdf_path is None:
            try:
                soil_hdf_path = Path(ras_object.rasmap_df.loc[0, 'soil_layer_path'][0])
                if not soil_hdf_path.exists():
                    logger.warning(f"Soil HDF path from rasmap_df does not exist: {soil_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving soil_layer_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get land cover map (raster to ID mapping)
        try:
            with h5py.File(landcover_hdf_path, 'r') as hdf:
                if '//Raster Map' not in hdf:
                    logger.error(f"No Raster Map found in {landcover_hdf_path}")
                    return pd.DataFrame()
                
                landcover_map_data = hdf['//Raster Map'][()]
                landcover_map = {int(item[0]): item[1].decode('utf-8').strip() for item in landcover_map_data}
        except Exception as e:
            logger.error(f"Error reading land cover data from HDF: {str(e)}")
            return pd.DataFrame()
        
        # Get soil map (raster to ID mapping)
        try:
            soil_map = HdfInfiltration.get_infiltration_map(hdf_path=soil_hdf_path, ras_object=ras_object)
            if not soil_map:
                logger.error(f"No soil map found in {soil_hdf_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error getting soil map: {str(e)}")
            return pd.DataFrame()
        
        # Get infiltration parameters
        try:
            infiltration_params = HdfInfiltration.get_infiltration_layer_data(soil_hdf_path)
            if infiltration_params is None or infiltration_params.empty:
                logger.warning(f"No infiltration parameters found in {soil_hdf_path}")
                infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        except Exception as e:
            logger.error(f"Error getting infiltration parameters: {str(e)}")
            infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Check for the TIF files with same name as HDF
        landcover_tif_path = landcover_hdf_path.with_suffix('.tif')
        soil_tif_path = soil_hdf_path.with_suffix('.tif')
        
        if not landcover_tif_path.exists():
            logger.error(f"No land cover raster file found at {landcover_tif_path}")
            return pd.DataFrame()
        
        if not soil_tif_path.exists():
            logger.error(f"No soil raster file found at {soil_tif_path}")
            return pd.DataFrame()
        
        # List to store all results
        all_results = []
        
        # Read the raster data
        try:
            with rasterio.open(landcover_tif_path) as landcover_src, rasterio.open(soil_tif_path) as soil_src:
                landcover_nodata = landcover_src.nodata if landcover_src.nodata is not None else -9999
                soil_nodata = soil_src.nodata if soil_src.nodata is not None else -9999
                
                # Calculate zonal statistics for each 2D flow area
                for _, mesh_row in mesh_areas.iterrows():
                    mesh_name = mesh_row['mesh_name']
                    mesh_geom = mesh_row['geometry']
                    
                    # Get zonal statistics for land cover
                    try:
                        landcover_stats = zonal_stats(
                            mesh_geom,
                            landcover_tif_path,
                            categorical=True,
                            nodata=landcover_nodata
                        )[0]
                        
                        # Get zonal statistics for soil
                        soil_stats = zonal_stats(
                            mesh_geom,
                            soil_tif_path,
                            categorical=True,
                            nodata=soil_nodata
                        )[0]
                        
                        # Skip if no stats
                        if not landcover_stats or not soil_stats:
                            logger.warning(f"No land cover or soil data found for 2D flow area: {mesh_name}")
                            continue
                        
                        # Calculate total area
                        landcover_total = sum(landcover_stats.values())
                        soil_total = sum(soil_stats.values())
                        
                        # Create a cross-tabulation of land cover and soil types
                        # This is an approximation since we don't have the exact pixel-by-pixel overlap
                        mesh_area_sqm = mesh_row['geometry'].area
                        
                        # Calculate percentage of each land cover type
                        landcover_pct = {k: v/landcover_total for k, v in landcover_stats.items() if k is not None and k != landcover_nodata}
                        
                        # Calculate percentage of each soil type
                        soil_pct = {k: v/soil_total for k, v in soil_stats.items() if k is not None and k != soil_nodata}
                        
                        # Generate combinations
                        for lc_id, lc_pct in landcover_pct.items():
                            lc_name = landcover_map.get(int(lc_id), f"Unknown-{lc_id}")
                            
                            for soil_id, soil_pct in soil_pct.items():
                                try:
                                    soil_name = soil_map.get(int(soil_id), f"Unknown-{soil_id}")
                                except (ValueError, TypeError):
                                    soil_name = f"Unknown-{soil_id}"
                                
                                # Calculate combined percentage (approximate)
                                # This is a simplification; actual overlap would require pixel-by-pixel analysis
                                combined_pct = lc_pct * soil_pct * 100
                                combined_area_sqm = mesh_area_sqm * (combined_pct / 100)
                                
                                # Create combined name
                                combined_name = f"{lc_name} : {soil_name}"
                                
                                # Look up infiltration parameters
                                param_row = infiltration_params[infiltration_params['Name'] == combined_name]
                                if param_row.empty:
                                    # Try with NoData for soil type
                                    param_row = infiltration_params[infiltration_params['Name'] == f"{lc_name} : NoData"]
                                
                                if not param_row.empty:
                                    curve_number = param_row.iloc[0]['Curve Number']
                                    abstraction_ratio = param_row.iloc[0]['Abstraction Ratio']
                                    min_infiltration_rate = param_row.iloc[0]['Minimum Infiltration Rate']
                                else:
                                    curve_number = None
                                    abstraction_ratio = None
                                    min_infiltration_rate = None
                                
                                all_results.append({
                                    'mesh_name': mesh_name,
                                    'combined_type': combined_name,
                                    'percentage': combined_pct,
                                    'area_sqm': combined_area_sqm,
                                    'area_acres': combined_area_sqm * HdfInfiltration.SQM_TO_ACRE,
                                    'area_sqmiles': combined_area_sqm * HdfInfiltration.SQM_TO_SQMILE,
                                    'curve_number': curve_number,
                                    'abstraction_ratio': abstraction_ratio,
                                    'min_infiltration_rate': min_infiltration_rate
                                })
                    except Exception as e:
                        logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error opening raster files: {str(e)}")
            return pd.DataFrame()
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name, percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df






    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_infiltration_stats(
        geom_hdf_path: Path,
        landcover_hdf_path: Path = None,
        soil_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate combined land cover and soil infiltration statistics for each 2D flow area.
        
        This function processes both land cover and soil data to calculate statistics
        for each combination (Land Cover : Soil Type) within each 2D flow area.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        landcover_hdf_path : Path, optional
            Path to the land cover HDF file. If None, uses landcover_hdf_path from rasmap_df
        soil_hdf_path : Path, optional
            Path to the soil HDF file. If None, uses soil_layer_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with combined statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - combined_type: Combined land cover and soil type (e.g. "Mixed Forest : B")
            - percentage: Percentage of 2D flow area covered by this combination
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
            - curve_number: Curve number for this combination
            - abstraction_ratio: Abstraction ratio for this combination
            - min_infiltration_rate: Minimum infiltration rate for this combination
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
            import rasterio
            from rasterio.merge import merge
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas rasterio'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get RAS object
        if ras_object is None:
            from ..RasPrj import ras
            ras_object = ras
        
        # Get the landcover HDF path
        if landcover_hdf_path is None:
            try:
                landcover_hdf_path = Path(ras_object.rasmap_df.loc[0, 'landcover_hdf_path'][0])
                if not landcover_hdf_path.exists():
                    logger.warning(f"Land cover HDF path from rasmap_df does not exist: {landcover_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving landcover_hdf_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get the soil HDF path
        if soil_hdf_path is None:
            try:
                soil_hdf_path = Path(ras_object.rasmap_df.loc[0, 'soil_layer_path'][0])
                if not soil_hdf_path.exists():
                    logger.warning(f"Soil HDF path from rasmap_df does not exist: {soil_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving soil_layer_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get land cover map (raster to ID mapping)
        try:
            with h5py.File(landcover_hdf_path, 'r') as hdf:
                if '//Raster Map' not in hdf:
                    logger.error(f"No Raster Map found in {landcover_hdf_path}")
                    return pd.DataFrame()
                
                landcover_map_data = hdf['//Raster Map'][()]
                landcover_map = {int(item[0]): item[1].decode('utf-8').strip() for item in landcover_map_data}
        except Exception as e:
            logger.error(f"Error reading land cover data from HDF: {str(e)}")
            return pd.DataFrame()
        
        # Get soil map (raster to ID mapping)
        try:
            soil_map = HdfInfiltration.get_infiltration_map(hdf_path=soil_hdf_path, ras_object=ras_object)
            if not soil_map:
                logger.error(f"No soil map found in {soil_hdf_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"Error getting soil map: {str(e)}")
            return pd.DataFrame()
        
        # Get infiltration parameters
        try:
            infiltration_params = HdfInfiltration.get_infiltration_layer_data(soil_hdf_path)
            if infiltration_params is None or infiltration_params.empty:
                logger.warning(f"No infiltration parameters found in {soil_hdf_path}")
                infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        except Exception as e:
            logger.error(f"Error getting infiltration parameters: {str(e)}")
            infiltration_params = pd.DataFrame(columns=['Name', 'Curve Number', 'Abstraction Ratio', 'Minimum Infiltration Rate'])
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Check for the TIF files with same name as HDF
        landcover_tif_path = landcover_hdf_path.with_suffix('.tif')
        soil_tif_path = soil_hdf_path.with_suffix('.tif')
        
        if not landcover_tif_path.exists():
            logger.error(f"No land cover raster file found at {landcover_tif_path}")
            return pd.DataFrame()
        
        if not soil_tif_path.exists():
            logger.error(f"No soil raster file found at {soil_tif_path}")
            return pd.DataFrame()
        
        # List to store all results
        all_results = []
        
        # Read the raster data
        try:
            with rasterio.open(landcover_tif_path) as landcover_src, rasterio.open(soil_tif_path) as soil_src:
                landcover_nodata = landcover_src.nodata if landcover_src.nodata is not None else -9999
                soil_nodata = soil_src.nodata if soil_src.nodata is not None else -9999
                
                # Calculate zonal statistics for each 2D flow area
                for _, mesh_row in mesh_areas.iterrows():
                    mesh_name = mesh_row['mesh_name']
                    mesh_geom = mesh_row['geometry']
                    
                    # Get zonal statistics for land cover
                    try:
                        landcover_stats = zonal_stats(
                            mesh_geom,
                            landcover_tif_path,
                            categorical=True,
                            nodata=landcover_nodata
                        )[0]
                        
                        # Get zonal statistics for soil
                        soil_stats = zonal_stats(
                            mesh_geom,
                            soil_tif_path,
                            categorical=True,
                            nodata=soil_nodata
                        )[0]
                        
                        # Skip if no stats
                        if not landcover_stats or not soil_stats:
                            logger.warning(f"No land cover or soil data found for 2D flow area: {mesh_name}")
                            continue
                        
                        # Calculate total area
                        landcover_total = sum(landcover_stats.values())
                        soil_total = sum(soil_stats.values())
                        
                        # Create a cross-tabulation of land cover and soil types
                        # This is an approximation since we don't have the exact pixel-by-pixel overlap
                        mesh_area_sqm = mesh_row['geometry'].area
                        
                        # Calculate percentage of each land cover type
                        landcover_pct = {k: v/landcover_total for k, v in landcover_stats.items() if k is not None and k != landcover_nodata}
                        
                        # Calculate percentage of each soil type
                        soil_pct = {k: v/soil_total for k, v in soil_stats.items() if k is not None and k != soil_nodata}
                        
                        # Generate combinations
                        for lc_id, lc_pct in landcover_pct.items():
                            lc_name = landcover_map.get(int(lc_id), f"Unknown-{lc_id}")
                            
                            for soil_id, soil_pct in soil_pct.items():
                                try:
                                    soil_name = soil_map.get(int(soil_id), f"Unknown-{soil_id}")
                                except (ValueError, TypeError):
                                    soil_name = f"Unknown-{soil_id}"
                                
                                # Calculate combined percentage (approximate)
                                # This is a simplification; actual overlap would require pixel-by-pixel analysis
                                combined_pct = lc_pct * soil_pct * 100
                                combined_area_sqm = mesh_area_sqm * (combined_pct / 100)
                                
                                # Create combined name
                                combined_name = f"{lc_name} : {soil_name}"
                                
                                # Look up infiltration parameters
                                param_row = infiltration_params[infiltration_params['Name'] == combined_name]
                                if param_row.empty:
                                    # Try with NoData for soil type
                                    param_row = infiltration_params[infiltration_params['Name'] == f"{lc_name} : NoData"]
                                
                                if not param_row.empty:
                                    curve_number = param_row.iloc[0]['Curve Number']
                                    abstraction_ratio = param_row.iloc[0]['Abstraction Ratio']
                                    min_infiltration_rate = param_row.iloc[0]['Minimum Infiltration Rate']
                                else:
                                    curve_number = None
                                    abstraction_ratio = None
                                    min_infiltration_rate = None
                                
                                all_results.append({
                                    'mesh_name': mesh_name,
                                    'combined_type': combined_name,
                                    'percentage': combined_pct,
                                    'area_sqm': combined_area_sqm,
                                    'area_acres': combined_area_sqm * HdfInfiltration.SQM_TO_ACRE,
                                    'area_sqmiles': combined_area_sqm * HdfInfiltration.SQM_TO_SQMILE,
                                    'curve_number': curve_number,
                                    'abstraction_ratio': abstraction_ratio,
                                    'min_infiltration_rate': min_infiltration_rate
                                })
                    except Exception as e:
                        logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error opening raster files: {str(e)}")
            return pd.DataFrame()
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name, percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df



















    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_infiltration_map(hdf_path: Path = None, ras_object: Any = None) -> dict:
        """Read the infiltration raster map from HDF file
        
        Args:
            hdf_path: Optional path to the HDF file. If not provided, uses first infiltration_hdf_path from rasmap_df
            ras_object: Optional RAS object. If not provided, uses global ras instance
            
        Returns:
            Dictionary mapping raster values to mukeys
        """
        if hdf_path is None:
            if ras_object is None:
                from ..RasPrj import ras
                ras_object = ras
            hdf_path = Path(ras_object.rasmap_df.iloc[0]['infiltration_hdf_path'][0])
            
        with h5py.File(hdf_path, 'r') as hdf:
            raster_map_data = hdf['Raster Map'][:]
            return {int(item[0]): item[1].decode('utf-8') for item in raster_map_data}

    @staticmethod
    @log_call
    def calculate_soil_statistics(zonal_stats: list, raster_map: dict) -> pd.DataFrame:
        """Calculate soil statistics from zonal statistics
        
        Args:
            zonal_stats: List of zonal statistics
            raster_map: Dictionary mapping raster values to mukeys
            
        Returns:
            DataFrame with soil statistics including percentages and areas
        """
        
        try:
            from rasterstats import zonal_stats
        except ImportError as e:
            logger.error("Failed to import rasterstats. Please run 'pip install rasterstats' and try again.")
            raise e
        # Initialize areas dictionary
        mukey_areas = {mukey: 0 for mukey in raster_map.values()}
        
        # Calculate total area and mukey areas
        total_area_sqm = 0
        for stat in zonal_stats:
            for raster_val, area in stat.items():
                mukey = raster_map.get(raster_val)
                if mukey:
                    mukey_areas[mukey] += area
                total_area_sqm += area

        # Create DataFrame rows
        rows = []
        for mukey, area_sqm in mukey_areas.items():
            if area_sqm > 0:
                rows.append({
                    'mukey': mukey,
                    'Percentage': (area_sqm / total_area_sqm) * 100,
                    'Area in Acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                    'Area in Square Miles': area_sqm * HdfInfiltration.SQM_TO_SQMILE
                })
        
        return pd.DataFrame(rows)

    @staticmethod
    @log_call
    def get_significant_mukeys(soil_stats: pd.DataFrame, 
                             threshold: float = 1.0) -> pd.DataFrame:
        """Get mukeys with percentage greater than threshold
        
        Args:
            soil_stats: DataFrame with soil statistics
            threshold: Minimum percentage threshold (default 1.0)
            
        Returns:
            DataFrame with significant mukeys and their statistics
        """
        significant = soil_stats[soil_stats['Percentage'] > threshold].copy()
        significant.sort_values('Percentage', ascending=False, inplace=True)
        return significant

    @staticmethod
    @log_call
    def calculate_total_significant_percentage(significant_mukeys: pd.DataFrame) -> float:
        """Calculate total percentage covered by significant mukeys
        
        Args:
            significant_mukeys: DataFrame of significant mukeys
            
        Returns:
            Total percentage covered by significant mukeys
        """
        return significant_mukeys['Percentage'].sum()

    @staticmethod
    @log_call
    def save_statistics(soil_stats: pd.DataFrame, output_path: Path, 
                       include_timestamp: bool = True):
        """Save soil statistics to CSV
        
        Args:
            soil_stats: DataFrame with soil statistics
            output_path: Path to save CSV file
            include_timestamp: Whether to include timestamp in filename
        """
        if include_timestamp:
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            output_path = output_path.with_name(
                f"{output_path.stem}_{timestamp}{output_path.suffix}")
        
        soil_stats.to_csv(output_path, index=False)

    @staticmethod
    @log_call
    @standardize_input
    def get_infiltration_parameters(hdf_path: Path = None, mukey: str = None, ras_object: Any = None) -> dict:
        """Get infiltration parameters for a specific mukey from HDF file
        
        Args:
            hdf_path: Optional path to the HDF file. If not provided, uses first infiltration_hdf_path from rasmap_df
            mukey: Mukey identifier
            ras_object: Optional RAS object. If not provided, uses global ras instance
            
        Returns:
            Dictionary of infiltration parameters
        """
        if hdf_path is None:
            if ras_object is None:
                from ..RasPrj import ras
                ras_object = ras
            hdf_path = Path(ras_object.rasmap_df.iloc[0]['infiltration_hdf_path'][0])
            
        with h5py.File(hdf_path, 'r') as hdf:
            if 'Infiltration Parameters' not in hdf:
                raise KeyError("No infiltration parameters found in HDF file")
                
            params = hdf['Infiltration Parameters'][:]
            for row in params:
                if row[0].decode('utf-8') == mukey:
                    return {
                        'Initial Loss (in)': float(row[1]),
                        'Constant Loss Rate (in/hr)': float(row[2]),
                        'Impervious Area (%)': float(row[3])
                    }
        return None

    @staticmethod
    @log_call
    def calculate_weighted_parameters(soil_stats: pd.DataFrame, 
                                   infiltration_params: dict) -> dict:
        """Calculate weighted infiltration parameters based on soil statistics
        
        Args:
            soil_stats: DataFrame with soil statistics
            infiltration_params: Dictionary of infiltration parameters by mukey
            
        Returns:
            Dictionary of weighted average infiltration parameters
        """
        total_weight = soil_stats['Percentage'].sum()
        
        weighted_params = {
            'Initial Loss (in)': 0.0,
            'Constant Loss Rate (in/hr)': 0.0,
            'Impervious Area (%)': 0.0
        }
        
        for _, row in soil_stats.iterrows():
            mukey = row['mukey']
            weight = row['Percentage'] / total_weight
            
            if mukey in infiltration_params:
                for param in weighted_params:
                    weighted_params[param] += (
                        infiltration_params[mukey][param] * weight
                    )
        
        return weighted_params
    

    @staticmethod
    def _get_table_info(hdf_file: h5py.File, table_path: str) -> Tuple[List[str], List[str], List[str]]:
        """Get column names and types from HDF table
        
        Args:
            hdf_file: Open HDF file object
            table_path: Path to table in HDF file
            
        Returns:
            Tuple of (column names, numpy dtypes, column descriptions)
        """
        if table_path not in hdf_file:
            return [], [], []
            
        dataset = hdf_file[table_path]
        dtype = dataset.dtype
        
        # Extract column names and types
        col_names = []
        col_types = []
        col_descs = []
        
        for name in dtype.names:
            col_names.append(name)
            col_types.append(dtype[name].str)
            col_descs.append(name)  # Could be enhanced to get actual descriptions
            
        return col_names, col_types, col_descs


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_landcover_raster_stats(
        geom_hdf_path: Path,
        landcover_hdf_path: Path = None,
        ras_object: Any = None
    ) -> pd.DataFrame:
        """
        Calculate land cover statistics for each 2D flow area using the area's perimeter.
        
        Parameters
        ----------
        geom_hdf_path : Path
            Path to the HEC-RAS geometry HDF file containing the 2D flow areas
        landcover_hdf_path : Path, optional
            Path to the land cover HDF file. If None, uses landcover_hdf_path from rasmap_df
        ras_object : Any, optional
            Optional RAS object. If not provided, uses global ras instance
            
        Returns
        -------
        pd.DataFrame
            DataFrame with land cover statistics for each 2D flow area, including:
            - mesh_name: Name of the 2D flow area
            - land_cover: Land cover classification name
            - percentage: Percentage of 2D flow area covered by this land cover type
            - area_sqm: Area in square meters
            - area_acres: Area in acres
            - area_sqmiles: Area in square miles
            - mannings_n: Manning's n value for this land cover type
            - percent_impervious: Percent impervious for this land cover type
        
        Notes
        -----
        Requires the rasterstats package to be installed.
        """
        try:
            from rasterstats import zonal_stats
            import shapely
            import geopandas as gpd
            import numpy as np
            import tempfile
            import os
            import rasterio
        except ImportError as e:
            logger.error(f"Failed to import required package: {e}. Please run 'pip install rasterstats shapely geopandas rasterio'")
            raise e
        
        # Import here to avoid circular imports
        from .HdfMesh import HdfMesh
        
        # Get the landcover HDF path
        if landcover_hdf_path is None:
            if ras_object is None:
                from ..RasPrj import ras
                ras_object = ras
            
            # Try to get landcover_hdf_path from rasmap_df
            try:
                landcover_hdf_path = Path(ras_object.rasmap_df.loc[0, 'landcover_hdf_path'][0])
                if not landcover_hdf_path.exists():
                    logger.warning(f"Land cover HDF path from rasmap_df does not exist: {landcover_hdf_path}")
                    return pd.DataFrame()
            except (KeyError, IndexError, AttributeError, TypeError) as e:
                logger.error(f"Error retrieving landcover_hdf_path from rasmap_df: {str(e)}")
                return pd.DataFrame()
        
        # Get land cover map (raster to ID mapping)
        try:
            with h5py.File(landcover_hdf_path, 'r') as hdf:
                if '//Raster Map' not in hdf:
                    logger.error(f"No Raster Map found in {landcover_hdf_path}")
                    return pd.DataFrame()
                
                raster_map_data = hdf['//Raster Map'][()]
                raster_map = {int(item[0]): item[1].decode('utf-8').strip() for item in raster_map_data}
                
                # Get land cover variables (mannings_n and percent_impervious)
                variables = {}
                if '//Variables' in hdf:
                    var_data = hdf['//Variables'][()]
                    for row in var_data:
                        name = row[0].decode('utf-8').strip()
                        mannings_n = float(row[1])
                        percent_impervious = float(row[2])
                        variables[name] = {
                            'mannings_n': mannings_n,
                            'percent_impervious': percent_impervious
                        }
        except Exception as e:
            logger.error(f"Error reading land cover data from HDF: {str(e)}")
            return pd.DataFrame()
        
        # Get 2D flow areas
        mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)
        if mesh_areas.empty:
            logger.warning(f"No 2D flow areas found in {geom_hdf_path}")
            return pd.DataFrame()
        
        # Check for the TIF file with same name as HDF
        tif_path = landcover_hdf_path.with_suffix('.tif')
        if not tif_path.exists():
            logger.error(f"No raster file found at {tif_path}")
            return pd.DataFrame()
        
        # List to store all results
        all_results = []
        
        # Read the raster data and info
        try:
            with rasterio.open(tif_path) as src:
                # Get transform directly from rasterio
                transform = src.transform
                no_data = src.nodata if src.nodata is not None else -9999
                
                # Calculate zonal statistics for each 2D flow area
                for _, mesh_row in mesh_areas.iterrows():
                    mesh_name = mesh_row['mesh_name']
                    mesh_geom = mesh_row['geometry']
                    
                    # Get zonal statistics directly using rasterio grid
                    try:
                        stats = zonal_stats(
                            mesh_geom,
                            tif_path,
                            categorical=True,
                            nodata=no_data
                        )[0]
                        
                        # Skip if no stats
                        if not stats:
                            logger.warning(f"No land cover data found for 2D flow area: {mesh_name}")
                            continue
                        
                        # Calculate total area and percentages
                        total_area_sqm = sum(stats.values())
                        
                        # Process each land cover type
                        for raster_val, area_sqm in stats.items():
                            # Skip NoData values
                            if raster_val is None or raster_val == no_data:
                                continue
                                
                            try:
                                # Get land cover name from raster map
                                land_cover = raster_map.get(int(raster_val), f"Unknown-{raster_val}")
                                
                                # Get Manning's n and percent impervious
                                mannings_n = variables.get(land_cover, {}).get('mannings_n', None)
                                percent_impervious = variables.get(land_cover, {}).get('percent_impervious', None)
                                
                                percentage = (area_sqm / total_area_sqm) * 100 if total_area_sqm > 0 else 0
                                
                                all_results.append({
                                    'mesh_name': mesh_name,
                                    'land_cover': land_cover,
                                    'percentage': percentage,
                                    'area_sqm': area_sqm,
                                    'area_acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                                    'area_sqmiles': area_sqm * HdfInfiltration.SQM_TO_SQMILE,
                                    'mannings_n': mannings_n,
                                    'percent_impervious': percent_impervious
                                })
                            except Exception as e:
                                logger.warning(f"Error processing raster value {raster_val}: {e}")
                                continue
                    except Exception as e:
                        logger.error(f"Error calculating statistics for mesh {mesh_name}: {str(e)}")
                        continue
        except Exception as e:
            logger.error(f"Error opening raster file {tif_path}: {str(e)}")
            return pd.DataFrame()
        
        # Create DataFrame with results
        results_df = pd.DataFrame(all_results)
        
        # Sort by mesh_name, percentage (descending)
        if not results_df.empty:
            results_df = results_df.sort_values(['mesh_name', 'percentage'], ascending=[True, False])
        
        return results_df



'''

THIS FUNCTION IS VERY CLOSE BUT DOES NOT WORK BECAUSE IT DOES NOT PRESERVE THE EXACT STRUCTURE OF THE HDF FILE.
WHEN RAS LOADS THE HDF, IT IGNORES THE DATA IN THE TABLE AND REPLACES IT WITH NULLS.


    @staticmethod
    @log_call
    def set_infiltration_baseoverrides(
        hdf_path: Path,
        infiltration_df: pd.DataFrame
    ) -> Optional[pd.DataFrame]:
        """
        Set base overrides for infiltration parameters in the HDF file while preserving
        the exact structure of the existing dataset.
        
        This function ensures that the HDF structure is maintained exactly as in the
        original file, including field names, data types, and string lengths. It updates
        the values while preserving all dataset attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters with columns matching HDF structure.
            The first column should be 'Name' or 'Land Cover Name'.

        Returns
        -------
        Optional[pd.DataFrame]
            The infiltration DataFrame if successful, None if operation fails
        """
        try:
            # Make a copy to avoid modifying the input DataFrame
            infiltration_df = infiltration_df.copy()
            
            # Check for and rename the first column if needed
            if "Land Cover Name" in infiltration_df.columns:
                name_col = "Land Cover Name"
            else:
                name_col = "Name"
                # Rename 'Name' to 'Land Cover Name' for HDF dataset
                infiltration_df = infiltration_df.rename(columns={"Name": "Land Cover Name"})
                
            table_path = '/Geometry/Infiltration/Base Overrides'
            
            with h5py.File(hdf_path, 'r') as hdf_file_read:
                # Check if dataset exists
                if table_path not in hdf_file_read:
                    logger.warning(f"No infiltration data found in {hdf_path}. Creating new dataset.")
                    # If dataset doesn't exist, use the standard set_infiltration_baseoverrides method
                    return HdfInfiltration.set_infiltration_baseoverrides(hdf_path, infiltration_df)
                
                # Get the exact dtype of the existing dataset
                existing_dtype = hdf_file_read[table_path].dtype
                
                # Extract column names from the existing dataset
                existing_columns = existing_dtype.names
                
                # Check if all columns in the DataFrame exist in the HDF dataset
                for col in infiltration_df.columns:
                    hdf_col = col
                    if col == "Name" and "Land Cover Name" in existing_columns:
                        hdf_col = "Land Cover Name"
                    
                    if hdf_col not in existing_columns:
                        logger.warning(f"Column {col} not found in existing dataset - it will be ignored")
                
                # Get current dataset to preserve structure for non-updated fields
                existing_data = hdf_file_read[table_path][()]
            
            # Create a structured array with the exact same dtype as the existing dataset
            structured_array = np.zeros(len(infiltration_df), dtype=existing_dtype)
            
            # Copy data from DataFrame to structured array, preserving existing structure
            for col in existing_columns:
                df_col = col
                # Map 'Land Cover Name' to 'Name' if needed
                if col == "Land Cover Name" and name_col == "Name":
                    df_col = "Name"
                    
                if df_col in infiltration_df.columns:
                    # Handle string fields - need to maintain exact string length
                    if existing_dtype[col].kind == 'S':
                        # Get the exact string length from dtype
                        max_str_len = existing_dtype[col].itemsize
                        # Convert to bytes with correct length
                        structured_array[col] = infiltration_df[df_col].astype(str).values.astype(f'|S{max_str_len}')
                    else:
                        # Handle numeric fields - ensure correct numeric type
                        if existing_dtype[col].kind in ('f', 'i'):
                            structured_array[col] = infiltration_df[df_col].values.astype(existing_dtype[col])
                        else:
                            # For any other type, just copy as is
                            structured_array[col] = infiltration_df[df_col].values
                else:
                    logger.warning(f"Column {col} not in DataFrame - using default values")
                    # Use zeros for numeric fields or empty strings for string fields
                    if existing_dtype[col].kind == 'S':
                        structured_array[col] = np.array([''] * len(infiltration_df), dtype=f'|S{existing_dtype[col].itemsize}')
            
            # Write back to HDF file
            with h5py.File(hdf_path, 'a') as hdf_file_write:
                # Delete existing dataset
                if table_path in hdf_file_write:
                    del hdf_file_write[table_path]
                
                # Create new dataset with exact same properties as original
                dataset = hdf_file_write.create_dataset(
                    table_path,
                    data=structured_array,
                    dtype=existing_dtype,
                    compression='gzip',
                    compression_opts=1,
                    chunks=(100,),
                    maxshape=(None,)
                )
            
            # Return the DataFrame with columns matching what was actually written
            result_df = pd.DataFrame()
            for col in existing_columns:
                if existing_dtype[col].kind == 'S':
                    # Convert bytes back to string
                    result_df[col] = [val.decode('utf-8').strip() for val in structured_array[col]]
                else:
                    result_df[col] = structured_array[col]
                    
            return result_df

        except Exception as e:
            logger.error(f"Error setting infiltration data in {hdf_path}: {str(e)}")
            return None






'''
==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfMesh.py
==================================================
"""
A static class for handling mesh-related operations on HEC-RAS HDF files.

This class provides static methods to extract and analyze mesh data from HEC-RAS HDF files,
including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
2D flow area attributes. No instantiation is required to use these methods.

All methods are designed to work with the mesh geometry data stored in
HEC-RAS HDF files, providing functionality to retrieve and process various aspects
of the 2D flow areas and their associated mesh structures.


List of Functions:
-----------------
get_mesh_area_names()
    Returns list of 2D mesh area names
get_mesh_areas()
    Returns 2D flow area perimeter polygons
get_mesh_cell_polygons()
    Returns 2D flow mesh cell polygons
get_mesh_cell_points()
    Returns 2D flow mesh cell center points
get_mesh_cell_faces()
    Returns 2D flow mesh cell faces
get_mesh_area_attributes()
    Returns geometry 2D flow area attributes
get_mesh_face_property_tables()
    Returns Face Property Tables for each Face in all 2D Flow Areas
get_mesh_cell_property_tables()
    Returns Cell Property Tables for each Cell in all 2D Flow Areas

Each function is decorated with @standardize_input and @log_call for consistent
input handling and logging functionality.
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from typing import List, Tuple, Optional, Dict, Any, TYPE_CHECKING
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import setup_logging, get_logger

# Type hints only - not imported at runtime
if TYPE_CHECKING:
    from geopandas import GeoDataFrame

logger = get_logger(__name__)


class HdfMesh:
    """
    A class for handling mesh-related operations on HEC-RAS HDF files.

    This class provides methods to extract and analyze mesh data from HEC-RAS HDF files,
    including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
    2D flow area attributes.

    Methods in this class are designed to work with the mesh geometry data stored in
    HEC-RAS HDF files, providing functionality to retrieve and process various aspects
    of the 2D flow areas and their associated mesh structures.

    Note: This class relies on HdfBase and HdfUtils for some underlying operations.
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_area_names(hdf_path: Path) -> List[str]:
        """
        Return a list of the 2D mesh area names from the RAS geometry.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        List[str]
            A list of the 2D mesh area names within the RAS geometry.
            Returns an empty list if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/2D Flow Areas" not in hdf_file:
                    return list()
                return list(
                    [
                        HdfUtils.convert_ras_string(n.decode('utf-8'))
                        for n in hdf_file["Geometry/2D Flow Areas/Attributes"][()]["Name"]
                    ]
                )
        except Exception as e:
            logger.error(f"Error reading mesh area names from {hdf_path}: {str(e)}")
            return list()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_areas(hdf_path: Path) -> 'GeoDataFrame':
        """
        Return 2D flow area perimeter polygons.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow area perimeter polygons if 2D areas exist.
        """
        # Lazy imports for heavy dependencies
        from geopandas import GeoDataFrame
        from shapely.geometry import Polygon

        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()
                mesh_area_polygons = [
                    Polygon(hdf_file["Geometry/2D Flow Areas/{}/Perimeter".format(n)][()])
                    for n in mesh_area_names
                ]
                return GeoDataFrame(
                    {"mesh_name": mesh_area_names, "geometry": mesh_area_polygons},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading mesh areas from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_cell_polygons(hdf_path: Path) -> 'GeoDataFrame':
        """
        Return 2D flow mesh cell polygons.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell polygons with columns:
            - mesh_name: name of the mesh area
            - cell_id: unique identifier for each cell
            - geometry: polygon geometry of the cell
            Returns an empty GeoDataFrame if no 2D areas exist or if there's an error.
        """
        # Lazy imports for heavy dependencies
        from geopandas import GeoDataFrame
        from shapely.geometry import Polygon
        from shapely.ops import polygonize

        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Get face geometries once
                face_gdf = HdfMesh.get_mesh_cell_faces(hdf_path)

                # Pre-allocate lists for better memory efficiency
                all_mesh_names = []
                all_cell_ids = []
                all_geometries = []

                for mesh_name in mesh_area_names:
                    # Get cell face info in one read
                    cell_face_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Face and Orientation Info"][()]
                    cell_face_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Face and Orientation Values"][()][:, 0]

                    # Create face lookup dictionary for this mesh
                    mesh_faces_dict = dict(face_gdf[face_gdf.mesh_name == mesh_name][["face_id", "geometry"]].values)

                    # Process each cell
                    for cell_id, (start, length) in enumerate(cell_face_info[:, :2]):
                        face_ids = cell_face_values[start:start + length]
                        face_geoms = [mesh_faces_dict[face_id] for face_id in face_ids]

                        # Create polygon
                        polygons = list(polygonize(face_geoms))
                        if polygons:
                            all_mesh_names.append(mesh_name)
                            all_cell_ids.append(cell_id)
                            all_geometries.append(Polygon(polygons[0]))

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "cell_id": all_cell_ids,
                        "geometry": all_geometries
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell polygons from {hdf_path}: {str(e)}")
            return GeoDataFrame()
        
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cell_points(hdf_path: Path) -> 'GeoDataFrame':
        """
        Return 2D flow mesh cell center points.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell center points.
        """
        # Lazy imports for heavy dependencies
        from geopandas import GeoDataFrame
        from shapely.geometry import Point

        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Pre-allocate lists
                all_mesh_names = []
                all_cell_ids = []
                all_points = []

                for mesh_name in mesh_area_names:
                    # Get all cell centers in one read
                    cell_centers = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"][()]
                    cell_count = len(cell_centers)

                    # Extend lists efficiently
                    all_mesh_names.extend([mesh_name] * cell_count)
                    all_cell_ids.extend(range(cell_count))
                    all_points.extend(Point(coords) for coords in cell_centers)

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "cell_id": all_cell_ids,
                        "geometry": all_points
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell points from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cell_faces(hdf_path: Path) -> 'GeoDataFrame':
        """
        Return 2D flow mesh cell faces.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell faces.
        """
        # Lazy imports for heavy dependencies
        from geopandas import GeoDataFrame
        from shapely.geometry import LineString

        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Pre-allocate lists
                all_mesh_names = []
                all_face_ids = []
                all_geometries = []

                for mesh_name in mesh_area_names:
                    # Read all data at once
                    facepoints_index = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Perimeter Values"][()]

                    # Process each face
                    for face_id, ((pnt_a_idx, pnt_b_idx), (start_row, count)) in enumerate(zip(facepoints_index, faces_perim_info)):
                        coords = [facepoints_coords[pnt_a_idx]]

                        if count > 0:
                            coords.extend(faces_perim_values[start_row:start_row + count])

                        coords.append(facepoints_coords[pnt_b_idx])

                        all_mesh_names.append(mesh_name)
                        all_face_ids.append(face_id)
                        all_geometries.append(LineString(coords))

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "face_id": all_face_ids,
                        "geometry": all_geometries
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell faces from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_area_attributes(hdf_path: Path) -> pd.DataFrame:
        """
        Return geometry 2D flow area attributes from a HEC-RAS HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing the 2D flow area attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                d2_flow_area = hdf_file.get("Geometry/2D Flow Areas/Attributes")
                if d2_flow_area is not None and isinstance(d2_flow_area, h5py.Dataset):
                    result = {}
                    for name in d2_flow_area.dtype.names:
                        try:
                            value = d2_flow_area[name][()]
                            if isinstance(value, bytes):
                                value = value.decode('utf-8')  # Decode as UTF-8
                            result[name] = value if not isinstance(value, bytes) else value.decode('utf-8')
                        except Exception as e:
                            logger.warning(f"Error converting attribute '{name}': {str(e)}")
                    return pd.DataFrame.from_dict(result, orient='index', columns=['Value'])
                else:
                    logger.info("No 2D Flow Area attributes found or invalid dataset.")
                    return pd.DataFrame()  # Return an empty DataFrame
        except Exception as e:
            logger.error(f"Error reading 2D flow area attributes from {hdf_path}: {str(e)}")
            return pd.DataFrame()  # Return an empty DataFrame

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_face_property_tables(hdf_path: Path) -> Dict[str, pd.DataFrame]:
        """
        Extract Face Property Tables for each Face in all 2D Flow Areas.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        Dict[str, pd.DataFrame]
            A dictionary where:
            - keys: mesh area names (str)
            - values: DataFrames with columns:
                - Face ID: unique identifier for each face
                - Z: elevation
                - Area: face area
                - Wetted Perimeter: wetted perimeter length
                - Manning's n: Manning's roughness coefficient
            Returns an empty dictionary if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return {}

                result = {}
                for mesh_name in mesh_area_names:
                    area_elevation_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Area Elevation Info"][()]
                    area_elevation_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Area Elevation Values"][()]
                    
                    face_data = []
                    for face_id, (start_index, count) in enumerate(area_elevation_info):
                        face_values = area_elevation_values[start_index:start_index+count]
                        for z, area, wetted_perimeter, mannings_n in face_values:
                            face_data.append({
                                'Face ID': face_id,
                                'Z': str(z),
                                'Area': str(area), 
                                'Wetted Perimeter': str(wetted_perimeter),
                                "Manning's n": str(mannings_n)
                            })
                    
                    result[mesh_name] = pd.DataFrame(face_data)
                
                return result

        except Exception as e:
            logger.error(f"Error extracting face property tables from {hdf_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_cell_property_tables(hdf_path: Path) -> Dict[str, pd.DataFrame]:
        """
        Extract Cell Property Tables for each Cell in all 2D Flow Areas.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        Dict[str, pd.DataFrame]
            A dictionary where:
            - keys: mesh area names (str)
            - values: DataFrames with columns:
                - Cell ID: unique identifier for each cell
                - Z: elevation
                - Volume: cell volume
                - Surface Area: cell surface area
            Returns an empty dictionary if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return {}

                result = {}
                for mesh_name in mesh_area_names:
                    cell_elevation_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Elevation Volume Info"][()]
                    cell_elevation_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Elevation Volume Values"][()]
                    
                    cell_data = []
                    for cell_id, (start_index, count) in enumerate(cell_elevation_info):
                        cell_values = cell_elevation_values[start_index:start_index+count]
                        for z, volume, surface_area in cell_values:
                            cell_data.append({
                                'Cell ID': cell_id,
                                'Z': str(z),
                                'Volume': str(volume),
                                'Surface Area': str(surface_area)
                            })
                    
                    result[mesh_name] = pd.DataFrame(cell_data)
                
                return result

        except Exception as e:
            logger.error(f"Error extracting cell property tables from {hdf_path}: {str(e)}")
            return {}

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfPipe.py
==================================================
"""
Class: HdfPipe

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPipe:
Geometry Retrieval Functions:
- get_pipe_conduits() - Get pipe conduit geometries and attributes
- get_pipe_nodes() - Get pipe node geometries and attributes
- get_pipe_network() - Get complete pipe network data
- get_pipe_profile() - Get elevation profile for a specific conduit
- extract_pipe_network_data() - Extract both nodes and conduits data

Results Retrieval Functions:
- get_pipe_network_timeseries() - Get timeseries data for pipe network variables
- get_pipe_network_summary() - Get summary statistics for pipe networks
- get_pipe_node_timeseries() - Get timeseries data for a specific node
- get_pipe_conduit_timeseries() - Get timeseries data for a specific conduit

Note: All functions use the @standardize_input decorator to validate input paths
and the @log_call decorator for logging function calls.
"""
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import LineString, Point, MultiLineString, Polygon, MultiPolygon
from typing import List, Dict, Any, Optional, Union, Tuple
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import get_logger
from .HdfResultsMesh import HdfResultsMesh
import logging  

logger = get_logger(__name__)

class HdfPipe:
    """
    Static methods for handling pipe network data from HEC-RAS HDF files.

    Contains methods for:
    - Geometry retrieval (nodes, conduits, networks, profiles)
    - Results retrieval (timeseries and summary data)

    All methods use @standardize_input for path validation and @log_call
    """

    # Geometry Retrieval Functions
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_conduits(hdf_path: Path, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Extracts pipe conduit geometries and attributes from HDF5 file.

        Parameters:
            hdf_path: Path to the HDF5 file
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame with columns:
            - Attributes from HDF5
            - Polyline: LineString geometries
            - Terrain_Profiles: List of (station, elevation) tuples
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Conduits/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Polyline Data ---
            polyline_info = group['Polyline Info'][:]  # Shape (132,4) - point_start_idx, point_count, part_start_idx, part_count
            polyline_points = group['Polyline Points'][:]  # Shape (396,2) - x,y coordinates
            
            polyline_geometries = []
            for info in polyline_info:
                point_start_idx = info[0]
                point_count = info[1]
                
                # Extract coordinates for this polyline directly using start index and count
                coords = polyline_points[point_start_idx:point_start_idx + point_count]
                
                if len(coords) < 2:
                    polyline_geometries.append(None)
                else:
                    polyline_geometries.append(LineString(coords))
            
            # --- Read Terrain Profiles Data ---
            terrain_info = group['Terrain Profiles Info'][:]
            terrain_values = group['Terrain Profiles Values'][:]
            
            # Create a list of (Station, Elevation) tuples for Terrain Profiles
            terrain_coords = list(zip(terrain_values[:, 0], terrain_values[:, 1]))
            
            terrain_profiles_list: List[List[Tuple[float, float]]] = []
            
            for i in range(len(terrain_info)):
                info = terrain_info[i]
                start_idx = info[0]
                count = info[1]
                
                # Extract (Station, Elevation) pairs
                segment = terrain_coords[start_idx : start_idx + count]
                
                terrain_profiles_list.append(segment)  # Store the list of (Station, Elevation) tuples
            
            # --- Combine Data into GeoDataFrame ---
            attr_df['Polyline'] = polyline_geometries
            attr_df['Terrain_Profiles'] = terrain_profiles_list
            
            # Initialize GeoDataFrame with Polyline geometries
            gdf = gpd.GeoDataFrame(attr_df, geometry='Polyline', crs=crs)
            
            return gdf


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_nodes(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for Pipe Node points and their attributes from an HDF5 file.
        
        Parameters:
        - hdf_path: Path to the HDF5 file.
        
        Returns:
        - A GeoDataFrame containing pipe node attributes and their geometries.
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Nodes/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns  # Changed 'S' to object
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Points Data ---
            points = group['Points'][:]
            # Create Shapely Point geometries
            geometries = [Point(xy) for xy in points]
            
            # --- Combine Attributes and Geometries into GeoDataFrame ---
            gdf = gpd.GeoDataFrame(attr_df, geometry=geometries)
            
            return gdf
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network(hdf_path: Path, pipe_network_name: Optional[str] = None, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for a pipe network's geometry.

        Parameters:
            hdf_path: Path to the HDF5 file
            pipe_network_name: Name of network (uses first if None)
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame containing:
            - Cell polygons (primary geometry)
            - Face polylines
            - Node points
            - Associated attributes
        """
        with h5py.File(hdf_path, 'r') as f:
            pipe_networks_group = f['/Geometry/Pipe Networks/']
            
            # --- Determine Pipe Network to Use ---
            attributes = pipe_networks_group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode 'Name' from byte strings to UTF-8
            attr_df['Name'] = attr_df['Name'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            if pipe_network_name:
                if pipe_network_name not in attr_df['Name'].values:
                    raise ValueError(f"Pipe network '{pipe_network_name}' not found in the HDF5 file.")
                network_idx = attr_df.index[attr_df['Name'] == pipe_network_name][0]
            else:
                network_idx = 0  # Default to first network
            
            # Get the name of the selected pipe network
            selected_network_name = attr_df.at[network_idx, 'Name']
            logging.info(f"Selected Pipe Network: {selected_network_name}")
            
            # Access the selected pipe network group
            network_group_path = f"/Geometry/Pipe Networks/{selected_network_name}/"
            network_group = f[network_group_path]
            
            # --- Helper Functions ---
            def decode_bytes(df: pd.DataFrame) -> pd.DataFrame:
                """Decode byte string columns to UTF-8."""
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                return df
            
            def build_polygons(info, parts, points) -> List[Optional[Polygon or MultiPolygon]]:
                """Build Shapely Polygon or MultiPolygon geometries from HDF5 datasets."""
                poly_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    cell_info = info[i]
                    point_start_idx = cell_info[0]
                    point_count = cell_info[1]
                    part_start_idx = cell_info[2]
                    part_count = cell_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = poly_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 3:
                            continue  # Not a valid polygon part
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        try:
                            geometries.append(Polygon(parts_list[0]))
                        except ValueError:
                            geometries.append(None)
                    else:
                        try:
                            geometries.append(MultiPolygon([Polygon(p) for p in parts_list if len(p) >= 3]))
                        except ValueError:
                            geometries.append(None)
                return geometries
            
            def build_multilinestring(info, parts, points) -> List[Optional[LineString or MultiLineString]]:
                """Build Shapely LineString or MultiLineString geometries from HDF5 datasets."""
                line_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    face_info = info[i]
                    point_start_idx = face_info[0]
                    point_count = face_info[1]
                    part_start_idx = face_info[2]
                    part_count = face_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = line_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 2:
                            continue  # Cannot form LineString with fewer than 2 points
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        geometries.append(LineString(parts_list[0]))
                    else:
                        geometries.append(MultiLineString(parts_list))
                return geometries
            
            # --- Read and Process Cell Polygons ---
            cell_polygons_info = network_group['Cell Polygons Info'][:]
            cell_polygons_parts = network_group['Cell Polygons Parts'][:]
            cell_polygons_points = network_group['Cell Polygons Points'][:]
            
            cell_polygons_geometries = build_polygons(cell_polygons_info, cell_polygons_parts, cell_polygons_points)
            
            # --- Read and Process Face Polylines ---
            face_polylines_info = network_group['Face Polylines Info'][:]
            face_polylines_parts = network_group['Face Polylines Parts'][:]
            face_polylines_points = network_group['Face Polylines Points'][:]
            
            face_polylines_geometries = build_multilinestring(face_polylines_info, face_polylines_parts, face_polylines_points)
            
            # --- Read and Process Node Points ---
            node_surface_connectivity_group = network_group.get('Node Surface Connectivity', None)
            if node_surface_connectivity_group is not None:
                node_surface_connectivity = node_surface_connectivity_group[:]
            else:
                node_surface_connectivity = None
            
            # Assuming Node Connectivity Info and Values contain node coordinates
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            node_indices = network_group['Node Indices'][:]
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            
            # For simplicity, assuming that node connectivity includes X and Y coordinates
            # This may need to be adjusted based on actual data structure
            # Here, we'll create dummy points as placeholder
            # Replace with actual coordinate extraction logic as per data structure
            # For demonstration, we'll create random points
            # You should replace this with actual data extraction
            # Example:
            # node_points = network_group['Node Coordinates'][:]
            # node_geometries = [Point(x, y) for x, y in node_points]
            
            # Placeholder for node geometries
            # Assuming node_indices contains Node IDs and coordinates
            # Adjust based on actual dataset structure
            # Here, we assume that node_indices has columns: [Node ID, X, Y]
            # But based on the log, Node Surface Connectivity has ['Node ID', 'Layer', 'Layer ID', 'Sublayer ID']
            # No coordinates are provided, so we cannot create Point geometries unless coordinates are available elsewhere
            # Therefore, this part may need to be adapted based on actual data
            # For now, we'll skip node points geometries
            node_geometries = [None] * len(node_indices)  # Placeholder
            
            # --- Read and Process Cell Property Table ---
            cell_property_table = network_group['Cell Property Table'][:]
            cell_property_df = pd.DataFrame(cell_property_table)
            
            # Decode byte strings if any
            cell_property_df = decode_bytes(cell_property_df)
            
            # --- Read and Process Cells DS Face Indices ---
            cells_ds_face_info = network_group['Cells DS Face Indices Info'][:]
            cells_ds_face_values = network_group['Cells DS Face Indices Values'][:]
            
            # Create lists of DS Face Indices per cell
            cells_ds_face_indices = []
            for i in range(len(cells_ds_face_info)):
                info = cells_ds_face_info[i]
                start_idx, count = info
                indices = cells_ds_face_values[start_idx : start_idx + count]
                cells_ds_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Face Indices ---
            cells_face_info = network_group['Cells Face Indices Info'][:]
            cells_face_values = network_group['Cells Face Indices Values'][:]
            
            # Create lists of Face Indices per cell
            cells_face_indices = []
            for i in range(len(cells_face_info)):
                info = cells_face_info[i]
                start_idx, count = info
                indices = cells_face_values[start_idx : start_idx + count]
                cells_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Minimum Elevations ---
            cells_min_elevations = network_group['Cells Minimum Elevations'][:]
            cells_min_elevations_df = pd.DataFrame(cells_min_elevations, columns=['Minimum_Elevation'])
            
            # --- Read and Process Cells Node and Conduit IDs ---
            cells_node_conduit_ids = network_group['Cells Node and Conduit IDs'][:]
            cells_node_conduit_df = pd.DataFrame(cells_node_conduit_ids, columns=['Node_ID', 'Conduit_ID'])
            
            # --- Read and Process Cells US Face Indices ---
            cells_us_face_info = network_group['Cells US Face Indices Info'][:]
            cells_us_face_values = network_group['Cells US Face Indices Values'][:]
            
            # Create lists of US Face Indices per cell
            cells_us_face_indices = []
            for i in range(len(cells_us_face_info)):
                info = cells_us_face_info[i]
                start_idx, count = info
                indices = cells_us_face_values[start_idx : start_idx + count]
                cells_us_face_indices.append(indices.tolist())
            
            # --- Read and Process Conduit Indices ---
            conduit_indices = network_group['Conduit Indices'][:]
            conduit_indices_df = pd.DataFrame(conduit_indices, columns=['Conduit_ID'])
            
            # --- Read and Process Face Property Table ---
            face_property_table = network_group['Face Property Table'][:]
            face_property_df = pd.DataFrame(face_property_table)
            
            # Decode byte strings if any
            face_property_df = decode_bytes(face_property_df)
            
            # --- Read and Process Face Conduit ID and Stations ---
            faces_conduit_id_stations = network_group['Faces Conduit ID and Stations'][:]
            faces_conduit_df = pd.DataFrame(faces_conduit_id_stations, columns=['ConduitID', 'ConduitStation', 'CellUS', 'CellDS', 'Elevation'])
            
            # --- Read and Process Node Connectivity Info and Values ---
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            
            # Create lists of connected nodes per node
            node_connectivity = []
            for i in range(len(node_connectivity_info)):
                info = node_connectivity_info[i]
                start_idx, count = info
                connections = node_connectivity_values[start_idx : start_idx + count]
                node_connectivity.append(connections.tolist())
            
            # --- Read and Process Node Indices ---
            node_indices = network_group['Node Indices'][:]
            node_indices_df = pd.DataFrame(node_indices, columns=['Node_ID'])
            
            # --- Read and Process Node Surface Connectivity ---
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            node_surface_connectivity_df = pd.DataFrame(node_surface_connectivity, columns=['Node_ID', 'Layer', 'Layer_ID', 'Sublayer_ID'])
            
            # --- Combine All Cell-Related Data ---
            cells_df = pd.DataFrame({
                'Cell_ID': range(len(cell_polygons_geometries)),
                'Conduit_ID': cells_node_conduit_df['Conduit_ID'],
                'Node_ID': cells_node_conduit_df['Node_ID'],
                'Minimum_Elevation': cells_min_elevations_df['Minimum_Elevation'],
                'DS_Face_Indices': cells_ds_face_indices,
                'Face_Indices': cells_face_indices,
                'US_Face_Indices': cells_us_face_indices,
                'Cell_Property_Info_Index': cell_property_df['Info Index'],
                # Add other cell properties as needed
            })
            
            # Merge with cell property table
            cells_df = cells_df.merge(cell_property_df, left_on='Cell_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Face-Related Data ---
            faces_df = pd.DataFrame({
                'Face_ID': range(len(face_polylines_geometries)),
                'Conduit_ID': faces_conduit_df['ConduitID'],
                'Conduit_Station': faces_conduit_df['ConduitStation'],
                'Cell_US': faces_conduit_df['CellUS'],
                'Cell_DS': faces_conduit_df['CellDS'],
                'Elevation': faces_conduit_df['Elevation'],
                'Face_Property_Info_Index': face_property_df['Info Index'],
                # Add other face properties as needed
            })
            
            # Merge with face property table
            faces_df = faces_df.merge(face_property_df, left_on='Face_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Node-Related Data ---
            nodes_df = pd.DataFrame({
                'Node_ID': node_indices_df['Node_ID'],
                'Connected_Nodes': node_connectivity,
                # Add other node properties as needed
            })
            
            # Merge with node surface connectivity
            nodes_df = nodes_df.merge(node_surface_connectivity_df, on='Node_ID', how='left')
            
            # --- Create GeoDataFrame ---
            # Main DataFrame will be cells with their polygons
            cells_df['Cell_Polygon'] = cell_polygons_geometries
            
            # Add face polylines as a separate column (list of geometries)
            cells_df['Face_Polylines'] = cells_df['Face_Indices'].apply(lambda indices: [face_polylines_geometries[i] for i in indices if i < len(face_polylines_geometries)])
            
            # Add node points if geometries are available
            # Currently, node_geometries are placeholders (None). Replace with actual geometries if available.
            cells_df['Node_Point'] = cells_df['Node_ID'].apply(lambda nid: node_geometries[nid] if nid < len(node_geometries) else None)
            
            # Initialize GeoDataFrame with Cell Polygons
            gdf = gpd.GeoDataFrame(cells_df, geometry='Cell_Polygon', crs=crs)
            
            # Optionally, add Face Polylines and Node Points as separate columns
            # Note: GeoPandas primarily supports one geometry column, so these are stored as object columns
            gdf['Face_Polylines'] = cells_df['Face_Polylines']
            gdf['Node_Point'] = cells_df['Node_Point']
            
            # You can further expand this GeoDataFrame by merging with faces_df and nodes_df if needed
            
            return gdf
        
        
        
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_profile(hdf_path: Path, conduit_id: int) -> pd.DataFrame:
        """
        Extract the profile data for a specific pipe conduit.

        Args:
            hdf_path (Path): Path to the HDF file.
            conduit_id (int): ID of the conduit to extract profile for.

        Returns:
            pd.DataFrame: DataFrame containing the pipe profile data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
            IndexError: If the specified conduit_id is out of range.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Get conduit info
                terrain_profiles_info = hdf['/Geometry/Pipe Conduits/Terrain Profiles Info'][()]
                
                if conduit_id >= len(terrain_profiles_info):
                    raise IndexError(f"conduit_id {conduit_id} is out of range")

                start, count = terrain_profiles_info[conduit_id]

                # Extract profile data
                profile_values = hdf['/Geometry/Pipe Conduits/Terrain Profiles Values'][start:start+count]

                # Create DataFrame
                df = pd.DataFrame(profile_values, columns=['Station', 'Elevation'])

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except IndexError as e:
            logger.error(f"Invalid conduit_id: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe profile data: {e}")
            raise
        
        
   









# RESULTS FUNCTIONS: 

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract results summary data for pipe networks from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pipe network summary data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pipe Network"
                if summary_path not in hdf:
                    logger.warning("Pipe Network summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame
                df = pd.DataFrame(summary_data)

                # Convert column names
                df.columns = [col.decode('utf-8') for col in df.columns]

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network summary data: {e}")
            raise




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_node(plan_hdf_path: Path, node_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific node.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        node_id : int
            ID of the node to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Depth
            - Drop Inlet Flow
            - Water Surface
        """
        try:
            node_variables = ["Nodes/Depth", "Nodes/Drop Inlet Flow", "Nodes/Water Surface"]
            node_data = {}

            for variable in node_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                node_data[variable] = data.sel(location=node_id)
            
            return node_data
        except Exception as e:
            logger.error(f"Error extracting time series data for node {node_id}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_conduit(plan_hdf_path: Path, conduit_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific conduit.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        conduit_id : int
            ID of the conduit to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Pipe Flow (US/DS)
            - Velocity (US/DS)
        """
        try:
            conduit_variables = ["Pipes/Pipe Flow DS", "Pipes/Pipe Flow US", 
                                "Pipes/Vel DS", "Pipes/Vel US"]
            conduit_data = {}

            for variable in conduit_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                conduit_data[variable] = data.sel(location=conduit_id)
            
            return conduit_data
        except Exception as e:
            logger.error(f"Error extracting time series data for conduit {conduit_id}: {str(e)}")
            raise


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_timeseries(hdf_path: Path, variable: str) -> xr.DataArray:
        """
        Extracts timeseries data for a pipe network variable.

        Parameters:
            hdf_path: Path to the HDF5 file
            variable: Variable name to extract. Valid options:
                - Cell: Courant, Water Surface
                - Face: Flow, Velocity, Water Surface
                - Pipes: Pipe Flow (DS/US), Vel (DS/US)
                - Nodes: Depth, Drop Inlet Flow, Water Surface

        Returns:
            xarray.DataArray with dimensions (time, location)
        """
        valid_variables = [
            "Cell Courant", "Cell Water Surface", "Face Flow", "Face Velocity",
            "Face Water Surface", "Pipes/Pipe Flow DS", "Pipes/Pipe Flow US",
            "Pipes/Vel DS", "Pipes/Vel US", "Nodes/Depth", "Nodes/Drop Inlet Flow",
            "Nodes/Water Surface"
        ]

        if variable not in valid_variables:
            raise ValueError(f"Invalid variable. Must be one of: {', '.join(valid_variables)}")

        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract timeseries data
                data_path = f"/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pipe Networks/Davis/{variable}"
                data = hdf[data_path][()]

                # Extract time information using the correct method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'location'],
                    coords={'time': time, 'location': range(data.shape[1])},
                    name=variable
                )

                # Add attributes
                da.attrs['units'] = hdf[data_path].attrs.get('Units', b'').decode('utf-8')
                da.attrs['variable'] = variable

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network timeseries data: {e}")
            raise





==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfPlan.py
==================================================
"""
Class: HdfPlan

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.


- get_plan_start_time()
- get_plan_end_time()
- get_plan_timestamps_list()     
- get_plan_information()
- get_plan_parameters()
- get_plan_met_precip()
- get_geometry_information()






"""

import h5py
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import re
import numpy as np

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfPlan:
    """
    A class for handling HEC-RAS plan HDF files.

    Provides static methods for extracting data from HEC-RAS plan HDF files including 
    simulation times, plan information, and geometry attributes. All methods use 
    @standardize_input for handling different input types and @log_call for logging.

    Note: This code is partially derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    under MIT license.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_start_time(hdf_path: Path) -> datetime:
        """
        Get the plan start time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan start time in UTC format.

        Raises:
            ValueError: If there's an error reading the plan start time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_simulation_start_time(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan start time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_end_time(hdf_path: Path) -> datetime:
        """
        Get the plan end time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan end time.

        Raises:
            ValueError: If there's an error reading the plan end time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info = hdf_file.get('Plan Data/Plan Information')
                if plan_info is None:
                    raise ValueError("Plan Information not found in HDF file")
                time_str = plan_info.attrs.get('Simulation End Time')
                return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))
        except Exception as e:
            raise ValueError(f"Failed to get plan end time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_timestamps_list(hdf_path: Path) -> List[datetime]:
        """
        Get the list of output timestamps from the plan simulation.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            List[datetime]: Chronological list of simulation output timestamps in UTC.

        Raises:
            ValueError: If there's an error retrieving the plan timestamps.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_unsteady_timestamps(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan timestamps: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_information(hdf_path: Path) -> Dict:
        """
        Get plan information from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Plan information including simulation times, flow regime, 
                computation settings, etc.

        Raises:
            ValueError: If there's an error retrieving the plan information.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info_path = "Plan Data/Plan Information"
                if plan_info_path not in hdf_file:
                    raise ValueError(f"Plan Information not found in {hdf_path}")
                
                attrs = {}
                for key in hdf_file[plan_info_path].attrs.keys():
                    value = hdf_file[plan_info_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            raise ValueError(f"Failed to get plan information attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_parameters(hdf_path: Path) -> pd.DataFrame:
        """
        Get plan parameter attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: A DataFrame containing the plan parameters with columns:
                - Parameter: Name of the parameter
                - Value: Value of the parameter (decoded if byte string)
                - Plan: Plan number (01-99) extracted from the filename (ProjectName.pXX.hdf)
            Returns empty DataFrame if Plan Parameters not found (e.g., steady flow results).

        Raises:
            ValueError: If there's an error retrieving the plan parameter attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_params_path = "Plan Data/Plan Parameters"
                if plan_params_path not in hdf_file:
                    logger.warning(f"Plan Parameters not found in {hdf_path} - may be a steady flow or minimal HDF file")
                    return pd.DataFrame(columns=['Plan', 'Parameter', 'Value'])
                
                # Extract parameters
                params_dict = {}
                for key in hdf_file[plan_params_path].attrs.keys():
                    value = hdf_file[plan_params_path].attrs[key]
                    
                    # Handle different types of values
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    elif isinstance(value, np.ndarray):
                        # Handle array values
                        if value.dtype.kind in {'S', 'a'}:  # Array of byte strings
                            value = [v.decode('utf-8') if isinstance(v, bytes) else v for v in value]
                        else:
                            value = value.tolist()  # Convert numpy array to list
                        
                        # If it's a single-item list, extract the value
                        if len(value) == 1:
                            value = value[0]
                    
                    params_dict[key] = value
                
                # Create DataFrame from parameters
                df = pd.DataFrame.from_dict(params_dict, orient='index', columns=['Value'])
                df.index.name = 'Parameter'
                df = df.reset_index()
                
                # Extract plan number from filename
                filename = Path(hdf_path).name
                plan_match = re.search(r'\.p(\d{2})\.', filename)
                if plan_match:
                    plan_num = plan_match.group(1)
                else:
                    plan_num = "00"  # Default if no match found
                    logger.warning(f"Could not extract plan number from filename: {filename}")
                
                df['Plan'] = plan_num
                
                # Reorder columns to put Plan first
                df = df[['Plan', 'Parameter', 'Value']]
                
                return df

        except Exception as e:
            raise ValueError(f"Failed to get plan parameter attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_met_precip(hdf_path: Path) -> Dict:
        """
        Get precipitation attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Precipitation attributes including method, time series data,
                and spatial distribution if available. Returns empty dict if
                no precipitation data exists.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                precip_path = "Event Conditions/Meteorology/Precipitation"
                if precip_path not in hdf_file:
                    logger.error(f"Precipitation data not found in {hdf_path}")
                    return {}
                
                attrs = {}
                for key in hdf_file[precip_path].attrs.keys():
                    value = hdf_file[precip_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            logger.error(f"Failed to get precipitation attributes: {str(e)}")
            return {}
        
    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_geometry_information(hdf_path: Path) -> pd.DataFrame:
        """
        Get root level geometry attributes from the HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: DataFrame with geometry attributes including Creation Date/Time,
                        Version, Units, and Projection information.

        Raises:
            ValueError: If Geometry group is missing or there's an error reading attributes.
        """
        logger.info(f"Getting geometry attributes from {hdf_path}")
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                geom_attrs_path = "Geometry"
                logger.info(f"Checking for Geometry group in {hdf_path}")
                if geom_attrs_path not in hdf_file:
                    logger.error(f"Geometry group not found in {hdf_path}")
                    raise ValueError(f"Geometry group not found in {hdf_path}")

                attrs = {}
                geom_group = hdf_file[geom_attrs_path]
                logger.info("Getting root level geometry attributes")
                # Get root level geometry attributes only
                for key, value in geom_group.attrs.items():
                    if isinstance(value, bytes):
                        try:
                            value = HdfUtils.convert_ras_string(value)
                        except UnicodeDecodeError:
                            logger.warning(f"Failed to decode byte string for root attribute {key}")
                            continue
                    attrs[key] = value
                    logger.debug(f"Geometry attribute: {key} = {value}")

                logger.info(f"Successfully extracted {len(attrs)} root level geometry attributes")
                return pd.DataFrame.from_dict(attrs, orient='index', columns=['Value'])

        except (OSError, RuntimeError) as e:
            logger.error(f"Failed to read HDF file {hdf_path}: {str(e)}")
            raise ValueError(f"Failed to read HDF file {hdf_path}: {str(e)}")
        except Exception as e:
            logger.error(f"Failed to get geometry attributes: {str(e)}")
            raise ValueError(f"Failed to get geometry attributes: {str(e)}")



==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfPlot.py
==================================================
"""
Class: HdfPlot

A collection of static methods for plotting general HDF data from HEC-RAS models.

Lazy Loading:
    matplotlib and geopandas are imported inside methods that use them,
    not at module level. This reduces import overhead for users who
    don't use plotting functionality.
"""

import pandas as pd
from typing import Optional, Union, Tuple, TYPE_CHECKING
from ..Decorators import log_call, standardize_input
from .HdfUtils import HdfUtils

# Type hints only - not imported at runtime
if TYPE_CHECKING:
    import geopandas as gpd


class HdfPlot:
    """
    A class containing static methods for plotting general HDF data from HEC-RAS models.

    This class provides plotting functionality for HDF data, focusing on
    geometric elements like cell polygons and time series data.

    Note:
        matplotlib and geopandas are lazy-loaded when plotting methods are called.
    """

    @staticmethod
    @log_call
    def plot_mesh_cells(
        cell_polygons_df: pd.DataFrame,
        projection: str,
        title: str = '2D Flow Area Mesh Cells',
        figsize: Tuple[int, int] = (12, 8)
    ) -> Optional['gpd.GeoDataFrame']:
        """
        Plots the mesh cells from the provided DataFrame and returns the GeoDataFrame.

        Args:
            cell_polygons_df (pd.DataFrame): DataFrame containing cell polygons.
            projection (str): The coordinate reference system to assign to the GeoDataFrame.
            title (str, optional): Plot title. Defaults to '2D Flow Area Mesh Cells'.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 8).

        Returns:
            Optional[gpd.GeoDataFrame]: GeoDataFrame containing the mesh cells, or None if no cells found.
        """
        # Lazy imports for heavy dependencies
        import matplotlib.pyplot as plt
        import geopandas as gpd

        if cell_polygons_df.empty:
            print("No Cell Polygons found.")
            return None

        # Convert any datetime columns to strings using HdfUtils
        cell_polygons_df = HdfUtils.convert_df_datetimes_to_str(cell_polygons_df)

        cell_polygons_gdf = gpd.GeoDataFrame(cell_polygons_df, crs=projection)

        print("Cell Polygons CRS:", cell_polygons_gdf.crs)
        try:
            display(cell_polygons_gdf.head())
        except NameError:
            # display() not available outside Jupyter
            print(cell_polygons_gdf.head())

        fig, ax = plt.subplots(figsize=figsize)
        cell_polygons_gdf.plot(ax=ax, edgecolor='blue', facecolor='none')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()

        return cell_polygons_gdf

    @staticmethod
    @log_call
    def plot_time_series(
        df: pd.DataFrame,
        x_col: str,
        y_col: str,
        title: str = None,
        figsize: Tuple[int, int] = (12, 6)
    ) -> None:
        """
        Plots time series data from HDF results.

        Args:
            df (pd.DataFrame): DataFrame containing the time series data
            x_col (str): Name of the column containing x-axis data (usually time)
            y_col (str): Name of the column containing y-axis data
            title (str, optional): Plot title. Defaults to None.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 6).
        """
        # Lazy import for heavy dependency
        import matplotlib.pyplot as plt

        # Convert any datetime columns to strings
        df = HdfUtils.convert_df_datetimes_to_str(df)

        fig, ax = plt.subplots(figsize=figsize)
        df.plot(x=x_col, y=y_col, ax=ax)

        if title:
            ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfPump.py
==================================================
"""
Class: HdfPump

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPump:
- get_pump_stations()
- get_pump_groups()
- get_pump_station_timeseries()
- get_pump_station_summary()
- get_pump_operation_timeseries()


"""


import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import Point
from typing import List, Dict, Any, Optional, Union
from .HdfUtils import HdfUtils
from .HdfBase import HdfBase
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfPump:
    """
    A class for handling pump station related data from HEC-RAS HDF files.

    This class provides static methods to extract and process pump station data, including:
    - Pump station locations and attributes
    - Pump group configurations and efficiency curves
    - Time series results for pump operations
    - Summary statistics for pump stations

    All methods are static and designed to work with HEC-RAS HDF files containing pump data.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_stations(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Extract pump station data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing pump station data with columns:
                - geometry: Point geometry of pump station location
                - station_id: Unique identifier for each pump station
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump station datasets are not found in the HDF file.
            Exception: If there are errors processing the pump station data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump station data
                attributes = hdf['/Geometry/Pump Stations/Attributes'][()]
                points = hdf['/Geometry/Pump Stations/Points'][()]

                # Create geometries
                geometries = [Point(x, y) for x, y in points]

                # Create GeoDataFrame
                gdf = gpd.GeoDataFrame(geometry=geometries)
                gdf['station_id'] = range(len(gdf))

                # Add attributes and decode byte strings
                attr_df = pd.DataFrame(attributes)
                string_columns = attr_df.select_dtypes([object]).columns
                for col in string_columns:
                    attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                
                for col in attr_df.columns:
                    gdf[col] = attr_df[col]

                # Set CRS if available
                crs = HdfBase.get_projection(hdf_path)
                if crs:
                    gdf.set_crs(crs, inplace=True)

                return gdf

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_groups(hdf_path: Path) -> pd.DataFrame:
        """
        Extract pump group data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump group data with columns:
                - efficiency_curve_start: Starting index of efficiency curve data
                - efficiency_curve_count: Number of points in efficiency curve
                - efficiency_curve: List of efficiency curve values
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump group datasets are not found in the HDF file.
            Exception: If there are errors processing the pump group data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump group data
                attributes = hdf['/Geometry/Pump Stations/Pump Groups/Attributes'][()]
                efficiency_curves_info = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Info'][()]
                efficiency_curves_values = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Values'][()]

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(attributes)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                # Add efficiency curve data
                df['efficiency_curve_start'] = efficiency_curves_info[:, 0]
                df['efficiency_curve_count'] = efficiency_curves_info[:, 1]

                # Process efficiency curves
                def get_efficiency_curve(start, count):
                    return efficiency_curves_values[start:start+count].tolist()

                df['efficiency_curve'] = df.apply(lambda row: get_efficiency_curve(row['efficiency_curve_start'], row['efficiency_curve_count']), axis=1)

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump group data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_station_timeseries(hdf_path: Path, pump_station: str) -> xr.DataArray:
        """
        Extract timeseries results data for a specific pump station.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.
            pump_station (str): Name or identifier of the pump station.

        Returns:
            xr.DataArray: DataArray containing the timeseries data with dimensions:
                - time: Timestamps of simulation
                - variable: Variables including ['Flow', 'Stage HW', 'Stage TW', 
                           'Pump Station', 'Pumps on']
            Attributes include units and pump station name.

        Raises:
            KeyError: If required datasets are not found in the HDF file.
            ValueError: If the specified pump station name is not found.
            Exception: If there are errors processing the timeseries data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Check if the pump station exists
                pumping_stations_path = "/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pumping Stations"
                if pump_station not in hdf[pumping_stations_path]:
                    raise ValueError(f"Pump station '{pump_station}' not found in HDF file")

                # Extract timeseries data
                data_path = f"{pumping_stations_path}/{pump_station}/Structure Variables"
                data = hdf[data_path][()]

                # Extract time information - Updated to use new method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'variable'],
                    coords={'time': time, 'variable': ['Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on']},
                    name=pump_station
                )

                # Add attributes and decode byte strings
                units = hdf[data_path].attrs.get('Variable_Unit', b'')
                da.attrs['units'] = units.decode('utf-8') if isinstance(units, bytes) else units
                da.attrs['pump_station'] = pump_station

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except ValueError as e:
            logger.error(str(e))
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station timeseries data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_station_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract summary statistics and performance data for all pump stations.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump station summary data including
                operational statistics and performance metrics. Returns empty DataFrame
                if no summary data is found.

        Raises:
            KeyError: If the summary dataset is not found in the HDF file.
            Exception: If there are errors processing the summary data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pump Station"
                if summary_path not in hdf:
                    logger.warning("Pump Station summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame and decode byte strings
                df = pd.DataFrame(summary_data)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station summary data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_operation_timeseries(hdf_path: Path, pump_station: str) -> pd.DataFrame:
        """
        Extract detailed pump operation results data for a specific pump station.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.
            pump_station (str): Name or identifier of the pump station.

        Returns:
            pd.DataFrame: DataFrame containing pump operation data with columns:
                - Time: Simulation timestamps
                - Flow: Pump flow rate
                - Stage HW: Headwater stage
                - Stage TW: Tailwater stage
                - Pump Station: Station identifier
                - Pumps on: Number of active pumps

        Raises:
            KeyError: If required datasets are not found in the HDF file.
            ValueError: If the specified pump station name is not found.
            Exception: If there are errors processing the operation data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Check if the pump station exists
                pump_stations_path = "/Results/Unsteady/Output/Output Blocks/DSS Profile Output/Unsteady Time Series/Pumping Stations"
                if pump_station not in hdf[pump_stations_path]:
                    raise ValueError(f"Pump station '{pump_station}' not found in HDF file")

                # Extract pump operation data
                data_path = f"{pump_stations_path}/{pump_station}/Structure Variables"
                data = hdf[data_path][()]

                # Extract time information - Updated to use new method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(data, columns=['Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on'])
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                    
                df['Time'] = time

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except ValueError as e:
            logger.error(str(e))
            raise
        except Exception as e:
            logger.error(f"Error extracting pump operation data: {e}")
            raise
==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfResultsBreach.py
==================================================
"""
HdfResultsBreach: Dam breach results extraction from HEC-RAS HDF files.

This module provides methods for extracting breach results from HDF output files,
including time series data, summary statistics, and breach geometry evolution.

Architectural Note:
    - HdfResultsBreach: Breach RESULTS from HDF files (.p##.hdf)
    - RasBreach: Breach PARAMETERS from plan files (.p##)
    - HdfStruc: Structure data from HDF files (non-breach specific)

This class focuses exclusively on HDF results extraction. For reading/writing breach
parameters in plan files, use RasBreach class.

Classes:
    HdfResultsBreach: Static methods for breach results extraction

Key Methods:
    - get_structure_variables(): Extract structure flow variables (Total Flow, HW, TW)
    - get_breaching_variables(): Extract breach geometry progression
    - get_breach_timeseries(): Combined breach + structure time series (primary method)
    - get_breach_summary(): Summary statistics (peaks, timing, final geometry)

Examples:
    >>> from ras_commander import HdfResultsBreach
    >>>
    >>> # Extract complete breach time series
    >>> df = HdfResultsBreach.get_breach_timeseries("02", "Laxton_Dam")
    >>>
    >>> # Get summary statistics
    >>> summary = HdfResultsBreach.get_breach_summary("02")
    >>>
    >>> # Plot breach evolution
    >>> import matplotlib.pyplot as plt
    >>> plt.plot(df['datetime'], df['bottom_width'])
    >>> plt.ylabel('Breach Width (ft)')

Author: ras-commander development team
Date: 2025
"""

from typing import Union, Optional
from pathlib import Path
import h5py
import pandas as pd
import numpy as np

from ..Decorators import standardize_input, log_call
from .HdfBase import HdfBase
from ..LoggingConfig import get_logger
from ..RasPrj import ras

logger = get_logger(__name__)


class HdfResultsBreach:
    """
    Handles dam breach results extraction from HEC-RAS HDF files.

    This class provides comprehensive breach results extraction including:
    - Time series data (flow, water levels, breach geometry)
    - Summary statistics (peak values, timing)
    - Structure-level flow variables

    All methods are static and designed for plan-number-based access
    via the @standardize_input decorator.

    Architectural Note:
        - Use HdfResultsBreach for extracting breach RESULTS from HDF files
        - Use RasBreach for reading/writing breach PARAMETERS in plan files
        - Use HdfStruc for structure listings and metadata
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_structure_variables(hdf_path: Path, structure_name: str = None, *,
                                ras_object=None) -> pd.DataFrame:
        """
        Extract structure-level flow variables (Total Flow, Weir Flow, HW, TW).

        This is the primary time series for overall structure performance.
        Available for all SA/2D connections (with or without breach).

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure name. If None, returns all structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Time series data with columns:
            - datetime: Timestamp
            - structure: Structure name (if multiple structures)
            - total_flow: Total flow through structure (cfs or m³/s)
            - weir_flow: Flow over weir (cfs or m³/s)
            - hw: Headwater elevation at representative station (ft or m)
            - tw: Tailwater elevation at representative station (ft or m)

        Examples
        --------
        >>> # Get all structures
        >>> df = HdfResultsBreach.get_structure_variables("02")

        >>> # Get specific structure
        >>> df = HdfResultsBreach.get_structure_variables("02", "Laxton_Dam")

        >>> # Plot flow hydrograph
        >>> import matplotlib.pyplot as plt
        >>> plt.plot(df['datetime'], df['total_flow'])
        >>> plt.ylabel('Flow (cfs)')

        Notes
        -----
        - HW and TW are at representative stations defined in structure attributes
        - For breach structures, use get_breaching_variables() for breach-specific data
        - Units depend on project unit system (US Customary or SI)
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                sa_conn_path = f"{base_path}/SA 2D Area Conn"

                if sa_conn_path not in hdf_file:
                    logger.warning(f"No SA 2D Area Conn data in {hdf_path.name}")
                    return pd.DataFrame()

                # Get timestamps
                time_stamps = HdfBase.get_unsteady_timestamps(hdf_file)

                # Get structure names
                if structure_name:
                    structures = [structure_name]
                else:
                    from .HdfStruc import HdfStruc
                    structures = HdfStruc.list_sa2d_connections(hdf_path, ras_object=ras_object)

                # Extract data for each structure
                data_list = []
                for struct in structures:
                    struct_path = f"{sa_conn_path}/{struct}"
                    var_path = f"{struct_path}/Structure Variables"

                    if var_path not in hdf_file:
                        logger.warning(f"Structure Variables not found for {struct}")
                        continue

                    # Extract dataset
                    dataset = hdf_file[var_path][:]  # shape: (n_timesteps, 4)

                    # Get variable names and units from attributes
                    if 'Variable_Unit' in hdf_file[var_path].attrs:
                        var_unit = hdf_file[var_path].attrs['Variable_Unit']
                        # var_unit is array of [name, unit] pairs

                    # Create DataFrame for this structure
                    struct_data = pd.DataFrame({
                        'datetime': time_stamps,
                        'total_flow': dataset[:, 0],
                        'weir_flow': dataset[:, 1],
                        'hw': dataset[:, 2],
                        'tw': dataset[:, 3]
                    })

                    if len(structures) > 1:
                        struct_data.insert(1, 'structure', struct)

                    data_list.append(struct_data)

                # Combine all structures
                if data_list:
                    result_df = pd.concat(data_list, ignore_index=True)
                    logger.info(f"Extracted {len(time_stamps)} timesteps for {len(structures)} structure(s)")
                    return result_df
                else:
                    return pd.DataFrame()

        except Exception as e:
            logger.error(f"Error extracting structure variables: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_breaching_variables(hdf_path: Path, structure_name: str = None, *,
                               ras_object=None) -> pd.DataFrame:
        """
        Extract breach-specific geometry progression and flow data.

        Only available for structures with breach capability. This dataset shows
        how the breach evolves over time (width, depth, flow, etc.).

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure name. If None, returns all breach structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Breach progression data with columns:
            - datetime: Timestamp
            - structure: Structure name (if multiple structures)
            - hw: Headwater stage at breach (ft or m)
            - tw: Tailwater stage at breach (ft or m)
            - bottom_width: Current breach bottom width (ft or m)
            - bottom_elevation: Current breach bottom elevation (ft or m)
            - left_slope: Left side slope (feet/feet or m/m)
            - right_slope: Right side slope (feet/feet or m/m)
            - breach_flow: Flow through breach opening (cfs or m³/s)
            - breach_velocity: Average velocity through breach (ft/s or m/s)
            - breach_flow_area: Flow area of breach (ft² or m²)

        Examples
        --------
        >>> # Get breach progression for specific dam
        >>> df = HdfResultsBreach.get_breaching_variables("02", "Laxton_Dam")

        >>> # Plot breach width evolution
        >>> import matplotlib.pyplot as plt
        >>> plt.plot(df['datetime'], df['bottom_width'])
        >>> plt.ylabel('Breach Width (ft)')

        >>> # Get all breach structures
        >>> df = HdfResultsBreach.get_breaching_variables("02")

        Notes
        -----
        - Returns empty DataFrame if structure has no breach capability
        - NaN values indicate breach not yet formed at that timestep
        - Units depend on project unit system
        - For total structure flow, use get_structure_variables()

        Raises
        ------
        ValueError
            If specified structure_name doesn't exist in HDF
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                sa_conn_path = f"{base_path}/SA 2D Area Conn"

                if sa_conn_path not in hdf_file:
                    logger.warning(f"No SA 2D Area Conn data in {hdf_path.name}")
                    return pd.DataFrame()

                # Get timestamps
                time_stamps = HdfBase.get_unsteady_timestamps(hdf_file)

                # Get structure names with breach capability
                from .HdfStruc import HdfStruc
                breach_info = HdfStruc.get_sa2d_breach_info(hdf_path, ras_object=ras_object)
                available_breach_structures = breach_info[breach_info['has_breach']]['structure'].tolist()

                if not available_breach_structures:
                    logger.warning("No breach structures found in HDF file")
                    return pd.DataFrame()

                # Determine structures to extract
                if structure_name:
                    if structure_name not in available_breach_structures:
                        raise ValueError(f"Structure '{structure_name}' does not have breach capability. "
                                       f"Available breach structures: {available_breach_structures}")
                    structures = [structure_name]
                else:
                    structures = available_breach_structures

                # Extract data for each structure
                data_list = []
                for struct in structures:
                    breach_var_path = f"{sa_conn_path}/{struct}/Breaching Variables"

                    # Extract dataset
                    dataset = hdf_file[breach_var_path][:]  # shape: (n_timesteps, 9)

                    # Get variable names and units from attributes
                    var_unit = hdf_file[breach_var_path].attrs['Variable_Unit']
                    # var_unit[0] = [b'Stage HW', b'ft'], etc.

                    # Create DataFrame for this structure
                    struct_data = pd.DataFrame({
                        'datetime': time_stamps,
                        'hw': dataset[:, 0],
                        'tw': dataset[:, 1],
                        'bottom_width': dataset[:, 2],
                        'bottom_elevation': dataset[:, 3],
                        'left_slope': dataset[:, 4],
                        'right_slope': dataset[:, 5],
                        'breach_flow': dataset[:, 6],
                        'breach_velocity': dataset[:, 7],
                        'breach_flow_area': dataset[:, 8]
                    })

                    if len(structures) > 1:
                        struct_data.insert(1, 'structure', struct)

                    data_list.append(struct_data)

                # Combine all structures
                if data_list:
                    result_df = pd.concat(data_list, ignore_index=True)
                    logger.info(f"Extracted breach variables for {len(structures)} structure(s), "
                              f"{len(time_stamps)} timesteps")
                    return result_df
                else:
                    return pd.DataFrame()

        except Exception as e:
            logger.error(f"Error extracting breaching variables: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_breach_timeseries(hdf_path: Path, structure_name: str = None, *,
                             ras_object=None) -> pd.DataFrame:
        """
        Extract combined breach and structure time series (primary user function).

        This is a convenience function that combines data from both:
        - Structure Variables (total flow, weir flow)
        - Breaching Variables (breach geometry and breach-specific flow)

        Provides a complete picture of dam breach behavior over time.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure name. If None, returns all breach structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Combined time series with columns:
            - datetime: Timestamp
            - structure: Structure name (if multiple structures)
            - total_flow: Total flow through structure (cfs)
            - weir_flow: Flow over remaining weir (cfs)
            - breach_flow: Flow through breach opening (cfs)
            - hw: Headwater elevation (ft)
            - tw: Tailwater elevation (ft)
            - bottom_width: Breach width (ft)
            - bottom_elevation: Breach bottom elevation (ft)
            - left_slope: Left side slope
            - right_slope: Right side slope
            - breach_velocity: Breach velocity (ft/s)
            - breach_flow_area: Breach flow area (ft²)

        Examples
        --------
        >>> # Extract all breach data for plan 02
        >>> df = HdfResultsBreach.get_breach_timeseries("02")

        >>> # Get specific dam
        >>> df = HdfResultsBreach.get_breach_timeseries("02", "Laxton_Dam")

        >>> # Visualize
        >>> import matplotlib.pyplot as plt
        >>> fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        >>>
        >>> # Flow hydrograph
        >>> ax1.plot(df['datetime'], df['total_flow'], label='Total Flow')
        >>> ax1.plot(df['datetime'], df['breach_flow'], label='Breach Flow')
        >>> ax1.set_ylabel('Flow (cfs)')
        >>> ax1.legend()
        >>>
        >>> # Breach width evolution
        >>> ax2.plot(df['datetime'], df['bottom_width'])
        >>> ax2.set_ylabel('Breach Width (ft)')
        >>> ax2.set_xlabel('Time')
        >>> plt.tight_layout()

        Notes
        -----
        - Only returns structures with breach capability
        - For non-breach SA/2D connections, use get_structure_variables()
        - NaN values in breach columns indicate breach not yet formed

        See Also
        --------
        get_structure_variables : Structure-level data only
        get_breaching_variables : Breach-specific data only
        """
        try:
            # Get structure variables (total flow, weir flow, hw, tw)
            struct_df = HdfResultsBreach.get_structure_variables(hdf_path, structure_name, ras_object=ras_object)

            # Get breaching variables (breach geometry and breach flow)
            breach_df = HdfResultsBreach.get_breaching_variables(hdf_path, structure_name, ras_object=ras_object)

            if struct_df.empty:
                logger.warning("No structure data available")
                return pd.DataFrame()

            if breach_df.empty:
                logger.warning("No breach data available, returning structure data only")
                return struct_df

            # Determine merge columns
            merge_cols = ['datetime']
            if 'structure' in struct_df.columns and 'structure' in breach_df.columns:
                merge_cols.append('structure')

            # Merge the two dataframes
            combined_df = pd.merge(
                struct_df,
                breach_df[['datetime', 'structure', 'bottom_width', 'bottom_elevation',
                          'left_slope', 'right_slope', 'breach_flow', 'breach_velocity',
                          'breach_flow_area']] if 'structure' in breach_df.columns
                        else breach_df[['datetime', 'bottom_width', 'bottom_elevation',
                                       'left_slope', 'right_slope', 'breach_flow',
                                       'breach_velocity', 'breach_flow_area']],
                on=merge_cols,
                how='left'  # Keep all structure timesteps, even if no breach data
            )

            # Reorder columns for better user experience
            col_order = ['datetime']
            if 'structure' in combined_df.columns:
                col_order.append('structure')
            col_order.extend(['total_flow', 'weir_flow', 'breach_flow', 'hw', 'tw',
                            'bottom_width', 'bottom_elevation', 'left_slope', 'right_slope',
                            'breach_velocity', 'breach_flow_area'])

            combined_df = combined_df[col_order]

            logger.info(f"Created combined breach timeseries with {len(combined_df)} rows")
            return combined_df

        except Exception as e:
            logger.error(f"Error creating combined breach timeseries: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_breach_summary(hdf_path: Path, structure_name: str = None, *,
                          ras_object=None) -> pd.DataFrame:
        """
        Extract breach summary statistics (peak values, timing, final geometry).

        Provides quick overview of breach performance without full time series.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        structure_name : str, optional
            Specific structure. If None, returns all breach structures.
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            Summary statistics with columns:
            - structure: Structure name
            - breach_initiated: Boolean, True if breach formed
            - breach_at_time: Time of breach initiation (days)
            - breach_at_date: Date/time of breach
            - max_total_flow: Maximum total flow (cfs)
            - max_total_flow_time: Time of max total flow
            - max_breach_flow: Maximum breach flow (cfs)
            - max_breach_flow_time: Time of max breach flow
            - final_breach_width: Final breach width (ft)
            - final_breach_depth: Final breach depth (ft)
            - max_hw: Maximum headwater elevation (ft)
            - max_tw: Maximum tailwater elevation (ft)

        Examples
        --------
        >>> summary = HdfResultsBreach.get_breach_summary("02")
        >>> print(summary[['structure', 'max_total_flow', 'final_breach_width']])

        Notes
        -----
        - Returns summary even if breach didn't fully form (NaN for incomplete data)
        - Times are pandas datetime objects
        - If only 1 timestep available, "max" values are that single value
        """
        try:
            # Get full timeseries
            ts_df = HdfResultsBreach.get_breach_timeseries(hdf_path, structure_name, ras_object=ras_object)

            if ts_df.empty:
                return pd.DataFrame()

            # Get breach info
            from .HdfStruc import HdfStruc
            info_df = HdfStruc.get_sa2d_breach_info(hdf_path, ras_object=ras_object)

            # Determine grouping
            if 'structure' in ts_df.columns:
                structures = ts_df['structure'].unique()
            else:
                # Single structure, create pseudo-structure column
                structures = [structure_name] if structure_name else ['Unknown']
                ts_df['structure'] = structures[0]

            summary_list = []
            for struct in structures:
                struct_ts = ts_df[ts_df['structure'] == struct].copy()
                struct_info = info_df[info_df['structure'] == struct].iloc[0] if len(info_df) > 0 else {}

                # Calculate summary stats
                summary = {
                    'structure': struct,
                    'breach_initiated': struct_info.get('has_breach', False),
                    'breach_at_time': struct_info.get('breach_at_time', None),
                    'breach_at_date': struct_info.get('breach_at_date', None),
                }

                # Max flows and timing
                if 'total_flow' in struct_ts.columns:
                    max_total_idx = struct_ts['total_flow'].idxmax()
                    summary['max_total_flow'] = struct_ts.loc[max_total_idx, 'total_flow']
                    summary['max_total_flow_time'] = struct_ts.loc[max_total_idx, 'datetime']

                if 'breach_flow' in struct_ts.columns:
                    # Filter out NaN values
                    valid_breach = struct_ts[struct_ts['breach_flow'].notna()]
                    if len(valid_breach) > 0:
                        max_breach_idx = valid_breach['breach_flow'].idxmax()
                        summary['max_breach_flow'] = valid_breach.loc[max_breach_idx, 'breach_flow']
                        summary['max_breach_flow_time'] = valid_breach.loc[max_breach_idx, 'datetime']
                    else:
                        summary['max_breach_flow'] = np.nan
                        summary['max_breach_flow_time'] = None

                # Final breach geometry (last non-NaN value)
                if 'bottom_width' in struct_ts.columns:
                    valid_width = struct_ts[struct_ts['bottom_width'].notna()]
                    summary['final_breach_width'] = valid_width['bottom_width'].iloc[-1] if len(valid_width) > 0 else np.nan

                if 'bottom_elevation' in struct_ts.columns:
                    valid_elev = struct_ts[struct_ts['bottom_elevation'].notna()]
                    if len(valid_elev) > 0:
                        final_bottom = valid_elev['bottom_elevation'].iloc[-1]
                        # Calculate depth if we have HW
                        if 'hw' in struct_ts.columns:
                            final_hw = struct_ts['hw'].iloc[-1]
                            summary['final_breach_depth'] = final_hw - final_bottom
                        else:
                            summary['final_breach_depth'] = np.nan
                    else:
                        summary['final_breach_depth'] = np.nan

                # Max water levels
                if 'hw' in struct_ts.columns:
                    summary['max_hw'] = struct_ts['hw'].max()
                if 'tw' in struct_ts.columns:
                    summary['max_tw'] = struct_ts['tw'].max()

                summary_list.append(summary)

            result_df = pd.DataFrame(summary_list)
            logger.info(f"Generated breach summary for {len(structures)} structure(s)")
            return result_df

        except Exception as e:
            logger.error(f"Error generating breach summary: {e}")
            raise

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfResultsMesh.py
==================================================
"""
Class: HdfResultsMesh

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All methods in this class are static and designed to be used without instantiation.

Public Functions:
- get_mesh_summary(): Get summary output data for a variable 
- get_mesh_timeseries(): Get timeseries output for a mesh and variable  
- get_mesh_faces_timeseries(): Get timeseries for all face-based variables
- get_mesh_cells_timeseries(): Get timeseries for mesh cells
- get_mesh_last_iter(): Get last iteration count for cells
- get_mesh_max_ws(): Get maximum water surface elevation at each cell   
- get_mesh_min_ws(): Get minimum water surface elevation at each cell
- get_mesh_max_face_v(): Get maximum face velocity at each face
- get_mesh_min_face_v(): Get minimum face velocity at each face
- get_mesh_max_ws_err(): Get maximum water surface error at each cell
- get_mesh_max_iter(): Get maximum iteration count at each cell

Private Functions:
- _get_mesh_timeseries_output_path(): Get HDF path for timeseries output  #REDUNDANT??
- _get_mesh_cells_timeseries_output(): Internal handler for cell timeseries   #REDUNDANT??
- _get_mesh_timeseries_output(): Internal handler for mesh timeseries       # FACES?? 
- _get_mesh_timeseries_output_values_units(): Get values and units for timeseries
- _get_available_meshes(): Get list of available meshes in HDF            #USE HDFBASE OR HDFUTIL
- get_mesh_summary_output(): Internal handler for summary output        
- get_mesh_summary_output_group(): Get HDF group for summary output         #REDUNDANT??  Include in Above

The class works with HEC-RAS version 6.0+ plan HDF files and uses HdfBase and 
HdfUtils for common operations. Methods use @log_call decorator for logging and 
@standardize_input decorator to handle different input types.









"""

import numpy as np
import pandas as pd
import xarray as xr
from pathlib import Path
import h5py
from typing import Union, List, Optional, Dict, Any, Tuple
from .HdfMesh import HdfMesh
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..Decorators import log_call, standardize_input
from ..LoggingConfig import setup_logging, get_logger
import geopandas as gpd

logger = get_logger(__name__)

class HdfResultsMesh:
    """
    Handles mesh-related results from HEC-RAS HDF files.

    Provides methods to extract and analyze:
    - Mesh summary outputs
    - Timeseries data
    - Water surface elevations
    - Velocities
    - Error metrics

    Works with HEC-RAS 6.0+ plan HDF files.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary(hdf_path: Path, var: str, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, var, round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_summary: {str(e)}")
            logger.error(f"Variable: {var}")
            raise ValueError(f"Failed to get summary output: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_timeseries(hdf_path: Path, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        with h5py.File(hdf_path, 'r') as hdf_path:
            return HdfResultsMesh._get_mesh_timeseries_output(hdf_path, mesh_name, var, truncate)

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_faces_timeseries(hdf_path: Path, mesh_name: str) -> xr.Dataset:
        """
        Get timeseries output for all face-based variables of a specific mesh.

        Args:
            hdf_path (Path): Path to the HDF file.
            mesh_name (str): Name of the mesh.

        Returns:
            xr.Dataset: Dataset containing the timeseries output for all face-based variables.
        """
        face_vars = ["Face Velocity", "Face Flow"]
        datasets = []
        
        for var in face_vars:
            try:
                da = HdfResultsMesh.get_mesh_timeseries(hdf_path, mesh_name, var)
                # Assign the variable name as the DataArray name
                da.name = var.lower().replace(' ', '_')
                datasets.append(da)
            except Exception as e:
                logger.warning(f"Failed to process {var} for mesh {mesh_name}: {str(e)}")
        
        if not datasets:
            logger.error(f"No valid data found for mesh {mesh_name}")
            return xr.Dataset()
        
        try:
            return xr.merge(datasets)
        except Exception as e:
            logger.error(f"Failed to merge datasets: {str(e)}")
            return xr.Dataset()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cells_timeseries(hdf_path: Path, mesh_names: Optional[Union[str, List[str]]] = None, var: Optional[str] = None, truncate: bool = False, ras_object: Optional[Any] = None) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output.

        Args:
            hdf_path (Path): Path to HDF file
            mesh_names (str|List[str], optional): Mesh name(s). If None, processes all meshes
            var (str, optional): Variable name. If None, retrieves all variables
            truncate (bool): Remove trailing zeros if True
            ras_object (Any, optional): RAS object if available

        Returns:
            Dict[str, xr.Dataset]: Dictionary mapping mesh names to datasets containing:
                - Time-indexed variables
                - Cell/face IDs
                - Variable metadata
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_path:
                return HdfResultsMesh._get_mesh_cells_timeseries_output(hdf_path, mesh_names, var, truncate)
        except Exception as e:
            logger.error(f"Error in get_mesh_cells_timeseries: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_last_iter(hdf_file: Path) -> pd.DataFrame:
        """
        Get last iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing last iteration counts.
        """
        return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Last Iteration")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get maximum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing maximum water surface elevations with geometry.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface: {str(e)}")
        




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get minimum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing minimum water surface elevations with geometry.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_ws: {str(e)}")
            raise ValueError(f"Failed to get minimum water surface: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum face velocity for each mesh face.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum face velocities.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_face_v: {str(e)}")
            raise ValueError(f"Failed to get maximum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get minimum face velocity for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing minimum face velocities.

        Raises:
            ValueError: If there's an error processing the minimum face velocity data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_face_v: {str(e)}")
            raise ValueError(f"Failed to get minimum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws_err(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum water surface error for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum water surface errors.

        Raises:
            ValueError: If there's an error processing the maximum water surface error data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Maximum Water Surface Error", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws_err: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface error: {str(e)}")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_iter(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get maximum iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing maximum iteration counts with geometry.
                Includes columns:
                - mesh_name: Name of the mesh
                - cell_id: ID of the cell
                - cell_last_iteration: Maximum number of iterations
                - cell_last_iteration_time: Time when max iterations occurred
                - geometry: Point geometry representing cell center
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Last Iteration", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_iter: {str(e)}")
            raise ValueError(f"Failed to get maximum iteration count: {str(e)}")
        
        


    @staticmethod
    def _get_mesh_timeseries_output_path(mesh_name: str, var_name: str) -> str:
        """
        Get the HDF path for mesh timeseries output.

        Args:
            mesh_name (str): Name of the mesh.
            var_name (str): Name of the variable.

        Returns:
            str: The HDF path for the specified mesh and variable.
        """
        return f"Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas/{mesh_name}/{var_name}"


    @staticmethod
    def _get_mesh_cells_timeseries_output(hdf_path: h5py.File, 
                                         mesh_names: Optional[Union[str, List[str]]] = None,
                                         var: Optional[str] = None, 
                                         truncate: bool = False) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output for specified meshes and variables.
        
        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_names (Optional[Union[str, List[str]]]): Name(s) of the mesh(es). If None, processes all available meshes.
            var (Optional[str]): Name of the variable to retrieve. If None, retrieves all variables.
            truncate (bool): If True, truncates the output to remove trailing zeros.

        Returns:
            Dict[str, xr.Dataset]: A dictionary of xarray Datasets, one for each mesh, containing the mesh cells timeseries output.

        Raises:
            ValueError: If there's an error processing the timeseries output data.
        """
        TIME_SERIES_OUTPUT_VARS = {
            "cell": [
                "Water Surface", "Depth", "Velocity", "Velocity X", "Velocity Y",
                "Froude Number", "Courant Number", "Shear Stress", "Bed Elevation",
                "Precipitation Rate", "Infiltration Rate", "Evaporation Rate",
                "Percolation Rate", "Groundwater Elevation", "Groundwater Depth",
                "Groundwater Flow", "Groundwater Velocity", "Groundwater Velocity X",
                "Groundwater Velocity Y"
            ],
            "face": [
                "Face Velocity", "Face Flow", "Face Water Surface", "Face Courant",
                "Face Cumulative Volume", "Face Eddy Viscosity", "Face Flow Period Average",
                "Face Friction Term", "Face Pressure Gradient Term", "Face Shear Stress",
                "Face Tangential Velocity"
            ]
        }

        try:
            start_time = HdfBase.get_simulation_start_time(hdf_path)
            time_stamps = HdfBase.get_unsteady_timestamps(hdf_path)

            if mesh_names is None:
                mesh_names = HdfResultsMesh._get_available_meshes(hdf_path)
            elif isinstance(mesh_names, str):
                mesh_names = [mesh_names]

            if var:
                variables = [var]
            else:
                variables = TIME_SERIES_OUTPUT_VARS["cell"] + TIME_SERIES_OUTPUT_VARS["face"]

            datasets = {}
            for mesh_name in mesh_names:
                data_vars = {}
                for variable in variables:
                    try:
                        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, variable)
                        dataset = hdf_path[path]
                        values = dataset[:]
                        units = dataset.attrs.get("Units", "").decode("utf-8")

                        if truncate:
                            last_nonzero = np.max(np.nonzero(values)[1]) + 1 if values.size > 0 else 0
                            values = values[:, :last_nonzero]
                            truncated_time_stamps = time_stamps[:last_nonzero]
                        else:
                            truncated_time_stamps = time_stamps

                        if values.shape[0] != len(truncated_time_stamps):
                            logger.warning(f"Mismatch between time steps ({len(truncated_time_stamps)}) and data shape ({values.shape}) for variable {variable}")
                            continue

                        # Determine if this is a face-based or cell-based variable
                        id_dim = "face_id" if any(face_var in variable for face_var in TIME_SERIES_OUTPUT_VARS["face"]) else "cell_id"

                        data_vars[variable] = xr.DataArray(
                            data=values,
                            dims=['time', id_dim],
                            coords={'time': truncated_time_stamps, id_dim: np.arange(values.shape[1])},
                            attrs={'units': units}
                        )
                    except KeyError:
                        logger.warning(f"Variable '{variable}' not found in the HDF file for mesh '{mesh_name}'. Skipping.")
                    except Exception as e:
                        logger.error(f"Error processing variable '{variable}' for mesh '{mesh_name}': {str(e)}")

                if data_vars:
                    datasets[mesh_name] = xr.Dataset(
                        data_vars=data_vars,
                        attrs={'mesh_name': mesh_name, 'start_time': start_time}
                    )
                else:
                    logger.warning(f"No valid data variables found for mesh '{mesh_name}'")

            return datasets
        except Exception as e:
            logger.error(f"Error in _mesh_cells_timeseries_output: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")



    @staticmethod
    def _get_mesh_timeseries_output(hdf_path: h5py.File, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.
            truncate (bool): Whether to truncate the output to remove trailing zeros (default True).

        Returns:
            xr.DataArray: DataArray containing the timeseries output.

        Raises:
            ValueError: If the specified path is not found in the HDF file or if there's an error processing the data.
        """
        try:
            path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
            
            if path not in hdf_path:
                raise ValueError(f"Path {path} not found in HDF file")

            dataset = hdf_path[path]
            values = dataset[:]
            units = dataset.attrs.get("Units", "").decode("utf-8")
            
            # Get start time and timesteps
            start_time = HdfBase.get_simulation_start_time(hdf_path)
            # Updated to use the new function name from HdfUtils
            timesteps = HdfUtils.convert_timesteps_to_datetimes(
                np.array(hdf_path["Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time"][:]),
                start_time
            )

            if truncate:
                non_zero = np.nonzero(values)[0]
                if len(non_zero) > 0:
                    start, end = non_zero[0], non_zero[-1] + 1
                    values = values[start:end]
                    timesteps = timesteps[start:end]

            # Determine if this is a face-based or cell-based variable
            id_dim = "face_id" if "Face" in var else "cell_id"
            dims = ["time", id_dim] if values.ndim == 2 else ["time"]
            coords = {"time": timesteps}
            if values.ndim == 2:
                coords[id_dim] = np.arange(values.shape[1])

            return xr.DataArray(
                values,
                coords=coords,
                dims=dims,
                attrs={"units": units, "mesh_name": mesh_name, "variable": var},
            )
        except Exception as e:
            logger.error(f"Error in get_mesh_timeseries_output: {str(e)}")
            raise ValueError(f"Failed to get timeseries output: {str(e)}")


    @staticmethod
    def _get_mesh_timeseries_output_values_units(hdf_path: h5py.File, mesh_name: str, var: str) -> Tuple[np.ndarray, str]:
        """
        Get the mesh timeseries output values and units for a specific variable from the HDF file.

        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.

        Returns:
            Tuple[np.ndarray, str]: A tuple containing the output values and units.
        """
        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
        group = hdf_path[path]
        values = group[:]
        units = group.attrs.get("Units")
        if units is not None:
            units = units.decode("utf-8")
        return values, units


    @staticmethod
    def _get_available_meshes(hdf_path: h5py.File) -> List[str]:
        """
        Get the names of all available meshes in the HDF file.

        Args:
            hdf_path (h5py.File): Open HDF file object.

        Returns:
            List[str]: A list of mesh names.
        """
        return HdfMesh.get_mesh_area_names(hdf_path)
    
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary_output(hdf_file: h5py.File, var: str, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get the summary output data for a given variable from the HDF file.

        Parameters
        ----------
        hdf_path : h5py.File
            Open HDF file object.
        var : str
            The summary output variable to retrieve.
        round_to : str, optional
            The time unit to round the datetimes to. Default is "100ms".

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the summary output data with decoded attributes as metadata.
            Returns empty GeoDataFrame if variable is not found.
        """
        try:
            dfs = []
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            
            logger.info(f"Processing summary output for variable: {var}")
            d2_flow_areas = hdf_file.get("Geometry/2D Flow Areas/Attributes")
            if d2_flow_areas is None:
                logger.info("No 2D Flow Areas found in HDF file")
                return gpd.GeoDataFrame()

            for d2_flow_area in d2_flow_areas[:]:
                mesh_name = HdfUtils.convert_ras_string(d2_flow_area[0])
                cell_count = d2_flow_area[-1]
                logger.debug(f"Processing mesh: {mesh_name} with {cell_count} cells")
                
                try:
                    group = HdfResultsMesh.get_mesh_summary_output_group(hdf_file, mesh_name, var)
                except ValueError:
                    logger.info(f"Variable '{var}' not present in output file for mesh '{mesh_name}', skipping")
                    continue
                
                data = group[:]
                logger.debug(f"Data shape for {var} in {mesh_name}: {data.shape}")
                logger.debug(f"Data type: {data.dtype}")
                logger.debug(f"Attributes: {dict(group.attrs)}")
                
                if data.ndim == 2 and data.shape[0] == 2:
                    # Handle 2D datasets (e.g. Maximum Water Surface)
                    row_variables = group.attrs.get('Row Variables', [b'Value', b'Time'])
                    row_variables = [v.decode('utf-8').strip() if isinstance(v, bytes) else v for v in row_variables]
                    
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * data.shape[1],
                        "cell_id" if "Face" not in var else "face_id": range(data.shape[1]),
                        f"{var.lower().replace(' ', '_')}": data[0, :],
                        f"{var.lower().replace(' ', '_')}_time": HdfUtils.convert_timesteps_to_datetimes(
                            data[1, :], start_time, time_unit="days", round_to=round_to
                        )
                    })
                    
                elif data.ndim == 1:
                    # Handle 1D datasets (e.g. Cell Last Iteration)
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * len(data),
                        "cell_id" if "Face" not in var else "face_id": range(len(data)),
                        var.lower().replace(' ', '_'): data
                    })
                    
                else:
                    raise ValueError(f"Unexpected data shape for {var} in {mesh_name}. "
                                  f"Got shape {data.shape}")
                
                # Add geometry based on variable type
                if "Face" in var:
                    face_df = HdfMesh.get_mesh_cell_faces(hdf_file)
                    if not face_df.empty:
                        df = df.merge(face_df[['mesh_name', 'face_id', 'geometry']], 
                                    on=['mesh_name', 'face_id'], 
                                    how='left')
                else:
                    cell_df = HdfMesh.get_mesh_cell_points(hdf_file)
                    if not cell_df.empty:
                        df = df.merge(cell_df[['mesh_name', 'cell_id', 'geometry']], 
                                    on=['mesh_name', 'cell_id'], 
                                    how='left')
                
                # Add group attributes as metadata with proper decoding
                df.attrs['mesh_name'] = mesh_name
                for attr_name, attr_value in group.attrs.items():
                    if isinstance(attr_value, bytes):
                        # Decode single byte string
                        decoded_value = attr_value.decode('utf-8')
                    elif isinstance(attr_value, np.ndarray):
                        if attr_value.dtype.kind in {'S', 'a'}:  # Array of byte strings
                            # Decode array of byte strings
                            decoded_value = [v.decode('utf-8') if isinstance(v, bytes) else v for v in attr_value]
                        else:
                            # Convert other numpy arrays to list
                            decoded_value = attr_value.tolist()
                    else:
                        decoded_value = attr_value
                    df.attrs[attr_name] = decoded_value
                
                dfs.append(df)
            
            if not dfs:
                return gpd.GeoDataFrame()
                
            result = pd.concat(dfs, ignore_index=True)
            
            # Convert to GeoDataFrame
            gdf = gpd.GeoDataFrame(result, geometry='geometry')
            
            # Get CRS from HdfUtils
            crs = HdfBase.get_projection(hdf_file)
            if crs:
                gdf.set_crs(crs, inplace=True)
            
            # Combine attributes from all meshes with decoded values
            combined_attrs = {}
            for df in dfs:
                for key, value in df.attrs.items():
                    if key not in combined_attrs:
                        combined_attrs[key] = value
                    elif combined_attrs[key] != value:
                        combined_attrs[key] = f"Multiple values: {combined_attrs[key]}, {value}"
            
            gdf.attrs.update(combined_attrs)
            
            logger.info(f"Processed {len(gdf)} rows of summary output data")
            return gdf
        
        except Exception as e:
            logger.error(f"Error processing summary output data: {e}")
            raise ValueError(f"Error processing summary output data: {e}")

    @staticmethod
    def get_mesh_summary_output_group(hdf_file: h5py.File, mesh_name: str, var: str) -> Union[h5py.Group, h5py.Dataset]:
        """
        Return the HDF group for a given mesh and summary output variable.

        Args:
            hdf_path (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Name of the summary output variable.

        Returns:
            Union[h5py.Group, h5py.Dataset]: The HDF group or dataset for the specified mesh and variable.

        Raises:
            ValueError: If the specified group or dataset is not found in the HDF file.
        """
        output_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/{var}"
        output_item = hdf_file.get(output_path)
        if output_item is None:
            raise ValueError(f"Dataset not found at path '{output_path}'")
        return output_item

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_boundary_conditions_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Get timeseries output for all boundary conditions as a single combined xarray Dataset.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            xr.Dataset: Dataset containing all boundary condition data with:
                - Dimensions: time, bc_name (boundary condition name), face_id
                - Variables: stage, flow, flow_per_face, stage_per_face
                - Coordinates and attributes preserving original metadata

        Example:
            >>> bc_data = HdfResultsMesh.get_boundary_conditions_timeseries_combined(hdf_path)
            >>> print(bc_data)
            >>> # Plot flow for all boundary conditions
            >>> bc_data.flow.plot(x='time', hue='bc_name')
            >>> # Extract data for a specific boundary condition
            >>> upstream_data = bc_data.sel(bc_name='Upstream Inflow')
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Get the base path and check if boundary conditions exist
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                bc_base_path = f"{base_path}/Boundary Conditions"
                
                if bc_base_path not in hdf_file:
                    logger.warning(f"No boundary conditions found in HDF file")
                    return xr.Dataset()
                
                # Get timestamps
                start_time = HdfBase.get_simulation_start_time(hdf_file)
                time_data = hdf_file[f"{base_path}/Time"][:]
                timestamps = HdfUtils.convert_timesteps_to_datetimes(time_data, start_time)
                
                # Get all boundary condition names (excluding those with " - Flow per Face" or " - Stage per Face" suffix)
                bc_names = [name for name in hdf_file[bc_base_path].keys() 
                        if " - Flow per Face" not in name and " - Stage per Face" not in name]
                
                if not bc_names:
                    logger.warning(f"No boundary conditions found in HDF file")
                    return xr.Dataset()
                
                # Initialize arrays for main stage and flow data
                num_timesteps = len(timestamps)
                num_bcs = len(bc_names)
                
                stage_data = np.full((num_timesteps, num_bcs), np.nan)
                flow_data = np.full((num_timesteps, num_bcs), np.nan)
                
                # Dictionary to store face-specific data
                face_data = {
                    'flow_per_face': {},
                    'stage_per_face': {}
                }
                
                # Extract metadata from all boundary conditions
                bc_metadata = {}
                
                # Process each boundary condition
                for bc_idx, bc_name in enumerate(bc_names):
                    bc_path = f"{bc_base_path}/{bc_name}"
                    
                    try:
                        # Extract main boundary data
                        bc_data = hdf_file[bc_path][:]
                        bc_attrs = dict(hdf_file[bc_path].attrs)
                        
                        # Store metadata
                        bc_metadata[bc_name] = {
                            k: v.decode('utf-8') if isinstance(v, bytes) else v 
                            for k, v in bc_attrs.items()
                        }
                        
                        # Get column indices for Stage and Flow
                        if 'Columns' in bc_attrs:
                            columns = [col.decode('utf-8') if isinstance(col, bytes) else col 
                                    for col in bc_attrs['Columns']]
                            
                            stage_idx = columns.index('Stage') if 'Stage' in columns else None
                            flow_idx = columns.index('Flow') if 'Flow' in columns else None
                            
                            if stage_idx is not None:
                                stage_data[:, bc_idx] = bc_data[:, stage_idx]
                            if flow_idx is not None:
                                flow_data[:, bc_idx] = bc_data[:, flow_idx]
                        
                        # Extract Flow per Face data
                        flow_face_path = f"{bc_path} - Flow per Face"
                        if flow_face_path in hdf_file:
                            flow_face_data = hdf_file[flow_face_path][:]
                            flow_face_attrs = dict(hdf_file[flow_face_path].attrs)
                            
                            # Get face IDs
                            face_ids = flow_face_attrs.get('Faces', [])
                            if isinstance(face_ids, np.ndarray):
                                face_ids = face_ids.tolist()
                            else:
                                face_ids = list(range(flow_face_data.shape[1]))
                            
                            face_data['flow_per_face'][bc_name] = {
                                'data': flow_face_data,
                                'faces': face_ids,
                                'attrs': {
                                    k: v.decode('utf-8') if isinstance(v, bytes) else v 
                                    for k, v in flow_face_attrs.items()
                                }
                            }
                        
                        # Extract Stage per Face data
                        stage_face_path = f"{bc_path} - Stage per Face"
                        if stage_face_path in hdf_file:
                            stage_face_data = hdf_file[stage_face_path][:]
                            stage_face_attrs = dict(hdf_file[stage_face_path].attrs)
                            
                            # Get face IDs
                            face_ids = stage_face_attrs.get('Faces', [])
                            if isinstance(face_ids, np.ndarray):
                                face_ids = face_ids.tolist()
                            else:
                                face_ids = list(range(stage_face_data.shape[1]))
                            
                            face_data['stage_per_face'][bc_name] = {
                                'data': stage_face_data,
                                'faces': face_ids,
                                'attrs': {
                                    k: v.decode('utf-8') if isinstance(v, bytes) else v 
                                    for k, v in stage_face_attrs.items()
                                }
                            }
                    
                    except Exception as e:
                        logger.warning(f"Error processing boundary condition '{bc_name}': {str(e)}")
                        continue
                
                # Create base dataset with stage and flow data
                ds = xr.Dataset(
                    data_vars={
                        'stage': xr.DataArray(
                            stage_data,
                            dims=['time', 'bc_name'],
                            coords={
                                'time': timestamps,
                                'bc_name': bc_names
                            },
                            attrs={'description': 'Water surface elevation at boundary condition'}
                        ),
                        'flow': xr.DataArray(
                            flow_data,
                            dims=['time', 'bc_name'],
                            coords={
                                'time': timestamps,
                                'bc_name': bc_names
                            },
                            attrs={'description': 'Flow at boundary condition'}
                        )
                    },
                    attrs={
                        'source': 'HEC-RAS HDF Boundary Conditions',
                        'start_time': start_time
                    }
                )
                
                # Add metadata as coordinates
                for key in bc_metadata[bc_names[0]]:
                    if key != 'Columns':  # Skip Columns attribute as it's used for Stage/Flow
                        try:
                            values = [bc_metadata[bc].get(key, '') for bc in bc_names]
                            ds = ds.assign_coords({f'{key.lower()}': ('bc_name', values)})
                        except Exception as e:
                            logger.debug(f"Could not add metadata coordinate '{key}': {str(e)}")
                
                # Add face-specific data variables if available
                if face_data['flow_per_face']:
                    # First determine the maximum number of faces across all BCs
                    all_flow_faces = set()
                    for bc_name in face_data['flow_per_face']:
                        all_flow_faces.update(face_data['flow_per_face'][bc_name]['faces'])
                    
                    # Create a merged array with NaN values for missing faces
                    all_flow_faces = sorted(list(all_flow_faces))
                    flow_face_data = np.full((num_timesteps, num_bcs, len(all_flow_faces)), np.nan)
                    
                    # Fill in the data where available
                    for bc_idx, bc_name in enumerate(bc_names):
                        if bc_name in face_data['flow_per_face']:
                            bc_faces = face_data['flow_per_face'][bc_name]['faces']
                            bc_data = face_data['flow_per_face'][bc_name]['data']
                            
                            for face_idx, face_id in enumerate(bc_faces):
                                if face_id in all_flow_faces:
                                    target_idx = all_flow_faces.index(face_id)
                                    flow_face_data[:, bc_idx, target_idx] = bc_data[:, face_idx]
                    
                    # Add to the dataset
                    ds['flow_per_face'] = xr.DataArray(
                        flow_face_data,
                        dims=['time', 'bc_name', 'face_id'],
                        coords={
                            'time': timestamps,
                            'bc_name': bc_names,
                            'face_id': all_flow_faces
                        },
                        attrs={'description': 'Flow per face at boundary condition'}
                    )
                
                # Similar approach for stage per face
                if face_data['stage_per_face']:
                    all_stage_faces = set()
                    for bc_name in face_data['stage_per_face']:
                        all_stage_faces.update(face_data['stage_per_face'][bc_name]['faces'])
                    
                    all_stage_faces = sorted(list(all_stage_faces))
                    stage_face_data = np.full((num_timesteps, num_bcs, len(all_stage_faces)), np.nan)
                    
                    for bc_idx, bc_name in enumerate(bc_names):
                        if bc_name in face_data['stage_per_face']:
                            bc_faces = face_data['stage_per_face'][bc_name]['faces']
                            bc_data = face_data['stage_per_face'][bc_name]['data']
                            
                            for face_idx, face_id in enumerate(bc_faces):
                                if face_id in all_stage_faces:
                                    target_idx = all_stage_faces.index(face_id)
                                    stage_face_data[:, bc_idx, target_idx] = bc_data[:, face_idx]
                    
                    ds['stage_per_face'] = xr.DataArray(
                        stage_face_data,
                        dims=['time', 'bc_name', 'face_id'],
                        coords={
                            'time': timestamps,
                            'bc_name': bc_names,
                            'face_id': all_stage_faces
                        },
                        attrs={'description': 'Water surface elevation per face at boundary condition'}
                    )
                
                return ds
                
        except Exception as e:
            logger.error(f"Error getting all boundary conditions timeseries: {str(e)}")
            return xr.Dataset()
==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfResultsPlan.py
==================================================
"""
HdfResultsPlan: A module for extracting and analyzing HEC-RAS plan HDF file results.

Attribution:
    Substantial code sourced/derived from https://github.com/fema-ffrd/rashdf
    Copyright (c) 2024 fema-ffrd, MIT license

Description:
    Provides static methods for extracting both unsteady and steady flow results,
    volume accounting, and reference data from HEC-RAS plan HDF files.

Available Functions:
    Unsteady Flow:
        - get_unsteady_info: Extract unsteady attributes
        - get_unsteady_summary: Extract unsteady summary data
        - get_volume_accounting: Extract volume accounting data
        - get_runtime_data: Extract runtime and compute time data
        - get_reference_timeseries: Extract reference line/point timeseries
        - get_reference_summary: Extract reference line/point summary

    Steady Flow:
        - is_steady_plan: Check if HDF contains steady state results
        - get_steady_profile_names: Extract steady state profile names
        - get_steady_wse: Extract WSE data for steady state profiles
        - get_steady_info: Extract steady flow attributes and metadata

    Computation Messages:
        - get_compute_messages: Extract computation messages from HDF (with .txt fallback)

Note:
    All methods are static and designed to be used without class instantiation.
"""

from typing import Dict, List, Union, Optional
from pathlib import Path
import h5py
import pandas as pd
import xarray as xr
from ..Decorators import standardize_input, log_call
from .HdfUtils import HdfUtils
from .HdfResultsXsec import HdfResultsXsec
from ..LoggingConfig import get_logger
import numpy as np
from datetime import datetime
from ..RasPrj import ras

logger = get_logger(__name__)


class HdfResultsPlan:
    """
    Handles extraction of results data from HEC-RAS plan HDF files.

    This class provides static methods for accessing and analyzing:
        - Unsteady flow results
        - Volume accounting data
        - Runtime statistics
        - Reference line/point time series outputs

    All methods use:
        - @standardize_input decorator for consistent file path handling
        - @log_call decorator for operation logging
        - HdfUtils class for common HDF operations

    Note:
        No instantiation required - all methods are static.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_info(hdf_path: Path) -> pd.DataFrame:
        """
        Get unsteady attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: A DataFrame containing the decoded unsteady attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady" group is not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady" not in hdf_file:
                    raise KeyError("Results/Unsteady group not found in the HDF file.")
                
                # Create dictionary from attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Results/Unsteady"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Get results unsteady summary attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: A DataFrame containing the decoded results unsteady summary attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady/Summary" group is not found in the HDF file.
        """
        try:           
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary" not in hdf_file:
                    raise KeyError("Results/Unsteady/Summary group not found in the HDF file.")
                
                # Create dictionary from attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Results/Unsteady/Summary"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady summary attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_volume_accounting(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Get volume accounting attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[pd.DataFrame]: DataFrame containing the decoded volume accounting attributes,
                                  or None if the group is not found.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary/Volume Accounting" not in hdf_file:
                    return None
                
                # Get attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Results/Unsteady/Summary/Volume Accounting"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading volume accounting attributes: {str(e)}")

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_runtime_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Extract detailed runtime and computational performance metrics from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[pd.DataFrame]: DataFrame containing runtime statistics or None if data cannot be extracted

        Notes:
            - Times are reported in multiple units (ms, s, hours)
            - Compute speeds are calculated as simulation-time/compute-time ratios
            - Process times include: geometry, preprocessing, event conditions, 
              and unsteady flow computations
        """
        try:
            if hdf_path is None:
                logger.error(f"Could not find HDF file for input")
                return None

            with h5py.File(hdf_path, 'r') as hdf_file:
                logger.info(f"Extracting Plan Information from: {Path(hdf_file.filename).name}")
                plan_info = hdf_file.get('/Plan Data/Plan Information')
                if plan_info is None:
                    logger.warning("Group '/Plan Data/Plan Information' not found.")
                    return None

                # Extract plan information
                plan_name = HdfUtils.convert_ras_string(plan_info.attrs.get('Plan Name', 'Unknown'))
                start_time_str = HdfUtils.convert_ras_string(plan_info.attrs.get('Simulation Start Time', 'Unknown'))
                end_time_str = HdfUtils.convert_ras_string(plan_info.attrs.get('Simulation End Time', 'Unknown'))

                try:
                    # Check if times are already datetime objects
                    if isinstance(start_time_str, datetime):
                        start_time = start_time_str
                    else:
                        start_time = datetime.strptime(start_time_str, "%d%b%Y %H:%M:%S")
                        
                    if isinstance(end_time_str, datetime):
                        end_time = end_time_str
                    else:
                        end_time = datetime.strptime(end_time_str, "%d%b%Y %H:%M:%S")
                        
                    simulation_duration = end_time - start_time
                    simulation_hours = simulation_duration.total_seconds() / 3600
                except ValueError as e:
                    logger.error(f"Error parsing simulation times: {e}")
                    return None

                logger.info(f"Plan Name: {plan_name}")
                logger.info(f"Simulation Duration (hours): {simulation_hours}")

                # Extract compute processes data
                compute_processes = hdf_file.get('/Results/Summary/Compute Processes')
                if compute_processes is None:
                    logger.warning("Dataset '/Results/Summary/Compute Processes' not found.")
                    return None

                # Process compute times
                process_names = [HdfUtils.convert_ras_string(name) for name in compute_processes['Process'][:]]
                filenames = [HdfUtils.convert_ras_string(filename) for filename in compute_processes['Filename'][:]]
                completion_times = compute_processes['Compute Time (ms)'][:]

                compute_processes_df = pd.DataFrame({
                    'Process': process_names,
                    'Filename': filenames,
                    'Compute Time (ms)': completion_times,
                    'Compute Time (s)': completion_times / 1000,
                    'Compute Time (hours)': completion_times / (1000 * 3600)
                })

                # Create summary DataFrame
                compute_processes_summary = {
                    'Plan Name': [plan_name],
                    'File Name': [Path(hdf_file.filename).name],
                    'Simulation Start Time': [start_time_str],
                    'Simulation End Time': [end_time_str],
                    'Simulation Duration (s)': [simulation_duration.total_seconds()],
                    'Simulation Time (hr)': [simulation_hours]
                }

                # Add process-specific times
                process_types = {
                    'Completing Geometry': 'Completing Geometry (hr)',
                    'Preprocessing Geometry': 'Preprocessing Geometry (hr)',
                    'Completing Event Conditions': 'Completing Event Conditions (hr)',
                    'Unsteady Flow Computations': 'Unsteady Flow Computations (hr)'
                }

                for process, column in process_types.items():
                    time_value = compute_processes_df[
                        compute_processes_df['Process'] == process
                    ]['Compute Time (hours)'].values[0] if process in process_names else 'N/A'
                    compute_processes_summary[column] = [time_value]

                # Add total process time
                total_time = compute_processes_df['Compute Time (hours)'].sum()
                compute_processes_summary['Complete Process (hr)'] = [total_time]

                # Calculate speeds
                if compute_processes_summary['Unsteady Flow Computations (hr)'][0] != 'N/A':
                    compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = [
                        simulation_hours / compute_processes_summary['Unsteady Flow Computations (hr)'][0]
                    ]
                else:
                    compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = ['N/A']

                compute_processes_summary['Complete Process Speed (hr/hr)'] = [
                    simulation_hours / total_time
                ]

                return pd.DataFrame(compute_processes_summary)

        except Exception as e:
            logger.error(f"Error in get_runtime_data: {str(e)}")
            return None

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_reference_timeseries(hdf_path: Path, reftype: str) -> pd.DataFrame:
        """
        Get reference line or point timeseries output from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            reftype (str): Type of reference data ('lines' or 'points')
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing reference timeseries data
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                ref_path = f"{base_path}/Reference {reftype.capitalize()}"
                
                if ref_path not in hdf_file:
                    logger.warning(f"Reference {reftype} data not found in HDF file")
                    return pd.DataFrame()

                ref_group = hdf_file[ref_path]
                time_data = hdf_file[f"{base_path}/Time"][:]
                
                dfs = []
                for ref_name in ref_group.keys():
                    ref_data = ref_group[ref_name][:]
                    df = pd.DataFrame(ref_data, columns=[ref_name])
                    df['Time'] = time_data
                    dfs.append(df)

                if not dfs:
                    return pd.DataFrame()

                return pd.concat(dfs, axis=1)

        except Exception as e:
            logger.error(f"Error reading reference {reftype} timeseries: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_reference_summary(hdf_path: Path, reftype: str) -> pd.DataFrame:
        """
        Get reference line or point summary output from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            reftype (str): Type of reference data ('lines' or 'points')
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing reference summary data
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Summary Output"
                ref_path = f"{base_path}/Reference {reftype.capitalize()}"
                
                if ref_path not in hdf_file:
                    logger.warning(f"Reference {reftype} summary data not found in HDF file")
                    return pd.DataFrame()

                ref_group = hdf_file[ref_path]
                dfs = []
                
                for ref_name in ref_group.keys():
                    ref_data = ref_group[ref_name][:]
                    if ref_data.ndim == 2:
                        df = pd.DataFrame(ref_data.T, columns=['Value', 'Time'])
                    else:
                        df = pd.DataFrame({'Value': ref_data})
                    df['Reference'] = ref_name
                    dfs.append(df)

                if not dfs:
                    return pd.DataFrame()

                return pd.concat(dfs, ignore_index=True)

        except Exception as e:
            logger.error(f"Error reading reference {reftype} summary: {str(e)}")
            return pd.DataFrame()

    # ==================== STEADY STATE METHODS ====================

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def is_steady_plan(hdf_path: Path) -> bool:
        """
        Check if HDF file contains steady state results.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            bool: True if the HDF contains steady state results, False otherwise

        Notes:
            - Checks for existence of Results/Steady group
            - Does not guarantee results are complete or valid
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return "Results/Steady" in hdf_file
        except Exception as e:
            logger.error(f"Error checking if plan is steady: {str(e)}")
            return False

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_steady_profile_names(hdf_path: Path) -> List[str]:
        """
        Extract profile names from steady state results.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            List[str]: List of profile names (e.g., ['50Pct', '10Pct', '1Pct'])

        Raises:
            FileNotFoundError: If the specified HDF file is not found
            KeyError: If steady state results or profile names are not found
            ValueError: If the plan is not a steady state plan

        Example:
            >>> from ras_commander import HdfResultsPlan, init_ras_project
            >>> init_ras_project(Path('/path/to/project'), '6.6')
            >>> profiles = HdfResultsPlan.get_steady_profile_names('01')
            >>> print(profiles)
            ['50Pct', '20Pct', '10Pct', '4Pct', '2Pct', '1Pct', '0.2Pct']
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Check if this is a steady state plan
                if "Results/Steady" not in hdf_file:
                    raise ValueError(f"HDF file does not contain steady state results: {hdf_path.name}")

                # Path to profile names
                profile_names_path = "Results/Steady/Output/Output Blocks/Base Output/Steady Profiles/Profile Names"

                if profile_names_path not in hdf_file:
                    raise KeyError(f"Profile names not found at: {profile_names_path}")

                # Read profile names dataset
                profile_names_ds = hdf_file[profile_names_path]
                profile_names_raw = profile_names_ds[()]

                # Decode byte strings to regular strings
                profile_names = []
                for name in profile_names_raw:
                    if isinstance(name, bytes):
                        profile_names.append(name.decode('utf-8').strip())
                    else:
                        profile_names.append(str(name).strip())

                logger.info(f"Found {len(profile_names)} steady state profiles: {profile_names}")
                return profile_names

        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except KeyError as e:
            raise KeyError(f"Error accessing steady state profile names: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading steady state profile names: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_steady_wse(
        hdf_path: Path,
        profile_index: Optional[int] = None,
        profile_name: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Extract water surface elevation (WSE) data for steady state profiles.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            profile_index (int, optional): Index of profile to extract (0-based). If None, extracts all profiles.
            profile_name (str, optional): Name of profile to extract (e.g., '1Pct'). If specified, overrides profile_index.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing WSE data with columns:
                - River: River name
                - Reach: Reach name
                - Station: Cross section river station
                - Profile: Profile name (if multiple profiles)
                - WSE: Water surface elevation (ft)

        Raises:
            FileNotFoundError: If the specified HDF file is not found
            KeyError: If steady state results or WSE data are not found
            ValueError: If profile_index or profile_name is invalid

        Example:
            >>> # Extract single profile by index
            >>> wse_df = HdfResultsPlan.get_steady_wse('01', profile_index=5)  # 100-year profile

            >>> # Extract single profile by name
            >>> wse_df = HdfResultsPlan.get_steady_wse('01', profile_name='1Pct')

            >>> # Extract all profiles
            >>> wse_df = HdfResultsPlan.get_steady_wse('01')
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Check if this is a steady state plan
                if "Results/Steady" not in hdf_file:
                    raise ValueError(f"HDF file does not contain steady state results: {hdf_path.name}")

                # Paths to data
                wse_path = "Results/Steady/Output/Output Blocks/Base Output/Steady Profiles/Cross Sections/Water Surface"
                xs_attrs_path = "Results/Steady/Output/Geometry Info/Cross Section Attributes"
                profile_names_path = "Results/Steady/Output/Output Blocks/Base Output/Steady Profiles/Profile Names"

                # Check required paths exist
                if wse_path not in hdf_file:
                    raise KeyError(f"WSE data not found at: {wse_path}")
                if xs_attrs_path not in hdf_file:
                    raise KeyError(f"Cross section attributes not found at: {xs_attrs_path}")

                # Get WSE dataset (shape: num_profiles × num_cross_sections)
                wse_ds = hdf_file[wse_path]
                wse_data = wse_ds[()]
                num_profiles, num_xs = wse_data.shape

                # Get profile names
                if profile_names_path in hdf_file:
                    profile_names_raw = hdf_file[profile_names_path][()]
                    profile_names = [
                        name.decode('utf-8').strip() if isinstance(name, bytes) else str(name).strip()
                        for name in profile_names_raw
                    ]
                else:
                    # Fallback to numbered profiles
                    profile_names = [f"Profile_{i+1}" for i in range(num_profiles)]

                # Get cross section attributes
                xs_attrs = hdf_file[xs_attrs_path][()]

                # Determine which profiles to extract
                if profile_name is not None:
                    # Find profile by name
                    try:
                        profile_idx = profile_names.index(profile_name)
                    except ValueError:
                        raise ValueError(
                            f"Profile name '{profile_name}' not found. "
                            f"Available profiles: {profile_names}"
                        )
                    profiles_to_extract = [(profile_idx, profile_name)]

                elif profile_index is not None:
                    # Validate profile index
                    if profile_index < 0 or profile_index >= num_profiles:
                        raise ValueError(
                            f"Profile index {profile_index} out of range. "
                            f"Valid range: 0 to {num_profiles-1}"
                        )
                    profiles_to_extract = [(profile_index, profile_names[profile_index])]

                else:
                    # Extract all profiles
                    profiles_to_extract = list(enumerate(profile_names))

                # Build DataFrame
                rows = []
                for prof_idx, prof_name in profiles_to_extract:
                    wse_values = wse_data[prof_idx, :]

                    for xs_idx in range(num_xs):
                        river = xs_attrs[xs_idx]['River']
                        reach = xs_attrs[xs_idx]['Reach']
                        station = xs_attrs[xs_idx]['Station']

                        # Decode byte strings
                        river = river.decode('utf-8') if isinstance(river, bytes) else str(river)
                        reach = reach.decode('utf-8') if isinstance(reach, bytes) else str(reach)
                        station = station.decode('utf-8') if isinstance(station, bytes) else str(station)

                        row = {
                            'River': river.strip(),
                            'Reach': reach.strip(),
                            'Station': station.strip(),
                            'WSE': float(wse_values[xs_idx])
                        }

                        # Only add Profile column if extracting multiple profiles
                        if len(profiles_to_extract) > 1:
                            row['Profile'] = prof_name

                        rows.append(row)

                df = pd.DataFrame(rows)

                # Reorder columns
                if 'Profile' in df.columns:
                    df = df[['River', 'Reach', 'Station', 'Profile', 'WSE']]
                else:
                    df = df[['River', 'Reach', 'Station', 'WSE']]

                logger.info(
                    f"Extracted WSE data for {len(profiles_to_extract)} profile(s), "
                    f"{num_xs} cross sections"
                )

                return df

        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except KeyError as e:
            raise KeyError(f"Error accessing steady state WSE data: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading steady state WSE data: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_steady_info(hdf_path: Path) -> pd.DataFrame:
        """
        Get steady flow attributes and metadata from HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing steady flow attributes including:
                - Program Name
                - Program Version
                - Type of Run
                - Run Time Window
                - Solution status
                - And other metadata attributes

        Raises:
            FileNotFoundError: If the specified HDF file is not found
            KeyError: If steady state results are not found
            ValueError: If the plan is not a steady state plan

        Example:
            >>> info_df = HdfResultsPlan.get_steady_info('01')
            >>> print(info_df['Solution'].values[0])
            'Steady Finished Successfully'
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Check if this is a steady state plan
                if "Results/Steady" not in hdf_file:
                    raise ValueError(f"HDF file does not contain steady state results: {hdf_path.name}")

                attrs_dict = {}

                # Get attributes from Results/Steady/Output
                output_path = "Results/Steady/Output"
                if output_path in hdf_file:
                    output_group = hdf_file[output_path]
                    for key, value in output_group.attrs.items():
                        if isinstance(value, bytes):
                            attrs_dict[key] = value.decode('utf-8')
                        else:
                            attrs_dict[key] = value

                # Get attributes from Results/Steady/Summary
                summary_path = "Results/Steady/Summary"
                if summary_path in hdf_file:
                    summary_group = hdf_file[summary_path]
                    for key, value in summary_group.attrs.items():
                        if isinstance(value, bytes):
                            attrs_dict[key] = value.decode('utf-8')
                        else:
                            attrs_dict[key] = value

                # Add flow file information from Plan Data
                plan_info_path = "Plan Data/Plan Information"
                if plan_info_path in hdf_file:
                    plan_info = hdf_file[plan_info_path]
                    for key in ['Flow Filename', 'Flow Title']:
                        if key in plan_info.attrs:
                            value = plan_info.attrs[key]
                            if isinstance(value, bytes):
                                attrs_dict[key] = value.decode('utf-8')
                            else:
                                attrs_dict[key] = value

                if not attrs_dict:
                    logger.warning("No steady state attributes found in HDF file")
                    return pd.DataFrame()

                logger.info(f"Extracted {len(attrs_dict)} steady state attributes")
                return pd.DataFrame(attrs_dict, index=[0])

        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except KeyError as e:
            raise KeyError(f"Error accessing steady state info: {str(e)}")
        except Exception as e:
            raise RuntimeError(f"Error reading steady state info: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_compute_messages(hdf_path: Path) -> str:
        """
        Read computation messages from HDF file with fallback to .txt file.

        Extracts computation messages from the HDF Results/Summary structure.
        This includes detailed information about the computation process,
        warnings, errors, convergence information, and performance metrics.

        If HDF path not found, falls back to .txt file extraction using RasControl.

        Args:
            hdf_path: Path to plan HDF file (or plan number string if using
                     standardize_input decorator, which resolves to HDF path)

        Returns:
            String containing computation messages, or empty string if unavailable

        Example:
            >>> from ras_commander import init_ras_project, HdfResultsPlan
            >>> init_ras_project(r"/path/to/project", "6.5")
            >>> msgs = HdfResultsPlan.get_compute_messages("01")
            >>> print(msgs)

        Note:
            Modern HEC-RAS versions (6.x+) store computation messages in HDF:
            /Results/Summary/Compute Messages (text)

            Older versions (pre-6.x) use .txt files which are accessed via
            fallback to RasControl.get_comp_msgs()

            Function naming follows HDF structure conventions (get_compute_messages)
            vs RasControl legacy naming (get_comp_msgs) to reflect technological lineage.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Define HDF path for compute messages
                compute_msgs_path = "Results/Summary/Compute Messages (text)"

                # Check if path exists in HDF
                if compute_msgs_path not in hdf_file:
                    logger.warning(
                        f"Compute Messages not found in HDF at '{compute_msgs_path}', "
                        f"falling back to .txt file extraction"
                    )

                    # Fallback to .txt file using RasControl
                    try:
                        # Late import to avoid circular dependency
                        from ..RasControl import RasControl

                        # Extract plan info from HDF path
                        # e.g., "C:/path/BaldEagle.p10.hdf" -> use path for RasControl
                        txt_contents = RasControl.get_comp_msgs(hdf_path)
                        if txt_contents:
                            logger.info(f"Successfully retrieved {len(txt_contents)} characters from .txt file")
                            return txt_contents
                    except Exception as e:
                        logger.debug(f".txt file fallback failed: {e}")

                    # Both methods failed
                    logger.debug(
                        f"No computation messages found in HDF or .txt sources for {hdf_path.name}"
                    )
                    return ""

                # Read dataset from HDF
                logger.info(f"Reading computation messages from HDF: {hdf_path.name}")
                dataset = hdf_file[compute_msgs_path]
                data = dataset[()]

                # Decode byte string to UTF-8
                if isinstance(data, bytes):
                    contents = data.decode('utf-8', errors='ignore')
                elif isinstance(data, np.ndarray) and len(data) > 0:
                    # Handle array of byte strings
                    if isinstance(data[0], bytes):
                        contents = data[0].decode('utf-8', errors='ignore')
                    else:
                        contents = str(data[0])
                else:
                    contents = str(data)

                logger.info(f"Successfully extracted {len(contents)} characters from HDF")
                return contents

        except FileNotFoundError:
            logger.debug(f"HDF file not found: {hdf_path}")

            # Try .txt fallback
            try:
                from ..RasControl import RasControl
                txt_contents = RasControl.get_comp_msgs(hdf_path)
                if txt_contents:
                    logger.warning(
                        f"HDF file not found, successfully retrieved computation messages from .txt file"
                    )
                    return txt_contents
            except Exception as e:
                logger.debug(f".txt file fallback failed: {e}")

            logger.debug(f"No computation messages found for {hdf_path.name}")
            return ""

        except Exception as e:
            logger.debug(f"Error reading computation messages from HDF: {str(e)}")

            # Try .txt fallback on any HDF error
            try:
                from ..RasControl import RasControl
                txt_contents = RasControl.get_comp_msgs(hdf_path)
                if txt_contents:
                    logger.warning(
                        f"HDF extraction failed, successfully retrieved computation messages from .txt file"
                    )
                    return txt_contents
            except Exception as fallback_error:
                logger.debug(f".txt file fallback failed: {fallback_error}")

            logger.debug(f"No computation messages found for {hdf_path.name}")
            return ""
==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfResultsPlot.py
==================================================
"""
Class: HdfResultsPlot

A collection of static methods for visualizing HEC-RAS results data from HDF files using matplotlib.

Public Functions:
    plot_results_mesh_variable(variable_df, variable_name, colormap='viridis', point_size=10):
        Generic plotting function for any mesh variable with customizable styling.
        
    plot_results_max_wsel(max_ws_df):
        Visualizes the maximum water surface elevation distribution across mesh cells.
        
    plot_results_max_wsel_time(max_ws_df):
        Displays the timing of maximum water surface elevation for each cell,
        including statistics about the temporal distribution.

Requirements:
    - matplotlib
    - pandas
    - geopandas (for geometry handling)

Input DataFrames must contain:
    - 'geometry' column with Point objects containing x,y coordinates
    - Variable data columns as specified in individual function docstrings
"""

import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict
from ..Decorators import log_call
from .HdfMesh import HdfMesh

class HdfResultsPlot:
    """
    A class containing static methods for plotting HEC-RAS results data.
    
    This class provides visualization methods for various types of HEC-RAS results,
    including maximum water surface elevations and timing information.
    """

    @staticmethod
    @log_call
    def plot_results_max_wsel(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the maximum water surface elevation per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface elevations.
        """
        # Extract x and y coordinates from the geometry column
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            print("Error: 'x' or 'y' columns not found in the merged dataframe.")
            print("Available columns:", max_ws_df.columns.tolist())
            return

        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=max_ws_df['maximum_water_surface'], 
                           cmap='viridis', s=10)

        ax.set_title('Max Water Surface per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label='Max Water Surface (ft)')

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

    @staticmethod
    @log_call
    def plot_results_max_wsel_time(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the time of the maximum water surface elevation (WSEL) per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface timing information.
        """
        # Convert datetime strings using the renamed utility function
        max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])
        
        # Extract coordinates
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            raise ValueError("x and y coordinates are missing from the DataFrame. Make sure the 'geometry' column exists and contains valid coordinate data.")

        fig, ax = plt.subplots(figsize=(12, 8))

        min_time = max_ws_df['max_wsel_time'].min()
        color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600

        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=color_values, cmap='viridis', s=10)

        ax.set_title('Time of Maximum Water Surface Elevation per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')

        cbar = plt.colorbar(scatter)
        cbar.set_label('Hours since simulation start')
        cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))
        cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

        # Print timing information
        print(f"\nSimulation Start Time: {min_time}")
        print(f"Time Range: {color_values.max():.1f} hours")
        print("\nTiming Statistics (hours since start):")
        print(color_values.describe()) 

    @staticmethod
    @log_call
    def plot_results_mesh_variable(variable_df: pd.DataFrame, variable_name: str, colormap: str = 'viridis', point_size: int = 10) -> None:
        """
        Plot any mesh variable with consistent styling.
        
        Args:
            variable_df (pd.DataFrame): DataFrame containing the variable data
            variable_name (str): Name of the variable (for labels)
            colormap (str): Matplotlib colormap to use. Default: 'viridis'
            point_size (int): Size of the scatter points. Default: 10

        Returns:
            None

        Raises:
            ImportError: If matplotlib is not installed
            ValueError: If required columns are missing from variable_df
        """
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            logger.error("matplotlib is required for plotting. Please install it with 'pip install matplotlib'")
            raise ImportError("matplotlib is required for plotting")

        # Get cell coordinates if not in variable_df
        if 'geometry' not in variable_df.columns:
            cell_coords = HdfMesh.mesh_cell_points(plan_hdf_path)
            merged_df = pd.merge(variable_df, cell_coords, on=['mesh_name', 'cell_id'])
        else:
            merged_df = variable_df
            
        # Extract coordinates, handling None values
        merged_df = merged_df.dropna(subset=['geometry'])
        merged_df['x'] = merged_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        merged_df['y'] = merged_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)
        
        # Drop any rows with None coordinates
        merged_df = merged_df.dropna(subset=['x', 'y'])
        
        if len(merged_df) == 0:
            logger.error("No valid coordinates found for plotting")
            raise ValueError("No valid coordinates found for plotting")
            
        # Create plot
        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(merged_df['x'], merged_df['y'], 
                           c=merged_df[variable_name], 
                           cmap=colormap, 
                           s=point_size)
        
        # Customize plot
        ax.set_title(f'{variable_name} per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label=variable_name)
        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfResultsXsec.py
==================================================
"""
Class: HdfResultsXsec

Contains methods for extracting 1D results data from HDF files. 
This includes cross section timeseries, structures and reference line/point timeseries as these are all 1D elements.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfResultsXsec:
- get_xsec_timeseries(): Extract cross-section timeseries data including water surface, velocity, and flow
- get_ref_lines_timeseries(): Get timeseries output for reference lines
- get_ref_points_timeseries(): Get timeseries output for reference points

TO BE IMPLEMENTED: 
DSS Hydrograph Extraction for 1D and 2D Structures. 

Planned functions:
- get_bridge_timeseries(): Extract timeseries data for bridge structures
- get_inline_structures_timeseries(): Extract timeseries data for inline structures

Notes:
- All functions use the get_ prefix to indicate they return data
- Results data functions use results_ prefix to indicate they handle results data
- All functions include proper error handling and logging
- Functions return xarray Datasets for efficient handling of multi-dimensional data
"""

from pathlib import Path
from typing import Union, Optional, List, Dict, Tuple

import h5py
import numpy as np
import pandas as pd
import xarray as xr

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfResultsXsec:
    """
    A static class for extracting and processing 1D results data from HEC-RAS HDF files.

    This class provides methods to extract and process unsteady flow simulation results
    for cross-sections, reference lines, and reference points. All methods are static
    and designed to be used without class instantiation.

    The class handles:
    - Cross-section timeseries (water surface, velocity, flow)
    - Reference line timeseries
    - Reference point timeseries

    Dependencies:
        - HdfBase: Core HDF file operations
        - HdfUtils: Utility functions for HDF processing
    """


# Tested functions from AWS webinar where the code was developed
# Need to add examples


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_xsec_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract Water Surface, Velocity Total, Velocity Channel, Flow Lateral, and Flow data from HEC-RAS HDF file.
        Includes Cross Section Only and Cross Section Attributes as coordinates in the xarray.Dataset.
        Also calculates maximum values for key parameters.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Xarray Dataset containing the extracted cross-section results with appropriate coordinates and attributes.
            Includes maximum values for Water Surface, Flow, Channel Velocity, Total Velocity, and Lateral Flow.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Define base paths
                base_output_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections/"
                time_stamp_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
                
                # Extract Cross Section Attributes
                attrs_dataset = hdf_file[f"{base_output_path}Cross Section Attributes"][:]
                rivers = [attr['River'].decode('utf-8').strip() for attr in attrs_dataset]
                reaches = [attr['Reach'].decode('utf-8').strip() for attr in attrs_dataset]
                stations = [attr['Station'].decode('utf-8').strip() for attr in attrs_dataset]
                names = [attr['Name'].decode('utf-8').strip() for attr in attrs_dataset]
                
                # Extract Cross Section Only (Unique Names)
                cross_section_only_dataset = hdf_file[f"{base_output_path}Cross Section Only"][:]
                cross_section_names = [cs.decode('utf-8').strip() for cs in cross_section_only_dataset]
                
                # Extract Time Stamps and convert to datetime
                time_stamps = hdf_file[time_stamp_path][:]
                if any(isinstance(ts, bytes) for ts in time_stamps):
                    time_stamps = [ts.decode('utf-8') for ts in time_stamps]
                # Convert RAS format timestamps to datetime
                times = pd.to_datetime(time_stamps, format='%d%b%Y %H:%M:%S:%f')
                
                # Extract Required Datasets
                water_surface = hdf_file[f"{base_output_path}Water Surface"][:]
                velocity_total = hdf_file[f"{base_output_path}Velocity Total"][:]
                velocity_channel = hdf_file[f"{base_output_path}Velocity Channel"][:]
                flow_lateral = hdf_file[f"{base_output_path}Flow Lateral"][:]
                flow = hdf_file[f"{base_output_path}Flow"][:]
                
                # Calculate maximum values along time axis
                max_water_surface = np.max(water_surface, axis=0)
                max_flow = np.max(flow, axis=0)
                max_velocity_channel = np.max(velocity_channel, axis=0)
                max_velocity_total = np.max(velocity_total, axis=0)
                max_flow_lateral = np.max(flow_lateral, axis=0)
                
                # Create Xarray Dataset
                ds = xr.Dataset(
                    {
                        'Water_Surface': (['time', 'cross_section'], water_surface),
                        'Velocity_Total': (['time', 'cross_section'], velocity_total),
                        'Velocity_Channel': (['time', 'cross_section'], velocity_channel),
                        'Flow_Lateral': (['time', 'cross_section'], flow_lateral),
                        'Flow': (['time', 'cross_section'], flow),
                    },
                    coords={
                        'time': times,
                        'cross_section': cross_section_names,
                        'River': ('cross_section', rivers),
                        'Reach': ('cross_section', reaches),
                        'Station': ('cross_section', stations),
                        'Name': ('cross_section', names),
                        'Maximum_Water_Surface': ('cross_section', max_water_surface),
                        'Maximum_Flow': ('cross_section', max_flow),
                        'Maximum_Channel_Velocity': ('cross_section', max_velocity_channel),
                        'Maximum_Velocity_Total': ('cross_section', max_velocity_total),
                        'Maximum_Flow_Lateral': ('cross_section', max_flow_lateral)
                    },
                    attrs={
                        'description': 'Cross-section results extracted from HEC-RAS HDF file',
                        'source_file': str(hdf_path)
                    }
                )
                
                return ds

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting cross section results: {e}")
            raise



    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_lines_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference lines from HEC-RAS HDF file.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing flow, velocity, and water surface data for reference lines.
            Returns empty dataset if reference line data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="lines")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_points_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference points from HEC-RAS HDF file.

        This method extracts flow, velocity, and water surface elevation data for all
        reference points defined in the model. Reference points are user-defined locations
        where detailed output is desired.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing the following variables for each reference point:
            - Flow [cfs or m³/s]
            - Velocity [ft/s or m/s]
            - Water Surface [ft or m]
            
            The dataset includes coordinates:
            - time: Simulation timesteps
            - refpt_id: Unique identifier for each reference point
            - refpt_name: Name of each reference point
            - mesh_name: Associated 2D mesh area name
            
            Returns empty dataset if reference point data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file

        Examples:
        --------
        >>> ds = HdfResultsXsec.get_ref_points_timeseries("path/to/plan.hdf")
        >>> # Get water surface timeseries for first reference point
        >>> ws = ds['Water Surface'].isel(refpt_id=0)
        >>> # Get all data for a specific reference point by name
        >>> point_data = ds.sel(refpt_name='Point1')
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="points")
    

    @staticmethod
    def _reference_timeseries_output(hdf_file: h5py.File, reftype: str = "lines") -> xr.Dataset:
        """
        Internal method to return timeseries output data for reference lines or points from a HEC-RAS HDF plan file.

        Parameters
        ----------
        hdf_file : h5py.File
            Open HDF file object.
        reftype : str, optional
            The type of reference data to retrieve. Must be either "lines" or "points".
            (default: "lines")

        Returns
        -------
        xr.Dataset
            An xarray Dataset with reference line or point timeseries data.
            Returns an empty Dataset if the reference output data is not found.

        Raises
        ------
        ValueError
            If reftype is not "lines" or "points".
        """
        if reftype == "lines":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Lines"
            abbrev = "refln"
        elif reftype == "points":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Points"
            abbrev = "refpt"
        else:
            raise ValueError('reftype must be either "lines" or "points".')

        try:
            reference_group = hdf_file[output_path]
        except KeyError:
            logger.error(f"Could not find HDF group at path '{output_path}'. "
                         f"The Plan HDF file may not contain reference {reftype[:-1]} output data.")
            return xr.Dataset()

        reference_names = reference_group["Name"][:]
        names = []
        mesh_areas = []
        for s in reference_names:
            name, mesh_area = s.decode("utf-8").split("|")
            names.append(name)
            mesh_areas.append(mesh_area)

        times = HdfBase.get_unsteady_timestamps(hdf_file)

        das = {}
        for var in ["Flow", "Velocity", "Water Surface"]:
            group = reference_group.get(var)
            if group is None:
                continue
            values = group[:]
            units = group.attrs["Units"].decode("utf-8")
            da = xr.DataArray(
                values,
                name=var,
                dims=["time", f"{abbrev}_id"],
                coords={
                    "time": times,
                    f"{abbrev}_id": range(values.shape[1]),
                    f"{abbrev}_name": (f"{abbrev}_id", names),
                    "mesh_name": (f"{abbrev}_id", mesh_areas),
                },
                attrs={"units": units, "hdf_path": f"{output_path}/{var}"},
            )
            das[var] = da
        return xr.Dataset(das)

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfStruc.py
==================================================
"""
Class: HdfStruc

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfStruc:
- get_structures()
- get_geom_structures_attrs()
"""
from typing import Dict, Any, List, Union
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point, GeometryCollection
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfBase import HdfBase
from ..Decorators import standardize_input, log_call
from ..LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfStruc:
    """
    Handles 2D structure geometry data extraction from HEC-RAS HDF files.

    This class provides static methods for extracting and analyzing structure geometries
    and their attributes from HEC-RAS geometry HDF files. All methods are designed to work
    without class instantiation.

    Notes
    -----
    - 1D Structure data should be accessed via the HdfResultsXsec class
    - All methods use @standardize_input for consistent file handling
    - All methods use @log_call for operation logging
    - Returns GeoDataFrames with both geometric and attribute data
    """
    
    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_structures(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts structure data from a HEC-RAS geometry HDF5 file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF5 file
        datetime_to_str : bool, optional
            If True, converts datetime objects to ISO format strings, by default False

        Returns
        -------
        GeoDataFrame
            Structure data with columns:
            - Structure ID: unique identifier
            - Geometry: LineString of structure centerline
            - Various attribute columns from the HDF file
            - Profile_Data: list of station/elevation dictionaries
            - Bridge coefficient attributes (if present)
            - Table info attributes (if present)

        Notes
        -----
        - Group-level attributes are stored in GeoDataFrame.attrs['group_attributes']
        - Invalid geometries are dropped with warning
        - All byte strings are decoded to UTF-8
        - CRS is preserved from the source file
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                if "Geometry/Structures" not in hdf:
                    logger.error(f"No Structures Found in the HDF, Empty Geodataframe Returned: {hdf_path}")
                    return GeoDataFrame()
                
                # Check if required datasets exist
                required_datasets = [
                    "Geometry/Structures/Centerline Info",
                    "Geometry/Structures/Centerline Points"
                ]
                
                for dataset in required_datasets:
                    if dataset not in hdf:
                        logger.error(f"No Structures Found in the HDF, Empty Geodataframe Returned: {hdf_path}")
                        return GeoDataFrame()

                def get_dataset_df(path: str) -> pd.DataFrame:
                    """
                    Converts an HDF5 dataset to a pandas DataFrame.

                    Parameters
                    ----------
                    path : str
                        Dataset path within the HDF5 file

                    Returns
                    -------
                    pd.DataFrame
                        DataFrame containing the dataset values.
                        - For compound datasets, column names match field names
                        - For simple datasets, generic column names (Value_0, Value_1, etc.)
                        - Empty DataFrame if dataset not found

                    Notes
                    -----
                    Automatically decodes byte strings to UTF-8 with error handling.
                    """
                    if path not in hdf:
                        logger.warning(f"Dataset not found: {path}")
                        return pd.DataFrame()
                    
                    data = hdf[path][()]
                    
                    if data.dtype.names:
                        df = pd.DataFrame(data)
                        # Decode byte strings to UTF-8
                        for col in df.columns:
                            if df[col].dtype.kind in {'S', 'a'}:  # Byte strings
                                df[col] = df[col].str.decode('utf-8', errors='ignore')
                        return df
                    else:
                        # If no named fields, assign generic column names
                        return pd.DataFrame(data, columns=[f'Value_{i}' for i in range(data.shape[1])])

                # Extract relevant datasets
                group_attrs = HdfBase.get_attrs(hdf, "Geometry/Structures")
                struct_attrs = get_dataset_df("Geometry/Structures/Attributes")
                bridge_coef = get_dataset_df("Geometry/Structures/Bridge Coefficient Attributes")
                table_info = get_dataset_df("Geometry/Structures/Table Info")
                profile_data = get_dataset_df("Geometry/Structures/Profile Data")

                # Assign 'Structure ID' based on index (starting from 1)
                struct_attrs.reset_index(drop=True, inplace=True)
                struct_attrs['Structure ID'] = range(1, len(struct_attrs) + 1)
                logger.debug(f"Assigned Structure IDs: {struct_attrs['Structure ID'].tolist()}")

                # Check if 'Structure ID' was successfully assigned
                if 'Structure ID' not in struct_attrs.columns:
                    logger.error("'Structure ID' column could not be assigned to Structures/Attributes.")
                    return GeoDataFrame()

                # Get centerline geometry
                centerline_info = hdf["Geometry/Structures/Centerline Info"][()]
                centerline_points = hdf["Geometry/Structures/Centerline Points"][()]
                
                # Create LineString geometries for each structure
                geoms = []
                for i in range(len(centerline_info)):
                    start_idx = centerline_info[i][0]  # Point Starting Index
                    point_count = centerline_info[i][1]  # Point Count
                    points = centerline_points[start_idx:start_idx + point_count]
                    if len(points) >= 2:
                        geoms.append(LineString(points))
                    else:
                        logger.warning(f"Insufficient points for LineString in structure index {i}.")
                        geoms.append(None)

                # Create base GeoDataFrame with Structures Attributes and geometries
                struct_gdf = GeoDataFrame(
                    struct_attrs,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Drop entries with invalid geometries
                initial_count = len(struct_gdf)
                struct_gdf = struct_gdf.dropna(subset=['geometry']).reset_index(drop=True)
                final_count = len(struct_gdf)
                if final_count < initial_count:
                    logger.warning(f"Dropped {initial_count - final_count} structures due to invalid geometries.")

                # Merge Bridge Coefficient Attributes on 'Structure ID'
                if not bridge_coef.empty and 'Structure ID' in bridge_coef.columns:
                    struct_gdf = struct_gdf.merge(
                        bridge_coef,
                        on='Structure ID',
                        how='left',
                        suffixes=('', '_bridge_coef')
                    )
                    logger.debug("Merged Bridge Coefficient Attributes successfully.")
                else:
                    logger.warning("Bridge Coefficient Attributes missing or 'Structure ID' not present.")

                # Merge Table Info based on the DataFrame index (one-to-one correspondence)
                if not table_info.empty:
                    if len(table_info) != len(struct_gdf):
                        logger.warning("Table Info count does not match Structures count. Skipping merge.")
                    else:
                        struct_gdf = pd.concat([struct_gdf, table_info.reset_index(drop=True)], axis=1)
                        logger.debug("Merged Table Info successfully.")
                else:
                    logger.warning("Table Info dataset is empty or missing.")

                # Process Profile Data based on Table Info
                if not profile_data.empty and not table_info.empty:
                    # Assuming 'Centerline Profile (Index)' and 'Centerline Profile (Count)' are in 'Table Info'
                    if ('Centerline Profile (Index)' in table_info.columns and
                        'Centerline Profile (Count)' in table_info.columns):
                        struct_gdf['Profile_Data'] = struct_gdf.apply(
                            lambda row: [
                                {'Station': float(profile_data.iloc[i, 0]),
                                 'Elevation': float(profile_data.iloc[i, 1])}
                                for i in range(
                                    int(row['Centerline Profile (Index)']),
                                    int(row['Centerline Profile (Index)']) + int(row['Centerline Profile (Count)'])
                                )
                            ],
                            axis=1
                        )
                        logger.debug("Processed Profile Data successfully.")
                    else:
                        logger.warning("Required columns for Profile Data not found in Table Info.")
                else:
                    logger.warning("Profile Data dataset is empty or Table Info is missing.")

                # Convert datetime columns to string if requested
                if datetime_to_str:
                    datetime_cols = struct_gdf.select_dtypes(include=['datetime64']).columns
                    for col in datetime_cols:
                        struct_gdf[col] = struct_gdf[col].dt.isoformat()
                        logger.debug(f"Converted datetime column '{col}' to string.")

                # Ensure all byte strings are decoded (if any remain)
                for col in struct_gdf.columns:
                    if struct_gdf[col].dtype == object:
                        struct_gdf[col] = struct_gdf[col].apply(
                            lambda x: x.decode('utf-8', errors='ignore') if isinstance(x, bytes) else x
                        )

                # Final GeoDataFrame
                logger.info("Successfully extracted structures GeoDataFrame.")
                
                # Add group attributes to the GeoDataFrame's attrs['group_attributes']
                struct_gdf.attrs['group_attributes'] = group_attrs
                
                logger.info("Successfully extracted structures GeoDataFrame with attributes.")
                
                return struct_gdf

        except Exception as e:
            logger.error(f"Error reading structures from {hdf_path}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_geom_structures_attrs(hdf_path: Path) -> pd.DataFrame:
        """
        Extracts structure attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        pd.DataFrame
            DataFrame containing structure attributes from the Geometry/Structures group.
            Returns empty DataFrame if no structures are found.

        Notes
        -----
        Attributes are extracted from the HDF5 group 'Geometry/Structures'.
        All byte strings in attributes are automatically decoded to UTF-8.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/Structures" not in hdf_file:
                    logger.info(f"No structures found in the geometry file: {hdf_path}")
                    return pd.DataFrame()
                
                # Get attributes and decode byte strings
                attrs_dict = {}
                for key, value in dict(hdf_file["Geometry/Structures"].attrs).items():
                    if isinstance(value, bytes):
                        attrs_dict[key] = value.decode('utf-8')
                    else:
                        attrs_dict[key] = value
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])

        except Exception as e:
            logger.error(f"Error reading geometry structures attributes: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def list_sa2d_connections(hdf_path: Path, *, ras_object=None) -> List[str]:
        """
        List all SA/2D Area Connection structures in HDF results file.

        This includes both breach structures and regular SA/2D connections with
        time series results.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        List[str]
            Names of all SA/2D Area Connection structures with time series results.
            Returns empty list if no SA/2D connections found.

        Examples
        --------
        >>> structures = HdfStruc.list_sa2d_connections("02")
        >>> print(structures)
        ['Laxton_Dam', 'PineCreek#1_Dam', 'US_2DArea_Res2']

        Notes
        -----
        - Not all structures returned have breach capability
        - Use get_sa2d_breach_info() to determine which have "Breaching Variables"
        - Empty list returned if no SA/2D connections in results
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/SA 2D Area Conn"

                if base_path not in hdf_file:
                    logger.warning(f"No SA 2D Area Conn data found in {hdf_path.name}")
                    return []

                # List all groups (structure names) under SA 2D Area Conn
                structures = list(hdf_file[base_path].keys())
                logger.info(f"Found {len(structures)} SA/2D connection structures: {structures}")
                return structures

        except Exception as e:
            logger.error(f"Error listing SA/2D connection structures: {e}")
            return []

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_sa2d_breach_info(hdf_path: Path, *, ras_object=None) -> pd.DataFrame:
        """
        Get information about which SA/2D connection structures have breach capability.

        Parameters
        ----------
        hdf_path : Path
            Path to HEC-RAS plan HDF file or plan number
        ras_object : RasPrj, optional
            RAS object for multi-project workflows

        Returns
        -------
        pd.DataFrame
            DataFrame with columns:
            - structure: Structure name
            - has_breach: Boolean, True if "Breaching Variables" dataset exists
            - breach_at_time: Time of breach initiation (if available)
            - breach_at_date: Date/time of breach (if available)
            - centerline_breach: Centerline station for breach (if available)

        Examples
        --------
        >>> info = HdfStruc.get_sa2d_breach_info("02")
        >>> breach_dams = info[info['has_breach']]['structure'].tolist()
        >>> print(f"Breach structures: {breach_dams}")

        Notes
        -----
        - Returns empty DataFrame if no SA/2D connections found
        - Only structures with "Breaching Variables" have has_breach=True
        - Use in conjunction with RasBreach for reading/modifying breach parameters
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                structures = HdfStruc.list_sa2d_connections(hdf_path, ras_object=ras_object)

                if not structures:
                    return pd.DataFrame()

                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/SA 2D Area Conn"

                info_list = []
                for struct_name in structures:
                    struct_path = f"{base_path}/{struct_name}"
                    breach_var_path = f"{struct_path}/Breaching Variables"

                    info = {'structure': struct_name}

                    # Check if breach variables exist
                    if breach_var_path in hdf_file:
                        info['has_breach'] = True

                        # Extract breach metadata from attributes
                        breach_dataset = hdf_file[breach_var_path]
                        if 'Breach at' in breach_dataset.attrs:
                            breach_at = breach_dataset.attrs['Breach at']
                            info['breach_at_date'] = breach_at.decode('utf-8') if isinstance(breach_at, bytes) else breach_at
                        else:
                            info['breach_at_date'] = None

                        if 'Breach at Time (Days)' in breach_dataset.attrs:
                            info['breach_at_time'] = float(breach_dataset.attrs['Breach at Time (Days)'])
                        else:
                            info['breach_at_time'] = None

                        if 'Centerline Breach' in breach_dataset.attrs:
                            info['centerline_breach'] = float(breach_dataset.attrs['Centerline Breach'])
                        else:
                            info['centerline_breach'] = None
                    else:
                        info['has_breach'] = False
                        info['breach_at_date'] = None
                        info['breach_at_time'] = None
                        info['centerline_breach'] = None

                    info_list.append(info)

                return pd.DataFrame(info_list)

        except Exception as e:
            logger.error(f"Error getting SA/2D breach info: {e}")
            raise

==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfUtils.py
==================================================
"""
HdfUtils Class
-------------

A utility class providing static methods for working with HEC-RAS HDF files.

Attribution:
    A substantial amount of code in this file is sourced or derived from the 
    https://github.com/fema-ffrd/rashdf library, released under MIT license 
    and Copyright (c) 2024 fema-ffrd. The file has been forked and modified 
    for use in RAS Commander.

Key Features:
- HDF file data conversion and parsing
- DateTime handling for RAS-specific formats
- Spatial operations using KDTree
- HDF attribute management

Main Method Categories:

1. Data Conversion
    - convert_ras_string: Convert RAS HDF strings to Python objects
    - convert_ras_hdf_value: Convert general HDF values to Python objects
    - convert_df_datetimes_to_str: Convert DataFrame datetime columns to strings
    - convert_hdf5_attrs_to_dict: Convert HDF5 attributes to dictionary
    - convert_timesteps_to_datetimes: Convert timesteps to datetime objects

2. Spatial Operations
    - perform_kdtree_query: KDTree search between datasets
    - find_nearest_neighbors: Find nearest neighbors within dataset

3. DateTime Parsing
    - parse_ras_datetime: Parse standard RAS datetime format (ddMMMYYYY HH:MM:SS)
    - parse_ras_window_datetime: Parse simulation window datetime (ddMMMYYYY HHMM)
    - parse_duration: Parse duration strings (HH:MM:SS)
    - parse_ras_datetime_ms: Parse datetime with milliseconds
    - parse_run_time_window: Parse time window strings

Usage Notes:
- All methods are static and can be called without class instantiation
- Methods handle both raw HDF data and converted Python objects
- Includes comprehensive error handling for RAS-specific data formats
- Supports various RAS datetime formats and conversions
"""
import logging
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Union, Optional, Dict, List, Tuple, Any
from scipy.spatial import KDTree
import re
from shapely.geometry import LineString  # Import LineString to avoid NameError

from ..Decorators import standardize_input, log_call 
from ..LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfUtils:
    """
    Utility class for working with HEC-RAS HDF files.

    This class provides general utility functions for HDF file operations,
    including attribute extraction, data conversion, and common HDF queries.
    It also includes spatial operations and helper methods for working with
    HEC-RAS specific data structures.

    Note:
    - Use this class for general HDF utility functions that are not specific to plan or geometry files.
    - All methods in this class are static and can be called without instantiating the class.
    """




# RENAME TO convert_ras_string and make public

    @staticmethod
    def convert_ras_string(value: Union[str, bytes]) -> Union[bool, datetime, List[datetime], timedelta, str]:
        """
        Convert a string value from an HEC-RAS HDF file into a Python object.

        Args:
            value (Union[str, bytes]): The value to convert.

        Returns:
            Union[bool, datetime, List[datetime], timedelta, str]: The converted value.
        """
        if isinstance(value, bytes):
            s = value.decode("utf-8")
        else:
            s = value

        if s == "True":
            return True
        elif s == "False":
            return False
        
        ras_datetime_format1_re = r"\d{2}\w{3}\d{4} \d{2}:\d{2}:\d{2}"
        ras_datetime_format2_re = r"\d{2}\w{3}\d{4} \d{2}\d{2}"
        ras_duration_format_re = r"\d{2}:\d{2}:\d{2}"

        if re.match(rf"^{ras_datetime_format1_re}", s):
            if re.match(rf"^{ras_datetime_format1_re} to {ras_datetime_format1_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_datetime(split[0]),
                    HdfUtils.parse_ras_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_datetime(s)
        elif re.match(rf"^{ras_datetime_format2_re}", s):
            if re.match(rf"^{ras_datetime_format2_re} to {ras_datetime_format2_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_window_datetime(split[0]),
                    HdfUtils.parse_ras_window_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_window_datetime(s)
        elif re.match(rf"^{ras_duration_format_re}$", s):
            return HdfUtils.parse_ras_duration(s)
        return s





    @staticmethod
    def convert_ras_hdf_value(value: Any) -> Union[None, bool, str, List[str], int, float, List[int], List[float]]:
        """
        Convert a value from a HEC-RAS HDF file into a Python object.

        Args:
            value (Any): The value to convert.

        Returns:
            Union[None, bool, str, List[str], int, float, List[int], List[float]]: The converted value.
        """
        if isinstance(value, np.floating) and np.isnan(value):
            return None
        elif isinstance(value, (bytes, np.bytes_)):
            return value.decode('utf-8')
        elif isinstance(value, np.integer):
            return int(value)
        elif isinstance(value, np.floating):
            return float(value)
        elif isinstance(value, (int, float)):
            return value
        elif isinstance(value, (list, tuple, np.ndarray)):
            if len(value) > 1:
                return [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                return HdfUtils.convert_ras_hdf_value(value[0])
        else:
            return str(value)










# RENAME TO convert_df_datetimes_to_str 

    @staticmethod
    def convert_df_datetimes_to_str(df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert any datetime64 columns in a DataFrame to strings.

        Args:
            df (pd.DataFrame): The DataFrame to convert.

        Returns:
            pd.DataFrame: The DataFrame with datetime columns converted to strings.
        """
        for col in df.select_dtypes(include=['datetime64']).columns:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
        return df


# KDTree Methods: 


    @staticmethod
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = HdfUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = HdfUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped




# Datetime Parsing Methods: 

    @staticmethod
    @log_call
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Public method to parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
# Rename to convert_timesteps_to_datetimes and make public
    @staticmethod
    def convert_timesteps_to_datetimes(timesteps: np.ndarray, start_time: datetime, time_unit: str = "days", round_to: str = "100ms") -> pd.DatetimeIndex:
        """
        Convert RAS timesteps to datetime objects.

        Args:
            timesteps (np.ndarray): Array of timesteps.
            start_time (datetime): Start time of the simulation.
            time_unit (str): Unit of the timesteps. Default is "days".
            round_to (str): Frequency string to round the times to. Default is "100ms" (100 milliseconds).

        Returns:
            pd.DatetimeIndex: DatetimeIndex of converted and rounded datetimes.
        """
        if time_unit == "days":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='D')
        elif time_unit == "hours":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='H')
        else:
            raise ValueError(f"Unsupported time unit: {time_unit}")

        return pd.DatetimeIndex(datetimes).round(round_to)
    
# rename to convert_hdf5_attrs_to_dict and make public

    @staticmethod
    def convert_hdf5_attrs_to_dict(attrs: Union[h5py.AttributeManager, Dict], prefix: Optional[str] = None) -> Dict:
        """
        Convert HDF5 attributes to a Python dictionary.

        Args:
            attrs (Union[h5py.AttributeManager, Dict]): The attributes to convert.
            prefix (Optional[str]): A prefix to add to the attribute keys.

        Returns:
            Dict: A dictionary of converted attributes.
        """
        result = {}
        for key, value in attrs.items():
            if prefix:
                key = f"{prefix}/{key}"
            if isinstance(value, (np.ndarray, list)):
                result[key] = [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                result[key] = HdfUtils.convert_ras_hdf_value(value)
        return result
    
    

    @staticmethod
    def parse_run_time_window(window: str) -> Tuple[datetime, datetime]:
        """
        Parse a run time window string into a tuple of datetime objects.

        Args:
            window (str): The run time window string to be parsed.

        Returns:
            Tuple[datetime, datetime]: A tuple containing two datetime objects representing the start and end of the run
            time window.
        """
        split = window.split(" to ")
        begin = HdfUtils._parse_ras_datetime(split[0])
        end = HdfUtils._parse_ras_datetime(split[1])
        return begin, end

    


                
                
                
                
                
                
                
                
                
                
                
                
## MOVED FROM HdfBase to HdfUtils:
# _parse_ras_datetime   
# _parse_ras_simulation_window_datetime
# _parse_duration
# _parse_ras_datetime_ms
# _convert_ras_hdf_string

# Which were renamed and made public as: 
# parse_ras_datetime
# parse_ras_window_datetime
# parse_ras_datetime_ms
# parse_ras_duration
# parse_ras_time_window


# Rename to parse_ras_datetime and make public

    @staticmethod
    def parse_ras_datetime(datetime_str: str) -> datetime:
        """
        Parse a RAS datetime string into a datetime object.

        Args:
            datetime_str (str): The datetime string in format "ddMMMYYYY HH:MM:SS"

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H:%M:%S")

# Rename to parse_ras_window_datetime and make public

    @staticmethod
    def parse_ras_window_datetime(datetime_str: str) -> datetime:
        """
        Parse a datetime string from a RAS simulation window into a datetime object.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H%M")


# Rename to parse_duration and make public


    @staticmethod
    def parse_duration(duration_str: str) -> timedelta:
        """
        Parse a duration string into a timedelta object.

        Args:
            duration_str (str): The duration string to parse.

        Returns:
            timedelta: The parsed duration as a timedelta object.
        """
        hours, minutes, seconds = map(int, duration_str.split(':'))
        return timedelta(hours=hours, minutes=minutes, seconds=seconds)
    
    
# Rename to parse_ras_datetime_ms and make public
    
    @staticmethod
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
    
==================================================

File: C:\GH\ras-commander\ras_commander\hdf\HdfXsec.py
==================================================
"""
Class: HdfXsec

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

This source code has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

Available Functions:
- get_cross_sections(): Extract cross sections from HDF geometry file
- get_river_centerlines(): Extract river centerlines from HDF geometry file
- get_river_stationing(): Calculate river stationing along centerlines
- get_river_reaches(): Return the model 1D river reach lines
- get_river_edge_lines(): Return the model river edge lines
- get_river_bank_lines(): Extract river bank lines from HDF geometry file
- _interpolate_station(): Private helper method for station interpolation

All functions follow the get_ prefix convention for methods that return data.
Private helper methods use the underscore prefix convention.

Each function returns a GeoDataFrame containing geometries and associated attributes
specific to the requested feature type. All functions include proper error handling
and logging.
"""

from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString
from typing import List  # Import List to avoid NameError
from ..Decorators import standardize_input, log_call
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from ..LoggingConfig import get_logger
import logging



logger = get_logger(__name__)

class HdfXsec:
    """
    Handles cross-section and river geometry data extraction from HEC-RAS HDF files.

    This class provides static methods to extract and process:
    - Cross-section geometries and attributes
    - River centerlines and reaches
    - River edge and bank lines
    - Station-elevation profiles

    All methods are designed to return GeoDataFrames with standardized geometries 
    and attributes following the HEC-RAS data structure.

    Note:
        Requires HEC-RAS geometry HDF files with standard structure and naming conventions.
        All methods use proper error handling and logging.
    """
    @staticmethod
    @log_call
    def get_cross_sections(hdf_path: str, datetime_to_str: bool = True, ras_object=None) -> gpd.GeoDataFrame:
        """
        Extracts cross-section geometries and attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : str
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to True
        ras_object : RasPrj, optional
            RAS project object for additional context, defaults to None

        Returns
        -------
        gpd.GeoDataFrame
            Cross-section data with columns:
            - geometry: LineString of cross-section path
            - station_elevation: Station-elevation profile points
            - mannings_n: Dictionary of Manning's n values and stations
            - ineffective_blocks: List of ineffective flow area blocks
            - River, Reach, RS: River system identifiers
            - Name, Description: Cross-section labels
            - Len Left/Channel/Right: Flow path lengths
            - Left/Right Bank: Bank station locations
            - Additional hydraulic parameters and attributes

        Notes
        -----
        The returned GeoDataFrame includes the coordinate system from the HDF file
        when available. All byte strings are converted to regular strings.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract required datasets
                poly_info = hdf['/Geometry/Cross Sections/Polyline Info'][:]
                poly_parts = hdf['/Geometry/Cross Sections/Polyline Parts'][:]
                poly_points = hdf['/Geometry/Cross Sections/Polyline Points'][:]
                
                station_info = hdf['/Geometry/Cross Sections/Station Elevation Info'][:]
                station_values = hdf['/Geometry/Cross Sections/Station Elevation Values'][:]
                
                # Get attributes for cross sections
                xs_attrs = hdf['/Geometry/Cross Sections/Attributes'][:]
                
                # Get Manning's n data
                mann_info = hdf["/Geometry/Cross Sections/Manning's n Info"][:]
                mann_values = hdf["/Geometry/Cross Sections/Manning's n Values"][:]
                
                # Get ineffective blocks data if they exist
                if '/Geometry/Cross Sections/Ineffective Blocks' in hdf:
                    ineff_blocks = hdf['/Geometry/Cross Sections/Ineffective Blocks'][:]
                    ineff_info = hdf['/Geometry/Cross Sections/Ineffective Info'][:]
                else:
                    ineff_blocks = None
                    ineff_info = None
                
                # Initialize lists to store data
                geometries = []
                station_elevations = []
                mannings_n = []
                ineffective_blocks = []
                
                # Process each cross section
                for i in range(len(poly_info)):
                    # Extract polyline info
                    point_start_idx = poly_info[i][0]
                    point_count = poly_info[i][1]
                    part_start_idx = poly_info[i][2]
                    part_count = poly_info[i][3]
                    
                    # Extract parts for current polyline
                    parts = poly_parts[part_start_idx:part_start_idx + part_count]
                    
                    # Collect all points for this cross section
                    xs_points = []
                    for part in parts:
                        part_point_start = point_start_idx + part[0]
                        part_point_count = part[1]
                        points = poly_points[part_point_start:part_point_start + part_point_count]
                        xs_points.extend(points)
                    
                    # Create LineString geometry
                    if len(xs_points) >= 2:
                        geometry = LineString(xs_points)
                        geometries.append(geometry)
                        
                        # Extract station-elevation data
                        start_idx = station_info[i][0]
                        count = station_info[i][1]
                        station_elev = station_values[start_idx:start_idx + count]
                        station_elevations.append(station_elev)
                        
                        # Extract Manning's n data
                        mann_start_idx = mann_info[i][0]
                        mann_count = mann_info[i][1]
                        mann_n_section = mann_values[mann_start_idx:mann_start_idx + mann_count]
                        mann_n_dict = {
                            'Station': mann_n_section[:, 0].tolist(),
                            'Mann n': mann_n_section[:, 1].tolist()
                        }
                        mannings_n.append(mann_n_dict)
                        
                        # Extract ineffective blocks data
                        if ineff_info is not None and ineff_blocks is not None:
                            ineff_start_idx = ineff_info[i][0]
                            ineff_count = ineff_info[i][1]
                            if ineff_count > 0:
                                blocks = ineff_blocks[ineff_start_idx:ineff_start_idx + ineff_count]
                                blocks_list = []
                                for block in blocks:
                                    block_dict = {
                                        'Left Sta': float(block['Left Sta']),
                                        'Right Sta': float(block['Right Sta']), 
                                        'Elevation': float(block['Elevation']),
                                        'Permanent': bool(block['Permanent'])
                                    }
                                    blocks_list.append(block_dict)
                                ineffective_blocks.append(blocks_list)
                            else:
                                ineffective_blocks.append([])
                        else:
                            ineffective_blocks.append([])
                
                # Create base dictionary with required fields
                data = {
                    'geometry': geometries,
                    'station_elevation': station_elevations,
                    'mannings_n': mannings_n,
                    'ineffective_blocks': ineffective_blocks,
                }
                
                # Define field mappings with default values
                field_mappings = {
                    'River': ('River', ''),
                    'Reach': ('Reach', ''),
                    'RS': ('RS', ''),
                    'Name': ('Name', ''),
                    'Description': ('Description', ''),
                    'Len Left': ('Len Left', 0.0),
                    'Len Channel': ('Len Channel', 0.0),
                    'Len Right': ('Len Right', 0.0),
                    'Left Bank': ('Left Bank', 0.0),
                    'Right Bank': ('Right Bank', 0.0),
                    'Friction Mode': ('Friction Mode', ''),
                    'Contr': ('Contr', 0.0),
                    'Expan': ('Expan', 0.0),
                    'Left Levee Sta': ('Left Levee Sta', None),
                    'Left Levee Elev': ('Left Levee Elev', None),
                    'Right Levee Sta': ('Right Levee Sta', None),
                    'Right Levee Elev': ('Right Levee Elev', None),
                    'HP Count': ('HP Count', 0),
                    'HP Start Elev': ('HP Start Elev', 0.0),
                    'HP Vert Incr': ('HP Vert Incr', 0.0),
                    'HP LOB Slices': ('HP LOB Slices', 0),
                    'HP Chan Slices': ('HP Chan Slices', 0),
                    'HP ROB Slices': ('HP ROB Slices', 0),
                    'Ineff Block Mode': ('Ineff Block Mode', 0),
                    'Obstr Block Mode': ('Obstr Block Mode', 0),
                    'Default Centerline': ('Default Centerline', 0),
                    'Last Edited': ('Last Edited', '')
                }
                
                # Add fields that exist in xs_attrs
                for field_name, (attr_name, default_value) in field_mappings.items():
                    if attr_name in xs_attrs.dtype.names:
                        if xs_attrs[attr_name].dtype.kind == 'S':
                            # Handle string fields
                            data[field_name] = [x[attr_name].decode('utf-8').strip() 
                                              for x in xs_attrs]
                        else:
                            # Handle numeric fields
                            data[field_name] = xs_attrs[attr_name]
                    else:
                        # Use default value if field doesn't exist
                        data[field_name] = [default_value] * len(geometries)
                        logger.debug(f"Field {attr_name} not found in attributes, using default value")
                
                if geometries:
                    gdf = gpd.GeoDataFrame(data)
                    
                    # Set CRS if available
                    if 'Projection' in hdf['/Geometry'].attrs:
                        proj = hdf['/Geometry'].attrs['Projection']
                        if isinstance(proj, bytes):
                            proj = proj.decode('utf-8')
                        gdf.set_crs(proj, allow_override=True)
                    
                    return gdf
                
                return gpd.GeoDataFrame()
                
        except Exception as e:
            logger.error(f"Error processing cross-section data: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_centerlines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts river centerline geometries and attributes from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to False

        Returns
        -------
        GeoDataFrame
            River centerline data with columns:
            - geometry: LineString of river centerline
            - River Name, Reach Name: River system identifiers
            - US/DS Type, Name: Upstream/downstream connection info
            - length: Centerline length in project units
            Additional attributes from the HDF file are included

        Notes
        -----
        Returns an empty GeoDataFrame if no centerlines are found.
        All string attributes are stripped of whitespace.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    logger.warning("No river centerlines found in geometry file")
                    return GeoDataFrame()

                centerline_data = hdf_file["Geometry/River Centerlines"]
                
                # Get attributes directly from HDF dataset
                attrs = centerline_data["Attributes"][()]
                
                # Create initial dictionary for DataFrame
                centerline_dict = {}
                
                # Process each attribute field
                for name in attrs.dtype.names:
                    values = attrs[name]
                    if values.dtype.kind == 'S':
                        # Convert byte strings to regular strings
                        centerline_dict[name] = [val.decode('utf-8').strip() for val in values]
                    else:
                        centerline_dict[name] = values.tolist()  # Convert numpy array to list

                # Get polylines using utility function
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Centerlines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                centerline_gdf = GeoDataFrame(
                    centerline_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Clean up string columns
                str_columns = ['River Name', 'Reach Name', 'US Type', 
                            'US Name', 'DS Type', 'DS Name']
                for col in str_columns:
                    if col in centerline_gdf.columns:
                        centerline_gdf[col] = centerline_gdf[col].str.strip()

                # Add length calculation in project units
                if not centerline_gdf.empty:
                    centerline_gdf['length'] = centerline_gdf.geometry.length
                    
                    # Convert datetime columns if requested
                    if datetime_to_str:
                        datetime_cols = centerline_gdf.select_dtypes(
                            include=['datetime64']).columns
                        for col in datetime_cols:
                            centerline_gdf[col] = centerline_gdf[col].dt.strftime(
                                '%Y-%m-%d %H:%M:%S')

                logger.info(f"Extracted {len(centerline_gdf)} river centerlines")
                return centerline_gdf

        except Exception as e:
            logger.error(f"Error reading river centerlines: {str(e)}")
            return GeoDataFrame()



    @staticmethod
    @log_call
    def get_river_stationing(centerlines_gdf: GeoDataFrame) -> GeoDataFrame:
        """
        Calculates stationing along river centerlines with interpolated points.

        Parameters
        ----------
        centerlines_gdf : GeoDataFrame
            River centerline geometries from get_river_centerlines()

        Returns
        -------
        GeoDataFrame
            Original centerlines with additional columns:
            - station_start: Starting station value (0 or length)
            - station_end: Ending station value (length or 0)
            - stations: Array of station values along centerline
            - points: Array of interpolated point geometries

        Notes
        -----
        Station direction (increasing/decreasing) is determined by
        upstream/downstream junction connections. Stations are calculated
        at 100 evenly spaced points along each centerline.
        """
        if centerlines_gdf.empty:
            logger.warning("Empty centerlines GeoDataFrame provided")
            return centerlines_gdf

        try:
            # Create copy to avoid modifying original
            result_gdf = centerlines_gdf.copy()
            
            # Initialize new columns
            result_gdf['station_start'] = 0.0
            result_gdf['station_end'] = 0.0
            result_gdf['stations'] = None
            result_gdf['points'] = None
            
            # Process each centerline
            for idx, row in result_gdf.iterrows():
                # Get line geometry
                line = row.geometry
                
                # Calculate length
                total_length = line.length
                
                # Generate points along the line
                distances = np.linspace(0, total_length, num=100)  # Adjust num for desired density
                points = [line.interpolate(distance) for distance in distances]
                
                # Store results
                result_gdf.at[idx, 'station_start'] = 0.0
                result_gdf.at[idx, 'station_end'] = total_length
                result_gdf.at[idx, 'stations'] = distances
                result_gdf.at[idx, 'points'] = points
                
                # Add stationing direction based on upstream/downstream info
                if row['US Type'] == 'Junction' and row['DS Type'] != 'Junction':
                    # Reverse stationing if upstream is junction
                    result_gdf.at[idx, 'station_start'] = total_length
                    result_gdf.at[idx, 'station_end'] = 0.0
                    result_gdf.at[idx, 'stations'] = total_length - distances
            
            return result_gdf

        except Exception as e:
            logger.error(f"Error calculating river stationing: {str(e)}")
            return centerlines_gdf

    @staticmethod
    def _interpolate_station(line, distance):
        """
        Interpolates a point along a line at a given distance.

        Parameters
        ----------
        line : LineString
            Shapely LineString geometry
        distance : float
            Distance along the line to interpolate

        Returns
        -------
        tuple
            (x, y) coordinates of interpolated point
        """
        if distance <= 0:
            return line.coords[0]
        elif distance >= line.length:
            return line.coords[-1]
        return line.interpolate(distance).coords[0]



    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_reaches(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Return the model 1D river reach lines.

        This method extracts river reach data from the HEC-RAS geometry HDF file,
        including attributes and geometry information.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        datetime_to_str : bool, optional
            If True, convert datetime objects to strings. Default is False.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the river reaches with their attributes and geometries.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    return GeoDataFrame()

                river_data = hdf_file["Geometry/River Centerlines"]
                v_conv_val = np.vectorize(HdfUtils.convert_ras_string)
                river_attrs = river_data["Attributes"][()]
                river_dict = {"river_id": range(river_attrs.shape[0])}
                river_dict.update(
                    {name: v_conv_val(river_attrs[name]) for name in river_attrs.dtype.names}
                )
                
                # Get polylines for river reaches
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, "Geometry/River Centerlines"
                )

                river_gdf = GeoDataFrame(
                    river_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path),
                )
                if datetime_to_str:
                    river_gdf["Last Edited"] = river_gdf["Last Edited"].apply(
                        lambda x: pd.Timestamp.isoformat(x)
                    )
                return river_gdf
        except Exception as e:
            logger.error(f"Error reading river reaches: {str(e)}")
            return GeoDataFrame()


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_edge_lines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Return the model river edge lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        datetime_to_str : bool, optional
            If True, convert datetime objects to strings. Default is False.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing river edge lines with their attributes and geometries.
            Each row represents a river bank (left or right) with associated attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Edge Lines" not in hdf_file:
                    logger.warning("No river edge lines found in geometry file")
                    return GeoDataFrame()

                edge_data = hdf_file["Geometry/River Edge Lines"]
                
                # Get attributes if they exist
                if "Attributes" in edge_data:
                    attrs = edge_data["Attributes"][()]
                    v_conv_val = np.vectorize(HdfUtils.convert_ras_string)
                    
                    # Create dictionary of attributes
                    edge_dict = {"edge_id": range(attrs.shape[0])}
                    edge_dict.update(
                        {name: v_conv_val(attrs[name]) for name in attrs.dtype.names}
                    )
                    
                    # Add bank side indicator
                    if edge_dict["edge_id"].size % 2 == 0:  # Ensure even number of edges
                        edge_dict["bank_side"] = ["Left", "Right"] * (edge_dict["edge_id"].size // 2)
                else:
                    edge_dict = {"edge_id": [], "bank_side": []}

                # Get polyline geometries
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Edge Lines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                edge_gdf = GeoDataFrame(
                    edge_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Convert datetime objects to strings if requested
                if datetime_to_str and 'Last Edited' in edge_gdf.columns:
                    edge_gdf["Last Edited"] = edge_gdf["Last Edited"].apply(
                        lambda x: pd.Timestamp.isoformat(x) if pd.notnull(x) else None
                    )

                # Add length calculation in project units
                if not edge_gdf.empty:
                    edge_gdf['length'] = edge_gdf.geometry.length

                return edge_gdf

        except Exception as e:
            logger.error(f"Error reading river edge lines: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_bank_lines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extract river bank lines from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, by default False

        Returns
        -------
        GeoDataFrame
            GeoDataFrame containing river bank line geometries with attributes:
            - bank_id: Unique identifier for each bank line
            - bank_side: Left or Right bank indicator
            - geometry: LineString geometry of the bank
            - length: Length of the bank line in project units
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Bank Lines" not in hdf_file:
                    logger.warning("No river bank lines found in geometry file")
                    return GeoDataFrame()

                # Get polyline geometries using existing helper method
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Bank Lines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create basic attributes
                bank_dict = {
                    "bank_id": range(len(geoms)),
                    "bank_side": ["Left", "Right"] * (len(geoms) // 2)  # Assuming pairs of left/right banks
                }

                # Create GeoDataFrame
                bank_gdf = GeoDataFrame(
                    bank_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Add length calculation in project units
                if not bank_gdf.empty:
                    bank_gdf['length'] = bank_gdf.geometry.length

                return bank_gdf

        except Exception as e:
            logger.error(f"Error reading river bank lines: {str(e)}")
            return GeoDataFrame()


==================================================

File: C:\GH\ras-commander\ras_commander\hdf\__init__.py
==================================================
"""
ras-commander HDF subpackage: HEC-RAS HDF file operations.

This subpackage provides comprehensive HDF5 file operations for HEC-RAS
plan files (.p##.hdf) and geometry files (.g##.hdf).

Classes are organized by function:

Core:
    - HdfBase: Foundation class for HDF operations
    - HdfUtils: Utility functions (time parsing, data conversion)
    - HdfPlan: Plan file information extraction

Geometry:
    - HdfMesh: 2D mesh operations (cells, faces, areas)
    - HdfXsec: Cross-section geometry extraction
    - HdfBndry: Boundary features (BC lines, breaklines, reference features)
    - HdfStruc: Structure geometry (2D structures)
    - HdfHydraulicTables: Hydraulic property tables (HTAB)

Results:
    - HdfResultsPlan: Plan results (steady/unsteady flow)
    - HdfResultsMesh: Mesh results (water surface, velocity, timeseries)
    - HdfResultsXsec: Cross-section results
    - HdfResultsBreach: Dam breach results

Infrastructure:
    - HdfPipe: Pipe network geometry and results
    - HdfPump: Pump station geometry and results
    - HdfInfiltration: Infiltration parameters

Visualization:
    - HdfPlot: General HDF plotting
    - HdfResultsPlot: Results visualization

Analysis:
    - HdfFluvialPluvial: Fluvial-pluvial boundary analysis

Lazy Loading:
    Heavy dependencies (geopandas, xarray, shapely, matplotlib, scipy) are
    lazy-loaded inside methods that need them to reduce import overhead.

Usage:
    from ras_commander import HdfResultsPlan, HdfMesh

    # Check if plan has steady results
    if HdfResultsPlan.is_steady_plan("plan.hdf"):
        wse = HdfResultsPlan.get_steady_wse("plan.hdf")

    # Get mesh cell polygons
    cells = HdfMesh.get_mesh_cell_polygons("plan.hdf")
"""

# Core classes
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .HdfPlan import HdfPlan

# Geometry classes
from .HdfMesh import HdfMesh
from .HdfXsec import HdfXsec
from .HdfBndry import HdfBndry
from .HdfStruc import HdfStruc
from .HdfHydraulicTables import HdfHydraulicTables

# Results classes
from .HdfResultsPlan import HdfResultsPlan
from .HdfResultsMesh import HdfResultsMesh
from .HdfResultsXsec import HdfResultsXsec
from .HdfResultsBreach import HdfResultsBreach

# Infrastructure classes
from .HdfPipe import HdfPipe
from .HdfPump import HdfPump
from .HdfInfiltration import HdfInfiltration

# Visualization classes
from .HdfPlot import HdfPlot
from .HdfResultsPlot import HdfResultsPlot

# Analysis classes
from .HdfFluvialPluvial import HdfFluvialPluvial

__all__ = [
    # Core
    'HdfBase', 'HdfUtils', 'HdfPlan',
    # Geometry
    'HdfMesh', 'HdfXsec', 'HdfBndry', 'HdfStruc', 'HdfHydraulicTables',
    # Results
    'HdfResultsPlan', 'HdfResultsMesh', 'HdfResultsXsec', 'HdfResultsBreach',
    # Infrastructure
    'HdfPipe', 'HdfPump', 'HdfInfiltration',
    # Visualization
    'HdfPlot', 'HdfResultsPlot',
    # Analysis
    'HdfFluvialPluvial',
]

==================================================

File: C:\GH\ras-commander\ras_commander\remote\AGENTS.md
==================================================
# AGENTS.md - Remote Execution Subpackage

This file provides guidance for AI agents working with the `ras_commander.remote` subpackage.

## Overview

The `remote` subpackage provides distributed execution capabilities for HEC-RAS simulations across local, remote, and cloud compute resources.

## Module Structure

```
ras_commander/remote/
├── __init__.py         # Exports all public classes and functions
├── RasWorker.py        # RasWorker base dataclass + init_ras_worker()
├── PsexecWorker.py     # PsexecWorker (IMPLEMENTED)
├── LocalWorker.py      # LocalWorker (stub)
├── SshWorker.py        # SshWorker (stub, requires paramiko)
├── WinrmWorker.py      # WinrmWorker (stub, requires pywinrm)
├── DockerWorker.py     # DockerWorker (stub, requires docker)
├── SlurmWorker.py      # SlurmWorker (stub)
├── AwsEc2Worker.py     # AwsEc2Worker (stub, requires boto3)
├── AzureFrWorker.py    # AzureFrWorker (stub, requires azure-*)
├── Execution.py        # compute_parallel_remote() + helpers
├── Utils.py            # Shared utilities
└── AGENTS.md           # This file
```

## Naming Convention

This subpackage follows the **PascalCase module naming** convention used throughout ras-commander:
- Module names match the primary class they contain (e.g., `RasWorker.py` contains `RasWorker` class)
- Factory functions are co-located with their classes (e.g., `init_ras_worker()` is in `RasWorker.py`)
- This pattern mirrors `RasPrj.py` which contains both `RasPrj` class and `init_ras_project()`

## Import Patterns

### Recommended (Top-Level)
```python
from ras_commander import init_ras_worker, compute_parallel_remote
```

### Direct Subpackage Import
```python
from ras_commander.remote import init_ras_worker, compute_parallel_remote
from ras_commander.remote import PsexecWorker
```

## Coding Conventions

### Internal Imports
All modules in this subpackage use **relative imports** to reference:
- Other modules within `remote/`: `from .RasWorker import RasWorker`
- Parent package modules: `from ..RasPrj import ras, RasPrj`
- Decorators and logging: `from ..Decorators import log_call`, `from ..LoggingConfig import get_logger`

### Lazy Loading for Optional Dependencies
Workers with optional dependencies implement a `check_*_dependencies()` function:
```python
def check_ssh_dependencies():
    try:
        import paramiko
        return paramiko
    except ImportError:
        raise ImportError(
            "SSH worker requires paramiko.\n"
            "Install with: pip install ras-commander[remote-ssh]"
        )
```

### Worker Implementation Pattern
Each worker module follows this pattern:
1. **Dataclass definition**: Extends `RasWorker` with worker-specific fields
2. **Validation in `__post_init__`**: Calls `super().__post_init__()`, raises `NotImplementedError` for stubs
3. **Init function**: `init_*_worker(**kwargs)` for factory routing
4. **Execute function**: `execute_*_plan(...)` for actual execution (if implemented)

### Factory Function Pattern
The `init_ras_worker()` factory function in `RasWorker.py`:
- Uses **lazy imports** inside the function body to avoid circular dependencies
- Routes to worker-specific `init_*_worker()` functions based on `worker_type`
- Auto-generates `worker_id` if not provided

```python
def init_ras_worker(worker_type: str, **kwargs) -> RasWorker:
    if worker_type == "psexec":
        from .PsexecWorker import init_psexec_worker
        return init_psexec_worker(**kwargs)
    elif worker_type == "local":
        from .LocalWorker import init_local_worker
        return init_local_worker(**kwargs)
    # ... etc
```

## Critical Implementation Notes

### PsExec Worker (The Only Implemented Worker)
- **HEC-RAS requires a desktop session**: Use `system_account=False, session_id=2`
- **Never use `system_account=True`**: HEC-RAS will hang without a desktop
- **UNC to local path conversion**: PsExec runs on remote filesystem, not UNC
- **Credentials are optional**: Windows auth preferred for GUI access

### Adding New Workers
1. Create `NewWorker.py` following PascalCase naming (module name = class name)
2. Add dataclass extending `RasWorker` from `.RasWorker`
3. Implement `check_*_dependencies()` if optional deps required
4. Add `init_*_worker()` function
5. Add execution function if implementing (not just stub)
6. Update `RasWorker.py` to route to new worker in `init_ras_worker()`
7. Update `Execution.py` to dispatch execution in `_execute_single_plan()`
8. Update `__init__.py` exports
9. Update `setup.py` extras_require if new deps

### Function Naming
- Public functions: `snake_case` (e.g., `init_ras_worker`, `compute_parallel_remote`)
- Worker init: `init_*_worker(**kwargs)` (e.g., `init_psexec_worker`)
- Execution: `execute_*_plan(worker, plan_number, ras_obj, ...)` (e.g., `execute_psexec_plan`)
- Dependency check: `check_*_dependencies()` (e.g., `check_ssh_dependencies`)

## Testing

Tests are in the example notebooks:
- `examples/23_remote_execution_psexec.ipynb` - Primary test for PsExec worker

Run the notebook to verify the remote subpackage works correctly.

## Dependencies by Worker

| Worker | Extra | Dependencies |
|--------|-------|--------------|
| PsexecWorker | (none) | Standard library only |
| SshWorker | `remote-ssh` | paramiko>=3.0 |
| WinrmWorker | `remote-winrm` | pywinrm>=0.4.3 |
| DockerWorker | `remote-docker` | docker>=6.0 |
| AwsEc2Worker | `remote-aws` | boto3>=1.28 |
| AzureFrWorker | `remote-azure` | azure-identity, azure-mgmt-compute |

## Common Issues

### Import Error on Missing Dependency
Expected behavior - clear error message directs to correct install command.

### PsExec Worker Hangs
Check `system_account` and `session_id` settings. HEC-RAS needs desktop access.

### UNC Path Errors
Ensure `local_path` matches the network share's local mount point on remote machine.

==================================================

File: C:\GH\ras-commander\ras_commander\remote\AwsEc2Worker.py
==================================================
"""
AwsEc2Worker - AWS EC2 cloud compute worker.

This module implements the AwsEc2Worker class for executing HEC-RAS on
AWS EC2 instances.

IMPLEMENTATION STATUS: STUB - Future Development

Requirements:
    pip install ras-commander[remote-aws]
    # or: pip install boto3
"""

from dataclasses import dataclass
from typing import Optional

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


def check_aws_dependencies():
    """
    Check if boto3 is available, raise clear error if not.

    This function is called lazily only when AWS functionality is actually used.
    """
    try:
        import boto3
        return boto3
    except ImportError:
        raise ImportError(
            "AWS EC2 worker requires boto3.\n"
            "Install with: pip install ras-commander[remote-aws]\n"
            "Or: pip install boto3"
        )


@dataclass
class AwsEc2Worker(RasWorker):
    """
    AWS EC2 cloud compute worker.

    IMPLEMENTATION STATUS: STUB - Future Development

    IMPLEMENTATION NOTES:
    AWS EC2 enables elastic compute capacity for burst workloads and large-scale
    parallel execution without local hardware constraints.

    When implemented, this worker will:
    1. Use boto3 library for AWS API access
    2. Launch EC2 instances on-demand or use existing instances
    3. Deploy projects via S3 or direct instance connection
    4. Execute HEC-RAS on Windows EC2 instances
    5. Collect results to S3 and optionally terminate instances
    6. Support spot instances for cost optimization

    Required Parameters:
        - region: AWS region (e.g., "us-east-1")
        - instance_type: EC2 instance type (e.g., "c5.2xlarge")
        - ami_id: AMI with HEC-RAS pre-installed
        - key_name: EC2 key pair name
        - security_group: Security group ID
        - iam_role: IAM role for S3 access
        - s3_bucket: S3 bucket for project deployment
        - spot_instance: Use spot instances (default False)

    Usage Pattern:
        aws_worker = init_ras_worker(
            "aws_ec2",
            region="us-east-1",
            instance_type="c5.4xlarge",
            ami_id="ami-hecras-6.3-windows",
            key_name="my-keypair",
            security_group="sg-xxxxxxxxx",
            iam_role="HECRASExecutionRole",
            s3_bucket="my-ras-projects",
            spot_instance=True,
            ras_exe_path=r"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe"
        )

    Dependencies:
        - boto3: AWS SDK for Python

    Cost Optimization Strategies:
        - Use spot instances for interruptible workloads
        - Terminate instances after execution
        - Use appropriate instance sizing
        - Store results in S3 Intelligent-Tiering
    """
    region: str = "us-east-1"
    instance_type: str = "c5.2xlarge"
    ami_id: str = None
    key_name: str = None
    security_group: str = None
    iam_role: str = None
    s3_bucket: str = None
    spot_instance: bool = False
    auto_terminate: bool = True

    def __post_init__(self):
        super().__post_init__()
        self.worker_type = "aws_ec2"
        raise NotImplementedError(
            "AwsEc2Worker is not yet implemented. "
            "Planned for future release. "
            "Will use boto3 for AWS EC2 cloud execution.\n"
            "Requires: pip install ras-commander[remote-aws]"
        )


def init_aws_ec2_worker(**kwargs) -> AwsEc2Worker:
    """Initialize AWS EC2 worker (stub - raises NotImplementedError)."""
    check_aws_dependencies()
    kwargs['worker_type'] = 'aws_ec2'
    return AwsEc2Worker(**kwargs)

==================================================

File: C:\GH\ras-commander\ras_commander\remote\AzureFrWorker.py
==================================================
"""
AzureFrWorker - Azure Functions/Container Instances execution worker.

This module implements the AzureFrWorker class for executing HEC-RAS on
Azure cloud infrastructure.

IMPLEMENTATION STATUS: STUB - Future Development

Requirements:
    pip install ras-commander[remote-azure]
    # or: pip install azure-identity azure-mgmt-compute
"""

from dataclasses import dataclass
from typing import Optional

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


def check_azure_dependencies():
    """
    Check if Azure SDK is available, raise clear error if not.

    This function is called lazily only when Azure functionality is actually used.
    """
    try:
        import azure.identity
        import azure.mgmt.compute
        return True
    except ImportError:
        raise ImportError(
            "Azure worker requires azure-identity and azure-mgmt-compute.\n"
            "Install with: pip install ras-commander[remote-azure]\n"
            "Or: pip install azure-identity azure-mgmt-compute"
        )


@dataclass
class AzureFrWorker(RasWorker):
    """
    Azure Functions serverless execution worker.

    IMPLEMENTATION STATUS: STUB - Future Development

    IMPLEMENTATION NOTES:
    Azure Functions enables serverless execution with automatic scaling and
    pay-per-execution pricing. Note: HEC-RAS execution may exceed typical
    Function time limits and require Durable Functions or Container Instances.

    When implemented, this worker will:
    1. Use Azure SDK for Python (azure-functions, azure-storage-blob)
    2. Deploy projects to Azure Blob Storage
    3. Trigger function execution or Container Instances
    4. Monitor execution via Azure APIs
    5. Collect results from Blob Storage
    6. Support Azure Container Instances for long-running models

    Required Parameters:
        - subscription_id: Azure subscription ID
        - resource_group: Resource group name
        - function_app: Function App name (if using Functions)
        - container_registry: Container registry (if using Container Instances)
        - storage_account: Azure Storage account name
        - storage_container: Blob container for projects
        - region: Azure region (e.g., "eastus")

    Usage Pattern:
        azure_worker = init_ras_worker(
            "azure_fr",
            subscription_id="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
            resource_group="ras-execution-rg",
            container_registry="myregistry.azurecr.io/hecras:6.3",
            storage_account="rasprojectsstorage",
            storage_container="ras-projects",
            region="eastus",
            ras_exe_path=r"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe"
        )

    Dependencies:
        - azure-functions: Azure Functions SDK
        - azure-storage-blob: Azure Blob Storage client
        - azure-identity: Azure authentication

    Considerations:
        - Azure Functions have execution time limits (10 min default, 60 min max)
        - Consider Azure Container Instances for long-running HEC-RAS models
        - Azure Batch may be more suitable for large-scale parallel workloads
        - Data transfer costs for large HDF result files
    """
    subscription_id: str = None
    resource_group: str = None
    function_app: str = None
    container_registry: str = None
    storage_account: str = None
    storage_container: str = None
    region: str = "eastus"
    use_container_instances: bool = True

    def __post_init__(self):
        super().__post_init__()
        self.worker_type = "azure_fr"
        raise NotImplementedError(
            "AzureFrWorker is not yet implemented. "
            "Planned for future release. "
            "Will use Azure SDK for serverless/container-based execution. "
            "Note: Consider Azure Batch for large-scale parallel workloads.\n"
            "Requires: pip install ras-commander[remote-azure]"
        )


def init_azure_fr_worker(**kwargs) -> AzureFrWorker:
    """Initialize Azure Functions worker (stub - raises NotImplementedError)."""
    check_azure_dependencies()
    kwargs['worker_type'] = 'azure_fr'
    return AzureFrWorker(**kwargs)

==================================================

File: C:\GH\ras-commander\ras_commander\remote\DockerWorker.py
==================================================
"""
DockerWorker - Docker containerized execution worker.

This module implements the DockerWorker class for executing HEC-RAS in
Docker containers using Rocky Linux 8 with native HEC-RAS 6.6 Linux binaries.

Workflow:
    1. Preprocess plan on Windows host (creates .tmp.hdf files)
    2. Execute simulation in Linux Docker container
    3. Copy results back to project folder

Requirements:
    pip install ras-commander[remote-docker]
    # or: pip install docker
"""

import shutil
import uuid
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Optional, Any

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger
from ..Decorators import log_call

logger = get_logger(__name__)


def check_docker_dependencies():
    """
    Check if docker is available, raise clear error if not.

    This function is called lazily only when Docker functionality is actually used.
    """
    try:
        import docker
        return docker
    except ImportError:
        raise ImportError(
            "Docker worker requires docker.\n"
            "Install with: pip install ras-commander[remote-docker]\n"
            "Or: pip install docker"
        )


@dataclass
class DockerWorker(RasWorker):
    """
    Docker containerized HEC-RAS execution worker.

    Uses Rocky Linux 8 container with native HEC-RAS 6.6 Linux binaries.
    Requires two-step workflow:
        1. Preprocess on Windows (creates .tmp.hdf files)
        2. Execute simulation in Linux container

    Attributes:
        docker_image: Docker image name/tag (e.g., "hecras:6.6")
        docker_host: Docker daemon URL (None for local, or "tcp://host:2375")
        container_input_path: Mount point for project files in container
        container_output_path: Mount point for results in container
        container_script_path: Path to run_ras.sh in container
        max_runtime_minutes: Timeout for simulation (default 480 = 8 hours)
        preprocess_on_host: Whether to run preprocessing on Windows host first
        cpu_limit: CPU limit for container (e.g., "4" or "0.5")
        memory_limit: Memory limit (e.g., "8g", "4096m")

    Example:
        >>> worker = init_docker_worker(
        ...     docker_image="hecras:6.6",
        ...     cores_total=8,
        ...     cores_per_plan=4,
        ...     preprocess_on_host=True
        ... )
    """

    # Docker configuration
    docker_image: str = None
    docker_host: Optional[str] = None

    # Container paths (Linux paths)
    container_input_path: str = "/app/input"
    container_output_path: str = "/app/output"
    container_script_path: str = "/app/scripts/core_execution/run_ras.sh"

    # Execution configuration
    max_runtime_minutes: int = 480
    preprocess_on_host: bool = True

    # Resource limits
    cpu_limit: Optional[str] = None
    memory_limit: Optional[str] = None

    # Worker configuration
    process_priority: str = "low"
    queue_priority: int = 0
    cores_total: Optional[int] = None
    cores_per_plan: int = 4
    max_parallel_plans: Optional[int] = None

    # Staging directory for file operations
    staging_directory: Optional[str] = None

    def __post_init__(self):
        """Validate Docker worker configuration."""
        super().__post_init__()
        self.worker_type = "docker"

        if not self.docker_image:
            raise ValueError("docker_image is required for DockerWorker")

        # Calculate parallel capacity
        if self.cores_total is not None and self.cores_per_plan:
            self.max_parallel_plans = max(1, self.cores_total // self.cores_per_plan)
        elif self.max_parallel_plans is None:
            self.max_parallel_plans = 1

        # Set default staging directory
        if self.staging_directory is None:
            import tempfile
            self.staging_directory = tempfile.gettempdir()

        logger.debug(f"DockerWorker initialized: image={self.docker_image}, "
                    f"host={self.docker_host or 'local'}, "
                    f"max_parallel={self.max_parallel_plans}")


@log_call
def init_docker_worker(**kwargs) -> DockerWorker:
    """
    Initialize and validate a Docker worker.

    Args:
        docker_image: Docker image with HEC-RAS Linux (required, e.g., "hecras:6.6")
        docker_host: Docker daemon URL (optional, default: local)
        worker_id: Custom worker ID (auto-generated if not provided)
        cores_total: Total CPU cores available for this worker
        cores_per_plan: CPU cores to allocate per plan
        max_runtime_minutes: Simulation timeout (default: 480)
        preprocess_on_host: Run Windows preprocessing first (default: True)
        cpu_limit: Container CPU limit
        memory_limit: Container memory limit (e.g., "8g")
        **kwargs: Additional DockerWorker parameters

    Returns:
        DockerWorker: Validated worker instance

    Raises:
        ImportError: If docker package not installed
        ValueError: If validation fails
        docker.errors.DockerException: If Docker daemon unreachable

    Example:
        >>> worker = init_docker_worker(
        ...     docker_image="hecras:6.6",
        ...     cores_total=8,
        ...     cores_per_plan=4
        ... )
    """
    docker = check_docker_dependencies()

    kwargs['worker_type'] = 'docker'

    # Default ras_exe_path for Linux container
    if 'ras_exe_path' not in kwargs:
        kwargs['ras_exe_path'] = '/app/bin/RasUnsteady'

    worker = DockerWorker(**kwargs)

    # Verify Docker daemon connectivity
    try:
        if worker.docker_host:
            client = docker.DockerClient(base_url=worker.docker_host)
        else:
            client = docker.from_env()

        client.ping()
        logger.info(f"Docker daemon connected: {worker.docker_host or 'local'}")

        # Check if image exists
        try:
            client.images.get(worker.docker_image)
            logger.info(f"Docker image found: {worker.docker_image}")
        except docker.errors.ImageNotFound:
            logger.warning(f"Docker image not found: {worker.docker_image}")
            logger.warning("Image must be built or pulled before execution")

        client.close()

    except docker.errors.DockerException as e:
        logger.error(f"Cannot connect to Docker daemon: {e}")
        raise

    logger.info(f"DockerWorker initialized:")
    logger.info(f"  Image: {worker.docker_image}")
    logger.info(f"  Host: {worker.docker_host or 'local'}")
    logger.info(f"  Preprocess on host: {worker.preprocess_on_host}")
    logger.info(f"  Max parallel plans: {worker.max_parallel_plans}")
    logger.info(f"  Timeout: {worker.max_runtime_minutes} minutes")

    return worker


def _extract_geometry_number(project_path: Path, plan_number: str) -> Optional[str]:
    """
    Extract geometry file number from plan file.

    HEC-RAS plan files reference geometry files with "Geom File=gXX" syntax.
    The geometry number is DIFFERENT from the plan number.

    Args:
        project_path: Path to project folder
        plan_number: Plan number (e.g., "01")

    Returns:
        Geometry number as string (e.g., "13") or None if not found
    """
    plan_files = list(project_path.glob(f"*.p{plan_number}"))
    if not plan_files:
        return None

    plan_file = plan_files[0]
    try:
        with open(plan_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if line.strip().startswith("Geom File="):
                    geom_ref = line.split('=')[1].strip()
                    if geom_ref.startswith('g'):
                        return geom_ref[1:]
        return None
    except Exception as e:
        logger.error(f"Error reading plan file {plan_file}: {e}")
        return None


def _preprocess_plan_for_linux(
    ras_obj,
    plan_number: str,
    project_staging: Path,
    stability_timeout: int = 10,
    max_wait: int = 120
) -> bool:
    """
    Preprocess a plan on Windows to create files needed for Linux execution.

    This runs HEC-RAS briefly on Windows to generate:
    - .tmp.hdf file (preprocessed geometry and initial conditions)
    - .b file (binary geometry)
    - .x file (execution file)

    Args:
        ras_obj: RasPrj object
        plan_number: Plan number to preprocess
        project_staging: Path to staged project copy
        stability_timeout: Seconds to wait for file stability
        max_wait: Maximum seconds to wait

    Returns:
        bool: True if preprocessing succeeded
    """
    try:
        # Import here to avoid circular imports
        from ..RasPrj import init_ras_project
        from ..RasGeo import RasGeo
        from ..RasPlan import RasPlan
        from ..RasCmdr import RasCmdr

        # Initialize the staged project
        temp_ras = init_ras_project(str(project_staging), ras_obj.ras_version)
        project_name = temp_ras.project_name

        logger.info(f"Preprocessing plan {plan_number} for Linux execution...")

        # Clear geometry preprocessor files
        logger.debug("Clearing geometry preprocessor files...")
        RasGeo.clear_geompre_files(plan_files=plan_number, ras_object=temp_ras)

        # Extract geometry number
        geometry_number = _extract_geometry_number(project_staging, plan_number)
        if not geometry_number:
            logger.error(f"Could not extract geometry number for plan {plan_number}")
            return False

        logger.info(f"Plan {plan_number} uses geometry {geometry_number}")

        # Clear existing HDF/binary files to force regeneration
        for pattern in [f"*.p{plan_number}.hdf", f"*.p{plan_number}.tmp.hdf",
                       f"*.b{plan_number}", f"*.x{geometry_number}"]:
            for f in project_staging.glob(pattern):
                try:
                    f.unlink()
                    logger.debug(f"Deleted: {f.name}")
                except:
                    pass

        # Set plan flags for preprocessing
        plan_file_path = project_staging / f"{project_name}.p{plan_number}"
        if plan_file_path.exists():
            RasPlan.update_run_flags(
                plan_number_or_path=str(plan_file_path),
                geometry_preprocessor=True,
                unsteady_flow_simulation=True,
                post_processor=True,
                ras_object=temp_ras
            )

        # Run computation (this will be terminated early)
        logger.info("Starting HEC-RAS preprocessing (early termination)...")

        # Start the computation
        success = RasCmdr.compute_plan(
            plan_number=plan_number,
            ras_object=temp_ras,
            clear_geompre=False,  # Already cleared
            num_cores=1
        )

        # Check if .tmp.hdf was created
        tmp_hdf = project_staging / f"{project_name}.p{plan_number}.tmp.hdf"
        hdf = project_staging / f"{project_name}.p{plan_number}.hdf"

        if tmp_hdf.exists():
            logger.info(f"Preprocessing complete: {tmp_hdf.name} ({tmp_hdf.stat().st_size / 1024 / 1024:.1f} MB)")
            return True
        elif hdf.exists():
            logger.info(f"HDF file exists: {hdf.name} ({hdf.stat().st_size / 1024 / 1024:.1f} MB)")
            return True

        logger.error("Preprocessing did not create HDF file")
        return False

    except Exception as e:
        logger.error(f"Preprocessing failed: {e}", exc_info=True)
        return False


@log_call
def execute_docker_plan(
    worker: DockerWorker,
    plan_number: str,
    ras_obj,
    num_cores: int,
    clear_geompre: bool,
    sub_worker_id: int = 1,
    autoclean: bool = True
) -> bool:
    """
    Execute a HEC-RAS plan in a Linux Docker container.

    Two-step workflow:
        1. Preprocess on Windows host (if preprocess_on_host=True)
        2. Run simulation in Linux container

    Args:
        worker: DockerWorker instance
        plan_number: Plan number to execute (e.g., "01")
        ras_obj: RasPrj object with project information
        num_cores: Number of cores for simulation
        clear_geompre: Whether to clear geometry preprocessor files
        sub_worker_id: Sub-worker identifier for parallel execution
        autoclean: Remove staging files after completion

    Returns:
        bool: True if execution succeeded, False otherwise
    """
    docker = check_docker_dependencies()

    project_folder = Path(ras_obj.project_folder)
    project_name = ras_obj.project_name

    logger.info(f"Starting Docker execution: plan {plan_number}, sub-worker {sub_worker_id}")

    # Create staging directory
    staging_base = Path(worker.staging_directory)
    staging_folder = staging_base / f"ras_docker_{project_name}_p{plan_number}_sw{sub_worker_id}_{uuid.uuid4().hex[:8]}"

    input_staging = staging_folder / "input"
    output_staging = staging_folder / "output"

    try:
        staging_folder.mkdir(parents=True, exist_ok=True)
        input_staging.mkdir(parents=True, exist_ok=True)
        output_staging.mkdir(parents=True, exist_ok=True)
        logger.debug(f"Created staging: {staging_folder}")

        # Copy project to staging (flat structure - files directly in input)
        logger.info(f"Copying project to staging...")
        for item in project_folder.iterdir():
            if item.is_file():
                shutil.copy2(item, input_staging / item.name)
            elif item.is_dir():
                shutil.copytree(item, input_staging / item.name, dirs_exist_ok=True)

        # Step 1: Preprocess on Windows (if enabled)
        if worker.preprocess_on_host:
            if not _preprocess_plan_for_linux(ras_obj, plan_number, input_staging):
                logger.error("Windows preprocessing failed")
                return False

        # Extract geometry number
        geometry_number = _extract_geometry_number(input_staging, plan_number)
        if not geometry_number:
            logger.error(f"Could not extract geometry number for plan {plan_number}")
            return False

        logger.info(f"Plan {plan_number} uses geometry {geometry_number}")

        # Step 2: Run in Docker container
        if worker.docker_host:
            client = docker.DockerClient(base_url=worker.docker_host)
        else:
            client = docker.from_env()

        # Volume mounts - convert Windows paths to Docker-compatible format
        input_path = str(input_staging).replace('\\', '/')
        output_path = str(output_staging).replace('\\', '/')

        volumes = {
            input_path: {'bind': worker.container_input_path, 'mode': 'rw'},
            output_path: {'bind': worker.container_output_path, 'mode': 'rw'},
        }

        # Environment variables
        environment = {
            'MAX_RUNTIME_MINUTES': str(worker.max_runtime_minutes),
            'GEOMETRY_NUMBER': geometry_number,
        }

        # Container configuration
        container_kwargs = {
            'image': worker.docker_image,
            'command': [worker.container_script_path, plan_number],
            'volumes': volumes,
            'environment': environment,
            'detach': True,
            'remove': False,
        }

        if worker.cpu_limit:
            container_kwargs['nano_cpus'] = int(float(worker.cpu_limit) * 1e9)
        if worker.memory_limit:
            container_kwargs['mem_limit'] = worker.memory_limit

        logger.info(f"Starting container: {worker.docker_image}")
        container = client.containers.run(**container_kwargs)
        container_id = container.short_id
        logger.info(f"Container started: {container_id}")

        # Wait for completion
        timeout_seconds = worker.max_runtime_minutes * 60
        start_time = time.time()

        try:
            result = container.wait(timeout=timeout_seconds)
            exit_code = result.get('StatusCode', -1)
            elapsed = time.time() - start_time

            logger.info(f"Container finished in {elapsed:.1f}s, exit code {exit_code}")

            logs = container.logs(stdout=True, stderr=True).decode('utf-8', errors='replace')
            if exit_code != 0:
                logger.error(f"Container logs:\n{logs}")
            else:
                logger.debug(f"Container logs:\n{logs}")

        except Exception as e:
            logger.error(f"Container execution failed: {e}")
            try:
                container.kill()
            except:
                pass
            return False
        finally:
            try:
                container.remove()
            except:
                pass
            client.close()

        if exit_code != 0:
            logger.error(f"Simulation failed with exit code {exit_code}")
            return False

        # Copy results back
        # Look for HDF results in both output and input staging
        result_patterns = [
            f"{project_name}.p{plan_number}*.hdf",
            f"{project_name}.p{plan_number}.tmp.hdf",
        ]

        result_files = []
        for pattern in result_patterns:
            result_files.extend(output_staging.glob(pattern))
            result_files.extend(input_staging.glob(pattern))

        # Remove duplicates
        result_files = list(set(result_files))

        if not result_files:
            logger.error(f"No HDF results found")
            return False

        for result_file in result_files:
            dest_file = project_folder / result_file.name
            logger.info(f"Copying result: {result_file.name}")
            shutil.copy2(result_file, dest_file)

        # Copy log files
        for log_pattern in ["*.log", "*.computeMsgs.txt", "ras_execution.log"]:
            for log_file in output_staging.glob(log_pattern):
                shutil.copy2(log_file, project_folder / log_file.name)
            for log_file in input_staging.glob(log_pattern):
                if not (project_folder / log_file.name).exists():
                    shutil.copy2(log_file, project_folder / log_file.name)

        logger.info(f"Docker execution completed for plan {plan_number}")
        return True

    except Exception as e:
        logger.error(f"Docker execution error: {e}", exc_info=True)
        return False

    finally:
        if autoclean and staging_folder.exists():
            try:
                shutil.rmtree(staging_folder, ignore_errors=True)
                logger.debug(f"Cleaned up staging")
            except:
                pass
        elif not autoclean:
            logger.info(f"Preserving staging: {staging_folder}")

==================================================

File: C:\GH\ras-commander\ras_commander\remote\Execution.py
==================================================
"""
Execution - Distributed parallel execution across remote workers.

This module provides the compute_parallel_remote() function for executing
HEC-RAS plans across multiple local and remote workers.

IMPLEMENTATION STATUS: ✓ FULLY IMPLEMENTED
"""

import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, List, Optional, Union
from pathlib import Path

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger
from ..Decorators import log_call

logger = get_logger(__name__)


@dataclass
class ExecutionResult:
    """
    Result of a single plan execution.

    Attributes:
        plan_number: Plan number that was executed
        worker_id: ID of worker that executed the plan
        success: True if execution completed successfully
        hdf_path: Path to output HDF file (if successful)
        error_message: Error message (if failed)
        execution_time: Time in seconds for execution
    """
    plan_number: str
    worker_id: str
    success: bool
    hdf_path: Optional[str] = None
    error_message: Optional[str] = None
    execution_time: float = 0.0


@log_call
def compute_parallel_remote(
    plan_numbers: Union[str, List[str]],
    workers: List[RasWorker],
    ras_object=None,
    num_cores: int = 4,
    clear_geompre: bool = False,
    max_concurrent: Optional[int] = None,
    autoclean: bool = True
) -> Dict[str, ExecutionResult]:
    """
    Execute HEC-RAS plans in parallel across multiple remote workers.

    Plans are distributed to workers using naive round-robin scheduling,
    respecting each worker's queue_priority (lower values execute first).

    Args:
        plan_numbers: Single plan number or list of plan numbers to execute
        workers: List of initialized worker objects (from init_ras_worker)
        ras_object: RasPrj object for the project. If None, uses global ras.
        num_cores: Number of cores to allocate per plan execution
        clear_geompre: Clear geometry preprocessor files before execution
        max_concurrent: Maximum concurrent executions (default: sum of all worker slots)
        autoclean: Delete temporary worker folders after execution (default True).
                   Set to False for debugging to preserve worker folders.

    Returns:
        Dict mapping plan_number to ExecutionResult

    Example:
        # Initialize workers
        worker1 = init_ras_worker("psexec", hostname="PC1", ...)
        worker2 = init_ras_worker("psexec", hostname="PC2", ...)

        # Execute plans
        results = compute_parallel_remote(
            ["01", "02", "03", "04"],
            workers=[worker1, worker2],
            ras_object=ras
        )

        # Check results
        for plan_num, result in results.items():
            if result.success:
                print(f"Plan {plan_num}: {result.hdf_path}")
            else:
                print(f"Plan {plan_num} failed: {result.error_message}")

    Scheduling:
        Workers are sorted by queue_priority (ascending), then plans are assigned
        round-robin to available worker slots. Workers with lower queue_priority
        (e.g., 0=local, 1=remote) are filled first.

    Multi-Core Workers:
        Workers with cores_total and cores_per_plan set can run multiple plans
        in parallel. For example, a worker with cores_total=16 and cores_per_plan=4
        can run 4 plans simultaneously. Each parallel slot is called a "sub-worker".
    """
    from ..RasPrj import ras as global_ras

    if ras_object is None:
        ras_object = global_ras

    if ras_object is None or not hasattr(ras_object, 'project_folder'):
        raise ValueError("No valid RAS project. Initialize with init_ras_project() first.")

    # Normalize plan_numbers to list
    if isinstance(plan_numbers, str):
        plan_numbers = [plan_numbers]

    if not plan_numbers:
        logger.warning("No plans to execute")
        return {}

    if not workers:
        raise ValueError("No workers provided. Initialize workers with init_ras_worker().")

    logger.info(f"Starting distributed execution of {len(plan_numbers)} plans across {len(workers)} workers")

    # Sort workers by queue_priority (lower first)
    sorted_workers = sorted(workers, key=lambda w: getattr(w, 'queue_priority', 0))

    # Build worker slot list (worker, sub_worker_id) for round-robin assignment
    worker_slots = []
    for worker in sorted_workers:
        max_parallel = getattr(worker, 'max_parallel_plans', 1) or 1
        for sub_id in range(1, max_parallel + 1):
            worker_slots.append((worker, sub_id))

    total_slots = len(worker_slots)
    logger.info(f"Total worker slots available: {total_slots}")

    # Calculate max concurrent executions
    if max_concurrent is None:
        max_concurrent = total_slots
    max_concurrent = min(max_concurrent, total_slots, len(plan_numbers))

    # Results dictionary
    results: Dict[str, ExecutionResult] = {}

    # Execute plans using thread pool
    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
        futures = {}

        for idx, plan_number in enumerate(plan_numbers):
            # Round-robin assignment to worker slots
            worker, sub_worker_id = worker_slots[idx % total_slots]

            logger.info(
                f"Submitting plan {plan_number} to worker {worker.worker_id} "
                f"(sub-worker #{sub_worker_id})"
            )

            future = executor.submit(
                _execute_single_plan,
                worker=worker,
                plan_number=plan_number,
                ras_object=ras_object,
                num_cores=num_cores,
                clear_geompre=clear_geompre,
                sub_worker_id=sub_worker_id,
                autoclean=autoclean
            )
            futures[future] = plan_number

        # Collect results as they complete
        for future in as_completed(futures):
            plan_number = futures[future]
            try:
                result = future.result()
                results[plan_number] = result

                if result.success:
                    logger.info(
                        f"Plan {plan_number} completed successfully "
                        f"({result.execution_time:.1f}s)"
                    )
                else:
                    logger.error(
                        f"Plan {plan_number} failed: {result.error_message}"
                    )

            except Exception as e:
                logger.error(f"Plan {plan_number} raised exception: {e}")
                results[plan_number] = ExecutionResult(
                    plan_number=plan_number,
                    worker_id="unknown",
                    success=False,
                    error_message=str(e)
                )

    # Summary
    successful = sum(1 for r in results.values() if r.success)
    failed = len(results) - successful
    logger.info(f"Distributed execution complete: {successful} succeeded, {failed} failed")

    return results


def _execute_single_plan(
    worker: RasWorker,
    plan_number: str,
    ras_object,
    num_cores: int,
    clear_geompre: bool,
    sub_worker_id: int,
    autoclean: bool = True
) -> ExecutionResult:
    """
    Execute a single plan on a specific worker.

    This internal function routes to the appropriate worker-specific execution
    function based on worker_type.

    Args:
        worker: Worker instance
        plan_number: Plan number to execute
        ras_object: RAS project object
        num_cores: Number of cores
        clear_geompre: Clear geompre files
        sub_worker_id: Sub-worker ID for multi-slot workers
        autoclean: Delete temporary worker folder after execution

    Returns:
        ExecutionResult with execution outcome
    """
    start_time = time.time()
    result = ExecutionResult(
        plan_number=plan_number,
        worker_id=worker.worker_id,
        success=False
    )

    try:
        # Route to worker-specific execution
        if worker.worker_type == "psexec":
            from .PsexecWorker import execute_psexec_plan
            success = execute_psexec_plan(
                worker=worker,
                plan_number=plan_number,
                ras_obj=ras_object,
                num_cores=num_cores,
                clear_geompre=clear_geompre,
                sub_worker_id=sub_worker_id,
                autoclean=autoclean
            )
            result.success = success

            if success:
                project_name = ras_object.project_name
                hdf_file = Path(ras_object.project_folder) / f"{project_name}.p{plan_number}.hdf"
                if hdf_file.exists():
                    result.hdf_path = str(hdf_file)

        elif worker.worker_type == "local":
            from .LocalWorker import execute_local_plan
            success = execute_local_plan(
                worker=worker,
                plan_number=plan_number,
                ras_obj=ras_object,
                num_cores=num_cores,
                clear_geompre=clear_geompre,
                sub_worker_id=sub_worker_id,
                autoclean=autoclean
            )
            result.success = success

            if success:
                project_name = ras_object.project_name
                hdf_file = Path(ras_object.project_folder) / f"{project_name}.p{plan_number}.hdf"
                if hdf_file.exists():
                    result.hdf_path = str(hdf_file)

        elif worker.worker_type == "ssh":
            result.error_message = "SSH worker not yet implemented"

        elif worker.worker_type == "winrm":
            result.error_message = "WinRM worker not yet implemented"

        elif worker.worker_type == "docker":
            from .DockerWorker import execute_docker_plan
            success = execute_docker_plan(
                worker=worker,
                plan_number=plan_number,
                ras_obj=ras_object,
                num_cores=num_cores,
                clear_geompre=clear_geompre,
                sub_worker_id=sub_worker_id,
                autoclean=autoclean
            )
            result.success = success

            if success:
                project_name = ras_object.project_name
                hdf_file = Path(ras_object.project_folder) / f"{project_name}.p{plan_number}.hdf"
                if hdf_file.exists():
                    result.hdf_path = str(hdf_file)
                else:
                    # Check for .tmp.hdf (Linux container output)
                    tmp_hdf = Path(ras_object.project_folder) / f"{project_name}.p{plan_number}.tmp.hdf"
                    if tmp_hdf.exists():
                        result.hdf_path = str(tmp_hdf)

        elif worker.worker_type == "slurm":
            result.error_message = "Slurm worker not yet implemented"

        elif worker.worker_type == "aws_ec2":
            result.error_message = "AWS EC2 worker not yet implemented"

        elif worker.worker_type == "azure_fr":
            result.error_message = "Azure worker not yet implemented"

        else:
            result.error_message = f"Unknown worker type: {worker.worker_type}"

    except NotImplementedError as e:
        result.error_message = str(e)
    except Exception as e:
        result.error_message = f"Execution error: {e}"
        logger.exception(f"Error executing plan {plan_number} on {worker.worker_id}")

    result.execution_time = time.time() - start_time
    return result


def get_worker_status(workers: List[RasWorker]) -> Dict[str, Dict]:
    """
    Get status summary for a list of workers.

    Args:
        workers: List of worker instances

    Returns:
        Dict mapping worker_id to status dict with keys:
        - worker_type: Type of worker
        - hostname: Target hostname
        - queue_priority: Queue priority level
        - max_parallel_plans: Max concurrent plans
        - available: True if worker is available for execution
    """
    status = {}
    for worker in workers:
        status[worker.worker_id] = {
            'worker_type': worker.worker_type,
            'hostname': getattr(worker, 'hostname', 'localhost'),
            'queue_priority': getattr(worker, 'queue_priority', 0),
            'max_parallel_plans': getattr(worker, 'max_parallel_plans', 1),
            'available': True  # Future: add connectivity check
        }
    return status

==================================================

File: C:\GH\ras-commander\ras_commander\remote\LocalWorker.py
==================================================
"""
LocalWorker - Local parallel execution worker.

This module implements the LocalWorker class for local parallel execution
using RasCmdr.compute_plan() internally.

IMPLEMENTATION STATUS: ✓ FULLY IMPLEMENTED
"""

import shutil
import uuid
from dataclasses import dataclass
from pathlib import Path

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


@dataclass
class LocalWorker(RasWorker):
    """
    Local parallel execution worker (uses RasCmdr.compute_plan internally).

    IMPLEMENTATION STATUS: ✓ FULLY IMPLEMENTED

    This worker provides a unified interface for local execution that matches
    the remote worker interface. It creates temporary worker folders, copies
    projects, executes using RasCmdr.compute_plan(), and copies results back.

    Attributes:
        worker_folder: Local path where worker folders are created (e.g., C:\\RasRemote)
        process_priority: OS process priority ("low", "below normal", "normal")
        queue_priority: Execution queue priority (0-9, lower executes first)
        cores_total: Total CPU cores available for this worker
        cores_per_plan: Cores to allocate per HEC-RAS plan
        max_parallel_plans: Max plans to run in parallel (calculated: cores_total/cores_per_plan)

    Example:
        worker = init_ras_worker(
            "local",
            worker_folder=r"C:\\RasRemote",
            process_priority="low",
            queue_priority=0,
            cores_total=8,
            cores_per_plan=2
        )
    """
    worker_folder: str = None
    process_priority: str = "low"
    queue_priority: int = 0
    cores_total: int = None
    cores_per_plan: int = 4
    max_parallel_plans: int = None

    def __post_init__(self):
        """Validate LocalWorker configuration."""
        super().__post_init__()
        self.worker_type = "local"
        self.hostname = "localhost"

        if not self.worker_folder:
            # Default to C:\RasRemote
            self.worker_folder = "C:\\RasRemote"
            logger.debug(f"Using default worker_folder: {self.worker_folder}")

        if self.process_priority not in ["low", "below normal", "normal"]:
            raise ValueError(
                f"process_priority must be 'low', 'below normal', or 'normal' "
                f"(got '{self.process_priority}'). 'low' is recommended."
            )

        if not isinstance(self.queue_priority, int) or self.queue_priority < 0 or self.queue_priority > 9:
            raise ValueError(
                f"queue_priority must be an integer from 0 to 9 (got {self.queue_priority}). "
                f"Lower values execute first. Default is 0."
            )

        # Calculate max parallel plans if cores_total specified
        if self.cores_total is not None:
            self.max_parallel_plans = self.cores_total // self.cores_per_plan
            if self.max_parallel_plans < 1:
                self.max_parallel_plans = 1
        else:
            self.max_parallel_plans = 1


def init_local_worker(**kwargs) -> LocalWorker:
    """
    Initialize local worker.

    Args:
        worker_folder: Local path where worker folders are created (default: C:\\RasRemote)
        process_priority: OS process priority ("low", "below normal", "normal")
        queue_priority: Execution queue priority (0-9)
        cores_total: Total CPU cores available
        cores_per_plan: Cores per plan (default 4)
        worker_id: Unique identifier (auto-generated if not provided)
        ras_exe_path: Path to HEC-RAS executable (obtained from ras object if not provided)

    Returns:
        LocalWorker: Configured worker ready for execution
    """
    logger.info("Initializing local worker")

    kwargs['worker_type'] = 'local'
    worker = LocalWorker(**kwargs)

    # Create worker folder if it doesn't exist
    worker_folder_path = Path(worker.worker_folder)
    worker_folder_path.mkdir(parents=True, exist_ok=True)

    logger.info(f"Local worker configured:")
    logger.info(f"  Worker folder: {worker.worker_folder}")
    logger.info(f"  RAS Exe: {worker.ras_exe_path}")
    logger.info(f"  Process Priority: {worker.process_priority}")
    logger.info(f"  Queue Priority: {worker.queue_priority}")
    if worker.max_parallel_plans > 1:
        logger.info(f"  Parallel Capacity: {worker.max_parallel_plans} plans simultaneously")
    else:
        logger.info(f"  Execution Mode: Sequential")

    return worker


def execute_local_plan(
    worker: LocalWorker,
    plan_number: str,
    ras_obj,
    num_cores: int,
    clear_geompre: bool,
    sub_worker_id: int = 1,
    autoclean: bool = True
) -> bool:
    """
    Execute a plan on a local worker using RasCmdr.compute_plan().

    Execution flow:
    1. Create temporary worker folder
    2. Copy project to worker folder
    3. Execute using RasCmdr.compute_plan()
    4. Copy results back
    5. Cleanup temporary folder (if autoclean=True)

    Args:
        worker: LocalWorker instance
        plan_number: Plan number to execute
        ras_obj: RAS project object
        num_cores: Number of cores
        clear_geompre: Clear geompre files
        sub_worker_id: Sub-worker ID for parallel execution (default 1)
        autoclean: Delete temporary worker folder after execution (default True)

    Returns:
        bool: True if successful
    """
    logger.info(f"Starting local execution of plan {plan_number} (sub-worker #{sub_worker_id})")

    project_folder = Path(ras_obj.project_folder)
    project_name = ras_obj.project_name

    # Step 1: Create temporary worker folder
    worker_folder_path = Path(worker.worker_folder)
    worker_folder_path.mkdir(parents=True, exist_ok=True)

    worker_temp_folder = worker_folder_path / f"{project_name}_{plan_number}_SW{sub_worker_id}_{uuid.uuid4().hex[:8]}"
    worker_temp_folder.mkdir(parents=True, exist_ok=True)
    logger.debug(f"Created worker folder: {worker_temp_folder}")

    try:
        # Step 2: Copy project to worker folder
        logger.info(f"Copying project to {worker_temp_folder}")
        worker_project_path = worker_temp_folder / project_name
        shutil.copytree(project_folder, worker_project_path, dirs_exist_ok=True)

        # Step 3: Execute using RasCmdr.compute_plan()
        from ..RasCmdr import RasCmdr
        from ..RasPrj import RasPrj, init_ras_project

        # Initialize project in worker folder
        logger.info(f"Initializing project in worker folder")
        temp_ras = RasPrj()
        prj_files = list(worker_project_path.glob("*.prj"))
        if not prj_files:
            logger.error(f"No .prj file found in {worker_project_path}")
            return False

        # Get version from original ras object
        ras_version = getattr(ras_obj, 'ras_version', '6.6')
        init_ras_project(str(worker_project_path), ras_version, ras_object=temp_ras)

        logger.info(f"Executing plan {plan_number} with RasCmdr.compute_plan()")
        success = RasCmdr.compute_plan(
            plan_number=plan_number,
            ras_object=temp_ras,
            clear_geompre=clear_geompre,
            num_cores=num_cores
        )

        if not success:
            logger.error(f"RasCmdr.compute_plan() returned False for plan {plan_number}")
            return False

        # Step 4: Copy results back (HDF file)
        hdf_file = worker_project_path / f"{project_name}.p{plan_number}.hdf"

        if not hdf_file.exists():
            logger.error(f"HDF file not created: {hdf_file}")
            return False

        logger.info(f"HDF file created successfully: {hdf_file}")

        dest_hdf = project_folder / hdf_file.name
        shutil.copy2(hdf_file, dest_hdf)
        logger.info(f"Copied results to {dest_hdf}")

        # Also copy any other result files (.computeMsgs.txt, etc.)
        for result_file in worker_project_path.glob(f"{project_name}.p{plan_number}.*"):
            if result_file.suffix not in ['.hdf']:  # HDF already copied
                dest_file = project_folder / result_file.name
                if not dest_file.exists() or result_file.stat().st_mtime > dest_file.stat().st_mtime:
                    shutil.copy2(result_file, dest_file)
                    logger.debug(f"Copied result file: {result_file.name}")

        # Step 5: Cleanup (if autoclean enabled)
        if autoclean:
            shutil.rmtree(worker_temp_folder, ignore_errors=True)
            logger.debug(f"Cleaned up worker folder: {worker_temp_folder}")
        else:
            logger.info(f"Preserving worker folder for debugging: {worker_temp_folder}")

        return True

    except Exception as e:
        logger.error(f"Error in local execution: {e}")
        import traceback
        logger.debug(traceback.format_exc())

        if autoclean:
            try:
                if worker_temp_folder.exists():
                    shutil.rmtree(worker_temp_folder, ignore_errors=True)
            except:
                pass
        else:
            logger.info(f"Preserving worker folder for debugging: {worker_temp_folder}")
        return False

==================================================

File: C:\GH\ras-commander\ras_commander\remote\PsexecWorker.py
==================================================
"""
PsexecWorker - Windows remote execution via Microsoft Sysinternals PsExec.

This module implements the PsexecWorker class for executing HEC-RAS on remote
Windows machines using PsExec over network shares.

IMPLEMENTATION STATUS: ✓ FULLY IMPLEMENTED
"""

import subprocess
import shutil
import time
import uuid
import urllib.request
import zipfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Optional

from .RasWorker import RasWorker
from .Utils import convert_unc_to_local_path, authenticate_network_share
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


@dataclass
class PsexecWorker(RasWorker):
    """
    PsExec-based Windows remote execution worker.

    Uses Microsoft Sysinternals PsExec to execute HEC-RAS on remote Windows machines
    via network share deployment.

    IMPLEMENTATION STATUS: ✓ FULLY IMPLEMENTED

    Attributes:
        share_path: UNC path to accessible network share (e.g., \\\\hostname\\RasRemote)
        worker_folder: Local path on remote machine that corresponds to share_path.
                      This is the actual folder path on the remote machine's filesystem.
                      Example: If share_path is \\\\hostname\\RasRemote and the share points
                      to C:\\RasRemote on the remote machine, set worker_folder="C:\\RasRemote".
                      If not specified, defaults to "C:\\{share_name}" (e.g., C:\\RasRemote).
        credentials: Dict with 'username' and 'password' for remote authentication.
                    OPTIONAL for trusted networks. When omitted (empty dict or None),
                    PsExec uses the current user's Windows authentication, which:
                    - Works on domain-joined machines with proper trust
                    - Avoids the "secondary logon" issue that prevents GUI access
                    - Is RECOMMENDED for most internal network setups
                    When credentials ARE provided, the specified user must be the same
                    user logged into the remote desktop session, or have
                    "Replace a process level token" (SeAssignPrimaryTokenPrivilege) right.
        session_id: Session ID to run in (default 2 - typical for single-user workstations)
        process_priority: OS process priority for HEC-RAS execution on remote machine.
                         Valid values: "low" (default), "below normal", "normal".
                         Recommended: "low" to minimize impact on remote user operations.
                         Note: Higher priorities (above normal, high, realtime) are NOT
                         supported to avoid impacting remote user operations.
        queue_priority: Execution queue priority level (0-9). Lower values execute first.
                       Workers at queue level 0 are fully utilized before queue level 1.
                       Default: 0. Use for tiered bursting (local=0, remote=1, cloud=2).
        system_account: Run as SYSTEM account (default False)
        psexec_path: Path to PsExec.exe (auto-detected from PATH if not specified)
        remote_temp_folder: Temporary folder name on remote machine
        cores_total: Total CPU cores available on remote machine (optional)
        cores_per_plan: Cores to allocate per HEC-RAS plan (default 4)
        max_parallel_plans: Max plans to run in parallel (calculated: cores_total/cores_per_plan)

    CRITICAL: HEC-RAS is a GUI application and REQUIRES session-based execution.
    - system_account=False (default) - Runs in user session with desktop (REQUIRED for HEC-RAS)
    - system_account=True - Runs as SYSTEM (no desktop, HEC-RAS will hang)

    Multi-Core Parallelism:
    - Set cores_total (e.g., 16) and cores_per_plan (e.g., 4) for parallel execution
    - Worker will run cores_total/cores_per_plan plans simultaneously (e.g., 4 plans)
    - Each plan gets cores_per_plan cores allocated
    - If not specified, executes plans sequentially (legacy behavior)

    Session-based execution requires additional Group Policy configuration on the remote machine.
    See REMOTE_WORKER_SETUP_GUIDE.md for complete setup instructions.

    Example:
        # RECOMMENDED: No credentials (uses Windows authentication, avoids GUI issues)
        worker = init_ras_worker(
            "psexec",
            hostname="WORKSTATION-01",
            share_path=r"\\\\WORKSTATION-01\\RasRemote",
            worker_folder=r"C:\\RasRemote",
            ras_exe_path=r"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe",
            session_id=2
        )

        # With explicit credentials (only if required by network policy)
        worker = init_ras_worker(
            "psexec",
            hostname="WORKSTATION-01",
            share_path=r"\\\\WORKSTATION-01\\RasRemote",
            worker_folder=r"C:\\RasRemote",
            credentials={"username": "DOMAIN\\\\user", "password": "SecurePass123"},
            ras_exe_path=r"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe",
            session_id=2,
            process_priority="low",
            queue_priority=0
        )
    """
    share_path: str = None
    worker_folder: str = None
    credentials: Dict[str, str] = field(default_factory=dict)
    session_id: int = 2
    process_priority: str = "low"
    queue_priority: int = 0
    system_account: bool = False
    psexec_path: str = None
    remote_temp_folder: str = None
    cores_total: int = None
    cores_per_plan: int = 4
    max_parallel_plans: int = None

    def __post_init__(self):
        """Validate PsExec worker configuration."""
        super().__post_init__()

        if not self.share_path:
            raise ValueError("share_path is required for PsExec workers")
        if not self.hostname:
            raise ValueError("hostname is required for PsExec workers")
        # Credentials are optional - if provided, must have both username and password
        if self.credentials:
            if "username" not in self.credentials or "password" not in self.credentials:
                raise ValueError("credentials must contain both 'username' and 'password' keys")
        if self.process_priority not in ["low", "below normal", "normal"]:
            raise ValueError(
                f"process_priority must be 'low', 'below normal', or 'normal' "
                f"(got '{self.process_priority}'). 'low' is recommended to minimize "
                f"impact on remote user operations."
            )
        if not isinstance(self.queue_priority, int) or self.queue_priority < 0 or self.queue_priority > 9:
            raise ValueError(
                f"queue_priority must be an integer from 0 to 9 (got {self.queue_priority}). "
                f"Lower values execute first. Default is 0."
            )

        # Auto-derive worker_folder from share_path if not specified
        if not self.worker_folder:
            share_parts = self.share_path.strip('\\').split('\\')
            if len(share_parts) >= 2:
                share_name = share_parts[1]
                self.worker_folder = f"C:\\{share_name}"
            else:
                raise ValueError(
                    f"Cannot auto-derive worker_folder from share_path '{self.share_path}'. "
                    f"Please specify worker_folder explicitly."
                )

        # Calculate max parallel plans if cores_total specified
        if self.cores_total is not None:
            self.max_parallel_plans = self.cores_total // self.cores_per_plan
            if self.max_parallel_plans < 1:
                self.max_parallel_plans = 1
        else:
            self.max_parallel_plans = 1


# =============================================================================
# PSEXEC HELPER FUNCTIONS
# =============================================================================

def find_psexec() -> str:
    """
    Find PsExec.exe on the system.

    Search order:
    1. System PATH
    2. User profile directory
    3. Common installation locations
    4. Auto-download from Microsoft Sysinternals

    Returns:
        str: Path to PsExec.exe

    Raises:
        FileNotFoundError: PsExec.exe not found and download failed
    """
    logger.debug("Searching for PsExec.exe")

    # Check PATH
    psexec_in_path = shutil.which("PsExec.exe") or shutil.which("psexec.exe")
    if psexec_in_path:
        logger.debug(f"Found PsExec in PATH: {psexec_in_path}")
        return psexec_in_path

    # Check common locations
    common_locations = [
        Path.home() / "PSTools" / "PsExec.exe",
        Path.home() / "Downloads" / "PSTools" / "PsExec.exe",
        Path("C:/PSTools/PsExec.exe"),
        Path("C:/Tools/PSTools/PsExec.exe"),
        Path("C:/Program Files/PSTools/PsExec.exe"),
        Path("C:/Program Files (x86)/PSTools/PsExec.exe"),
    ]

    for loc in common_locations:
        if loc.exists():
            logger.debug(f"Found PsExec at: {loc}")
            return str(loc)

    # Try to download
    logger.info("PsExec.exe not found. Attempting to download from Microsoft Sysinternals...")
    target_dir = Path.home() / "PSTools"
    try:
        psexec_path = download_psexec(target_dir)
        logger.info(f"Downloaded PsExec to: {psexec_path}")
        return str(psexec_path)
    except Exception as e:
        logger.error(f"Failed to download PsExec: {e}")
        raise FileNotFoundError(
            "PsExec.exe not found. Please download from "
            "https://docs.microsoft.com/en-us/sysinternals/downloads/psexec "
            "and add to PATH or specify psexec_path parameter."
        )


def download_psexec(target_dir: Path) -> Path:
    """
    Download PsExec from Microsoft Sysinternals.

    Args:
        target_dir: Directory to extract PSTools to

    Returns:
        Path: Path to PsExec.exe

    Raises:
        Exception: Download or extraction failed
    """
    url = "https://download.sysinternals.com/files/PSTools.zip"
    target_dir = Path(target_dir)
    target_dir.mkdir(parents=True, exist_ok=True)

    zip_path = target_dir / "PSTools.zip"

    try:
        logger.debug(f"Downloading {url}")
        urllib.request.urlretrieve(url, zip_path)

        logger.debug(f"Extracting to {target_dir}")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(target_dir)

        zip_path.unlink()

        psexec_exe = target_dir / "PsExec.exe"
        if not psexec_exe.exists():
            psexec_exe = target_dir / "psexec.exe"

        if not psexec_exe.exists():
            raise FileNotFoundError("PsExec.exe not found after extraction")

        return psexec_exe

    except Exception as e:
        if zip_path.exists():
            zip_path.unlink()
        raise e


def init_psexec_worker(**kwargs) -> PsexecWorker:
    """
    Initialize PsExec worker.

    NOTE: Full validation (share access, remote connectivity) is deferred until
    execution time. This prevents false failures during initialization due to
    authentication and UAC complexities.

    Validation performed:
    1. Check PsExec.exe is available locally

    Validation deferred to execution:
    2. Network share accessibility (requires authenticated session)
    3. Remote execution permissions (depends on UAC, firewall, services)
    4. HEC-RAS.exe existence on remote machine

    Returns:
        PsexecWorker: Configured worker ready for execution

    Raises:
        FileNotFoundError: PsExec.exe not found locally
    """
    logger.info(f"Initializing PsExec worker for {kwargs.get('hostname', 'unknown')}")

    kwargs['worker_type'] = 'psexec'
    worker = PsexecWorker(**kwargs)

    # Find or validate PsExec.exe locally
    if not worker.psexec_path:
        worker.psexec_path = find_psexec()
    else:
        if not Path(worker.psexec_path).exists():
            raise FileNotFoundError(f"PsExec.exe not found at {worker.psexec_path}")

    logger.debug(f"Using PsExec at: {worker.psexec_path}")

    # Log configuration (obfuscate credentials)
    logger.info(f"PsExec worker configured:")
    logger.info(f"  Hostname: {worker.hostname}")
    logger.info(f"  Share path: {worker.share_path}")
    logger.info(f"  Worker folder: {worker.worker_folder}")
    if worker.credentials:
        logger.info(f"  User: {worker.credentials.get('username', '<unknown>')}")
    else:
        logger.info(f"  User: <Windows authentication>")
    logger.info(f"  System account: {worker.system_account}")
    logger.info(f"  Session ID: {worker.session_id if not worker.system_account else 'N/A'}")
    logger.info(f"  Process Priority: {worker.process_priority}")
    logger.info(f"  Queue Priority: {worker.queue_priority}")
    logger.warning(
        f"Validation deferred - share access and remote execution will be "
        f"tested during actual plan execution"
    )

    return worker


def execute_psexec_plan(
    worker: PsexecWorker,
    plan_number: str,
    ras_obj,
    num_cores: int,
    clear_geompre: bool,
    sub_worker_id: int = 1,
    autoclean: bool = True
) -> bool:
    """
    Execute a plan on a PsExec worker.

    Execution flow:
    1. Authenticate to network share (if credentials provided)
    2. Create temporary worker folder in network share
    3. Copy project to worker folder
    4. Generate batch file for HEC-RAS execution
    5. Execute batch file via PsExec
    6. Monitor execution (poll for .hdf file)
    7. Copy results back
    8. Cleanup temporary folder (if autoclean=True)

    Args:
        worker: PsexecWorker instance
        plan_number: Plan number to execute
        ras_obj: RAS project object
        num_cores: Number of cores
        clear_geompre: Clear geompre files
        sub_worker_id: Sub-worker ID for parallel execution (default 1)
        autoclean: Delete temporary worker folder after execution (default True).
                   Set to False for debugging to preserve worker folders.

    Returns:
        bool: True if successful
    """
    logger.info(f"Starting PsExec execution of plan {plan_number} (sub-worker #{sub_worker_id})")

    project_folder = Path(ras_obj.project_folder)
    project_name = ras_obj.project_name

    # Step 0: Authenticate to network share
    if worker.credentials:
        auth_success = authenticate_network_share(
            worker.share_path,
            worker.credentials["username"],
            worker.credentials["password"]
        )
        if not auth_success:
            logger.error(f"Failed to authenticate to share {worker.share_path}")
            return False

    # Step 1: Create temporary worker folder
    worker_temp_folder = Path(worker.share_path) / f"{project_name}_{plan_number}_SW{sub_worker_id}_{uuid.uuid4().hex[:8]}"
    worker_temp_folder.mkdir(parents=True, exist_ok=True)
    logger.debug(f"Created worker folder: {worker_temp_folder}")

    try:
        # Step 2: Copy project to worker folder
        logger.info(f"Copying project to {worker_temp_folder}")
        shutil.copytree(project_folder, worker_temp_folder / project_name, dirs_exist_ok=True)

        worker_project_path = worker_temp_folder / project_name
        prj_file = list(worker_project_path.glob("*.prj"))[0]
        plan_file = worker_project_path / f"{project_name}.p{plan_number}"

        # Step 3: Generate batch file
        prj_file_local = convert_unc_to_local_path(str(prj_file), worker.share_path, worker.worker_folder)
        plan_file_local = convert_unc_to_local_path(str(plan_file), worker.share_path, worker.worker_folder)

        batch_file = worker_temp_folder / f"run_plan_{plan_number}.bat"
        batch_content = f'"{worker.ras_exe_path}" -c "{prj_file_local}" "{plan_file_local}"'
        batch_file.write_text(batch_content)
        logger.debug(f"Created batch file: {batch_file}")
        logger.debug(f"Batch file content: {batch_content}")

        # Step 4: Build PsExec command
        psexec_cmd = [
            worker.psexec_path,
            f"\\\\{worker.hostname}",
        ]

        # Add credentials only if provided (otherwise uses Windows authentication)
        if worker.credentials:
            psexec_cmd.extend(["-u", worker.credentials["username"]])
            psexec_cmd.extend(["-p", worker.credentials["password"]])

        psexec_cmd.extend(["-accepteula", "-h"])

        if worker.system_account:
            psexec_cmd.append("-s")
        else:
            psexec_cmd.extend(["-i", str(worker.session_id)])

        priority_flags = {
            "low": "-low",
            "below normal": "-belownormal",
            "normal": ""
        }
        priority_flag = priority_flags.get(worker.process_priority, "")
        if priority_flag:
            psexec_cmd.append(priority_flag)

        batch_file_local = convert_unc_to_local_path(str(batch_file), worker.share_path, worker.worker_folder)
        psexec_cmd.append(batch_file_local)

        if worker.credentials:
            cmd_display = ' '.join(psexec_cmd[:2]) + " -u <user> -p <password> -accepteula -h ..."
        else:
            cmd_display = ' '.join(psexec_cmd[:2]) + " -accepteula -h ..."
        logger.info(f"Executing: {cmd_display}")

        # Step 5: Execute PsExec command
        result = subprocess.run(
            psexec_cmd,
            capture_output=True,
            text=True,
            timeout=7200
        )

        # Step 6: Check for HDF file
        hdf_file = worker_project_path / f"{project_name}.p{plan_number}.hdf"

        max_wait = 60
        wait_interval = 5
        elapsed = 0

        while not hdf_file.exists() and elapsed < max_wait:
            time.sleep(wait_interval)
            elapsed += wait_interval
            logger.debug(f"Waiting for HDF file... ({elapsed}s)")

        if not hdf_file.exists():
            logger.error(f"HDF file not created: {hdf_file}")
            logger.error(f"PsExec stdout: {result.stdout}")
            logger.error(f"PsExec stderr: {result.stderr}")
            return False

        logger.info(f"HDF file created successfully: {hdf_file}")

        # Step 7: Copy results back
        dest_hdf = project_folder / hdf_file.name
        shutil.copy2(hdf_file, dest_hdf)
        logger.info(f"Copied results to {dest_hdf}")

        # Step 8: Cleanup (if autoclean enabled)
        if autoclean:
            shutil.rmtree(worker_temp_folder, ignore_errors=True)
            logger.debug(f"Cleaned up worker folder: {worker_temp_folder}")
        else:
            logger.info(f"Preserving worker folder for debugging: {worker_temp_folder}")

        return True

    except Exception as e:
        logger.error(f"Error in PsExec execution: {e}")
        if autoclean:
            try:
                if worker_temp_folder.exists():
                    shutil.rmtree(worker_temp_folder, ignore_errors=True)
            except:
                pass
        else:
            logger.info(f"Preserving worker folder for debugging: {worker_temp_folder}")
        return False

==================================================

File: C:\GH\ras-commander\ras_commander\remote\RasWorker.py
==================================================
"""
RasWorker - Base class and factory function for remote execution workers.

This module provides the RasWorker base dataclass and the init_ras_worker()
factory function for creating remote execution workers of various types.

Pattern follows RasPrj.py which contains both RasPrj class and init_ras_project().
"""

import json
import uuid
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

from ..LoggingConfig import get_logger
from ..Decorators import log_call

logger = get_logger(__name__)


# =============================================================================
# WORKER BASE CLASS
# =============================================================================

@dataclass
class RasWorker:
    """
    Base class for remote execution workers.

    All worker types inherit from this base class and implement type-specific
    connection, deployment, and execution logic.

    Attributes:
        worker_type: Type identifier ("psexec", "ssh", "local", etc.)
        worker_id: Unique identifier for this worker instance
        hostname: Remote machine hostname or IP (None for local)
        ras_exe_path: Path to HEC-RAS.exe on target machine (optional, obtained from ras object)
        capabilities: Dict of worker capabilities (cores, memory, etc.)
        metadata: Additional worker-specific configuration
    """
    worker_type: str
    worker_id: str = None
    hostname: Optional[str] = None
    ras_exe_path: str = None
    capabilities: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """Validate worker configuration after initialization."""
        if not self.worker_type:
            raise ValueError("worker_type is required")
        # Note: ras_exe_path is optional - will be obtained from ras object during execution


# =============================================================================
# WORKER FACTORY FUNCTION
# =============================================================================

@log_call
def init_ras_worker(
    worker_type: str,
    ras_object=None,
    **kwargs
) -> RasWorker:
    """
    Initialize and validate a remote execution worker.

    This factory function creates worker objects of various types, validates
    connectivity, and ensures HEC-RAS is available on the target system.

    Args:
        worker_type: Type of worker - "psexec", "local", "ssh", "winrm", "docker",
                     "slurm", "aws_ec2", "azure_fr"
        ras_object: RasPrj object to get ras_exe_path from. If None, uses global ras.
        **kwargs: Worker-type specific configuration parameters

    Common kwargs (all worker types):
        ras_exe_path: Path to HEC-RAS.exe on target machine (optional - obtained from ras object if not provided)
        worker_id: Unique identifier (auto-generated if not provided)

    PsExec-specific kwargs:
        hostname: Remote machine hostname or IP (required)
        share_path: UNC path to network share (required, e.g., \\\\hostname\\RasRemote)
        worker_folder: Local path on remote machine corresponding to share_path (optional).
                      If not specified, defaults to C:\\{share_name} (e.g., C:\\RasRemote).
                      Set this if your share points to a different local path.
        credentials: Dict with 'username' and 'password' (OPTIONAL - recommended to omit).
                    When omitted, uses Windows authentication which avoids GUI access issues.
                    Only provide if your network requires explicit authentication.
        session_id: Session ID to run in (default 2). Use "query user" on remote to check.
        process_priority: OS process priority for HEC-RAS execution.
                         Valid values: "low" (default), "below normal", "normal".
                         Recommended: "low" to minimize impact on remote user operations.
        queue_priority: Execution queue priority (0-9). Lower values execute first.
                       Workers at queue level 0 are filled before queue level 1, etc.
                       Default: 0. Use for tiered bursting (local=0, remote=1, cloud=2).
        system_account: Run as SYSTEM (default False)
        psexec_path: Path to PsExec.exe (auto-detected if not provided)

    Returns:
        RasWorker: Initialized and validated worker object ready for execution

    Raises:
        ValueError: Invalid worker_type or missing required parameters
        ConnectionError: Cannot connect to remote machine
        FileNotFoundError: PsExec.exe or RAS.exe not found
        PermissionError: Insufficient permissions for remote execution
        NotImplementedError: Worker type not yet implemented

    Example:
        # Initialize PsExec worker - ras_exe_path obtained from ras object
        worker = init_ras_worker(
            "psexec",
            hostname="WORKSTATION-01",
            share_path=r"\\\\WORKSTATION-01\\RasRemote",
            session_id=2  # Check with "query user" on remote machine
        )

        # Or with explicit ras_object
        worker = init_ras_worker(
            "psexec",
            ras_object=my_ras_project,
            hostname="WORKSTATION-01",
            share_path=r"\\\\WORKSTATION-01\\RasRemote"
        )
    """
    logger.info(f"Initializing {worker_type} worker")

    # Validate worker_type
    valid_types = ["psexec", "local", "ssh", "winrm", "docker", "slurm", "aws_ec2", "azure_fr"]
    if worker_type not in valid_types:
        raise ValueError(
            f"Invalid worker_type '{worker_type}'. "
            f"Valid types: {', '.join(valid_types)}"
        )

    # Get ras_exe_path from ras object if not provided
    if "ras_exe_path" not in kwargs or kwargs.get("ras_exe_path") is None:
        from ..RasPrj import ras as global_ras, get_ras_exe

        ras_obj = ras_object if ras_object is not None else global_ras

        if ras_obj is not None and hasattr(ras_obj, 'ras_exe_path') and ras_obj.ras_exe_path:
            kwargs["ras_exe_path"] = ras_obj.ras_exe_path
            logger.debug(f"Using ras_exe_path from ras object: {kwargs['ras_exe_path']}")
        else:
            # Try to get from get_ras_exe() which uses default paths
            try:
                kwargs["ras_exe_path"] = get_ras_exe()
                logger.debug(f"Using ras_exe_path from get_ras_exe(): {kwargs['ras_exe_path']}")
            except Exception:
                logger.warning("Could not determine ras_exe_path - will need to be set before execution")

    # Auto-generate worker_id if not provided
    if "worker_id" not in kwargs or kwargs.get("worker_id") is None:
        kwargs["worker_id"] = f"{worker_type}_{uuid.uuid4().hex[:8]}"

    # Route to appropriate worker initialization
    # Using lazy imports inside function to avoid circular dependencies
    if worker_type == "psexec":
        from .PsexecWorker import init_psexec_worker
        return init_psexec_worker(**kwargs)

    elif worker_type == "local":
        from .LocalWorker import init_local_worker
        return init_local_worker(**kwargs)

    elif worker_type == "ssh":
        from .SshWorker import init_ssh_worker
        return init_ssh_worker(**kwargs)

    elif worker_type == "winrm":
        from .WinrmWorker import init_winrm_worker
        return init_winrm_worker(**kwargs)

    elif worker_type == "docker":
        from .DockerWorker import init_docker_worker
        return init_docker_worker(**kwargs)

    elif worker_type == "slurm":
        from .SlurmWorker import init_slurm_worker
        return init_slurm_worker(**kwargs)

    elif worker_type == "aws_ec2":
        from .AwsEc2Worker import init_aws_ec2_worker
        return init_aws_ec2_worker(**kwargs)

    elif worker_type == "azure_fr":
        from .AzureFrWorker import init_azure_fr_worker
        return init_azure_fr_worker(**kwargs)


@log_call
def load_workers_from_json(
    json_path: Union[str, Path],
    ras_object=None,
    enabled_only: bool = True
) -> List[RasWorker]:
    """
    Load worker configurations from a JSON file.

    Args:
        json_path: Path to JSON file containing worker configurations
        ras_object: RasPrj object to get ras_exe_path from. If None, uses global ras.
        enabled_only: If True, only load workers with "enabled": true (default True)

    Returns:
        List[RasWorker]: List of initialized worker objects

    JSON Format:
        {
            "workers": [
                {
                    "name": "Local Compute",
                    "worker_type": "local",
                    "worker_folder": "C:\\\\RasRemote",
                    "process_priority": "low",
                    "queue_priority": 0,
                    "cores_total": 4,
                    "cores_per_plan": 2,
                    "enabled": true
                },
                {
                    "name": "Remote PC",
                    "worker_type": "psexec",
                    "hostname": "192.168.1.100",
                    "share_path": "\\\\192.168.1.100\\RasRemote",
                    "worker_folder": "C:\\\\RasRemote",
                    "username": ".\\user",
                    "password": "password",
                    "session_id": 2,
                    "process_priority": "low",
                    "queue_priority": 0,
                    "cores_total": 8,
                    "cores_per_plan": 2,
                    "enabled": true
                }
            ]
        }

    Example:
        workers = load_workers_from_json("RemoteWorkers.json")
        results = compute_parallel_remote(["01", "02"], workers=workers)
    """
    json_path = Path(json_path)

    if not json_path.exists():
        raise FileNotFoundError(f"Worker configuration file not found: {json_path}")

    with open(json_path, 'r') as f:
        config = json.load(f)

    if "workers" not in config:
        raise ValueError("JSON file must contain a 'workers' array")

    workers = []
    for worker_config in config["workers"]:
        # Check if enabled
        if enabled_only and not worker_config.get("enabled", True):
            logger.debug(f"Skipping disabled worker: {worker_config.get('name', 'unnamed')}")
            continue

        # Extract worker_type (required)
        worker_type = worker_config.get("worker_type")
        if not worker_type:
            logger.warning(f"Skipping worker without worker_type: {worker_config.get('name', 'unnamed')}")
            continue

        # Build kwargs from config, excluding non-parameter fields
        kwargs = {}
        exclude_fields = {"name", "worker_type", "enabled"}

        for key, value in worker_config.items():
            if key not in exclude_fields:
                # Handle credentials - convert username/password to credentials dict
                if key == "username":
                    if "credentials" not in kwargs:
                        kwargs["credentials"] = {}
                    kwargs["credentials"]["username"] = value
                elif key == "password":
                    if "credentials" not in kwargs:
                        kwargs["credentials"] = {}
                    kwargs["credentials"]["password"] = value
                else:
                    kwargs[key] = value

        # Use name as worker_id if provided
        if "name" in worker_config:
            kwargs["worker_id"] = worker_config["name"]

        try:
            worker = init_ras_worker(worker_type, ras_object=ras_object, **kwargs)
            workers.append(worker)
            logger.info(f"Loaded worker: {worker.worker_id} ({worker_type})")
        except NotImplementedError as e:
            logger.warning(f"Skipping unimplemented worker type '{worker_type}': {e}")
        except Exception as e:
            logger.error(f"Failed to initialize worker '{worker_config.get('name', 'unnamed')}': {e}")

    logger.info(f"Loaded {len(workers)} workers from {json_path}")
    return workers

==================================================

File: C:\GH\ras-commander\ras_commander\remote\RemoteWorkers.json.template
==================================================
{
    "description": "Worker configuration template for ras-commander remote execution. Copy to RemoteWorkers.json and configure your workers.",
    "workers": [
        {
            "name": "Local Compute",
            "worker_type": "local",
            "worker_folder": "C:\\RasRemote",
            "process_priority": "low",
            "queue_priority": 0,
            "cores_total": 4,
            "cores_per_plan": 2,
            "enabled": true
        },
        {
            "name": "Remote-PC-01",
            "worker_type": "psexec",
            "hostname": "192.168.1.100",
            "share_path": "\\\\192.168.1.100\\RasRemote",
            "worker_folder": "C:\\RasRemote",
            "username": ".\\username",
            "password": "your_password_here",
            "ras_exe_path": "C:\\Program Files\\HEC\\HEC-RAS\\6.6\\RAS.exe",
            "session_id": 2,
            "process_priority": "low",
            "queue_priority": 1,
            "cores_total": 8,
            "cores_per_plan": 4,
            "enabled": false
        }
    ],
    "_comments": {
        "worker_types": "Available: local, psexec. Planned: ssh, winrm, docker, slurm, aws_ec2, azure_fr",
        "worker_folder": "Local path where temporary worker folders are created during execution",
        "share_path": "(psexec only) UNC path to network share that maps to worker_folder",
        "process_priority": "OS process priority: 'low', 'below normal', or 'normal'. 'low' recommended.",
        "queue_priority": "Execution priority 0-9. Lower values execute first. Use 0 for local, 1+ for remote.",
        "cores_total": "Total CPU cores available on this worker",
        "cores_per_plan": "Cores to allocate per HEC-RAS plan. max_parallel_plans = cores_total / cores_per_plan",
        "session_id": "(psexec only) Desktop session ID on remote machine. Use 'query user' to find.",
        "enabled": "Set to false to skip this worker during execution"
    }
}

==================================================

File: C:\GH\ras-commander\ras_commander\remote\SlurmWorker.py
==================================================
"""
SlurmWorker - Slurm HPC cluster execution worker.

This module implements the SlurmWorker class for executing HEC-RAS on
HPC clusters using the Slurm job scheduler.

IMPLEMENTATION STATUS: STUB - Future Development
"""

from dataclasses import dataclass
from typing import Optional

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


@dataclass
class SlurmWorker(RasWorker):
    """
    Slurm HPC cluster execution worker.

    IMPLEMENTATION STATUS: STUB - Future Development

    IMPLEMENTATION NOTES:
    Slurm is a common job scheduler for HPC clusters and enables large-scale
    parallel execution across cluster nodes.

    When implemented, this worker will:
    1. Submit HEC-RAS jobs to Slurm queue using sbatch
    2. Monitor job status using squeue/sacct
    3. Use shared filesystem (NFS/Lustre) for project access
    4. Support array jobs for multiple plan execution
    5. Handle node allocation and resource requests

    Required Parameters:
        - partition: Slurm partition name
        - nodes: Number of nodes to request
        - cpus_per_task: CPUs per task
        - memory: Memory per node
        - time_limit: Wall time limit
        - shared_fs_path: Shared filesystem path accessible to all nodes
        - job_name_prefix: Prefix for Slurm job names

    Usage Pattern:
        slurm_worker = init_ras_worker(
            "slurm",
            partition="compute",
            nodes=4,
            cpus_per_task=8,
            memory="32G",
            time_limit="02:00:00",
            shared_fs_path="/mnt/shared/ras_projects",
            ras_exe_path="/software/hecras/6.3/RAS.exe"
        )

    Dependencies:
        - pyslurm or subprocess for sbatch/squeue commands

    Typical Use Case:
        - Large ensemble runs (100+ rainfall events)
        - Complex 2D models requiring significant compute time
        - Research institutions with HPC infrastructure
    """
    partition: str = None
    nodes: int = 1
    cpus_per_task: int = 8
    memory: str = "32G"
    time_limit: str = "02:00:00"
    shared_fs_path: str = None
    job_name_prefix: str = "ras_job"

    def __post_init__(self):
        super().__post_init__()
        self.worker_type = "slurm"
        raise NotImplementedError(
            "SlurmWorker is not yet implemented. "
            "Planned for future release. "
            "Will use pyslurm or subprocess for HPC cluster execution."
        )


def init_slurm_worker(**kwargs) -> SlurmWorker:
    """Initialize Slurm worker (stub - raises NotImplementedError)."""
    kwargs['worker_type'] = 'slurm'
    return SlurmWorker(**kwargs)

==================================================

File: C:\GH\ras-commander\ras_commander\remote\SshWorker.py
==================================================
"""
SshWorker - SSH-based remote execution for Linux/Mac systems.

This module implements the SshWorker class for executing HEC-RAS on remote
machines via SSH connections.

IMPLEMENTATION STATUS: STUB - Future Development

Requirements:
    pip install ras-commander[remote-ssh]
    # or: pip install paramiko
"""

from dataclasses import dataclass
from typing import Optional

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


def check_ssh_dependencies():
    """
    Check if paramiko is available, raise clear error if not.

    This function is called lazily only when SSH functionality is actually used.
    """
    try:
        import paramiko
        return paramiko
    except ImportError:
        raise ImportError(
            "SSH worker requires paramiko.\n"
            "Install with: pip install ras-commander[remote-ssh]\n"
            "Or: pip install paramiko"
        )


@dataclass
class SshWorker(RasWorker):
    """
    SSH-based remote execution worker for Linux/Mac systems.

    IMPLEMENTATION STATUS: STUB - Future Development

    IMPLEMENTATION NOTES:
    When implemented, this worker will:
    1. Use paramiko library for SSH connections
    2. Deploy projects via scp or rsync
    3. Execute HEC-RAS using SSH remote command execution
    4. Support SSH key-based or password authentication
    5. Work with Linux/Mac HEC-RAS installations (if available) or Wine

    Required Parameters:
        - hostname: SSH server hostname/IP
        - port: SSH port (default 22)
        - username: SSH username
        - auth_method: "password" or "key"
        - password or key_path: Authentication credentials
        - remote_path: Remote directory for project deployment

    Usage Pattern:
        ssh_worker = init_ras_worker(
            "ssh",
            hostname="linux-server.example.com",
            port=22,
            username="user",
            auth_method="key",
            key_path="/home/user/.ssh/id_rsa",
            remote_path="/tmp/ras_runs",
            ras_exe_path="/opt/hecras/bin/ras"
        )

    Dependencies:
        - paramiko: SSH client library
        - scp or subprocess for rsync: File transfer
    """
    port: int = 22
    username: str = None
    auth_method: str = "password"
    password: str = None
    key_path: str = None
    remote_path: str = None

    def __post_init__(self):
        super().__post_init__()
        self.worker_type = "ssh"
        raise NotImplementedError(
            "SshWorker is not yet implemented. "
            "Planned for future release. "
            "Will use paramiko for SSH connections and scp/rsync for file transfer.\n"
            "Requires: pip install ras-commander[remote-ssh]"
        )


def init_ssh_worker(**kwargs) -> SshWorker:
    """Initialize SSH worker (stub - raises NotImplementedError)."""
    check_ssh_dependencies()
    kwargs['worker_type'] = 'ssh'
    return SshWorker(**kwargs)

==================================================

File: C:\GH\ras-commander\ras_commander\remote\Utils.py
==================================================
"""
Utils - Shared utilities for remote execution operations.

This module contains helper functions used across multiple worker implementations.
"""

import subprocess
from pathlib import Path

from ..LoggingConfig import get_logger

logger = get_logger(__name__)


def convert_unc_to_local_path(unc_path: str, share_path: str, local_path: str) -> str:
    """
    Convert UNC path to local path on remote machine.

    PsExec executes commands on the remote machine's local filesystem, so UNC paths
    must be converted to the corresponding local paths.

    Args:
        unc_path: Full UNC path (e.g., \\\\192.168.3.8\\RasRemote\\folder\\file.bat)
        share_path: Base share path (e.g., \\\\192.168.3.8\\RasRemote)
        local_path: Local path on remote machine that share_path maps to (e.g., C:\\RasRemote)

    Returns:
        str: Local path on remote machine (e.g., C:\\RasRemote\\folder\\file.bat)

    Example:
        >>> convert_unc_to_local_path(
        ...     r"\\\\192.168.3.8\\RasRemote\\temp\\file.bat",
        ...     r"\\\\192.168.3.8\\RasRemote",
        ...     r"C:\\RasRemote"
        ... )
        'C:\\\\RasRemote\\\\temp\\\\file.bat'
    """
    # Normalize paths (handle both \\ and \ separators)
    unc_normalized = unc_path.replace('/', '\\')
    share_normalized = share_path.replace('/', '\\').rstrip('\\')
    local_normalized = local_path.replace('/', '\\').rstrip('\\')

    # Replace the share path prefix with local path
    if unc_normalized.lower().startswith(share_normalized.lower()):
        relative_part = unc_normalized[len(share_normalized):]
        return local_normalized + relative_part
    else:
        # If UNC path doesn't start with share_path, return as-is
        logger.warning(
            f"UNC path '{unc_path}' doesn't start with share_path '{share_path}'. "
            f"Returning path as-is."
        )
        return unc_path


def authenticate_network_share(share_path: str, username: str, password: str) -> bool:
    """
    Authenticate to a network share using net use command.

    This establishes a connection to the remote share using the provided credentials,
    allowing subsequent file operations (copy, mkdir) to succeed.

    Args:
        share_path: UNC path to share (e.g., \\\\hostname\\ShareName)
        username: Username for authentication (e.g., .\\user or DOMAIN\\user)
        password: Password for authentication

    Returns:
        bool: True if authentication succeeded or share already accessible
    """
    # Extract base share path (\\hostname\ShareName) from full path
    share_parts = share_path.strip('\\').split('\\')
    if len(share_parts) >= 2:
        base_share = f"\\\\{share_parts[0]}\\{share_parts[1]}"
    else:
        base_share = share_path

    # First, try to disconnect any existing connection (ignore errors)
    try:
        subprocess.run(
            ["net", "use", base_share, "/delete", "/y"],
            capture_output=True,
            timeout=30
        )
    except Exception:
        pass

    # Establish new connection with credentials
    try:
        result = subprocess.run(
            ["net", "use", base_share, f"/user:{username}", password],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            logger.debug(f"Successfully authenticated to {base_share}")
            return True
        else:
            # Check if already connected (error 1219 = multiple connections not allowed)
            if "1219" in result.stderr or "already" in result.stderr.lower():
                logger.debug(f"Share {base_share} already connected")
                return True
            logger.error(f"Failed to authenticate to {base_share}: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        logger.error(f"Timeout authenticating to {base_share}")
        return False
    except Exception as e:
        logger.error(f"Error authenticating to {base_share}: {e}")
        return False

==================================================

File: C:\GH\ras-commander\ras_commander\remote\WinrmWorker.py
==================================================
"""
WinrmWorker - Windows Remote Management (WinRM) worker.

This module implements the WinrmWorker class for executing HEC-RAS on remote
Windows machines using the native WinRM protocol.

IMPLEMENTATION STATUS: STUB - Future Development

Requirements:
    pip install ras-commander[remote-winrm]
    # or: pip install pywinrm
"""

from dataclasses import dataclass
from typing import Optional

from .RasWorker import RasWorker
from ..LoggingConfig import get_logger

logger = get_logger(__name__)


def check_winrm_dependencies():
    """
    Check if pywinrm is available, raise clear error if not.

    This function is called lazily only when WinRM functionality is actually used.
    """
    try:
        import winrm
        return winrm
    except ImportError:
        raise ImportError(
            "WinRM worker requires pywinrm.\n"
            "Install with: pip install ras-commander[remote-winrm]\n"
            "Or: pip install pywinrm"
        )


@dataclass
class WinrmWorker(RasWorker):
    """
    Windows Remote Management (WinRM) worker.

    IMPLEMENTATION STATUS: STUB - Future Development

    IMPLEMENTATION NOTES:
    WinRM is the native Windows remote management protocol and may provide
    better performance than PsExec in enterprise environments with proper
    WinRM configuration.

    When implemented, this worker will:
    1. Use pywinrm library for Windows remote management
    2. Deploy projects via network shares or WinRM file copy
    3. Execute HEC-RAS using WinRM remote command execution
    4. Support Kerberos, NTLM, or CredSSP authentication
    5. Require WinRM to be enabled on target machines

    Required Parameters:
        - hostname: Windows machine hostname/IP
        - username: Windows username (domain\\user format)
        - password: Windows password
        - auth: Authentication method ("ntlm", "kerberos", "credssp")
        - transport: Transport protocol ("http" or "https")
        - share_path: UNC path for file deployment

    Usage Pattern:
        winrm_worker = init_ras_worker(
            "winrm",
            hostname="WORKSTATION-01",
            username="DOMAIN\\\\user",
            password="password",
            auth="ntlm",
            transport="https",
            share_path=r"\\\\WORKSTATION-01\\Temp\\RAS_Runs",
            ras_exe_path=r"C:\\Program Files\\HEC\\HEC-RAS\\6.3\\RAS.exe"
        )

    Dependencies:
        - pywinrm: Windows Remote Management client library

    Advantages over PsExec:
        - Native Windows protocol (no external tool required)
        - Better integration with Windows security
        - Can use Kerberos for enterprise authentication
    """
    username: str = None
    password: str = None
    auth: str = "ntlm"
    transport: str = "https"
    share_path: str = None

    def __post_init__(self):
        super().__post_init__()
        self.worker_type = "winrm"
        raise NotImplementedError(
            "WinrmWorker is not yet implemented. "
            "Planned for future release. "
            "Will use pywinrm for native Windows remote management.\n"
            "Requires: pip install ras-commander[remote-winrm]"
        )


def init_winrm_worker(**kwargs) -> WinrmWorker:
    """Initialize WinRM worker (stub - raises NotImplementedError)."""
    check_winrm_dependencies()
    kwargs['worker_type'] = 'winrm'
    return WinrmWorker(**kwargs)

==================================================

File: C:\GH\ras-commander\ras_commander\remote\__init__.py
==================================================
"""
Remote and distributed execution for HEC-RAS simulations.

This subpackage provides worker abstractions for executing HEC-RAS
across local, remote, and cloud compute resources.

Available Workers:
    - PsexecWorker: Windows remote execution via PsExec (IMPLEMENTED)
    - LocalWorker: Local parallel execution using RasCmdr.compute_plan() (IMPLEMENTED)
    - DockerWorker: Linux container execution using HEC-RAS 6.6 (IMPLEMENTED, requires: docker)
    - SshWorker: SSH-based remote execution (stub, requires: paramiko)
    - WinrmWorker: WinRM-based remote execution (stub, requires: pywinrm)
    - SlurmWorker: HPC cluster execution (stub)
    - AwsEc2Worker: AWS EC2 cloud execution (stub, requires: boto3)
    - AzureFrWorker: Azure cloud execution (stub, requires: azure-*)

Usage:
    from ras_commander import init_ras_worker, compute_parallel_remote

    # Or import directly from subpackage:
    from ras_commander.remote import init_ras_worker, compute_parallel_remote

    worker = init_ras_worker("psexec", hostname="PC1", ...)
    results = compute_parallel_remote(["01", "02"], workers=[worker])

Installation Options:
    pip install ras-commander                    # Base (includes PsExec worker)
    pip install ras-commander[remote-ssh]        # + SSH worker
    pip install ras-commander[remote-winrm]      # + WinRM worker
    pip install ras-commander[remote-docker]     # + Docker worker
    pip install ras-commander[remote-aws]        # + AWS EC2 worker
    pip install ras-commander[remote-azure]      # + Azure worker
    pip install ras-commander[remote-all]        # All remote backends
"""

# Base class and factory function
from .RasWorker import RasWorker, init_ras_worker, load_workers_from_json

# Worker implementations
from .PsexecWorker import PsexecWorker
from .LocalWorker import LocalWorker
from .SshWorker import SshWorker
from .WinrmWorker import WinrmWorker
from .DockerWorker import DockerWorker
from .SlurmWorker import SlurmWorker
from .AwsEc2Worker import AwsEc2Worker
from .AzureFrWorker import AzureFrWorker

# Execution functions
from .Execution import compute_parallel_remote, ExecutionResult, get_worker_status

__all__ = [
    # Base class
    'RasWorker',

    # Worker implementations
    'PsexecWorker',
    'LocalWorker',
    'SshWorker',
    'WinrmWorker',
    'DockerWorker',
    'SlurmWorker',
    'AwsEc2Worker',
    'AzureFrWorker',

    # Functions
    'init_ras_worker',
    'load_workers_from_json',
    'compute_parallel_remote',
    'ExecutionResult',
    'get_worker_status',
]

==================================================

File: C:\GH\ras-commander\examples\hyetographs\hyetograph_ARI_100_years_pos50pct_24hr.csv
==================================================
Time_hour,Precipitation_in
1,0.09040491041908805
2,0.0955077907605224
3,0.10146987810261088
4,0.10855497206608167
5,0.1171543680932885
6,0.1278780203205363
7,0.146970561522203
8,0.16593941033554094
9,0.1936155742022425
10,0.2697157353332229
11,0.3599999999999999
12,1.2
13,0.47
14,0.306960245354186
15,0.2433240193125914
16,0.17831774068604123
17,0.15566677680236962
18,0.13948993645160312
19,0.1221999761240089
20,0.11263390133511564
21,0.10485136436093967
22,0.09836722663443354
23,0.09286196347106301
24,0.08811562831231079

==================================================

File: C:\GH\ras-commander\examples\hyetographs\hyetograph_ARI_10_years_pos50pct_24hr.csv
==================================================
Time_hour,Precipitation_in
1,0.06039156315395777
2,0.06375521611108814
3,0.06768230354162785
4,0.07234523361103751
5,0.07799952445440228
6,0.0850430998832925
7,0.09928617513316373
8,0.11172527137238264
9,0.1298047855496105
10,0.18441307951840247
11,0.24
12,0.714
13,0.32600000000000007
14,0.20806923839369285
15,0.16751768208790474
16,0.11982101594250372
17,0.10499403807898
18,0.09436871392335933
19,0.08131460382996858
20,0.07502791218397897
21,0.06990827571771163
22,0.06563904234299356
23,0.06201147001043372
24,0.05888175515950733

==================================================

File: C:\GH\ras-commander\examples\hyetographs\hyetograph_ARI_25_years_pos50pct_24hr.csv
==================================================
Time_hour,Precipitation_in
1,0.07260426998002334
2,0.07665162231333023
3,0.08137715944970658
4,0.0869884475092384
5,0.09379312728081013
6,0.10227032363387689
7,0.1187727466883497
8,0.13373867053642075
9,0.1555066993867329
10,0.21723588907914926
11,0.29000000000000004
12,0.886
13,0.384
14,0.24600987393864338
15,0.1967542369822075
16,0.14348388189104222
17,0.1256388908154591
18,0.11285911068199495
19,0.09778286786544221
20,0.09021687457399974
21,0.08405581609065793
22,0.07891843577904822
23,0.07455341770829094
24,0.07078763781557518

==================================================

File: C:\GH\ras-commander\examples\hyetographs\hyetograph_ARI_2_years_pos50pct_24hr.csv
==================================================
Time_hour,Precipitation_in
1,0.039444248438275764
2,0.04167741641696132
3,0.04428704307942355
4,0.04738880027716674
5,0.05115429786673542
6,0.055851100908542506
7,0.06613796588496257
8,0.07448873415720048
9,0.08663816527821133
10,0.1248195283605078
11,0.16500000000000004
12,0.474
13,0.22199999999999998
14,0.140734851235629
15,0.11344562040386319
16,0.07992740765575301
17,0.06996893628396816
18,0.0628387907399044
19,0.053364049126953805
20,0.04917477333825038
21,0.04576733441289149
22,0.042928943389559215
23,0.0405194807297482
24,0.03844251201549165

==================================================

File: C:\GH\ras-commander\examples\hyetographs\hyetograph_ARI_50_years_pos50pct_24hr.csv
==================================================
Time_hour,Precipitation_in
1,0.08192031837455449
2,0.08649776823482647
3,0.09184292815034834
4,0.09819090381104534
5,0.10589020511972391
6,0.11548369828277005
7,0.13208308858222972
8,0.14899627794001002
9,0.17364775403361454
10,0.24348456417803233
11,0.32000000000000006
12,1.03
13,0.43999999999999995
14,0.27642512827887034
15,0.2200903075430971
16,0.1600253022656406
17,0.13983878833910923
18,0.12540878883939577
19,0.11040508620362299
20,0.10184361090041572
21,0.09487313550894516
22,0.08906171957461506
23,0.08412468238815496
24,0.07986594345097853

==================================================

File: C:\GH\ras-commander\examples\hyetographs\hyetograph_ARI_5_years_pos50pct_24hr.csv
==================================================
Time_hour,Precipitation_in
1,0.05100624290168243
2,0.053861614565144045
3,0.0571962224527196
4,0.06115689800693991
5,0.06596132300525692
6,0.07194862056091855
7,0.08420784635189071
8,0.09480066356128192
9,0.11020467787180732
10,0.15808375054017998
11,0.21000000000000008
12,0.601
13,0.279
14,0.1782322703437993
15,0.1436839791160207
16,0.10169725937061136
17,0.08906792110929018
18,0.08002163173511834
19,0.06877894224371994
20,0.06343613417866356
21,0.05908679180980725
22,0.05546110471921306
23,0.052381270043452055
24,0.049724835512482635

==================================================

