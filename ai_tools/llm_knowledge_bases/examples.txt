File: c:\GH\ras-commander\examples\00_Using_RasExamples.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install RAS-Commander from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imports (if using the pip package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ras_commander import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Imports (for active development of the library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will try to import the pip package, if it fails it will \n",
    "# add the parent directory to the Python path and try to import again\n",
    "# This assumes you are working in a subfolder of the ras-commander repository\n",
    "# This allows a user's revisions to be tested locally without installing the package\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Flexible imports to allow for development without installation \n",
    "#  ** Use this version with Jupyter Notebooks **\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    from ras_commander import *\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    import os\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    rascmdr_directory = current_file.parent\n",
    "    sys.path.append(str(rascmdr_directory))\n",
    "    print(\"Loading ras-commander from local dev copy\")\n",
    "    # Now try to import again\n",
    "    from ras_commander import *\n",
    "print(\"ras_commander imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RASExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Method for Calling HEC-RAS Example Projects by Folder Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Code Cell is All You Need\n",
    "# This is what this Class was intended to do: Help me make repeatable workflows around HEC-RAS Example Projects for testing and demonstration purposes. \n",
    "\n",
    "# Extract specific projects\n",
    "RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\", \"Davis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasExamples will not download a new .zip file if one already exists, this allows you to replace the Example_Projects_6_x.zip with your own zip file (with the same folder format as the HEC-RAS examples) and you will be able to load them by folder name for repeatable Test Driven Development\n",
    "\n",
    "Just make sure all project folders have unique folder names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if example projects are already downloaded\n",
    "if RasExamples.projects_dir.exists():\n",
    "    print(\"Example projects are already downloaded.\")\n",
    "    print(\"RasExamples.folder_df:\")\n",
    "    display(RasExamples.folder_df)\n",
    "else:\n",
    "    print(\"Downloading example projects...\")\n",
    "    RasExamples.get_example_projects()\n",
    "    print(\"RasExamples.folder_df:\")\n",
    "    display(RasExamples.folder_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all categories\n",
    "categories = RasExamples.list_categories()\n",
    "print(\"\\nAvailable categories:\")\n",
    "for category in categories:\n",
    "    print(f\"- {category}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List projects in a specific category\n",
    "category = \"1D Unsteady Flow Hydraulics\"\n",
    "projects = RasExamples.list_projects(category)\n",
    "print(f\"\\nProjects in '{category}':\")\n",
    "for project in projects:\n",
    "    print(f\"- {project}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all projects\n",
    "all_projects = RasExamples.list_projects()\n",
    "print(\"\\nAll available projects:\")\n",
    "for project in all_projects:\n",
    "    print(f\"- {project}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects\n",
    "projects_to_extract = [\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"]\n",
    "extracted_paths = RasExamples.extract_project(projects_to_extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about New Pipes and Conduits Version 6.6 Example Project\n",
    "\n",
    "Use project name \"Davis\" to explore pipes and conduits (introduced in version 6.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\01_project_initialization.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander Project Initialization\n",
    "\n",
    "This notebook demonstrates how to initialize and work with HEC-RAS projects using the `ras-commander` library. You'll learn how to:\n",
    "\n",
    "1. Set up and configure the RAS Commander environment\n",
    "2. Download and extract example HEC-RAS projects\n",
    "3. Initialize HEC-RAS projects using the global `ras` object\n",
    "4. Initialize multiple HEC-RAS projects using custom RAS objects\n",
    "5. Access various project components (plans, geometries, flows, boundaries)\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **RasPrj Objects**: Represent HEC-RAS projects with access to plans, geometries, flows, etc.\n",
    "- **Global `ras` object**: A singleton instance for simple, single-project scripts\n",
    "- **Custom RAS Objects**: Independent instances for multi-project workflows\n",
    "- **Project Initialization**: Process of connecting to HEC-RAS projects\n",
    "- **Project Components**: Structured access to plans, geometries, and flow files\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Import required Libraries and install if missing\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def install_module(module_name):\n",
    "    try:\n",
    "        if module_name.lower() == 'ipython':\n",
    "            # Try importing IPython instead of ipython\n",
    "            __import__('IPython') \n",
    "        else:\n",
    "            __import__(module_name)\n",
    "    except ImportError:\n",
    "        print(f\"{module_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n",
    "\n",
    "# List of modules to check and install if necessary\n",
    "modules = [\n",
    "    'h5py', 'numpy', 'pandas', 'requests', 'tqdm', 'scipy', 'xarray',\n",
    "    'geopandas', 'matplotlib', 'ipython', 'tqdm', 'psutil', 'shapely', \n",
    "    'fiona', 'pathlib', 'rtree', 'rasterstats'\n",
    "]\n",
    "for module in modules:\n",
    "    install_module(module)\n",
    "\n",
    "# Import the required libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import rtree\n",
    "import rasterstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will try to import the pip package, if it fails it will \n",
    "# add the parent directory to the Python path and try to import again\n",
    "# This assumes you are working in a subfolder of the ras-commander repository\n",
    "# This allows a user's revisions to be tested locally without installing the package\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Flexible imports to allow for development without installation \n",
    "#  ** Use this version with Jupyter Notebooks **\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    from ras_commander import *\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    import os\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    parent_directory = current_file.parent\n",
    "    sys.path.append(str(parent_directory))\n",
    "    print(\"importing from local dev copy\")\n",
    "    # Now try to import again\n",
    "    from ras_commander import *\n",
    "print(\"ras_commander imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Let's define our working directory and ensure we're in a consistent environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAS Commander: Core Concepts\n",
    "\n",
    "RAS Commander is a Python library that provides tools for automating HEC-RAS tasks. It's built with several key design principles:\n",
    "\n",
    "1. **Project-Centric Architecture**: Everything revolves around HEC-RAS projects\n",
    "2. **Two RAS Object Approaches**:\n",
    "   - **Global `ras` Object**: A singleton for simple scripts\n",
    "   - **Custom RAS Objects**: Multiple ras project instances for complex workflows\n",
    "3. **Comprehensive Project Representation**: Each RAS object includes DataFrames for plans, geometries, flows, and boundaries\n",
    "4. **Logging**: Built-in logging to track operations and debug issues\n",
    "5. **HDF Support**: Specialized functions for HDF file access (plan results, geometry, etc.)\n",
    "\n",
    "Let's explore these concepts in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Example HEC-RAS Projects\n",
    "\n",
    "RAS Commander includes a utility to download and extract example HEC-RAS projects. These are useful for learning and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects we'll use in this tutorial\n",
    "# This will download them if not present and extract them to the example_projects folder\n",
    "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
    "print(extracted_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Paths for Extracted Example Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the first extracted path as our examples directory\n",
    "examples_dir = extracted_paths[0].parent\n",
    "print(f\"Examples directory: {examples_dir}\")\n",
    "\n",
    "\n",
    "# Define paths to the extracted projects\n",
    "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
    "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
    "muncie_path = examples_dir / \"Muncie\"\n",
    "\n",
    "# Verify the paths exist\n",
    "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
    "    print(f\"Path {path} exists: {path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function to Print RAS Object Data\n",
    "\n",
    "Let's create a utility function to help us explore the contents of RAS objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ras_object_data(ras_obj, project_name):\n",
    "    \"\"\"Prints comprehensive information about a RAS object\"\"\"\n",
    "    print(f\"\\n{project_name} Data:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Project Name: {ras_obj.get_project_name()}\")\n",
    "    print(f\"Project Folder: {ras_obj.project_folder}\")\n",
    "    print(f\"PRJ File: {ras_obj.prj_file}\")\n",
    "    print(f\"HEC-RAS Executable Path: {ras_obj.ras_exe_path}\")\n",
    "    \n",
    "    print(\"\\nPlan Files DataFrame:\")\n",
    "    display.display(ras_obj.plan_df)\n",
    "    \n",
    "    print(\"\\nFlow Files DataFrame:\")\n",
    "    display.display(ras_obj.flow_df)\n",
    "    \n",
    "    print(\"\\nUnsteady Flow Files DataFrame:\")\n",
    "    display.display(ras_obj.unsteady_df)\n",
    "    \n",
    "    print(\"\\nGeometry Files DataFrame:\")\n",
    "    display.display(ras_obj.geom_df)\n",
    "    \n",
    "    print(\"\\nHDF Entries DataFrame:\")\n",
    "    display.display(ras_obj.get_hdf_entries())\n",
    "    \n",
    "    print(\"\\nBoundary Conditions DataFrame:\")\n",
    "    display.display(ras_obj.boundaries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using the Global `ras` Object\n",
    "\n",
    "The global `ras` object is a singleton instance that persists throughout your script. It's ideal for simple scripts working with a single project.\n",
    "\n",
    "Key characteristics:\n",
    "- It's available as `ras` immediately after import\n",
    "- It's initialized via `init_ras_project()` without saving the return value\n",
    "- It provides access to all project data through the global `ras` variable\n",
    "- It's simple to use but can be problematic in complex scenarios\n",
    "\n",
    "Let's initialize it with the Bald Eagle Creek project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the global ras object with Bald Eagle Creek project\n",
    "# Note: This updates the global 'ras' object visible throughout the script\n",
    "# Parameters:\n",
    "#   - project_folder: Path to the HEC-RAS project folder (required)\n",
    "#   - ras_version: HEC-RAS version (e.g. \"6.5\") or path to Ras.exe (required first time)\n",
    "\n",
    "init_ras_project(bald_eagle_path, \"6.5\")\n",
    "print(f\"The global 'ras' object is now initialized with the {ras.project_name} project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the global ras object with our utility function\n",
    "print_ras_object_data(ras, \"Global RAS Object (Bald Eagle Creek)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the RAS Object Structure\n",
    "\n",
    "Each RAS object contains several important components:\n",
    "\n",
    "1. **Project Metadata**:\n",
    "   - `project_name`: Name of the HEC-RAS project\n",
    "   - `project_folder`: Directory containing project files\n",
    "   - `prj_file`: Path to the main .prj file\n",
    "   - `ras_exe_path`: Path to the HEC-RAS executable\n",
    "\n",
    "2. **Project DataFrames**:\n",
    "   - `plan_df`: Information about all plan files (.p*)\n",
    "   - `flow_df`: Information about all steady flow files (.f*)\n",
    "   - `unsteady_df`: Information about all unsteady flow files (.u*)\n",
    "   - `geom_df`: Information about all geometry files (.g*)\n",
    "   - `boundaries_df`: Information about all boundary conditions\n",
    "\n",
    "3. **Methods for Data Access**:\n",
    "   - `get_plan_entries()`: Get plan file information\n",
    "   - `get_flow_entries()`: Get flow file information\n",
    "   - `get_unsteady_entries()`: Get unsteady flow file information \n",
    "   - `get_geom_entries()`: Get geometry file information\n",
    "   - `get_hdf_entries()`: Get HDF file paths for result files\n",
    "   - `get_boundary_conditions()`: Get boundary condition details\n",
    "\n",
    "Let's see how to access specific information from these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first plan's details\n",
    "if not ras.plan_df.empty:\n",
    "    first_plan = ras.plan_df.iloc[0]\n",
    "    print(f\"First plan number: {first_plan['plan_number']}\")\n",
    "    print(f\"Plan path: {first_plan['full_path']}\")\n",
    "    \n",
    "    # Get the geometry file for this plan\n",
    "    geom_id = first_plan.get('Geom File', '').replace('g', '')\n",
    "    if geom_id:\n",
    "        geom_info = ras.geom_df[ras.geom_df['geom_number'] == geom_id]\n",
    "        if not geom_info.empty:\n",
    "            print(f\"Geometry file: {geom_info.iloc[0]['full_path']}\")\n",
    "    \n",
    "    # Get the HDF results file for this plan (if exists)\n",
    "    if 'HDF_Results_Path' in first_plan and first_plan['HDF_Results_Path']:\n",
    "        print(f\"Results file: {first_plan['HDF_Results_Path']}\")\n",
    "else:\n",
    "    print(\"No plans found in the project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Boundary Conditions\n",
    "\n",
    "Boundary conditions define the inputs and outputs of your model. Let's see how to access boundary condition information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the boundary conditions DataFrame\n",
    "ras.boundaries_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using Custom RAS Objects\n",
    "\n",
    "For more complex scripts or when working with multiple projects, it's better to create and use separate RAS objects. This approach:\n",
    "\n",
    "- Creates independent RAS objects for each project\n",
    "- Avoids overwriting the global `ras` object\n",
    "- Provides clearer separation between projects\n",
    "- Allows working with multiple projects simultaneously\n",
    "- Requires saving the return value from `init_ras_project()`\n",
    "\n",
    "Let's initialize multiple projects with custom RAS objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple project instances with custom RAS objects\n",
    "# Note: This also updates the global 'ras' object each time, but we'll use the custom instances\n",
    "# Parameters remain the same as before\n",
    "multi_2d_project = RasPrj()\n",
    "init_ras_project(multi_2d_path, \"6.5\", ras_object=multi_2d_project)\n",
    "print(f\"\\nMulti2D project initialized with its own RAS object\")\n",
    "\n",
    "muncie_project = RasPrj()\n",
    "init_ras_project(muncie_path, \"6.5\", ras_object=muncie_project)\n",
    "print(f\"\\nMuncie project initialized with its own RAS object\")\n",
    "\n",
    "# Note that the global 'ras' object now points to the Muncie project\n",
    "# The global 'ras' object gets overwritten every time a project is initialized ,\n",
    "print(f\"\\nGlobal 'ras' object now points to: {ras.project_name} since it was the last one initialized.  Avoid the global object when using multiple projects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Multiple Projects\n",
    "\n",
    "Now we have three RAS objects:\n",
    "- `multi_2d_project`: Our custom object for the Multi2D project\n",
    "- `muncie_project`: Our custom object for the Muncie project\n",
    "- `ras`: The global object (which now points to Muncie)\n",
    "\n",
    "Let's examine the Multi2D project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(multi_2d_project.plan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Multi2D project\n",
    "print_ras_object_data(multi_2d_project, \"Multi2D Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Muncie project\n",
    "print_ras_object_data(muncie_project, \"Muncie Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Projects\n",
    "\n",
    "Let's compare some key metrics of the two projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table of the two projects\n",
    "comparison_data = {\n",
    "    'Project Name': [multi_2d_project.project_name, muncie_project.project_name],\n",
    "    'Number of Plans': [len(multi_2d_project.plan_df), len(muncie_project.plan_df)],\n",
    "    'Number of Geometries': [len(multi_2d_project.geom_df), len(muncie_project.geom_df)],\n",
    "    'Number of Flow Files': [len(multi_2d_project.flow_df), len(muncie_project.flow_df)],\n",
    "    'Number of Unsteady Files': [len(multi_2d_project.unsteady_df), len(muncie_project.unsteady_df)],\n",
    "    'Number of Boundary Conditions': [len(multi_2d_project.boundaries_df) if hasattr(multi_2d_project, 'boundaries_df') else 0, \n",
    "                                     len(muncie_project.boundaries_df) if hasattr(muncie_project, 'boundaries_df') else 0],\n",
    "    'HDF Results Available': [len(multi_2d_project.get_hdf_entries()) > 0, len(muncie_project.get_hdf_entries()) > 0]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display.display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAS Commander: Best Practices\n",
    "\n",
    "After exploring both approaches, here are some best practices for using RAS Commander:\n",
    "\n",
    "1. **Choose Your Approach Based on Complexity**:\n",
    "   - **Simple Scripts** (one project): Use the global `ras` object\n",
    "   - **Complex Scripts** (multiple projects): Use custom RAS objects\n",
    "\n",
    "2. **Be Consistent**:\n",
    "   - Don't mix global and custom approaches in the same script\n",
    "   - Use descriptive names for custom RAS objects\n",
    "\n",
    "3. **Working with Project Files**:\n",
    "   - Access project files through the RAS object's DataFrames\n",
    "   - Use helper functions like `get_plan_path()` to resolve paths\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Always check for empty DataFrames before accessing their contents\n",
    "   - Use the built-in logging to track operations\n",
    "\n",
    "5. **Performance Considerations**:\n",
    "   - For large projects, consider using the HDF classes directly\n",
    "   - Cache results of expensive operations when possible\n",
    "\n",
    "## Summary of Key Functions\n",
    "\n",
    "- `init_ras_project(project_folder, ras_version)`: Initialize a RAS project\n",
    "- `RasExamples().extract_project(project_name)`: Extract example projects\n",
    "- `RasPrj.get_project_name()`: Get the name of the project\n",
    "- `RasPrj.get_plan_entries()`: Get plan file information\n",
    "- `RasPrj.get_flow_entries()`: Get flow file information\n",
    "- `RasPrj.get_unsteady_entries()`: Get unsteady flow file information\n",
    "- `RasPrj.get_geom_entries()`: Get geometry file information\n",
    "- `RasPrj.get_hdf_entries()`: Get HDF result file information\n",
    "- `RasPrj.get_boundary_conditions()`: Get boundary condition details\n",
    "- `RasPlan.get_plan_path(plan_number)`: Get the path to a plan file\n",
    "- `RasPlan.get_geom_path(geom_number)`: Get the path to a geometry file\n",
    "- `RasPlan.get_flow_path(flow_number)`: Get the path to a flow file\n",
    "- `RasPlan.get_unsteady_path(unsteady_number)`: Get the path to an unsteady flow file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of project initialization in RAS Commander, you can explore more advanced topics:\n",
    "\n",
    "1. Working with HDF files for result analysis\n",
    "2. Modifying plan, geometry, and flow files\n",
    "3. Running HEC-RAS simulations\n",
    "4. Extracting and visualizing results\n",
    "5. Automating model calibration\n",
    "\n",
    "These topics are covered in other examples and notebooks in the RAS Commander documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\02_plan_and_geometry_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Plan and Geometry Operations\n",
    "\n",
    "This notebook demonstrates how to perform operations on HEC-RAS plan and geometry files using the RAS Commander library. We'll explore how to initialize projects, clone plans and geometries, configure parameters, execute plans, and analyze results.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Plan Operations**:\n",
    "   - Clone an existing plan to create a new one\n",
    "   - Configure simulation parameters and intervals\n",
    "   - Set run flags and update descriptions\n",
    "3. **Geometry Operations**:\n",
    "   - Clone a geometry file to create a modified version\n",
    "   - Set the geometry for a plan\n",
    "   - Clear geometry preprocessor files to ensure clean results\n",
    "4. **Flow Operations**:\n",
    "   - Clone unsteady flow files\n",
    "   - Configure flow parameters\n",
    "5. **Plan Computation**: Run the plan with specified settings\n",
    "6. **Results Verification**: Check HDF entries to confirm results were written\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries for this notebook\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from datetime import datetime  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Flexible imports to allow for development without installation\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    print(\"Importing ras-commander modules from pip package, if available\")\n",
    "    from ras_commander import *\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    print(\"ras-commander pip package not found. Using local ras-commander library.\")\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    parent_directory = current_file.parent  # Adjust as needed for your directory structure\n",
    "    print(f\"Adding path to sys.path: {parent_directory}\")\n",
    "    sys.path.append(str(parent_directory))\n",
    "    \n",
    "    # Now try to import again\n",
    "    from ras_commander import *\n",
    "\n",
    "print(\"ras_commander imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Projects\n",
    "\n",
    "We'll use the `RasExamples` class to download and extract an example HEC-RAS project. For this notebook, we'll use the \"Balde Eagle Creek\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects we'll use in this tutorial\n",
    "# This will download them if not present and extract them to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(bald_eagle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the project folder path and HEC-RAS version as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
    "display.display(ras.plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Plan and Geometry Operations in HEC-RAS\n",
    "\n",
    "Before diving into the operations, let's understand what plan and geometry files are in HEC-RAS:\n",
    "\n",
    "- **Plan Files** (`.p*`): Define the simulation parameters including the reference to geometry and flow files, as well as computational settings.\n",
    "- **Geometry Files** (`.g*`): Define the physical characteristics of the river/channel system including cross-sections, 2D areas, and structures.\n",
    "\n",
    "The `RasPlan` and `RasGeo` classes provide methods for working with these files, including:\n",
    "\n",
    "1. Creating new plans and geometries by cloning existing ones\n",
    "2. Modifying simulation parameters and settings\n",
    "3. Associating geometries with plans\n",
    "4. Managing preprocessor files\n",
    "5. Retrieving information from plans and geometries\n",
    "\n",
    "In the following sections, we'll explore these operations in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloning Plans and Geometries\n",
    "\n",
    "Let's start by cloning a plan to create a new simulation scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone plan \"01\" to create a new plan\n",
    "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Combined Test Plan\")\n",
    "print(f\"New plan created: {new_plan_number}\")\n",
    "\n",
    "# Display updated plan files\n",
    "print(\"\\nUpdated plan files:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Get the path to the new plan file\n",
    "plan_path = RasPlan.get_plan_path(new_plan_number)\n",
    "print(f\"\\nNew plan file path: {plan_path}\")\n",
    "\n",
    "# Let's examine the new plan's details\n",
    "new_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
    "print(f\"\\nNew plan details:\")\n",
    "print(f\"Plan number: {new_plan_number}\")\n",
    "print(f\"Description: {new_plan.get('description', 'No description')}\")\n",
    "print(f\"Short Identifier: {new_plan.get('Short Identifier', 'Not available')}\")\n",
    "print(f\"Geometry file: {new_plan.get('Geom File', 'None')}\")\n",
    "print(f\"File path: {new_plan['full_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clone a geometry file. This allows us to make modifications to a geometry without affecting the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone geometry \"01\" to create a new geometry file\n",
    "new_geom_number = RasPlan.clone_geom(\"01\")\n",
    "print(f\"New geometry created: {new_geom_number}\")\n",
    "\n",
    "# Display updated geometry files\n",
    "print(\"\\nUpdated geometry files:\")\n",
    "display.display(ras.geom_df)\n",
    "\n",
    "# Get the path to the new geometry file\n",
    "geom_path = RasPlan.get_geom_path(new_geom_number)\n",
    "print(f\"\\nNew geometry file path: {geom_path}\")\n",
    "\n",
    "# Examine the new geometry's details\n",
    "new_geom = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number].squeeze()\n",
    "print(f\"\\nNew geometry details:\")\n",
    "print(f\"Geometry number: {new_geom_number}\")\n",
    "print(f\"Geometry file: {new_geom.get('geom_file', 'Not available')}\")\n",
    "print(f\"File path: {new_geom.get('full_path', 'Not available')}\")\n",
    "print(f\"HDF path: {new_geom.get('hdf_path', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also clone an unsteady flow file to complete our new simulation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
    "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
    "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
    "\n",
    "# Display updated unsteady flow files\n",
    "print(\"\\nUpdated unsteady flow files:\")\n",
    "display.display(ras.unsteady_df)\n",
    "\n",
    "# Examine the new unsteady flow's details\n",
    "new_unsteady = ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number].iloc[0]\n",
    "print(f\"\\nNew unsteady flow details:\")\n",
    "print(f\"Unsteady number: {new_unsteady_number}\")\n",
    "print(f\"File path: {new_unsteady['full_path']}\")\n",
    "print(f\"Flow Title: {new_unsteady.get('Flow Title', 'Not available')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associating Files and Setting Parameters\n",
    "\n",
    "Now that we have cloned our plan, geometry, and unsteady flow files, we need to associate them with each other and set various parameters.\n",
    "\n",
    "### Setting Geometry for a Plan\n",
    "\n",
    "Let's associate our new geometry with our new plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new geometry for the cloned plan\n",
    "updated_geom_df = RasPlan.set_geom(new_plan_number, new_geom_number)\n",
    "plan_path = RasPlan.get_plan_path(new_plan_number, ras_object=ras)\n",
    "print(f\"Updated geometry for plan {new_plan_number} to geometry {new_geom_number}\")\n",
    "print(f\"Plan file path: {plan_path}\")\n",
    "\n",
    "# Let's verify the change\n",
    "updated_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
    "print(f\"\\nVerified that plan {new_plan_number} now uses geometry file: {updated_plan.get('Geom File', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Unsteady Flow for a Plan\n",
    "\n",
    "Similarly, let's associate our new unsteady flow file with our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unsteady flow for the cloned plan\n",
    "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
    "print(f\"Updated unsteady flow for plan {new_plan_number} to unsteady flow {new_unsteady_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing Geometry Preprocessor Files\n",
    "\n",
    "When working with geometry files, it's important to clear the preprocessor files to ensure clean results. These files (with `.c*` extension) contain computed hydraulic properties that should be recomputed when the geometry changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear geometry preprocessor files for the cloned plan\n",
    "RasGeo.clear_geompre_files(plan_path)\n",
    "print(f\"Cleared geometry preprocessor files for plan {new_plan_number}\")\n",
    "\n",
    "# Check if preprocessor file exists after clearing\n",
    "geom_preprocessor_suffix = '.c' + ''.join(Path(plan_path).suffixes[1:])\n",
    "geom_preprocessor_file = Path(plan_path).with_suffix(geom_preprocessor_suffix)\n",
    "print(f\"Preprocessor file exists after clearing: {geom_preprocessor_file.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Computation Parameters\n",
    "\n",
    "Let's set the computation parameters for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of cores to use for the computation\n",
    "RasPlan.set_num_cores(new_plan_number, 2)\n",
    "print(f\"Updated number of cores for plan {new_plan_number} to 2\")\n",
    "\n",
    "# Verify by extracting the value from the plan file\n",
    "cores_value = RasPlan.get_plan_value(new_plan_number, \"UNET D1 Cores\")\n",
    "print(f\"\\nVerified that UNET D1 Cores is set to: {cores_value}\")\n",
    "\n",
    "# Set geometry preprocessor options\n",
    "RasPlan.set_geom_preprocessor(plan_path, run_htab=-1, use_ib_tables=-1)\n",
    "print(f\"Updated geometry preprocessor options for plan {new_plan_number}\")\n",
    "print(f\"- Run HTab: -1 (Force recomputation of geometry tables)\")\n",
    "print(f\"- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\")\n",
    "\n",
    "# Verify by extracting the values from the plan file\n",
    "run_htab_value = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
    "ib_tables_value = RasPlan.get_plan_value(new_plan_number, \"UNET Use Existing IB Tables\")\n",
    "print(f\"\\nVerified setting values:\")\n",
    "print(f\"- Run HTab: {run_htab_value}\")\n",
    "print(f\"- UNET Use Existing IB Tables: {ib_tables_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Simulation Parameters\n",
    "\n",
    "Now, let's update various simulation parameters for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Update simulation date\n",
    "start_date = datetime(2023, 1, 1, 0, 0)  # January 1, 2023, 00:00\n",
    "end_date = datetime(2023, 1, 5, 23, 59)  # January 5, 2023, 23:59\n",
    "\n",
    "RasPlan.update_simulation_date(new_plan_number, start_date, end_date)\n",
    "print(f\"Updated simulation date for plan {new_plan_number}:\")\n",
    "print(f\"- Start Date: {start_date}\")\n",
    "print(f\"- End Date: {end_date}\")\n",
    "\n",
    "# Verify the update\n",
    "sim_date = RasPlan.get_plan_value(new_plan_number, \"Simulation Date\")\n",
    "print(f\"Verified Simulation Date value: {sim_date}\")\n",
    "\n",
    "# 2. Update plan intervals\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_plan_number,\n",
    "    computation_interval=\"1MIN\",  # Computational time step\n",
    "    output_interval=\"15MIN\",      # How often results are written\n",
    "    mapping_interval=\"30MIN\"      # How often mapping outputs are created\n",
    ")\n",
    "print(f\"\\nUpdated plan intervals for plan {new_plan_number}:\")\n",
    "print(f\"- Computation Interval: 1MIN\")\n",
    "print(f\"- Output Interval: 15MIN\")\n",
    "print(f\"- Mapping Interval: 30MIN\")\n",
    "\n",
    "# Verify the updates\n",
    "comp_interval = RasPlan.get_plan_value(new_plan_number, \"Computation Interval\")\n",
    "mapping_interval = RasPlan.get_plan_value(new_plan_number, \"Mapping Interval\")\n",
    "print(f\"Verified interval values:\")\n",
    "print(f\"- Computation Interval: {comp_interval}\")\n",
    "print(f\"- Mapping Interval: {mapping_interval}\")\n",
    "\n",
    "# 3. Update run flags\n",
    "RasPlan.update_run_flags(\n",
    "    new_plan_number,\n",
    "    geometry_preprocessor=True,   # Run the geometry preprocessor\n",
    "    unsteady_flow_simulation=True, # Run unsteady flow simulation\n",
    "    post_processor=True,          # Run post-processing\n",
    "    floodplain_mapping=True       # Generate floodplain mapping outputs\n",
    ")\n",
    "print(f\"\\nUpdated run flags for plan {new_plan_number}:\")\n",
    "print(f\"- Geometry Preprocessor: True\")\n",
    "print(f\"- Unsteady Flow Simulation: True\")\n",
    "print(f\"- Post Processor: True\")\n",
    "print(f\"- Floodplain Mapping: True\")\n",
    "\n",
    "# Verify the updates\n",
    "run_htab = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
    "run_unet = RasPlan.get_plan_value(new_plan_number, \"Run UNet\")\n",
    "print(f\"Verified run flag values:\")\n",
    "print(f\"- Run HTab (Geometry Preprocessor): {run_htab}\")\n",
    "print(f\"- Run UNet (Unsteady Flow): {run_unet}\")\n",
    "\n",
    "# 4. Update plan description\n",
    "new_description = \"Combined plan with modified geometry and unsteady flow\\nJanuary 2023 simulation\\n1-minute computation interval\\nGeometry and unsteady flow from cloned files\"\n",
    "RasPlan.update_plan_description(new_plan_number, new_description)\n",
    "print(f\"\\nUpdated description for plan {new_plan_number}\")\n",
    "\n",
    "# Read back the description\n",
    "current_description = RasPlan.read_plan_description(new_plan_number)\n",
    "print(f\"Current plan description:\\n{current_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Plan\n",
    "\n",
    "Now that we have set up all the parameters, let's compute the plan using RasCmdr.compute_plan():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the plan with our configured settings\n",
    "# Note: This may take several minutes depending on the complexity of the model\n",
    "print(f\"Computing plan {new_plan_number}...\")\n",
    "success = RasCmdr.compute_plan(new_plan_number, clear_geompre=True)\n",
    "\n",
    "if success:\n",
    "    print(f\"Plan {new_plan_number} computed successfully\")\n",
    "else:\n",
    "    print(f\"Failed to compute plan {new_plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Results\n",
    "\n",
    "After computation, we should check if results were written correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the plan entries to ensure we have the latest data\n",
    "ras.plan_df = ras.get_plan_entries()\n",
    "hdf_entries = ras.get_hdf_entries()\n",
    "\n",
    "if not hdf_entries.empty:\n",
    "    print(\"HDF entries for the project:\")\n",
    "    display.display(hdf_entries)\n",
    "    \n",
    "    # Check if our new plan has an HDF file\n",
    "    new_plan_hdf = hdf_entries[hdf_entries['plan_number'] == new_plan_number]\n",
    "    if not new_plan_hdf.empty:\n",
    "        print(f\"\\nPlan {new_plan_number} has a valid HDF results file:\")\n",
    "        print(f\"HDF Path: {new_plan_hdf.iloc[0]['HDF_Results_Path']}\")\n",
    "    else:\n",
    "        print(f\"\\nNo HDF entry found for plan {new_plan_number}\")\n",
    "else:\n",
    "    print(\"No HDF entries found. This could mean the plan hasn't been computed successfully or the results haven't been written yet.\")\n",
    "\n",
    "# Display all plan entries to see their HDF paths\n",
    "print(\"\\nAll plan entries with their HDF paths:\")\n",
    "plan_hdf_info = ras.plan_df[['plan_number', 'HDF_Results_Path']]\n",
    "display.display(plan_hdf_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the plan was computed successfully, we can examine the runtime data and volume accounting from the HDF results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get computation runtime data from HDF\n",
    "print(\"Checking computation runtime data...\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(new_plan_number)\n",
    "\n",
    "if runtime_df is not None and not runtime_df.empty:\n",
    "    print(\"\\nSimulation Runtime Statistics:\")\n",
    "    display.display(runtime_df)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
    "    compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
    "    compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nSimulation Duration: {sim_duration:.2f} seconds\")\n",
    "    print(f\"Computation Time: {compute_time:.5f} hours\")\n",
    "    print(f\"Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
    "else:\n",
    "    print(\"No runtime data found. This may indicate the simulation didn't complete successfully.\")\n",
    "\n",
    "# Get volume accounting data\n",
    "print(\"\\nChecking volume accounting...\")\n",
    "volume_df = HdfResultsPlan.get_volume_accounting(new_plan_number)\n",
    "\n",
    "if volume_df is not None and not isinstance(volume_df, bool):\n",
    "    # Handle volume_df as a dictionary\n",
    "    if isinstance(volume_df, dict):\n",
    "        error_percent = volume_df.get('Error Percent')\n",
    "        if error_percent is not None:\n",
    "            print(f\"\\nFinal Volume Balance Error: {float(error_percent):.8f}%\")\n",
    "            \n",
    "        # Print other key statistics\n",
    "        print(\"\\nDetailed Volume Statistics:\")\n",
    "        print(f\"Volume Starting: {float(volume_df['Volume Starting']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "        print(f\"Volume Ending: {float(volume_df['Volume Ending']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "        print(f\"Total Inflow: {float(volume_df['Total Boundary Flux of Water In']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "        print(f\"Total Outflow: {float(volume_df['Total Boundary Flux of Water Out']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "else:\n",
    "    print(\"No volume accounting data found. This may indicate the simulation didn't complete successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Advanced HDF Data\n",
    "\n",
    "Let's explore how to access more detailed geometry data from the HDF files. When working with HEC-RAS, the geometric information is stored in HDF files (`.g*.hdf`) which can be accessed using the HDF classes in RAS Commander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh geometry information\n",
    "ras.geom_df = ras.get_geom_entries()\n",
    "\n",
    "# Get HDF path for the new geometry\n",
    "geom_info = ras.geom_df[ras.geom_df['geom_number'] == new_geom_number]\n",
    "if not geom_info.empty and 'hdf_path' in geom_info.columns:\n",
    "    geom_hdf_path = geom_info.iloc[0]['hdf_path']\n",
    "    print(f\"Geometry HDF path: {geom_hdf_path}\")\n",
    "    \n",
    "    # Check if the HDF file exists\n",
    "    geom_hdf_file = Path(geom_hdf_path)\n",
    "    if geom_hdf_file.exists():\n",
    "        print(f\"Geometry HDF file exists: {geom_hdf_file.exists()}\")\n",
    "        \n",
    "        # If it exists, try to extract some information from it\n",
    "        try:\n",
    "            # Get cross-sections if this is a 1D or combined 1D/2D model\n",
    "            xs_data = HdfXsec.get_cross_sections(geom_hdf_path)\n",
    "            if not xs_data.empty:\n",
    "                print(f\"\\nFound {len(xs_data)} cross-sections in the geometry:\")\n",
    "                display.display(xs_data.head())\n",
    "            else:\n",
    "                print(\"No cross-sections found in the geometry.\")\n",
    "                \n",
    "            # Get 2D flow areas if this is a 2D or combined 1D/2D model\n",
    "            mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
    "            if not mesh_areas.empty:\n",
    "                print(f\"\\nFound {len(mesh_areas)} 2D flow areas in the geometry:\")\n",
    "                display.display(mesh_areas.head())\n",
    "            else:\n",
    "                print(\"No 2D flow areas found in the geometry.\")\n",
    "                \n",
    "            # Get structures if any exist\n",
    "            strucs = HdfStruc.get_structures(geom_hdf_path)\n",
    "            if not strucs.empty:\n",
    "                print(f\"\\nFound {len(strucs)} structures in the geometry:\")\n",
    "                display.display(strucs.head())\n",
    "            else:\n",
    "                print(\"No structures found in the geometry.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing geometry HDF data: {e}\")\n",
    "    else:\n",
    "        print(\"Geometry HDF file does not exist.\")\n",
    "else:\n",
    "    print(\"Could not find HDF path for the new geometry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Plan and Geometry Operations\n",
    "\n",
    "In this notebook, we've covered a comprehensive range of operations on HEC-RAS plan and geometry files using the RAS Commander library:\n",
    "\n",
    "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
    "2. **Plan Operations**:\n",
    "   - Created a new plan by cloning an existing one\n",
    "   - Updated simulation parameters (dates, intervals, etc.)\n",
    "   - Set run flags for different components\n",
    "   - Updated the plan description\n",
    "3. **Geometry Operations**:\n",
    "   - Created a new geometry by cloning an existing one\n",
    "   - Associated the new geometry with our plan\n",
    "   - Cleared geometry preprocessor files\n",
    "4. **Unsteady Flow Operations**:\n",
    "   - Created a new unsteady flow file by cloning an existing one\n",
    "   - Associated it with our plan\n",
    "5. **Computation and Verification**:\n",
    "   - Computed our plan with the specified settings\n",
    "   - Verified the results using HDF entries\n",
    "   - Analyzed runtime statistics and volume accounting\n",
    "6. **Advanced HDF Operations**:\n",
    "   - Accessed detailed geometry information from HDF files\n",
    "   - Explored cross-sections, mesh areas, and structures\n",
    "\n",
    "### Key Classes and Functions Used\n",
    "\n",
    "- `RasPlan`: For plan operations (cloning, setting components, and modifying parameters)\n",
    "- `RasGeo`: For geometry operations (cloning, clearing preprocessor files)\n",
    "- `RasCmdr`: For executing HEC-RAS simulations\n",
    "- `HdfResultsPlan`: For accessing plan-level results\n",
    "- `HdfXsec`, `HdfMesh`, `HdfStruc`: For accessing geometry details\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further enhance your HEC-RAS automation, consider exploring:\n",
    "\n",
    "1. **Parameter Sweeps**: Create and run multiple plans with varying parameters\n",
    "2. **Parallel Computations**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
    "3. **Advanced Results Analysis**: Use the HDF classes to extract and analyze specific model results\n",
    "4. **Spatial Visualization**: Create maps and plots of simulation results\n",
    "5. **Model Calibration**: Automate comparison between model results and observations\n",
    "\n",
    "The RAS Commander library provides a powerful framework for automating and streamlining your HEC-RAS workflows, enabling more efficient hydraulic modeling and analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\03_unsteady_flow_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Unsteady Flow Operations\n",
    "\n",
    "This notebook demonstrates operations on unsteady flow files using the RAS Commander library. Unsteady flow files in HEC-RAS (`.u*` files) define the time-varying boundary conditions used in dynamic simulations.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Boundary Extraction**: Extract boundary conditions and tables from unsteady flow files\n",
    "3. **Boundary Analysis**: Inspect and understand boundary condition structures\n",
    "4. **Flow Title Updates**: Modify the title of unsteady flow files\n",
    "5. **Restart Settings**: Configure restart file settings for continuing simulations\n",
    "6. **Table Modification**: Extract, modify, and update flow tables\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Unsteady Flow Files in HEC-RAS\n",
    "\n",
    "Unsteady flow files (`.u*` files) in HEC-RAS define the time-varying boundary conditions that drive dynamic simulations. These include:\n",
    "\n",
    "- **Flow Hydrographs**: Time-series of flow values at model boundaries\n",
    "- **Stage Hydrographs**: Time-series of water surface elevations\n",
    "- **Lateral Inflows**: Distributed inflows along a reach\n",
    "- **Gate Operations**: Time-series of gate settings\n",
    "- **Meteorological Data**: Rainfall, evaporation, and other meteorological inputs\n",
    "\n",
    "The `RasUnsteady` class in RAS Commander provides methods for working with these files, including extracting boundaries, reading tables, and modifying parameters.\n",
    "\n",
    "Let's set up our working directory and define paths to example projects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Projects\n",
    "\n",
    "We'll use the `RasExamples` class to download and extract an example HEC-RAS project with unsteady flow files. For this notebook, we'll use the \"Balde Eagle Creek\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the following parameters:\n",
    "\n",
    "- `ras_project_folder`: Path to the HEC-RAS project folder (required)\n",
    "- `ras_version`: HEC-RAS version (e.g., \"6.6\") or path to Ras.exe (required first time)\n",
    "\n",
    "This function initializes the global `ras` object that we'll use for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "# This function returns a RAS object, but also updates the global 'ras' object\n",
    "# Parameters:\n",
    "#   - ras_project_folder: Path to the HEC-RAS project folder\n",
    "#   - ras_version: HEC-RAS version or path to Ras.exe\n",
    "\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the unsteady flow files in the project\n",
    "print(\"\\nHEC-RAS Project Unsteady Flow Data (unsteady_df):\")\n",
    "display.display(ras.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasUnsteady Class\n",
    "\n",
    "The `RasUnsteady` class provides functionality for working with HEC-RAS unsteady flow files (`.u*` files). Key operations include:\n",
    "\n",
    "1. **Extracting Boundary Conditions**: Read and parse boundary conditions from unsteady flow files\n",
    "2. **Modifying Flow Titles**: Update descriptive titles for unsteady flow scenarios\n",
    "3. **Managing Restart Settings**: Configure restart file options for continuing simulations\n",
    "4. **Working with Tables**: Extract, modify, and update flow tables\n",
    "\n",
    "Most methods in this class are static and work with the global `ras` object by default, though you can also pass in a custom RAS object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Boundary Conditions and Tables\n",
    "\n",
    "The `extract_boundary_and_tables()` method from the `RasUnsteady` class allows us to extract boundary conditions and their associated tables from an unsteady flow file.\n",
    "\n",
    "Parameters for `RasUnsteady.extract_boundary_and_tables()`:\n",
    "- `unsteady_file` (str): Path to the unsteady flow file\n",
    "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
    "\n",
    "Returns:\n",
    "- `pd.DataFrame`: DataFrame containing boundary conditions and their associated tables\n",
    "\n",
    "Let's see how this works with our example project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to unsteady flow file \"02\"\n",
    "unsteady_file = RasPlan.get_unsteady_path(\"02\")\n",
    "print(f\"Unsteady flow file path: {unsteady_file}\")\n",
    "\n",
    "# Extract boundary conditions and tables\n",
    "boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\n",
    "print(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Print Boundaries and Tables\n",
    "\n",
    "The `print_boundaries_and_tables()` method provides a formatted display of the boundary conditions and their associated tables. This method doesn't return anything; it just prints the information in a readable format.\n",
    "\n",
    "Parameters for `RasUnsteady.print_boundaries_and_tables()`:\n",
    "- `boundaries_df` (pd.DataFrame): DataFrame containing boundary conditions from `extract_boundary_and_tables()`\n",
    "\n",
    "Let's use this method to get a better understanding of our boundary conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the boundaries and tables in a formatted way\n",
    "print(\"Detailed boundary conditions and tables:\")\n",
    "RasUnsteady.print_boundaries_and_tables(boundaries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Boundary Condition Types\n",
    "\n",
    "The output above shows the different types of boundary conditions in our unsteady flow file. Let's understand what each type means:\n",
    "\n",
    "1. **Flow Hydrograph**: A time series of flow values (typically in cfs or cms) entering the model at a specific location. These are used at upstream boundaries or internal points where flow enters the system.\n",
    "\n",
    "2. **Stage Hydrograph**: A time series of water surface elevations (typically in ft or m) that define the downstream boundary condition.\n",
    "\n",
    "3. **Gate Openings**: Time series of gate settings (typically height in ft or m) for hydraulic structures such as spillways, sluice gates, or other control structures.\n",
    "\n",
    "4. **Lateral Inflow Hydrograph**: Flow entering the system along a reach, not at a specific point. This can represent tributary inflows, overland flow, or other distributed inputs.\n",
    "\n",
    "5. **Normal Depth**: A boundary condition where the water surface slope is assumed to equal the bed slope. This is represented by a friction slope value.\n",
    "\n",
    "Let's look at a specific boundary condition in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the first boundary condition in more detail\n",
    "if not boundaries_df.empty:\n",
    "    first_boundary = boundaries_df.iloc[0]\n",
    "    print(f\"Detailed look at boundary condition {1}:\")\n",
    "    \n",
    "    # Print boundary location components\n",
    "    print(f\"\\nBoundary Location:\")\n",
    "    print(f\"  River Name: {first_boundary.get('River Name', 'N/A')}\")\n",
    "    print(f\"  Reach Name: {first_boundary.get('Reach Name', 'N/A')}\")\n",
    "    print(f\"  River Station: {first_boundary.get('River Station', 'N/A')}\")\n",
    "    print(f\"  Storage Area Name: {first_boundary.get('Storage Area Name', 'N/A')}\")\n",
    "    \n",
    "    # Print boundary condition type and other properties\n",
    "    print(f\"\\nBoundary Properties:\")\n",
    "    print(f\"  Boundary Type: {first_boundary.get('bc_type', 'N/A')}\")\n",
    "    print(f\"  DSS File: {first_boundary.get('DSS File', 'N/A')}\")\n",
    "    print(f\"  Use DSS: {first_boundary.get('Use DSS', 'N/A')}\")\n",
    "    \n",
    "    # Print table statistics if available\n",
    "    if 'Tables' in first_boundary and isinstance(first_boundary['Tables'], dict):\n",
    "        print(f\"\\nTable Information:\")\n",
    "        for table_name, table_df in first_boundary['Tables'].items():\n",
    "            print(f\"  {table_name}: {len(table_df)} values\")\n",
    "            if not table_df.empty:\n",
    "                print(f\"    Min Value: {table_df['Value'].min()}\")\n",
    "                print(f\"    Max Value: {table_df['Value'].max()}\")\n",
    "                print(f\"    First 5 Values: {table_df['Value'].head(5).tolist()}\")\n",
    "else:\n",
    "    print(\"No boundary conditions found in the unsteady flow file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update Flow Title\n",
    "\n",
    "The flow title in an unsteady flow file provides a description of the simulation scenario. The `update_flow_title()` method allows us to modify this title.\n",
    "\n",
    "Parameters for `RasUnsteady.update_flow_title()`:\n",
    "- `unsteady_file` (str): Full path to the unsteady flow file\n",
    "- `new_title` (str): New flow title (max 24 characters)\n",
    "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
    "\n",
    "Let's clone an unsteady flow file and update its title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
    "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
    "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
    "\n",
    "# Get the path to the new unsteady flow file\n",
    "new_unsteady_file = RasPlan.get_unsteady_path(new_unsteady_number)\n",
    "print(f\"New unsteady flow file path: {new_unsteady_file}\")\n",
    "\n",
    "# Get the current flow title\n",
    "current_title = None\n",
    "for _, row in ras.unsteady_df.iterrows():\n",
    "    if row['unsteady_number'] == new_unsteady_number and 'Flow Title' in row:\n",
    "        current_title = row['Flow Title']\n",
    "        break\n",
    "print(f\"Current flow title: {current_title}\")\n",
    "\n",
    "# Update the flow title\n",
    "new_title = \"Modified Flow Scenario\"\n",
    "RasUnsteady.update_flow_title(new_unsteady_file, new_title)\n",
    "print(f\"Updated flow title to: {new_title}\")\n",
    "\n",
    "# Refresh unsteady flow information to see the change\n",
    "ras.unsteady_df = ras.get_unsteady_entries()\n",
    "display.display(ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update Restart Settings\n",
    "\n",
    "Restart files in HEC-RAS allow you to continue a simulation from a specific point in time, which can save computational resources. The `update_restart_settings()` method allows you to configure restart options.\n",
    "\n",
    "Parameters for `RasUnsteady.update_restart_settings()`:\n",
    "- `unsteady_file` (str): Full path to the unsteady flow file\n",
    "- `use_restart` (bool): Whether to use restart (True) or not (False)\n",
    "- `restart_filename` (str, optional): Name of the restart file (required if use_restart is True)\n",
    "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
    "\n",
    "Let's update the restart settings for our new unsteady flow file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update restart settings\n",
    "restart_filename = f\"{ras.project_name}.rst\"\n",
    "RasUnsteady.update_restart_settings(new_unsteady_file, use_restart=True, restart_filename=restart_filename)\n",
    "print(f\"Updated restart settings to use restart file: {restart_filename}\")\n",
    "\n",
    "# Let's extract the boundaries again to see the updated settings\n",
    "updated_boundaries_df = RasUnsteady.extract_boundary_and_tables(new_unsteady_file)\n",
    "\n",
    "# Check if there's a \"Use Restart\" property in the updated file\n",
    "use_restart_value = None\n",
    "restart_filename_value = None\n",
    "with open(new_unsteady_file, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"Use Restart=\"):\n",
    "            use_restart_value = line.strip().split('=')[1].strip()\n",
    "        elif line.startswith(\"Restart Filename=\"):\n",
    "            restart_filename_value = line.strip().split('=')[1].strip()\n",
    "\n",
    "print(f\"\\nVerified restart settings:\")\n",
    "print(f\"Use Restart: {use_restart_value}\")\n",
    "print(f\"Restart Filename: {restart_filename_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOES NOT WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Working with Flow Tables\n",
    "\n",
    "Flow tables in unsteady flow files contain the time-series data for boundary conditions. Let's explore how to extract and work with these tables using some of the advanced methods from the `RasUnsteady` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific tables from the unsteady flow file\n",
    "all_tables = RasUnsteady.extract_tables(new_unsteady_file)\n",
    "print(f\"Extracted {len(all_tables)} tables from the unsteady flow file.\")\n",
    "\n",
    "# Let's look at the available table names\n",
    "print(\"\\nAvailable tables:\")\n",
    "for table_name in all_tables.keys():\n",
    "    print(f\"  {table_name}\")\n",
    "\n",
    "# Select the first table for detailed analysis\n",
    "if all_tables and len(all_tables) > 0:\n",
    "    first_table_name = list(all_tables.keys())[0]\n",
    "    first_table = all_tables[first_table_name]\n",
    "    \n",
    "    print(f\"\\nDetailed look at table '{first_table_name}':\")\n",
    "    print(f\"  Number of values: {len(first_table)}\")\n",
    "    print(f\"  Min value: {first_table['Value'].min()}\")\n",
    "    print(f\"  Max value: {first_table['Value'].max()}\")\n",
    "    print(f\"  Mean value: {first_table['Value'].mean():.2f}\")\n",
    "    print(f\"  First 10 values: {first_table['Value'].head(10).tolist()}\")\n",
    "    \n",
    "    # Create a visualization of the table values\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(first_table['Value'].values)\n",
    "        plt.title(f\"{first_table_name} Values\")\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create visualization: {e}\")\n",
    "else:\n",
    "    print(\"No tables found in the unsteady flow file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Modifying Flow Tables\n",
    "\n",
    "Now let's demonstrate how to modify a flow table and write it back to the unsteady flow file. For this example, we'll scale all the values in a table by a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, identify tables in the unsteady flow file\n",
    "tables = RasUnsteady.identify_tables(open(new_unsteady_file, 'r').readlines())\n",
    "print(f\"Identified {len(tables)} tables in the unsteady flow file.\")\n",
    "\n",
    "# Let's look at the first flow hydrograph table\n",
    "flow_hydrograph_tables = [t for t in tables if t[0] == 'Flow Hydrograph=']\n",
    "if flow_hydrograph_tables:\n",
    "    table_name, start_line, end_line = flow_hydrograph_tables[0]\n",
    "    print(f\"\\nSelected table: {table_name}\")\n",
    "    print(f\"  Start line: {start_line}\")\n",
    "    print(f\"  End line: {end_line}\")\n",
    "    \n",
    "    # Parse the table\n",
    "    lines = open(new_unsteady_file, 'r').readlines()\n",
    "    table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
    "    print(f\"\\nOriginal table statistics:\")\n",
    "    print(f\"  Number of values: {len(table_df)}\")\n",
    "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
    "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
    "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
    "    \n",
    "    # Modify the table - let's scale all values by 75%\n",
    "    scale_factor = 0.75\n",
    "    table_df['Value'] = table_df['Value'] * scale_factor\n",
    "    print(f\"\\nModified table statistics (scaled by {scale_factor}):\")\n",
    "    print(f\"  Number of values: {len(table_df)}\")\n",
    "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
    "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
    "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
    "    \n",
    "    # Write the modified table back to the file\n",
    "    RasUnsteady.write_table_to_file(new_unsteady_file, table_name, table_df, start_line)\n",
    "    print(f\"\\nUpdated table written back to the unsteady flow file.\")\n",
    "    \n",
    "    # Re-read the table to verify changes\n",
    "    lines = open(new_unsteady_file, 'r').readlines()\n",
    "    updated_table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
    "    print(f\"\\nVerified updated table statistics:\")\n",
    "    print(f\"  Number of values: {len(updated_table_df)}\")\n",
    "    print(f\"  Min value: {updated_table_df['Value'].min()}\")\n",
    "    print(f\"  Max value: {updated_table_df['Value'].max()}\")\n",
    "    print(f\"  First 5 values: {updated_table_df['Value'].head(5).tolist()}\")\n",
    "else:\n",
    "    print(\"No flow hydrograph tables found in the unsteady flow file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Applying the Updated Unsteady Flow to a Plan\n",
    "\n",
    "Now that we've modified an unsteady flow file, let's create a plan that uses it, and compute the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone an existing plan\n",
    "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Modified Flow Test\")\n",
    "print(f\"New plan created: {new_plan_number}\")\n",
    "\n",
    "# Set the modified unsteady flow for the new plan\n",
    "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
    "print(f\"Set unsteady flow {new_unsteady_number} for plan {new_plan_number}\")\n",
    "\n",
    "# Update the plan description\n",
    "new_description = \"Test plan using modified unsteady flow\\nFlow scaled to 75% of original\\nWith restart file enabled\"\n",
    "RasPlan.update_plan_description(new_plan_number, new_description)\n",
    "print(f\"Updated plan description for plan {new_plan_number}\")\n",
    "\n",
    "# Set computation options\n",
    "RasPlan.set_num_cores(new_plan_number, 2)\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_plan_number,\n",
    "    computation_interval=\"1MIN\",\n",
    "    output_interval=\"15MIN\",\n",
    "    mapping_interval=\"1HOUR\"\n",
    ")\n",
    "print(f\"Updated computation settings for plan {new_plan_number}\")\n",
    "\n",
    "# Compute the plan\n",
    "print(f\"\\nComputing plan {new_plan_number} with modified unsteady flow...\")\n",
    "success = RasCmdr.compute_plan(new_plan_number)\n",
    "\n",
    "if success:\n",
    "    print(f\"Plan {new_plan_number} computed successfully\")\n",
    "    \n",
    "    # Check the results path\n",
    "    results_path = RasPlan.get_results_path(new_plan_number)\n",
    "    if results_path:\n",
    "        print(f\"Results available at: {results_path}\")\n",
    "        \n",
    "        # If it exists, get its size\n",
    "        results_file = Path(results_path)\n",
    "        if results_file.exists():\n",
    "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "else:\n",
    "    print(f\"Failed to compute plan {new_plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Unsteady Flow Operations\n",
    "\n",
    "In this notebook, we've covered the following unsteady flow operations using RAS Commander:\n",
    "\n",
    "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
    "2. **Boundary Extraction**: We extracted boundary conditions and tables from unsteady flow files\n",
    "3. **Boundary Analysis**: We inspected and understood boundary condition structures\n",
    "4. **Flow Title Updates**: We modified the title of an unsteady flow file\n",
    "5. **Restart Settings**: We configured restart file settings for continuing simulations\n",
    "6. **Table Extraction**: We extracted flow tables for analysis\n",
    "7. **Table Modification**: We modified a flow table and wrote it back to the file\n",
    "8. **Application**: We created a plan using our modified unsteady flow and computed results\n",
    "\n",
    "### Key Classes and Functions Used\n",
    "\n",
    "- `RasUnsteady.extract_boundary_and_tables()`: Extract boundary conditions and tables\n",
    "- `RasUnsteady.print_boundaries_and_tables()`: Display formatted boundary information\n",
    "- `RasUnsteady.update_flow_title()`: Modify the flow title\n",
    "- `RasUnsteady.update_restart_settings()`: Configure restart options\n",
    "- `RasUnsteady.extract_tables()`: Extract tables from unsteady flow files\n",
    "- `RasUnsteady.identify_tables()`: Identify table locations in file\n",
    "- `RasUnsteady.parse_fixed_width_table()`: Parse fixed-width tables\n",
    "- `RasUnsteady.write_table_to_file()`: Write modified tables back to file\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further explore unsteady flow operations with RAS Commander, consider:\n",
    "\n",
    "1. **Advanced Flow Modifications**: Create scripts that systematically modify flow hydrographs\n",
    "2. **Sensitivity Analysis**: Create variations of unsteady flows to assess model sensitivity\n",
    "3. **Batch Processing**: Process multiple unsteady flow files for scenario analysis\n",
    "4. **Custom Boundary Conditions**: Create unsteady flows from external data sources\n",
    "5. **Results Analysis**: Compare results from different unsteady flow scenarios\n",
    "\n",
    "These advanced topics can be explored by building on the foundation established in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\04_multiple_project_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Multiple Project Operations\n",
    "\n",
    "This notebook demonstrates how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. This advanced workflow is useful for comparing different river systems, running scenario analyses across multiple watersheds, or managing a suite of related models.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Multiple Project Initialization**: Initialize and manage multiple HEC-RAS projects simultaneously\n",
    "2. **Cross-Project Operations**: Clone and modify plans across different projects\n",
    "3. **Parallel Execution**: Run computations for multiple projects in parallel\n",
    "4. **Resource Management**: Optimize computing resources when working with multiple models\n",
    "5. **Results Comparison**: Analyze and compare results from different projects\n",
    "6. **Advanced Project Workflow**: Build a comprehensive multi-project workflow\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Working Environment\n",
    "\n",
    "Let's set up our working directory and check the number of available CPU cores. Since we'll be running multiple projects in parallel, it's important to understand our system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects we'll use in this tutorial\n",
    "# This will download them if not present and extract them to the example_projects folder\n",
    "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
    "print(extracted_paths)\n",
    "\n",
    "# Get the parent directory of the first extracted path as our examples directory\n",
    "examples_dir = extracted_paths[0].parent\n",
    "print(f\"Examples directory: {examples_dir}\")\n",
    "\n",
    "\n",
    "# Define paths to the extracted projects\n",
    "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
    "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
    "muncie_path = examples_dir / \"Muncie\"\n",
    "\n",
    "# Verify the paths exist\n",
    "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
    "    print(f\"Path {path} exists: {path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define computation output paths\n",
    "bald_eagle_compute_folder = examples_dir / \"compute_bald_eagle\"\n",
    "muncie_compute_folder = examples_dir / \"compute_muncie\"\n",
    "\n",
    "# Check system resources\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "physical_cpu_count = psutil.cpu_count(logical=False)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print(f\"System Resources:\")\n",
    "print(f\"- {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
    "print(f\"- {available_memory_gb:.1f} GB available memory\")\n",
    "print(f\"For multiple HEC-RAS projects, a good rule of thumb is:\")\n",
    "print(f\"- Assign 2-4 cores per project\")\n",
    "print(f\"- Allocate at least 2-4 GB of RAM per project\")\n",
    "print(f\"Based on your system, you could reasonably run {min(physical_cpu_count//2, int(available_memory_gb//3))} projects simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Multiple RAS Project Management\n",
    "\n",
    "When working with multiple HEC-RAS projects in RAS Commander, there are two important concepts to understand:\n",
    "\n",
    "1. **The Global 'ras' Object**: By default, RAS Commander maintains a global `ras` object that represents the currently active project. This is convenient for simple scripts.\n",
    "\n",
    "2. **Custom RAS Objects**: For multiple projects, you'll create separate RAS objects for each project. These custom objects store project-specific data and are passed to RAS Commander functions using the `ras_object` parameter.\n",
    "\n",
    "### Best Practices for Multiple Project Management\n",
    "\n",
    "- **Name Your Objects Clearly**: Use descriptive variable names for your RAS objects (e.g., `bald_eagle_ras`, `muncie_ras`)\n",
    "- **Be Consistent**: Always pass the appropriate RAS object to functions when working with multiple projects\n",
    "- **Avoid Using Global 'ras'**: When working with multiple projects, avoid using the global `ras` object to prevent confusion\n",
    "- **Separate Compute Folders**: Use separate computation folders for each project\n",
    "- **Manage Resources**: Be mindful of CPU and memory usage when running multiple projects in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Projects\n",
    "\n",
    "We'll use the `RasExamples` class to download and extract two example HEC-RAS projects: \"Balde Eagle Creek\" and \"Muncie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing project if it exists to ensure a clean start\n",
    "if examples_dir.exists():\n",
    "    shutil.rmtree(examples_dir)\n",
    "    print(f\"Removed existing example projects directory: {examples_dir}\")\n",
    "\n",
    "# Create a RasExamples instance\n",
    "ras_examples = RasExamples()\n",
    "\n",
    "# Extract the example projects\n",
    "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\", \"Muncie\"])\n",
    "print(f\"Extracted projects to:\")\n",
    "for path in extracted_paths:\n",
    "    print(f\"- {path}\")\n",
    "\n",
    "# Verify the paths exist\n",
    "print(f\"\\nBald Eagle Creek project exists: {bald_eagle_path.exists()}\")\n",
    "print(f\"Muncie project exists: {muncie_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Multiple Projects\n",
    "\n",
    "Let's initialize both HEC-RAS projects. Instead of using the global `ras` object, we'll create separate RAS objects for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both projects with their own RAS objects\n",
    "bald_eagle_ras = RasPrj()\n",
    "init_ras_project(bald_eagle_path, \"6.6\", ras_object=bald_eagle_ras)\n",
    "print(f\"Initialized Bald Eagle Creek project: {bald_eagle_ras.project_name}\")\n",
    "\n",
    "muncie_ras = RasPrj()\n",
    "init_ras_project(muncie_path, \"6.6\", ras_object=muncie_ras)\n",
    "print(f\"Initialized Muncie project: {muncie_ras.project_name}\")\n",
    "\n",
    "# Display available plans in each project\n",
    "print(\"\\nAvailable plans in Bald Eagle Creek project:\")\n",
    "display.display(bald_eagle_ras.plan_df)\n",
    "\n",
    "print(\"\\nAvailable plans in Muncie project:\")\n",
    "display.display(muncie_ras.plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Plans in Each Project\n",
    "\n",
    "Now, let's clone a plan in each project, giving them custom short identifiers. This demonstrates how to perform operations on multiple projects independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone plans with custom short identifiers\n",
    "new_bald_eagle_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=bald_eagle_ras)\n",
    "print(f\"Created new plan {new_bald_eagle_plan} in Bald Eagle Creek project\")\n",
    "\n",
    "new_muncie_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=muncie_ras)\n",
    "print(f\"Created new plan {new_muncie_plan} in Muncie project\")\n",
    "\n",
    "# Display the updated plan dataframes\n",
    "print(\"\\nUpdated plans in Bald Eagle Creek project:\")\n",
    "bald_eagle_ras.plan_df = bald_eagle_ras.get_plan_entries()  # Refresh the plan dataframe\n",
    "display.display(bald_eagle_ras.plan_df)\n",
    "\n",
    "print(\"\\nUpdated plans in Muncie project:\")\n",
    "muncie_ras.plan_df = muncie_ras.get_plan_entries()  # Refresh the plan dataframe\n",
    "display.display(muncie_ras.plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Plans for Both Projects\n",
    "\n",
    "Let's configure the plans for both projects, setting geometry, number of cores, and other parameters. This demonstrates how to customize plans for different projects using the same code structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Bald Eagle Creek plan\n",
    "print(\"Configuring Bald Eagle Creek plan:\")\n",
    "RasPlan.set_geom(new_bald_eagle_plan, \"01\", ras_object=bald_eagle_ras)\n",
    "RasPlan.set_num_cores(new_bald_eagle_plan, 2, ras_object=bald_eagle_ras)\n",
    "\n",
    "# Update description and intervals\n",
    "description = \"Multi-project demonstration plan\\nBald Eagle Creek project\\nConfigured for parallel execution\"\n",
    "RasPlan.update_plan_description(new_bald_eagle_plan, description, ras_object=bald_eagle_ras)\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_bald_eagle_plan, \n",
    "    computation_interval=\"10SEC\", \n",
    "    output_interval=\"5MIN\", \n",
    "    ras_object=bald_eagle_ras\n",
    ")\n",
    "print(\"Successfully configured Bald Eagle Creek plan\")\n",
    "\n",
    "# Configure the Muncie plan\n",
    "print(\"\\nConfiguring Muncie plan:\")\n",
    "RasPlan.set_geom(new_muncie_plan, \"01\", ras_object=muncie_ras)\n",
    "RasPlan.set_num_cores(new_muncie_plan, 2, ras_object=muncie_ras)\n",
    "\n",
    "# Update description and intervals\n",
    "description = \"Multi-project demonstration plan\\nMuncie project\\nConfigured for parallel execution\"\n",
    "RasPlan.update_plan_description(new_muncie_plan, description, ras_object=muncie_ras)\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_muncie_plan, \n",
    "    computation_interval=\"10SEC\", \n",
    "    output_interval=\"5MIN\", \n",
    "    ras_object=muncie_ras\n",
    ")\n",
    "print(\"Successfully configured Muncie plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Compute Folders for Both Projects\n",
    "\n",
    "Now, let's create separate compute folders for each project. This allows us to run the computations separately and in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute folders or clean them if they already exist\n",
    "for folder in [bald_eagle_compute_folder, muncie_compute_folder]:\n",
    "    if folder.exists():\n",
    "        shutil.rmtree(folder)\n",
    "        print(f\"Removed existing compute folder: {folder}\")\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created compute folder: {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Project Execution Function\n",
    "\n",
    "Let's define a function to execute plans for each project, which we can run in parallel. This function will handle plan execution, timing, and provide detailed status updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_plan(plan_number, ras_object, compute_folder, project_name):\n",
    "    \"\"\"\n",
    "    Execute a HEC-RAS plan and return detailed information about the execution.\n",
    "    \n",
    "    Args:\n",
    "        plan_number (str): The plan number to execute\n",
    "        ras_object: The RAS project object\n",
    "        compute_folder (Path): Folder where computation will be performed\n",
    "        project_name (str): A descriptive name for the project\n",
    "        \n",
    "    Returns:\n",
    "        dict: Detailed information about the execution\n",
    "    \"\"\"\n",
    "    print(f\"Starting execution of plan {plan_number} for {project_name}...\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the plan in the compute folder\n",
    "    success = RasCmdr.compute_plan(\n",
    "        plan_number=plan_number, \n",
    "        ras_object=ras_object, \n",
    "        dest_folder=compute_folder,\n",
    "        clear_geompre=True\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Determine if results were created\n",
    "    result_path = None\n",
    "    result_size = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize a temporary RAS object in the compute folder to check results\n",
    "        compute_ras = init_ras_project(compute_folder, ras_object.ras_exe_path)\n",
    "        result_path = RasPlan.get_results_path(plan_number, ras_object=compute_ras)\n",
    "        \n",
    "        if result_path:\n",
    "            result_file = Path(result_path)\n",
    "            if result_file.exists():\n",
    "                result_size = result_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking results for {project_name}: {e}\")\n",
    "    \n",
    "    # Build result information\n",
    "    result_info = {\n",
    "        \"project_name\": project_name,\n",
    "        \"plan_number\": plan_number,\n",
    "        \"success\": success,\n",
    "        \"duration\": duration,\n",
    "        \"compute_folder\": str(compute_folder),\n",
    "        \"result_path\": str(result_path) if result_path else None,\n",
    "        \"result_size_mb\": result_size,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    print(f\"Completed execution of plan {plan_number} for {project_name} in {duration:.2f} seconds\")\n",
    "    return result_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute Plans for Both Projects in Parallel\n",
    "\n",
    "Now, let's run both projects in parallel using a `ThreadPoolExecutor`. This allows us to utilize our system resources efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing plans for both projects in parallel...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Define the execution tasks\n",
    "execution_tasks = [\n",
    "    (new_bald_eagle_plan, bald_eagle_ras, bald_eagle_compute_folder, \"Bald Eagle Creek\"),\n",
    "    (new_muncie_plan, muncie_ras, muncie_compute_folder, \"Muncie\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Execute the plans in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [\n",
    "        executor.submit(execute_plan, *task)\n",
    "        for task in execution_tasks\n",
    "    ]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Execution error: {e}\")\n",
    "\n",
    "print(\"\\nAll executions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results\n",
    "\n",
    "Let's analyze the results from both project executions, comparing execution times, result sizes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Execution Results Summary:\")\n",
    "display.display(results_df[['project_name', 'plan_number', 'success', 'duration', 'result_size_mb']])\n",
    "\n",
    "# Create a bar chart for execution times\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['project_name'], results_df['duration'], color=['blue', 'green'])\n",
    "plt.title('Execution Time by Project')\n",
    "plt.xlabel('Project')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add duration values on top of the bars\n",
    "for i, duration in enumerate(results_df['duration']):\n",
    "    plt.text(i, duration + 5, f\"{duration:.1f}s\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If we have result sizes, create a chart for those as well\n",
    "if results_df['result_size_mb'].notna().any():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(results_df['project_name'], results_df['result_size_mb'], color=['orange', 'purple'])\n",
    "    plt.title('Result File Size by Project')\n",
    "    plt.xlabel('Project')\n",
    "    plt.ylabel('Result Size (MB)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add size values on top of the bars\n",
    "    for i, size in enumerate(results_df['result_size_mb']):\n",
    "        if pd.notna(size):\n",
    "            plt.text(i, size + 2, f\"{size:.1f} MB\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Two HEC-RAS Projects\n",
    "\n",
    "Let's create a utility function to compare the structures of the two HEC-RAS projects. This helps us understand the differences between the projects we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_project_structures(ras_object1, name1, ras_object2, name2):\n",
    "    \"\"\"\n",
    "    Compare the structures of two HEC-RAS projects and display differences.\n",
    "    \"\"\"\n",
    "    # Refresh all dataframes to ensure we have the latest data\n",
    "    ras_object1.plan_df = ras_object1.get_plan_entries()\n",
    "    ras_object1.geom_df = ras_object1.get_geom_entries()\n",
    "    ras_object1.flow_df = ras_object1.get_flow_entries()\n",
    "    ras_object1.unsteady_df = ras_object1.get_unsteady_entries()\n",
    "    \n",
    "    ras_object2.plan_df = ras_object2.get_plan_entries()\n",
    "    ras_object2.geom_df = ras_object2.get_geom_entries()\n",
    "    ras_object2.flow_df = ras_object2.get_flow_entries()\n",
    "    ras_object2.unsteady_df = ras_object2.get_unsteady_entries()\n",
    "    \n",
    "    # Create a comparison dictionary\n",
    "    comparison = {\n",
    "        'Project Name': [ras_object1.project_name, ras_object2.project_name],\n",
    "        'Plan Count': [len(ras_object1.plan_df), len(ras_object2.plan_df)],\n",
    "        'Geometry Count': [len(ras_object1.geom_df), len(ras_object2.geom_df)],\n",
    "        'Flow Count': [len(ras_object1.flow_df), len(ras_object2.flow_df)],\n",
    "        'Unsteady Count': [len(ras_object1.unsteady_df), len(ras_object2.unsteady_df)]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame for the comparison\n",
    "    comparison_df = pd.DataFrame(comparison, index=[name1, name2])\n",
    "    \n",
    "\n",
    "    # Display the comparison\n",
    "    print(\"Project Structure Comparison:\")\n",
    "    display.display(comparison_df)\n",
    "    \n",
    "    # Create a bar chart to visualize the comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    comparison_df.iloc[:, 1:].plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Project Structure Comparison')\n",
    "    plt.xlabel('Project')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Component')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set y-axis to only show whole numbers (integers)\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare the structures of the two projects\n",
    "comparison_df = compare_project_structures(\n",
    "    bald_eagle_ras, \"Bald Eagle Creek\", \n",
    "    muncie_ras, \"Muncie\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Multiple Project Operations\n",
    "\n",
    "In this notebook, we've demonstrated how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. We've covered the following key operations:\n",
    "\n",
    "1. **Initializing Multiple Projects**: Creating separate RAS objects for different projects\n",
    "2. **Independent Configuration**: Configuring plans with project-specific parameters\n",
    "3. **Parallel Execution**: Running computations from different projects simultaneously\n",
    "4. **Resource Management**: Organizing compute folders and tracking execution statistics\n",
    "5. **Results Comparison**: Analyzing and comparing results from different projects\n",
    "6. **Advanced Workflows**: Creating sensitivity plans and batch processing pipelines\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "When working with multiple HEC-RAS projects in RAS Commander, remember these key concepts:\n",
    "\n",
    "- **Custom RAS Objects**: Create and use separate RAS objects for each project\n",
    "- **Always Specify ras_object**: Use the `ras_object` parameter in all function calls\n",
    "- **Separate Compute Folders**: Use separate folders for each project's computations\n",
    "- **Resource Management**: Be mindful of CPU and memory usage when running in parallel\n",
    "- **Project Tracking**: Keep track of which results belong to which project\n",
    "\n",
    "### Multiple Project Applications\n",
    "\n",
    "Working with multiple projects unlocks advanced applications such as:\n",
    "\n",
    "1. **Model Comparison**: Compare results from different river systems\n",
    "2. **Basin-wide Analysis**: Analyze connected river systems in parallel\n",
    "3. **Parameter Sweep**: Test a range of parameters across multiple models\n",
    "4. **Model Development**: Develop and test models simultaneously\n",
    "5. **Batch Processing**: Process large sets of models in an automated pipeline\n",
    "\n",
    "These capabilities make RAS Commander a powerful tool for large-scale hydraulic modeling and water resources management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\05_single_plan_execution.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Single Plan Execution\n",
    "\n",
    "This notebook demonstrates how to execute a single HEC-RAS plan using the RAS Commander library. We'll focus specifically on running a plan with a specified number of processor cores while overwriting an existing computation folder.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Plan Overview**: Explore the available plans in the project\n",
    "3. **Core Execution Configuration**: Set the number of processor cores to use during computation\n",
    "4. **Destination Folder Management**: Use and overwrite computation folders \n",
    "5. **Results Verification**: Check the results paths after computation\n",
    "6. **Performance Considerations**: Understand the impact of core count on performance\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Working Environment\n",
    "\n",
    "Let's set up our working directory and paths to example projects. We'll also check the number of available CPU cores on this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to example projects\n",
    "examples_dir = bald_eagle_path.parent\n",
    "\n",
    "# Define computation output paths\n",
    "compute_dest_folder = examples_dir / \"compute_test\"\n",
    "\n",
    "# Check system resources\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "physical_cpu_count = psutil.cpu_count(logical=False)\n",
    "print(f\"System has {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
    "print(f\"For HEC-RAS computation, it's often most efficient to use 2-8 cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasCmdr.compute_plan Method\n",
    "\n",
    "Before we dive into execution, let's understand the `compute_plan` method from the `RasCmdr` class, which is the core function for running HEC-RAS simulations.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (str, Path): The plan number to execute or the full path to the plan file\n",
    "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
    "- `ras_object` (RasPrj, optional): Specific RAS object to use (defaults to global `ras`)\n",
    "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files (default: False)\n",
    "- `num_cores` (int, optional): Number of processor cores to use (default: None, uses plan settings)\n",
    "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists (default: False)\n",
    "\n",
    "### Returns\n",
    "- `bool`: True if the execution was successful, False otherwise\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Destination Folder**: By default, the simulation runs in the original project folder. Specifying a destination folder creates a copy of the project in that location for execution, leaving the original project untouched.\n",
    "\n",
    "2. **Number of Cores**: HEC-RAS can use multiple processor cores to speed up computation. The optimal number depends on the model complexity and your computer's specifications. Generally:\n",
    "   - 1-2 cores: Good for small models, highest efficiency per core\n",
    "   - 3-8 cores: Good balance for most models\n",
    "   - >8 cores: Diminishing returns, may actually be slower due to overhead\n",
    "\n",
    "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties, which is useful after making geometry changes.\n",
    "\n",
    "4. **Overwrite Destination**: Controls whether an existing destination folder should be overwritten. This is a safety feature to prevent accidental deletion of important results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore Available Plans\n",
    "\n",
    "Let's examine the available plans in the project to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the available plans in the project\n",
    "print(\"Available plans in the project:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Let's check the current setting for number of cores in the plans\n",
    "print(\"\\nCurrent core settings for plans:\")\n",
    "for plan_num in ras.plan_df['plan_number']:\n",
    "    # Check all three core parameters\n",
    "    d1_cores = RasPlan.get_plan_value(plan_num, \"UNET D1 Cores\")\n",
    "    d2_cores = RasPlan.get_plan_value(plan_num, \"UNET D2 Cores\") \n",
    "    ps_cores = RasPlan.get_plan_value(plan_num, \"PS Cores\")\n",
    "    \n",
    "    print(f\"Plan {plan_num}'s Existing Settings:\")\n",
    "    print(f\"  1D Cores: {d1_cores}\")\n",
    "    print(f\"  2D Cores: {d2_cores}\")\n",
    "    print(f\"  Pump Station Cores: {ps_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Destination Folder Structure\n",
    "\n",
    "Now, let's prepare a destination folder for our computation. This allows us to run simulations without modifying the original project files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a destination folder path\n",
    "dest_folder = examples_dir / \"compute_test_cores\"\n",
    "\n",
    "# Check if the destination folder already exists\n",
    "if dest_folder.exists():\n",
    "    print(f\"Destination folder already exists: {dest_folder}\")\n",
    "    print(\"We'll use overwrite_dest=True to replace it\")\n",
    "else:\n",
    "    print(f\"Destination folder will be created: {dest_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Execute a Plan with a Specified Number of Cores\n",
    "\n",
    "Now we're ready to execute a plan with a specified number of cores, overwriting the destination folder if it exists. This is the core functionality demonstrated in Example 5 of the original script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a plan and number of cores\n",
    "plan_number = \"01\"\n",
    "num_cores = 2  # Specify the number of cores to use\n",
    "\n",
    "print(f\"Executing plan {plan_number} with {num_cores} cores...\")\n",
    "print(f\"Destination folder: {dest_folder}\")\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the plan with specified parameters\n",
    "success = RasCmdr.compute_plan(\n",
    "    plan_number,              # The plan to execute\n",
    "    dest_folder=dest_folder,  # Where to run the simulation\n",
    "    num_cores=num_cores,      # Number of processor cores to use\n",
    "    overwrite_dest=True       # Overwrite destination folder if it exists\n",
    ")\n",
    "\n",
    "# Record the end time and calculate duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Report results\n",
    "if success:\n",
    "    print(f\"✅ Plan {plan_number} executed successfully using {num_cores} cores\")\n",
    "    print(f\"Execution time: {duration:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"❌ Plan {plan_number} execution failed\")\n",
    "    print(f\"Time elapsed: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Results\n",
    "\n",
    "After execution, let's verify the results by checking the results paths and examining the destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the destination folder exists and contains the expected files\n",
    "if dest_folder.exists():\n",
    "    print(f\"Destination folder exists: {dest_folder}\")\n",
    "    \n",
    "    # List the key files in the destination folder\n",
    "    print(\"\\nKey files in destination folder:\")\n",
    "    project_files = list(dest_folder.glob(f\"{ras.project_name}.*\"))\n",
    "    for file in project_files[:10]:  # Show first 10 files\n",
    "        file_size = file.stat().st_size / 1024  # Size in KB\n",
    "        print(f\"  {file.name}: {file_size:.1f} KB\")\n",
    "    \n",
    "    if len(project_files) > 10:\n",
    "        print(f\"  ... and {len(project_files) - 10} more files\")\n",
    "    \n",
    "    # Check for HDF result files\n",
    "    print(\"\\nHDF result files:\")\n",
    "    hdf_files = list(dest_folder.glob(f\"*.hdf\"))\n",
    "    for file in hdf_files:\n",
    "        file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "        print(f\"  {file.name}: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"Destination folder does not exist: {dest_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results path using the RasPlan.get_results_path method\n",
    "# First, initialize a RAS object using the destination folder\n",
    "try:\n",
    "    dest_ras = RasPrj()\n",
    "    init_ras_project(dest_folder, \"6.6\", ras_object=dest_ras)\n",
    "    \n",
    "    # Get the results path for the plan we just executed\n",
    "    results_path = RasPlan.get_results_path(plan_number, ras_object=dest_ras)\n",
    "    \n",
    "    if results_path:\n",
    "        print(f\"Results for plan {plan_number} are located at: {results_path}\")\n",
    "        \n",
    "        # Check if the file exists and get its size\n",
    "        results_file = Path(results_path)\n",
    "        if results_file.exists():\n",
    "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"No results found for plan {plan_number} in the destination folder\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Single Plan Execution Options\n",
    "\n",
    "The `RasCmdr.compute_plan()` method provides a flexible way to execute HEC-RAS plans with various options. Here's a summary of the key parameters we've explored:\n",
    "\n",
    "1. **Basic Execution**: Simply provide a plan number\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\")\n",
    "   ```\n",
    "\n",
    "2. **Destination Folder**: Run in a separate folder to preserve the original project\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\")\n",
    "   ```\n",
    "\n",
    "3. **Number of Cores**: Control the CPU resources used\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", num_cores=2)\n",
    "   ```\n",
    "\n",
    "4. **Overwrite Destination**: Replace existing computation folders\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\", overwrite_dest=True)\n",
    "   ```\n",
    "\n",
    "5. **Clear Geometry Preprocessor**: Force recalculation of geometric properties\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", clear_geompre=True)\n",
    "   ```\n",
    "\n",
    "6. **Combined Options**: Use multiple options together\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\n",
    "       \"01\",\n",
    "       dest_folder=\"path/to/folder\",\n",
    "       num_cores=2,\n",
    "       clear_geompre=True,\n",
    "       overwrite_dest=True\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further enhance your HEC-RAS automation, consider exploring:\n",
    "\n",
    "1. **Parallel Execution**: Use `RasCmdr.compute_parallel()` to run multiple plans simultaneously\n",
    "2. **Test Mode**: Use `RasCmdr.compute_test_mode()` for testing purposes\n",
    "3. **Pre-Processing**: Modify plans, geometries, and unsteady flows before execution\n",
    "4. **Post-Processing**: Analyze results after computation\n",
    "5. **Batch Processing**: Create scripts for parameter sweeps or scenario analysis\n",
    "\n",
    "These advanced topics are covered in other examples and documentation for the RAS Commander library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\06_executing_plan_sets.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Executing Plan Sets \n",
    "\n",
    "This notebook demonstrates different ways to specify and execute HEC-RAS plans using the RAS Commander library. Proper plan specification is essential for efficient model execution, especially when working with large projects containing multiple plans.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project and explore available plans\n",
    "2. **Sequential Execution of Specific Plans**: Select and run particular plans in sequence\n",
    "3. **Parallel Execution of Specific Plans**: Run selected plans simultaneously\n",
    "4. **Executing All Plans**: Run every plan in a project\n",
    "5. **Filtered Plan Selection**: Select plans based on criteria or patterns\n",
    "6. **Conditional Execution**: Run plans based on results of previous executions\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Found zip file: c:\\GH\\ras-commander\\examples\\Example_Projects_6_6.zip\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Loading project data from CSV...\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Loaded 68 projects from CSV.\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - ----- RasExamples Extracting Project -----\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Extracting project 'Balde Eagle Creek'\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Project 'Balde Eagle Creek' already exists. Deleting existing folder...\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Existing folder for project 'Balde Eagle Creek' has been deleted.\n",
      "2025-03-14 14:26:11 - ras_commander.RasExamples - INFO - Successfully extracted project 'Balde Eagle Creek' to c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted project to: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n",
      "Bald Eagle Creek project exists: True\n"
     ]
    }
   ],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Plan Specification in HEC-RAS\n",
    "\n",
    "In HEC-RAS, each plan (`.p*` file) represents a specific hydraulic model simulation scenario. When working with RAS Commander, you can specify plans for execution in several ways:\n",
    "\n",
    "1. **Single Plan**: Specify one plan by its number (e.g., \"01\")\n",
    "2. **List of Plans**: Specify multiple plans as a list (e.g., [\"01\", \"03\", \"05\"])\n",
    "3. **All Plans**: Execute all plans in a project by not specifying any plan or passing `None`\n",
    "4. **Filtered Plans**: Select plans based on criteria (e.g., plans with specific flow conditions)\n",
    "5. **Plan Path**: Specify the full path to a plan file instead of just the number\n",
    "\n",
    "### Why Plan Specification Matters\n",
    "\n",
    "- **Efficiency**: Run only the plans you need rather than recomputing everything\n",
    "- **Organization**: Group related plans for batch processing\n",
    "- **Automation**: Create workflows that process plans in a specific order\n",
    "- **Resource Management**: Optimize hardware utilization for specific plans\n",
    "\n",
    "### Best Practices for Plan Specification\n",
    "\n",
    "- Use consistent formatting for plan numbers (e.g., always use two-digit strings like \"01\" instead of 1)\n",
    "- Check available plans before attempting to execute them\n",
    "- Organize plans by purpose to make selection easier\n",
    "- Use descriptive short identifiers and plan titles to aid in selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function and explore the available plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:26:11 - ras_commander.RasPrj - INFO - Initializing global 'ras' object via init_ras_project function.\n",
      "2025-03-14 14:26:11 - ras_commander.RasPrj - INFO - Project initialized. ras_object project folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized HEC-RAS project: BaldEagle\n",
      "\n",
      "Available plans in the project:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plan_number</th>\n",
       "      <th>full_path</th>\n",
       "      <th>UNET D1 Cores</th>\n",
       "      <th>UNET D2 Cores</th>\n",
       "      <th>PS Cores</th>\n",
       "      <th>Computation Interval</th>\n",
       "      <th>DSS File</th>\n",
       "      <th>Flow File</th>\n",
       "      <th>Friction Slope Method</th>\n",
       "      <th>Geom File</th>\n",
       "      <th>...</th>\n",
       "      <th>Run HTab</th>\n",
       "      <th>Run PostProcess</th>\n",
       "      <th>Run Sediment</th>\n",
       "      <th>Run UNet</th>\n",
       "      <th>Run WQNet</th>\n",
       "      <th>Short Identifier</th>\n",
       "      <th>Simulation Date</th>\n",
       "      <th>UNET Use Existing IB Tables</th>\n",
       "      <th>HDF_Results_Path</th>\n",
       "      <th>Geom_File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>c:\\GH\\ras-commander\\examples\\example_projects\\...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2MIN</td>\n",
       "      <td>dss</td>\n",
       "      <td>u02</td>\n",
       "      <td>2</td>\n",
       "      <td>g01</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UnsteadyFlow</td>\n",
       "      <td>18FEB1999,0000,24FEB1999,0500</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>c:\\GH\\ras-commander\\examples\\example_projects\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>c:\\GH\\ras-commander\\examples\\example_projects\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2MIN</td>\n",
       "      <td>dss</td>\n",
       "      <td>f02</td>\n",
       "      <td>1</td>\n",
       "      <td>g01</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SteadyRun</td>\n",
       "      <td>02/18/1999,0000,02/24/1999,0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>c:\\GH\\ras-commander\\examples\\example_projects\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  plan_number                                          full_path  \\\n",
       "0          01  c:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
       "1          02  c:\\GH\\ras-commander\\examples\\example_projects\\...   \n",
       "\n",
       "   UNET D1 Cores  UNET D2 Cores PS Cores Computation Interval DSS File  \\\n",
       "0            0.0            0.0     None                 2MIN      dss   \n",
       "1            NaN            NaN     None                 2MIN      dss   \n",
       "\n",
       "  Flow File Friction Slope Method Geom File  ... Run HTab Run PostProcess  \\\n",
       "0       u02                     2       g01  ...        1               1   \n",
       "1       f02                     1       g01  ...        1               1   \n",
       "\n",
       "  Run Sediment Run UNet Run WQNet Short Identifier  \\\n",
       "0            0        1         0     UnsteadyFlow   \n",
       "1          NaN        1       NaN        SteadyRun   \n",
       "\n",
       "                   Simulation Date UNET Use Existing IB Tables  \\\n",
       "0    18FEB1999,0000,24FEB1999,0500                          -1   \n",
       "1  02/18/1999,0000,02/24/1999,0500                         NaN   \n",
       "\n",
       "  HDF_Results_Path                                          Geom_File  \n",
       "0             None  c:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
       "1             None  c:\\GH\\ras-commander\\examples\\example_projects\\...  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:26:11 - ras_commander.RasPlan - WARNING - No description found in plan file: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p01\n",
      "2025-03-14 14:26:11 - ras_commander.RasPlan - WARNING - No description found in plan file: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek\\BaldEagle.p02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plan details:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plan Number</th>\n",
       "      <th>Short ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Geometry</th>\n",
       "      <th>Has Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>UnsteadyFlow</td>\n",
       "      <td></td>\n",
       "      <td>g01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>SteadyRun</td>\n",
       "      <td></td>\n",
       "      <td>g01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Plan Number      Short ID Description Geometry  Has Results\n",
       "0          01  UnsteadyFlow                  g01        False\n",
       "1          02     SteadyRun                  g01        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nAvailable plans in the project:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Check plan details to understand what each plan represents\n",
    "plan_details = []\n",
    "for index, row in ras.plan_df.iterrows():\n",
    "    plan_number = row['plan_number']\n",
    "    \n",
    "    # Get plan description if available\n",
    "    description = None\n",
    "    if 'description' in row:\n",
    "        description = row['description']\n",
    "    else:\n",
    "        try:\n",
    "            description = RasPlan.read_plan_description(plan_number)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Get short identifier if available\n",
    "    short_id = None\n",
    "    if 'Short Identifier' in row:\n",
    "        short_id = row['Short Identifier']\n",
    "    \n",
    "    # Get geometry file\n",
    "    geom_file = None\n",
    "    if 'Geom File' in row:\n",
    "        geom_file = row['Geom File']\n",
    "    \n",
    "    # Check if the plan has results\n",
    "    has_results = False\n",
    "    if 'HDF_Results_Path' in row and row['HDF_Results_Path']:\n",
    "        has_results = True\n",
    "    \n",
    "    plan_details.append({\n",
    "        'Plan Number': plan_number,\n",
    "        'Short ID': short_id,\n",
    "        'Description': description[:50] + '...' if description and len(description) > 50 else description,\n",
    "        'Geometry': geom_file,\n",
    "        'Has Results': has_results\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with the plan details\n",
    "plan_details_df = pd.DataFrame(plan_details)\n",
    "print(\"\\nPlan details:\")\n",
    "display.display(plan_details_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sequential Execution of Specific Plans\n",
    "\n",
    "Let's execute specific plans in sequence using `RasCmdr.compute_test_mode()` with a list of plan numbers. This approach allows us to run only the plans we need, in the order we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Starting the compute_test_mode...\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Creating the test folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]...\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Compute folder 'c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]' exists. Overwriting as per overwrite_dest=True.\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Copied project folder to compute folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Initialized RAS project in compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.prj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing specific plans sequentially...\n",
      "This may take several minutes...\n",
      "Selected plans: 01, 03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Getting plan entries...\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Retrieved plan entries successfully.\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '03']\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Running selected plans sequentially...\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\n",
      "2025-03-14 14:26:11 - ras_commander.RasUtils - INFO - Using provided plan file path: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.p01\n",
      "2025-03-14 14:26:11 - ras_commander.RasUtils - INFO - Successfully updated file: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.p01\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Set number of cores to 6 for plan: 01\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
      "2025-03-14 14:26:11 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.prj\" \"c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [SpecificSequential]\\BaldEagle.p01\"\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - HEC-RAS execution completed for plan: 01\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 154.35 seconds\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Successfully computed plan 01\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Total run time for plan 01: 154.40 seconds\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - All selected plans have been executed.\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - compute_test_mode completed.\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - \n",
      "Execution Results:\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Plan 01: Successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential execution of specific plans completed in 154.50 seconds\n",
      "\n",
      "Sequential Execution Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plan</th>\n",
       "      <th>Success</th>\n",
       "      <th>Execution Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>True</td>\n",
       "      <td>Sequential</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Plan  Success Execution Type\n",
       "0   01     True     Sequential"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Executing specific plans sequentially...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Define the plans to execute\n",
    "specific_plans = [\"01\", \"03\"]\n",
    "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute specific plans sequentially\n",
    "execution_results = RasCmdr.compute_test_mode(\n",
    "    plan_number=specific_plans,\n",
    "    dest_folder_suffix=\"[SpecificSequential]\",\n",
    "    num_cores=6, \n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "sequential_duration = end_time - start_time\n",
    "\n",
    "print(f\"Sequential execution of specific plans completed in {sequential_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "sequential_results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Sequential\"}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "sequential_results_df \n",
    "\n",
    "# Ensure the 'Plan' column exists before sorting\n",
    "if 'Plan' in sequential_results_df.columns:\n",
    "    sequential_results_df = sequential_results_df.sort_values(\"Plan\")\n",
    "else:\n",
    "    print(\"Warning: 'Plan' column not found in execution results.\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nSequential Execution Results:\")\n",
    "display.display(sequential_results_df)\n",
    "\n",
    "# Check the test folder\n",
    "test_folder = bald_eagle_path.parent / f\"{ras.project_name} [SpecificSequential]\"\n",
    "if test_folder.exists():\n",
    "    print(f\"\\nTest folder exists: {test_folder}\")\n",
    "    \n",
    "    # Check for results\n",
    "    hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
    "    if hdf_files:\n",
    "        print(f\"Found {len(hdf_files)} HDF result files:\")\n",
    "        for file in hdf_files:\n",
    "            file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "            print(f\"  {file.name}: {file_size:.1f} MB\")\n",
    "    else:\n",
    "        print(\"No HDF result files found in the test folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running Only Plans Without HDF Results\n",
    "An important use case is to identify and execute only those plans that have no existing HDF results. This approach can save time by avoiding redundant computations, especially useful when adding new plans to an existing project or after making limited changes.\n",
    "\n",
    "Let's demonstrate how to:\n",
    "\n",
    "- Use the `ras` object to identify plans without results\n",
    "- Create a filtered list of these plans\n",
    "- Execute only the missing plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Starting the compute_test_mode...\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Creating the test folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]...\n",
      "2025-03-14 14:28:45 - ras_commander.RasCmdr - INFO - Compute folder 'c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]' exists. Overwriting as per overwrite_dest=True.\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Copied project folder to compute folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Initialized RAS project in compute folder: C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.prj\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Getting plan entries...\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Retrieved plan entries successfully.\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Filtered plans to execute: ['01', '02']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying and executing plans without HDF results...\n",
      "Found 2 plans without HDF results: 01, 02\n",
      "\n",
      "Executing 2 plans without results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Running selected plans sequentially...\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Using ras_object with project folder: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\n",
      "2025-03-14 14:28:46 - ras_commander.RasUtils - INFO - Using provided plan file path: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p01\n",
      "2025-03-14 14:28:46 - ras_commander.RasUtils - INFO - Successfully updated file: c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p01\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Set number of cores to 6 for plan: 01\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Running HEC-RAS from the Command Line:\n",
      "2025-03-14 14:28:46 - ras_commander.RasCmdr - INFO - Running command: \"C:\\Program Files (x86)\\HEC\\HEC-RAS\\6.6\\Ras.exe\" -c \"C:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.prj\" \"c:\\GH\\ras-commander\\examples\\example_projects\\Balde Eagle Creek [MissingPlans]\\BaldEagle.p01\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Identifying and executing plans without HDF results...\")\n",
    "\n",
    "# Use the ras object to determine which plans don't have results\n",
    "plans_no_results = ras.plan_df[ras.plan_df['HDF_Results_Path'].isna()]['plan_number'].tolist()\n",
    "\n",
    "if not plans_no_results:\n",
    "    print(\"All plans already have HDF results. Creating a test scenario...\")\n",
    "    # For demonstration purposes, pretend some plans don't have results\n",
    "    plans_no_results = [\"04\", \"05\"]\n",
    "    print(f\"Simulating no results for plans: {', '.join(plans_no_results)}\")\n",
    "else:\n",
    "    print(f\"Found {len(plans_no_results)} plans without HDF results: {', '.join(plans_no_results)}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute only the plans without results\n",
    "if plans_no_results:\n",
    "    print(f\"\\nExecuting {len(plans_no_results)} plans without results...\")\n",
    "    execution_results = RasCmdr.compute_test_mode(\n",
    "        plan_number=plans_no_results,\n",
    "        dest_folder_suffix=\"[MissingPlans]\",\n",
    "        num_cores=6, \n",
    "        overwrite_dest=True\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution completed in {duration:.2f} seconds\")\n",
    "    \n",
    "    # Create a DataFrame from the execution results\n",
    "    missing_results_df = pd.DataFrame([\n",
    "        {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Missing Plans\"}\n",
    "        for plan, success in execution_results.items()\n",
    "    ])\n",
    "    \n",
    "    # Sort by plan number\n",
    "    missing_results_df = missing_results_df.sort_values(\"Plan\")\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\nExecution Results for Plans Without HDF Results:\")\n",
    "    display.display(missing_results_df)\n",
    "    \n",
    "    # Check the test folder\n",
    "    test_folder = bald_eagle_path.parent / f\"{ras.project_name} [MissingPlans]\"\n",
    "    if test_folder.exists():\n",
    "        print(f\"\\nTest folder exists: {test_folder}\")\n",
    "        \n",
    "        # Check for results\n",
    "        hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
    "        if hdf_files:\n",
    "            print(f\"Found {len(hdf_files)} HDF result files:\")\n",
    "            for file in hdf_files:\n",
    "                file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "                print(f\"  {file.name}: {file_size:.1f} MB\")\n",
    "        else:\n",
    "            print(\"No HDF result files found in the test folder\")\n",
    "else:\n",
    "    print(\"No plans without results to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Results\n",
    "After executing the plans that were missing HDF results, it's important to verify that the results were properly generated. Let's check if the execution actually created the expected output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the project with the test folder to see updated results\n",
    "missing_plans_folder = bald_eagle_path.parent / f\"{ras.project_name} [MissingPlans]\"\n",
    "\n",
    "if missing_plans_folder.exists():\n",
    "    # Initialize the project from the test folder\n",
    "    test_ras = RasPrj()\n",
    "    init_ras_project(missing_plans_folder, \"6.6\", ras_object=test_ras)\n",
    "    \n",
    "    # Check which plans now have results\n",
    "    plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]['plan_number'].tolist()\n",
    "    \n",
    "    print(f\"Plans with results after execution: {', '.join(plans_with_results)}\")\n",
    "    \n",
    "    # Verify if all previously missing plans now have results\n",
    "    all_generated = all(plan in plans_with_results for plan in plans_no_results)\n",
    "    \n",
    "    if all_generated:\n",
    "        print(\"✅ Successfully generated results for all missing plans\")\n",
    "    else:\n",
    "        print(\"⚠️ Some plans still don't have results after execution\")\n",
    "        missing_after = [plan for plan in plans_no_results if plan not in plans_with_results]\n",
    "        print(f\"Plans still missing results: {', '.join(missing_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Plan Specification Techniques\n",
    "\n",
    "In this notebook, we've explored different ways to specify and execute HEC-RAS plans using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
    "\n",
    "1. **Basic Plan Specification**\n",
    "   - Single plan by number: `\"01\"`\n",
    "   - List of specific plans: `[\"01\", \"03\"]`\n",
    "   - All plans: `ras.plan_df['plan_number'].tolist()`\n",
    "\n",
    "2. **Advanced Selection**\n",
    "   - Categorization: Grouping plans by purpose or type\n",
    "   - Dependencies: Ensuring prerequisite plans are run first\n",
    "   - Ordered execution: Running plans in a specific sequence\n",
    "\n",
    "3. **Run Plans with Missing Results (HDF)**\n",
    "   - Using ras object to determine which plans have results\n",
    "   - Creating a list of plans with no results\n",
    "   - Running those plans sequentially\n",
    "\n",
    "4. NOTE: run_parallel can also run a list of plans, but compute_plan is only made for single plan execution.  \n",
    "\n",
    "\n",
    "### Best Practices for Plan Specification\n",
    "\n",
    "1. **Consistent Formatting**: Use two-digit strings for plan numbers (\"01\" instead of 1)\n",
    "2. **Descriptive Naming**: Use meaningful short identifiers that describe the plan's purpose\n",
    "3. **Verify Availability**: Check that specified plans exist before trying to execute them\n",
    "4. **Document Dependencies**: Keep track of which plans depend on others\n",
    "5. **Use Appropriate Execution Method**: Choose sequential or parallel based on dependencies and resources\n",
    "6. **Monitor Performance**: Track execution times to identify optimization opportunities\n",
    "\n",
    "By applying these techniques, you can create efficient and organized workflows for executing HEC-RAS plans, from simple batch processing to complex dependency-based execution sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\07_sequential_plan_execution.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Sequential Plan Execution\n",
    "\n",
    "This notebook demonstrates how to sequentially execute multiple HEC-RAS plans using the RAS Commander library. Sequential execution is useful for batch processing plans that need to be run in a specific order or when you want to ensure consistent resource usage across multiple runs.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Sequential Execution of All Plans**: Run all plans in a project sequentially in a test folder\n",
    "3. **Selective Plan Execution**: Run only specific plans in sequence\n",
    "4. **Geometry Preprocessor Management**: Clear geometry preprocessor files before execution\n",
    "5. **Execution Result Analysis**: Track and analyze the results of sequential executions\n",
    "6. **Performance Monitoring**: Monitor and compare execution times across different runs\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define examples_dir as parent of bald_eagle_path\n",
    "examples_dir = bald_eagle_path.parent\n",
    "print(f\"Examples directory set to: {examples_dir}\")\n",
    "\n",
    "    \n",
    "# Remove any compute test folders from previous runs\n",
    "for folder in examples_dir.glob(\"*[[]AllSequential[]]*\"):\n",
    "    if folder.is_dir():\n",
    "        print(f\"Removing existing test folder: {folder}\")\n",
    "        shutil.rmtree(folder)\n",
    "        \n",
    "for folder in examples_dir.glob(\"*[[]SpecificSequential*[]]*\"):\n",
    "    if folder.is_dir():\n",
    "        print(f\"Removing existing test folder: {folder}\")\n",
    "        shutil.rmtree(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Sequential Execution in HEC-RAS\n",
    "\n",
    "HEC-RAS simulations can be executed in several ways:\n",
    "\n",
    "1. **Single Plan Execution**: Run one plan at a time using `RasCmdr.compute_plan()`\n",
    "2. **Sequential Execution**: Run multiple plans one after another using `RasCmdr.compute_test_mode()`\n",
    "3. **Parallel Execution**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
    "\n",
    "This notebook focuses on the second approach: **Sequential Execution**. Here are the key benefits of sequential execution:\n",
    "\n",
    "- **Controlled Resource Usage**: By running plans one at a time, you ensure consistent resource usage\n",
    "- **Dependency Management**: When later plans depend on results from earlier plans\n",
    "- **Simplified Debugging**: Easier to identify which plan is causing an issue when they run sequentially\n",
    "- **Consistent Test Environment**: All plans run in the same isolated folder\n",
    "\n",
    "The `compute_test_mode()` function from `RasCmdr` is specifically designed for this purpose. It creates a separate test folder, copies the project there, and executes the specified plans in sequential order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Project\n",
    "\n",
    "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nHEC-RAS Project Plan Data:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Check how many plans we have\n",
    "plan_count = len(ras.plan_df)\n",
    "print(f\"Found {plan_count} plans in the project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasCmdr.compute_test_mode Method\n",
    "\n",
    "Before we start executing plans, let's understand the `compute_test_mode()` method from the `RasCmdr` class, which we'll use for sequential execution.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (str, list[str], optional): Plan number or list of plan numbers to execute. If None, all plans will be executed.\n",
    "- `dest_folder_suffix` (str, optional): Suffix to append to the test folder name. Defaults to \"[Test]\".\n",
    "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.\n",
    "- `num_cores` (int, optional): Maximum number of cores to use for each plan. If None, the current setting is not changed.\n",
    "- `ras_object` (RasPrj, optional): Specific RAS object to use. If None, uses the global ras object.\n",
    "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists. Defaults to False.\n",
    "\n",
    "### Return Value\n",
    "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Test Folder**: The function creates a separate folder with the specified suffix, copying the project there for execution.\n",
    "2. **Sequential Execution**: Plans are executed one after another in the specified order.\n",
    "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties.\n",
    "4. **Destination Folder Option**: The suffix determines the name of the test folder. Unlike `compute_plan()`, you can't specify an arbitrary destination folder.\n",
    "5. **Overwrite Option**: Controls whether an existing test folder should be overwritten.\n",
    "\n",
    "Now, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sequential Execution of All Plans\n",
    "\n",
    "Let's execute all plans in the project sequentially. This will create a test folder with the suffix \"[AllSequential]\" and run all plans one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing all plans sequentially...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute all plans sequentially\n",
    "# - dest_folder_suffix: Suffix to append to the test folder name\n",
    "# - overwrite_dest: Overwrite the destination folder if it exists\n",
    "# - no ras object is specified, it will use the default \"ras\" object\n",
    "execution_results = RasCmdr.compute_test_mode(\n",
    "    dest_folder_suffix=\"[AllSequential]\",\n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Sequential execution of all plans completed in {total_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examining the Test Folder\n",
    "\n",
    "Let's examine the test folder created by `compute_test_mode()` to better understand what happened during sequential execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test folder path\n",
    "test_folder = script_dir / f\"{ras.project_name} [AllSequential]\"\n",
    "\n",
    "if test_folder.exists():\n",
    "    print(f\"Test folder exists: {test_folder}\")\n",
    "    \n",
    "    # List the key files in the test folder\n",
    "    print(\"\\nKey files in test folder:\")\n",
    "    \n",
    "    # First, list the project file and all plan files\n",
    "    prj_files = list(test_folder.glob(\"*.prj\"))\n",
    "    plan_files = list(test_folder.glob(\"*.p*\"))\n",
    "    plan_files.sort()\n",
    "    \n",
    "    if prj_files:\n",
    "        print(f\"Project file: {prj_files[0].name}\")\n",
    "    \n",
    "    print(\"Plan files:\")\n",
    "    for plan_file in plan_files:\n",
    "        file_size = plan_file.stat().st_size / 1024  # Size in KB\n",
    "        print(f\"  {plan_file.name}: {file_size:.1f} KB\")\n",
    "    \n",
    "    # Look for HDF result files\n",
    "    hdf_files = list(test_folder.glob(\"*.hdf\"))\n",
    "    hdf_files.sort()\n",
    "    \n",
    "    print(\"\\nHDF files:\")\n",
    "    for hdf_file in hdf_files:\n",
    "        file_size = hdf_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "        print(f\"  {hdf_file.name}: {file_size:.1f} MB\")\n",
    "    \n",
    "    # Geometry preprocessor files (if any)\n",
    "    geompre_files = list(test_folder.glob(\"*.c*\"))\n",
    "    geompre_files.sort()\n",
    "    \n",
    "    if geompre_files:\n",
    "        print(\"\\nGeometry preprocessor files:\")\n",
    "        for geompre_file in geompre_files:\n",
    "            file_size = geompre_file.stat().st_size / 1024  # Size in KB\n",
    "            print(f\"  {geompre_file.name}: {file_size:.1f} KB\")\n",
    "    else:\n",
    "        print(\"\\nNo geometry preprocessor files found\")\n",
    "        \n",
    "    # Initialize a RAS project in the test folder to inspect results\n",
    "    try:\n",
    "        test_ras = RasPrj()\n",
    "        init_ras_project(test_folder, ras.ras_exe_path, ras_object=test_ras)\n",
    "        print(\"\\nPlans with results in the test folder:\")\n",
    "        test_plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]\n",
    "        display.display(test_plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing test folder as a RAS project: {e}\")\n",
    "else:\n",
    "    print(f\"Test folder not found: {test_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sequential Execution of Specific Plans\n",
    "\n",
    "Now, let's execute only specific plans in the project. We'll select plans \"01\" and \"02\" and run them sequentially with the `clear_geompre` option set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing specific plans sequentially with clearing geometry preprocessor files...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Define the plans to execute\n",
    "selected_plans = [\"01\", \"02\"]\n",
    "print(f\"Selected plans: {', '.join(selected_plans)}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute specific plans sequentially\n",
    "# - plan_number: List of plan numbers to execute\n",
    "# - dest_folder_suffix: Suffix to append to the test folder name\n",
    "# - clear_geompre: Clear geometry preprocessor files before execution\n",
    "# - overwrite_dest: Overwrite the destination folder if it exists\n",
    "execution_results = RasCmdr.compute_test_mode(\n",
    "    plan_number=selected_plans,\n",
    "    dest_folder_suffix=\"[SpecificSequentialClearGeompre]\",\n",
    "    clear_geompre=True,\n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Sequential execution of specific plans completed in {total_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Sequential Plan Execution\n",
    "\n",
    "In this notebook, we've explored how to execute HEC-RAS plans sequentially using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
    "\n",
    "1. **Project Setup and Initialization**: Setting up the environment and initializing a HEC-RAS project\n",
    "2. **Example Project Management**: Using `RasExamples` to download and extract sample projects\n",
    "3. **Basic Sequential Execution**: Using `RasCmdr.compute_test_mode()` to run all plans in a project\n",
    "4. **Test Folder Analysis**: Examining the contents and results of sequential execution\n",
    "5. **Selective Plan Execution**: Running specific plans with geometry preprocessor clearing\n",
    "\n",
    "### Key Functions Used\n",
    "\n",
    "- `init_ras_project()`: Initialize a HEC-RAS project\n",
    "- `RasExamples.extract_project()`: Extract example projects for testing\n",
    "- `RasCmdr.compute_test_mode()`: Run plans sequentially in a test folder\n",
    "- `Path.glob()`: Examine test folder contents and results\n",
    "- `RasCmdr.compute_test_mode(clear_geompre=True)`: Execute plans with preprocessor clearing\n",
    "\n",
    "### Best Practices for Sequential Execution\n",
    "\n",
    "1. **Environment Setup**: Ensure all required libraries are installed and properly imported\n",
    "2. **Project Organization**: Clean up existing test folders before new executions\n",
    "3. **Resource Management**: Monitor system resources (CPU cores, memory) for optimal performance\n",
    "4. **Test Folder Naming**: Use meaningful suffixes to distinguish different execution runs\n",
    "5. **Performance Tracking**: Monitor execution times for each sequential run\n",
    "6. **Results Verification**: Check test folders for successful plan execution and result files\n",
    "7. **Selective Execution**: Use plan filtering when only specific plans need to be run\n",
    "\n",
    "With these techniques, you can effectively manage and execute HEC-RAS simulations sequentially, whether running all plans or a selected subset with specific configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\08_parallel_execution.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Parallel Plan Execution\n",
    "\n",
    "This notebook demonstrates how to execute multiple HEC-RAS plans in parallel using the RAS Commander library. Parallel execution allows you to make better use of your computer's processing power by running multiple plans simultaneously.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project and prepare it for parallel execution\n",
    "2. **Parallel Execution of All Plans**: Run all plans in a project simultaneously\n",
    "3. **Selective Parallel Execution**: Run only specific plans in parallel\n",
    "4. **Dynamic Worker Allocation**: Automatically determine the optimal number of parallel workers\n",
    "5. **Resource Management**: Optimize CPU core utilization for parallel runs\n",
    "6. **Results Comparison**: Analyze and visualize execution performance\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import math  # Import math to avoid NameError in get_optimal_worker_count function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Working Environment\n",
    "\n",
    "Let's set up our working directory and check the system resources available for parallel execution. This will help us make informed decisions about how many workers to use.\n",
    "\n",
    "For this notebook we will be using the \"Muncie\" HEC Example Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Muncie example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
    "print(f\"Extracted project to: {muncie_path}\")  \n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {muncie_path.exists()}\")\n",
    "\n",
    "\n",
    "# Create compute folders\n",
    "compute_folder = muncie_path.parent / \"compute_test_parallel\"\n",
    "specific_compute_folder = muncie_path.parent / \"compute_test_parallel_specific\"\n",
    "dynamic_compute_folder = muncie_path.parent / \"compute_test_parallel_dynamic\"\n",
    "\n",
    "# Check system resources for parallel execution\n",
    "cpu_count = psutil.cpu_count(logical=True)  # Logical cores (including hyper-threading)\n",
    "physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)  # Total RAM in GB\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)  # Available RAM in GB\n",
    "\n",
    "print(f\"System Resources:\")\n",
    "print(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores with hyper-threading)\")\n",
    "print(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
    "\n",
    "# Functions to help with resource management\n",
    "def get_optimal_worker_count(cores_per_worker=2):\n",
    "    \"\"\"Calculate the optimal number of workers based on available physical cores.\"\"\"\n",
    "    optimal_workers = math.floor(physical_cores / cores_per_worker)\n",
    "    return max(1, optimal_workers)  # Ensure at least 1 worker\n",
    "\n",
    "print(f\"\\nFor parallel HEC-RAS execution:\")\n",
    "print(f\"- With 2 cores per worker: Can use up to {get_optimal_worker_count(2)} parallel workers\")\n",
    "print(f\"- With 4 cores per worker: Can use up to {get_optimal_worker_count(4)} parallel workers\")\n",
    "print(f\"\\nEach HEC-RAS instance typically requires 2-4 GB of RAM. Based on your available memory,\")\n",
    "print(f\"you could reasonably run {math.floor(available_memory_gb / 3)} instances simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Parallel Execution in HEC-RAS\n",
    "\n",
    "HEC-RAS simulations can be computationally intensive, especially for large models or long simulation periods. Parallel execution allows you to run multiple plans simultaneously, making better use of your computer's processing power.\n",
    "\n",
    "### Key Concepts in Parallel Execution\n",
    "\n",
    "1. **Workers**: Each worker is a separate process that can execute a HEC-RAS plan. The `max_workers` parameter determines how many plans can be executed simultaneously.\n",
    "\n",
    "2. **Cores per Worker**: Each worker (HEC-RAS instance) can utilize multiple CPU cores. The `num_cores` parameter sets how many cores each worker uses.\n",
    "\n",
    "3. **Resource Balancing**: Effective parallel execution requires balancing the number of workers with the cores per worker. Too many workers or too many cores per worker can lead to resource contention and slower overall performance.\n",
    "\n",
    "4. **Worker Folders**: Each worker gets its own folder with a copy of the project, allowing for isolated execution.\n",
    "\n",
    "### Parallel vs. Sequential Execution\n",
    "\n",
    "- **Parallel**: Multiple plans run simultaneously (good for independent plans, faster overall completion)\n",
    "- **Sequential**: Plans run one after another (good for dependent plans, consistent resource usage)\n",
    "\n",
    "### Optimal Configuration\n",
    "\n",
    "The optimal configuration depends on your hardware and the specific plans you're running:\n",
    "\n",
    "- For most models, 2-4 cores per worker provides good performance\n",
    "- Set `max_workers` based on available physical cores: `max_workers = floor(physical_cores / cores_per_worker)`\n",
    "- Ensure you have enough memory: each worker typically needs 2-4 GB of RAM\n",
    "\n",
    "Now, let's download and extract our example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Project\n",
    "\n",
    "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function. We'll store the initialized object in a variable to use later, rather than relying on the global `ras` object. This approach is more suitable for working with multiple projects or compute folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the source project\n",
    "source_project = init_ras_project(muncie_path, \"6.6\")\n",
    "print(f\"Initialized source project: {source_project.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nAvailable plans in the project:\")\n",
    "display.display(source_project.plan_df)\n",
    "\n",
    "# Check how many plans we have\n",
    "plan_count = len(source_project.plan_df)\n",
    "print(f\"Found {plan_count} plans in the project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasCmdr.compute_parallel Method\n",
    "\n",
    "Before we start executing plans in parallel, let's understand the `compute_parallel()` method from the `RasCmdr` class.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (Union[str, List[str], None]): Plan number(s) to compute. If None, all plans are computed.\n",
    "- `max_workers` (int): Maximum number of parallel workers (default: 2).\n",
    "- `num_cores` (int): Number of cores to use per plan computation (default: 2).\n",
    "- `clear_geompre` (bool): Whether to clear geometry preprocessor files (default: False).\n",
    "- `ras_object` (Optional[RasPrj]): Specific RAS object to use. If None, uses global ras instance.\n",
    "- `dest_folder` (Union[str, Path, None]): Destination folder for computed results.\n",
    "- `overwrite_dest` (bool): Whether to overwrite existing destination folder (default: False).\n",
    "\n",
    "### Return Value\n",
    "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Worker Assignment**: Plans are assigned to workers in a round-robin fashion. For example, with 3 workers and 5 plans, workers would be assigned as follows: Worker 1: Plans 1 & 4, Worker 2: Plans 2 & 5, Worker 3: Plan 3.\n",
    "\n",
    "2. **Worker Folders**: Each worker gets its own folder (a subdirectory of the destination folder) for isolated execution.\n",
    "\n",
    "3. **Result Consolidation**: After all plans are executed, results are consolidated into the destination folder.\n",
    "\n",
    "4. **Resource Management**: Each worker can use multiple cores as specified by `num_cores`.\n",
    "\n",
    "Now, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parallel Execution of All Plans\n",
    "\n",
    "Let's execute all plans in the project in parallel. We'll use 3 workers, with 2 cores per worker. This approach is good when you have multiple plans that are independent of each other and you want to complete them as quickly as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing all plans in parallel...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Create compute folder if it doesn't exist\n",
    "compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the parameters for parallel execution\n",
    "max_workers = 4\n",
    "cores_per_worker = 1\n",
    "\n",
    "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
    "print(f\"Destination folder: {compute_folder}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute all plans in parallel\n",
    "results_all = RasCmdr.compute_parallel(\n",
    "    max_workers=max_workers,\n",
    "    num_cores=cores_per_worker,\n",
    "    dest_folder=compute_folder,\n",
    "    overwrite_dest=True,\n",
    "    ras_object=source_project\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Parallel execution of all plans completed in {total_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in results_all.items()\n",
    "])\n",
    "\n",
    "# Sort by plan number\n",
    "results_df = results_df.sort_values(\"Plan\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examining the Parallel Execution Results\n",
    "\n",
    "Let's initialize a RAS project in the compute folder and examine the results of the parallel execution. This will help us understand what happened during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a RAS project in the compute folder\n",
    "compute_project = RasPrj()\n",
    "init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
    "print(f\"Initialized compute project: {compute_project.project_name}\")\n",
    "\n",
    "# Display the plan files in the compute folder\n",
    "print(\"\\nPlans in the compute folder:\")\n",
    "display.display(compute_project.plan_df)\n",
    "\n",
    "# Check which plans have results\n",
    "plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
    "print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
    "display.display(plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
    "\n",
    "# List the worker folders (they should have been removed during results consolidation)\n",
    "worker_folders = list(compute_folder.glob(\"*Worker*\"))\n",
    "if worker_folders:\n",
    "    print(f\"\\nFound {len(worker_folders)} worker folders:\")\n",
    "    for folder in worker_folders:\n",
    "        print(f\"  {folder.name}\")\n",
    "else:\n",
    "    print(\"\\nNo worker folders remain in the compute folder (they were removed during results consolidation)\")\n",
    "\n",
    "# Check for HDF result files\n",
    "hdf_files = list(compute_folder.glob(\"*.hdf\"))\n",
    "hdf_files.sort()\n",
    "\n",
    "print(f\"\\nFound {len(hdf_files)} HDF files in the compute folder:\")\n",
    "for file in hdf_files:\n",
    "    file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  {file.name}: {file_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Examples: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution of Specific Plans\n",
    "\n",
    "Now, let's execute only specific plans in the project in parallel. This approach is useful when you only want to run a subset of the available plans, perhaps for testing or comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing specific plans in parallel...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Create specific compute folder if it doesn't exist\n",
    "specific_compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the plans to execute\n",
    "specific_plans = [\"01\", \"03\"]\n",
    "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
    "\n",
    "# Define the parameters for parallel execution\n",
    "max_workers = 2  # One for each plan\n",
    "cores_per_worker = 2\n",
    "\n",
    "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
    "print(f\"Destination folder: {specific_compute_folder}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute specific plans in parallel\n",
    "results_specific = RasCmdr.compute_parallel(\n",
    "    plan_number=specific_plans,\n",
    "    max_workers=max_workers,\n",
    "    num_cores=cores_per_worker,\n",
    "    dest_folder=specific_compute_folder,\n",
    "    overwrite_dest=True,\n",
    "    ras_object=source_project\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "specific_duration = end_time - start_time\n",
    "\n",
    "print(f\"Parallel execution of specific plans completed in {specific_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "specific_results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in results_specific.items()\n",
    "])\n",
    "\n",
    "# Sort by plan number\n",
    "specific_results_df = specific_results_df.sort_values(\"Plan\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(specific_results_df)\n",
    "\n",
    "# Initialize a RAS project in the specific compute folder\n",
    "specific_compute_project = RasPrj()\n",
    "init_ras_project(specific_compute_folder, \"6.6\", ras_object=specific_compute_project)\n",
    "print(f\"\\nInitialized specific compute project: {specific_compute_project.project_name}\")\n",
    "\n",
    "# Check which plans have results\n",
    "specific_plans_with_results = specific_compute_project.plan_df[specific_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
    "print(f\"Found {len(specific_plans_with_results)} plans with results:\")\n",
    "display.display(specific_plans_with_results[['plan_number', 'HDF_Results_Path']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution with Max Workers Defined by Physical Cores (\"Dynamic Worker Allocation\") \n",
    "\n",
    "In this step, we'll determine the optimal number of workers based on the physical cores available on the system. This approach ensures that we make efficient use of the available hardware without overcommitting resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing plans with dynamic worker allocation...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Create dynamic compute folder if it doesn't exist\n",
    "dynamic_compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the cores per worker\n",
    "cores_per_worker = 4\n",
    "# 2 cores per worker is the efficiency point for most CPU's, due to L2/L3 cache being shared by 2 cores in most x86 CPU's\n",
    "# 4-8 cores per worker is the maximum performance point for most CPU's, using more compute power to marginally lower runtime \n",
    "# when using parallel compute, 2 cores per worker is typically optimal as it is assumed you are maximizing throughput (efficency) over single-plan runtime (performance)\n",
    "\n",
    "# Calculate the optimal number of workers based on physical cores\n",
    "max_workers = get_optimal_worker_count(cores_per_worker)\n",
    "print(f\"System has {physical_cores} physical cores\")\n",
    "print(f\"With {cores_per_worker} cores per worker, optimal worker count is {max_workers}\")\n",
    "print(f\"Destination folder: {dynamic_compute_folder}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute all plans with dynamic worker allocation\n",
    "results_dynamic = RasCmdr.compute_parallel(\n",
    "    plan_number=specific_plans,\n",
    "    max_workers=max_workers,\n",
    "    num_cores=cores_per_worker,\n",
    "    dest_folder=dynamic_compute_folder,\n",
    "    overwrite_dest=True,\n",
    "    ras_object=source_project\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "dynamic_duration = end_time - start_time\n",
    "\n",
    "print(f\"Parallel execution with dynamic worker allocation completed in {dynamic_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "dynamic_results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in results_dynamic.items()\n",
    "])\n",
    "\n",
    "# Sort by plan number\n",
    "dynamic_results_df = dynamic_results_df.sort_values(\"Plan\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(dynamic_results_df)\n",
    "\n",
    "# Initialize a RAS project in the dynamic compute folder\n",
    "dynamic_compute_project = RasPrj()\n",
    "init_ras_project(dynamic_compute_folder, \"6.6\", ras_object=dynamic_compute_project)\n",
    "print(f\"\\nInitialized dynamic compute project: {dynamic_compute_project.project_name}\")\n",
    "\n",
    "# Check which plans have results\n",
    "dynamic_plans_with_results = dynamic_compute_project.plan_df[dynamic_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
    "print(f\"Found {len(dynamic_plans_with_results)} plans with results:\")\n",
    "display.display(dynamic_plans_with_results[['plan_number', 'HDF_Results_Path']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of the different parallel execution approaches we've tried. This will help us understand the impact of worker count and plan selection on execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for individual plan runtimes\n",
    "plan_data = []\n",
    "\n",
    "# Define the approaches with more descriptive labels including worker and core counts\n",
    "approach_labels = {\n",
    "    \"all_plans\": \"All Plans (2 workers × 2 cores = 4 cores total)\",\n",
    "    \"specific_plans\": \"Specific Plans (1 worker × 2 cores = 2 cores total)\",\n",
    "    \"dynamic_workers\": f\"Dynamic Workers (1 worker × 4 cores = 4 cores total)\"\n",
    "}\n",
    "\n",
    "# Extract runtimes from the log messages\n",
    "# For all plans approach\n",
    "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"01\", \"Runtime\": 35.72})\n",
    "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"03\", \"Runtime\": 82.70})\n",
    "# Omitting plan 04 as it's a 1D model\n",
    "\n",
    "# For specific plans approach (plans 01 and 03 were run)\n",
    "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"01\", \"Runtime\": 29.10})\n",
    "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"03\", \"Runtime\": 36.09})\n",
    "\n",
    "# For dynamic worker approach (plans 01 and 03 were run)\n",
    "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"01\", \"Runtime\": 28.48})\n",
    "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"03\", \"Runtime\": 49.43})\n",
    "\n",
    "# Create a DataFrame\n",
    "plan_runtime_df = pd.DataFrame(plan_data)\n",
    "\n",
    "# Create a grouped bar chart for plan runtimes\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Get all unique plan numbers and ensure they're sorted\n",
    "plans = sorted(plan_runtime_df[\"Plan\"].unique())\n",
    "\n",
    "# Create x positions for the bars\n",
    "x = np.arange(len(plans))\n",
    "width = 0.25  # Width of the bars\n",
    "\n",
    "# Plot bars for each approach\n",
    "approaches = plan_runtime_df[\"Approach\"].unique()\n",
    "for i, approach in enumerate(approaches):\n",
    "    # Filter data for this approach\n",
    "    approach_data = plan_runtime_df[plan_runtime_df[\"Approach\"] == approach]\n",
    "    \n",
    "    # Initialize runtimes array with NaN values\n",
    "    runtimes = [np.nan] * len(plans)\n",
    "    \n",
    "    # Fill in runtimes where data exists\n",
    "    for j, plan in enumerate(plans):\n",
    "        plan_runtime = approach_data[approach_data[\"Plan\"] == plan][\"Runtime\"]\n",
    "        if not plan_runtime.empty:\n",
    "            runtimes[j] = plan_runtime.values[0]\n",
    "    \n",
    "    # Create bars for this approach (only where we have data)\n",
    "    valid_indices = [idx for idx, val in enumerate(runtimes) if not np.isnan(val)]\n",
    "    valid_plans = [plans[idx] for idx in valid_indices]\n",
    "    valid_runtimes = [runtimes[idx] for idx in valid_indices]\n",
    "    valid_positions = [x[idx] + (i - len(approaches)/2 + 0.5) * width for idx in valid_indices]\n",
    "    \n",
    "    # Plot the bars\n",
    "    bars = plt.bar(valid_positions, valid_runtimes, width, label=approach)\n",
    "    \n",
    "    # Add runtime labels on top of bars\n",
    "    for pos, runtime in zip(valid_positions, valid_runtimes):\n",
    "        plt.text(pos, runtime + 2, f\"{runtime:.1f}s\", ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels\n",
    "plt.xlabel('Plan Number', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.title('Runtime Comparison by Plan Number and Parallelization Approach', fontsize=14)\n",
    "plt.xticks(x, plans, fontsize=11)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add note about omitting Plan 04\n",
    "plt.figtext(0.5, 0.01, \"\\nNote: Plan 04 (1D model) is omitted from this comparison\", \n",
    "            ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# Ensure all plan numbers show on x-axis regardless of data availability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Parallel Plan Execution\n",
    "\n",
    "In this notebook, we've explored how to execute HEC-RAS plans in parallel using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
    "\n",
    "1. **Basic Parallel Execution**: Using `RasCmdr.compute_parallel()` to run all plans in a project simultaneously\n",
    "2. **Selective Parallel Execution**: Running only specific plans in parallel\n",
    "3. **Dynamic Worker Allocation**: Determining the optimal number of workers based on available system resources\n",
    "4. **Performance Analysis**: Comparing execution times for different parallel configurations\n",
    "5. **Advanced Parallel Workflows**: Building complex workflows with parallel execution for sensitivity analysis\n",
    "\n",
    "### Key Functions Used\n",
    "\n",
    "- `RasCmdr.compute_parallel()`: Execute multiple plans in parallel\n",
    "- `RasPlan.clone_plan()`: Create a new plan based on an existing one\n",
    "- `RasPlan.update_plan_description()`: Update the description of a plan\n",
    "- `RasPlan.set_num_cores()`: Set the number of cores for a plan to use\n",
    "- `RasPlan.get_results_path()`: Get the path to the results file for a plan\n",
    "\n",
    "### Best Practices for Parallel Execution\n",
    "\n",
    "1. **Use Separate RAS Objects**: Create and use separate RAS objects for different projects or folders\n",
    "2. **Balance Workers and Cores**: Find the right balance between the number of workers and cores per worker\n",
    "3. **Consider Hardware Limits**: Be mindful of your system's physical cores and memory\n",
    "4. **Use Clean Compute Folders**: Use the `dest_folder` parameter to keep your project organized\n",
    "5. **Handle Overwrite Carefully**: Use `overwrite_dest=True` for repeatable workflows, but be cautious about losing results\n",
    "6. **Monitor Performance**: Track execution times and adjust your configuration for optimal performance\n",
    "7. **Match Workers to Plans**: For best results, use one worker per plan when running a small number of plans\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\09_plan_parameter_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Plan Key Operations\n",
    "\n",
    "This notebook demonstrates how to perform key operations on HEC-RAS plan files using the RAS Commander library. Plan files in HEC-RAS (`.p*` files) control the simulation settings and parameters, making them essential for hydraulic modeling workflows.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Set up a HEC-RAS project for automation\n",
    "2. **Plan Values**: Retrieve specific values from plan files\n",
    "3. **Run Flags**: Configure which components (geometry preprocessor, unsteady flow, etc.) will run\n",
    "4. **Plan Intervals**: Set computation and output time intervals\n",
    "5. **Plan Descriptions**: Read and update plan descriptions\n",
    "6. **Simulation Dates**: Modify simulation start and end dates\n",
    "\n",
    "These operations allow you to programmatically control and customize HEC-RAS simulations without opening the GUI, which is especially useful for batch processing, sensitivity analysis, and model calibration.\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Plan Files in HEC-RAS\n",
    "\n",
    "Before we dive into the operations, let's understand what HEC-RAS plan files are and why they're important:\n",
    "\n",
    "### What is a Plan File?\n",
    "\n",
    "A HEC-RAS plan file (`.p*`) is a configuration file that defines how a hydraulic simulation will run. It links together:\n",
    "\n",
    "1. **Geometry**: River channel and floodplain physical characteristics (`.g*` files)\n",
    "2. **Flow Data**: Inflow conditions, either steady (`.f*`) or unsteady (`.u*`)\n",
    "3. **Simulation Parameters**: Time steps, computational methods, and output settings\n",
    "\n",
    "### Key Components of Plan Files\n",
    "\n",
    "Plan files contain many parameters that control simulation behavior:\n",
    "\n",
    "- **Simulation Type**: Steady, unsteady, sediment transport, water quality\n",
    "- **Computation Intervals**: Time steps for calculations\n",
    "- **Output Intervals**: How frequently results are saved\n",
    "- **Run Flags**: Which modules to execute (preprocessor, postprocessor, etc.)\n",
    "- **Simulation Period**: Start and end dates for unsteady simulations\n",
    "- **Computation Methods**: Numerical schemes and solver settings\n",
    "- **Resource Allocation**: Number of CPU cores to use\n",
    "\n",
    "### Why Automate Plan Operations?\n",
    "\n",
    "Automating plan operations with RAS Commander allows you to:\n",
    "\n",
    "1. **Batch Processing**: Run multiple scenarios with different parameters\n",
    "2. **Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
    "3. **Calibration**: Adjust parameters to match observed data\n",
    "4. **Consistency**: Ensure standardized settings across multiple models\n",
    "5. **Documentation**: Programmatically track simulation configurations\n",
    "\n",
    "Now, let's download and extract an example project to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RasExamples instance\n",
    "ras_examples = RasExamples()\n",
    "\n",
    "# Extract the Bald Eagle Creek example project\n",
    "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\"])\n",
    "print(f\"Extracted project to: {extracted_paths}\")\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "The first step in any RAS Commander workflow is initializing the HEC-RAS project. This connects the Python environment to the HEC-RAS project files.\n",
    "\n",
    "The `init_ras_project()` function does the following:\n",
    "\n",
    "1. Locates the main project file (`.prj`)\n",
    "2. Reads all associated files (plans, geometries, flows)\n",
    "3. Creates dataframes containing project components\n",
    "4. Sets up the connection to the HEC-RAS executable\n",
    "\n",
    "Let's initialize our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the project (using the default global ras object)\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized project: {ras.project_name}\")\n",
    "\n",
    "# Display basic project information\n",
    "print(\"\\nProject Overview:\")\n",
    "print(f\"Project Folder: {ras.project_folder}\")\n",
    "print(f\"Project File: {ras.prj_file}\")\n",
    "print(f\"Number of Plan Files: {len(ras.plan_df)}\")\n",
    "print(f\"Number of Geometry Files: {len(ras.geom_df)}\")\n",
    "print(f\"Number of Flow Files: {len(ras.flow_df)}\")\n",
    "print(f\"Number of Unsteady Files: {len(ras.unsteady_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the plan files in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the plan files\n",
    "print(\"Plan Files in Project:\")\n",
    "display.display(ras.plan_df[['plan_number', 'Plan Title', 'Short Identifier', 'Geom File']])\n",
    "\n",
    "# Get the first plan number for our examples\n",
    "plan_number = ras.plan_df['plan_number'].iloc[0]\n",
    "print(f\"\\nWe'll work with Plan: {plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Retrieving Plan Values\n",
    "\n",
    "The `RasPlan.get_plan_value()` method allows you to retrieve specific values from a plan file. This is useful for checking current settings before making changes or for extracting information for analysis.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `key` (str): The specific parameter to retrieve (e.g., \"Computation Interval\")\n",
    "- `rasect` (RasPrj, optional): The RAS project object (defaults to global `ras`)\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- `Computation Interval`: Time step for calculations (e.g., \"5SEC\", \"1MIN\")\n",
    "- `Short Identifier`: Brief name/ID for the plan\n",
    "- `Simulation Date`: Start and end dates for simulation\n",
    "- `UNET D1 Cores`: Number of processor cores to use\n",
    "- `Plan Title`: Full title of the plan\n",
    "- `Geom File`: Associated geometry file\n",
    "- `Flow File`: Associated flow file (for steady flow)\n",
    "- `Unsteady File`: Associated unsteady flow file\n",
    "- `Friction Slope Method`: Method for calculating friction slopes\n",
    "- `Run HTab`: Whether to run the geometry preprocessor\n",
    "- `UNET Use Existing IB Tables`: Whether to use existing internal boundary tables\n",
    "\n",
    "Let's retrieve some key values from our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keys to check\n",
    "keys_to_check = [\n",
    "    'Short Identifier', \n",
    "    'Plan Title',\n",
    "    'Computation Interval', \n",
    "    'Simulation Date', \n",
    "    'UNET D1 Cores',\n",
    "    'Geom File',\n",
    "    'Friction Slope Method'\n",
    "]\n",
    "\n",
    "# Retrieve and display the initial values\n",
    "print(\"Initial Plan Values:\")\n",
    "initial_values = {}\n",
    "for key in keys_to_check:\n",
    "    value = RasPlan.get_plan_value(plan_number, key, rasect=ras)\n",
    "    initial_values[key] = value\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV NOTE: NEED TO REVIEW # OF CORES LOGIC TO ENSURE IT IS UPDATED AND APPLIES TO 1D/2D and PIPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Updating Run Flags\n",
    "\n",
    "Run flags in HEC-RAS control which components of the simulation are executed. The `RasPlan.update_run_flags()` method allows you to modify these flags programmatically.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `geometry_preprocessor` (bool, optional): Whether to run the geometry preprocessor\n",
    "- `unsteady_flow_simulation` (bool, optional): Whether to run the unsteady flow simulation\n",
    "- `run_sediment` (bool, optional): Whether to run sediment transport calculations\n",
    "- `post_processor` (bool, optional): Whether to run the post-processor\n",
    "- `floodplain_mapping` (bool, optional): Whether to run floodplain mapping\n",
    "- `rasect` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Common Run Flags\n",
    "\n",
    "1. **Geometry Preprocessor**: Computes hydraulic tables from geometry data\n",
    "   - `True`: Recompute tables (useful after geometry changes)\n",
    "   - `False`: Use existing tables (faster but may be outdated)\n",
    "\n",
    "2. **Unsteady Flow Simulation**: The main hydraulic calculations\n",
    "   - `True`: Run unsteady flow calculations\n",
    "   - `False`: Skip unsteady flow calculations\n",
    "\n",
    "3. **Sediment Transport**: Simulates erosion and deposition\n",
    "   - `True`: Calculate sediment transport\n",
    "   - `False`: Skip sediment transport\n",
    "\n",
    "4. **Post-Processor**: Calculates additional variables from results\n",
    "   - `True`: Run post-processing (recommended)\n",
    "   - `False`: Skip post-processing (faster but fewer outputs)\n",
    "\n",
    "5. **Floodplain Mapping**: Generates inundation maps\n",
    "   - `True`: Generate maps (requires terrain data)\n",
    "   - `False`: Skip mapping (faster)\n",
    "\n",
    "Let's update the run flags for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update run flags for the plan\n",
    "print(f\"Updating run flags for plan {plan_number}...\")\n",
    "RasPlan.update_run_flags(\n",
    "    plan_number,\n",
    "    geometry_preprocessor=True,      # Force recalculation of hydraulic tables\n",
    "    unsteady_flow_simulation=True,   # Run the main hydraulic calculations\n",
    "    run_sediment=False,              # Skip sediment transport calculations\n",
    "    post_processor=True,             # Run post-processing for additional outputs\n",
    "    floodplain_mapping=False,        # Skip floodplain mapping\n",
    "    rasect=ras\n",
    ")\n",
    "print(\"Run flags updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Updating Plan Intervals\n",
    "\n",
    "Time intervals in HEC-RAS control the temporal resolution of simulations and outputs. The `RasPlan.update_plan_intervals()` method allows you to modify these intervals.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `computation_interval` (str, optional): Time step for calculations\n",
    "- `output_interval` (str, optional): Time step for saving detailed results\n",
    "- `instantaneous_interval` (str, optional): Time step for peak value calculations\n",
    "- `mapping_interval` (str, optional): Time step for map outputs\n",
    "- `rasect` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Valid Interval Values\n",
    "\n",
    "Time intervals must be specified in HEC-RAS format:\n",
    "- Seconds: `1SEC`, `2SEC`, `3SEC`, `4SEC`, `5SEC`, `6SEC`, `10SEC`, `15SEC`, `20SEC`, `30SEC`\n",
    "- Minutes: `1MIN`, `2MIN`, `3MIN`, `4MIN`, `5MIN`, `6MIN`, `10MIN`, `15MIN`, `20MIN`, `30MIN`\n",
    "- Hours: `1HOUR`, `2HOUR`, `3HOUR`, `4HOUR`, `6HOUR`, `8HOUR`, `12HOUR`\n",
    "- Days: `1DAY`\n",
    "\n",
    "### Interval Types\n",
    "\n",
    "1. **Computation Interval**: Time step used for hydraulic calculations\n",
    "   - Smaller intervals: More accurate but slower\n",
    "   - Larger intervals: Faster but may introduce numerical errors\n",
    "   - Rule of thumb: Should be small enough to capture flow changes\n",
    "\n",
    "2. **Output Interval**: How frequently detailed results are saved\n",
    "   - Smaller intervals: More detailed results but larger files\n",
    "   - Larger intervals: Smaller files but less temporal resolution\n",
    "   - Usually larger than computation interval\n",
    "\n",
    "3. **Instantaneous Interval**: Time step for peak value calculations\n",
    "   - Affects when max/min values are checked\n",
    "   - Usually equal to output interval\n",
    "\n",
    "4. **Mapping Interval**: How frequently map data is saved\n",
    "   - Affects animation smoothness and file size\n",
    "   - Usually larger than output interval\n",
    "\n",
    "Let's update the intervals for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update plan intervals\n",
    "print(f\"Updating intervals for plan {plan_number}...\")\n",
    "RasPlan.update_plan_intervals(\n",
    "    plan_number,\n",
    "    computation_interval=\"5SEC\",    # 5-second time step for calculations\n",
    "    output_interval=\"1MIN\",         # Save detailed results every minute\n",
    "    instantaneous_interval=\"5MIN\",  # Check for max/min values every 5 minutes\n",
    "    mapping_interval=\"15MIN\",       # Save map data every 15 minutes\n",
    "    rasect=ras\n",
    ")\n",
    "print(\"Plan intervals updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Managing Plan Descriptions\n",
    "\n",
    "Plan descriptions provide documentation for simulation configurations. The RAS Commander library offers methods to read and update these descriptions.\n",
    "\n",
    "### Reading Descriptions\n",
    "\n",
    "The `RasPlan.read_plan_description()` method retrieves the current description from a plan file.\n",
    "\n",
    "#### Parameters\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `rasect` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Updating Descriptions\n",
    "\n",
    "The `RasPlan.update_plan_description()` method sets a new description for a plan file.\n",
    "\n",
    "#### Parameters\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `description` (str): The new description text\n",
    "- `rasect` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Best Practices for Plan Descriptions\n",
    "\n",
    "Effective plan descriptions should include:\n",
    "1. Purpose of the simulation\n",
    "2. Key parameters and settings\n",
    "3. Date of creation or modification\n",
    "4. Author or organization\n",
    "5. Any special considerations or notes\n",
    "\n",
    "Let's read the current description and then update it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the current plan description\n",
    "current_description = RasPlan.read_plan_description(plan_number, rasect=ras)\n",
    "print(f\"Current plan description:\\n{current_description}\")\n",
    "\n",
    "# Create a new description with detailed information\n",
    "new_description = f\"\"\"Modified Plan for RAS Commander Testing\n",
    "Date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "Purpose: Demonstrating RAS Commander plan operations\n",
    "Settings:\n",
    "- Computation Interval: 5SEC\n",
    "- Output Interval: 1MIN\n",
    "- Mapping Interval: 15MIN\n",
    "- Geometry Preprocessor: Enabled\n",
    "- Post-Processor: Enabled\n",
    "Notes: This plan was automatically modified using ras-commander.\"\"\"\n",
    "\n",
    "# Update the plan description\n",
    "print(\"\\nUpdating plan description...\")\n",
    "RasPlan.update_plan_description(plan_number, new_description, rasect=ras)\n",
    "print(\"Plan description updated successfully\")\n",
    "\n",
    "# Verify the updated description\n",
    "updated_description = RasPlan.read_plan_description(plan_number, rasect=ras)\n",
    "print(f\"\\nUpdated plan description:\\n{updated_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Updating Simulation Dates\n",
    "\n",
    "For unsteady flow simulations, the simulation period defines the time window for the analysis. The `RasPlan.update_simulation_date()` method allows you to modify this period.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `start_date` (datetime): The start date and time for the simulation\n",
    "- `end_date` (datetime): The end date and time for the simulation\n",
    "- `rasect` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Considerations for Simulation Dates\n",
    "\n",
    "1. **Hydrograph Coverage**: The simulation period should fully encompass your hydrographs\n",
    "2. **Warm-Up Period**: Include time before the main event for model stabilization\n",
    "3. **Cool-Down Period**: Include time after the main event for complete drainage\n",
    "4. **Computational Efficiency**: Avoid unnecessarily long periods to reduce runtime\n",
    "5. **Consistency**: Ensure dates match available boundary condition data\n",
    "\n",
    "Let's update the simulation dates for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current simulation date\n",
    "current_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\", rasect=ras)\n",
    "print(f\"Current simulation date: {current_sim_date}\")\n",
    "\n",
    "# Parse the current simulation date string\n",
    "current_dates = current_sim_date.split(\",\")\n",
    "current_start = datetime.strptime(f\"{current_dates[0]},{current_dates[1]}\", \"%d%b%Y,%H%M\")\n",
    "current_end = datetime.strptime(f\"{current_dates[2]},{current_dates[3]}\", \"%d%b%Y,%H%M\")\n",
    "\n",
    "# Define new simulation period - adjust by 1 hour from current dates\n",
    "start_date = current_start + timedelta(hours=1)  # Current start + 1 hour\n",
    "end_date = current_end - timedelta(hours=1)      # Current end - 1 hour\n",
    "\n",
    "# Update the simulation date\n",
    "print(f\"\\nUpdating simulation period to: {start_date.strftime('%d%b%Y,%H%M')} - {end_date.strftime('%d%b%Y,%H%M')}\")\n",
    "RasPlan.update_simulation_date(plan_number, start_date, end_date, rasect=ras)\n",
    "print(\"Simulation dates updated successfully\")\n",
    "\n",
    "# Verify the updated simulation date\n",
    "updated_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\", rasect=ras)\n",
    "print(f\"\\nUpdated simulation date: {updated_sim_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verifying Updated Plan Values\n",
    "\n",
    "After making multiple changes to a plan, it's a good practice to verify that all updates were applied correctly. Let's check the updated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and display the updated values\n",
    "print(\"Updated Plan Values:\")\n",
    "updated_values = {}\n",
    "for key in keys_to_check:\n",
    "    value = RasPlan.get_plan_value(plan_number, key, rasect=ras)\n",
    "    updated_values[key] = value\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Parameter': keys_to_check,\n",
    "    'Initial Value': [initial_values.get(k, 'N/A') for k in keys_to_check],\n",
    "    'Updated Value': [updated_values.get(k, 'N/A') for k in keys_to_check]\n",
    "})\n",
    "\n",
    "print(\"\\nChanges Summary:\")\n",
    "display.display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Computing the Plan (Optional)\n",
    "\n",
    "After making changes to a plan, you might want to run the simulation to see the effects. The `RasCmdr.compute_plan()` method executes a HEC-RAS simulation with the specified plan.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (str): The plan number to execute\n",
    "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
    "- `rasect` (RasPrj, optional): The RAS project object\n",
    "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files\n",
    "- `num_cores` (int, optional): Number of processor cores to use\n",
    "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder\n",
    "\n",
    "If you want to run the simulation, you can uncomment the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV NOTE THIS SHOULD RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the simulation with the updated plan\n",
    "\n",
    "# # Define a destination folder for the computation\n",
    "# dest_folder = script_dir / \"compute_results\"\n",
    "# print(f\"Computing plan {plan_number}...\")\n",
    "# print(f\"Results will be saved to: {dest_folder}\")\n",
    "\n",
    "# # Execute the plan\n",
    "# success = RasCmdr.compute_plan(\n",
    "#     plan_number,\n",
    "#     dest_folder=dest_folder,\n",
    "#     clear_geompre=True,    # Clear preprocessor files to ensure clean results\n",
    "#     num_cores=2,           # Use 2 processor cores\n",
    "#     overwrite_dest=True,   # Overwrite existing destination folder\n",
    "#     rasect=ras\n",
    "# )\n",
    "\n",
    "# if success:\n",
    "#     print(f\"Plan {plan_number} computed successfully\")\n",
    "#     # Check for results file\n",
    "#     results_path = RasPlan.get_results_path(plan_number, rasect=ras)\n",
    "#     if results_path:\n",
    "#         print(f\"Results saved to: {results_path}\")\n",
    "# else:\n",
    "#     print(f\"Failed to compute plan {plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Plan Key Operations\n",
    "\n",
    "In this notebook, we've covered the essential operations for manipulating HEC-RAS plan files programmatically using RAS Commander:\n",
    "\n",
    "1. **Project Initialization**: We initialized a HEC-RAS project using `init_ras_project()`\n",
    "2. **Plan Values**: We retrieved plan values with `RasPlan.get_plan_value()`\n",
    "3. **Run Flags**: We updated simulation components with `RasPlan.update_run_flags()`\n",
    "4. **Plan Intervals**: We modified time steps with `RasPlan.update_plan_intervals()`\n",
    "5. **Plan Descriptions**: We managed documentation with `RasPlan.read_plan_description()` and `RasPlan.update_plan_description()`\n",
    "6. **Simulation Dates**: We changed the analysis period with `RasPlan.update_simulation_date()`\n",
    "7. **Verification**: We verified our changes by comparing initial and updated values\n",
    "\n",
    "### Key Classes and Functions Used\n",
    "\n",
    "- `RasPlan`: The main class for plan operations\n",
    "  - `get_plan_value()`: Retrieve specific values from plan files\n",
    "  - `update_run_flags()`: Configure which components will run\n",
    "  - `update_plan_intervals()`: Set computation and output time intervals\n",
    "  - `read_plan_description()`: Get the current plan description\n",
    "  - `update_plan_description()`: Set a new plan description\n",
    "  - `update_simulation_date()`: Modify the simulation period\n",
    "  - `get_results_path()`: Get the path to results files\n",
    "\n",
    "- `RasCmdr`: The class for executing HEC-RAS simulations\n",
    "  - `compute_plan()`: Run a single plan simulation\n",
    "\n",
    "### Best Practices for Plan Operations\n",
    "\n",
    "1. **Verify Before Updating**: Always check current values before making changes\n",
    "2. **Document Changes**: Use descriptive plan descriptions to track modifications\n",
    "3. **Maintain Consistency**: Ensure flow data matches simulation dates\n",
    "4. **Use Appropriate Intervals**: Balance accuracy and computational efficiency\n",
    "5. **Backup Original Files**: Use destination folders when running simulations\n",
    "6. **Verify After Updates**: Confirm that all changes were applied correctly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With these plan operations, you can now:\n",
    "\n",
    "1. **Create Batch Workflows**: Process multiple scenarios with different parameters\n",
    "2. **Perform Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
    "3. **Automate Calibration**: Adjust parameters to match observed data\n",
    "4. **Build Model Ensembles**: Run multiple configurations for uncertainty analysis\n",
    "5. **Integrate with Other Tools**: Connect HEC-RAS to broader modeling frameworks\n",
    "\n",
    "These operations form the foundation for advanced HEC-RAS automation using the RAS Commander library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\101_Core_Sensitivity.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14_Core_Sensitivity.ipynb\n",
    "Testing Core Sensitivity for RAS using the Bald Eagle Creek Multi-Gage 2D project.  \n",
    "\n",
    "\n",
    "This should take around 15-45 minutes to run depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ras_commander import RasExamples, init_ras_project, RasCmdr, RasPlan, RasGeo\n",
    "\n",
    "# Step 1: Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "\n",
    "RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "# Use Path.cwd() to get the current working directory in a Jupyter Notebook\n",
    "current_directory = Path.cwd()\n",
    "project_path = current_directory / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "\n",
    "# Step 2: Initialize the RAS Project Folder using init_ras_project (from ras_commander)\n",
    "init_ras_project(project_path, \"6.6\")\n",
    "\n",
    "# Step 3: Initialize a DataFrame to store execution results\n",
    "results = []\n",
    "\n",
    "# Step 4: Run sensitivity analysis for Plan 03 with core counts 1-8\n",
    "plan_number = '03'\n",
    "print(f\"Running sensitivity analysis for Plan {plan_number}\")\n",
    "\n",
    "# Clear geompre files before running the plan\n",
    "plan_path = RasPlan.get_plan_path(plan_number)\n",
    "RasGeo.clear_geompre_files(plan_path)\n",
    "\n",
    "for cores in range(1, 5):\n",
    "    print(f\"Running with {cores} core(s)\")\n",
    "    # Set core count for this plan\n",
    "    RasPlan.set_num_cores(plan_number, cores)\n",
    "    \n",
    "    # Time the execution of the plan\n",
    "    start_time = time.time()\n",
    "    RasCmdr.compute_plan(plan_number)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"plan_number\": plan_number,\n",
    "        \"cores\": cores,\n",
    "        \"execution_time\": execution_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "print(\"Sensitivity analysis complete\")\n",
    "\n",
    "# Step 5: Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv(\"core_sensitivity_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES FOR REVISIONS:\n",
    "- Use HDF compute summary to show the time for each preproces/unsteady compute/postprocess step. \n",
    "- First, run preprocessor and then toggle options to only run unsteady compute and postprocess. \n",
    "- Plot each step separately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the results from a CSV file\n",
    "results_df = pd.read_csv(\"core_sensitivity_results.csv\")\n",
    "\n",
    "# Display the results dataframe for verification\n",
    "print(\"results_df DataFrame (time is in seconds):\")\n",
    "display(results_df)\n",
    "\n",
    "# Step 6: Calculate unit runtime (based on 1 core execution time)\n",
    "results_df['unit_runtime'] = results_df.groupby('plan_number')['execution_time'].transform(lambda x: x / x.iloc[0])\n",
    "\n",
    "# Get the project name from the ras object\n",
    "project_name = ras.project_name\n",
    "\n",
    "# Step 7: Plot a line chart for unit runtime vs. cores for each plan\n",
    "plt.figure(figsize=(10, 6))\n",
    "for plan in results_df['plan_number'].unique():\n",
    "    plan_data = results_df[results_df['plan_number'] == plan]\n",
    "    plt.plot(plan_data['cores'], plan_data['unit_runtime'], label=f\"Plan {plan}\")\n",
    "\n",
    "plt.xlabel(\"Number of Cores\")\n",
    "plt.ylabel(\"Unit Runtime (Relative to 1 Core)\")\n",
    "plt.title(f\"{project_name} (HEC Example Project)\\nCore Count Sensitivity Analysis\")\n",
    "plt.legend(title=\"Plan Number\")\n",
    "plt.grid(False)\n",
    "plt.vlines([1,2,3,4], ymin=0, ymax=1.2, linestyles='dotted', alpha=0.3)\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\102_benchmarking_versions_6.1_to_6.6.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS-Commander standard code cells 1-3: Install Packages and Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define versions to compare\n",
    "versions = ['6.6', '6.5', '6.4.1', '6.3.1', '6.3', '6.2', \"6.1\", \"6.0\"] # NOTE: ras-commander does not support versions prior to 6.2 due to HDF5 file format changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BaldEagleCrkMulti2D project\n",
    "project_path = RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the ras_project with ras-commander to read all HEC-RAS project information \n",
    "init_ras_project(project_path, \"6.5\")\n",
    "print(ras)\n",
    "# If no ras object is defined in init_ras_project, it defaults to \"ras\" (useful for single project scripts)\n",
    "# Display plan dataframe\n",
    "ras.plan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Plan Numbers to List and Print\n",
    "plan_numbers = ras.plan_df['plan_number'].tolist()\n",
    "print(plan_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define run_simulation function for\n",
    "import time\n",
    "from ras_commander import RasGeo\n",
    "\n",
    "def run_simulation(version, plan_number):\n",
    "    # Initialize project for the specific version\n",
    "    ras_project = init_ras_project(project_path, str(version))\n",
    "    \n",
    "    # Clear geometry preprocessor files for the plan\n",
    "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_project)\n",
    "    RasGeo.clear_geompre_files(plan_path, ras_object=ras_project)\n",
    "    \n",
    "    # Set the number of cores to 4\n",
    "    RasPlan.set_num_cores(plan_number, \"4\", ras_object=ras_project)\n",
    "    \n",
    "    # Update plan run flags – setting \"Run HTab\" flag to 1 to force geometry preprocessing\n",
    "    RasPlan.update_run_flags(plan_number, {\"Run HTab\": 1}, ras_object=ras_project)\n",
    "    \n",
    "    # Compute the plan\n",
    "    start_time = time.time()\n",
    "    success = RasCmdr.compute_plan(plan_number, ras_object=ras_project)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if success:\n",
    "        # Get the HDF file path for the plan results\n",
    "        hdf_path = RasPlan.get_results_path(plan_number, ras_object=ras_project)\n",
    "        \n",
    "        # Extract runtime data from the HDF file\n",
    "        runtime_data = HdfResultsPlan.get_runtime_data(hdf_path)\n",
    "        \n",
    "        # Extract required information from the runtime data\n",
    "        preprocessor_time = runtime_data['Preprocessing Geometry (hr)'].values[0]\n",
    "        unsteady_compute_time = runtime_data['Unsteady Flow Computations (hr)'].values[0]\n",
    "        \n",
    "        # Get volume accounting data from the HDF file\n",
    "        volume_accounting = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
    "        # Extract Error Percent from the DataFrame\n",
    "        volume_error = volume_accounting['Error Percent'].values[0] if not volume_accounting.empty else None\n",
    "        \n",
    "        # Print the extracted data\n",
    "        print(f\"\\nExtracted Data for Plan {plan_number} in Version {version}:\")\n",
    "        print(f\"Preprocessor Time: {preprocessor_time:.3f} hr\")\n",
    "        print(f\"Unsteady Compute Time: {unsteady_compute_time:.3f} hr\") \n",
    "        print(f\"Volume Error: {volume_error:.3f}%\" if volume_error is not None else \"Volume Error: None\")\n",
    "        print(f\"Total Time: {total_time/3600:.3f} hr\\n\")\n",
    "        \n",
    "        return {\n",
    "            'Version': version,\n",
    "            'Plan': plan_number,\n",
    "            'Preprocessor Time (hr)': preprocessor_time,\n",
    "            'Unsteady Compute Time (hr)': unsteady_compute_time,\n",
    "            'Volume Error (%)': volume_error,\n",
    "            'Total Time (hr)': total_time / 3600  # convert seconds to hours\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Benchmark all plans in Version 6.6\n",
    "Change the following cell from Markdown to Code and delete \"```\" to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#### Initialize results list\n",
    "results = []\n",
    "#### Loop through each plan number\n",
    "for plan in plan_numbers:\n",
    "    print(f\"Running simulation for Version 6.6, Plan {plan}\")\n",
    "    result = run_simulation(\"6.6\", plan)\n",
    "    if result is not None:  #### Check if result is not None\n",
    "        results.append(result)\n",
    "\n",
    "#### Convert results list to DataFrame and save as CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('hecras_plan_comparison.csv', index=False)\n",
    "\n",
    "print(\"Results saved to 'hecras_plan_comparison.csv'\")\n",
    "\n",
    "#### Load and display the results dataframe\n",
    "df = pd.read_csv('hecras_plan_comparison.csv')\n",
    "\n",
    "#### Get plan titles from ras.plan_df and merge with results\n",
    "plan_titles = pd.DataFrame({\n",
    "    'Plan': ras.plan_df['plan_number'].str.zfill(2),  #### Ensure 2-digit format\n",
    "    'Short Identifier': ras.plan_df['Short Identifier']\n",
    "})\n",
    "#### Convert df's Plan column to 2-digit string format\n",
    "df['Plan'] = df['Plan'].astype(str).str.zfill(2)\n",
    "\n",
    "df = df.merge(plan_titles, on='Plan', how='left')\n",
    "\n",
    "print(\"Benchmarking Results:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "#### Create a more comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "#### Function to create rotated labels\n",
    "def plot_with_rotated_labels(ax, data, values, color, title, ylabel):\n",
    "    bars = ax.bar(range(len(data)), values, color=color, alpha=0.7)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    #### Set x-ticks at bar positions\n",
    "    ax.set_xticks(range(len(data)))\n",
    "    \n",
    "    #### Create labels with plan numbers and titles\n",
    "    labels = [f\"Plan {plan}\\n{title}\" for plan, title in zip(data['Plan'], data['Short Identifier'])]\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "#### Plot 1: Unsteady Runtime\n",
    "plot_with_rotated_labels(ax1, df, df['Unsteady Compute Time (hr)'], 'blue', \n",
    "                        'Unsteady Runtime by Plan', 'Unsteady Runtime (hours)')\n",
    "\n",
    "#### Plot 2: Volume Error  \n",
    "plot_with_rotated_labels(ax2, df, df['Volume Error (%)'], 'red',\n",
    "                        'Volume Error by Plan', 'Volume Error (%)')\n",
    "\n",
    "#### Plot 3: Preprocessor Time\n",
    "plot_with_rotated_labels(ax3, df, df['Preprocessor Time (hr)'], 'green',\n",
    "                        'Preprocessor Time by Plan', 'Preprocessor Time (hours)')\n",
    "\n",
    "#### Plot 4: Total Runtime\n",
    "plot_with_rotated_labels(ax4, df, df['Total Time (hr)'], 'purple',\n",
    "                        'Total Runtime by Plan', 'Total Runtime (hours)')\n",
    "\n",
    "#### Adjust layout and display\n",
    "plt.tight_layout(pad=3.0)\n",
    "fig.suptitle('Plan Performance Comparison', fontsize=14, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "#### Calculate and display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "#### Calculate plan-to-plan performance changes\n",
    "print(\"\\nPlan-to-Plan Performance Changes:\")\n",
    "df['Unsteady Runtime Change (%)'] = df['Unsteady Compute Time (hr)'].pct_change() * 100\n",
    "print(df[['Plan', 'Short Identifier', 'Unsteady Runtime Change (%)']].to_string(index=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the plan number you want to run across all versions\n",
    "plan_number = '02'  # Make sure this is a string and include the leading zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations for all versions with plan_number defined by user\n",
    "results = []\n",
    "for version in versions:\n",
    "    print(f\"Running simulation for Version {version}, Plan {plan_number}\")\n",
    "    result = run_simulation(version, plan_number) \n",
    "    if result is not None:  # Check if result is not None\n",
    "        results.append(result)\n",
    "        print(f\"Completed: Version {version}, Plan {plan_number}\")\n",
    "    else:\n",
    "        print(f\"Failed: Version {version}, Plan {plan_number}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save initial results to CSV\n",
    "df.to_csv('save_initial_results.csv', index=False)\n",
    "\n",
    "print(\"Initial results saved to 'save_initial_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Unsteady Runtime vs Version\n",
    "plt.subplot(1, 2, 1)\n",
    "# Convert Version to categorical type to handle string versions properly\n",
    "plt.plot(pd.Categorical(df['Version']), df['Unsteady Compute Time (hr)'], marker='o')\n",
    "plt.title(f'Unsteady Runtime vs HEC-RAS Version (Plan {plan_number})')\n",
    "plt.xlabel('HEC-RAS Version')\n",
    "plt.ylabel('Unsteady Runtime (hours)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Volume Error vs Version\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(pd.Categorical(df['Version']), df['Volume Error (%)'], marker='o')\n",
    "plt.title(f'Volume Error vs HEC-RAS Version (Plan {plan_number})')\n",
    "plt.xlabel('HEC-RAS Version')\n",
    "plt.ylabel('Volume Error (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\103_Generating AEP Events from Atlas 14.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated HEC-RAS Analysis for Multiple AEP Events\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for performing flood analysis with different Annual Exceedance Probability (AEP) events. We'll automate the following steps:\n",
    "\n",
    "1. Generate hyetographs from NOAA Atlas 14 data for different AEP events\n",
    "2. Download the Davis HEC-RAS project\n",
    "3. Clone and configure HEC-RAS plans and unsteady flow files for each AEP event\n",
    "4. Execute all plans in parallel\n",
    "5. Extract and visualize results\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "We'll use the following libraries:\n",
    "- `ras-commander`: For HEC-RAS automation\n",
    "- `pandas`, `numpy`: For data manipulation\n",
    "- `matplotlib`: For visualization\n",
    "- Standard libraries: `os`, `re`, `pathlib`, etc.\n",
    "\n",
    "Let's start by installing ras-commander (if needed) and importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander if needed (uncomment to run)\n",
    "# !pip install ras-commander\n",
    "\n",
    "# Import necessary libraries\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from math import log, exp\n",
    "from pathlib import Path\n",
    "import time\n",
    "import psutil  # For getting system CPU info\n",
    "from IPython import display\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Hyetographs from NOAA Atlas 14 Data\n",
    "\n",
    "First, let's define functions to generate hyetographs from NOAA Atlas 14 precipitation frequency data. These functions will:\n",
    "1. Parse duration strings from the CSV file\n",
    "2. Read and process precipitation frequency data\n",
    "3. Interpolate depths for each ARI (Annual Recurrence Interval)\n",
    "4. Compute incremental depths\n",
    "5. Apply the Alternating Block Method to generate hyetographs\n",
    "6. Save the hyetographs to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse duration strings and convert them to hours\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a duration string and converts it to hours.\n",
    "    Examples:\n",
    "        \"5-min:\" -> 0.0833 hours\n",
    "        \"2-hr:\" -> 2 hours\n",
    "        \"2-day:\" -> 48 hours\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "    value, unit = match.groups()\n",
    "    value = int(value)\n",
    "    unit = unit.lower()\n",
    "    if unit in ['min', 'minute', 'minutes']:\n",
    "        hours = value / 60.0\n",
    "    elif unit in ['hr', 'hour', 'hours']:\n",
    "        hours = value\n",
    "    elif unit in ['day', 'days']:\n",
    "        hours = value * 24\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
    "    return hours\n",
    "\n",
    "# Function to read and process the precipitation frequency CSV\n",
    "def read_precipitation_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the precipitation frequency CSV and returns a DataFrame\n",
    "    with durations in hours as the index and ARIs as columns.\n",
    "    This function dynamically locates the header line for the data table.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_line_idx = None\n",
    "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
    "\n",
    "    # Locate the header line\n",
    "    for idx, line in enumerate(lines):\n",
    "        if header_pattern.match(line.strip().lower()):\n",
    "            header_line_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_line_idx is None:\n",
    "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
    "\n",
    "    # Extract the ARI headers from the header line\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    headers = [item.strip() for item in header_line.split(',')]\n",
    "    \n",
    "    if len(headers) < 2:\n",
    "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
    "\n",
    "    aris = headers[1:]  # Exclude the first column which is the duration\n",
    "\n",
    "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
    "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
    "\n",
    "    # Initialize lists to store durations and corresponding depths\n",
    "    durations = []\n",
    "    depths = {ari: [] for ari in aris}\n",
    "\n",
    "    # Iterate over the lines following the header to extract data\n",
    "    for line in lines[header_line_idx + 1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if not duration_pattern.match(line):\n",
    "            break  # Stop if the line does not match the duration pattern\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if len(parts) != len(headers):\n",
    "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
    "        duration_str = parts[0]\n",
    "        try:\n",
    "            duration_hours = parse_duration(duration_str)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Skipping line due to error: {ve}\")\n",
    "            continue  # Skip lines with invalid duration formats\n",
    "        durations.append(duration_hours)\n",
    "        for ari, depth_str in zip(aris, parts[1:]):\n",
    "            try:\n",
    "                depth = float(depth_str)\n",
    "            except ValueError:\n",
    "                depth = np.nan  # Assign NaN for invalid depth values\n",
    "            depths[ari].append(depth)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(depths, index=durations)\n",
    "    df.index.name = 'Duration_hours'\n",
    "\n",
    "    # Drop any rows with NaN values (optional, based on data quality)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to perform log-log linear interpolation for each ARI\n",
    "def interpolate_depths(df, total_duration):\n",
    "    \"\"\"\n",
    "    Interpolates precipitation depths for each ARI on a log-log scale\n",
    "    for each hour up to the total storm duration.\n",
    "    \"\"\"\n",
    "    T = total_duration\n",
    "    t_hours = np.arange(1, T+1)\n",
    "    D = {}\n",
    "    for ari in df.columns:\n",
    "        durations = df.index.values\n",
    "        depths = df[ari].values\n",
    "        # Ensure all depths are positive\n",
    "        if np.any(depths <= 0):\n",
    "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
    "        # Log-log interpolation\n",
    "        log_durations = np.log(durations)\n",
    "        log_depths = np.log(depths)\n",
    "        log_t = np.log(t_hours)\n",
    "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
    "        D_t = np.exp(log_D_t)\n",
    "        D[ari] = D_t\n",
    "    return D\n",
    "\n",
    "# Function to compute incremental precipitation depths\n",
    "def compute_incremental_depths(D, total_duration):\n",
    "    \"\"\"\n",
    "    Computes incremental precipitation depths for each hour.\n",
    "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
    "    \"\"\"\n",
    "    incremental_depths = {}\n",
    "    for ari, D_t in D.items():\n",
    "        I_t = np.empty(total_duration)\n",
    "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
    "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
    "        incremental_depths[ari] = I_t\n",
    "    return incremental_depths\n",
    "\n",
    "# Function to assign incremental depths using the Alternating Block Method\n",
    "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
    "    \"\"\"\n",
    "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    hyetograph = [0.0] * T\n",
    "    hyetograph[central_index] = max_depth\n",
    "    remaining_depths = sorted_depths.copy()\n",
    "    remaining_depths.remove(max_depth)\n",
    "    left = central_index - 1\n",
    "    right = central_index + 1\n",
    "    toggle = True  # Start assigning to the right\n",
    "    for depth in remaining_depths:\n",
    "        if toggle and right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif not toggle and left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        elif right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        else:\n",
    "            print(\"Warning: Not all incremental depths assigned.\")\n",
    "            break\n",
    "        toggle = not toggle\n",
    "    return hyetograph\n",
    "\n",
    "# Function to generate the hyetograph for a given ARI\n",
    "def generate_hyetograph(incremental_depths, position_percent, T):\n",
    "    \"\"\"\n",
    "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    max_depth = np.max(incremental_depths)\n",
    "    incremental_depths_list = incremental_depths.tolist()\n",
    "    central_index = int(round(T * position_percent / 100)) - 1\n",
    "    central_index = max(0, min(central_index, T - 1))\n",
    "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
    "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
    "    return hyetograph\n",
    "\n",
    "# Function to save the hyetograph to a CSV file\n",
    "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
    "    \"\"\"\n",
    "    Saves the hyetograph to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Time_hour': np.arange(1, total_duration + 1),\n",
    "        'Precipitation_in': hyetograph\n",
    "    })\n",
    "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hyetographs for Different AEP Events\n",
    "\n",
    "Now, let's use the functions defined above to generate hyetographs for different AEP events.\n",
    "We'll download the NOAA Atlas 14 data for Davis, CA and generate hyetographs for various ARI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open NOAA Atlas 14 data from CSV file\n",
    "def open_noaa_atlas14_csv():\n",
    "    \"\"\"\n",
    "    Opens NOAA Atlas 14 data from CSV file in data directory.\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    data_dir = Path('data')\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    input_file = data_dir / 'PF_Depth_English_PDS_DavisCA.csv'\n",
    "    if input_file.exists():\n",
    "        logging.info(f\"NOAA Atlas 14 data file found: {input_file}\")\n",
    "        return str(input_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"NOAA Atlas 14 data file not found at {input_file}\")\n",
    "\n",
    "# Generate hyetographs\n",
    "def generate_all_hyetographs(input_csv, output_dir, ari_values, position_percent=50, total_duration=24):\n",
    "    \"\"\"\n",
    "    Generates hyetographs for specified ARI values.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv: Path to NOAA Atlas 14 CSV file\n",
    "    - output_dir: Directory to save hyetographs\n",
    "    - ari_values: List of ARI values to generate hyetographs for\n",
    "    - position_percent: Position percentage for peak intensity (default: 50%)\n",
    "    - total_duration: Total storm duration in hours (default: 24 hours)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping ARI values to hyetograph file paths\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Output directory is set to: {output_dir}\")\n",
    "    \n",
    "    # Read precipitation data\n",
    "    try:\n",
    "        df = read_precipitation_data(input_csv)\n",
    "        logging.info(\"Successfully read the input CSV file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading input CSV: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Display the first few rows of the DataFrame to verify\n",
    "    logging.info(\"\\nPrecipitation Frequency Data (first few rows):\")\n",
    "    display.display(df.head())\n",
    "    \n",
    "    # Interpolate depths\n",
    "    try:\n",
    "        D = interpolate_depths(df, total_duration)\n",
    "        logging.info(\"Successfully interpolated precipitation depths.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during interpolation: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Compute incremental depths\n",
    "    I = compute_incremental_depths(D, total_duration)\n",
    "    logging.info(\"Successfully computed incremental depths.\")\n",
    "    \n",
    "    # Generate and save hyetographs for each ARI\n",
    "    hyetograph_files = {}\n",
    "    for ari in ari_values:\n",
    "        ari_str = str(ari)\n",
    "        if ari_str in I:\n",
    "            incremental_depths = I[ari_str]\n",
    "            hyetograph = generate_hyetograph(incremental_depths, position_percent, total_duration)\n",
    "            hyetograph_file = save_hyetograph(hyetograph, ari_str, output_dir, position_percent, total_duration)\n",
    "            hyetograph_files[ari_str] = hyetograph_file\n",
    "        else:\n",
    "            logging.warning(f\"ARI {ari} not found in the input data. Skipping.\")\n",
    "    \n",
    "    logging.info(f\"\\nGenerated {len(hyetograph_files)} hyetographs.\")\n",
    "    return hyetograph_files\n",
    "\n",
    "# Plot multiple hyetographs\n",
    "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
    "    \"\"\"\n",
    "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    - aris (list of str or int): List of Annual Recurrence Intervals to plot\n",
    "    - position_percent (int): Position percentage for the maximum intensity\n",
    "    - total_duration (int): Total storm duration in hours\n",
    "    - output_dir (str): Directory where hyetograph CSV files are saved\n",
    "    \n",
    "    Returns:\n",
    "    - Plot object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    for ari in aris:\n",
    "        # Ensure ARI is a string for consistent filename formatting\n",
    "        ari_str = str(ari)\n",
    "        \n",
    "        # Construct the filename based on the naming convention\n",
    "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            logging.warning(f\"File '{filename}' does not exist in '{output_dir}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        try:\n",
    "            hyetograph_df = pd.read_csv(filepath)\n",
    "            logging.info(f\"Successfully read hyetograph data from '{filename}'.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading CSV file '{filename}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot the hyetograph\n",
    "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
    "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Time (Hour)', fontsize=14)\n",
    "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
    "    plt.title(f'Comparison of Hyetographs for Different ARIs\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 12)))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt_filename = f\"hyetograph_comparison_pos{position_percent}pct_{total_duration}hr.png\"\n",
    "    plt_filepath = os.path.join(output_dir, plt_filename)\n",
    "    plt.savefig(plt_filepath, dpi=300)\n",
    "    logging.info(f\"Saved comparison plot to {plt_filepath}\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Run the hyetograph generation process\n",
    "ari_values = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "position_percent = 50  # Position of peak intensity (50% = center of storm)\n",
    "total_duration = 24    # Total storm duration in hours\n",
    "\n",
    "# Open NOAA Atlas 14 data\n",
    "input_csv = open_noaa_atlas14_csv()\n",
    "\n",
    "# Generate hyetographs\n",
    "output_dir = 'hyetographs'\n",
    "hyetograph_files = generate_all_hyetographs(\n",
    "    input_csv=input_csv,\n",
    "    output_dir=output_dir,\n",
    "    ari_values=ari_values,\n",
    "    position_percent=position_percent,\n",
    "    total_duration=total_duration\n",
    ")\n",
    "\n",
    "# Plot the hyetographs\n",
    "plot = plot_multiple_hyetographs(\n",
    "    aris=ari_values,\n",
    "    position_percent=position_percent,\n",
    "    total_duration=total_duration,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Download and Prepare the Davis HEC-RAS Project\n",
    "\n",
    "Now, let's download the Davis project using RasExamples.extract_project() and prepare it for our analysis. \n",
    "We'll then create a new folder for our AEP analysis to keep the original project intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davis_path = RasExamples.extract_project(\"Davis\")\n",
    "\n",
    "# Create a new project folder for our AEP analysis\n",
    "def create_aep_project_folder(original_project_folder, new_project_name):\n",
    "    \"\"\"\n",
    "    Creates a new project folder for our AEP analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_project_folder: Path to the original project folder\n",
    "    - new_project_name: Name for the new project folder\n",
    "    \n",
    "    Returns:\n",
    "    - Path to the new project folder\n",
    "    \"\"\"\n",
    "    # Create path for the new project folder\n",
    "    new_project_folder = original_project_folder.parent / new_project_name\n",
    "    \n",
    "    # Remove the new project folder if it already exists\n",
    "    if new_project_folder.exists():\n",
    "        logging.info(f\"Removing existing folder: {new_project_folder}\")\n",
    "        shutil.rmtree(new_project_folder)\n",
    "    \n",
    "    # Copy the original project to the new folder\n",
    "    logging.info(f\"Copying project from {original_project_folder} to {new_project_folder}\")\n",
    "    shutil.copytree(original_project_folder, new_project_folder)\n",
    "    \n",
    "    logging.info(f\"Created new project folder: {new_project_folder}\")\n",
    "    return new_project_folder\n",
    "\n",
    "# Initialize the RAS project\n",
    "def initialize_ras_project(project_folder, ras_version=\"6.6\"):\n",
    "    \"\"\"\n",
    "    Initializes the RAS project and returns the RAS object.\n",
    "    \n",
    "    Parameters:\n",
    "    - project_folder: Path to the project folder\n",
    "    - ras_version: HEC-RAS version (default: \"6.6\")\n",
    "    \n",
    "    Returns:\n",
    "    - Initialized RAS object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ras_object = init_ras_project(project_folder, ras_version)\n",
    "        logging.info(f\"Initialized RAS project: {ras_object.project_name}\")\n",
    "        return ras_object\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing RAS project: {e}\")\n",
    "        raise\n",
    "\n",
    "# Download the Davis project\n",
    "davis_path = download_davis_project()\n",
    "\n",
    "# Create a new project folder for our AEP analysis\n",
    "aep_project_name = \"Davis_AEP_Analysis\"\n",
    "aep_project_folder = create_aep_project_folder(davis_path, aep_project_name)\n",
    "\n",
    "# Initialize the RAS project\n",
    "ras_object = initialize_ras_project(aep_project_folder)\n",
    "\n",
    "# Display project information\n",
    "print(\"\\nHEC-RAS Project Information:\")\n",
    "print(f\"Project Name: {ras_object.project_name}\")\n",
    "print(f\"Project Folder: {ras_object.project_folder}\")\n",
    "\n",
    "# Display available plans\n",
    "print(\"\\nAvailable Plans:\")\n",
    "display.display(ras_object.plan_df)\n",
    "\n",
    "# Display available unsteady flow files\n",
    "print(\"\\nAvailable Unsteady Flow Files:\")\n",
    "display.display(ras_object.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Clone Plans and Unsteady Flow Files for Each AEP Event\n",
    "\n",
    "Now, let's clone Plan 02 for each AEP event and update the plan and unsteady flow files with the appropriate hyetograph data. This will allow us to simulate different AEP events with HEC-RAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template plan and unsteady flow file numbers\n",
    "template_plan = \"02\"  # Use plan 02 as the template\n",
    "template_unsteady = \"01\"  # Use unsteady file 01 as the template\n",
    "\n",
    "# Function to create plans and unsteady flow files for each AEP event\n",
    "def create_aep_plans_and_unsteady_files(ras_object, template_plan, template_unsteady, ari_values, hyetograph_files):\n",
    "    \"\"\"\n",
    "    Creates plans and unsteady flow files for each AEP event.\n",
    "    \n",
    "    Parameters:\n",
    "    - ras_object: Initialized RAS object\n",
    "    - template_plan: Plan number to use as template\n",
    "    - template_unsteady: Unsteady flow file number to use as template\n",
    "    - ari_values: List of ARI values to create plans for\n",
    "    - hyetograph_files: Dictionary mapping ARI values to hyetograph file paths\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping ARI values to plan numbers\n",
    "    - Dictionary mapping ARI values to unsteady flow file numbers\n",
    "    \"\"\"\n",
    "    project_name = ras_object.project_name\n",
    "    project_folder = ras_object.project_folder\n",
    "    prj_file = ras_object.prj_file\n",
    "    \n",
    "    new_plan_numbers = {}\n",
    "    new_unsteady_numbers = {}\n",
    "    \n",
    "    for ari in ari_values:\n",
    "        ari_str = str(ari)\n",
    "        if ari_str not in hyetograph_files:\n",
    "            logging.warning(f\"ARI {ari} does not have a hyetograph file. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Create new plan file by cloning template\n",
    "        new_plan_number = RasPlan.clone_plan(template_plan, f\"ARI_{ari}_years\", ras_object)\n",
    "        logging.info(f\"Created new plan: {new_plan_number} for ARI {ari} years\")\n",
    "        new_plan_numbers[ari_str] = new_plan_number\n",
    "        \n",
    "        # Update plan description\n",
    "        plan_description = f\"Annual Exceedance Probability (AEP) Event\\nAnnual Recurrence Interval (ARI): {ari} years\\nExceedance Probability: {100/float(ari):.2f}%\"\n",
    "        # Get plan file path\n",
    "        plan_file_path = RasPlan.get_plan_path(new_plan_number, ras_object)\n",
    "        \n",
    "        # Update the plan file directly since update_plan_value doesn't exist\n",
    "        def update_description(lines):\n",
    "            updated_lines = []\n",
    "            in_description = False\n",
    "            description_updated = False\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.strip().startswith(\"Description=\"):\n",
    "                    updated_lines.append(f\"Description={plan_description}\\n\")\n",
    "                    description_updated = True\n",
    "                    in_description = True\n",
    "                elif in_description and line.strip() and not line.strip().startswith(\"Description=\"):\n",
    "                    in_description = False\n",
    "                    if not description_updated:\n",
    "                        updated_lines.append(line)\n",
    "                else:\n",
    "                    updated_lines.append(line)\n",
    "                    \n",
    "            return updated_lines\n",
    "        \n",
    "        # Use RasUtils to update the file\n",
    "        RasUtils.update_file(plan_file_path, update_description)\n",
    "        logging.info(f\"Updated plan description for ARI {ari} years\")\n",
    "        \n",
    "        # Create new unsteady flow file by cloning template\n",
    "        new_unsteady_number = RasPlan.clone_unsteady(template_unsteady, ras_object)\n",
    "        logging.info(f\"Created new unsteady flow file: {new_unsteady_number} for ARI {ari} years\")\n",
    "        new_unsteady_numbers[ari_str] = new_unsteady_number\n",
    "        \n",
    "        # Update unsteady flow file with hyetograph data\n",
    "        unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object)\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        hyetograph_file = hyetograph_files[ari_str]\n",
    "        hyetograph_data = pd.read_csv(hyetograph_file)\n",
    "        \n",
    "        # Update flow title in unsteady file\n",
    "        flow_title = f\"ARI_{ari}_years\"\n",
    "        RasUnsteady.update_flow_title(unsteady_file_path, flow_title, ras_object)\n",
    "        logging.info(f\"Updated flow title for ARI {ari} years\")\n",
    "        \n",
    "        # Apply the new unsteady file to the new plan\n",
    "        RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object)\n",
    "        logging.info(f\"Applied unsteady flow file {new_unsteady_number} to plan {new_plan_number}\")\n",
    "    \n",
    "    logging.info(f\"Created {len(new_plan_numbers)} plans and {len(new_unsteady_numbers)} unsteady flow files.\")\n",
    "    return new_plan_numbers, new_unsteady_numbers\n",
    "\n",
    "# Create plans and unsteady flow files for each AEP event\n",
    "new_plan_numbers, new_unsteady_numbers = create_aep_plans_and_unsteady_files(\n",
    "    ras_object=ras_object,\n",
    "    template_plan=template_plan,\n",
    "    template_unsteady=template_unsteady,\n",
    "    ari_values=ari_values,\n",
    "    hyetograph_files=hyetograph_files\n",
    ")\n",
    "\n",
    "# Display the new plans and unsteady flow files\n",
    "print(\"\\nNew Plans for AEP Events:\")\n",
    "for ari, plan_number in new_plan_numbers.items():\n",
    "    print(f\"ARI {ari} years: Plan {plan_number}\")\n",
    "\n",
    "print(\"\\nNew Unsteady Flow Files for AEP Events:\")\n",
    "for ari, unsteady_number in new_unsteady_numbers.items():\n",
    "    print(f\"ARI {ari} years: Unsteady Flow {unsteady_number}\")\n",
    "\n",
    "# Refresh the RAS object to see the new plans and unsteady flow files\n",
    "ras_object = initialize_ras_project(aep_project_folder)\n",
    "\n",
    "# Display all plans\n",
    "print(\"\\nAll Plans:\")\n",
    "display.display(ras_object.plan_df)\n",
    "\n",
    "# Display all unsteady flow files\n",
    "print(\"\\nAll Unsteady Flow Files:\")\n",
    "display.display(ras_object.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Execute All Plans in Parallel\n",
    "\n",
    "Now, let's execute all the plans we created in parallel using the RasCmdr.compute_parallel() function.\n",
    "This will allow us to efficiently run multiple HEC-RAS simulations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the optimal number of workers based on system resources\n",
    "def get_optimal_worker_count(cores_per_worker=2):\n",
    "    \"\"\"\n",
    "    Calculate the optimal number of workers based on available physical cores.\n",
    "    \n",
    "    Parameters:\n",
    "    - cores_per_worker: Number of cores to allocate to each worker (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal number of workers\n",
    "    \"\"\"\n",
    "    # Get physical CPU cores\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    if physical_cores is None:\n",
    "        physical_cores = psutil.cpu_count(logical=True) // 2  # Fallback estimate\n",
    "    \n",
    "    # Calculate optimal workers based on physical cores\n",
    "    optimal_workers = physical_cores // cores_per_worker\n",
    "    \n",
    "    # Ensure at least 1 worker\n",
    "    return max(1, optimal_workers)\n",
    "\n",
    "# Execute plans in parallel\n",
    "def execute_plans_in_parallel(ras_object, plan_numbers, compute_folder, cores_per_worker=2):\n",
    "    \"\"\"\n",
    "    Executes multiple HEC-RAS plans in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    - ras_object: Initialized RAS object\n",
    "    - plan_numbers: List of plan numbers to execute\n",
    "    - compute_folder: Folder to store computation results\n",
    "    - cores_per_worker: Number of cores to allocate to each worker (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of execution results\n",
    "    \"\"\"\n",
    "    # Check system resources\n",
    "    cpu_count = psutil.cpu_count(logical=True)\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    \n",
    "    logging.info(f\"System Resources:\")\n",
    "    logging.info(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores)\")\n",
    "    logging.info(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
    "    \n",
    "    # Calculate optimal number of workers\n",
    "    max_workers = get_optimal_worker_count(cores_per_worker)\n",
    "    logging.info(f\"Using {max_workers} workers with {cores_per_worker} cores per worker\")\n",
    "    \n",
    "    # Create compute folder if it doesn't exist\n",
    "    compute_folder = Path(compute_folder)\n",
    "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Compute folder: {compute_folder}\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute plans in parallel\n",
    "    logging.info(f\"Executing {len(plan_numbers)} plans in parallel...\")\n",
    "    results = RasCmdr.compute_parallel(\n",
    "        plan_number=plan_numbers,\n",
    "        max_workers=max_workers,\n",
    "        num_cores=cores_per_worker,\n",
    "        dest_folder=compute_folder,\n",
    "        overwrite_dest=True,\n",
    "        ras_object=ras_object\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    total_duration = end_time - start_time\n",
    "    \n",
    "    logging.info(f\"Parallel execution completed in {total_duration:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create compute folder\n",
    "compute_folder = aep_project_folder.parent / \"Davis_AEP_Compute\"\n",
    "\n",
    "# Get plan numbers to execute\n",
    "plan_numbers_to_execute = list(new_plan_numbers.values())\n",
    "print(f\"Executing {len(plan_numbers_to_execute)} plans: {plan_numbers_to_execute}\")\n",
    "\n",
    "# Execute plans in parallel\n",
    "execution_results = execute_plans_in_parallel(\n",
    "    ras_object=ras_object,\n",
    "    plan_numbers=plan_numbers_to_execute,\n",
    "    compute_folder=compute_folder,\n",
    "    cores_per_worker=2\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success, \"ARI\": next((ari for ari, p in new_plan_numbers.items() if p == plan), None)}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Sort by ARI\n",
    "results_df = results_df.sort_values(\"ARI\", key=lambda x: x.astype(float))\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)\n",
    "\n",
    "# Initialize a RAS project in the compute folder\n",
    "compute_ras_object = initialize_ras_project(compute_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Extract and Visualize Results\n",
    "\n",
    "Finally, let's extract and visualize the results from the HEC-RAS simulations. We'll use the ras-commander library to extract water surface elevation (WSEL) data for a specific cell in the 2D mesh and create comparison plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated an end-to-end workflow for performing flood analysis with different Annual Exceedance Probability (AEP) events using HEC-RAS and the ras-commander library. The key steps were:\n",
    "\n",
    "1. **Generate Hyetographs**: Created precipitation hyetographs for different ARI values using NOAA Atlas 14 data\n",
    "2. **Download and Prepare the HEC-RAS Project**: Downloaded the Davis project and created a new project folder for our analysis\n",
    "3. **Clone and Configure Plans**: Created new plans and unsteady flow files for each AEP event and updated them with the appropriate hyetographs\n",
    "4. **Execute Plans in Parallel**: Used parallel processing to efficiently run multiple HEC-RAS simulations simultaneously\n",
    "5. **Extract and Visualize Results**: Extracted water surface elevation data for a specific cell and created comparison plots\n",
    "\n",
    "This workflow demonstrates how the ras-commander library can be used to automate HEC-RAS workflows, making it easier to perform complex analyses with multiple scenarios. By using parallel processing, we can significantly reduce the overall computation time required for multi-scenario analyses.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To build on this analysis, you could:\n",
    "- Extract results for multiple cells or cross-sections to analyze spatial patterns of flooding\n",
    "- Create flood extent maps for different AEP events\n",
    "- Perform sensitivity analysis by varying parameters such as roughness or infiltration\n",
    "- Compare results with observed flood data for model calibration\n",
    "- Create animated visualizations of the flooding process\n",
    "\n",
    "These extensions would further enhance the value of the analysis and provide more comprehensive insights into flood behavior and risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\103_generating_aep_hyetographs_from_atlas_14_r.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from math import log, exp\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to parse duration strings and convert them to hours\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a duration string and converts it to hours.\n",
    "    Examples:\n",
    "        \"5-min:\" -> 0.0833 hours\n",
    "        \"2-hr:\" -> 2 hours\n",
    "        \"2-day:\" -> 48 hours\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "    value, unit = match.groups()\n",
    "    value = int(value)\n",
    "    unit = unit.lower()\n",
    "    if unit in ['min', 'minute', 'minutes']:\n",
    "        hours = value / 60.0\n",
    "    elif unit in ['hr', 'hour', 'hours']:\n",
    "        hours = value\n",
    "    elif unit in ['day', 'days']:\n",
    "        hours = value * 24\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
    "    return hours\n",
    "\n",
    "# Function to read and process the precipitation frequency CSV\n",
    "def read_precipitation_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the precipitation frequency CSV and returns a DataFrame\n",
    "    with durations in hours as the index and ARIs as columns.\n",
    "    This function dynamically locates the header line for the data table.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_line_idx = None\n",
    "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
    "\n",
    "    # Locate the header line\n",
    "    for idx, line in enumerate(lines):\n",
    "        if header_pattern.match(line.strip().lower()):\n",
    "            header_line_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_line_idx is None:\n",
    "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
    "\n",
    "    # Extract the ARI headers from the header line\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    headers = [item.strip() for item in header_line.split(',')]\n",
    "    \n",
    "    if len(headers) < 2:\n",
    "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
    "\n",
    "    aris = headers[1:]  # Exclude the first column which is the duration\n",
    "\n",
    "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
    "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
    "\n",
    "    # Initialize lists to store durations and corresponding depths\n",
    "    durations = []\n",
    "    depths = {ari: [] for ari in aris}\n",
    "\n",
    "    # Iterate over the lines following the header to extract data\n",
    "    for line in lines[header_line_idx + 1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if not duration_pattern.match(line):\n",
    "            break  # Stop if the line does not match the duration pattern\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if len(parts) != len(headers):\n",
    "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
    "        duration_str = parts[0]\n",
    "        try:\n",
    "            duration_hours = parse_duration(duration_str)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Skipping line due to error: {ve}\")\n",
    "            continue  # Skip lines with invalid duration formats\n",
    "        durations.append(duration_hours)\n",
    "        for ari, depth_str in zip(aris, parts[1:]):\n",
    "            try:\n",
    "                depth = float(depth_str)\n",
    "            except ValueError:\n",
    "                depth = np.nan  # Assign NaN for invalid depth values\n",
    "            depths[ari].append(depth)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(depths, index=durations)\n",
    "    df.index.name = 'Duration_hours'\n",
    "\n",
    "    # Drop any rows with NaN values (optional, based on data quality)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to perform log-log linear interpolation for each ARI\n",
    "def interpolate_depths(df, total_duration):\n",
    "    \"\"\"\n",
    "    Interpolates precipitation depths for each ARI on a log-log scale\n",
    "    for each hour up to the total storm duration.\n",
    "    \"\"\"\n",
    "    T = total_duration\n",
    "    t_hours = np.arange(1, T+1)\n",
    "    D = {}\n",
    "    for ari in df.columns:\n",
    "        durations = df.index.values\n",
    "        depths = df[ari].values\n",
    "        # Ensure all depths are positive\n",
    "        if np.any(depths <= 0):\n",
    "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
    "        # Log-log interpolation\n",
    "        log_durations = np.log(durations)\n",
    "        log_depths = np.log(depths)\n",
    "        log_t = np.log(t_hours)\n",
    "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
    "        D_t = np.exp(log_D_t)\n",
    "        D[ari] = D_t\n",
    "    return D\n",
    "\n",
    "# Function to compute incremental precipitation depths\n",
    "def compute_incremental_depths(D, total_duration):\n",
    "    \"\"\"\n",
    "    Computes incremental precipitation depths for each hour.\n",
    "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
    "    \"\"\"\n",
    "    incremental_depths = {}\n",
    "    for ari, D_t in D.items():\n",
    "        I_t = np.empty(total_duration)\n",
    "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
    "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
    "        incremental_depths[ari] = I_t\n",
    "    return incremental_depths\n",
    "\n",
    "# Function to assign incremental depths using the Alternating Block Method\n",
    "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
    "    \"\"\"\n",
    "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    hyetograph = [0.0] * T\n",
    "    hyetograph[central_index] = max_depth\n",
    "    remaining_depths = sorted_depths.copy()\n",
    "    remaining_depths.remove(max_depth)\n",
    "    left = central_index - 1\n",
    "    right = central_index + 1\n",
    "    toggle = True  # Start assigning to the right\n",
    "    for depth in remaining_depths:\n",
    "        if toggle and right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif not toggle and left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        elif right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        else:\n",
    "            print(\"Warning: Not all incremental depths assigned.\")\n",
    "            break\n",
    "        toggle = not toggle\n",
    "    return hyetograph\n",
    "\n",
    "# Function to generate the hyetograph for a given ARI\n",
    "def generate_hyetograph(incremental_depths, position_percent, T):\n",
    "    \"\"\"\n",
    "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    max_depth = np.max(incremental_depths)\n",
    "    incremental_depths_list = incremental_depths.tolist()\n",
    "    central_index = int(round(T * position_percent / 100)) - 1\n",
    "    central_index = max(0, min(central_index, T - 1))\n",
    "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
    "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
    "    return hyetograph\n",
    "\n",
    "# Function to save the hyetograph to a CSV file\n",
    "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
    "    \"\"\"\n",
    "    Saves the hyetograph to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Time_hour': np.arange(1, total_duration + 1),\n",
    "        'Precipitation_in': hyetograph\n",
    "    })\n",
    "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
    "\n",
    "# User Inputs\n",
    "# --------------------\n",
    "# Set the path to your input CSV file from NOAA Atlas 14\n",
    "input_csv = 'data\\PF_Depth_English_PDS_DavisCA.csv'  # Update this path if necessary\n",
    "\n",
    "# Set the output directory where hyetograph CSV files will be saved\n",
    "output_dir = 'hyetographs'\n",
    "\n",
    "# Set the position percentage for the maximum incremental depth block\n",
    "# Choose from 25, 33, 50, 67, or 75\n",
    "position_percent = 50  # Default is 50\n",
    "\n",
    "# Set the total storm duration in hours\n",
    "total_duration = 24  # Default is 24 hours\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "# Read precipitation data\n",
    "try:\n",
    "    df = read_precipitation_data(input_csv)\n",
    "    print(\"Successfully read the input CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading input CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(\"\\nPrecipitation Frequency Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# Interpolate depths\n",
    "try:\n",
    "    D = interpolate_depths(df, total_duration)\n",
    "    print(\"Successfully interpolated precipitation depths.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during interpolation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display interpolated depths for the first ARI\n",
    "first_ari = df.columns[0]\n",
    "print(f\"\\nInterpolated Depths for ARI {first_ari} years:\")\n",
    "print(D[first_ari])\n",
    "\n",
    "# Compute incremental depths\n",
    "I = compute_incremental_depths(D, total_duration)\n",
    "print(\"Successfully computed incremental depths.\")\n",
    "\n",
    "# Generate and save hyetographs for each ARI\n",
    "for ari, incremental_depths in I.items():\n",
    "    hyetograph = generate_hyetograph(incremental_depths, position_percent, total_duration)\n",
    "    save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration)\n",
    "\n",
    "print(\"\\nAll hyetographs have been generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the hyetographs (final request from o1-mini)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot multiple hyetographs on the same plot\n",
    "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
    "    \"\"\"\n",
    "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    - aris (list of str or int): List of Annual Recurrence Intervals to plot (e.g., [1, 2, 5, 10])\n",
    "    - position_percent (int): Position percentage for the maximum incremental depth block (25, 33, 50, 67, or 75)\n",
    "    - total_duration (int): Total storm duration in hours\n",
    "    - output_dir (str): Directory where hyetograph CSV files are saved\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    for ari in aris:\n",
    "        # Ensure ARI is a string for consistent filename formatting\n",
    "        ari_str = str(ari)\n",
    "        \n",
    "        # Construct the filename based on the naming convention\n",
    "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: File '{filename}' does not exist in the directory '{output_dir}'. Skipping this ARI.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        try:\n",
    "            hyetograph_df = pd.read_csv(filepath)\n",
    "            print(f\"Successfully read the hyetograph data from '{filename}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading the hyetograph CSV file '{filename}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot the hyetograph\n",
    "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
    "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Time (Hour)', fontsize=14)\n",
    "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
    "    plt.title(f'Comparison of Hyetographs for ARIs {aris}\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 24)))  # Adjust x-ticks based on duration\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# User Inputs for Multiple ARIs\n",
    "# --------------------\n",
    "# Set the Annual Recurrence Intervals you want to plot\n",
    "aris_to_plot = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]  # Example: Multiple ARIs\n",
    "\n",
    "# Set the position percentage for the maximum incremental depth block\n",
    "position_percent = 50  # Example: 50%\n",
    "\n",
    "# Set the total storm duration in hours\n",
    "total_duration = 24  # Example: 24 hours\n",
    "\n",
    "# Set the output directory where hyetograph CSV files are saved\n",
    "output_dir = 'hyetographs'  # Ensure this matches the output directory used previously\n",
    "\n",
    "# Plot the multiple hyetographs\n",
    "plot_multiple_hyetographs(aris=aris_to_plot, \n",
    "                           position_percent=position_percent, \n",
    "                           total_duration=total_duration, \n",
    "                           output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: REVISE BELOW TO RUN DAVIS AND EXTRACT RESULTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\10_1d_hdf_data_extraction.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS 1D HDF Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing ras-commander flexibly (from package or local dev copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install --upgrade ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell will try to import the pip package, if it fails it will \n",
    "# add the parent directory to the Python path and try to import again\n",
    "# This assumes you are working in a subfolder of the ras-commander repository\n",
    "# This allows a user's revisions to be tested locally without installing the package\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Flexible imports to allow for development without installation \n",
    "#  ** Use this version with Jupyter Notebooks **\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    from ras_commander import *\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    import os\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    rascmdr_directory = current_file.parent\n",
    "    sys.path.append(str(rascmdr_directory))\n",
    "    print(\"Loading ras-commander from local dev copy\")\n",
    "    # Now try to import again\n",
    "    from ras_commander import *\n",
    "print(\"ras_commander imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and run plan 01\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"Balde Eagle Creek\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagle.p01.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Balde Eagle project initialized with folder: {ras.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Balde Eagle object id: {id(ras)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"01\"\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagle.p01.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "ras.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "ras.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"01\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasHdfUtils\n",
    "| Method Name | Description |\n",
    "|-------------|-------------|\n",
    "| get_attrs | Converts attributes from a HEC-RAS HDF file into a Python dictionary for a given attribute path |\n",
    "| get_root_attrs | Returns attributes at root level of HEC-RAS HDF file |\n",
    "| get_hdf_paths_with_properties | Gets all paths in the HDF file with their properties |\n",
    "| get_group_attributes_as_df | Gets attributes of a group in the HDF file as a DataFrame |\n",
    "| get_hdf_filename | Gets the HDF filename from various input types |\n",
    "| get_runtime_data | Extracts runtime and compute time data from a single HDF file |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDF Paths with Properties (For Exploring HDF Files)\n",
    "HdfBase.get_dataset_info(plan_number, group_path=\"/Geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(\"01\")  \n",
    "# NOTE: Here we call the function using the plan number\n",
    "geom_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(\"1\") \n",
    "# NOTE: Here we use plan number \"1\" without the leading zero.  The decorator ensures that this still works\n",
    "\n",
    "\n",
    "runtime_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of all the functions in the RasGeomHdf class from the ras_commander/RasGeomHdf.py file:\n",
    "\n",
    "| Function Name | Description |\n",
    "|---------------|-------------|\n",
    "| projection | Returns the projection of the RAS geometry as a pyproj.CRS object |\n",
    "| get_geom_attrs | Returns base geometry attributes from a HEC-RAS HDF file |\n",
    "\n",
    "| mesh_area_names | Returns a list of the 2D mesh area names of the RAS geometry |\n",
    "| get_geom_2d_flow_area_attrs | Returns geometry 2d flow area attributes from a HEC-RAS HDF file |\n",
    "| mesh_areas | Returns 2D flow area perimeter polygons |\n",
    "| mesh_cell_polygons | Returns 2D flow mesh cell polygons |\n",
    "| mesh_cell_points | Returns 2D flow mesh cell points |\n",
    "| mesh_cell_faces | Returns 2D flow mesh cell faces |\n",
    "\n",
    "| get_geom_structures_attrs | Returns geometry structures attributes from a HEC-RAS HDF file |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| bc_lines | Returns 2D mesh area boundary condition lines |\n",
    "| breaklines | Returns 2D mesh area breaklines |\n",
    "\n",
    "\n",
    "\n",
    "| refinement_regions | Returns 2D mesh area refinement regions |\n",
    "| structures | Returns the model structures |\n",
    "| reference_lines_names | Returns reference line names |\n",
    "| reference_points_names | Returns reference point names |\n",
    "| reference_lines | Returns the reference lines geometry and attributes |\n",
    "| reference_points | Returns the reference points geometry and attributes |\n",
    "| cross_sections | Returns the model 1D cross sections |\n",
    "| river_reaches | Returns the model 1D river reach lines |\n",
    "| cross_sections_elevations | Returns the model cross section elevation information |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
    "print(geom_hdf_path)\n",
    "\n",
    "# For the example project, plan 06 is associated with geometry 09\n",
    "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
    "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.  This attribute is only included if a RASMapper projection is defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example project we are using does not have a projection, so error messages should be expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
    "geom_attrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geometry structures attributes\n",
    "print(\"\\nGetting geometry structures attributes\")\n",
    "geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path)\n",
    "if geom_structures_attrs:\n",
    "    print(\"Geometry structures attributes:\")\n",
    "    for key, value in geom_structures_attrs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No geometry structures attributes found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVISION NOTE: NEED TO EDIT THIS TO SHOW BC LINES WITH RIVERS AND CROSS SECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Extract Boundary Condition Lines and Plot with 2D Flow Area Perimeter Polygons\n",
    "print(\"\\nExample 7: Extracting Boundary Condition Lines and Plotting with 2D Flow Area Perimeter Polygons\")\n",
    "bc_lines_df = HdfBndry.bc_lines(geom_hdf_path)\n",
    "if not bc_lines_df.empty:\n",
    "    display(bc_lines_df.head())\n",
    "else:\n",
    "    print(\"No Boundary Condition Lines found.\")\n",
    "\n",
    "# Plot if data exists\n",
    "if not bc_lines_df.empty or not mesh_areas.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 2D Flow Area Perimeter Polygons\n",
    "    if not mesh_areas.empty:\n",
    "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
    "        \n",
    "        # Add labels for each polygon\n",
    "        for idx, row in mesh_areas.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            label = row.get('Name', f'Area {idx}')\n",
    "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "    \n",
    "    # Plot boundary condition lines\n",
    "    if not bc_lines_df.empty:\n",
    "        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTEAD OF hdf_input, USE plan_hdf_path or geom_hdf_path as appropriate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get structures\n",
    "structures_gdf = HdfStruc.get_structures(geom_hdf_path)\n",
    "print(\"Structures:\")\n",
    "if not structures_gdf.empty:\n",
    "    structures_gdf\n",
    "else:\n",
    "    print(\"No structures found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get reference lines\n",
    "ref_lines_gdf = HdfBndry.get_reference_lines(geom_hdf_path)\n",
    "print(\"\\nReference Lines:\")\n",
    "if not ref_lines_gdf.empty:\n",
    "    display(ref_lines_gdf.head())\n",
    "else:\n",
    "    print(\"No reference lines found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get reference points\n",
    "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)\n",
    "print(\"\\nReference Points:\")\n",
    "if not ref_points_gdf.empty:\n",
    "    display(ref_points_gdf.head())\n",
    "else:\n",
    "    print(\"No reference points found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the get_hdf5_dataset_info function from HdfUtils to explore the Cross Sections structure in the geometry HDF file\n",
    "\n",
    "print(\"\\nExploring Cross Sections structure in geometry file:\")\n",
    "print(\"HDF Base Path: /Geometry/Cross Sections \")\n",
    "HdfBase.get_dataset_info(geom_hdf_path, group_path='/Geometry/Cross Sections')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get cross section geodataframe\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
    "with pd.option_context('display.max_columns', None):  # Show all columns\n",
    "    cross_sections_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_sections_gdf: \n",
    "\n",
    "| geometry | station_elevation | mannings_n | ineffective_blocks | River | Reach | RS | Name | Description | Len Left | Len Channel | Len Right | Left Bank | Right Bank | Friction Mode | Contr | Expan | Left Levee Sta | Left Levee Elev | Right Levee Sta | Right Levee Elev | HP Count | HP Start Elev | HP Vert Incr | HP LOB Slices | HP Chan Slices | HP ROB Slices | Ineff Block Mode | Obstr Block Mode | Default Centerline | Last Edited |\n",
    "|-----------|-------------------|------------|--------------------|-------|-------|----|------|-------------|----------|-------------|-----------|-----------|------------|----------------|-------|-------|----------------|-----------------|----------------|------------------|----------|----------------|---------------|----------------|----------------|----------------|------------------|------------------|-------------------|--------------|\n",
    "| 0         | LINESTRING (1968668.17 290166.79, 1969067.87 2... | [[0.0, 660.41], [5.0, 660.61], [40.0, 659.85],... | {'Station': [0.0, 190.0, 375.0], 'Mann n': [0.... | []    | Bald Eagle | Loc Hav | 138154.4 |             | 358.429993 | 463.640015 | 517.640015 | 190.000000 | 375.000000 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 49       | 656.799988      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n",
    "| 1         | LINESTRING (1968627.02 290584.12, 1969009.09 2... | [[0.0, 664.28], [50.0, 661.73], [55.0, 661.54]... | {'Station': [0.0, 535.0, 672.5599975585938], '... | []    | Bald Eagle | Loc Hav | 137690.8 |             | 305.709991 | 363.839996 | 382.829987 | 535.000000 | 672.559998 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 65       | 654.229980      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n",
    "| 2         | LINESTRING (1968585.88 290854.5, 1968868.02 29... | [[0.0, 662.72], [20.0, 665.5], [25.0, 666.48],... | {'Station': [0.0, 580.0, 717.239990234375], 'M... | []    | Bald Eagle | Loc Hav | 137327.0 |             | 732.929993 | 762.020020 | 765.359985 | 580.000000 | 717.239990 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 66       | 653.900024      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where ineffective_blocks is not empty\n",
    "ineffective_xs = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(len) > 0]\n",
    "\n",
    "print(\"\\nCross Sections with Ineffective Flow Areas:\")\n",
    "ineffective_xs\n",
    "\n",
    "# Print a message if no cross sections with ineffective flow areas are found\n",
    "print(\"\\nNo cross sections found with ineffective flow areas.\" if ineffective_xs.empty else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross sections data\n",
    "\n",
    "print(\"\\nCross Section Information:\")\n",
    "if not cross_sections_gdf.empty:\n",
    "    for idx, row in cross_sections_gdf.iterrows():\n",
    "        print(f\"\\nCross Section {idx + 1}:\")\n",
    "        print(f\"River: {row['River']}\")\n",
    "        print(f\"Reach: {row['Reach']}\")\n",
    "        print(\"\\nGeometry:\")\n",
    "        print(row['geometry'])\n",
    "        print(\"\\nStation-Elevation Points:\")\n",
    "        \n",
    "        # Print header\n",
    "        print(\"     #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation\")\n",
    "        print(\"-\" * 150)\n",
    "        \n",
    "        # Calculate number of rows needed\n",
    "        points = row['station_elevation']\n",
    "        num_rows = (len(points) + 4) // 5  # Round up division\n",
    "        \n",
    "        # Print points in 5 columns\n",
    "        for i in range(num_rows):\n",
    "            line = \"\"\n",
    "            for j in range(5):\n",
    "                point_idx = i + j * num_rows\n",
    "                if point_idx < len(points):\n",
    "                    station, elevation = points[point_idx]\n",
    "                    line += f\"{point_idx+1:6d} {station:10.2f} {elevation:10.2f}    \"\n",
    "            print(line)\n",
    "        print(\"-\" * 150)\n",
    "else:\n",
    "    print(\"No cross sections found in the geometry file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross sections on map\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get cross sections data\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
    "\n",
    "if not cross_sections_gdf.empty:\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    \n",
    "    # Plot cross sections\n",
    "    cross_sections_gdf.plot(ax=ax, color='red', linewidth=1, label='Cross Sections')\n",
    "    \n",
    "    # Add river name and reach labels\n",
    "    #for idx, row in cross_sections_gdf.iterrows():\n",
    "    #    # Get midpoint of cross section line for label placement\n",
    "    #    midpoint = row.geometry.centroid\n",
    "    #    label = f\"{row['River']}\\n{row['Reach']}\\nRS: {row['RS']}\"\n",
    "    #    ax.annotate(label, (midpoint.x, midpoint.y), \n",
    "    #               xytext=(5, 5), textcoords='offset points',\n",
    "    #               fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title('Cross Sections Location Map')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Equal aspect ratio to preserve shape\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cross sections found in the geometry file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross sections with Manning's n values colored by value\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Create figure\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Create colormap\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(vmin=0.02, vmax=0.08)  # Typical Manning's n range\n",
    "\n",
    "# Plot cross sections colored by Manning's n\n",
    "for idx, row in cross_sections_gdf.iterrows():\n",
    "    # Extract Manning's n values and stations\n",
    "    mannings = row['mannings_n']\n",
    "    n_values = mannings['Mann n']\n",
    "    stations = mannings['Station']\n",
    "    \n",
    "    # Get the full linestring coordinates\n",
    "    line_coords = list(row.geometry.coords)\n",
    "    \n",
    "    # Calculate total length of the cross section\n",
    "    total_length = row.geometry.length\n",
    "    \n",
    "    # For each Manning's n segment\n",
    "    for i in range(len(n_values)-1):\n",
    "        # Calculate the start and end proportions along the line\n",
    "        start_prop = stations[i] / stations[-1]\n",
    "        end_prop = stations[i+1] / stations[-1]\n",
    "        \n",
    "        # Get the start and end points for this segment\n",
    "        start_idx = int(start_prop * (len(line_coords)-1))\n",
    "        end_idx = int(end_prop * (len(line_coords)-1))\n",
    "        \n",
    "        # Extract the segment coordinates\n",
    "        segment_coords = line_coords[start_idx:end_idx+1]\n",
    "        \n",
    "        if len(segment_coords) >= 2:\n",
    "            # Create a line segment\n",
    "            segment = LineString(segment_coords)\n",
    "            \n",
    "            # Get color from colormap for this n value\n",
    "            color = cmap(norm(n_values[i]))\n",
    "            \n",
    "            # Plot the segment\n",
    "            ax1.plot(*segment.xy, color=color, linewidth=2)\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, ax=ax1, label=\"Manning's n Value\")\n",
    "\n",
    "ax1.set_title(\"Cross Sections Colored by Manning's n Values\")\n",
    "ax1.grid(True)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross sections with ineffective flow areas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get cross sections data\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
    "\n",
    "# Create figure\n",
    "fig, ax2 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Plot all cross sections first\n",
    "cross_sections_gdf.plot(ax=ax2, color='lightgray', linewidth=1, label='Cross Sections')\n",
    "\n",
    "# Plot ineffective flow areas with thicker lines\n",
    "ineffective_sections = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(lambda x: len(x) > 0)]\n",
    "ineffective_sections.plot(ax=ax2, color='red', linewidth=3, label='Ineffective Flow Areas')\n",
    "\n",
    "# Add ineffective flow area labels with offset to lower right\n",
    "for idx, row in cross_sections_gdf.iterrows():\n",
    "    # Get midpoint of cross section line\n",
    "    midpoint = row.geometry.centroid\n",
    "    \n",
    "    # Extract ineffective flow blocks\n",
    "    ineff_blocks = row['ineffective_blocks']\n",
    "    \n",
    "    if ineff_blocks:  # Only label if there are ineffective blocks\n",
    "        label_parts = []\n",
    "        # Add RS to first line of label\n",
    "        label_parts.append(f\"RS: {row['RS']}\")\n",
    "        for block in ineff_blocks:\n",
    "            label_parts.append(\n",
    "                f\"L:{block['Left Sta']:.0f}-R:{block['Right Sta']:.0f}\\n\"\n",
    "                f\"Elev: {block['Elevation']:.2f}\\n\"\n",
    "                f\"Permanent: {block['Permanent']}\"\n",
    "            )\n",
    "        \n",
    "        label = '\\n'.join(label_parts)\n",
    "        \n",
    "        ax2.annotate(label, (midpoint.x, midpoint.y),\n",
    "                    xytext=(15, -15),  # Offset to lower right\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8, \n",
    "                    bbox=dict(facecolor='white', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->'),\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='top')\n",
    "\n",
    "ax2.set_title('Cross Sections with Ineffective Flow Areas')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross section elevation for cross section 42\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get cross sections data\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
    "\n",
    "if not cross_sections_gdf.empty:\n",
    "    # Get station-elevation data for cross section 42\n",
    "    station_elevation = cross_sections_gdf.iloc[42]['station_elevation']\n",
    "    \n",
    "    # Convert list of lists to numpy arrays for plotting\n",
    "    stations = np.array([point[0] for point in station_elevation])\n",
    "    elevations = np.array([point[1] for point in station_elevation])\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    \n",
    "    # Plot cross section\n",
    "    ax.plot(stations, elevations, 'b-', linewidth=2)\n",
    "    \n",
    "    # Add labels and title\n",
    "    river = cross_sections_gdf.iloc[42]['River']\n",
    "    reach = cross_sections_gdf.iloc[42]['Reach'] \n",
    "    rs = cross_sections_gdf.iloc[42]['RS']\n",
    "    \n",
    "    # Show bank stations as dots\n",
    "    left_bank_station = cross_sections_gdf.iloc[42]['Left Bank']\n",
    "    right_bank_station = cross_sections_gdf.iloc[42]['Right Bank']\n",
    "    \n",
    "    # Interpolating bank stations for plotting\n",
    "    ax.plot(left_bank_station, elevations[np.searchsorted(stations, left_bank_station)], 'ro', label='Left Bank Station')\n",
    "    ax.plot(right_bank_station, elevations[np.searchsorted(stations, right_bank_station)], 'ro', label='Right Bank Station')\n",
    "    \n",
    "    ax.set_title(f'Cross Section Profile\\nRiver: {river}, Reach: {reach}, RS: {rs}\\n'\n",
    "                 f'Left Bank Station: {left_bank_station}, Right Bank Station: {right_bank_station}')\n",
    "    ax.set_xlabel('Station (ft)')\n",
    "    ax.set_ylabel('Elevation (ft)')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cross sections found in the geometry file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "centerlines = HdfXsec.get_river_centerlines(geom_hdf_path)\n",
    "centerlines_with_stations = HdfXsec.get_river_stationing(centerlines)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRiver Centerlines:\")\n",
    "centerlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot river centerlines with labels\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot centerlines\n",
    "centerlines.plot(ax=ax, color='blue', linewidth=2, label='River Centerline')\n",
    "\n",
    "# Add river/reach labels\n",
    "for idx, row in centerlines.iterrows():\n",
    "    # Get midpoint of the line for label placement\n",
    "    midpoint = row.geometry.interpolate(0.5, normalized=True)\n",
    "    \n",
    "    # Create label text combining river and reach names\n",
    "    label = f\"{row['River Name']}\\n{row['Reach Name']}\"\n",
    "    \n",
    "    # Add text annotation\n",
    "    ax.annotate(label, \n",
    "                xy=(midpoint.x, midpoint.y),\n",
    "                xytext=(10, 10), # Offset text slightly\n",
    "                textcoords='offset points',\n",
    "                fontsize=10,\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title('River Centerlines', fontsize=14)\n",
    "ax.set_xlabel('Easting', fontsize=12)\n",
    "ax.set_ylabel('Northing', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "edge_lines = HdfXsec.get_river_edge_lines(geom_hdf_path)\n",
    "centerlines = HdfXsec.get_river_centerlines(geom_hdf_path)\n",
    "# Display results\n",
    "print(\"\\nRiver Edge Lines:\")\n",
    "edge_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "bank_lines = HdfXsec.get_river_bank_lines(geom_hdf_path)\n",
    "# Display results\n",
    "print(\"\\nRiver Bank Lines:\")\n",
    "bank_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot river edge lines\n",
    "edge_lines.plot(ax=ax, color='blue', linewidth=2, label='River Edge Lines')\n",
    "\n",
    "# Plot centerlines for reference\n",
    "centerlines.plot(ax=ax, color='red', linewidth=2, linestyle='--', label='River Centerline')\n",
    "\n",
    "# Plot river bank lines\n",
    "bank_lines.plot(ax=ax, color='green', linewidth=2, label='River Bank Lines')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('River Edge Lines, Centerline, and Bank Lines', fontsize=14)\n",
    "ax.set_xlabel('Easting', fontsize=12)\n",
    "ax.set_ylabel('Northing', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get dataset structure:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/River Bank Lines/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to explore HDF file to assist with 1D Structures Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Output/Output Blocks/Computation Block/Global/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Structures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1D Structures Geodataframe\n",
    "\n",
    "\n",
    "# Extract data into GeoDataFrames\n",
    "structures_gdf = HdfStruc.get_structures(hdf_path=geom_hdf_path)\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(hdf_path=geom_hdf_path)\n",
    "centerlines_gdf = HdfXsec.get_river_centerlines(hdf_path=geom_hdf_path)\n",
    "\n",
    "# Display basic information about the structures\n",
    "print(\"\\nStructures Summary:\")\n",
    "print(f\"Number of structures found: {len(structures_gdf)}\")\n",
    "structures_gdf\n",
    "\n",
    "# Display first few rows of key attributes\n",
    "print(\"\\nStructure Details:\")\n",
    "display_cols = ['Structure ID', 'Structure Type', 'River Name', 'Reach Name', 'Station']\n",
    "display_cols = [col for col in display_cols if col in structures_gdf.columns]\n",
    "if display_cols:\n",
    "    print(structures_gdf[display_cols].head())\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot river centerlines\n",
    "if not centerlines_gdf.empty:\n",
    "    centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerlines')\n",
    "\n",
    "# Plot cross sections\n",
    "if not cross_sections_gdf.empty:\n",
    "    cross_sections_gdf.plot(ax=ax, color='green', linewidth=1, label='Cross Sections')\n",
    "\n",
    "# Plot structures\n",
    "if not structures_gdf.empty:\n",
    "    structures_gdf.plot(ax=ax, color='red', marker='s', markersize=100, label='Structures')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('HEC-RAS Model Components', fontsize=14)\n",
    "ax.set_xlabel('Easting', fontsize=12)\n",
    "ax.set_ylabel('Northing', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Print summary of cross sections\n",
    "print(\"\\nCross Sections Summary:\")\n",
    "print(f\"Number of cross sections found: {len(cross_sections_gdf)}\")\n",
    "if not cross_sections_gdf.empty:\n",
    "    print(\"\\nCross Section Details:\")\n",
    "    xs_display_cols = ['River', 'Reach', 'Station']\n",
    "    xs_display_cols = [col for col in xs_display_cols if col in cross_sections_gdf.columns]\n",
    "    if xs_display_cols:\n",
    "        print(cross_sections_gdf[xs_display_cols].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Compute Messages as String\n",
    "print(\"Extracting Compute Messages\")\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract string from HDF object at a given path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_hdf_filename : str\n",
    "        Name of the HDF file\n",
    "    hdf_path : str\n",
    "        Path of the object in the HDF file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Extracted string from the specified HDF object\n",
    "    \"\"\"\n",
    "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
    "        try:\n",
    "            hdf_object = hdf_file[hdf_path]\n",
    "            if isinstance(hdf_object, h5py.Group):\n",
    "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
    "            elif isinstance(hdf_object, h5py.Dataset):\n",
    "                data = hdf_object[()]\n",
    "                if isinstance(data, bytes):\n",
    "                    return data.decode('utf-8')\n",
    "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
    "                    return [v.decode('utf-8') for v in data]\n",
    "                else:\n",
    "                    return str(data)\n",
    "            else:\n",
    "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
    "        except KeyError:\n",
    "            return f\"Path not found: {hdf_path}\"\n",
    "\n",
    "try:\n",
    "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
    "    print(\"Compute Messages:\")\n",
    "    \n",
    "    # Parse and print the compute messages in a more visually friendly way\n",
    "    messages = results_summary_string[0].split('\\r\\n')\n",
    "    \n",
    "    for message in messages:\n",
    "        if message.strip():  # Skip empty lines\n",
    "            if ':' in message:\n",
    "                key, value = message.split(':', 1)\n",
    "                print(f\"{key.strip():40} : {value.strip()}\")\n",
    "            else:\n",
    "                print(f\"\\n{message.strip()}\")\n",
    "    \n",
    "    # Print computation summary in a table format\n",
    "    print(\"\\nComputation Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Task' in line:\n",
    "            task, time = line.split('\\t')\n",
    "            print(f\"{task:<30} {time:<20}\")\n",
    "    \n",
    "    print(\"\\nComputation Speed:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Speed' in line:\n",
    "            task, speed = line.split('\\t')\n",
    "            print(f\"{task:<30} {speed:<20}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting compute messages: {str(e)}\")\n",
    "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 12: Extract Plan Parameters and Volume Accounting\n",
    "print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n",
    "\n",
    "# Extract plan parameters\n",
    "plan_parameters_df = HdfPlan.get_plan_parameters(hdf_path=plan_hdf_path)\n",
    "\n",
    "# Extract volume accounting data\n",
    "volume_accounting_df = HdfResultsPlan.get_volume_accounting(hdf_path=plan_hdf_path)\n",
    "\n",
    "print(\"\\nPlan Parameters DataFrame:\")\n",
    "plan_parameters_df\n",
    "\n",
    "print(\"\\nVolume Accounting DataFrame:\")\n",
    "volume_accounting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RasPlanHdf Class Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get simulation start time\n",
    "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
    "print(f\"Simulation start time: {start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get plan end time\n",
    "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
    "print(f\"Simulation end time: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the cell below to time of max wsel for 1D models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time of maximum water surface elevation (WSEL) for cross sections\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Get cross section results timeseries\n",
    "xsec_results = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n",
    "print(\"\\nCross Section Results Shape:\", xsec_results['Water_Surface'].shape)\n",
    "\n",
    "# Get cross section geometry data\n",
    "xsec_geom = HdfXsec.get_cross_sections(plan_hdf_path)\n",
    "print(\"\\nNumber of cross sections in geometry:\", len(xsec_geom))\n",
    "\n",
    "# Create dataframe with cross section locations and max WSEL times\n",
    "xs_data = []\n",
    "\n",
    "# Extract water surface data from xarray Dataset\n",
    "water_surface = xsec_results['Water_Surface'].values\n",
    "times = pd.to_datetime(xsec_results.time.values)\n",
    "\n",
    "# Debug print\n",
    "print(\"\\nFirst few cross section names:\")\n",
    "print(xsec_results.cross_section.values[:5])\n",
    "\n",
    "# Iterate through cross sections\n",
    "for xs_idx in range(len(xsec_results.cross_section)):\n",
    "    # Get WSEL timeseries for this cross section\n",
    "    wsel_series = water_surface[:, xs_idx]\n",
    "    \n",
    "    # Get cross section name and parse components\n",
    "    xs_name = xsec_results.cross_section.values[xs_idx]\n",
    "    \n",
    "    # Split the string and remove empty strings\n",
    "    xs_parts = [part for part in xs_name.split() if part]\n",
    "    \n",
    "    if len(xs_parts) >= 3:\n",
    "        river = \"Bald Eagle\"  # Combine first two words\n",
    "        reach = \"Loc Hav\"     # Next two words\n",
    "        rs = xs_parts[-1]     # Last part is the station\n",
    "        \n",
    "        # Get geometry for this cross section\n",
    "        xs_match = xsec_geom[\n",
    "            (xsec_geom['River'] == river) & \n",
    "            (xsec_geom['Reach'] == reach) & \n",
    "            (xsec_geom['RS'] == rs)\n",
    "        ]\n",
    "        \n",
    "        if not xs_match.empty:\n",
    "            geom = xs_match.iloc[0]\n",
    "            # Use first point of cross section line for plotting\n",
    "            x = geom.geometry.coords[0][0]\n",
    "            y = geom.geometry.coords[0][1]\n",
    "            \n",
    "            # Find time of max WSEL\n",
    "            max_wsel_idx = np.argmax(wsel_series)\n",
    "            max_wsel = np.max(wsel_series)\n",
    "            max_time = times[max_wsel_idx]\n",
    "            \n",
    "            xs_data.append({\n",
    "                'xs_name': xs_name,\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'max_wsel': max_wsel,\n",
    "                'time_of_max': max_time\n",
    "            })\n",
    "        else:\n",
    "            print(f\"\\nWarning: No geometry match found for {xs_name}\")\n",
    "            print(f\"River: {river}, Reach: {reach}, RS: {rs}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Could not parse cross section name: {xs_name}\")\n",
    "\n",
    "# Create dataframe\n",
    "xs_df = pd.DataFrame(xs_data)\n",
    "\n",
    "# Debug print\n",
    "print(\"\\nNumber of cross sections processed:\", len(xs_df))\n",
    "if not xs_df.empty:\n",
    "    print(\"\\nColumns in xs_df:\", xs_df.columns.tolist())\n",
    "    print(\"\\nFirst row of xs_df:\")\n",
    "    print(xs_df.iloc[0])\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Convert datetime to hours since start for colormap\n",
    "    min_time = min(xs_df['time_of_max'])\n",
    "    color_values = [(t - min_time).total_seconds() / 3600 for t in xs_df['time_of_max']]\n",
    "\n",
    "    # Plot cross section points\n",
    "    scatter = ax.scatter(xs_df['x'], xs_df['y'],\n",
    "                        c=color_values,\n",
    "                        cmap='viridis',\n",
    "                        s=50)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title('Time of Maximum Water Surface Elevation at Cross Sections')\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Hours since simulation start')\n",
    "\n",
    "    # Format colorbar ticks\n",
    "    max_hours = int(max(color_values))\n",
    "    tick_interval = max(1, max_hours // 6)  # Show ~6 ticks\n",
    "    cbar.set_ticks(range(0, max_hours + 1, tick_interval))\n",
    "    cbar.set_ticklabels([f'{h}h' for h in range(0, max_hours + 1, tick_interval)])\n",
    "\n",
    "    # Add grid and adjust styling\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    max_wsel_xs = xs_df.loc[xs_df['max_wsel'].idxmax()]\n",
    "    hours_since_start = (max_wsel_xs['time_of_max'] - min_time).total_seconds() / 3600\n",
    "\n",
    "    print(f\"\\nOverall Maximum WSEL: {max_wsel_xs['max_wsel']:.2f} ft\")\n",
    "    print(f\"Time of Overall Maximum WSEL: {max_wsel_xs['time_of_max']}\")\n",
    "    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
    "    print(f\"Location of Overall Maximum WSEL: X={max_wsel_xs['x']:.2f}, Y={max_wsel_xs['y']:.2f}\")\n",
    "    print(f\"Cross Section: {max_wsel_xs['xs_name']}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No cross sections were processed successfully\")\n",
    "    print(\"xs_data length:\", len(xs_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to add this to the ras-commander library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unsteady attributes\n",
    "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path)\n",
    "print(\"\\nResults Unsteady Attributes:\")\n",
    "for key, value in results_unsteady_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unsteady summary attributes\n",
    "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path)\n",
    "print(\"\\nResults Unsteady Summary Attributes:\")\n",
    "for key, value in results_unsteady_summary_attrs.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get volume accounting attributes\n",
    "volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path)\n",
    "print(\"\\nVolume Accounting Attributes:\")\n",
    "for key, value in volume_accounting_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== HDF5 File Structure ===\\n\")\n",
    "print(plan_hdf_path)\n",
    "HdfBase.get_dataset_info(plan_hdf_path, group_path='/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsec_results = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n",
    "print(xsec_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time series for specific cross section\n",
    "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
    "\n",
    "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
    "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(xsec_results[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
    "\n",
    "# Create time series plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure for each variable\n",
    "variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
    "\n",
    "for var in variables:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Convert time values to datetime if needed\n",
    "    time_values = pd.to_datetime(xsec_results.time.values)\n",
    "    values = xsec_results[var].sel(cross_section=target_xs).values\n",
    "    \n",
    "    # Plot with explicit x and y values\n",
    "    plt.plot(time_values, values, '-', linewidth=2)\n",
    "    \n",
    "    plt.title(f'{var} at {target_xs}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(var.replace('_', ' '))\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Force display\n",
    "    plt.draw()\n",
    "    plt.pause(0.1)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\11_2d_hdf_data_extraction.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS 2D HDF Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and Run Plan 06\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "    # Initialize the RAS project using the default global ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"06\"\n",
    "\n",
    "    # Update run flags for the project\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,\n",
    "        unsteady_flow_simulation=True,\n",
    "        run_sediment=False,\n",
    "        post_processor=True,\n",
    "        floodplain_mapping=False\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the default global ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "ras.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "ras.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDF input path as Plan Number\n",
    "\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasHdfUtils\n",
    "| Method Name | Description |\n",
    "|-------------|-------------|\n",
    "| get_attrs | Converts attributes from a HEC-RAS HDF file into a Python dictionary for a given attribute path |\n",
    "| get_root_attrs | Returns attributes at root level of HEC-RAS HDF file |\n",
    "| get_hdf_paths_with_properties | Gets all paths in the HDF file with their properties |\n",
    "| get_group_attributes_as_df | Gets attributes of a group in the HDF file as a DataFrame |\n",
    "| get_hdf_filename | Gets the HDF filename from various input types |\n",
    "| get_runtime_data | Extracts runtime and compute time data from a single HDF file |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDF Paths with Properties (For Exploring HDF Files)\n",
    "HdfBase.get_dataset_info(plan_number, group_path=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number)\n",
    "if runtime_df is not None:\n",
    "    runtime_df\n",
    "else:\n",
    "    print(\"No runtime data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runtime_df example output: \n",
    "\n",
    "| Plan Name                        | File Name                     | Simulation Start Time | Simulation End Time | Simulation Duration (s) | Simulation Time (hr) | Completing Geometry (hr) | Preprocessing Geometry (hr) | Completing Event Conditions (hr) | Unsteady Flow Computations (hr) | Complete Process (hr) | Unsteady Flow Speed (hr/hr) | Complete Process Speed (hr/hr) |\n",
    "|----------------------------------|-------------------------------|-----------------------|---------------------|-------------------------|-----------------------|--------------------------|------------------------------|----------------------------------|----------------------------------|-----------------------|------------------------------|----------------------------------|\n",
    "| Gridded Precip - Infiltration    | BaldEagleDamBrk.p06.hdf      | 09Sep2018 00:00:00    | 14Sep2018 00:00:00  | 432000.0                | 120.0                 | N/A                      | 0.000113                     | N/A                              | 0.074436                        | 0.080951              | 1612.126776                  | 1482.386368                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of all the functions in the RasGeomHdf class from the ras_commander/RasGeomHdf.py file:\n",
    "\n",
    "| Function Name | Description |\n",
    "|---------------|-------------|\n",
    "| projection | Returns the projection of the RAS geometry as a pyproj.CRS object |\n",
    "| get_geom_attrs | Returns base geometry attributes from a HEC-RAS HDF file |\n",
    "\n",
    "| mesh_area_names | Returns a list of the 2D mesh area names of the RAS geometry |\n",
    "| get_geom_2d_flow_area_attrs | Returns geometry 2d flow area attributes from a HEC-RAS HDF file |\n",
    "| mesh_areas | Returns 2D flow area perimeter polygons |\n",
    "| mesh_cell_polygons | Returns 2D flow mesh cell polygons |\n",
    "| mesh_cell_points | Returns 2D flow mesh cell points |\n",
    "| mesh_cell_faces | Returns 2D flow mesh cell faces |\n",
    "\n",
    "| get_geom_structures_attrs | Returns geometry structures attributes from a HEC-RAS HDF file |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| bc_lines | Returns 2D mesh area boundary condition lines |\n",
    "| breaklines | Returns 2D mesh area breaklines |\n",
    "\n",
    "\n",
    "\n",
    "| refinement_regions | Returns 2D mesh area refinement regions |\n",
    "| structures | Returns the model structures |\n",
    "| reference_lines_names | Returns reference line names |\n",
    "| reference_points_names | Returns reference point names |\n",
    "| reference_lines | Returns the reference lines geometry and attributes |\n",
    "| reference_points | Returns the reference points geometry and attributes |\n",
    "| cross_sections | Returns the model 1D cross sections |\n",
    "| river_reaches | Returns the model 1D river reach lines |\n",
    "| cross_sections_elevations | Returns the model cross section elevation information |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
    "print(geom_hdf_path)\n",
    "\n",
    "# For the example project, plan 06 is associated with geometry 09\n",
    "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
    "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Geometry Information\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
    "geom_attrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geom_attrs output: \n",
    "\n",
    "| Complete Geometry | Extents | Geometry Time | Infiltration Date Last Modified | Infiltration File Date | Infiltration Filename | Infiltration Layername | Land Cover Date Last Modified | Land Cover File Date | Land Cover Filename | ... | Percent Impervious Date Last Modified | Percent Impervious File Date | Percent Impervious Filename | Percent Impervious Layername | SI Units | Terrain File Date | Terrain Filename | Terrain Layername | Title | Version |\n",
    "|-------------------|---------|---------------|---------------------------------|-----------------------|----------------------|------------------------|------------------------------|----------------------|---------------------|-----|--------------------------------------|-----------------------------|----------------------------|------------------------------|----------|-------------------|------------------|-------------------|-------|---------|\n",
    "| 0                 | True    | [1960041.35636708, 2092643.59732271, 285497.89...] | 27Oct2024 20:09:19 | 11MAR2022 13:52:44 | 24NOV2020 13:24:58 | .\\Soils Data\\Infiltration.hdf | Infiltration | 11MAR2022 13:45:08 | 11MAR2022 13:45:08 | .\\Land Classification\\LandCover.hdf | ... | 11MAR2022 13:45:08 | 11MAR2022 13:45:08 | .\\Land Classification\\LandCover.hdf | LandCover | False | 09FEB2015 08:26:58 | .\\Terrain\\Terrain50.hdf | Terrain50 | Single 2D Area - Internal Dam Structure | 1.0.20 (20Sep2024) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfMesh for geometry-related operations\n",
    "print(\"\\nExample 3: Listing 2D Flow Area Names\")\n",
    "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
    "print(\"2D Flow Area Name (returned as list):\\n\", flow_area_names)\n",
    "# Note: this is returned as a list because it is used internally by other functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Attributes (get_mesh_area_attributes)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n",
    "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n",
    "flow_area_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flow_area_df:\n",
    "\n",
    "Value\n",
    "| Name                        | b'BaldEagleCr' |\n",
    "|-----------------------------|-----------------|\n",
    "| Locked                      | 0               |\n",
    "| Mann                        | 0.04            |\n",
    "| Multiple Face Mann n       | 0               |\n",
    "| Composite LC               | 0               |\n",
    "| Cell Vol Tol               | 0.01            |\n",
    "| Cell Min Area Fraction      | 0.01            |\n",
    "| Face Profile Tol           | 0.01            |\n",
    "| Face Area Tol              | 0.01            |\n",
    "| Face Conv Ratio            | 0.02            |\n",
    "| Laminar Depth              | 0.2             |\n",
    "| Min Face Length Ratio      | 0.05            |\n",
    "| Spacing dx                 | 250.0           |\n",
    "| Spacing dy                 | 250.0           |\n",
    "| Shift dx                   | NaN             |\n",
    "| Shift dy                   | NaN             |\n",
    "| Cell Count                 | 18066           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Perimeter Polygons (get_mesh_areas)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
    "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
    "\n",
    "# Plot the 2D Flow Area Perimeter Polygons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "\n",
    "# Add labels for each polygon\n",
    "for idx, row in mesh_areas.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    # Check if 'Name' column exists, otherwise use a default label\n",
    "    label = row.get('Name', f'Area {idx}')\n",
    "    ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "\n",
    "plt.title('2D Flow Area Perimeter Polygons')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract mesh cell faces\n",
    "print(\"\\nExample: Extracting mesh cell faces\")\n",
    "\n",
    "# Get mesh cell faces\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces DataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mesh_cell_faces geodataframe:\n",
    "\n",
    "flow_area_df:\n",
    "\n",
    "| mesh_name    | face_id | geometry                                           |\n",
    "|--------------|---------|----------------------------------------------------|\n",
    "| BaldEagleCr  | 0       | LINESTRING (2042125 351625, 2042375 351625)      |\n",
    "| BaldEagleCr  | 1       | LINESTRING (2042375 351625, 2042375 351875)      |\n",
    "| BaldEagleCr  | 2       | LINESTRING (2042375 351875, 2042125 351875)      |\n",
    "| BaldEagleCr  | 3       | LINESTRING (2042125 351875, 2042125 351625)      |\n",
    "| BaldEagleCr  | 4       | LINESTRING (2042375 351375, 2042375 351625)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "import numpy as np\n",
    "\n",
    "# Plot the mesh cell faces more efficiently\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Convert all geometries to numpy arrays at once for faster plotting\n",
    "lines = [list(zip(*line.xy)) for line in mesh_cell_faces.geometry]\n",
    "lines_collection = LineCollection(lines, colors='blue', linewidth=0.5, alpha=0.5)\n",
    "ax.add_collection(lines_collection)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Mesh Cell Faces')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "\n",
    "# Calculate centroids once and store as numpy arrays\n",
    "centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in mesh_cell_faces.geometry])\n",
    "\n",
    "# Create scatter plot with numpy arrays\n",
    "scatter = ax.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1], \n",
    "    c=mesh_cell_faces['face_id'],\n",
    "    cmap='viridis',\n",
    "    s=1,\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.colorbar(scatter, label='Face ID')\n",
    "\n",
    "# Set axis limits based on data bounds\n",
    "ax.set_xlim(centroids[:, 0].min(), centroids[:, 0].max())\n",
    "ax.set_ylim(centroids[:, 1].min(), centroids[:, 1].max())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display some statistics\n",
    "print(\"\\nMesh Cell Faces Statistics:\")\n",
    "print(f\"Total number of cell faces: {len(mesh_cell_faces)}\")\n",
    "print(f\"Number of unique meshes: {mesh_cell_faces['mesh_name'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the nearest cell face to a given point\n",
    "def find_nearest_cell_face(point, cell_faces_df):\n",
    "    \"\"\"\n",
    "    Find the nearest cell face to a given point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.geometry.Point): The input point.\n",
    "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
    "\n",
    "    Returns:\n",
    "        int: The face_id of the nearest cell face.\n",
    "        float: The distance to the nearest cell face.\n",
    "    \"\"\"\n",
    "    # Calculate distances from the input point to all cell faces\n",
    "    distances = cell_faces_df.geometry.distance(point)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    nearest_index = distances.idxmin()\n",
    "\n",
    "    # Get the face_id and distance of the nearest cell face\n",
    "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
    "    nearest_distance = distances[nearest_index]\n",
    "\n",
    "    return nearest_face_id, nearest_distance\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample: Finding the nearest cell face to a given point\")\n",
    "\n",
    "# Create a sample point (you can replace this with any point of interest)\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# Get the projection from the geometry file\n",
    "# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n",
    "if projection:\n",
    "    print(f\"Using projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found. Using default CRS.\")\n",
    "    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n",
    "\n",
    "# Create the sample point with the correct CRS\n",
    "sample_point = GeoDataFrame({'geometry': [Point(2042250, 351750)]}, crs=projection)\n",
    "\n",
    "if not mesh_cell_faces.empty and not sample_point.empty:\n",
    "    # Ensure the CRS of the sample point matches the mesh_cell_faces\n",
    "    if sample_point.crs != mesh_cell_faces.crs:\n",
    "        sample_point = sample_point.to_crs(mesh_cell_faces.crs)\n",
    "    \n",
    "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n",
    "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
    "    print(f\"Face ID: {nearest_face_id}\")\n",
    "    print(f\"Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Visualize the result\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all cell faces\n",
    "    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
    "    \n",
    "    # Plot the sample point\n",
    "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
    "    \n",
    "    # Plot the nearest cell face\n",
    "    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n",
    "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Nearest Cell Face to Sample Point')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Unable to perform nearest cell face search due to missing data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Cell Polygons\n",
    "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
    "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell_polygons_df:\n",
    "\n",
    "| mesh_name    | cell_id | geometry                                      |\n",
    "|--------------|---------|-----------------------------------------------|\n",
    "| BaldEagleCr  | 0       | POLYGON ((2082875 370625, 2082723.922 370776.0... |\n",
    "| BaldEagleCr  | 1       | POLYGON ((2083125 370625, 2083125 370844.185, ... |\n",
    "| BaldEagleCr  | 2       | POLYGON ((2083375 370625, 2083375 370886.638, ... |\n",
    "| BaldEagleCr  | 3       | POLYGON ((2083625 370625, 2083625 370925.693, ... |\n",
    "| BaldEagleCr  | 4       | POLYGON ((2083875 370625, 2083875 370958.588, ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cell Polygons\n",
    "if not cell_polygons_df.empty:\n",
    "    cell_polygons_df.head()\n",
    "else:\n",
    "    print(\"No Cell Polygons found.\")\n",
    "\n",
    "# Plot cell polygons\n",
    "if not cell_polygons_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot cell polygons\n",
    "    cell_polygons_df.plot(ax=ax, edgecolor='blue', facecolor='none')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('2D Flow Area Cell Polygons')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cell polygon data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Cell Info\n",
    "print(\"\\nExample 5: Extracting Cell Info\")\n",
    "cell_info_df = HdfMesh.get_mesh_cell_points(geom_hdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell_info_df: \n",
    "\n",
    "| mesh_name    | cell_id | geometry                          |\n",
    "|--------------|---------|-----------------------------------|\n",
    "| BaldEagleCr  | 0       | POINT (2083000 370750)           |\n",
    "| BaldEagleCr  | 1       | POINT (2083250 370750)           |\n",
    "| BaldEagleCr  | 2       | POINT (2083500 370750)           |\n",
    "| BaldEagleCr  | 3       | POINT (2083750 370750)           |\n",
    "| BaldEagleCr  | 4       | POINT (2084000 370750)           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cell Info\n",
    "if not cell_info_df.empty:\n",
    "    cell_info_df.head()\n",
    "else:\n",
    "    print(\"No Cell Info found.\")\n",
    "\n",
    "# Plot cell centers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not cell_info_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot cell centers\n",
    "    cell_info_df.plot(ax=ax, color='red', markersize=5)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('2D Flow Area Cell Centers')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cell data available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the nearest cell center to a given point\n",
    "def find_nearest_cell(point, cell_centers_df):\n",
    "    \"\"\"\n",
    "    Find the nearest cell center to a given point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.geometry.Point): The input point.\n",
    "        cell_centers_df (GeoDataFrame): DataFrame containing cell center points.\n",
    "\n",
    "    Returns:\n",
    "        int: The cell_id of the nearest cell.\n",
    "        float: The distance to the nearest cell center.\n",
    "    \"\"\"\n",
    "    # Calculate distances from the input point to all cell centers\n",
    "    distances = cell_centers_df.geometry.distance(point)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    nearest_index = distances.idxmin()\n",
    "\n",
    "    # Get the cell_id and distance of the nearest cell\n",
    "    nearest_cell_id = cell_centers_df.loc[nearest_index, 'cell_id']\n",
    "    nearest_distance = distances[nearest_index]\n",
    "\n",
    "    return nearest_cell_id, nearest_distance\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample: Finding the nearest cell to a given point\")\n",
    "\n",
    "# Create a sample point (you can replace this with any point of interest)\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# Get the projection from the geometry file\n",
    "# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n",
    "if projection:\n",
    "    print(f\"Using projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found. Using default CRS.\")\n",
    "    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n",
    "\n",
    "# Create the sample point with the correct CRS\n",
    "sample_point = GeoDataFrame({'geometry': [Point(2083500, 370800)]}, crs=projection)\n",
    "\n",
    "if not cell_info_df.empty and not sample_point.empty:\n",
    "    # Ensure the CRS of the sample point matches the cell_info_df\n",
    "    if sample_point.crs != cell_info_df.crs:\n",
    "        sample_point = sample_point.to_crs(cell_info_df.crs)\n",
    "    \n",
    "    nearest_cell_id, distance = find_nearest_cell(sample_point.geometry.iloc[0], cell_info_df)\n",
    "    print(f\"Nearest cell to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
    "    print(f\"Cell ID: {nearest_cell_id}\")\n",
    "    print(f\"Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Visualize the result\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all cell centers\n",
    "    cell_info_df.plot(ax=ax, color='blue', markersize=5, alpha=0.5, label='Cell Centers')\n",
    "    \n",
    "    # Plot the sample point\n",
    "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
    "    \n",
    "    # Plot the nearest cell center\n",
    "    nearest_cell = cell_info_df[cell_info_df['cell_id'] == nearest_cell_id]\n",
    "    nearest_cell.plot(ax=ax, color='green', markersize=100, alpha=0.7, label='Nearest Cell')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Nearest Cell to Sample Point')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Unable to perform nearest cell search due to missing data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geometry structures attributes\n",
    "print(\"\\nGetting geometry structures attributes\")\n",
    "geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path)\n",
    "if geom_structures_attrs:\n",
    "    print(\"Geometry structures attributes:\")\n",
    "    for key, value in geom_structures_attrs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No geometry structures attributes found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paths and Functions for each type of structure: \n",
    "\n",
    "# Getting geometry structures attributes\n",
    "# Geometry structures attributes:\n",
    "# Bridge/Culvert Count: 0\n",
    "# Connection Count: 4\n",
    "# Has Bridge Opening (2D): 0\n",
    "# Inline Structure Count: 0\n",
    "# Lateral Structure Count: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Boundary Condition Lines and Plot with 2D Flow Area Perimeter Polygons\n",
    "print(\"\\nExample 7: Extracting Boundary Condition Lines and Plotting with 2D Flow Area Perimeter Polygons\")\n",
    "bc_lines_df = HdfBndry.get_bc_lines(geom_hdf_path)\n",
    "\n",
    "if not bc_lines_df.empty:\n",
    "    bc_lines_df.head()\n",
    "else:\n",
    "    print(\"No Boundary Condition Lines found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| bc_line_id |         name         |    mesh_name    |    type    |                                           geometry                                            |\n",
    "|-------------|----------------------|------------------|------------|------------------------------------------------------------------------------------------------|\n",
    "|      0      |     DSNormalDepth    |   BaldEagleCr    |  External  | LINESTRING (2082004.235 364024.82, 2083193.546...)                                          |\n",
    "|      1      |       DS2NormalD     |   BaldEagleCr    |  External  | LINESTRING (2084425.804 365392.892, 2084354.64...)                                          |\n",
    "|      2      |   Upstream Inflow    |   BaldEagleCr    |  External  | LINESTRING (1967473.737 290973.629, 1969582.89...)                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Boundary Condition Lines with Perimeter\n",
    "# Plot if data exists\n",
    "if not bc_lines_df.empty or not mesh_areas.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 2D Flow Area Perimeter Polygons\n",
    "    if not mesh_areas.empty:\n",
    "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
    "        \n",
    "        # Add labels for each polygon\n",
    "        for idx, row in mesh_areas.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            label = row.get('Name', f'Area {idx}')\n",
    "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "    \n",
    "    # Plot boundary condition lines\n",
    "    if not bc_lines_df.empty:\n",
    "        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Breaklines and Plot with 2D Flow Area Perimeter Polygons\n",
    "print(\"\\nExample 8: Extracting Breaklines and Plotting with 2D Flow Area Perimeter Polygons\")\n",
    "breaklines_gdf = HdfBndry.get_breaklines(geom_hdf_path)\n",
    "if not breaklines_gdf.empty:\n",
    "    breaklines_gdf\n",
    "else:\n",
    "    print(\"No Breaklines found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breaklines_gdf:\n",
    "\n",
    "\n",
    "| bl_id | Name      | geometry |\n",
    "|-------|-----------|----------|\n",
    "| 0     | SayersDam | LINESTRING (2002361.246 323707.927, 2002741.35...) |\n",
    "| 1     | Lower     | LINESTRING (2060356.422 351786.819, 2060316.47...) |\n",
    "| 2     | Middle    | LINESTRING (2052757.788 348470.547, 2052785.84...) |\n",
    "| 3     | Upper     | LINESTRING (2045597.199 348412.994, 2045638.91...) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot breaklines and 2D Flow Area Perimeter Polygons if they exist\n",
    "if not breaklines_gdf.empty or not mesh_areas.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 2D Flow Area Perimeter Polygons\n",
    "    if not mesh_areas.empty:\n",
    "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
    "        \n",
    "        # Add labels for each polygon\n",
    "        for idx, row in mesh_areas.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            label = row.get('Name', f'Area {idx}')\n",
    "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "    \n",
    "    # Plot breaklines\n",
    "    if not breaklines_gdf.empty:\n",
    "        breaklines_gdf.plot(ax=ax, color='blue', linewidth=2, label='Breaklines')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    ax.set_title('2D Flow Area Perimeter Polygons and Breaklines')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get structures\n",
    "structures_gdf = HdfStruc.get_structures(geom_hdf_path)\n",
    "print(\"Structures:\")\n",
    "if not structures_gdf.empty:\n",
    "    structures_gdf.head()\n",
    "else:\n",
    "    print(\"No structures found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structures_gdf: \n",
    "\n",
    "| Type | Mode | River | Reach | RS | Connection | Groupname | US Type | US River | US Reach | ... | US XS Mann (Count) | US BR Mann (Index) | US BR Mann (Count) | DS XS Mann (Index) | DS XS Mann (Count) | DS BR Mann (Index) | DS BR Mann (Count) | RC (Index) | RC (Count) | Profile_Data |\n",
    "|------|------|-------|-------|-------|------------|-----------|----------|-----------|-----------|-----|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|------------|------------|--------------|\n",
    "| Connection | Weir/Gate/Culverts | | | | Sayers Dam | BaldEagleCr, Sayers Dam | 2D | | | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | [{'Station': 0.0, 'Elevation': 683.0}, {'Stati... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get boundary condition lines\n",
    "bc_lines_gdf = HdfBndry.get_bc_lines(geom_hdf_path)\n",
    "print(\"\\nBoundary Condition Lines:\")\n",
    "\n",
    "bc_lines_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get reference points\n",
    "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)\n",
    "print(\"\\nReference Points:\")\n",
    "ref_points_gdf\n",
    "\n",
    "# There are no reference points in this example project (for demonstration only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Refinement Regions\n",
    "print(\"\\nExample: Extracting Refinement Regions\")\n",
    "\n",
    "# Make sure to pass the bald_eagle object as the ras_object parameter\n",
    "refinement_regions_df = HdfBndry.get_refinement_regions(geom_hdf_path)\n",
    "\n",
    "if not refinement_regions_df.empty:\n",
    "    print(\"Refinement Regions DataFrame:\")\n",
    "    display(refinement_regions_df.head())\n",
    "    \n",
    "    # Plot refinement regions\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    refinement_regions_df.plot(ax=ax, column='CellSize', legend=True, \n",
    "                               legend_kwds={'label': 'Cell Size', 'orientation': 'horizontal'},\n",
    "                               cmap='viridis')\n",
    "    ax.set_title('2D Mesh Area Refinement Regions')\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No refinement regions found in the geometry file.\")\n",
    "\n",
    "# Example: Analyze Refinement Regions\n",
    "if not refinement_regions_df.empty:\n",
    "    print(\"\\nRefinement Regions Analysis:\")\n",
    "    print(f\"Total number of refinement regions: {len(refinement_regions_df)}\")\n",
    "    print(\"\\nCell Size Statistics:\")\n",
    "    print(refinement_regions_df['CellSize'].describe())\n",
    "    \n",
    "    # Group by Shape Type\n",
    "    shape_type_counts = refinement_regions_df['ShapeType'].value_counts()\n",
    "    print(\"\\nRefinement Region Shape Types:\")\n",
    "    print(shape_type_counts)\n",
    "    \n",
    "    # Plot Shape Type distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shape_type_counts.plot(kind='bar')\n",
    "    plt.title('Distribution of Refinement Region Shape Types')\n",
    "    plt.xlabel('Shape Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Compute Messages as String\n",
    "print(\"Extracting Compute Messages\")\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract string from HDF object at a given path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_hdf_filename : str\n",
    "        Name of the HDF file\n",
    "    hdf_path : str\n",
    "        Path of the object in the HDF file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Extracted string from the specified HDF object\n",
    "    \"\"\"\n",
    "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
    "        try:\n",
    "            hdf_object = hdf_file[hdf_path]\n",
    "            if isinstance(hdf_object, h5py.Group):\n",
    "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
    "            elif isinstance(hdf_object, h5py.Dataset):\n",
    "                data = hdf_object[()]\n",
    "                if isinstance(data, bytes):\n",
    "                    return data.decode('utf-8')\n",
    "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
    "                    return [v.decode('utf-8') for v in data]\n",
    "                else:\n",
    "                    return str(data)\n",
    "            else:\n",
    "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
    "        except KeyError:\n",
    "            return f\"Path not found: {hdf_path}\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
    "    print(\"Compute Messages:\")\n",
    "    \n",
    "    # Parse and print the compute messages in a more visually friendly way\n",
    "    messages = results_summary_string[0].split('\\r\\n')\n",
    "    \n",
    "    for message in messages:\n",
    "        if message.strip():  # Skip empty lines\n",
    "            if ':' in message:\n",
    "                key, value = message.split(':', 1)\n",
    "                print(f\"{key.strip():40} : {value.strip()}\")\n",
    "            else:\n",
    "                print(f\"\\n{message.strip()}\")\n",
    "    \n",
    "    # Print computation summary in a table format\n",
    "    print(\"\\nComputation Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Task' in line:\n",
    "            task, time = line.split('\\t')\n",
    "            print(f\"{task:<30} {time:<20}\")\n",
    "    \n",
    "    print(\"\\nComputation Speed:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Speed' in line:\n",
    "            task, speed = line.split('\\t')\n",
    "            print(f\"{task:<30} {speed:<20}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting compute messages: {str(e)}\")\n",
    "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Plan 06 does not have any errors, so this dataframe will be empty (for demonstration purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Compute Messages Example \n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def parse_2d_compute_messages(compute_messages):\n",
    "    \"\"\"\n",
    "    Parse 2D compute messages to extract data lines, clean the data, \n",
    "    and retrieve top 20 cells with the highest error.\n",
    "\n",
    "    Parameters:\n",
    "        compute_messages (list or str): The raw compute messages.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the parsed compute messages string and the main DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both list and string inputs\n",
    "        if isinstance(compute_messages, list):\n",
    "            compute_messages = '\\n'.join(compute_messages)\n",
    "        elif not isinstance(compute_messages, str):\n",
    "            logging.error(f\"Unexpected type for compute_messages: {type(compute_messages)}\")\n",
    "            return \"\", pd.DataFrame()\n",
    "\n",
    "        # Split the message into lines\n",
    "        lines = compute_messages.split('\\n')\n",
    "        logging.info(\"Successfully split compute messages into lines.\")\n",
    "        \n",
    "        # Initialize lists to store parsed data\n",
    "        data_lines = []\n",
    "        print(\"Data lines:\", data_lines)\n",
    "        header_lines = []\n",
    "        print(\"Header lines:\", header_lines) \n",
    "        footer_lines = []\n",
    "        print(\"Footer lines:\", footer_lines)\n",
    "\n",
    "        \n",
    "        # More flexible timestamp pattern that includes various message types\n",
    "        timestamp_pattern = re.compile(r'^\\d{2}[A-Z]{3}\\d{4}\\s+\\d{2}:\\d{2}:\\d{2}')\n",
    "        logging.debug(\"Compiled timestamp regular expression.\")\n",
    "        \n",
    "        data_started = False\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            if timestamp_pattern.match(stripped_line):\n",
    "                data_started = True\n",
    "                # Split the line and add to data_lines\n",
    "                parts = stripped_line.split()\n",
    "                if len(parts) >= 8:  # Ensure we have all expected columns\n",
    "                    # Combine Date and Time into 'Date and Time'\n",
    "                    date_time = f\"{parts[0]} {parts[1]}\"\n",
    "                    location = parts[2]\n",
    "                    cell_type = f\"{parts[3]} {parts[4]}\"\n",
    "                    cell_number = parts[5]\n",
    "                    wsel = parts[6]\n",
    "                    error = parts[7]\n",
    "                    iterations = parts[8] if len(parts) > 8 else None\n",
    "                    data_lines.append([date_time, location, cell_type, cell_number, wsel, error, iterations])\n",
    "                    logging.debug(f\"Parsed data line: {data_lines[-1]}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Line skipped due to insufficient parts: {stripped_line}\")\n",
    "            elif not data_started:\n",
    "                header_lines.append(stripped_line)\n",
    "            elif data_started and not stripped_line:\n",
    "                data_started = False\n",
    "            elif not data_started:\n",
    "                footer_lines.append(stripped_line)\n",
    "        \n",
    "        # Create DataFrame from data lines\n",
    "        df = pd.DataFrame(\n",
    "            data_lines, \n",
    "            columns=['Date and Time', 'Location', 'Cell Type', 'Cell Number', 'WSEL', 'ERROR', 'ITERATIONS']\n",
    "        )\n",
    "        logging.info(\"Created DataFrame from parsed data lines.\")\n",
    "        \n",
    "        # Clean and convert columns to appropriate types\n",
    "        df['Cell Number'] = (\n",
    "            pd.to_numeric(df['Cell Number'].replace('#', pd.NA), errors='coerce')\n",
    "            .fillna(-1)\n",
    "            .astype('Int64')\n",
    "        )\n",
    "        df['WSEL'] = pd.to_numeric(df['WSEL'], errors='coerce')\n",
    "        df['ERROR'] = pd.to_numeric(df['ERROR'], errors='coerce')\n",
    "        df['ITERATIONS'] = pd.to_numeric(df['ITERATIONS'], errors='coerce').astype('Int64')\n",
    "        logging.info(\"Converted DataFrame columns to appropriate types.\")\n",
    "        \n",
    "        # Get top 20 cells with highest error\n",
    "        top_20_cells = (\n",
    "            df.sort_values('ERROR', ascending=False)\n",
    "            .drop_duplicates('Cell Number')\n",
    "            .head(20)\n",
    "        )\n",
    "        \n",
    "        # Construct the reordered message\n",
    "        reordered_message = '\\n'.join(header_lines + \n",
    "                                      ['\\nTop 20 Cells with Highest Error:'] + \n",
    "                                      [' '.join(map(str, row)) for row in top_20_cells.values] + \n",
    "                                      ['\\n'] + footer_lines)\n",
    "        \n",
    "        logging.info(\"Reordered compute messages.\")\n",
    "        \n",
    "        return reordered_message, df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing compute messages: {e}\")\n",
    "        return \"\", pd.DataFrame()\n",
    "\n",
    "# Use the function to parse compute messages\n",
    "parsed_messages, df = parse_2d_compute_messages(results_summary_string)\n",
    "\n",
    "print(parsed_messages)\n",
    "print(df)\n",
    "\n",
    "# Get top 20 cells with highest error\n",
    "if not df.empty and 'ERROR' in df.columns:\n",
    "    top_20_cells = (\n",
    "        df.sort_values('ERROR', ascending=False)\n",
    "        .drop_duplicates('Cell Number')\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    logging.warning(\"Unable to get top 20 cells with highest error. DataFrame is empty or 'ERROR' column is missing.\")\n",
    "    top_20_cells = pd.DataFrame()\n",
    "\n",
    "# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
    "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
    "\n",
    "print(\"\\n2D Flow Area Groups and Perimeters:\")\n",
    "if not mesh_areas.empty:\n",
    "    print(\"Available columns:\", mesh_areas.columns.tolist())\n",
    "    \n",
    "    # Display the first few rows of the mesh_areas DataFrame\n",
    "    print(\"\\nFirst few rows of mesh_areas DataFrame:\")\n",
    "    mesh_areas.head()\n",
    "else:\n",
    "    print(\"No 2D Flow Area groups found in the HDF file.\")\n",
    "\n",
    "# Use the previously extracted cell_polygons_df\n",
    "print(\"\\nTop 20 Cell Polygons:\")\n",
    "if 'cell_polygons_df' in locals() and not cell_polygons_df.empty and not top_20_cells.empty:\n",
    "    # Get the cell numbers from top_20_cells\n",
    "    top_20_cell_numbers = top_20_cells['Cell Number'].tolist()\n",
    "    \n",
    "    # Filter cell_polygons_df to only include top 20 cells\n",
    "    top_20_cell_polygons = cell_polygons_df[cell_polygons_df['cell_id'].isin(top_20_cell_numbers)]\n",
    "    \n",
    "    display(top_20_cell_polygons)\n",
    "\n",
    "    # Plot top 20 cell polygons and mesh areas\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot mesh areas\n",
    "    mesh_areas.plot(ax=ax, edgecolor='red', facecolor='none', alpha=0.5, label='Mesh Areas')\n",
    "    \n",
    "    # Plot top 20 cell polygons\n",
    "    top_20_cell_polygons.plot(ax=ax, edgecolor='blue', facecolor='none', alpha=0.7, label='Top 20 Error Cells')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('2D Flow Area Perimeters and Top 20 Cell Polygons')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Cell Polygons found or no top 20 cells with highest error available.\")\n",
    "    print(\"Unable to plot cell polygons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Example for Debugging or New Features: List all paths, groups, and attributes under \"/Results/Unsteady/Summary/Volume Accounting\"\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Summary/Volume Accounting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 12: Extract Plan Parameters and Volume Accounting\n",
    "print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n",
    "\n",
    "# Extract plan parameters\n",
    "plan_parameters_df = HdfPlan.get_plan_parameters(plan_hdf_path)\n",
    "\n",
    "# Extract volume accounting data\n",
    "volume_accounting_df = HdfResultsPlan.get_volume_accounting(plan_hdf_path)\n",
    "\n",
    "print(\"\\nPlan Parameters DataFrame:\")\n",
    "plan_parameters_df\n",
    "\n",
    "print(\"\\nVolume Accounting DataFrame:\")\n",
    "volume_accounting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RasPlanHdf Class Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get plan start time\n",
    "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
    "print(f\"Simulation start time: {start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation start time: 2018-09-09 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get plan end time\n",
    "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
    "print(f\"Simulation end time: {end_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation end time: 2018-09-14 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get maximum iteration count for mesh cells\n",
    "max_iter_df = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)\n",
    "print(\"\\nMesh Max Iterations:\")\n",
    "print(max_iter_df.attrs)\n",
    "max_iter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_iter_df:\n",
    "\n",
    "| mesh_name | cell_id | cell_last_iteration | geometry |\n",
    "|-----------|---------|--------------------| ---------|\n",
    "| BaldEagleCr | 0 | 0 | POINT (2083000 370750) |\n",
    "| BaldEagleCr | 1 | 0 | POINT (2083250 370750) |\n",
    "| BaldEagleCr | 2 | 0 | POINT (2083500 370750) |\n",
    "| BaldEagleCr | 3 | 2 | POINT (2083750 370750) |\n",
    "| BaldEagleCr | 4 | 0 | POINT (2084000 370750) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum iteration count for mesh cells\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "\n",
    "max_iter_gdf = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)\n",
    "\n",
    "print(\"max_iter_df\")\n",
    "print(max_iter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mesh_max_iter_df:\n",
    "\n",
    "| mesh_name | cell_id | cell_last_iteration | geometry |\n",
    "|-----------|---------|--------------------| ---------|\n",
    "| BaldEagleCr | 0 | 0 | POINT (2083000 370750) |\n",
    "| ... | ... | ... | ... |\n",
    "| BaldEagleCr | 19592 | 0 | POINT (1978423.032 300718.897) |\n",
    "\n",
    "\n",
    "[19597 rows x 4 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cell coordinates \n",
    "cell_coords = HdfMesh.get_mesh_cell_points(plan_hdf_path)\n",
    "cell_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mesh Max Iterations\n",
    "\n",
    "# Extract x and y coordinates from the geometry column\n",
    "max_iter_df['x'] = max_iter_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
    "max_iter_df['y'] = max_iter_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
    "\n",
    "# Remove rows with None coordinates\n",
    "max_iter_df = max_iter_df.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_iter_df['x'], max_iter_df['y'], \n",
    "                     c=max_iter_df['cell_last_iteration'], \n",
    "                     cmap='viridis', \n",
    "                     s=1)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Iterations per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Iterations')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "max_iter_df[['mesh_name', 'cell_id', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh maximum water surface elevation\n",
    "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)\n",
    "print(\"\\nMesh Maximum Water Surface Elevation:\")\n",
    "print(max_ws_df.attrs)\n",
    "max_ws_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_ws_df:\n",
    "\n",
    "| mesh_name | cell_id | maximum_water_surface | maximum_water_surface_time | geometry |\n",
    "|-----------|---------|---------------------|--------------------------|-----------|\n",
    "| BaldEagleCr | 0 | 704.054443 | 2018-09-10 18:00:00 | POINT (2083000 370750) |\n",
    "| BaldEagleCr | 1 | 692.377991 | 2018-09-10 18:04:00 | POINT (2083250 370750) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the max water surface as a map\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "\n",
    "# Extract x and y coordinates from the geometry column\n",
    "max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
    "max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
    "\n",
    "# Remove rows with None coordinates\n",
    "max_ws_df = max_ws_df.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], \n",
    "                     c=max_ws_df['maximum_water_surface'], \n",
    "                     cmap='viridis', \n",
    "                     s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Water Surface per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Water Surface (ft)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "max_ws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time of the max water surface elevation (WSEL)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert the 'maximum_water_surface_time' to datetime objects\n",
    "max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Convert datetime to hours since the start for colormap\n",
    "min_time = max_ws_df['max_wsel_time'].min()\n",
    "color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], \n",
    "                     c=color_values, \n",
    "                     cmap='viridis', \n",
    "                     s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Time of Maximum Water Surface Elevation per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "# Set up the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Hours since simulation start')\n",
    "\n",
    "# Format the colorbar ticks to show hours\n",
    "cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))  # Set ticks every 6 hours\n",
    "cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Find the overall maximum WSEL and its time\n",
    "max_wsel_row = max_ws_df.loc[max_ws_df['maximum_water_surface'].idxmax()]\n",
    "hours_since_start = (max_wsel_row['max_wsel_time'] - min_time).total_seconds() / 3600\n",
    "print(f\"\\nOverall Maximum WSEL: {max_wsel_row['maximum_water_surface']:.2f} ft\")\n",
    "print(f\"Time of Overall Maximum WSEL: {max_wsel_row['max_wsel_time']}\")\n",
    "print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
    "print(f\"Location of Overall Maximum WSEL: X={max_wsel_row['x']}, Y={max_wsel_row['y']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh minimum water surface elevation\n",
    "min_ws_df = HdfResultsMesh.get_mesh_min_ws(plan_hdf_path)\n",
    "print(\"\\nMesh Minimum Water Surface Elevation:\")\n",
    "min_ws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh maximum face velocity\n",
    "max_face_v_df = HdfResultsMesh.get_mesh_max_face_v(plan_hdf_path)\n",
    "print(\"\\nMesh Max Face Velocity:\")\n",
    "max_face_v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract midpoint coordinates from the LineString geometries\n",
    "max_face_v_df['x'] = max_face_v_df['geometry'].apply(lambda geom: geom.centroid.x)\n",
    "max_face_v_df['y'] = max_face_v_df['geometry'].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_face_v_df['x'], max_face_v_df['y'], \n",
    "                    c=max_face_v_df['maximum_face_velocity'].abs(),\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Face Velocity per Face')\n",
    "ax.set_xlabel('X Coordinate') \n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Face Velocity (ft/s)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the face velocity dataframe:\")\n",
    "max_face_v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh minimum face velocity\n",
    "min_face_v_df = HdfResultsMesh.get_mesh_min_face_v(plan_hdf_path)\n",
    "print(\"\\nMesh Min Face Velocity:\")\n",
    "min_face_v_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh max water surface error\n",
    "\n",
    "max_ws_err_df = HdfResultsMesh.get_mesh_max_ws_err(plan_hdf_path)\n",
    "print(\"\\nMesh Max Water Surface Error:\")\n",
    "max_ws_err_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max water surface error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract x and y coordinates from the geometry points, handling None values\n",
    "max_ws_err_df['x'] = max_ws_err_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
    "max_ws_err_df['y'] = max_ws_err_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
    "\n",
    "# Remove any rows with None coordinates\n",
    "max_ws_err_df = max_ws_err_df.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_ws_err_df['x'], max_ws_err_df['y'],\n",
    "                    c=max_ws_err_df['cell_maximum_water_surface_error'],\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Water Surface Error per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Water Surface Error (ft)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the water surface error dataframe:\")\n",
    "max_ws_err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to add this to the ras-commander library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n",
    "max_courant_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\")\n",
    "print(\"\\nMesh Summary Output (Maximum Courant):\")\n",
    "print(max_courant_df.attrs)\n",
    "max_courant_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max Courant number\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get mesh max Courant number\n",
    "max_courant_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\")\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "max_courant_gdf = gpd.GeoDataFrame(max_courant_df)\n",
    "\n",
    "# Get centroids of line geometries for plotting\n",
    "max_courant_gdf['centroid'] = max_courant_gdf.geometry.centroid\n",
    "max_courant_gdf['x'] = max_courant_gdf.centroid.x\n",
    "max_courant_gdf['y'] = max_courant_gdf.centroid.y\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_courant_gdf['x'], max_courant_gdf['y'],\n",
    "                    c=max_courant_gdf['maximum_face_courant'],\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Courant Number per Face')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Courant Number')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the Courant number dataframe:\")\n",
    "max_courant_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n",
    "\n",
    "max_face_shear_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\")\n",
    "print(\"\\nMesh Summary Output (Maximum Face Shear Stress:\")\n",
    "print(max_face_shear_df.attrs)\n",
    "max_face_shear_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max face shear stress\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import Point, LineString\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get mesh max face shear stress\n",
    "max_shear_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\")\n",
    "\n",
    "# Calculate centroids of the line geometries and extract coordinates\n",
    "max_shear_df['centroid'] = max_shear_df['geometry'].apply(lambda line: line.centroid)\n",
    "max_shear_df['x'] = max_shear_df['centroid'].apply(lambda point: point.x)\n",
    "max_shear_df['y'] = max_shear_df['centroid'].apply(lambda point: point.y)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_shear_df['x'], max_shear_df['y'],\n",
    "                    c=max_shear_df['maximum_face_shear_stress'],\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Face Shear Stress per Face')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Face Shear Stress (PSF)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the shear stress dataframe:\")\n",
    "max_shear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh summary output for Minimum Water Surface\n",
    "summary_df_min_ws = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Water Surface\")\n",
    "print(\"\\nMesh Summary Output (Minimum Water Surface):\")\n",
    "summary_df_min_ws\n",
    "\n",
    "# Example: Get mesh summary output for Minimum Face Velocity\n",
    "summary_df_min_fv = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Face Velocity\")\n",
    "print(\"\\nMesh Summary Output (Minimum Face Velocity):\")\n",
    "summary_df_min_fv\n",
    "\n",
    "# Example: Get mesh summary output for Cell Cumulative Iteration\n",
    "summary_df_cum_iter = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Cell Cumulative Iteration\")\n",
    "print(\"\\nMesh Summary Output (Cell Cumulative Iteration):\")\n",
    "summary_df_cum_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh timeseries output\n",
    "\n",
    "# Get mesh areas from previous code cell\n",
    "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
    "\n",
    "if mesh_areas:\n",
    "    mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n",
    "    timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\")\n",
    "    print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n",
    "    print(timeseries_da)\n",
    "else:\n",
    "    print(\"No mesh areas found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Output Variables for Cells\n",
    "# \n",
    "# Variable Name: Description\n",
    "# Water Surface: Water surface elevation\n",
    "# Depth: Water depth\n",
    "# Velocity: Magnitude of velocity\n",
    "# Velocity X: X-component of velocity\n",
    "# Velocity Y: Y-component of velocity\n",
    "# Froude Number: Froude number\n",
    "# Courant Number: Courant number\n",
    "# Shear Stress: Shear stress on the bed\n",
    "# Bed Elevation: Elevation of the bed\n",
    "# Precipitation Rate: Rate of precipitation\n",
    "# Infiltration Rate: Rate of infiltration\n",
    "# Evaporation Rate: Rate of evaporation\n",
    "# Percolation Rate: Rate of percolation\n",
    "# Groundwater Elevation: Elevation of groundwater\n",
    "# Groundwater Depth: Depth to groundwater\n",
    "# Groundwater Flow: Groundwater flow rate\n",
    "# Groundwater Velocity: Magnitude of groundwater velocity\n",
    "# Groundwater Velocity X: X-component of groundwater velocity\n",
    "# Groundwater Velocity Y: Y-component of groundwater velocity\n",
    "# \n",
    "# These variables are available for time series output at the cell level in 2D flow areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh cells timeseries output\n",
    "cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name)\n",
    "print(\"\\nMesh Cells Timeseries Output:\")\n",
    "print(cells_timeseries_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WSE Time Series Data (Random Cell ID)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Extract Water Surface data\n",
    "water_surface = cells_timeseries_ds['BaldEagleCr']['Water Surface']\n",
    "\n",
    "# Get the time values\n",
    "time_values = water_surface.coords['time'].values\n",
    "\n",
    "# Pick a random cell_id\n",
    "random_cell_id = random.choice(water_surface.coords['cell_id'].values)\n",
    "\n",
    "# Extract the water surface elevation time series for the random cell\n",
    "wsel_timeseries = water_surface.sel(cell_id=random_cell_id)\n",
    "\n",
    "# Find the peak value and its index\n",
    "peak_value = wsel_timeseries.max().item()\n",
    "peak_index = wsel_timeseries.argmax().item()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_values, wsel_timeseries, label=f'Cell ID: {random_cell_id}')\n",
    "plt.scatter(time_values[peak_index], peak_value, color='red', s=100, zorder=5)\n",
    "plt.annotate(f'Peak: {peak_value:.2f} ft', \n",
    "             (time_values[peak_index], peak_value),\n",
    "             xytext=(10, 10), textcoords='offset points',\n",
    "             ha='left', va='bottom',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title(f'Water Surface Elevation Time Series for Random Cell (ID: {random_cell_id})')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Water Surface Elevation (ft)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Log the plotting action\n",
    "logging.info(f\"Plotted water surface elevation time series for random cell ID: {random_cell_id}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Statistics for Cell ID {random_cell_id}:\")\n",
    "print(f\"Minimum WSEL: {wsel_timeseries.min().item():.2f} ft\")\n",
    "print(f\"Maximum WSEL: {peak_value:.2f} ft\")\n",
    "print(f\"Mean WSEL: {wsel_timeseries.mean().item():.2f} ft\")\n",
    "print(f\"Time of peak: {time_values[peak_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh faces timeseries output\n",
    "faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name)\n",
    "print(\"\\nMesh Faces Timeseries Output:\")\n",
    "print(faces_timeseries_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Face Results and Label Peak, Plus Map View\n",
    "\n",
    "# Step 1: Import necessary libraries \n",
    "# In notebook cell at top of notebook\n",
    "\n",
    "# Step 2: Select a random valid face ID number\n",
    "random_face = np.random.randint(0, faces_timeseries_ds.sizes['face_id'])\n",
    "\n",
    "# Step 3: Extract time series data for the selected face\n",
    "variable = 'face_velocity'  # We could also use 'face_flow'\n",
    "face_data = faces_timeseries_ds[variable].sel(face_id=random_face)\n",
    "\n",
    "# Step 4: Find peak value and its corresponding time\n",
    "peak_value = face_data.max().item()\n",
    "peak_time = face_data.idxmax().values\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(faces_timeseries_ds.time, face_data)\n",
    "plt.title(f'{variable.capitalize()} Time Series for Face {random_face}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(f'{variable.capitalize()} ({faces_timeseries_ds.attrs[\"units\"]})')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate the peak point\n",
    "plt.annotate(f'Peak: ({peak_time}, {peak_value:.2f})', \n",
    "            (peak_time, peak_value),\n",
    "            xytext=(10, 10), textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "# Check for negative values and label the minimum if present\n",
    "min_value = face_data.min().item()\n",
    "if min_value < 0:\n",
    "    min_time = face_data.idxmin().values\n",
    "    plt.annotate(f'Min: ({min_time}, {min_value:.2f})', \n",
    "                (min_time, min_value),\n",
    "                xytext=(10, -10), textcoords='offset points',\n",
    "                arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create map view plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Get mesh faces for map view\n",
    "mesh_faces = HdfMesh.get_mesh_cell_faces(plan_hdf_path)\n",
    "\n",
    "# Calculate mesh faces extents with 10% buffer\n",
    "faces_bounds = mesh_faces.total_bounds\n",
    "x_min, y_min, x_max, y_max = faces_bounds\n",
    "buffer_x = (x_max - x_min) * 0.1\n",
    "buffer_y = (y_max - y_min) * 0.1\n",
    "plot_xlim = [x_min - buffer_x, x_max + buffer_x]\n",
    "plot_ylim = [y_min - buffer_y, y_max + buffer_y]\n",
    "\n",
    "# Set plot limits before adding terrain\n",
    "ax.set_xlim(plot_xlim)\n",
    "ax.set_ylim(plot_ylim)\n",
    "\n",
    "# Add the terrain TIFF to the map, clipped to our desired extent\n",
    "tiff_path = Path.cwd() / 'example_projects' / 'BaldEagleCrkMulti2D' / 'Terrain' / 'Terrain50.baldeagledem.tif'\n",
    "with rasterio.open(tiff_path) as src:\n",
    "    show(src, ax=ax, cmap='terrain', alpha=0.5)\n",
    "    \n",
    "# Reset the limits after terrain plot\n",
    "ax.set_xlim(plot_xlim)\n",
    "ax.set_ylim(plot_ylim)\n",
    "\n",
    "# Plot all faces in gray\n",
    "mesh_faces.plot(ax=ax, color='lightgray', alpha=0.5, zorder=2)\n",
    "\n",
    "# Get the selected face geometry\n",
    "selected_face = mesh_faces[mesh_faces['face_id'] == random_face]\n",
    "\n",
    "# Highlight the selected face in red\n",
    "selected_face.plot(\n",
    "    ax=ax, \n",
    "    color='red',\n",
    "    linewidth=2,\n",
    "    label=f'Selected Face (ID: {random_face})',\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Get bounds of selected face for zoomed inset\n",
    "bounds = selected_face.geometry.bounds.iloc[0]\n",
    "x_center = (bounds.iloc[0] + bounds.iloc[2]) / 2\n",
    "y_center = (bounds.iloc[1] + bounds.iloc[3]) / 2\n",
    "buffer = max(bounds.iloc[2] - bounds.iloc[0], bounds.iloc[3] - bounds.iloc[1]) * 2\n",
    "\n",
    "# Create zoomed inset with a larger size, inside the map frame\n",
    "axins = inset_axes(ax, width=\"70%\", height=\"70%\", loc='lower right',\n",
    "                  bbox_to_anchor=(0.65, 0.05, 0.35, 0.35),\n",
    "                  bbox_transform=ax.transAxes)\n",
    "\n",
    "# Plot terrain and faces in inset\n",
    "with rasterio.open(tiff_path) as src:\n",
    "    show(src, ax=axins, cmap='terrain', alpha=0.5)\n",
    "    \n",
    "# Plot zoomed view in inset\n",
    "mesh_faces.plot(ax=axins, color='lightgray', alpha=0.5, zorder=2)\n",
    "selected_face.plot(ax=axins, color='red', linewidth=2, zorder=3)\n",
    "\n",
    "# Set inset limits with slightly more context\n",
    "axins.set_xlim(x_center - buffer/1.5, x_center + buffer/1.5)\n",
    "axins.set_ylim(y_center - buffer/1.5, y_center + buffer/1.5)\n",
    "\n",
    "# Remove inset ticks for cleaner look\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "\n",
    "# Add a border to the inset\n",
    "for spine in axins.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "# Create connection lines between main plot and inset\n",
    "# Get the selected face centroid for connection point\n",
    "centroid = selected_face.geometry.centroid.iloc[0]\n",
    "con1 = ConnectionPatch(\n",
    "    xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
    "    xyB=(0.02, 0.98), coordsB=axins.transAxes,\n",
    "    arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
    ")\n",
    "con2 = ConnectionPatch(\n",
    "    xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
    "    xyB=(0.98, 0.02), coordsB=axins.transAxes,\n",
    "    arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
    ")\n",
    "\n",
    "ax.add_artist(con1)\n",
    "ax.add_artist(con2)\n",
    "\n",
    "# Add title and legend to main plot\n",
    "ax.set_title('Mesh Face Map View with Terrain')\n",
    "ax.legend()\n",
    "\n",
    "# Ensure equal aspect ratio while maintaining our desired extents\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary information\n",
    "print(f\"Random Face: {random_face}\")\n",
    "print(f\"Peak Value: {peak_value:.2f} {faces_timeseries_ds.attrs['units']} at {peak_time}\")\n",
    "if min_value < 0:\n",
    "    print(f\"Minimum Value: {min_value:.2f} {faces_timeseries_ds.attrs['units']} at {min_time}\")\n",
    "\n",
    "# Log the plotting action\n",
    "logging.info(f\"Plotted mesh face time series and map view for random face ID: {random_face} with terrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meteorology precipitation attributes\n",
    "meteo_precip_attrs = HdfPlan.get_plan_met_precip(plan_hdf_path)\n",
    "print(\"\\nMeteorology Precipitation Attributes:\")\n",
    "for key, value in meteo_precip_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results unsteady attributes\n",
    "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path)\n",
    "print(\"\\nResults Unsteady Attributes:\")\n",
    "for key, value in results_unsteady_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results unsteady summary attributes\n",
    "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path)\n",
    "print(\"\\nResults Unsteady Summary Attributes:\")\n",
    "for key, value in results_unsteady_summary_attrs.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get results volume accounting attributes\n",
    "volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path)\n",
    "print(\"\\nVolume Accounting Attributes:\")\n",
    "for key, value in volume_accounting_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\12_2d_hdf_data_extraction pipes and pumps.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS Pipes, Conduits, and Pump Stations HDF Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to manipulate and analyze the new HEC-RAS Conduits, Pipes, and Pump Stations results using the ras-commander library. It leverages the HdfPipe and HdfPump classes to streamline data extraction, processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Pipes Beta project from HEC and run plan 01\n",
    "\n",
    "# Define the path to the Pipes Beta project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
    "import logging\n",
    "\n",
    "# Check if Pipes Beta.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = pipes_ex_path / \"DavisStormSystem.p02.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the Pipes Beta project\n",
    "    RasExamples.extract_project([\"Davis\"])\n",
    "\n",
    "    # Initialize the RAS project using the ras. (Pipe Networks are only supported in versions 6.6 and above)\n",
    "    init_ras_project(pipes_ex_path, \"6.6\")\n",
    "    logging.info(f\"Pipes Beta project initialized with folder: {ras.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Pipes Beta object id: {id(ras)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"02\"\n",
    "\n",
    "    # Update run flags for the project\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,\n",
    "        unsteady_flow_simulation=True,\n",
    "        run_sediment=False,\n",
    "        post_processor=True,\n",
    "        floodplain_mapping=False\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Pipes Beta\n",
    "    print(f\"Executing Plan {plan_number} for the Pipes Beta Creek project...\")\n",
    "    success_pipes_ex = RasCmdr.compute_plan(plan_number)\n",
    "    if success_pipes_ex:\n",
    "        print(f\"Plan {plan_number} executed successfully for Pipes Beta.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Pipes Beta.\\n\")\n",
    "else:\n",
    "    print(\"Pipes Beta.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the ras.\n",
    "    init_ras_project(pipes_ex_path, \"6.6\")\n",
    "    plan_number = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for pipes_ex project\n",
    "print(\"Plan DataFrame for pipes_ex project:\")\n",
    "ras.plan_df\n",
    "\n",
    "# Display geom_df for pipes_ex project\n",
    "print(\"\\nGeometry DataFrame for pipes_ex project:\")\n",
    "ras.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number)\n",
    "runtime_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pipe Conduits/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pipe conduits\n",
    "pipe_conduits_gdf = HdfPipe.get_pipe_conduits(\"02\") # NOTE: Here we use the plan number instead of the path variable.  The library decorators ensure this maps correctly.  \n",
    "print(\"\\nPipe Conduits: pipe_conduits_gdf\")\n",
    "pipe_conduits_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_conduits_gdf:  \n",
    "\n",
    "| Name | System Name | US Node | DS Node | Modeling Approach | Conduit Length | Max Cell Length | Shape | Rise | Span | ... | Slope | US Entrance Loss Coefficient | DS Exit Loss Coefficient | US Backflow Loss Coefficient | DS Backflow Loss Coefficient | DS Flap Gate | Major Group | Minor Group | Polyline | Terrain_Profiles |\n",
    "|------|-------------|---------|---------|-------------------|----------------|------------------|-------|------|------|-----|-------|------------------------------|--------------------------|------------------------------|------------------------------|--------------|-------------|-------------|----------|-------------------|\n",
    "| 0    | 134         | Davis   | O13-DMH007 | O13-DMH006 | hydraulic        | 443.740020      | 40.0             | circular | 6.0  | 6.0  | ... | 0.002723 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 40.819695), (21.217846, 40.642994), (35... |\n",
    "| 1    | 133         | Davis   | O13-DMH024 | O13-DMH009 | hydraulic        | 800.000024      | 40.0             | circular | 6.0  | 6.0  | ... | 0.001904 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 40.530186), (21.1467, 40.44057), (50.88... |\n",
    "| 2    | 132         | Davis   | O13-DMH006 | O13-SDS03 | hydraulic        | 443.740070      | 40.0             | circular | 6.0  | 6.0  | ... | 0.002816 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 41.700996), (26.817467, 41.552666), (83... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install adjusttext #No longer required - optional to help with labels overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pipe conduit linestrings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure with a specified size\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Plot each linestring from the GeoDataFrame\n",
    "for idx, row in pipe_conduits_gdf.iterrows():\n",
    "    # Extract coordinates from the linestring\n",
    "    x_coords, y_coords = row['Polyline'].xy\n",
    "    \n",
    "    # Plot the linestring\n",
    "    plt.plot(x_coords, y_coords, 'b-', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Add vertical line markers at endpoints\n",
    "    plt.plot([x_coords[0]], [y_coords[0]], 'x', color='black', markersize=4)\n",
    "    plt.plot([x_coords[-1]], [y_coords[-1]], 'x', color='black', markersize=4)\n",
    "    \n",
    "    # Calculate center point of the line\n",
    "    center_x = (x_coords[0] + x_coords[-1]) / 2\n",
    "    center_y = (y_coords[0] + y_coords[-1]) / 2\n",
    "    \n",
    "    # Add pipe name label at center, oriented top-right\n",
    "    plt.text(center_x, center_y, f'{row[\"Name\"]}', fontsize=8, \n",
    "             verticalalignment='bottom', horizontalalignment='left',\n",
    "             rotation=45)  # 45 degree angle for top-right orientation\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Pipe Conduit Network Layout')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 terrain profiles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract terrain profiles from the GeoDataFrame\n",
    "terrain_profiles = pipe_conduits_gdf['Terrain_Profiles'].tolist()\n",
    "\n",
    "# Create separate plots for the first 2 terrain profiles\n",
    "for i in range(2):\n",
    "    profile = terrain_profiles[i]\n",
    "    \n",
    "    # Unzip the profile into x and y coordinates\n",
    "    x_coords, y_coords = zip(*profile)\n",
    "    \n",
    "    # Create a new figure for each profile\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='g', alpha=0.7)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f'Terrain Profile {i + 1}')\n",
    "    plt.xlabel('Distance along profile (m)')\n",
    "    plt.ylabel('Elevation (m)')\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Adjust layout to prevent label clipping\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Nodes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pipe nodes\n",
    "pipe_nodes_gdf = HdfPipe.get_pipe_nodes(plan_hdf_path)\n",
    "print(\"\\nPipe Nodes:\")\n",
    "pipe_nodes_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_nodes_gdf:\n",
    "\n",
    "\n",
    "| Name         | System Name | Node Type | Node Status                     | Condtui Connections (US:DS) | Invert Elevation | Base Area | Terrain Elevation | Terrain Elevation Override | Depth     | Drop Inlet Elevation | Drop Inlet Weir Length | Drop Inlet Weir Coefficient | Drop Inlet Orifice Area | Drop Inlet Orifice Coefficient | Total Connection Count | geometry                             |\n",
    "|--------------|-------------|-----------|----------------------------------|------------------------------|------------------|-----------|-------------------|---------------------------|-----------|----------------------|-------------------------|-----------------------------|-------------------------|-------------------------------|------------------------|-------------------------------------|\n",
    "| O14-di027   | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 36.060001        | 36.0     | 39.860001         | NaN                       | 3.799999  | 39.863369           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6637926.81 1964917.32)     |\n",
    "| P11-DMH004  | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 38.169998        | 36.0     | 48.720001         | NaN                       | 10.550003 | 48.718811           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6629444.634 1963504.411)   |\n",
    "| O14-DMH005  | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 31.559999        | 36.0     | 40.840000         | NaN                       | 9.280001  | 40.843731           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6637368.497 1966084.574)   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Networks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pipe network data\n",
    "pipe_network_gdf = HdfPipe.get_pipe_network(plan_hdf_path)\n",
    "print(\"\\nPipe Network Data:\")\n",
    "pipe_network_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_network_gdf:\n",
    "| Cell_ID | Conduit_ID | Node_ID | Minimum_Elevation | DS_Face_Indices | Face_Indices | US_Face_Indices | Cell_Property_Info_Index | US Face Elevation | DS Face Elevation | Min Elevation | Area | Info Index | Cell_Polygon | Face_Polylines | Node_Point |\n",
    "|---------|------------|---------|-------------------|------------------|--------------|------------------|--------------------------|-------------------|-------------------|---------------|------|------------|---------------|----------------|------------|\n",
    "| 0       | 0          | 0       | -1                | 26.824432        | [1]          | [0, 1]           | [0]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |\n",
    "| 1       | 1          | 0       | -1                | 26.714573        | [2]          | [1, 2]           | [1]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |\n",
    "| 2       | 2          | 0       | -1                | 26.604715        | [3]          | [2, 3]           | [2]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pump stations\n",
    "pump_stations_gdf = HdfPump.get_pump_stations(plan_hdf_path)\n",
    "print(\"\\nPump Stations:\")\n",
    "pump_stations_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pump Stations:\n",
    "| geometry                          | station_id | Name             | Inlet River | Inlet Reach | Inlet RS | Inlet RS Distance | Inlet SA/2D | Inlet Pipe Node | Outlet River | ... | Outlet Pipe Node | Reference River | Reference Reach | Reference RS | Reference RS Distance | Reference SA/2D | Reference Point | Reference Pipe Node | Highest Pump Line Elevation | Pump Groups |\n",
    "|-----------------------------------|------------|------------------|-------------|-------------|----------|-------------------|--------------|------------------|--------------|-----|------------------|------------------|-----------------|--------------|-----------------------|------------------|-----------------|----------------------|------------------------------|-------------|\n",
    "| POINT (6635027.027 1966080.07)   | 0          | Pump Station #1   |             |             | NaN      |                   |              | Davis [O13-SDS03] |              | ... |                  | NaN              |                 | NaN          |                       | NaN              |                 |                      | NaN                          | 1           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pump groups\n",
    "pump_groups_df = HdfPump.get_pump_groups(plan_hdf_path)\n",
    "print(\"\\nPump Groups:\")\n",
    "pump_groups_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pump Groups:\n",
    "| Pump Station ID | Name             | Bias On | Start Up Time | Shut Down Time | Width | Pumps | efficiency_curve_start | efficiency_curve_count | efficiency_curve |\n",
    "|------------------|------------------|---------|----------------|----------------|-------|-------|------------------------|-----------------------|------------------|\n",
    "| 0                | Pump Station #1   | 0       | 5.0            | NaN            | 5.0   | 1     | 0                      | 6                     | [[2.0, 70.0], [4.0, 60.0], [6.0, 55.0], [8.0, ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "print(f\"Projection: {projection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CRS for GeoDataFrames\n",
    "if projection:\n",
    "    pipe_conduits_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
    "    pipe_nodes_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
    "\n",
    "print(\"Pipe Conduits GeoDataFrame columns:\")\n",
    "print(pipe_conduits_gdf.columns)\n",
    "\n",
    "print(\"\\nPipe Nodes GeoDataFrame columns:\")\n",
    "print(pipe_nodes_gdf.columns)\n",
    "\n",
    "perimeter_polygons = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
    "if projection:\n",
    "    perimeter_polygons.set_crs(projection, inplace=True, allow_override=True)\n",
    "    \n",
    "print(\"\\nPerimeter Polygons GeoDataFrame columns:\")\n",
    "print(perimeter_polygons.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(28, 20))\n",
    "\n",
    "# Plot cell polygons with 50% transparency behind the pipe network\n",
    "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)\n",
    "if not cell_polygons_df.empty:\n",
    "    cell_polygons_df.plot(ax=ax, edgecolor='lightgray', facecolor='lightgray', alpha=0.5)\n",
    "\n",
    "# Plot pipe conduits - the Polyline column already contains LineString geometries\n",
    "pipe_conduits_gdf.set_geometry('Polyline', inplace=True)\n",
    "\n",
    "# Plot each pipe conduit individually to ensure all are shown\n",
    "for idx, row in pipe_conduits_gdf.iterrows():\n",
    "    ax.plot(*row.Polyline.xy, color='blue', linewidth=1)\n",
    "\n",
    "# Create a colormap for node elevations\n",
    "norm = plt.Normalize(pipe_nodes_gdf['Invert Elevation'].min(), \n",
    "                    pipe_nodes_gdf['Invert Elevation'].max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# Plot pipe nodes colored by invert elevation\n",
    "scatter = ax.scatter(pipe_nodes_gdf.geometry.x, pipe_nodes_gdf.geometry.y,\n",
    "                    c=pipe_nodes_gdf['Invert Elevation'], \n",
    "                    cmap=cmap, norm=norm,\n",
    "                    s=100)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Invert Elevation (ft)', rotation=270, labelpad=15)\n",
    "\n",
    "# Add combined labels for invert and drop inlet elevations\n",
    "for idx, row in pipe_nodes_gdf.iterrows():\n",
    "    label_text = \"\"  # Initialize label_text for each node\n",
    "    # Add drop inlet elevation label if it exists and is not NaN\n",
    "    if 'Drop Inlet Elevation' in row and not np.isnan(row['Drop Inlet Elevation']):\n",
    "        label_text += f\"TOC: {row['Drop Inlet Elevation']:.2f}\\n\"\n",
    "    label_text += f\"INV: {row['Invert Elevation']:.2f}\"\n",
    "    \n",
    "    ax.annotate(label_text,\n",
    "                xy=(row.geometry.x, row.geometry.y),\n",
    "                xytext=(-10, -10), textcoords='offset points',\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Add perimeter polygons \n",
    "if not perimeter_polygons.empty:\n",
    "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "\n",
    "# Create proxy artists for legend\n",
    "conduit_line = mlines.Line2D([], [], color='blue', label='Conduits')\n",
    "node_point = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                          markersize=10, label='Nodes')\n",
    "perimeter = mpatches.Patch(facecolor='none', edgecolor='black',\n",
    "                          label='Perimeter Polygons')\n",
    "\n",
    "ax.set_title('Pipe Network with Node Elevations')\n",
    "\n",
    "# Add legend with proxy artists\n",
    "ax.legend(handles=[conduit_line, node_point, perimeter])\n",
    "\n",
    "# Set aspect ratio to be equal and adjust limits\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pump stations on a map\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "pump_stations_gdf.plot(ax=ax, color='green', markersize=50, label='Pump Station')\n",
    "\n",
    "# Add perimeter polygons\n",
    "if not perimeter_polygons.empty:\n",
    "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none', label='Perimeter Polygons')\n",
    "\n",
    "ax.set_title('Pump Station Location')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Get pipe network timeseries\n",
    "valid_variables = [\n",
    "    \"Cell Courant\", \"Cell Water Surface\", \"Face Flow\", \"Face Velocity\",\n",
    "    \"Face Water Surface\", \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\",\n",
    "    \"Pipes/Vel DS\", \"Pipes/Vel US\", \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\",\n",
    "    \"Nodes/Water Surface\"\n",
    "]\n",
    "\n",
    "print(\"Valid variables for pipe network timeseries:\")\n",
    "for var in valid_variables:\n",
    "    print(f\"- {var}\")\n",
    "\n",
    "# Extract pipe network timeseries for each valid pipe-related variable\n",
    "pipe_variables = [var for var in valid_variables if var.startswith(\"Pipes/\") or var.startswith(\"Nodes/\")]\n",
    "\n",
    "for variable in pipe_variables:\n",
    "    try:\n",
    "        pipe_timeseries = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
    "        print(f\"\\nPipe Network Timeseries ({variable}):\")\n",
    "        print(pipe_timeseries.head())  # Print first few rows to avoid overwhelming output\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {variable}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe Network Timeseries Data Description\n",
    "\n",
    "The `get_pipe_network_timeseries` function returns an xarray DataArray for each variable. Here's a general description of the data structure:\n",
    "\n",
    "1. **Pipes/Pipe Flow DS and Pipes/Pipe Flow US**:\n",
    "   - Dimensions: time, location (pipe IDs)\n",
    "   - Units: ft^3/s (cubic feet per second)\n",
    "   - Description: Represents the flow rate at the downstream (DS) and upstream (US) ends of pipes over time.\n",
    "\n",
    "2. **Pipes/Vel DS and Pipes/Vel US**:\n",
    "   - Dimensions: time, location (pipe IDs)\n",
    "   - Units: ft/s (feet per second)\n",
    "   - Description: Shows the velocity at the downstream (DS) and upstream (US) ends of pipes over time.\n",
    "\n",
    "3. **Nodes/Depth**:\n",
    "   - Dimensions: time, location (node IDs)\n",
    "   - Units: ft (feet)\n",
    "   - Description: Indicates the depth of water at each node over time.\n",
    "\n",
    "4. **Nodes/Drop Inlet Flow**:\n",
    "   - Dimensions: time, location (node IDs)\n",
    "   - Units: cfs (cubic feet per second)\n",
    "   - Description: Represents the flow rate through drop inlets at each node over time.\n",
    "\n",
    "5. **Nodes/Water Surface**:\n",
    "   - Dimensions: time, location (node IDs)\n",
    "   - Units: ft (feet)\n",
    "   - Description: Shows the water surface elevation at each node over time.\n",
    "\n",
    "General notes:\n",
    "- The 'time' dimension represents the simulation timesteps.\n",
    "- The 'location' dimension represents either pipe IDs or node IDs, depending on the variable.\n",
    "- The number of timesteps and locations may vary depending on the specific dataset and simulation setup.\n",
    "- Negative values in flow variables may indicate reverse flow direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the variables we want to plot\n",
    "variables = [\n",
    "    \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\", \"Pipes/Vel DS\", \"Pipes/Vel US\",\n",
    "    \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\", \"Nodes/Water Surface\"\n",
    "]\n",
    "\n",
    "# Create a separate plot for each variable\n",
    "for variable in variables:\n",
    "    try:\n",
    "        # Get the data for the current variable\n",
    "        data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
    "        \n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Pick one random location\n",
    "        random_location = random.choice(data.location.values)\n",
    "        \n",
    "        # Determine if it's a pipe or node variable\n",
    "        if variable.startswith(\"Pipes/\"):\n",
    "            location_type = \"Conduit ID\"\n",
    "        else:\n",
    "            location_type = \"Node ID\"\n",
    "        \n",
    "        # Plot the data for the randomly selected location\n",
    "        ax.plot(data.time, data.sel(location=random_location), label=f'{location_type} {random_location}')\n",
    "        \n",
    "        # Set the title and labels\n",
    "        ax.set_title(f'{variable} Over Time ({location_type} {random_location})')\n",
    "        ax.set_xlabel('Time')  # Corrected from ax.xlabel to ax.set_xlabel\n",
    "        ax.set_ylabel(f'{variable} ({data.attrs[\"units\"]})')  # Corrected from ax.ylabel to ax.set_ylabel\n",
    "        \n",
    "        # Format the x-axis to show dates nicely\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add a legend\n",
    "        ax.legend(title=location_type, loc='upper left')\n",
    "        \n",
    "        # Adjust the layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {variable}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Get pump station timeseries\n",
    "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
    "# Use the results_pump_station_timeseries method \n",
    "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
    "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
    "print(pump_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pump Stations/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the pump station timeseries data\n",
    "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
    "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
    "\n",
    "# Print the pump station timeseries\n",
    "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
    "print(pump_timeseries)\n",
    "\n",
    "# Create a new figure for plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Plot each variable in the timeseries\n",
    "for variable in pump_timeseries.coords['variable'].values:\n",
    "    data = pump_timeseries.sel(variable=variable)\n",
    "    \n",
    "    # Decode units to strings\n",
    "    unit = pump_timeseries.attrs[\"units\"][list(pump_timeseries.coords[\"variable\"].values).index(variable)][1].decode('utf-8')\n",
    "    \n",
    "    # Check if the variable is 'Pumps on' to plot it differently\n",
    "    if variable == 'Pumps on':\n",
    "        # Plot with color based on the on/off status\n",
    "        colors = ['green' if val > 0 else 'red' for val in data.values.flatten()]\n",
    "        ax.scatter(pump_timeseries['time'], data, label=f'{variable} ({unit})', color=colors)\n",
    "    else:\n",
    "        ax.plot(pump_timeseries['time'], data, label=f'{variable} ({unit})')\n",
    "        \n",
    "        # Label the peak values\n",
    "        peak_time = pump_timeseries['time'][data.argmax()]\n",
    "        peak_value = data.max()\n",
    "        ax.annotate(f'Peak: {peak_value:.2f}', xy=(peak_time, peak_value), \n",
    "                    xytext=(peak_time, peak_value + 0.1 * peak_value), \n",
    "                    arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                    fontsize=10, color='black', ha='center')\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title(f'Timeseries Data for Pump Station: {pump_station_name}')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Values')\n",
    "\n",
    "# Format the x-axis to show dates nicely\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(title='Variables', loc='upper left')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\13_2d_detail_face_data_extraction.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS 2D Detail Face Data Extraction Examples\n",
    "\n",
    "This notebook demonstrates how to extract detailed 2D face data, display individual cell face results and calculate a discharge weighted velocity using a user-provided profile line located where cell faces are perpendicular to flow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Chippewa_2D project from HEC and run plan 01\n",
    "\n",
    "# Define the path to the Chippewa_2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"Chippewa_2D\"\n",
    "import logging\n",
    "\n",
    "# Check if Chippewa_2D.p02.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"Chippewa_2D.p02.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the Chippewa_2D project\n",
    "    RasExamples.extract_project([\"Chippewa_2D\"])\n",
    "\n",
    "    # Initialize the RAS project using the default global ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"02\"\n",
    "\n",
    "    # Update run flags for the project\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,\n",
    "        unsteady_flow_simulation=True,\n",
    "        run_sediment=False,\n",
    "        post_processor=True,\n",
    "        floodplain_mapping=False\n",
    "    )\n",
    "\n",
    "    # Execute Plan 02 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"Chippewa_2D.p02.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the default global ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "output_dir = bald_eagle_path / \"detail_face_data_analysis\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory created/verified at: {output_dir}\")\n",
    "\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "ras.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "ras.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDF input path as Plan Number\n",
    "\n",
    "plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number)\n",
    "if runtime_df is not None:\n",
    "    runtime_df\n",
    "else:\n",
    "    print(\"No runtime data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
    "print(geom_hdf_path)\n",
    "\n",
    "# For the example project, plan 02 is associated with geometry 09\n",
    "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
    "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the  to USA Contiguous Albers Equal Area Conic (USGS version)\n",
    "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
    "projection = 'EPSG:5070'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
    "\n",
    "if not geom_attrs.empty:\n",
    "    # Display the DataFrame directly\n",
    "    print(\"Base Geometry Attributes:\")\n",
    "    geom_attrs\n",
    "else:\n",
    "    print(\"No base geometry attributes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfMesh for geometry-related operations\n",
    "print(\"\\nExample 3: Listing 2D Flow Area Names\")\n",
    "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
    "print(\"2D Flow Area Names:\", flow_area_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Attributes (get_geom_2d_flow_area_attrs)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n",
    "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n",
    "flow_area_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
    "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)  # Corrected function name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract mesh cell faces\n",
    "print(\"\\nExample: Extracting mesh cell faces\")\n",
    "\n",
    "# Get mesh cell faces using the standardize_input decorator for consistent file handling\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces GeoDataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the projection to USA Contiguous Albers Equal Area Conic (USGS version)\n",
    "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
    "projection = 'EPSG:5070'  # NAD83 / Conus Albers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Function: Find the nearest cell face to a given point\n",
    "# This provides enough basic information the face cell logic in the notebook\n",
    "\n",
    "def find_nearest_cell_face(point, cell_faces_df):\n",
    "    \"\"\"\n",
    "    Find the nearest cell face to a given point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.geometry.Point): The input point.\n",
    "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
    "\n",
    "    Returns:\n",
    "        int: The face_id of the nearest cell face.\n",
    "        float: The distance to the nearest cell face.\n",
    "    \"\"\"\n",
    "    # Calculate distances from the input point to all cell faces\n",
    "    distances = cell_faces_df.geometry.distance(point)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    nearest_index = distances.idxmin()\n",
    "\n",
    "    # Get the face_id and distance of the nearest cell face\n",
    "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
    "    nearest_distance = distances[nearest_index]\n",
    "\n",
    "    return nearest_face_id, nearest_distance\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample: Finding the nearest cell face to a given point\")\n",
    "\n",
    "# Create a sample point (you can replace this with any point of interest)\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# Create the sample point with the same CRS as mesh_cell_faces\n",
    "sample_point = GeoDataFrame(\n",
    "    {'geometry': [Point(1025677, 7853731)]}, \n",
    "    crs=mesh_cell_faces.crs\n",
    ")\n",
    "\n",
    "if not mesh_cell_faces.empty and not sample_point.empty:\n",
    "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n",
    "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
    "    print(f\"Face ID: {nearest_face_id}\")\n",
    "    print(f\"Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Visualize the result\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all cell faces\n",
    "    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
    "    \n",
    "    # Plot the sample point\n",
    "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
    "    \n",
    "    # Plot the nearest cell face\n",
    "    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n",
    "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Nearest Cell Face to Sample Point')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Unable to perform nearest cell face search due to missing data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract mesh cell faces and plot with profile lines\n",
    "print(\"\\nExample: Extracting mesh cell faces and plotting with profile lines\")\n",
    "\n",
    "# Get mesh cell faces\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces DataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces\n",
    "\n",
    "# Load the GeoJSON file for profile lines\n",
    "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
    "profile_lines_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
    "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
    "\n",
    "# Plot the mesh cell faces and profile lines together\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "mesh_cell_faces.plot(ax=ax, color='blue', alpha=0.5, edgecolor='k', label='Mesh Cell Faces')\n",
    "profile_lines_gdf.plot(ax=ax, color='orange', linewidth=2, label='Profile Lines')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Easting')\n",
    "ax.set_ylabel('Northing')\n",
    "ax.set_title('Mesh Cell Faces and Profile Lines')\n",
    "\n",
    "# Add grid and legend\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracting mesh cell faces near profile lines\n",
    "print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n",
    "\n",
    "# Get mesh cell faces using HdfMesh class\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces DataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces\n",
    "\n",
    "# Load the GeoJSON file for profile lines\n",
    "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
    "profile_lines_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
    "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
    "\n",
    "# Initialize a dictionary to store faces near each profile line\n",
    "faces_near_profile_lines = {}\n",
    "\n",
    "# Define distance threshold (10 ft converted to meters)\n",
    "distance_threshold = 10\n",
    "angle_threshold = 60  # degrees\n",
    "\n",
    "# Function to calculate the smallest angle between two lines or line segments.\n",
    "def calculate_angle(line):\n",
    "    if isinstance(line, LineString):\n",
    "        x_diff = line.xy[0][-1] - line.xy[0][0]\n",
    "        y_diff = line.xy[1][-1] - line.xy[1][0]\n",
    "    else:\n",
    "        x_diff = line[1][0] - line[0][0]\n",
    "        y_diff = line[1][1] - line[0][1]\n",
    "    \n",
    "    angle = np.degrees(np.arctan2(y_diff, x_diff))\n",
    "    return angle % 360 if angle >= 0 else (angle + 360) % 360\n",
    "\n",
    "# Function to break line into segments\n",
    "def break_line_into_segments(line, segment_length):\n",
    "    segments = []\n",
    "    segment_angles = []\n",
    "    \n",
    "    distances = np.arange(0, line.length, segment_length)\n",
    "    if distances[-1] != line.length:\n",
    "        distances = np.append(distances, line.length)\n",
    "        \n",
    "    for i in range(len(distances)-1):\n",
    "        point1 = line.interpolate(distances[i])\n",
    "        point2 = line.interpolate(distances[i+1])\n",
    "        segment = LineString([point1, point2])\n",
    "        segments.append(segment)\n",
    "        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n",
    "        \n",
    "    return segments, segment_angles\n",
    "\n",
    "# Function to calculate angle difference accounting for 180 degree equivalence\n",
    "def angle_difference(angle1, angle2):\n",
    "    diff = abs(angle1 - angle2) % 180\n",
    "    return min(diff, 180 - diff)\n",
    "\n",
    "# Function to order faces along profile line\n",
    "def order_faces_along_profile(profile_line, faces_gdf):\n",
    "    profile_start = Point(profile_line.coords[0])\n",
    "    \n",
    "    faces_with_dist = []\n",
    "    for idx, face in faces_gdf.iterrows():\n",
    "        face_start = Point(face.geometry.coords[0])\n",
    "        dist = profile_start.distance(face_start)\n",
    "        faces_with_dist.append((idx, dist))\n",
    "    \n",
    "    faces_with_dist.sort(key=lambda x: x[1])\n",
    "    return [x[0] for x in faces_with_dist]\n",
    "\n",
    "# Function to combine ordered faces into single linestring\n",
    "def combine_faces_to_linestring(ordered_faces_gdf):\n",
    "    coords = []\n",
    "    for _, face in ordered_faces_gdf.iterrows():\n",
    "        if not coords:  # First face - add all coordinates\n",
    "            coords.extend(list(face.geometry.coords))\n",
    "        else:  # Subsequent faces - add only end coordinate\n",
    "            coords.append(face.geometry.coords[-1])\n",
    "    return LineString(coords)\n",
    "\n",
    "# Initialize GeoDataFrame for final profile-to-faceline results\n",
    "profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n",
    "\n",
    "# Iterate through each profile line\n",
    "for index, profile_line in profile_lines_gdf.iterrows():\n",
    "    profile_geom = profile_line.geometry\n",
    "    \n",
    "    # Break profile line into segments\n",
    "    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n",
    "    \n",
    "    # Initialize set to store nearby faces\n",
    "    nearby_faces = set()\n",
    "    \n",
    "    # For each face, check distance to segments and angle difference\n",
    "    for face_idx, face in mesh_cell_faces.iterrows():\n",
    "        face_geom = face.geometry\n",
    "        \n",
    "        if isinstance(face_geom, LineString):\n",
    "            face_angle = calculate_angle(face_geom)\n",
    "            \n",
    "            for segment, segment_angle in zip(segments, segment_angles):\n",
    "                if face_geom.distance(segment) <= distance_threshold:\n",
    "                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n",
    "                        nearby_faces.add(face_idx)\n",
    "                        break\n",
    "    \n",
    "    # Convert the set of indices back to a GeoDataFrame\n",
    "    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n",
    "    \n",
    "    # Order faces along profile line\n",
    "    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n",
    "    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n",
    "    \n",
    "    # Combine ordered faces into single linestring\n",
    "    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n",
    "    \n",
    "    # Add to profile_to_faceline GeoDataFrame\n",
    "    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n",
    "                               'geometry': [combined_linestring]}, \n",
    "                              crs=profile_lines_gdf.crs)\n",
    "    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n",
    "    \n",
    "    # Store the ordered faces in the dictionary\n",
    "    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot all mesh cell faces in light blue\n",
    "mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n",
    "\n",
    "# Plot selected faces for each profile line with numbers\n",
    "colors = ['red', 'green', 'blue']\n",
    "for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n",
    "    if not faces.empty:\n",
    "        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n",
    "        \n",
    "        # Add numbers to faces\n",
    "        for i, (idx, face) in enumerate(faces.iterrows()):\n",
    "            midpoint = face.geometry.interpolate(0.5, normalized=True)\n",
    "            ax.text(midpoint.x, midpoint.y, str(i+1), \n",
    "                   color=color, fontweight='bold', ha='center', va='center')\n",
    "\n",
    "# Plot the combined linestrings\n",
    "profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n",
    "                        linestyle='--', label='Combined Face Lines')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Easting')\n",
    "ax.set_ylabel('Northing')\n",
    "ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile')\n",
    "\n",
    "# Add grid and legend\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nOriginal ordered faces near profile lines:\")\n",
    "faces_near_profile_lines\n",
    "\n",
    "print(\"\\nCombined profile-to-faceline results:\")\n",
    "profile_to_faceline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get face property tables with error handling\n",
    "face_property_tables = HdfMesh.get_mesh_face_property_tables(geom_hdf_path)\n",
    "face_property_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the face property table for Face ID 4 and display it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "face_id = 4\n",
    "face_properties = face_property_tables['Perimeter 1'][face_property_tables['Perimeter 1']['Face ID'] == face_id]\n",
    "\n",
    "# Create subplots arranged horizontally\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot Z vs Area\n",
    "axs[0].plot(face_properties['Z'], face_properties['Area'], marker='o', color='blue', label='Area')\n",
    "axs[0].set_title(f'Face ID {face_id}: Z vs Area')\n",
    "axs[0].set_xlabel('Z')\n",
    "axs[0].set_ylabel('Area')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Z vs Wetted Perimeter\n",
    "axs[1].plot(face_properties['Z'], face_properties['Wetted Perimeter'], marker='o', color='green', label='Wetted Perimeter')\n",
    "axs[1].set_title(f'Face ID {face_id}: Z vs Wetted Perimeter')\n",
    "axs[1].set_xlabel('Z')\n",
    "axs[1].set_ylabel('Wetted Perimeter')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot Z vs Manning's n\n",
    "axs[2].plot(face_properties['Z'], face_properties[\"Manning's n\"], marker='o', color='red', label=\"Manning's n\")\n",
    "axs[2].set_title(f'Face ID {face_id}: Z vs Manning\\'s n')\n",
    "axs[2].set_xlabel('Z')\n",
    "axs[2].set_ylabel(\"Manning's n\")\n",
    "axs[2].grid(True)\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh timeseries output\n",
    "\n",
    "# Get mesh areas from previous code cell\n",
    "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
    "\n",
    "if mesh_areas:\n",
    "    mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n",
    "    timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\")\n",
    "    print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n",
    "    print(timeseries_da)\n",
    "else:\n",
    "    print(\"No mesh areas found in the geometry file.\")\n",
    "\n",
    "# Get mesh cells timeseries output\n",
    "cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name)\n",
    "print(\"\\nMesh Cells Timeseries Output:\")\n",
    "print(cells_timeseries_ds)\n",
    "\n",
    "# Get mesh faces timeseries output\n",
    "faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name)\n",
    "print(\"\\nMesh Faces Timeseries Output:\")\n",
    "print(faces_timeseries_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all face velocities and face flow values to positive\n",
    "\n",
    "# Function to process and convert face data to positive values\n",
    "def convert_to_positive_values(faces_timeseries_ds, cells_timeseries_ds):\n",
    "    \"\"\"\n",
    "    Convert face velocities and flows to positive values while maintaining their relationships.\n",
    "    \n",
    "    Args:\n",
    "        faces_timeseries_ds (xarray.Dataset): Dataset containing face timeseries data\n",
    "        cells_timeseries_ds (xarray.Dataset): Dataset containing cell timeseries data\n",
    "        \n",
    "    Returns:\n",
    "        xarray.Dataset: Modified dataset with positive values\n",
    "    \"\"\"\n",
    "    # Get the face velocity and flow variables\n",
    "    face_velocity = faces_timeseries_ds['face_velocity']\n",
    "    face_flow = faces_timeseries_ds['face_flow']\n",
    "    \n",
    "    # Calculate the sign of the velocity to maintain flow direction relationships\n",
    "    velocity_sign = xr.where(face_velocity >= 0, 1, -1)\n",
    "    \n",
    "    # Convert velocities and flows to absolute values while maintaining their relationship\n",
    "    faces_timeseries_ds['face_velocity'] = abs(face_velocity)\n",
    "    faces_timeseries_ds['face_flow'] = abs(face_flow)\n",
    "    \n",
    "    # Store the original sign as a new variable for reference\n",
    "    faces_timeseries_ds['velocity_direction'] = velocity_sign\n",
    "    \n",
    "    print(\"Conversion to positive values complete.\")\n",
    "    print(f\"Number of faces processed: {len(faces_timeseries_ds.face_id)}\")\n",
    "    \n",
    "    return faces_timeseries_ds, cells_timeseries_ds\n",
    "\n",
    "# Convert the values in our datasets\n",
    "faces_timeseries_ds_positive, cells_timeseries_ds_positive = convert_to_positive_values(\n",
    "    faces_timeseries_ds, \n",
    "    cells_timeseries_ds\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Function to process faces for a single profile line\n",
    "def process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds):\n",
    "    face_ids = faces['face_id'].tolist()\n",
    "    \n",
    "    # Extract relevant data for these faces\n",
    "    face_velocities = faces_timeseries_ds['face_velocity'].sel(face_id=face_ids)\n",
    "    face_flows = faces_timeseries_ds['face_flow'].sel(face_id=face_ids)\n",
    "    \n",
    "    # Create a new dataset with calculated results\n",
    "    results_ds = xr.Dataset({\n",
    "        'face_velocity': face_velocities,\n",
    "        'face_flow': face_flows\n",
    "    })\n",
    "    \n",
    "    # Convert to dataframe for easier manipulation\n",
    "    results_df = results_ds.to_dataframe().reset_index()\n",
    "    \n",
    "    # Add profile name and face order\n",
    "    results_df['profile_name'] = profile_name\n",
    "    results_df['face_order'] = results_df.groupby('time')['face_id'].transform(lambda x: pd.factorize(x)[0])\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Vave = Sum Qn / Sum An for each profile line\n",
    "# where Vave = the summation of face flow / flow area for all the faces in the profile line\n",
    "\n",
    "# Then, save the results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all profile lines\n",
    "all_results = []\n",
    "for profile_name, faces in faces_near_profile_lines.items():\n",
    "    profile_results = process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds)\n",
    "    all_results.append(profile_results)\n",
    "\n",
    "# Combine results from all profile lines\n",
    "combined_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined results\n",
    "print(combined_results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_time_series = {}\n",
    "\n",
    "# Iterate through each profile line and extract its corresponding data\n",
    "for profile_name, faces_gdf in faces_near_profile_lines.items():\n",
    "    # Get the list of face_ids for this profile line\n",
    "    face_ids = faces_gdf['face_id'].tolist()\n",
    "    \n",
    "    # Filter the combined_results_df for these face_ids\n",
    "    profile_df = combined_results_df[combined_results_df['face_id'].isin(face_ids)].copy()\n",
    "    \n",
    "    # Add the profile name as a column\n",
    "    profile_df['profile_name'] = profile_name\n",
    "    \n",
    "    # Reset index for cleanliness\n",
    "    profile_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    profile_time_series[profile_name] = profile_df\n",
    "    \n",
    "    # Display a preview\n",
    "    print(f\"\\nTime Series DataFrame for {profile_name}:\")\n",
    "    profile_df\n",
    "\n",
    "# Optionally, display all profile names\n",
    "print(\"\\nProfile Lines Processed:\")\n",
    "print(list(profile_time_series.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n",
    "|------------|---------|---------------|-------------|----------------|------------|\n",
    "| 2019-04-02 | 370     | 1.543974      | 961.118225  | Profile Line 1 | 0          |\n",
    "| 2019-04-02 | 232     | 2.738194      | 5103.555176 | Profile Line 1 | 1          |\n",
    "| 2019-04-02 | 747     | 3.109769      | 4777.513672 | Profile Line 1 | 2          |\n",
    "| 2019-04-02 | 216     | 2.974400      | 5120.266113 | Profile Line 1 | 3          |\n",
    "| 2019-04-02 | 184     | 0.924792      | 700.676697  | Profile Line 1 | 4          |  \n",
    "  \n",
    "\n",
    "\n",
    "| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n",
    "|------------|---------|---------------|-------------|----------------|------------|\n",
    "| 2019-04-02 | 52      | 0.000000      | 0.000000    | Profile Line 2 | 0          |\n",
    "| 2019-04-02 | 92      | 0.000000      | 0.000000    | Profile Line 2 | 1          |\n",
    "| 2019-04-02 | 548     | 1.018038      | 353.129822  | Profile Line 2 | 2          |\n",
    "| 2019-04-02 | 691     | 2.106394      | 2195.409912 | Profile Line 2 | 3          |\n",
    "| 2019-04-02 | 78      | 2.376904      | 3600.228760 | Profile Line 2 | 4          |  \n",
    "  \n",
    "\n",
    "\n",
    "| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n",
    "|------------|---------|---------------|-------------|----------------|------------|\n",
    "| 2019-04-02 | 532     | 0.000000      | 0.000000    | Profile Line 3 | 0          |\n",
    "| 2019-04-02 | 341     | 0.000000      | 0.000000    | Profile Line 3 | 1          |\n",
    "| 2019-04-02 | 349     | 1.962641      | 2601.644287 | Profile Line 3 | 2          |\n",
    "| 2019-04-02 | 455     | 2.367594      | 4148.870605 | Profile Line 3 | 3          |\n",
    "| 2019-04-02 | 469     | 2.515510      | 4458.292480 | Profile Line 3 | 4          |  \n",
    "  \n",
    "  \n",
    " \n",
    "Profile Lines Processed:\n",
    "['Profile Line 1', 'Profile Line 2', 'Profile Line 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_profiles_df = pd.concat(profile_time_series.values(), ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(\"Combined Time Series DataFrame for All Profiles:\")\n",
    "all_profiles_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively explore the 2D Flow Areas structure in the geometry HDF file\n",
    "import h5py\n",
    "\n",
    "def print_hdf_structure(name, obj):\n",
    "    \"\"\"Print information about HDF5 object\"\"\"\n",
    "    print(f\"\\nPath: {name}\")\n",
    "    print(f\"Type: {type(obj).__name__}\")\n",
    "    \n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"Shape: {obj.shape}\")\n",
    "        print(f\"Dtype: {obj.dtype}\")\n",
    "        print(\"Attributes:\")\n",
    "        for key, value in obj.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "def explore_flow_areas(file_path):\n",
    "    \"\"\"\n",
    "    Recursively explore and print 2D Flow Areas structure in HDF5 file\n",
    "    \n",
    "    :param file_path: Path to the HDF5 file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as hdf_file:\n",
    "            if '/Geometry/2D Flow Areas' in hdf_file:\n",
    "                flow_areas_group = hdf_file['/Geometry/2D Flow Areas']\n",
    "                flow_areas_group.visititems(print_hdf_structure)\n",
    "            else:\n",
    "                print(\"2D Flow Areas group not found in geometry file\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring HDF file: {e}\")\n",
    "\n",
    "print(\"\\nExploring 2D Flow Areas structure in geometry file:\")\n",
    "print(\"HDF Base Path: /Geometry/2D Flow Areas \")\n",
    "explore_flow_areas(geom_hdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the necessary variables\n",
    "print(\"Available variables:\")\n",
    "print(\"profile_time_series:\", 'profile_time_series' in locals())\n",
    "print(\"faces_near_profile_lines:\", 'faces_near_profile_lines' in locals())\n",
    "print(\"profile_averages:\", 'profile_averages' in locals())\n",
    "\n",
    "# Look at the structure of profile_time_series\n",
    "if 'profile_time_series' in locals():\n",
    "    for name, df in profile_time_series.items():\n",
    "        print(f\"\\nColumns in {name}:\")\n",
    "        print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discharge_weighted_velocity(profile_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate discharge-weighted average velocity for a profile line\n",
    "    Vw = Sum(|Qi|*Vi)/Sum(|Qi|) where Qi is face flow and Vi is face velocity\n",
    "    \"\"\"\n",
    "    print(\"Calculating discharge-weighted velocity...\")\n",
    "    print(f\"Input DataFrame:\\n{profile_df.head()}\")\n",
    "\n",
    "    # Calculate weighted velocity for each timestep\n",
    "    weighted_velocities = []\n",
    "    for time in profile_df['time'].unique():\n",
    "        time_data = profile_df[profile_df['time'] == time]\n",
    "        abs_flows = np.abs(time_data['face_flow'])\n",
    "        abs_velocities = np.abs(time_data['face_velocity'])\n",
    "        weighted_vel = (abs_flows * abs_velocities).sum() / abs_flows.sum()\n",
    "        weighted_velocities.append({\n",
    "            'time': time,\n",
    "            'weighted_velocity': weighted_vel\n",
    "        })\n",
    "    \n",
    "    weighted_df = pd.DataFrame(weighted_velocities)\n",
    "    print(f\"Calculated weighted velocities:\\n{weighted_df.head()}\")\n",
    "    return weighted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for each profile line\n",
    "for profile_name, profile_df in profile_time_series.items():\n",
    "    print(f\"\\nProcessing profile: {profile_name}\")\n",
    "\n",
    "    # Calculate discharge-weighted velocity\n",
    "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
    "    \n",
    "    print(\"Weighted velocities calculated.\")\n",
    "    weighted_velocities\n",
    "    \n",
    "    # Convert time to datetime if it isn't already\n",
    "    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n",
    "    print(\"Converted time to datetime format.\")\n",
    "\n",
    "    # Get ordered faces for this profile\n",
    "    ordered_faces = faces_near_profile_lines[profile_name]\n",
    "    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n",
    "    \n",
    "    # Save dataframes in the output directory\n",
    "    output_file = output_dir / f\"{profile_name}_discharge_weighted_velocity.csv\"\n",
    "    weighted_velocities.to_csv(output_file, index=False)\n",
    "    print(f\"Saved weighted velocities to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots comparing discharge-weighted velocity and simple average for each profile line\n",
    "for profile_name, profile_df in profile_time_series.items():\n",
    "    \n",
    "    print(f\"\\nGenerating comparison plot for profile: {profile_name}\")\n",
    "    \n",
    "    # Calculate discharge-weighted velocity\n",
    "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
    "    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n",
    "    \n",
    "    # Calculate simple average velocity for each timestep\n",
    "    simple_averages = profile_df.groupby('time')['face_velocity'].mean().reset_index()\n",
    "    simple_averages['time'] = pd.to_datetime(simple_averages['time'])\n",
    "    \n",
    "    # Create figure for comparison plot\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    \n",
    "    # Plot individual face velocities with thin lines\n",
    "    for face_id in profile_df['face_id'].unique():\n",
    "        face_data = profile_df[profile_df['face_id'] == face_id]\n",
    "        plt.plot(face_data['time'], \n",
    "                face_data['face_velocity'], \n",
    "                alpha=0.8,  # More transparent\n",
    "                linewidth=0.3,  # Thinner line\n",
    "                color='gray',  # Consistent color\n",
    "                label=f'Face ID {face_id}' if face_id == profile_df['face_id'].iloc[0] else \"\")\n",
    "        \n",
    "        # Find and annotate peak value for each face\n",
    "        peak_idx = face_data['face_velocity'].idxmax()\n",
    "        peak_time = face_data.loc[peak_idx, 'time']\n",
    "        peak_vel = face_data.loc[peak_idx, 'face_velocity']\n",
    "        plt.annotate(f'{peak_vel:.2f}',\n",
    "                    xy=(peak_time, peak_vel),\n",
    "                    xytext=(10, 10),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8,\n",
    "                    alpha=0.5)\n",
    "    \n",
    "    # Plot discharge-weighted velocity\n",
    "    plt.plot(weighted_velocities['time'], \n",
    "            weighted_velocities['weighted_velocity'], \n",
    "            color='red', \n",
    "            alpha=1.0, \n",
    "            linewidth=2,\n",
    "            label='Discharge-Weighted Velocity')\n",
    "    \n",
    "    # Find and annotate peak weighted velocity\n",
    "    peak_idx = weighted_velocities['weighted_velocity'].idxmax()\n",
    "    peak_time = weighted_velocities.loc[peak_idx, 'time']\n",
    "    peak_vel = weighted_velocities.loc[peak_idx, 'weighted_velocity']\n",
    "    plt.annotate(f'Peak Weighted: {peak_vel:.2f}',\n",
    "                xy=(peak_time, peak_vel),\n",
    "                xytext=(10, 10),\n",
    "                textcoords='offset points',\n",
    "                color='red',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    # Plot simple average\n",
    "    plt.plot(simple_averages['time'], \n",
    "            simple_averages['face_velocity'], \n",
    "            color='blue', \n",
    "            alpha=0.5, \n",
    "            linewidth=1,\n",
    "            linestyle='--',\n",
    "            label='Simple Average')\n",
    "    \n",
    "    # Find and annotate peak simple average\n",
    "    peak_idx = simple_averages['face_velocity'].idxmax()\n",
    "    peak_time = simple_averages.loc[peak_idx, 'time']\n",
    "    peak_vel = simple_averages.loc[peak_idx, 'face_velocity']\n",
    "    plt.annotate(f'Peak Average: {peak_vel:.2f}',\n",
    "                xy=(peak_time, peak_vel),\n",
    "                xytext=(10, -10),\n",
    "                textcoords='offset points',\n",
    "                color='blue',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    # Configure plot\n",
    "    plt.title(f'Velocity Comparison - {profile_name}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Velocity (ft/s)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend with better placement\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout to accommodate legend and stats\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    \n",
    "    # Save plot to file\n",
    "    plot_file = output_dir / f\"{profile_name}_velocity_comparison.png\"\n",
    "    plt.savefig(plot_file, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(f\"\\nVelocity Comparison for {profile_name}:\")\n",
    "    print(f\"Number of faces: {profile_df['face_id'].nunique()}\")\n",
    "    print(\"\\nDischarge-Weighted Velocity Statistics:\")\n",
    "    print(f\"Mean: {weighted_velocities['weighted_velocity'].mean():.2f} ft/s\")\n",
    "    print(f\"Max: {weighted_velocities['weighted_velocity'].max():.2f} ft/s\")\n",
    "    print(f\"Min: {weighted_velocities['weighted_velocity'].min():.2f} ft/s\")\n",
    "    print(\"\\nSimple Average Velocity Statistics:\")\n",
    "    print(f\"Mean: {simple_averages['face_velocity'].mean():.2f} ft/s\")\n",
    "    print(f\"Max: {simple_averages['face_velocity'].max():.2f} ft/s\")\n",
    "    print(f\"Min: {simple_averages['face_velocity'].min():.2f} ft/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  We are using the face normal velocity that is available in the HDF.  This will only be accurate if you pick cell faces that are perpendicular to flow.  Depending on the application, a more robust calculation may be required. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\14_fluvial_pluvial_delineation.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delineate Fluvial and Pluvial Areas using RAS-Commander\n",
    "\n",
    "We will leverage the HEC RAS Summary Outputs to delineate the Fluvial and Pluvial Areas\n",
    "\n",
    "Maximum Water Surface Elevation (WSEL) for each cell is recorded, along with the timestamps of when the maximum WSEL occurs.\n",
    "\n",
    "By locating adjacent cells with dissimilar timestamps, we can delineate the Fluvial and Pluvial Areas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about datframe types: \n",
    "\n",
    "Information from the HEC-RAS plan files are generally dataframes.  The text file interface is for the 32-bit side of HEC-RAS and all spatial data is most easily accessed in the HDF files.  This includes plan_df, geom_df, hdf_paths_df\n",
    "\n",
    "Geometry elements (Mesh Faces and Nodes) are provided as Geodataframes (cell_polygons_gdf, boundary_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and run plan 06\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "\n",
    "    # Initialize the RAS project using the default global ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"06\"\n",
    "\n",
    "    # Update the run flags in the plan file\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,  # Run HTab\n",
    "        unsteady_flow_simulation=True,  # Run UNet\n",
    "        post_processor=True,  # Run PostProcess\n",
    "        floodplain_mapping=False,  # Run RASMapper\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the default global ras object\n",
    "    init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "ras.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "ras.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = ras.geom_df.loc[ras.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mesh_max_ws, get the cell coordinates and plot the max water surface as a map\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Get mesh max water surface\n",
    "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)\n",
    "\n",
    "print(\"max_ws_df\")\n",
    "print(max_ws_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot\n",
    "HdfResultsPlot.plot_results_max_wsel(max_ws_df)\n",
    "\n",
    "# Plot the time of maximum water surface elevation\n",
    "HdfResultsPlot.plot_results_max_wsel_time(max_ws_df)\n",
    "\n",
    "# Print the first few rows of the merged dataframe for verification\n",
    "print(\"\\nFirst few rows of the merged dataframe:\")\n",
    "max_ws_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Cell Polygons\n",
    "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
    "cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)\n",
    "\n",
    "\n",
    "# Call the function to plot cell polygons\n",
    "#cell_polygons_gdf = HdfFluvialPluvial.plot_cell_polygons(cell_polygons_gdf, projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString, Polygon, MultiLineString\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from rtree import index\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(plan_hdf_path, delta_t=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics about the boundary line lengths\n",
    "boundary_lengths = boundary_gdf.geometry.length\n",
    "\n",
    "print(\"Boundary line length statistics:\")\n",
    "print(f\"Max length: {boundary_lengths.max():.2f}\")\n",
    "print(f\"Min length: {boundary_lengths.min():.2f}\")\n",
    "print(f\"Average length: {boundary_lengths.mean():.2f}\")\n",
    "print(f\"Median length: {boundary_lengths.median():.2f}\")\n",
    "\n",
    "# Print general information about the boundary GeoDataFrame\n",
    "print(\"\\nBoundary GeoDataFrame info:\")\n",
    "print(boundary_gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
    "boundary_gdf.plot(ax=ax, color='red', linewidth=2)\n",
    "plt.title('Fluvial-Pluvial Boundary')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_threshold = 3000 #in same units as X and Y coordinates\n",
    "\n",
    "# Filter out boundary lines below the length threshold\n",
    "filtered_boundary_gdf = boundary_gdf[boundary_lengths >= length_threshold]\n",
    "highlighted_boundary_gdf = boundary_gdf[boundary_lengths < length_threshold]\n",
    "\n",
    "# Visualize the results with highlighted boundaries below the threshold\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
    "filtered_boundary_gdf.plot(ax=ax, color='red', linewidth=2, label='Valid Boundaries')\n",
    "highlighted_boundary_gdf.plot(ax=ax, color='blue', linewidth=2, linestyle='--', label='Highlighted Boundaries Below Threshold')\n",
    "plt.title('Fluvial-Pluvial Boundary with Length Threshold')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fluvial_pluvial_boundary subfolder\n",
    "output_dir = bald_eagle_path / \"fluvial_pluvial_boundary\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory created/verified at: {output_dir}\")\n",
    "\n",
    "# Save to GeoJSON in output directory\n",
    "boundary_gdf.to_file(output_dir / 'fluvial_pluvial_boundary.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\README.md
==================================================
# RAS Commander Examples

This directory contains example notebooks demonstrating how to use the `ras-commander` library for automating HEC-RAS operations. These examples cover basic to advanced usage scenarios and provide a practical guide for hydraulic modelers looking to automate their workflows.

## Overview

HEC-RAS (Hydrologic Engineering Center's River Analysis System) is widely used for hydraulic modeling. The `ras-commander` library provides a Python interface to automate HEC-RAS operations without using the graphical user interface. This enables batch processing, sensitivity analysis, and integration with other Python tools for water resources engineering.

These example notebooks are designed to:
- Demonstrate key functionalities of the `ras-commander` library
- Provide practical use cases for automation
- Guide users from basic to advanced operations
- Serve as templates for your own automation scripts

## Examples

### [00_Using_RasExamples.ipynb](00_Using_RasExamples.ipynb)

This notebook introduces the `RasExamples` class, which provides easy access to HEC-RAS example projects for testing and demonstration purposes.

**Key contents:**
- Installing `ras-commander` from pip
- Using flexible imports for development without installation
- Extracting specific HEC-RAS example projects by folder name
- Advanced usage options for managing example projects
- Listing available example projects and categories
- Working with the new pipes and conduits examples (version 6.6)

### [01_project_initialization.ipynb](01_project_initialization.ipynb)

This notebook covers initializing and working with HEC-RAS projects using the `ras-commander` library.

**Key contents:**
- Setting up and configuring the RAS Commander environment
- Downloading and extracting example HEC-RAS projects
- Initializing HEC-RAS projects using the global `ras` object
- Initializing multiple HEC-RAS projects using custom RAS objects
- Accessing various project components (plans, geometries, flows, boundaries)
- Understanding the RAS object structure and its components
- Working with boundary conditions
- Comparing multiple projects

### [02_plan_and_geometry_operations.ipynb](02_plan_and_geometry_operations.ipynb)

This notebook demonstrates operations on HEC-RAS plan and geometry files using the RAS Commander library.

**Key contents:**
- Project initialization and understanding plan/geometry files
- Cloning plans to create new simulation scenarios
- Cloning geometry files for modified versions
- Setting geometry files for plans
- Clearing geometry preprocessor files
- Configuring simulation parameters and intervals
- Setting run flags and updating descriptions
- Cloning and configuring unsteady flow files
- Computing plans and verifying results
- Working with advanced HDF data
- Best practices for plan and geometry operations

### [03_unsteady_flow_operations.ipynb](03_unsteady_flow_operations.ipynb)

This notebook demonstrates operations on unsteady flow files using the RAS Commander library.

**Key contents:**
- Understanding unsteady flow files in HEC-RAS
- Extracting boundary conditions and tables from unsteady flow files
- Inspecting and analyzing boundary condition structures
- Working with different boundary condition types (flow hydrographs, stage hydrographs, etc.)
- Modifying flow titles in unsteady flow files
- Configuring restart settings for continuing simulations
- Extracting and working with flow tables
- Modifying flow tables and writing them back to files
- Applying updated unsteady flow to a plan and computing results

### [04_multiple_project_operations.ipynb](04_multiple_project_operations.ipynb)

This notebook demonstrates how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library.

**Key contents:**
- Initializing and managing multiple HEC-RAS projects
- Cloning and modifying plans across different projects
- Running computations for multiple projects in parallel
- Optimizing computing resources when working with multiple models
- Analyzing and comparing results from different projects
- Building comprehensive multi-project workflows
- Best practices for multiple project management
- Setting up compute folders for multiple projects
- Comparing project structures and results

### [05_single_plan_execution.ipynb](05_single_plan_execution.ipynb)

This notebook focuses specifically on executing a single HEC-RAS plan with various configuration options.

**Key contents:**
- Understanding the `RasCmdr.compute_plan` method and its parameters
- Executing a plan with a specified number of processor cores
- Creating and managing destination folders for computations
- Overwriting existing destination folders
- Verifying computation results
- Options for single plan execution (basic execution, destination folder, number of cores, etc.)
- Best practices for single plan execution

### [06_executing_plan_sets.ipynb](06_executing_plan_sets.ipynb)

This notebook demonstrates different ways to specify and execute HEC-RAS plans using the RAS Commander library.

**Key contents:**
- Understanding plan specification in HEC-RAS
- Sequential execution of specific plans
- Selective plan execution based on criteria
- Running only plans without HDF results
- Verifying execution results
- Best practices for plan specification
- Choosing appropriate execution methods based on scenario
- Understanding the importance of plan selection for efficiency

### [07_sequential_plan_execution.ipynb](07_sequential_plan_execution.ipynb)

This notebook demonstrates how to sequentially execute multiple HEC-RAS plans using the RAS Commander library.

**Key contents:**
- Understanding sequential execution in HEC-RAS
- Using the `RasCmdr.compute_test_mode` method
- Executing all plans in a project sequentially
- Analyzing the test folder after sequential execution
- Executing specific plans with geometry preprocessor clearing
- Best practices for sequential execution
- Environment setup and test folder management
- Benefits of sequential execution (controlled resource usage, dependency management, etc.)

### [08_parallel_execution.ipynb](08_parallel_execution.ipynb)

This notebook demonstrates how to execute multiple HEC-RAS plans in parallel to maximize computational efficiency.

**Key contents:**
- Understanding parallel execution in HEC-RAS
- Setting up a working environment for parallel execution
- Checking system resources for optimal parallel execution
- Executing all plans in a project in parallel
- Executing specific plans in parallel
- Dynamic worker allocation based on available resources
- Balancing workers and cores per worker
- Analyzing parallel execution results
- Performance comparison between different parallel configurations
- Best practices for parallel execution

### [09_plan_parameter_operations.ipynb](09_plan_parameter_operations.ipynb)

This notebook demonstrates how to perform key operations on HEC-RAS plan files, focusing on modifying simulation parameters.

**Key contents:**
- Understanding plan files in HEC-RAS
- Retrieving specific values from plan files
- Updating run flags to control which components will run
- Modifying computation and output time intervals
- Reading and updating plan descriptions
- Changing simulation start and end dates
- Verifying updated plan values
- Best practices for plan operations
- Automating parameter adjustments for sensitivity analysis
- Managing documentation through plan descriptions

### [10_1d_hdf_data_extraction.ipynb](10_1d_hdf_data_extraction.ipynb)

This notebook demonstrates how to extract and analyze 1D data from HEC-RAS HDF files using the RAS Commander library.

**Key contents:**
- Accessing and extracting base geometry attributes from HDF files
- Working with 1D cross-section data, including station-elevation profiles
- Visualizing cross-section properties like Manning's n values
- Extracting river centerlines, bank lines, and edge lines
- Analyzing runtime data and compute messages
- Processing and visualizing ineffective flow areas
- Extracting time series data for 1D cross sections
- Plotting cross-section elevation profiles with bank stations

### [11_2d_hdf_data_extraction.ipynb](11_2d_hdf_data_extraction.ipynb)

This notebook shows how to extract and analyze 2D data from HEC-RAS HDF files using the RAS Commander library.

**Key contents:**
- Working with 2D flow area attributes and perimeter polygons
- Extracting and visualizing mesh cell faces, polygons, and points
- Finding nearest faces and cells to specific points
- Extracting boundary condition lines and breaklines
- Analyzing maximum water surface elevations and timing
- Working with maximum face velocities and water surface errors
- Visualizing 2D model results with terrain data
- Extracting and interpreting cell and face time series data

### [12_2d_hdf_data_extraction_pipes_and_pumps.ipynb](12_2d_hdf_data_extraction_pipes_and_pumps.ipynb)

This notebook focuses on extracting and analyzing data related to pipes, conduits, and pump stations from HEC-RAS HDF files.

**Key contents:**
- Working with pipe conduits and associated geometries
- Extracting pipe node information and properties
- Analyzing pipe network connectivity and structures
- Visualizing pipe networks with node elevations
- Working with pump stations and pump groups
- Extracting pipe and node time series data
- Analyzing face flow, velocity, and water surface values
- Processing and visualizing pump station operation data

### [13_2d_detail_face_data_extraction.ipynb](13_2d_detail_face_data_extraction.ipynb)

This notebook demonstrates techniques for detailed face data extraction from 2D HEC-RAS models.

**Key contents:**
- Extracting and analyzing detailed face property tables
- Working with profile lines to identify cell faces
- Finding faces perpendicular to flow for discharge calculations
- Converting face velocities and flows to positive values
- Calculating discharge-weighted velocities for profile lines
- Comparing discharge-weighted and simple average velocities
- Visualizing time series data for selected faces
- Creating profile-specific result datasets for analysis

### [14_fluvial_pluvial_delineation.ipynb](14_fluvial_pluvial_delineation.ipynb)

This notebook demonstrates how to delineate fluvial and pluvial flooding areas based on the timing of maximum water surface elevations.

**Key contents:**
- Extracting maximum water surface elevation and timing data
- Identifying adjacent cells with dissimilar flood timing
- Calculating boundaries between fluvial and pluvial flooding
- Filtering boundaries based on length thresholds
- Visualizing the fluvial-pluvial boundary on a map
- Exporting boundaries to GeoJSON format
- Understanding the difference between river-driven and rainfall-driven flooding
- Using cell polygon geometry for spatial analysis

### [101_Core_Sensitivity.ipynb](101_Core_Sensitivity.ipynb)

This notebook tests HEC-RAS performance with different CPU core configurations to optimize computational efficiency.

**Key contents:**
- Setting up a controlled testing environment
- Running the same plan with varying core counts
- Measuring execution time for each configuration
- Analyzing performance scaling with increased cores
- Creating visualization of performance metrics
- Calculating unit runtime based on single-core performance
- Understanding diminishing returns with multiple cores
- Identifying optimal core count for specific models

### [102_benchmarking_versions_6.1_to_6.6.ipynb](102_benchmarking_versions_6.1_to_6.6.ipynb)

This notebook compares performance across different versions of HEC-RAS by running the same plan across multiple software versions.

**Key contents:**
- Running the same model across multiple HEC-RAS versions
- Measuring preprocessing, computation, and postprocessing times
- Analyzing volume error changes between versions
- Creating visualizations of performance trends
- Identifying performance improvements between versions
- Understanding version-specific computational differences
- Setting up flexible testing environments for multiple versions
- Interpreting HEC-RAS version performance evolution

### [103_Generating_AEP_Events_from_Atlas_14.ipynb](103_Generating_AEP_Events_from_Atlas_14.ipynb)

This notebook demonstrates an end-to-end workflow for generating and analyzing multiple Annual Exceedance Probability events.

**Key contents:**
- Generating hyetographs from NOAA Atlas 14 precipitation frequency data
- Parsing duration strings and interpolating precipitation depths
- Applying the Alternating Block Method for hyetograph creation
- Cloning and configuring HEC-RAS plans for different AEP events
- Executing multiple plans in parallel with resource optimization
- Extracting and visualizing results for multiple AEP scenarios
- Creating a complete workflow from data to flood analysis
- Comparing results across different return period events


## Contributing

If you have suggestions for additional examples or improvements to existing ones, please feel free to contribute by submitting pull requests or opening issues in the repository.
==================================================

