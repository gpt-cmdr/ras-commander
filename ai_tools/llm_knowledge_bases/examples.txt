File: c:\GH\ras-commander\examples\00_Using_RasExamples.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install RAS-Commander from pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install --upgrade ras-commander"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Imports (if using the pip package)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flexible Imports (for active development of the library)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This cell will try to import the pip package, if it fails it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "# This allows a user's revisions to be tested locally without installing the package\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Flexible imports to allow for development without installation \n",
        "#  ** Use this version with Jupyter Notebooks **\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    import os\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "print(\"ras_commander imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using RASExamples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Method for Calling HEC-RAS Example Projects by Folder Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This Code Cell is All You Need\n",
        "# This is what this Class was intended to do: Help me make repeatable workflows around HEC-RAS Example Projects for testing and demonstration purposes. \n",
        "\n",
        "# Extract specific projects\n",
        "RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\", \"Davis\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RasExamples will not download a new .zip file if one already exists, this allows you to replace the Example_Projects_6_x.zip with your own zip file (with the same folder format as the HEC-RAS examples) and you will be able to load them by folder name for repeatable Test Driven Development\n",
        "\n",
        "Just make sure all project folders have unique folder names. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if example projects are already downloaded\n",
        "if RasExamples.projects_dir.exists():\n",
        "    print(\"Example projects are already downloaded.\")\n",
        "    print(\"RasExamples.folder_df:\")\n",
        "    display(RasExamples.folder_df)\n",
        "else:\n",
        "    print(\"Downloading example projects...\")\n",
        "    RasExamples.get_example_projects()\n",
        "    print(\"RasExamples.folder_df:\")\n",
        "    display(RasExamples.folder_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all categories\n",
        "categories = RasExamples.list_categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nAvailable categories:\")\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List projects in a specific category\n",
        "projects = RasExamples.list_projects(\"1D Sediment Transport\")\n",
        "projects\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all projects\n",
        "all_projects = RasExamples.list_projects()\n",
        "all_projects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific projects\n",
        "projects_to_extract = [\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"]\n",
        "extracted_paths = RasExamples.extract_project(projects_to_extract)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note about New Pipes and Conduits Version 6.6 Example Project\n",
        "\n",
        "Use project name \"Davis\" to explore pipes and conduits (introduced in version 6.6)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\01_project_initialization.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander Project Initialization\n",
        "\n",
        "This notebook demonstrates how to initialize and work with HEC-RAS projects using the `ras-commander` library. You'll learn how to:\n",
        "\n",
        "1. Set up and configure the RAS Commander environment\n",
        "2. Download and extract example HEC-RAS projects\n",
        "3. Initialize HEC-RAS projects using the global `ras` object\n",
        "4. Initialize multiple HEC-RAS projects using custom RAS objects\n",
        "5. Access various project components (plans, geometries, flows, boundaries)\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- **RasPrj Objects**: Represent HEC-RAS projects with access to plans, geometries, flows, etc.\n",
        "- **Global `ras` object**: A singleton instance for simple, single-project scripts\n",
        "- **Custom RAS Objects**: Independent instances for multi-project workflows\n",
        "- **Project Initialization**: Process of connecting to HEC-RAS projects\n",
        "- **Project Components**: Structured access to plans, geometries, and flow files\n",
        "\n",
        "Let's start by importing the necessary libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS Commander: Core Concepts\n",
        "\n",
        "RAS Commander is a Python library that provides tools for automating HEC-RAS tasks. It's built with several key design principles:\n",
        "\n",
        "1. **Project-Centric Architecture**: Everything revolves around HEC-RAS projects\n",
        "2. **Two RAS Object Approaches**:\n",
        "   - **Global `ras` Object**: A singleton for simple scripts\n",
        "   - **Custom RAS Objects**: Multiple ras project instances for complex workflows\n",
        "3. **Comprehensive Project Representation**: Each RAS object includes DataFrames for plans, geometries, flows, and boundaries\n",
        "4. **Logging**: Built-in logging to track operations and debug issues\n",
        "5. **HDF Support**: Specialized functions for HDF file access (plan results, geometry, etc.)\n",
        "\n",
        "Let's explore these concepts in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading Example HEC-RAS Projects\n",
        "\n",
        "RAS Commander includes a utility to download and extract example HEC-RAS projects. These are useful for learning and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
        "print(extracted_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Paths for Extracted Example Projects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the parent directory of the first extracted path as our examples directory\n",
        "examples_dir = extracted_paths[0].parent\n",
        "print(f\"Examples directory: {examples_dir}\")\n",
        "\n",
        "\n",
        "# Define paths to the extracted projects\n",
        "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
        "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
        "muncie_path = examples_dir / \"Muncie\"\n",
        "\n",
        "# Verify the paths exist\n",
        "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
        "    print(f\"Path {path} exists: {path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Function to Print RAS Object Data\n",
        "\n",
        "Let's create a utility function to help us explore the contents of RAS objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_ras_object_data(ras_obj, project_name):\n",
        "    \"\"\"Prints comprehensive information about a RAS object\"\"\"\n",
        "    print(f\"\\n{project_name} Data:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Project Name: {ras_obj.get_project_name()}\")\n",
        "    print(f\"Project Folder: {ras_obj.project_folder}\")\n",
        "    print(f\"PRJ File: {ras_obj.prj_file}\")\n",
        "    print(f\"HEC-RAS Executable Path: {ras_obj.ras_exe_path}\")\n",
        "    \n",
        "    print(\"\\nPlan Files DataFrame:\")\n",
        "    with pd.option_context('display.max_columns', None):\n",
        "        display.display(ras_obj.plan_df)\n",
        "    \n",
        "    print(\"\\nFlow Files DataFrame:\")\n",
        "    display.display(ras_obj.flow_df)\n",
        "    \n",
        "    print(\"\\nUnsteady Flow Files DataFrame:\")\n",
        "    display.display(ras_obj.unsteady_df)\n",
        "    \n",
        "    print(\"\\nGeometry Files DataFrame:\")\n",
        "    display.display(ras_obj.geom_df)\n",
        "    \n",
        "    print(\"\\nHDF Entries DataFrame:\")\n",
        "    display.display(ras_obj.get_hdf_entries())\n",
        "    \n",
        "    print(\"\\nBoundary Conditions DataFrame:\")\n",
        "    display.display(ras_obj.boundaries_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 1: Using the Global `ras` Object\n",
        "\n",
        "The global `ras` object is a singleton instance that persists throughout your script. It's ideal for simple scripts working with a single project.\n",
        "\n",
        "Key characteristics:\n",
        "- It's available as `ras` immediately after import\n",
        "- It's initialized via `init_ras_project()` without saving the return value\n",
        "- It provides access to all project data through the global `ras` variable\n",
        "- It's simple to use but can be problematic in complex scenarios\n",
        "\n",
        "Let's initialize it with the Bald Eagle Creek project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the global ras object with Bald Eagle Creek project\n",
        "# Note: This updates the global 'ras' object visible throughout the script\n",
        "# Parameters:\n",
        "#   - project_folder: Path to the HEC-RAS project folder (required)\n",
        "#   - ras_version: HEC-RAS version (e.g. \"6.5\") or path to Ras.exe (required first time)\n",
        "\n",
        "init_ras_project(bald_eagle_path, \"6.5\")\n",
        "print(f\"The global 'ras' object is now initialized with the {ras.project_name} project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the global ras object with our utility function\n",
        "print_ras_object_data(ras, \"Global RAS Object (Bald Eagle Creek)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the RAS Object Structure\n",
        "\n",
        "Each RAS object contains several important components:\n",
        "\n",
        "1. **Project Metadata**:\n",
        "   - `project_name`: Name of the HEC-RAS project\n",
        "   - `project_folder`: Directory containing project files\n",
        "   - `prj_file`: Path to the main .prj file\n",
        "   - `ras_exe_path`: Path to the HEC-RAS executable\n",
        "\n",
        "2. **Project DataFrames**:\n",
        "   - `plan_df`: Information about all plan files (.p*)\n",
        "   - `flow_df`: Information about all steady flow files (.f*)\n",
        "   - `unsteady_df`: Information about all unsteady flow files (.u*)\n",
        "   - `geom_df`: Information about all geometry files (.g*)\n",
        "   - `boundaries_df`: Information about all boundary conditions\n",
        "\n",
        "3. **Methods for Data Access**:\n",
        "   - `get_plan_entries()`: Get plan file information\n",
        "   - `get_flow_entries()`: Get flow file information\n",
        "   - `get_unsteady_entries()`: Get unsteady flow file information \n",
        "   - `get_geom_entries()`: Get geometry file information\n",
        "   - `get_hdf_entries()`: Get HDF file paths for result files\n",
        "   - `get_boundary_conditions()`: Get boundary condition details\n",
        "\n",
        "Let's see how to access specific information from these components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first plan's details\n",
        "if not ras.plan_df.empty:\n",
        "    first_plan = ras.plan_df.iloc[0]\n",
        "    print(f\"First plan number: {first_plan['plan_number']}\")\n",
        "    print(f\"Plan path: {first_plan['full_path']}\")\n",
        "    \n",
        "    # Get the geometry file for this plan\n",
        "    geom_id = first_plan.get('Geom File', '').replace('g', '')\n",
        "    if geom_id:\n",
        "        geom_info = ras.geom_df[ras.geom_df['geom_number'] == geom_id]\n",
        "        if not geom_info.empty:\n",
        "            print(f\"Geometry file: {geom_info.iloc[0]['full_path']}\")\n",
        "    \n",
        "    # Get the HDF results file for this plan (if exists)\n",
        "    if 'HDF_Results_Path' in first_plan and first_plan['HDF_Results_Path']:\n",
        "        print(f\"Results file: {first_plan['HDF_Results_Path']}\")\n",
        "else:\n",
        "    print(\"No plans found in the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Working with Boundary Conditions\n",
        "\n",
        "Boundary conditions define the inputs and outputs of your model. Let's see how to access boundary condition information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the boundary conditions DataFrame\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 2: Using Custom RAS Objects\n",
        "\n",
        "For more complex scripts or when working with multiple projects, it's better to create and use separate RAS objects. This approach:\n",
        "\n",
        "- Creates independent RAS objects for each project\n",
        "- Avoids overwriting the global `ras` object\n",
        "- Provides clearer separation between projects\n",
        "- Allows working with multiple projects simultaneously\n",
        "- Requires saving the return value from `init_ras_project()`\n",
        "\n",
        "Let's initialize multiple projects with custom RAS objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize multiple project instances with custom RAS objects\n",
        "# Note: This also updates the global 'ras' object each time, but we'll use the custom instances\n",
        "# Parameters remain the same as before\n",
        "multi_2d_project = RasPrj()\n",
        "init_ras_project(multi_2d_path, \"6.5\", ras_object=multi_2d_project)\n",
        "print(f\"\\nMulti2D project initialized with its own RAS object\")\n",
        "\n",
        "muncie_project = RasPrj()\n",
        "init_ras_project(muncie_path, \"6.5\", ras_object=muncie_project)\n",
        "print(f\"\\nMuncie project initialized with its own RAS object\")\n",
        "\n",
        "# Note that the global 'ras' object now points to the Muncie project\n",
        "# The global 'ras' object gets overwritten every time a project is initialized ,\n",
        "print(f\"\\nGlobal 'ras' object now points to: {ras.project_name} since it was the last one initialized.  Avoid the global object when using multiple projects.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploring Multiple Projects\n",
        "\n",
        "Now we have three RAS objects:\n",
        "- `multi_2d_project`: Our custom object for the Multi2D project\n",
        "- `muncie_project`: Our custom object for the Muncie project\n",
        "- `ras`: The global object (which now points to Muncie)\n",
        "\n",
        "Let's examine the Multi2D project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display.display(multi_2d_project.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the Multi2D project\n",
        "print_ras_object_data(multi_2d_project, \"Multi2D Project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the Muncie project\n",
        "print_ras_object_data(muncie_project, \"Muncie Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Projects\n",
        "\n",
        "Let's compare some key metrics of the two projects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comparison table of the two projects\n",
        "comparison_data = {\n",
        "    'Project Name': [multi_2d_project.project_name, muncie_project.project_name],\n",
        "    'Number of Plans': [len(multi_2d_project.plan_df), len(muncie_project.plan_df)],\n",
        "    'Number of Geometries': [len(multi_2d_project.geom_df), len(muncie_project.geom_df)],\n",
        "    'Number of Flow Files': [len(multi_2d_project.flow_df), len(muncie_project.flow_df)],\n",
        "    'Number of Unsteady Files': [len(multi_2d_project.unsteady_df), len(muncie_project.unsteady_df)],\n",
        "    'Number of Boundary Conditions': [len(multi_2d_project.boundaries_df) if hasattr(multi_2d_project, 'boundaries_df') else 0, \n",
        "                                     len(muncie_project.boundaries_df) if hasattr(muncie_project, 'boundaries_df') else 0],\n",
        "    'HDF Results Available': [len(multi_2d_project.get_hdf_entries()) > 0, len(muncie_project.get_hdf_entries()) > 0]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "display.display(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS Commander: Best Practices\n",
        "\n",
        "After exploring both approaches, here are some best practices for using RAS Commander:\n",
        "\n",
        "1. **Choose Your Approach Based on Complexity**:\n",
        "   - **Simple Scripts** (one project): Use the global `ras` object\n",
        "   - **Complex Scripts** (multiple projects): Use custom RAS objects\n",
        "\n",
        "2. **Be Consistent**:\n",
        "   - Don't mix global and custom approaches in the same script\n",
        "   - Use descriptive names for custom RAS objects\n",
        "\n",
        "3. **Working with Project Files**:\n",
        "   - Access project files through the RAS object's DataFrames\n",
        "   - Use helper functions like `get_plan_path()` to resolve paths\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - Always check for empty DataFrames before accessing their contents\n",
        "   - Use the built-in logging to track operations\n",
        "\n",
        "5. **Performance Considerations**:\n",
        "   - For large projects, consider using the HDF classes directly\n",
        "   - Cache results of expensive operations when possible\n",
        "\n",
        "## Summary of Key Functions\n",
        "\n",
        "- `init_ras_project(project_folder, ras_version)`: Initialize a RAS project\n",
        "- `RasExamples().extract_project(project_name)`: Extract example projects\n",
        "- `RasPrj.get_project_name()`: Get the name of the project\n",
        "- `RasPrj.get_plan_entries()`: Get plan file information\n",
        "- `RasPrj.get_flow_entries()`: Get flow file information\n",
        "- `RasPrj.get_unsteady_entries()`: Get unsteady flow file information\n",
        "- `RasPrj.get_geom_entries()`: Get geometry file information\n",
        "- `RasPrj.get_hdf_entries()`: Get HDF result file information\n",
        "- `RasPrj.get_boundary_conditions()`: Get boundary condition details\n",
        "- `RasPlan.get_plan_path(plan_number)`: Get the path to a plan file\n",
        "- `RasPlan.get_geom_path(geom_number)`: Get the path to a geometry file\n",
        "- `RasPlan.get_flow_path(flow_number)`: Get the path to a flow file\n",
        "- `RasPlan.get_unsteady_path(unsteady_number)`: Get the path to an unsteady flow file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you understand the basics of project initialization in RAS Commander, you can explore more advanced topics:\n",
        "\n",
        "1. Working with HDF files for result analysis\n",
        "2. Modifying plan, geometry, and flow files\n",
        "3. Running HEC-RAS simulations\n",
        "4. Extracting and visualizing results\n",
        "5. Automating model calibration\n",
        "\n",
        "These topics are covered in other examples and notebooks in the RAS Commander documentation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\02_plan_and_geometry_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Plan and Geometry Operations\n",
        "\n",
        "This notebook demonstrates how to perform operations on HEC-RAS plan and geometry files using the RAS Commander library. We'll explore how to initialize projects, clone plans and geometries, configure parameters, execute plans, and analyze results.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
        "2. **Plan Operations**:\n",
        "   - Clone an existing plan to create a new one\n",
        "   - Configure simulation parameters and intervals\n",
        "   - Set run flags and update descriptions\n",
        "3. **Geometry Operations**:\n",
        "   - Clone a geometry file to create a modified version\n",
        "   - Set the geometry for a plan\n",
        "   - Clear geometry preprocessor files to ensure clean results\n",
        "4. **Flow Operations**:\n",
        "   - Clone unsteady flow files\n",
        "   - Configure flow parameters\n",
        "5. **Plan Computation**: Run the plan with specified settings\n",
        "6. **Results Verification**: Check HDF entries to confirm results were written"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the required libraries for this notebook\n",
        "from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from datetime import datetime  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract an example HEC-RAS project. For this notebook, we'll use the \"Balde Eagle Creek\" project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(bald_eagle_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Initialization\n",
        "\n",
        "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the project folder path and HEC-RAS version as parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan and Geometry Operations in HEC-RAS\n",
        "\n",
        "Before diving into the operations, let's understand what plan and geometry files are in HEC-RAS:\n",
        "\n",
        "- **Plan Files** (`.p*`): Define the simulation parameters including the reference to geometry and flow files, as well as computational settings.\n",
        "- **Geometry Files** (`.g*`): Define the physical characteristics of the river/channel system including cross-sections, 2D areas, and structures.\n",
        "\n",
        "The `RasPlan` and `RasGeo` classes provide methods for working with these files, including:\n",
        "\n",
        "1. Creating new plans and geometries by cloning existing ones\n",
        "2. Modifying simulation parameters and settings\n",
        "3. Associating geometries with plans\n",
        "4. Managing preprocessor files\n",
        "5. Retrieving information from plans and geometries\n",
        "\n",
        "In the following sections, we'll explore these operations in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cloning Plans and Geometries\n",
        "\n",
        "Let's start by cloning a plan to create a new simulation scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone plan \"01\" to create a new plan\n",
        "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Combined Test Plan\")\n",
        "print(f\"New plan created: {new_plan_number}\")\n",
        "\n",
        "# Display updated plan files\n",
        "print(\"\\nUpdated plan files:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Get the path to the new plan file\n",
        "plan_path = RasPlan.get_plan_path(new_plan_number)\n",
        "print(f\"\\nNew plan file path: {plan_path}\")\n",
        "\n",
        "# Let's examine the new plan's details\n",
        "new_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
        "print(f\"\\nNew plan details:\")\n",
        "print(f\"Plan number: {new_plan_number}\")\n",
        "print(f\"Description: {new_plan.get('description', 'No description')}\")\n",
        "print(f\"Short Identifier: {new_plan.get('Short Identifier', 'Not available')}\")\n",
        "print(f\"Geometry file: {new_plan.get('Geom File', 'None')}\")\n",
        "print(f\"File path: {new_plan['full_path']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current plan title and shortid\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the title and shortid to append \" clonedplan\"\n",
        "new_title = f\"{current_title} clonedplan\"\n",
        "new_shortid = f\"{current_shortid} clonedplan\"\n",
        "\n",
        "RasPlan.set_plan_title(new_plan_number, new_title)\n",
        "RasPlan.set_shortid(new_plan_number, new_shortid)\n",
        "\n",
        "print(f\"\\nUpdated plan title: {RasPlan.get_plan_title(new_plan_number)}\")\n",
        "print(f\"Updated plan shortid: {RasPlan.get_shortid(new_plan_number)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current plan title and shortid again to confirm the changes\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's clone a geometry file. This allows us to make modifications to a geometry without affecting the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone geometry \"01\" to create a new geometry file\n",
        "new_geom_number = RasPlan.clone_geom(\"01\")\n",
        "print(f\"New geometry created: {new_geom_number}\")\n",
        "\n",
        "# Display updated geometry files\n",
        "print(\"\\nUpdated geometry files:\")\n",
        "display.display(ras.geom_df)\n",
        "\n",
        "# Get the path to the new geometry file\n",
        "geom_path = RasPlan.get_geom_path(new_geom_number)\n",
        "print(f\"\\nNew geometry file path: {geom_path}\")\n",
        "\n",
        "# Examine the new geometry's details\n",
        "new_geom = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number].squeeze()\n",
        "print(f\"\\nNew geometry details:\")\n",
        "print(f\"Geometry number: {new_geom_number}\")\n",
        "print(f\"Geometry file: {new_geom.get('geom_file', 'Not available')}\")\n",
        "print(f\"File path: {new_geom.get('full_path', 'Not available')}\")\n",
        "print(f\"HDF path: {new_geom.get('hdf_path', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also clone an unsteady flow file to complete our new simulation setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
        "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
        "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
        "\n",
        "# Display updated unsteady flow files\n",
        "print(\"\\nUpdated unsteady flow files:\")\n",
        "display.display(ras.unsteady_df)\n",
        "\n",
        "# Examine the new unsteady flow's details\n",
        "new_unsteady = ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number].iloc[0]\n",
        "print(f\"\\nNew unsteady flow details:\")\n",
        "print(f\"Unsteady number: {new_unsteady_number}\")\n",
        "print(f\"File path: {new_unsteady['full_path']}\")\n",
        "print(f\"Flow Title: {new_unsteady.get('Flow Title', 'Not available')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Associating Files and Setting Parameters\n",
        "\n",
        "Now that we have cloned our plan, geometry, and unsteady flow files, we need to associate them with each other and set various parameters.\n",
        "\n",
        "### Setting Geometry for a Plan\n",
        "\n",
        "Let's associate our new geometry with our new plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the new geometry for the cloned plan\n",
        "updated_geom_df = RasPlan.set_geom(new_plan_number, new_geom_number)\n",
        "plan_path = RasPlan.get_plan_path(new_plan_number, ras_object=ras)\n",
        "print(f\"Updated geometry for plan {new_plan_number} to geometry {new_geom_number}\")\n",
        "print(f\"Plan file path: {plan_path}\")\n",
        "\n",
        "# Let's verify the change\n",
        "updated_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
        "print(f\"\\nVerified that plan {new_plan_number} now uses geometry file: {updated_plan.get('Geom File', 'None')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Unsteady Flow for a Plan\n",
        "\n",
        "Similarly, let's associate our new unsteady flow file with our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set unsteady flow for the cloned plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Updated unsteady flow for plan {new_plan_number} to unsteady flow {new_unsteady_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clearing Geometry Preprocessor Files\n",
        "\n",
        "When working with geometry files, it's important to clear the preprocessor files to ensure clean results. These files (with `.c*` extension) contain computed hydraulic properties that should be recomputed when the geometry changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear geometry preprocessor files for the cloned plan\n",
        "RasGeo.clear_geompre_files(plan_path)\n",
        "print(f\"Cleared geometry preprocessor files for plan {new_plan_number}\")\n",
        "\n",
        "# Check if preprocessor file exists after clearing\n",
        "geom_preprocessor_suffix = '.c' + ''.join(Path(plan_path).suffixes[1:])\n",
        "geom_preprocessor_file = Path(plan_path).with_suffix(geom_preprocessor_suffix)\n",
        "print(f\"Preprocessor file exists after clearing: {geom_preprocessor_file.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Computation Parameters\n",
        "\n",
        "Let's set the computation parameters for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the number of cores to use for the computation\n",
        "RasPlan.set_num_cores(new_plan_number, 2)\n",
        "print(f\"Updated number of cores for plan {new_plan_number} to 2\")\n",
        "\n",
        "# Verify by extracting the value from the plan file\n",
        "cores_value = RasPlan.get_plan_value(new_plan_number, \"UNET D1 Cores\")\n",
        "print(f\"\\nVerified that UNET D1 Cores is set to: {cores_value}\")\n",
        "\n",
        "# Set geometry preprocessor options\n",
        "RasPlan.set_geom_preprocessor(plan_path, run_htab=-1, use_ib_tables=-1)\n",
        "print(f\"Updated geometry preprocessor options for plan {new_plan_number}\")\n",
        "print(f\"- Run HTab: -1 (Force recomputation of geometry tables)\")\n",
        "print(f\"- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\")\n",
        "\n",
        "# Verify by extracting the values from the plan file\n",
        "run_htab_value = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
        "ib_tables_value = RasPlan.get_plan_value(new_plan_number, \"UNET Use Existing IB Tables\")\n",
        "print(f\"\\nVerified setting values:\")\n",
        "print(f\"- Run HTab: {run_htab_value}\")\n",
        "print(f\"- UNET Use Existing IB Tables: {ib_tables_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating Simulation Parameters\n",
        "\n",
        "Now, let's update various simulation parameters for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Update simulation date\n",
        "start_date = datetime(2023, 1, 1, 0, 0)  # January 1, 2023, 00:00\n",
        "end_date = datetime(2023, 1, 5, 23, 59)  # January 5, 2023, 23:59\n",
        "\n",
        "RasPlan.update_simulation_date(new_plan_number, start_date, end_date)\n",
        "print(f\"Updated simulation date for plan {new_plan_number}:\")\n",
        "print(f\"- Start Date: {start_date}\")\n",
        "print(f\"- End Date: {end_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the update\n",
        "sim_date = RasPlan.get_plan_value(new_plan_number, \"Simulation Date\")\n",
        "print(f\"Verified Simulation Date value: {sim_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Update plan intervals\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_plan_number,\n",
        "    computation_interval=\"1MIN\",  # Computational time step\n",
        "    output_interval=\"15MIN\",      # How often results are written\n",
        "    mapping_interval=\"30MIN\"      # How often mapping outputs are created\n",
        ")\n",
        "print(f\"\\nUpdated plan intervals for plan {new_plan_number}:\")\n",
        "print(f\"- Computation Interval: 1MIN\")\n",
        "print(f\"- Output Interval: 15MIN\")\n",
        "print(f\"- Mapping Interval: 30MIN\")\n",
        "\n",
        "# Verify the updates\n",
        "comp_interval = RasPlan.get_plan_value(new_plan_number, \"Computation Interval\")\n",
        "mapping_interval = RasPlan.get_plan_value(new_plan_number, \"Mapping Interval\")\n",
        "print(f\"Verified interval values:\")\n",
        "print(f\"- Computation Interval: {comp_interval}\")\n",
        "print(f\"- Mapping Interval: {mapping_interval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Update run flags\n",
        "RasPlan.update_run_flags(\n",
        "    new_plan_number,\n",
        "    geometry_preprocessor=True,   # Run the geometry preprocessor\n",
        "    unsteady_flow_simulation=True, # Run unsteady flow simulation\n",
        "    post_processor=True,          # Run post-processing\n",
        "    floodplain_mapping=True       # Generate floodplain mapping outputs\n",
        ")\n",
        "print(f\"\\nUpdated run flags for plan {new_plan_number}:\")\n",
        "print(f\"- Geometry Preprocessor: True\")\n",
        "print(f\"- Unsteady Flow Simulation: True\")\n",
        "print(f\"- Post Processor: True\")\n",
        "print(f\"- Floodplain Mapping: True\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the updates\n",
        "run_htab = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
        "run_unet = RasPlan.get_plan_value(new_plan_number, \"Run UNet\")\n",
        "print(f\"Verified run flag values:\")\n",
        "print(f\"- Run HTab (Geometry Preprocessor): {run_htab}\")\n",
        "print(f\"- Run UNet (Unsteady Flow): {run_unet}\")\n",
        "\n",
        "# 4. Update plan description\n",
        "new_description = \"Combined plan with modified geometry and unsteady flow\\nJanuary 2023 simulation\\n1-minute computation interval\\nGeometry and unsteady flow from cloned files\"\n",
        "RasPlan.update_plan_description(new_plan_number, new_description)\n",
        "print(f\"\\nUpdated description for plan {new_plan_number}\")\n",
        "\n",
        "# Read back the description\n",
        "current_description = RasPlan.read_plan_description(new_plan_number)\n",
        "print(f\"Current plan description:\\n{current_description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Plan\n",
        "\n",
        "Now that we have set up all the parameters, let's compute the plan using RasCmdr.compute_plan():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the plan with our configured settings\n",
        "# Note: This may take several minutes depending on the complexity of the model\n",
        "print(f\"Computing plan {new_plan_number}...\")\n",
        "success = RasCmdr.compute_plan(new_plan_number, clear_geompre=True)\n",
        "\n",
        "if success:\n",
        "    print(f\"Plan {new_plan_number} computed successfully\")\n",
        "else:\n",
        "    print(f\"Failed to compute plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verifying Results\n",
        "\n",
        "After computation, we should check if results were written correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refresh the plan entries to ensure we have the latest data\n",
        "ras.plan_df = ras.get_plan_entries()\n",
        "hdf_entries = ras.get_hdf_entries()\n",
        "\n",
        "if not hdf_entries.empty:\n",
        "    print(\"HDF entries for the project:\")\n",
        "    display.display(hdf_entries)\n",
        "    \n",
        "    # Check if our new plan has an HDF file\n",
        "    new_plan_hdf = hdf_entries[hdf_entries['plan_number'] == new_plan_number]\n",
        "    if not new_plan_hdf.empty:\n",
        "        print(f\"\\nPlan {new_plan_number} has a valid HDF results file:\")\n",
        "        print(f\"HDF Path: {new_plan_hdf.iloc[0]['HDF_Results_Path']}\")\n",
        "    else:\n",
        "        print(f\"\\nNo HDF entry found for plan {new_plan_number}\")\n",
        "else:\n",
        "    print(\"No HDF entries found. This could mean the plan hasn't been computed successfully or the results haven't been written yet.\")\n",
        "\n",
        "# Display all plan entries to see their HDF paths\n",
        "print(\"\\nAll plan entries with their HDF paths:\")\n",
        "plan_hdf_info = ras.plan_df[['plan_number', 'HDF_Results_Path']]\n",
        "display.display(plan_hdf_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the plan was computed successfully, we can examine the runtime data and volume accounting from the HDF results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get computation runtime data from HDF\n",
        "print(\"Checking computation runtime data...\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(new_plan_number)\n",
        "\n",
        "if runtime_df is not None and not runtime_df.empty:\n",
        "    print(\"\\nSimulation Runtime Statistics:\")\n",
        "    display.display(runtime_df)\n",
        "    \n",
        "    # Extract key metrics\n",
        "    sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
        "    compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
        "    compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
        "    \n",
        "    print(f\"\\nSimulation Duration: {sim_duration:.2f} seconds\")\n",
        "    print(f\"Computation Time: {compute_time:.5f} hours\")\n",
        "    print(f\"Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
        "else:\n",
        "    print(\"No runtime data found. This may indicate the simulation didn't complete successfully.\")\n",
        "\n",
        "# Get volume accounting data\n",
        "print(\"\\nChecking volume accounting...\")\n",
        "volume_df = HdfResultsPlan.get_volume_accounting(new_plan_number)\n",
        "\n",
        "if volume_df is not None and not isinstance(volume_df, bool):\n",
        "    # Handle volume_df as a dictionary\n",
        "    if isinstance(volume_df, dict):\n",
        "        error_percent = volume_df.get('Error Percent')\n",
        "        if error_percent is not None:\n",
        "            print(f\"\\nFinal Volume Balance Error: {float(error_percent):.8f}%\")\n",
        "            \n",
        "        # Print other key statistics\n",
        "        print(\"\\nDetailed Volume Statistics:\")\n",
        "        print(f\"Volume Starting: {float(volume_df['Volume Starting']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Volume Ending: {float(volume_df['Volume Ending']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Total Inflow: {float(volume_df['Total Boundary Flux of Water In']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "        print(f\"Total Outflow: {float(volume_df['Total Boundary Flux of Water Out']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
        "else:\n",
        "    print(\"No volume accounting data found. This may indicate the simulation didn't complete successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan and Geometry Operations\n",
        "\n",
        "In this notebook, we've covered a comprehensive range of operations on HEC-RAS plan and geometry files using the RAS Commander library:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
        "2. **Plan Operations**:\n",
        "   - Created a new plan by cloning an existing one\n",
        "   - Updated simulation parameters (dates, intervals, etc.)\n",
        "   - Set run flags for different components\n",
        "   - Updated the plan description\n",
        "3. **Geometry Operations**:\n",
        "   - Created a new geometry by cloning an existing one\n",
        "   - Associated the new geometry with our plan\n",
        "   - Cleared geometry preprocessor files\n",
        "4. **Unsteady Flow Operations**:\n",
        "   - Created a new unsteady flow file by cloning an existing one\n",
        "   - Associated it with our plan\n",
        "5. **Computation and Verification**:\n",
        "   - Computed our plan with the specified settings\n",
        "   - Verified the results using HDF entries\n",
        "   - Analyzed runtime statistics and volume accounting\n",
        "\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasPlan`: For plan operations (cloning, setting components, and modifying parameters)\n",
        "- `RasGeo`: For geometry operations (cloning, clearing preprocessor files)\n",
        "- `RasCmdr`: For executing HEC-RAS simulations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further enhance your HEC-RAS automation, consider exploring:\n",
        "\n",
        "1. **Parameter Sweeps**: Create and run multiple plans with varying parameters\n",
        "2. **Parallel Computations**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
        "3. **Advanced Results Analysis**: Use the HDF classes to extract and analyze specific model results\n",
        "4. **Spatial Visualization**: Create maps and plots of simulation results\n",
        "5. **Model Calibration**: Automate comparison between model results and observations\n",
        "\n",
        "The RAS Commander library provides a powerful framework for automating and streamlining your HEC-RAS workflows, enabling more efficient hydraulic modeling and analyses."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\03_unsteady_flow_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies\n",
        "\n",
        "generate_plots = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Unsteady Flow Files in HEC-RAS\n",
        "\n",
        "Unsteady flow files (`.u*` files) in HEC-RAS define the time-varying boundary conditions that drive dynamic simulations. These include:\n",
        "\n",
        "- **Flow Hydrographs**: Time-series of flow values at model boundaries\n",
        "- **Stage Hydrographs**: Time-series of water surface elevations\n",
        "- **Lateral Inflows**: Distributed inflows along a reach\n",
        "- **Gate Operations**: Time-series of gate settings\n",
        "- **Meteorological Data**: Rainfall, evaporation, and other meteorological inputs\n",
        "\n",
        "The `RasUnsteady` class in RAS Commander provides methods for working with these files, including extracting boundaries, reading tables, and modifying parameters.\n",
        "\n",
        "Let's set up our working directory and define paths to example projects:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract an example HEC-RAS project with unsteady flow files. For this notebook, we'll use the \"Balde Eagle Creek\" project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the following parameters:\n",
        "\n",
        "- `ras_project_folder`: Path to the HEC-RAS project folder (required)\n",
        "- `ras_version`: HEC-RAS version (e.g., \"6.6\") or path to Ras.exe (required first time)\n",
        "\n",
        "This function initializes the global `ras` object that we'll use for the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "# This function returns a RAS object, but also updates the global 'ras' object\n",
        "# Parameters:\n",
        "#   - ras_project_folder: Path to the HEC-RAS project folder\n",
        "#   - ras_version: HEC-RAS version or path to Ras.exe\n",
        "\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the unsteady flow files in the project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Geometry Data (geom_df):\")\n",
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Unsteady Flow Data (unsteady_df):\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHEC-RAS Project Boundary Data (boundaries_df):\")\n",
        "ras.boundaries_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasUnsteady Class\n",
        "\n",
        "The `RasUnsteady` class provides functionality for working with HEC-RAS unsteady flow files (`.u*` files). Key operations include:\n",
        "\n",
        "1. **Extracting Boundary Conditions**: Read and parse boundary conditions from unsteady flow files\n",
        "2. **Modifying Flow Titles**: Update descriptive titles for unsteady flow scenarios\n",
        "3. **Managing Restart Settings**: Configure restart file options for continuing simulations\n",
        "4. **Working with Tables**: Extract, modify, and update flow tables\n",
        "\n",
        "Most methods in this class are static and work with the global `ras` object by default, though you can also pass in a custom RAS object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Extract Boundary Conditions and Tables\n",
        "\n",
        "The `extract_boundary_and_tables()` method from the `RasUnsteady` class allows us to extract boundary conditions and their associated tables from an unsteady flow file.\n",
        "\n",
        "Parameters for `RasUnsteady.extract_boundary_and_tables()`:\n",
        "- `unsteady_file` (str): Path to the unsteady flow file\n",
        "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
        "\n",
        "Returns:\n",
        "- `pd.DataFrame`: DataFrame containing boundary conditions and their associated tables\n",
        "\n",
        "Let's see how this works with our example project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file \"02\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"02\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")\n",
        "\n",
        "# Extract boundary conditions and tables\n",
        "boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\n",
        "print(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Print Boundaries and Tables\n",
        "\n",
        "The `print_boundaries_and_tables()` method provides a formatted display of the boundary conditions and their associated tables. This method doesn't return anything; it just prints the information in a readable format.\n",
        "\n",
        "Parameters for `RasUnsteady.print_boundaries_and_tables()`:\n",
        "- `boundaries_df` (pd.DataFrame): DataFrame containing boundary conditions from `extract_boundary_and_tables()`\n",
        "\n",
        "Let's use this method to get a better understanding of our boundary conditions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the boundaries and tables in a formatted way\n",
        "print(\"Detailed boundary conditions and tables:\")\n",
        "RasUnsteady.print_boundaries_and_tables(boundaries_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Boundary Condition Types\n",
        "\n",
        "The output above shows the different types of boundary conditions in our unsteady flow file. Let's understand what each type means:\n",
        "\n",
        "1. **Flow Hydrograph**: A time series of flow values (typically in cfs or cms) entering the model at a specific location. These are used at upstream boundaries or internal points where flow enters the system.\n",
        "\n",
        "2. **Stage Hydrograph**: A time series of water surface elevations (typically in ft or m) that define the downstream boundary condition.\n",
        "\n",
        "3. **Gate Openings**: Time series of gate settings (typically height in ft or m) for hydraulic structures such as spillways, sluice gates, or other control structures.\n",
        "\n",
        "4. **Lateral Inflow Hydrograph**: Flow entering the system along a reach, not at a specific point. This can represent tributary inflows, overland flow, or other distributed inputs.\n",
        "\n",
        "5. **Normal Depth**: A boundary condition where the water surface slope is assumed to equal the bed slope. This is represented by a friction slope value.\n",
        "\n",
        "Let's look at a specific boundary condition in more detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine the first boundary condition in more detail\n",
        "if not boundaries_df.empty:\n",
        "    first_boundary = boundaries_df.iloc[0]\n",
        "    print(f\"Detailed look at boundary condition {1}:\")\n",
        "    \n",
        "    # Print boundary location components\n",
        "    print(f\"\\nBoundary Location:\")\n",
        "    print(f\"  River Name: {first_boundary.get('River Name', 'N/A')}\")\n",
        "    print(f\"  Reach Name: {first_boundary.get('Reach Name', 'N/A')}\")\n",
        "    print(f\"  River Station: {first_boundary.get('River Station', 'N/A')}\")\n",
        "    print(f\"  Storage Area Name: {first_boundary.get('Storage Area Name', 'N/A')}\")\n",
        "    \n",
        "    # Print boundary condition type and other properties\n",
        "    print(f\"\\nBoundary Properties:\")\n",
        "    print(f\"  Boundary Type: {first_boundary.get('bc_type', 'N/A')}\")\n",
        "    print(f\"  DSS File: {first_boundary.get('DSS File', 'N/A')}\")\n",
        "    print(f\"  Use DSS: {first_boundary.get('Use DSS', 'N/A')}\")\n",
        "    \n",
        "    # Print table statistics if available\n",
        "    if 'Tables' in first_boundary and isinstance(first_boundary['Tables'], dict):\n",
        "        print(f\"\\nTable Information:\")\n",
        "        for table_name, table_df in first_boundary['Tables'].items():\n",
        "            print(f\"  {table_name}: {len(table_df)} values\")\n",
        "            if not table_df.empty:\n",
        "                print(f\"    Min Value: {table_df['Value'].min()}\")\n",
        "                print(f\"    Max Value: {table_df['Value'].max()}\")\n",
        "                print(f\"    First 5 Values: {table_df['Value'].head(5).tolist()}\")\n",
        "else:\n",
        "    print(\"No boundary conditions found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Update Flow Title\n",
        "\n",
        "The flow title in an unsteady flow file provides a description of the simulation scenario. The `update_flow_title()` method allows us to modify this title.\n",
        "\n",
        "Parameters for `RasUnsteady.update_flow_title()`:\n",
        "- `unsteady_file` (str): Full path to the unsteady flow file\n",
        "- `new_title` (str): New flow title (max 24 characters)\n",
        "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
        "\n",
        "Let's clone an unsteady flow file and update its title:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
        "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
        "print(f\"New unsteady flow created: {new_unsteady_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_unsteady_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to the new unsteady flow file\n",
        "new_unsteady_file = RasPlan.get_unsteady_path(new_unsteady_number)\n",
        "print(f\"New unsteady flow file path: {new_unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_unsteady_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current flow title\n",
        "current_title = None\n",
        "for _, row in ras.unsteady_df.iterrows():\n",
        "    if row['unsteady_number'] == new_unsteady_number and 'Flow Title' in row:\n",
        "        current_title = row['Flow Title']\n",
        "        break\n",
        "print(f\"Current flow title: {current_title}\")\n",
        "\n",
        "# Update the flow title\n",
        "new_title = \"Modified Flow Scenario\"\n",
        "RasUnsteady.update_flow_title(new_unsteady_file, new_title)\n",
        "print(f\"Updated flow title to: {new_title}\")\n",
        "\n",
        "# Refresh unsteady flow information to see the change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Review unsteady flow information to see the change\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Working with Flow Tables\n",
        "\n",
        "Flow tables in unsteady flow files contain the time-series data for boundary conditions. Let's explore how to extract and work with these tables using some of the advanced methods from the `RasUnsteady` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific tables from the unsteady flow file\n",
        "all_tables = RasUnsteady.extract_tables(new_unsteady_file)\n",
        "print(f\"Extracted {len(all_tables)} tables from the unsteady flow file.\")\n",
        "\n",
        "# Let's look at the available table names\n",
        "print(\"\\nAvailable tables:\")\n",
        "for table_name in all_tables.keys():\n",
        "    print(f\"  {table_name}\")\n",
        "\n",
        "# Select the first table for detailed analysis\n",
        "if all_tables and len(all_tables) > 0:\n",
        "    first_table_name = list(all_tables.keys())[0]\n",
        "    first_table = all_tables[first_table_name]\n",
        "    \n",
        "    print(f\"\\nDetailed look at table '{first_table_name}':\")\n",
        "    print(f\"  Number of values: {len(first_table)}\")\n",
        "    print(f\"  Min value: {first_table['Value'].min()}\")\n",
        "    print(f\"  Max value: {first_table['Value'].max()}\")\n",
        "    print(f\"  Mean value: {first_table['Value'].mean():.2f}\")\n",
        "    print(f\"  First 10 values: {first_table['Value'].head(10).tolist()}\")\n",
        "    \n",
        "    # Create a visualization of the table values\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(first_table['Value'].values)\n",
        "        plt.title(f\"{first_table_name} Values\")\n",
        "        plt.xlabel('Time Step')\n",
        "        plt.ylabel('Value')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create visualization: {e}\")\n",
        "else:\n",
        "    print(\"No tables found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Modifying Flow Tables\n",
        "\n",
        "Now let's demonstrate how to modify a flow table and write it back to the unsteady flow file. For this example, we'll scale all the values in a table by a factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling existing values down by a 0.75 scale factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, identify tables in the unsteady flow file\n",
        "tables = RasUnsteady.identify_tables(open(new_unsteady_file, 'r').readlines())\n",
        "print(f\"Identified {len(tables)} tables in the unsteady flow file.\")\n",
        "\n",
        "# Let's look at the first flow hydrograph table\n",
        "flow_hydrograph_tables = [t for t in tables if t[0] == 'Flow Hydrograph=']\n",
        "if flow_hydrograph_tables:\n",
        "    table_name, start_line, end_line = flow_hydrograph_tables[0]\n",
        "    print(f\"\\nSelected table: {table_name}\")\n",
        "    print(f\"  Start line: {start_line}\")\n",
        "    print(f\"  End line: {end_line}\")\n",
        "    \n",
        "    # Parse the table\n",
        "    lines = open(new_unsteady_file, 'r').readlines()\n",
        "    table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
        "    print(f\"\\nOriginal table statistics:\")\n",
        "    print(f\"  Number of values: {len(table_df)}\")\n",
        "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
        "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
        "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
        "    \n",
        "    # Modify the table - let's scale all values by 75%\n",
        "    scale_factor = 0.75\n",
        "    table_df['Value'] = table_df['Value'] * scale_factor\n",
        "    print(f\"\\nModified table statistics (scaled by {scale_factor}):\")\n",
        "    print(f\"  Number of values: {len(table_df)}\")\n",
        "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
        "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
        "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
        "    \n",
        "    # Write the modified table back to the file\n",
        "    RasUnsteady.write_table_to_file(new_unsteady_file, table_name, table_df, start_line)\n",
        "    print(f\"\\nUpdated table written back to the unsteady flow file.\")\n",
        "    \n",
        "    # Re-read the table to verify changes\n",
        "    lines = open(new_unsteady_file, 'r').readlines()\n",
        "    updated_table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
        "    print(f\"\\nVerified updated table statistics:\")\n",
        "    print(f\"  Number of values: {len(updated_table_df)}\")\n",
        "    print(f\"  Min value: {updated_table_df['Value'].min()}\")\n",
        "    print(f\"  Max value: {updated_table_df['Value'].max()}\")\n",
        "    print(f\"  First 5 values: {updated_table_df['Value'].head(5).tolist()}\")\n",
        "else:\n",
        "    print(\"No flow hydrograph tables found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific tables from the unsteady flow file\n",
        "all_tables = RasUnsteady.extract_tables(new_unsteady_file)\n",
        "\n",
        "# Get the updated flow hydrograph table\n",
        "flow_hydrograph_tables = [t for t in all_tables.keys() if 'Flow Hydrograph=' in t]\n",
        "if flow_hydrograph_tables:\n",
        "    table_name = flow_hydrograph_tables[0]\n",
        "    table_df = all_tables[table_name]\n",
        "    \n",
        "    # Create visualization of the updated flow values\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(table_df['Value'].values, 'b-', label='Updated Flow')\n",
        "    plt.title('Updated Flow Hydrograph')\n",
        "    plt.xlabel('Time Step') \n",
        "    plt.ylabel('Flow (cfs)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nUpdated flow hydrograph statistics:\")\n",
        "    print(f\"  Number of values: {len(table_df)}\")\n",
        "    print(f\"  Min flow: {table_df['Value'].min():.1f} cfs\")\n",
        "    print(f\"  Max flow: {table_df['Value'].max():.1f} cfs\")\n",
        "    print(f\"  Mean flow: {table_df['Value'].mean():.1f} cfs\")\n",
        "else:\n",
        "    print(\"No flow hydrograph tables found in the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute Plan 01 to generate model results\n",
        "\n",
        "RasCmdr.compute_plan(\"01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries as xarray dataset\n",
        "xsec_results_xr_plan1 = HdfResultsXsec.get_xsec_timeseries(\"01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr_plan1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print time series for specific cross section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
        "\n",
        "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
        "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
        "    print(f\"\\n{var}:\")\n",
        "    print(f\"Plan 1:\")\n",
        "    print(xsec_results_xr_plan1[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "\n",
        "\n",
        "# Create time series plots\n",
        "if generate_plots:\n",
        "\n",
        "    # Create a figure for each variable\n",
        "    variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
        "\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        \n",
        "        # Convert time values to datetime if needed\n",
        "        time_values1 = pd.to_datetime(xsec_results_xr_plan1.time.values)\n",
        "        values1 = xsec_results_xr_plan1[var].sel(cross_section=target_xs).values\n",
        "\n",
        "        \n",
        "        # Plot both plans\n",
        "        plt.plot(time_values1, values1, '-', linewidth=2, label='Plan 1')\n",
        "        \n",
        "        plt.title(f'{var} at {target_xs}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(var.replace('_', ' '))\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Force display\n",
        "        plt.draw()\n",
        "        plt.pause(0.1)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Applying the Updated Unsteady Flow to a New Plan\n",
        "\n",
        "Now that we've modified an unsteady flow file, let's create a plan that uses it, and compute the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone an existing plan\n",
        "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Modified Flow Test\")\n",
        "print(f\"New plan created: {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_plan_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current plan title and shortid\n",
        "current_title = RasPlan.get_plan_title(new_plan_number)\n",
        "current_shortid = RasPlan.get_shortid(new_plan_number)\n",
        "\n",
        "print(f\"Current plan title: {current_title}\")\n",
        "print(f\"Current plan shortid: {current_shortid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the title and shortid to append \" clonedplan\"\n",
        "new_title = f\"{current_title} 0.75 Flow Scale Factor\"\n",
        "new_shortid = f\"{current_shortid} 0.75 FSF\"\n",
        "\n",
        "RasPlan.set_plan_title(new_plan_number, new_title)\n",
        "RasPlan.set_shortid(new_plan_number, new_shortid)\n",
        "\n",
        "print(f\"\\nUpdated plan title: {RasPlan.get_plan_title(new_plan_number)}\")\n",
        "print(f\"Updated plan shortid: {RasPlan.get_shortid(new_plan_number)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print new_unsteady_number again as a reminder of it's current value\n",
        "new_unsteady_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the modified unsteady flow for the new plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Set unsteady flow {new_unsteady_number} for plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the modified unsteady flow for the new plan\n",
        "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
        "print(f\"Set unsteady flow {new_unsteady_number} for plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to the new plan file\n",
        "new_plan_path = RasPlan.get_plan_path(new_plan_number)\n",
        "\n",
        "# Print contents of new plan file to confirm changes\n",
        "# Read and display the contents of the plan file\n",
        "with open(new_plan_path, 'r') as f:\n",
        "    plan_contents = f.read()\n",
        "print(f\"Contents of plan file {new_plan_number}:\")\n",
        "print(plan_contents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the plan description\n",
        "new_description = \"Test plan using modified unsteady flow\\nFlow scaled to 75% of original\\nWith restart file enabled\"\n",
        "RasPlan.update_plan_description(new_plan_number, new_description)\n",
        "print(f\"Updated plan description for plan {new_plan_number}\")\n",
        "\n",
        "# Set computation options\n",
        "RasPlan.set_num_cores(new_plan_number, 2)\n",
        "\n",
        "# Consider any other changes you want to make at this step, such as computation intervals etc: \n",
        "# RasPlan.update_plan_intervals(\n",
        "#    new_plan_number,\n",
        "#    computation_interval=\"1MIN\",\n",
        "#    output_interval=\"15MIN\",\n",
        "#    mapping_interval=\"1HOUR\"\n",
        "#)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the plan\n",
        "print(f\"\\nComputing plan {new_plan_number} with modified unsteady flow...\")\n",
        "success = RasCmdr.compute_plan(new_plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if success:\n",
        "    print(f\"Plan {new_plan_number} computed successfully\")\n",
        "    \n",
        "    # Check the results path\n",
        "    results_path = RasPlan.get_results_path(new_plan_number)\n",
        "    if results_path:\n",
        "        print(f\"Results available at: {results_path}\")\n",
        "        \n",
        "        # If it exists, get its size\n",
        "        results_file = Path(results_path)\n",
        "        if results_file.exists():\n",
        "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
        "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"No results found.\")\n",
        "else:\n",
        "    print(f\"Failed to compute plan {new_plan_number}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show updated plan_df dataframe, which should show the HDF results files\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get results for Plan 03 and Compare with Plan 01's results for the specified Cross Section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries as xarray dataset\n",
        "xsec_results_xr_plan2 = HdfResultsXsec.get_xsec_timeseries(\"03\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr_plan2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print time series for specific cross section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
        "\n",
        "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
        "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
        "    print(f\"\\n{var}:\")\n",
        "    print(f\"Plan 1:\")\n",
        "    print(xsec_results_xr_plan1[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "    print(f\"Plan 2:\")\n",
        "    print(xsec_results_xr_plan2[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "\n",
        "# Create time series plots\n",
        "if generate_plots:\n",
        "\n",
        "    # Create a figure for each variable\n",
        "    variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
        "\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        \n",
        "        # Convert time values to datetime if needed\n",
        "        time_values1 = pd.to_datetime(xsec_results_xr_plan1.time.values)\n",
        "        time_values2 = pd.to_datetime(xsec_results_xr_plan2.time.values)\n",
        "        values1 = xsec_results_xr_plan1[var].sel(cross_section=target_xs).values\n",
        "        values2 = xsec_results_xr_plan2[var].sel(cross_section=target_xs).values\n",
        "        \n",
        "        # Get plan titles from plan_df\n",
        "        plan1_title = ras.plan_df.loc[ras.plan_df['plan_number'] == '01', 'Plan Title'].iloc[0]\n",
        "        plan2_title = ras.plan_df.loc[ras.plan_df['plan_number'] == '03', 'Plan Title'].iloc[0]\n",
        "        \n",
        "        # Plot both plans with titles\n",
        "        plt.plot(time_values1, values1, '-', linewidth=2, label=f'{plan1_title} (Plan 01)')\n",
        "        plt.plot(time_values2, values2, '--', linewidth=2, label=f'{plan2_title} (Plan 03)')\n",
        "        \n",
        "        plt.title(f'{var} at {target_xs}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(var.replace('_', ' '))\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Force display\n",
        "        plt.draw()\n",
        "        plt.pause(0.1)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Unsteady Flow Operations\n",
        "\n",
        "In this notebook, we've covered the following unsteady flow operations using RAS Commander:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
        "2. **Boundary Extraction**: We extracted boundary conditions and tables from unsteady flow files\n",
        "3. **Boundary Analysis**: We inspected and understood boundary condition structures\n",
        "4. **Flow Title Updates**: We modified the title of an unsteady flow file\n",
        "5. **Restart Settings**: We configured restart file settings for continuing simulations\n",
        "6. **Table Extraction**: We extracted flow tables for analysis\n",
        "7. **Table Modification**: We modified a flow table and wrote it back to the file\n",
        "8. **Application**: We created a plan using our modified unsteady flow and computed results\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasUnsteady.extract_boundary_and_tables()`: Extract boundary conditions and tables\n",
        "- `RasUnsteady.print_boundaries_and_tables()`: Display formatted boundary information\n",
        "- `RasUnsteady.update_flow_title()`: Modify the flow title\n",
        "- `RasUnsteady.update_restart_settings()`: Configure restart options\n",
        "- `RasUnsteady.extract_tables()`: Extract tables from unsteady flow files\n",
        "- `RasUnsteady.identify_tables()`: Identify table locations in file\n",
        "- `RasUnsteady.parse_fixed_width_table()`: Parse fixed-width tables\n",
        "- `RasUnsteady.write_table_to_file()`: Write modified tables back to file\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further explore unsteady flow operations with RAS Commander, consider:\n",
        "\n",
        "1. **Advanced Flow Modifications**: Create scripts that systematically modify flow hydrographs\n",
        "2. **Sensitivity Analysis**: Create variations of unsteady flows to assess model sensitivity\n",
        "3. **Batch Processing**: Process multiple unsteady flow files for scenario analysis\n",
        "4. **Custom Boundary Conditions**: Create unsteady flows from external data sources\n",
        "5. **Results Analysis**: Compare results from different unsteady flow scenarios\n",
        "\n",
        "These advanced topics can be explored by building on the foundation established in this notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\04_multiple_project_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Multiple Project Operations\n",
        "\n",
        "This notebook demonstrates how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. This advanced workflow is useful for comparing different river systems, running scenario analyses across multiple watersheds, or managing a suite of related models.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Multiple Project Initialization**: Initialize and manage multiple HEC-RAS projects simultaneously\n",
        "2. **Cross-Project Operations**: Clone and modify plans across different projects\n",
        "3. **Parallel Execution**: Run computations for multiple projects in parallel\n",
        "4. **Resource Management**: Optimize computing resources when working with multiple models\n",
        "5. **Results Comparison**: Analyze and compare results from different projects\n",
        "6. **Advanced Project Workflow**: Build a comprehensive multi-project workflow\n",
        "\n",
        "Let's begin by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Working Environment\n",
        "\n",
        "Let's set up our working directory and check the number of available CPU cores. Since we'll be running multiple projects in parallel, it's important to understand our system resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific projects we'll use in this tutorial\n",
        "# This will download them if not present and extract them to the example_projects folder\n",
        "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
        "print(extracted_paths)\n",
        "\n",
        "# Get the parent directory of the first extracted path as our examples directory\n",
        "examples_dir = extracted_paths[0].parent\n",
        "print(f\"Examples directory: {examples_dir}\")\n",
        "\n",
        "\n",
        "# Define paths to the extracted projects\n",
        "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
        "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
        "muncie_path = examples_dir / \"Muncie\"\n",
        "\n",
        "# Verify the paths exist\n",
        "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
        "    print(f\"Path {path} exists: {path.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define computation output paths\n",
        "bald_eagle_compute_folder = examples_dir / \"compute_bald_eagle\"\n",
        "muncie_compute_folder = examples_dir / \"compute_muncie\"\n",
        "\n",
        "# Check system resources\n",
        "cpu_count = psutil.cpu_count(logical=True)\n",
        "physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
        "\n",
        "print(f\"System Resources:\")\n",
        "print(f\"- {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
        "print(f\"- {available_memory_gb:.1f} GB available memory\")\n",
        "print(f\"For multiple HEC-RAS projects, a good rule of thumb is:\")\n",
        "print(f\"- Assign 2-4 cores per project\")\n",
        "print(f\"- Allocate at least 2-4 GB of RAM per project\")\n",
        "print(f\"Based on your system, you could reasonably run {min(physical_cpu_count//2, int(available_memory_gb//3))} projects simultaneously.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Multiple RAS Project Management\n",
        "\n",
        "When working with multiple HEC-RAS projects in RAS Commander, there are two important concepts to understand:\n",
        "\n",
        "1. **The Global 'ras' Object**: By default, RAS Commander maintains a global `ras` object that represents the currently active project. This is convenient for simple scripts.\n",
        "\n",
        "2. **Custom RAS Objects**: For multiple projects, you'll create separate RAS objects for each project. These custom objects store project-specific data and are passed to RAS Commander functions using the `ras_object` parameter.\n",
        "\n",
        "### Best Practices for Multiple Project Management\n",
        "\n",
        "- **Name Your Objects Clearly**: Use descriptive variable names for your RAS objects (e.g., `bald_eagle_ras`, `muncie_ras`)\n",
        "- **Be Consistent**: Always pass the appropriate RAS object to functions when working with multiple projects\n",
        "- **Avoid Using Global 'ras'**: When working with multiple projects, avoid using the global `ras` object to prevent confusion\n",
        "- **Separate Compute Folders**: Use separate computation folders for each project\n",
        "- **Manage Resources**: Be mindful of CPU and memory usage when running multiple projects in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Projects\n",
        "\n",
        "We'll use the `RasExamples` class to download and extract two example HEC-RAS projects: \"Balde Eagle Creek\" and \"Muncie\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete existing project if it exists to ensure a clean start\n",
        "if examples_dir.exists():\n",
        "    shutil.rmtree(examples_dir)\n",
        "    print(f\"Removed existing example projects directory: {examples_dir}\")\n",
        "\n",
        "# Create a RasExamples instance\n",
        "ras_examples = RasExamples()\n",
        "\n",
        "# Extract the example projects\n",
        "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\", \"Muncie\"])\n",
        "print(f\"Extracted projects to:\")\n",
        "for path in extracted_paths:\n",
        "    print(f\"- {path}\")\n",
        "\n",
        "# Verify the paths exist\n",
        "print(f\"\\nBald Eagle Creek project exists: {bald_eagle_path.exists()}\")\n",
        "print(f\"Muncie project exists: {muncie_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Multiple Projects\n",
        "\n",
        "Let's initialize both HEC-RAS projects. Instead of using the global `ras` object, we'll create separate RAS objects for each project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize both projects with their own RAS objects\n",
        "bald_eagle_ras = RasPrj()\n",
        "init_ras_project(bald_eagle_path, \"6.6\", ras_object=bald_eagle_ras)\n",
        "print(f\"Initialized Bald Eagle Creek project: {bald_eagle_ras.project_name}\")\n",
        "\n",
        "muncie_ras = RasPrj()\n",
        "init_ras_project(muncie_path, \"6.6\", ras_object=muncie_ras)\n",
        "print(f\"Initialized Muncie project: {muncie_ras.project_name}\")\n",
        "\n",
        "# Display available plans in each project\n",
        "print(\"\\nAvailable plans in Bald Eagle Creek project:\")\n",
        "display.display(bald_eagle_ras.plan_df)\n",
        "\n",
        "print(\"\\nAvailable plans in Muncie project:\")\n",
        "display.display(muncie_ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone Plans in Each Project\n",
        "\n",
        "Now, let's clone a plan in each project, giving them custom short identifiers. This demonstrates how to perform operations on multiple projects independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone plans with custom short identifiers\n",
        "new_bald_eagle_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=bald_eagle_ras)\n",
        "print(f\"Created new plan {new_bald_eagle_plan} in Bald Eagle Creek project\")\n",
        "\n",
        "new_muncie_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=muncie_ras)\n",
        "print(f\"Created new plan {new_muncie_plan} in Muncie project\")\n",
        "\n",
        "# Display the updated plan dataframes\n",
        "print(\"\\nUpdated plans in Bald Eagle Creek project:\")\n",
        "bald_eagle_ras.plan_df = bald_eagle_ras.get_plan_entries()  # Refresh the plan dataframe\n",
        "display.display(bald_eagle_ras.plan_df)\n",
        "\n",
        "print(\"\\nUpdated plans in Muncie project:\")\n",
        "muncie_ras.plan_df = muncie_ras.get_plan_entries()  # Refresh the plan dataframe\n",
        "display.display(muncie_ras.plan_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Plans for Both Projects\n",
        "\n",
        "Let's configure the plans for both projects, setting geometry, number of cores, and other parameters. This demonstrates how to customize plans for different projects using the same code structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the Bald Eagle Creek plan\n",
        "print(\"Configuring Bald Eagle Creek plan:\")\n",
        "RasPlan.set_geom(new_bald_eagle_plan, \"01\", ras_object=bald_eagle_ras)\n",
        "RasPlan.set_num_cores(new_bald_eagle_plan, 2, ras_object=bald_eagle_ras)\n",
        "\n",
        "# Update description and intervals\n",
        "description = \"Multi-project demonstration plan\\nBald Eagle Creek project\\nConfigured for parallel execution\"\n",
        "RasPlan.update_plan_description(new_bald_eagle_plan, description, ras_object=bald_eagle_ras)\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_bald_eagle_plan, \n",
        "    computation_interval=\"10SEC\", \n",
        "    output_interval=\"5MIN\", \n",
        "    ras_object=bald_eagle_ras\n",
        ")\n",
        "print(\"Successfully configured Bald Eagle Creek plan\")\n",
        "\n",
        "# Configure the Muncie plan\n",
        "print(\"\\nConfiguring Muncie plan:\")\n",
        "RasPlan.set_geom(new_muncie_plan, \"01\", ras_object=muncie_ras)\n",
        "RasPlan.set_num_cores(new_muncie_plan, 2, ras_object=muncie_ras)\n",
        "\n",
        "# Update description and intervals\n",
        "description = \"Multi-project demonstration plan\\nMuncie project\\nConfigured for parallel execution\"\n",
        "RasPlan.update_plan_description(new_muncie_plan, description, ras_object=muncie_ras)\n",
        "RasPlan.update_plan_intervals(\n",
        "    new_muncie_plan, \n",
        "    computation_interval=\"10SEC\", \n",
        "    output_interval=\"5MIN\", \n",
        "    ras_object=muncie_ras\n",
        ")\n",
        "print(\"Successfully configured Muncie plan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Compute Folders for Both Projects\n",
        "\n",
        "Now, let's create separate compute folders for each project. This allows us to run the computations separately and in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create compute folders or clean them if they already exist\n",
        "for folder in [bald_eagle_compute_folder, muncie_compute_folder]:\n",
        "    if folder.exists():\n",
        "        shutil.rmtree(folder)\n",
        "        print(f\"Removed existing compute folder: {folder}\")\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created compute folder: {folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Define Project Execution Function\n",
        "\n",
        "Let's define a function to execute plans for each project, which we can run in parallel. This function will handle plan execution, timing, and provide detailed status updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_plan(plan_number, ras_object, compute_folder, project_name):\n",
        "    \"\"\"\n",
        "    Execute a HEC-RAS plan and return detailed information about the execution.\n",
        "    \n",
        "    Args:\n",
        "        plan_number (str): The plan number to execute\n",
        "        ras_object: The RAS project object\n",
        "        compute_folder (Path): Folder where computation will be performed\n",
        "        project_name (str): A descriptive name for the project\n",
        "        \n",
        "    Returns:\n",
        "        dict: Detailed information about the execution\n",
        "    \"\"\"\n",
        "    print(f\"Starting execution of plan {plan_number} for {project_name}...\")\n",
        "    \n",
        "    # Record start time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Execute the plan in the compute folder\n",
        "    success = RasCmdr.compute_plan(\n",
        "        plan_number=plan_number, \n",
        "        ras_object=ras_object, \n",
        "        dest_folder=compute_folder,\n",
        "        clear_geompre=True\n",
        "    )\n",
        "    \n",
        "    # Record end time and calculate duration\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    # Determine if results were created\n",
        "    result_path = None\n",
        "    result_size = None\n",
        "    \n",
        "    try:\n",
        "        # Initialize a temporary RAS object in the compute folder to check results\n",
        "        compute_ras = init_ras_project(compute_folder, ras_object.ras_exe_path)\n",
        "        result_path = RasPlan.get_results_path(plan_number, ras_object=compute_ras)\n",
        "        \n",
        "        if result_path:\n",
        "            result_file = Path(result_path)\n",
        "            if result_file.exists():\n",
        "                result_size = result_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking results for {project_name}: {e}\")\n",
        "    \n",
        "    # Build result information\n",
        "    result_info = {\n",
        "        \"project_name\": project_name,\n",
        "        \"plan_number\": plan_number,\n",
        "        \"success\": success,\n",
        "        \"duration\": duration,\n",
        "        \"compute_folder\": str(compute_folder),\n",
        "        \"result_path\": str(result_path) if result_path else None,\n",
        "        \"result_size_mb\": result_size,\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    \n",
        "    print(f\"Completed execution of plan {plan_number} for {project_name} in {duration:.2f} seconds\")\n",
        "    return result_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Execute Plans for Both Projects in Parallel\n",
        "\n",
        "Now, let's run both projects in parallel using a `ThreadPoolExecutor`. This allows us to utilize our system resources efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing plans for both projects in parallel...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Define the execution tasks\n",
        "execution_tasks = [\n",
        "    (new_bald_eagle_plan, bald_eagle_ras, bald_eagle_compute_folder, \"Bald Eagle Creek\"),\n",
        "    (new_muncie_plan, muncie_ras, muncie_compute_folder, \"Muncie\")\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Execute the plans in parallel using ThreadPoolExecutor\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "    futures = [\n",
        "        executor.submit(execute_plan, *task)\n",
        "        for task in execution_tasks\n",
        "    ]\n",
        "    \n",
        "    # Collect results as they complete\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            result = future.result()\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Execution error: {e}\")\n",
        "\n",
        "print(\"\\nAll executions complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analyze Results\n",
        "\n",
        "Let's analyze the results from both project executions, comparing execution times, result sizes, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the results table\n",
        "print(\"Execution Results Summary:\")\n",
        "display.display(results_df[['project_name', 'plan_number', 'success', 'duration', 'result_size_mb']])\n",
        "\n",
        "# Create a bar chart for execution times\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(results_df['project_name'], results_df['duration'], color=['blue', 'green'])\n",
        "plt.title('Execution Time by Project')\n",
        "plt.xlabel('Muncie Plan 02 vs Bald Eagle Creek Plan 02\\n (2 separate projects, for demonstration purposes only)')\n",
        "plt.ylabel('Execution Time (seconds)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add duration values on top of the bars\n",
        "for i, duration in enumerate(results_df['duration']):\n",
        "    plt.text(i, duration + 5, f\"{duration:.1f}s\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# If we have result sizes, create a chart for those as well\n",
        "if results_df['result_size_mb'].notna().any():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(results_df['project_name'], results_df['result_size_mb'], color=['orange', 'purple'])\n",
        "    plt.title('Result File Size by Project')\n",
        "    plt.xlabel('Project')\n",
        "    plt.ylabel('Result Size (MB)')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Add size values on top of the bars\n",
        "    for i, size in enumerate(results_df['result_size_mb']):\n",
        "        if pd.notna(size):\n",
        "            plt.text(i, size + 2, f\"{size:.1f} MB\", ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Compare Two HEC-RAS Projects\n",
        "\n",
        "Let's create a utility function to compare the structures of the two HEC-RAS projects. This helps us understand the differences between the projects we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_project_structures(ras_object1, name1, ras_object2, name2):\n",
        "    \"\"\"\n",
        "    Compare the structures of two HEC-RAS projects and display differences.\n",
        "    \"\"\"\n",
        "    # Refresh all dataframes to ensure we have the latest data\n",
        "    ras_object1.plan_df = ras_object1.get_plan_entries()\n",
        "    ras_object1.geom_df = ras_object1.get_geom_entries()\n",
        "    ras_object1.flow_df = ras_object1.get_flow_entries()\n",
        "    ras_object1.unsteady_df = ras_object1.get_unsteady_entries()\n",
        "    \n",
        "    ras_object2.plan_df = ras_object2.get_plan_entries()\n",
        "    ras_object2.geom_df = ras_object2.get_geom_entries()\n",
        "    ras_object2.flow_df = ras_object2.get_flow_entries()\n",
        "    ras_object2.unsteady_df = ras_object2.get_unsteady_entries()\n",
        "    \n",
        "    # Create a comparison dictionary\n",
        "    comparison = {\n",
        "        'Project Name': [ras_object1.project_name, ras_object2.project_name],\n",
        "        'Plan Count': [len(ras_object1.plan_df), len(ras_object2.plan_df)],\n",
        "        'Geometry Count': [len(ras_object1.geom_df), len(ras_object2.geom_df)],\n",
        "        'Flow Count': [len(ras_object1.flow_df), len(ras_object2.flow_df)],\n",
        "        'Unsteady Count': [len(ras_object1.unsteady_df), len(ras_object2.unsteady_df)]\n",
        "    }\n",
        "    \n",
        "    # Create a DataFrame for the comparison\n",
        "    comparison_df = pd.DataFrame(comparison, index=[name1, name2])\n",
        "    \n",
        "\n",
        "    # Display the comparison\n",
        "    print(\"Project Structure Comparison:\")\n",
        "    display.display(comparison_df)\n",
        "    \n",
        "    # Create a bar chart to visualize the comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    comparison_df.iloc[:, 1:].plot(kind='bar', ax=plt.gca())\n",
        "    plt.title('Project Structure Comparison')\n",
        "    plt.xlabel('Project')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Component')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Set y-axis to only show whole numbers (integers)\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "# Compare the structures of the two projects\n",
        "comparison_df = compare_project_structures(\n",
        "    bald_eagle_ras, \"Bald Eagle Creek\", \n",
        "    muncie_ras, \"Muncie\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## This approach can also be used to programmatically compare 2 copies of the same project to ensure all of the plan parameters, boundary condition definitions, etc remained the same, and for other QAQC processes.\n",
        "\n",
        "This will be shown in further examples in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Multiple Project Operations\n",
        "\n",
        "In this notebook, we've demonstrated how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. We've covered the following key operations:\n",
        "\n",
        "1. **Initializing Multiple Projects**: Creating separate RAS objects for different projects\n",
        "2. **Independent Configuration**: Configuring plans with project-specific parameters\n",
        "3. **Parallel Execution**: Running computations from different projects simultaneously\n",
        "4. **Resource Management**: Organizing compute folders and tracking execution statistics\n",
        "5. **Results Comparison**: Analyzing and comparing results from different projects\n",
        "6. **Advanced Workflows**: Creating sensitivity plans and batch processing pipelines\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "When working with multiple HEC-RAS projects in RAS Commander, remember these key concepts:\n",
        "\n",
        "- **Custom RAS Objects**: Create and use separate RAS objects for each project\n",
        "- **Always Specify ras_object**: Use the `ras_object` parameter in all function calls\n",
        "- **Separate Compute Folders**: Use separate folders for each project's computations\n",
        "- **Resource Management**: Be mindful of CPU and memory usage when running in parallel\n",
        "- **Project Tracking**: Keep track of which results belong to which project\n",
        "\n",
        "### Multiple Project Applications\n",
        "\n",
        "Working with multiple projects unlocks advanced applications such as:\n",
        "\n",
        "1. **Model Comparison**: Compare results from different river systems\n",
        "2. **Basin-wide Analysis**: Analyze connected river systems in parallel\n",
        "3. **Parameter Sweep**: Test a range of parameters across multiple models\n",
        "4. **Model Development**: Develop and test models simultaneously\n",
        "5. **Batch Processing**: Process large sets of models in an automated pipeline\n",
        "\n",
        "These capabilities make RAS Commander a powerful tool for large-scale hydraulic modeling and water resources management."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\05_single_plan_execution.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Single Plan Execution\n",
        "\n",
        "This notebook demonstrates how to execute a single HEC-RAS plan using the RAS Commander library. We'll focus specifically on running a plan with a specified number of processor cores while overwriting an existing computation folder.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
        "2. **Plan Overview**: Explore the available plans in the project\n",
        "3. **Core Execution Configuration**: Set the number of processor cores to use during computation\n",
        "4. **Destination Folder Management**: Use and overwrite computation folders \n",
        "5. **Results Verification**: Check the results paths after computation\n",
        "6. **Performance Considerations**: Understand the impact of core count on performance\n",
        "\n",
        "Let's begin by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Working Environment\n",
        "\n",
        "Let's set up our working directory and paths to example projects. We'll also check the number of available CPU cores on this system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths to example projects\n",
        "examples_dir = bald_eagle_path.parent\n",
        "\n",
        "# Define computation output paths\n",
        "compute_dest_folder = examples_dir / \"compute_test\"\n",
        "\n",
        "# Check system resources\n",
        "cpu_count = psutil.cpu_count(logical=True)\n",
        "physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "print(f\"System has {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
        "print(f\"For HEC-RAS computation, it's often most efficient to use 2-8 cores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasCmdr.compute_plan Method\n",
        "\n",
        "Before we dive into execution, let's understand the `compute_plan` method from the `RasCmdr` class, which is the core function for running HEC-RAS simulations.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (str, Path): The plan number to execute or the full path to the plan file\n",
        "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
        "- `ras_object` (RasPrj, optional): Specific RAS object to use (defaults to global `ras`)\n",
        "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files (default: False)\n",
        "- `num_cores` (int, optional): Number of processor cores to use (default: None, uses plan settings)\n",
        "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists (default: False)\n",
        "\n",
        "### Returns\n",
        "- `bool`: True if the execution was successful, False otherwise\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Destination Folder**: By default, the simulation runs in the original project folder. Specifying a destination folder creates a copy of the project in that location for execution, leaving the original project untouched.\n",
        "\n",
        "2. **Number of Cores**: HEC-RAS can use multiple processor cores to speed up computation. The optimal number depends on the model complexity and your computer's specifications. Generally:\n",
        "   - 1-2 cores: Good for small models, highest efficiency per core\n",
        "   - 3-8 cores: Good balance for most models\n",
        "   - >8 cores: Diminishing returns, may actually be slower due to overhead\n",
        "\n",
        "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties, which is useful after making geometry changes.\n",
        "\n",
        "4. **Overwrite Destination**: Controls whether an existing destination folder should be overwritten. This is a safety feature to prevent accidental deletion of important results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Explore Available Plans\n",
        "\n",
        "Let's examine the available plans in the project to understand what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the available plans in the project\n",
        "print(\"Available plans in the project:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Let's check the current setting for number of cores in the plans\n",
        "print(\"\\nCurrent core settings for plans:\")\n",
        "for plan_num in ras.plan_df['plan_number']:\n",
        "    # Check all three core parameters\n",
        "    d1_cores = RasPlan.get_plan_value(plan_num, \"UNET D1 Cores\")\n",
        "    d2_cores = RasPlan.get_plan_value(plan_num, \"UNET D2 Cores\") \n",
        "    ps_cores = RasPlan.get_plan_value(plan_num, \"PS Cores\")\n",
        "    \n",
        "    print(f\"Plan {plan_num}'s Existing Settings:\")\n",
        "    print(f\"  1D Cores: {d1_cores}\")\n",
        "    print(f\"  2D Cores: {d2_cores}\")\n",
        "    print(f\"  Pump Station Cores: {ps_cores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a Destination Folder Structure\n",
        "\n",
        "Now, let's prepare a destination folder for our computation. This allows us to run simulations without modifying the original project files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a destination folder path\n",
        "dest_folder = examples_dir / \"compute_test_cores\"\n",
        "\n",
        "# Check if the destination folder already exists\n",
        "if dest_folder.exists():\n",
        "    print(f\"Destination folder already exists: {dest_folder}\")\n",
        "    print(\"We'll use overwrite_dest=True to replace it\")\n",
        "else:\n",
        "    print(f\"Destination folder will be created: {dest_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Execute a Plan with a Specified Number of Cores\n",
        "\n",
        "Now we're ready to execute a plan with a specified number of cores, overwriting the destination folder if it exists. This is the core functionality demonstrated in Example 5 of the original script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a plan and number of cores\n",
        "plan_number = \"01\"\n",
        "num_cores = 2  # Specify the number of cores to use\n",
        "\n",
        "print(f\"Executing plan {plan_number} with {num_cores} cores...\")\n",
        "print(f\"Destination folder: {dest_folder}\")\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute the plan with specified parameters\n",
        "success = RasCmdr.compute_plan(\n",
        "    plan_number,              # The plan to execute\n",
        "    dest_folder=dest_folder,  # Where to run the simulation\n",
        "    num_cores=num_cores,      # Number of processor cores to use\n",
        "    overwrite_dest=True       # Overwrite destination folder if it exists\n",
        ")\n",
        "\n",
        "# Record the end time and calculate duration\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "# Report results\n",
        "if success:\n",
        "    print(f\"\u2705 Plan {plan_number} executed successfully using {num_cores} cores\")\n",
        "    print(f\"Execution time: {duration:.2f} seconds\")\n",
        "else:\n",
        "    print(f\"\u274c Plan {plan_number} execution failed\")\n",
        "    print(f\"Time elapsed: {duration:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Results\n",
        "\n",
        "After execution, let's verify the results by checking the results paths and examining the destination folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that the destination folder exists and contains the expected files\n",
        "if dest_folder.exists():\n",
        "    print(f\"Destination folder exists: {dest_folder}\")\n",
        "    \n",
        "    # List the key files in the destination folder\n",
        "    print(\"\\nKey files in destination folder:\")\n",
        "    project_files = list(dest_folder.glob(f\"{ras.project_name}.*\"))\n",
        "    for file in project_files[:10]:  # Show first 10 files\n",
        "        file_size = file.stat().st_size / 1024  # Size in KB\n",
        "        print(f\"  {file.name}: {file_size:.1f} KB\")\n",
        "    \n",
        "    if len(project_files) > 10:\n",
        "        print(f\"  ... and {len(project_files) - 10} more files\")\n",
        "    \n",
        "    # Check for HDF result files\n",
        "    print(\"\\nHDF result files:\")\n",
        "    hdf_files = list(dest_folder.glob(f\"*.hdf\"))\n",
        "    for file in hdf_files:\n",
        "        file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "        print(f\"  {file.name}: {file_size:.1f} MB\")\n",
        "else:\n",
        "    print(f\"Destination folder does not exist: {dest_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the results path using the RasPlan.get_results_path method\n",
        "# First, initialize a RAS object using the destination folder\n",
        "try:\n",
        "    dest_ras = RasPrj()\n",
        "    init_ras_project(dest_folder, \"6.6\", ras_object=dest_ras)\n",
        "    \n",
        "    # Get the results path for the plan we just executed\n",
        "    results_path = RasPlan.get_results_path(plan_number, ras_object=dest_ras)\n",
        "    \n",
        "    if results_path:\n",
        "        print(f\"Results for plan {plan_number} are located at: {results_path}\")\n",
        "        \n",
        "        # Check if the file exists and get its size\n",
        "        results_file = Path(results_path)\n",
        "        if results_file.exists():\n",
        "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
        "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"No results found for plan {plan_number} in the destination folder\")\n",
        "except Exception as e:\n",
        "    print(f\"Error checking results: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Single Plan Execution Options\n",
        "\n",
        "The `RasCmdr.compute_plan()` method provides a flexible way to execute HEC-RAS plans with various options. Here's a summary of the key parameters we've explored:\n",
        "\n",
        "1. **Basic Execution**: Simply provide a plan number\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\")\n",
        "   ```\n",
        "\n",
        "2. **Destination Folder**: Run in a separate folder to preserve the original project\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\")\n",
        "   ```\n",
        "\n",
        "3. **Number of Cores**: Control the CPU resources used\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", num_cores=2)\n",
        "   ```\n",
        "\n",
        "4. **Overwrite Destination**: Replace existing computation folders\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\", overwrite_dest=True)\n",
        "   ```\n",
        "\n",
        "5. **Clear Geometry Preprocessor**: Force recalculation of geometric properties\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\"01\", clear_geompre=True)\n",
        "   ```\n",
        "\n",
        "6. **Combined Options**: Use multiple options together\n",
        "   ```python\n",
        "   RasCmdr.compute_plan(\n",
        "       \"01\",\n",
        "       dest_folder=\"path/to/folder\",\n",
        "       num_cores=2,\n",
        "       clear_geompre=True,\n",
        "       overwrite_dest=True\n",
        "   )\n",
        "   ```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further enhance your HEC-RAS automation, consider exploring:\n",
        "\n",
        "1. **Parallel Execution**: Use `RasCmdr.compute_parallel()` to run multiple plans simultaneously\n",
        "2. **Test Mode**: Use `RasCmdr.compute_test_mode()` for testing purposes\n",
        "3. **Pre-Processing**: Modify plans, geometries, and unsteady flows before execution\n",
        "4. **Post-Processing**: Analyze results after computation\n",
        "5. **Batch Processing**: Create scripts for parameter sweeps or scenario analysis\n",
        "\n",
        "These advanced topics are covered in other examples and documentation for the RAS Commander library."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\06_executing_plan_sets.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Executing Plan Sets \n",
        "\n",
        "This notebook demonstrates different ways to specify and execute HEC-RAS plans using the RAS Commander library. Proper plan specification is essential for efficient model execution, especially when working with large projects containing multiple plans.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project and explore available plans\n",
        "2. **Sequential Execution of Specific Plans**: Select and run particular plans in sequence\n",
        "3. **Parallel Execution of Specific Plans**: Run selected plans simultaneously\n",
        "4. **Executing All Plans**: Run every plan in a project\n",
        "5. **Filtered Plan Selection**: Select plans based on criteria or patterns\n",
        "6. **Conditional Execution**: Run plans based on results of previous executions\n",
        "\n",
        "Let's begin by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan Specification in HEC-RAS\n",
        "\n",
        "In HEC-RAS, each plan (`.p*` file) represents a specific hydraulic model simulation scenario. When working with RAS Commander, you can specify plans for execution in several ways:\n",
        "\n",
        "1. **Single Plan**: Specify one plan by its number (e.g., \"01\")\n",
        "2. **List of Plans**: Specify multiple plans as a list (e.g., [\"01\", \"03\", \"05\"])\n",
        "3. **All Plans**: Execute all plans in a project by not specifying any plan or passing `None`\n",
        "4. **Filtered Plans**: Select plans based on criteria (e.g., plans with specific flow conditions)\n",
        "5. **Plan Path**: Specify the full path to a plan file instead of just the number\n",
        "\n",
        "### Why Plan Specification Matters\n",
        "\n",
        "- **Efficiency**: Run only the plans you need rather than recomputing everything\n",
        "- **Organization**: Group related plans for batch processing\n",
        "- **Automation**: Create workflows that process plans in a specific order\n",
        "- **Resource Management**: Optimize hardware utilization for specific plans\n",
        "\n",
        "### Best Practices for Plan Specification\n",
        "\n",
        "- Use consistent formatting for plan numbers (e.g., always use two-digit strings like \"01\" instead of 1)\n",
        "- Check available plans before attempting to execute them\n",
        "- Organize plans by purpose to make selection easier\n",
        "- Use descriptive short identifiers and plan titles to aid in selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function and explore the available plans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nAvailable plans in the project:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Check plan details to understand what each plan represents\n",
        "plan_details = []\n",
        "for index, row in ras.plan_df.iterrows():\n",
        "    plan_number = row['plan_number']\n",
        "    \n",
        "    # Get plan description if available\n",
        "    description = None\n",
        "    if 'description' in row:\n",
        "        description = row['description']\n",
        "    else:\n",
        "        try:\n",
        "            description = RasPlan.read_plan_description(plan_number)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Get short identifier if available\n",
        "    short_id = None\n",
        "    if 'Short Identifier' in row:\n",
        "        short_id = row['Short Identifier']\n",
        "    \n",
        "    # Get geometry file\n",
        "    geom_file = None\n",
        "    if 'Geom File' in row:\n",
        "        geom_file = row['Geom File']\n",
        "    \n",
        "    # Check if the plan has results\n",
        "    has_results = False\n",
        "    if 'HDF_Results_Path' in row and row['HDF_Results_Path']:\n",
        "        has_results = True\n",
        "    \n",
        "    plan_details.append({\n",
        "        'Plan Number': plan_number,\n",
        "        'Short ID': short_id,\n",
        "        'Description': description[:50] + '...' if description and len(description) > 50 else description,\n",
        "        'Geometry': geom_file,\n",
        "        'Has Results': has_results\n",
        "    })\n",
        "\n",
        "# Create a DataFrame with the plan details\n",
        "plan_details_df = pd.DataFrame(plan_details)\n",
        "print(\"\\nPlan details:\")\n",
        "display.display(plan_details_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sequential Execution of Specific Plans\n",
        "\n",
        "Let's execute specific plans in sequence using `RasCmdr.compute_test_mode()` with a list of plan numbers. This approach allows us to run only the plans we need, in the order we specify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing specific plans sequentially...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Define the plans to execute\n",
        "specific_plans = [\"01\", \"03\"]\n",
        "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute specific plans sequentially\n",
        "execution_results = RasCmdr.compute_test_mode(\n",
        "    plan_number=specific_plans,\n",
        "    dest_folder_suffix=\"[SpecificSequential]\",\n",
        "    num_cores=6, \n",
        "    overwrite_dest=True\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "sequential_duration = end_time - start_time\n",
        "\n",
        "print(f\"Sequential execution of specific plans completed in {sequential_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "sequential_results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Sequential\"}\n",
        "    for plan, success in execution_results.items()\n",
        "])\n",
        "\n",
        "sequential_results_df \n",
        "\n",
        "# Ensure the 'Plan' column exists before sorting\n",
        "if 'Plan' in sequential_results_df.columns:\n",
        "    sequential_results_df = sequential_results_df.sort_values(\"Plan\")\n",
        "else:\n",
        "    print(\"Warning: 'Plan' column not found in execution results.\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nSequential Execution Results:\")\n",
        "display.display(sequential_results_df)\n",
        "\n",
        "# Check the test folder\n",
        "test_folder = bald_eagle_path.parent / f\"{ras.project_name} [SpecificSequential]\"\n",
        "if test_folder.exists():\n",
        "    print(f\"\\nTest folder exists: {test_folder}\")\n",
        "    \n",
        "    # Check for results\n",
        "    hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
        "    if hdf_files:\n",
        "        print(f\"Found {len(hdf_files)} HDF result files:\")\n",
        "        for file in hdf_files:\n",
        "            file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "            print(f\"  {file.name}: {file_size:.1f} MB\")\n",
        "    else:\n",
        "        print(\"No HDF result files found in the test folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Running Only Plans Without HDF Results\n",
        "An important use case is to identify and execute only those plans that have no existing HDF results. This approach can save time by avoiding redundant computations, especially useful when adding new plans to an existing project or after making limited changes.\n",
        "\n",
        "Let's demonstrate how to:\n",
        "\n",
        "- Use the `ras` object to identify plans without results\n",
        "- Create a filtered list of these plans\n",
        "- Execute only the missing plans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Identifying and executing plans without HDF results...\")\n",
        "\n",
        "# Use the ras object to determine which plans don't have results\n",
        "plans_no_results = ras.plan_df[ras.plan_df['HDF_Results_Path'].isna()]['plan_number'].tolist()\n",
        "\n",
        "if not plans_no_results:\n",
        "    print(\"All plans already have HDF results. Creating a test scenario...\")\n",
        "    # For demonstration purposes, pretend some plans don't have results\n",
        "    plans_no_results = [\"04\", \"05\"]\n",
        "    print(f\"Simulating no results for plans: {', '.join(plans_no_results)}\")\n",
        "else:\n",
        "    print(f\"Found {len(plans_no_results)} plans without HDF results: {', '.join(plans_no_results)}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute only the plans without results\n",
        "if plans_no_results:\n",
        "    print(f\"\\nExecuting {len(plans_no_results)} plans without results...\")\n",
        "    execution_results = RasCmdr.compute_test_mode(\n",
        "        plan_number=plans_no_results,\n",
        "        dest_folder_suffix=\"[MissingPlans]\",\n",
        "        num_cores=6, \n",
        "        overwrite_dest=True\n",
        "    )\n",
        "    \n",
        "    # Record end time and calculate duration\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    \n",
        "    print(f\"Execution completed in {duration:.2f} seconds\")\n",
        "    \n",
        "    # Create a DataFrame from the execution results\n",
        "    missing_results_df = pd.DataFrame([\n",
        "        {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Missing Plans\"}\n",
        "        for plan, success in execution_results.items()\n",
        "    ])\n",
        "    \n",
        "    # Sort by plan number\n",
        "    missing_results_df = missing_results_df.sort_values(\"Plan\")\n",
        "    \n",
        "    # Display the results\n",
        "    print(\"\\nExecution Results for Plans Without HDF Results:\")\n",
        "    display.display(missing_results_df)\n",
        "    \n",
        "    # Check the test folder\n",
        "    test_folder = bald_eagle_path.parent / f\"{ras.project_name} [MissingPlans]\"\n",
        "    if test_folder.exists():\n",
        "        print(f\"\\nTest folder exists: {test_folder}\")\n",
        "        \n",
        "        # Check for results\n",
        "        hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
        "        if hdf_files:\n",
        "            print(f\"Found {len(hdf_files)} HDF result files:\")\n",
        "            for file in hdf_files:\n",
        "                file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "                print(f\"  {file.name}: {file_size:.1f} MB\")\n",
        "        else:\n",
        "            print(\"No HDF result files found in the test folder\")\n",
        "else:\n",
        "    print(\"No plans without results to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification of Results\n",
        "After executing the plans that were missing HDF results, it's important to verify that the results were properly generated. Let's check if the execution actually created the expected output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-initialize the project with the test folder to see updated results\n",
        "missing_plans_folder = bald_eagle_path.parent / f\"{ras.project_name} [MissingPlans]\"\n",
        "\n",
        "if missing_plans_folder.exists():\n",
        "    # Initialize the project from the test folder\n",
        "    test_ras = RasPrj()\n",
        "    init_ras_project(missing_plans_folder, \"6.6\", ras_object=test_ras)\n",
        "    \n",
        "    # Check which plans now have results\n",
        "    plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]['plan_number'].tolist()\n",
        "    \n",
        "    print(f\"Plans with results after execution: {', '.join(plans_with_results)}\")\n",
        "    \n",
        "    # Verify if all previously missing plans now have results\n",
        "    all_generated = all(plan in plans_with_results for plan in plans_no_results)\n",
        "    \n",
        "    if all_generated:\n",
        "        print(\"\u2705 Successfully generated results for all missing plans\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f Some plans still don't have results after execution\")\n",
        "        missing_after = [plan for plan in plans_no_results if plan not in plans_with_results]\n",
        "        print(f\"Plans still missing results: {', '.join(missing_after)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan Specification Techniques\n",
        "\n",
        "In this notebook, we've explored different ways to specify and execute HEC-RAS plans using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
        "\n",
        "1. **Basic Plan Specification**\n",
        "   - Single plan by number: `\"01\"`\n",
        "   - List of specific plans: `[\"01\", \"03\"]`\n",
        "   - All plans: `ras.plan_df['plan_number'].tolist()`\n",
        "\n",
        "2. **Advanced Selection**\n",
        "   - Categorization: Grouping plans by purpose or type\n",
        "   - Dependencies: Ensuring prerequisite plans are run first\n",
        "   - Ordered execution: Running plans in a specific sequence\n",
        "\n",
        "3. **Run Plans with Missing Results (HDF)**\n",
        "   - Using ras object to determine which plans have results\n",
        "   - Creating a list of plans with no results\n",
        "   - Running those plans sequentially\n",
        "\n",
        "4. NOTE: run_parallel can also run a list of plans, but compute_plan is only made for single plan execution.  \n",
        "\n",
        "\n",
        "### Best Practices for Plan Specification\n",
        "\n",
        "1. **Consistent Formatting**: Use two-digit strings for plan numbers (\"01\" instead of 1)\n",
        "2. **Descriptive Naming**: Use meaningful short identifiers that describe the plan's purpose\n",
        "3. **Verify Availability**: Check that specified plans exist before trying to execute them\n",
        "4. **Document Dependencies**: Keep track of which plans depend on others\n",
        "5. **Use Appropriate Execution Method**: Choose sequential or parallel based on dependencies and resources\n",
        "6. **Monitor Performance**: Track execution times to identify optimization opportunities\n",
        "\n",
        "By applying these techniques, you can create efficient and organized workflows for executing HEC-RAS plans, from simple batch processing to complex dependency-based execution sequences."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\07_sequential_plan_execution.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Sequential Plan Execution\n",
        "\n",
        "This notebook demonstrates how to sequentially execute multiple HEC-RAS plans using the RAS Commander library. Sequential execution is useful for batch processing plans that need to be run in a specific order or when you want to ensure consistent resource usage across multiple runs.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
        "2. **Sequential Execution of All Plans**: Run all plans in a project sequentially in a test folder\n",
        "3. **Selective Plan Execution**: Run only specific plans in sequence\n",
        "4. **Geometry Preprocessor Management**: Clear geometry preprocessor files before execution\n",
        "5. **Execution Result Analysis**: Track and analyze the results of sequential executions\n",
        "6. **Performance Monitoring**: Monitor and compare execution times across different runs\n",
        "\n",
        "Let's begin by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define examples_dir as parent of bald_eagle_path\n",
        "examples_dir = bald_eagle_path.parent\n",
        "print(f\"Examples directory set to: {examples_dir}\")\n",
        "\n",
        "    \n",
        "# Remove any compute test folders from previous runs\n",
        "for folder in examples_dir.glob(\"*[[]AllSequential[]]*\"):\n",
        "    if folder.is_dir():\n",
        "        print(f\"Removing existing test folder: {folder}\")\n",
        "        shutil.rmtree(folder)\n",
        "        \n",
        "for folder in examples_dir.glob(\"*[[]SpecificSequential*[]]*\"):\n",
        "    if folder.is_dir():\n",
        "        print(f\"Removing existing test folder: {folder}\")\n",
        "        shutil.rmtree(folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Sequential Execution in HEC-RAS\n",
        "\n",
        "HEC-RAS simulations can be executed in several ways:\n",
        "\n",
        "1. **Single Plan Execution**: Run one plan at a time using `RasCmdr.compute_plan()`\n",
        "2. **Sequential Execution**: Run multiple plans one after another using `RasCmdr.compute_test_mode()`\n",
        "3. **Parallel Execution**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
        "\n",
        "This notebook focuses on the second approach: **Sequential Execution**. Here are the key benefits of sequential execution:\n",
        "\n",
        "- **Controlled Resource Usage**: By running plans one at a time, you ensure consistent resource usage\n",
        "- **Dependency Management**: When later plans depend on results from earlier plans\n",
        "- **Simplified Debugging**: Easier to identify which plan is causing an issue when they run sequentially\n",
        "- **Consistent Test Environment**: All plans run in the same isolated folder\n",
        "\n",
        "The `compute_test_mode()` function from `RasCmdr` is specifically designed for this purpose. It creates a separate test folder, copies the project there, and executes the specified plans in sequential order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Project\n",
        "\n",
        "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the HEC-RAS project\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nHEC-RAS Project Plan Data:\")\n",
        "display.display(ras.plan_df)\n",
        "\n",
        "# Check how many plans we have\n",
        "plan_count = len(ras.plan_df)\n",
        "print(f\"Found {plan_count} plans in the project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasCmdr.compute_test_mode Method\n",
        "\n",
        "Before we start executing plans, let's understand the `compute_test_mode()` method from the `RasCmdr` class, which we'll use for sequential execution.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (str, list[str], optional): Plan number or list of plan numbers to execute. If None, all plans will be executed.\n",
        "- `dest_folder_suffix` (str, optional): Suffix to append to the test folder name. Defaults to \"[Test]\".\n",
        "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.\n",
        "- `num_cores` (int, optional): Maximum number of cores to use for each plan. If None, the current setting is not changed.\n",
        "- `ras_object` (RasPrj, optional): Specific RAS object to use. If None, uses the global ras object.\n",
        "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists. Defaults to False.\n",
        "\n",
        "### Return Value\n",
        "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Test Folder**: The function creates a separate folder with the specified suffix, copying the project there for execution.\n",
        "2. **Sequential Execution**: Plans are executed one after another in the specified order.\n",
        "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties.\n",
        "4. **Destination Folder Option**: The suffix determines the name of the test folder. Unlike `compute_plan()`, you can't specify an arbitrary destination folder.\n",
        "5. **Overwrite Option**: Controls whether an existing test folder should be overwritten.\n",
        "\n",
        "Now, let's see how this works in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sequential Execution of All Plans\n",
        "\n",
        "Let's execute all plans in the project sequentially. This will create a test folder with the suffix \"[AllSequential]\" and run all plans one after another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing all plans sequentially...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute all plans sequentially\n",
        "# - dest_folder_suffix: Suffix to append to the test folder name\n",
        "# - overwrite_dest: Overwrite the destination folder if it exists\n",
        "# - no ras object is specified, it will use the default \"ras\" object\n",
        "execution_results = RasCmdr.compute_test_mode(\n",
        "    dest_folder_suffix=\"[AllSequential]\",\n",
        "    overwrite_dest=True\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Sequential execution of all plans completed in {total_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in execution_results.items()\n",
        "])\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examining the Test Folder\n",
        "\n",
        "Let's examine the test folder created by `compute_test_mode()` to better understand what happened during sequential execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the test folder path\n",
        "test_folder = bald_eagle_path.parent / f\"Balde Eagle Creek [AllSequential]\"\n",
        "test_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test folder if it doesn't exist using pathlib\n",
        "if not test_folder.exists():\n",
        "    test_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "if test_folder.exists():\n",
        "    print(f\"Test folder exists: {test_folder}\")\n",
        "    \n",
        "    # List the key files in the test folder\n",
        "    print(\"\\nKey files in test folder:\")\n",
        "    \n",
        "    # First, list the project file and all plan files\n",
        "    prj_files = list(test_folder.glob(\"*.prj\"))\n",
        "    plan_files = list(test_folder.glob(\"*.p*\"))\n",
        "    plan_files.sort()\n",
        "    \n",
        "    if prj_files:\n",
        "        print(f\"Project file: {prj_files[0].name}\")\n",
        "    \n",
        "    print(\"Plan files:\")\n",
        "    for plan_file in plan_files:\n",
        "        file_size = plan_file.stat().st_size / 1024  # Size in KB\n",
        "        print(f\"  {plan_file.name}: {file_size:.1f} KB\")\n",
        "    \n",
        "    # Look for HDF result files\n",
        "    hdf_files = list(test_folder.glob(\"*.hdf\"))\n",
        "    hdf_files.sort()\n",
        "    \n",
        "    print(\"\\nHDF files:\")\n",
        "    for hdf_file in hdf_files:\n",
        "        file_size = hdf_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "        print(f\"  {hdf_file.name}: {file_size:.1f} MB\")\n",
        "    \n",
        "    # Geometry preprocessor files (if any)\n",
        "    geompre_files = list(test_folder.glob(\"*.c*\"))\n",
        "    geompre_files.sort()\n",
        "    \n",
        "    if geompre_files:\n",
        "        print(\"\\nGeometry preprocessor files:\")\n",
        "        for geompre_file in geompre_files:\n",
        "            file_size = geompre_file.stat().st_size / 1024  # Size in KB\n",
        "            print(f\"  {geompre_file.name}: {file_size:.1f} KB\")\n",
        "    else:\n",
        "        print(\"\\nNo geometry preprocessor files found\")\n",
        "        \n",
        "    # Initialize a RAS project in the test folder to inspect results\n",
        "    try:\n",
        "        test_ras = RasPrj()\n",
        "        init_ras_project(test_folder, ras.ras_exe_path, ras_object=test_ras)\n",
        "        print(\"\\nPlans with results in the test folder:\")\n",
        "        test_plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]\n",
        "        display.display(test_plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing test folder as a RAS project: {e}\")\n",
        "else:\n",
        "    print(f\"Test folder not found: {test_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Sequential Execution of Specific Plans\n",
        "\n",
        "Now, let's execute only specific plans in the project. We'll select plans \"01\" and \"02\" and run them sequentially with the `clear_geompre` option set to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing specific plans sequentially with clearing geometry preprocessor files...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Define the plans to execute\n",
        "selected_plans = [\"01\", \"02\"]\n",
        "print(f\"Selected plans: {', '.join(selected_plans)}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute specific plans sequentially\n",
        "# - plan_number: List of plan numbers to execute\n",
        "# - dest_folder_suffix: Suffix to append to the test folder name\n",
        "# - clear_geompre: Clear geometry preprocessor files before execution\n",
        "# - overwrite_dest: Overwrite the destination folder if it exists\n",
        "execution_results = RasCmdr.compute_test_mode(\n",
        "    plan_number=selected_plans,\n",
        "    dest_folder_suffix=\"[SpecificSequentialClearGeompre]\",\n",
        "    clear_geompre=True,\n",
        "    overwrite_dest=True\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Sequential execution of specific plans completed in {total_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in execution_results.items()\n",
        "])\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Sequential Plan Execution\n",
        "\n",
        "In this notebook, we've explored how to execute HEC-RAS plans sequentially using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
        "\n",
        "1. **Project Setup and Initialization**: Setting up the environment and initializing a HEC-RAS project\n",
        "2. **Example Project Management**: Using `RasExamples` to download and extract sample projects\n",
        "3. **Basic Sequential Execution**: Using `RasCmdr.compute_test_mode()` to run all plans in a project\n",
        "4. **Test Folder Analysis**: Examining the contents and results of sequential execution\n",
        "5. **Selective Plan Execution**: Running specific plans with geometry preprocessor clearing\n",
        "\n",
        "### Key Functions Used\n",
        "\n",
        "- `init_ras_project()`: Initialize a HEC-RAS project\n",
        "- `RasExamples.extract_project()`: Extract example projects for testing\n",
        "- `RasCmdr.compute_test_mode()`: Run plans sequentially in a test folder\n",
        "- `Path.glob()`: Examine test folder contents and results\n",
        "- `RasCmdr.compute_test_mode(clear_geompre=True)`: Execute plans with preprocessor clearing\n",
        "\n",
        "### Best Practices for Sequential Execution\n",
        "\n",
        "1. **Environment Setup**: Ensure all required libraries are installed and properly imported\n",
        "2. **Project Organization**: Clean up existing test folders before new executions\n",
        "3. **Resource Management**: Monitor system resources (CPU cores, memory) for optimal performance\n",
        "4. **Test Folder Naming**: Use meaningful suffixes to distinguish different execution runs\n",
        "5. **Performance Tracking**: Monitor execution times for each sequential run\n",
        "6. **Results Verification**: Check test folders for successful plan execution and result files\n",
        "7. **Selective Execution**: Use plan filtering when only specific plans need to be run\n",
        "\n",
        "With these techniques, you can effectively manage and execute HEC-RAS simulations sequentially, whether running all plans or a selected subset with specific configurations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdrpip5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\08_parallel_execution.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Parallel Plan Execution\n",
        "\n",
        "This notebook demonstrates how to execute multiple HEC-RAS plans in parallel using the RAS Commander library. Parallel execution allows you to make better use of your computer's processing power by running multiple plans simultaneously.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Initialize a HEC-RAS project and prepare it for parallel execution\n",
        "2. **Parallel Execution of All Plans**: Run all plans in a project simultaneously\n",
        "3. **Selective Parallel Execution**: Run only specific plans in parallel\n",
        "4. **Dynamic Worker Allocation**: Automatically determine the optimal number of parallel workers\n",
        "5. **Resource Management**: Optimize CPU core utilization for parallel runs\n",
        "6. **Results Comparison**: Analyze and visualize execution performance\n",
        "\n",
        "Let's begin by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import math  # Import math to avoid NameError in get_optimal_worker_count function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Our Working Environment\n",
        "\n",
        "Let's set up our working directory and check the system resources available for parallel execution. This will help us make informed decisions about how many workers to use.\n",
        "\n",
        "For this notebook we will be using the \"Muncie\" HEC Example Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Muncie example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
        "print(f\"Extracted project to: {muncie_path}\")  \n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {muncie_path.exists()}\")\n",
        "\n",
        "\n",
        "# Create compute folders\n",
        "compute_folder = muncie_path.parent / \"compute_test_parallel\"\n",
        "specific_compute_folder = muncie_path.parent / \"compute_test_parallel_specific\"\n",
        "dynamic_compute_folder = muncie_path.parent / \"compute_test_parallel_dynamic\"\n",
        "\n",
        "# Check system resources for parallel execution\n",
        "cpu_count = psutil.cpu_count(logical=True)  # Logical cores (including hyper-threading)\n",
        "physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
        "memory_gb = psutil.virtual_memory().total / (1024**3)  # Total RAM in GB\n",
        "available_memory_gb = psutil.virtual_memory().available / (1024**3)  # Available RAM in GB\n",
        "\n",
        "print(f\"System Resources:\")\n",
        "print(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores with hyper-threading)\")\n",
        "print(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
        "\n",
        "# Functions to help with resource management\n",
        "def get_optimal_worker_count(cores_per_worker=2):\n",
        "    \"\"\"Calculate the optimal number of workers based on available physical cores.\"\"\"\n",
        "    optimal_workers = math.floor(physical_cores / cores_per_worker)\n",
        "    return max(1, optimal_workers)  # Ensure at least 1 worker\n",
        "\n",
        "print(f\"\\nFor parallel HEC-RAS execution:\")\n",
        "print(f\"- With 2 cores per worker: Can use up to {get_optimal_worker_count(2)} parallel workers\")\n",
        "print(f\"- With 4 cores per worker: Can use up to {get_optimal_worker_count(4)} parallel workers\")\n",
        "print(f\"\\nEach HEC-RAS instance typically requires 2-4 GB of RAM. Based on your available memory,\")\n",
        "print(f\"you could reasonably run {math.floor(available_memory_gb / 3)} instances simultaneously.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Parallel Execution in HEC-RAS\n",
        "\n",
        "HEC-RAS simulations can be computationally intensive, especially for large models or long simulation periods. Parallel execution allows you to run multiple plans simultaneously, making better use of your computer's processing power.\n",
        "\n",
        "### Key Concepts in Parallel Execution\n",
        "\n",
        "1. **Workers**: Each worker is a separate process that can execute a HEC-RAS plan. The `max_workers` parameter determines how many plans can be executed simultaneously.\n",
        "\n",
        "2. **Cores per Worker**: Each worker (HEC-RAS instance) can utilize multiple CPU cores. The `num_cores` parameter sets how many cores each worker uses.\n",
        "\n",
        "3. **Resource Balancing**: Effective parallel execution requires balancing the number of workers with the cores per worker. Too many workers or too many cores per worker can lead to resource contention and slower overall performance.\n",
        "\n",
        "4. **Worker Folders**: Each worker gets its own folder with a copy of the project, allowing for isolated execution.\n",
        "\n",
        "### Parallel vs. Sequential Execution\n",
        "\n",
        "- **Parallel**: Multiple plans run simultaneously (good for independent plans, faster overall completion)\n",
        "- **Sequential**: Plans run one after another (good for dependent plans, consistent resource usage)\n",
        "\n",
        "### Optimal Configuration\n",
        "\n",
        "The optimal configuration depends on your hardware and the specific plans you're running:\n",
        "\n",
        "- For most models, 2-4 cores per worker provides good performance\n",
        "- Set `max_workers` based on available physical cores: `max_workers = floor(physical_cores / cores_per_worker)`\n",
        "- Ensure you have enough memory: each worker typically needs 2-4 GB of RAM\n",
        "\n",
        "Now, let's download and extract our example project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading and Extracting Example HEC-RAS Project\n",
        "\n",
        "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "Let's initialize the HEC-RAS project using the `init_ras_project()` function. We'll store the initialized object in a variable to use later, rather than relying on the global `ras` object. This approach is more suitable for working with multiple projects or compute folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the source project\n",
        "source_project = init_ras_project(muncie_path, \"6.6\")\n",
        "print(f\"Initialized source project: {source_project.project_name}\")\n",
        "\n",
        "# Display the current plan files in the project\n",
        "print(\"\\nAvailable plans in the project:\")\n",
        "display.display(source_project.plan_df)\n",
        "\n",
        "# Check how many plans we have\n",
        "plan_count = len(source_project.plan_df)\n",
        "print(f\"Found {plan_count} plans in the project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the RasCmdr.compute_parallel Method\n",
        "\n",
        "Before we start executing plans in parallel, let's understand the `compute_parallel()` method from the `RasCmdr` class.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (Union[str, List[str], None]): Plan number(s) to compute. If None, all plans are computed.\n",
        "- `max_workers` (int): Maximum number of parallel workers (default: 2).\n",
        "- `num_cores` (int): Number of cores to use per plan computation (default: 2).\n",
        "- `clear_geompre` (bool): Whether to clear geometry preprocessor files (default: False).\n",
        "- `ras_object` (Optional[RasPrj]): Specific RAS object to use. If None, uses global ras instance.\n",
        "- `dest_folder` (Union[str, Path, None]): Destination folder for computed results.\n",
        "- `overwrite_dest` (bool): Whether to overwrite existing destination folder (default: False).\n",
        "\n",
        "### Return Value\n",
        "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Worker Assignment**: Plans are assigned to workers in a round-robin fashion. For example, with 3 workers and 5 plans, workers would be assigned as follows: Worker 1: Plans 1 & 4, Worker 2: Plans 2 & 5, Worker 3: Plan 3.\n",
        "\n",
        "2. **Worker Folders**: Each worker gets its own folder (a subdirectory of the destination folder) for isolated execution.\n",
        "\n",
        "3. **Result Consolidation**: After all plans are executed, results are consolidated into the destination folder.\n",
        "\n",
        "4. **Resource Management**: Each worker can use multiple cores as specified by `num_cores`.\n",
        "\n",
        "Now, let's see how this works in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Parallel Execution of All Plans\n",
        "\n",
        "Let's execute all plans in the project in parallel. We'll use 3 workers, with 2 cores per worker. This approach is good when you have multiple plans that are independent of each other and you want to complete them as quickly as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing all plans in parallel...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Create compute folder if it doesn't exist\n",
        "compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define the parameters for parallel execution\n",
        "max_workers = 4\n",
        "cores_per_worker = 1\n",
        "\n",
        "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
        "print(f\"Destination folder: {compute_folder}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute all plans in parallel\n",
        "results_all = RasCmdr.compute_parallel(\n",
        "    max_workers=max_workers,\n",
        "    num_cores=cores_per_worker,\n",
        "    dest_folder=compute_folder,\n",
        "    overwrite_dest=True,\n",
        "    ras_object=source_project\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "print(f\"Parallel execution of all plans completed in {total_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in results_all.items()\n",
        "])\n",
        "\n",
        "# Sort by plan number\n",
        "results_df = results_df.sort_values(\"Plan\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Examining the Parallel Execution Results\n",
        "\n",
        "Let's initialize a RAS project in the compute folder and examine the results of the parallel execution. This will help us understand what happened during the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a RAS project in the compute folder\n",
        "compute_project = RasPrj()\n",
        "init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "print(f\"Initialized compute project: {compute_project.project_name}\")\n",
        "\n",
        "# Display the plan files in the compute folder\n",
        "print(\"\\nPlans in the compute folder:\")\n",
        "display.display(compute_project.plan_df)\n",
        "\n",
        "# Check which plans have results\n",
        "plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
        "display.display(plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
        "\n",
        "# List the worker folders (they should have been removed during results consolidation)\n",
        "worker_folders = list(compute_folder.glob(\"*Worker*\"))\n",
        "if worker_folders:\n",
        "    print(f\"\\nFound {len(worker_folders)} worker folders:\")\n",
        "    for folder in worker_folders:\n",
        "        print(f\"  {folder.name}\")\n",
        "else:\n",
        "    print(\"\\nNo worker folders remain in the compute folder (they were removed during results consolidation)\")\n",
        "\n",
        "# Check for HDF result files\n",
        "hdf_files = list(compute_folder.glob(\"*.hdf\"))\n",
        "hdf_files.sort()\n",
        "\n",
        "print(f\"\\nFound {len(hdf_files)} HDF files in the compute folder:\")\n",
        "for file in hdf_files:\n",
        "    file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "    print(f\"  {file.name}: {file_size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Additional Examples: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Execution of Specific Plans\n",
        "\n",
        "Now, let's execute only specific plans in the project in parallel. This approach is useful when you only want to run a subset of the available plans, perhaps for testing or comparison purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing specific plans in parallel...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Create specific compute folder if it doesn't exist\n",
        "specific_compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define the plans to execute\n",
        "specific_plans = [\"01\", \"03\"]\n",
        "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
        "\n",
        "# Define the parameters for parallel execution\n",
        "max_workers = 2  # One for each plan\n",
        "cores_per_worker = 2\n",
        "\n",
        "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
        "print(f\"Destination folder: {specific_compute_folder}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute specific plans in parallel\n",
        "results_specific = RasCmdr.compute_parallel(\n",
        "    plan_number=specific_plans,\n",
        "    max_workers=max_workers,\n",
        "    num_cores=cores_per_worker,\n",
        "    dest_folder=specific_compute_folder,\n",
        "    overwrite_dest=True,\n",
        "    ras_object=source_project\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "specific_duration = end_time - start_time\n",
        "\n",
        "print(f\"Parallel execution of specific plans completed in {specific_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "specific_results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in results_specific.items()\n",
        "])\n",
        "\n",
        "# Sort by plan number\n",
        "specific_results_df = specific_results_df.sort_values(\"Plan\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(specific_results_df)\n",
        "\n",
        "# Initialize a RAS project in the specific compute folder\n",
        "specific_compute_project = RasPrj()\n",
        "init_ras_project(specific_compute_folder, \"6.6\", ras_object=specific_compute_project)\n",
        "print(f\"\\nInitialized specific compute project: {specific_compute_project.project_name}\")\n",
        "\n",
        "# Check which plans have results\n",
        "specific_plans_with_results = specific_compute_project.plan_df[specific_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "print(f\"Found {len(specific_plans_with_results)} plans with results:\")\n",
        "display.display(specific_plans_with_results[['plan_number', 'HDF_Results_Path']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Execution with Max Workers Defined by Physical Cores (\"Dynamic Worker Allocation\") \n",
        "\n",
        "In this step, we'll determine the optimal number of workers based on the physical cores available on the system. This approach ensures that we make efficient use of the available hardware without overcommitting resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Executing plans with dynamic worker allocation...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Create dynamic compute folder if it doesn't exist\n",
        "dynamic_compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define the cores per worker\n",
        "cores_per_worker = 4\n",
        "# 2 cores per worker is the efficiency point for most CPU's, due to L2/L3 cache being shared by 2 cores in most x86 CPU's\n",
        "# 4-8 cores per worker is the maximum performance point for most CPU's, using more compute power to marginally lower runtime \n",
        "# when using parallel compute, 2 cores per worker is typically optimal as it is assumed you are maximizing throughput (efficency) over single-plan runtime (performance)\n",
        "\n",
        "# Calculate the optimal number of workers based on physical cores\n",
        "max_workers = get_optimal_worker_count(cores_per_worker)\n",
        "print(f\"System has {physical_cores} physical cores\")\n",
        "print(f\"With {cores_per_worker} cores per worker, optimal worker count is {max_workers}\")\n",
        "print(f\"Destination folder: {dynamic_compute_folder}\")\n",
        "\n",
        "# Record start time for performance measurement\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute all plans with dynamic worker allocation\n",
        "results_dynamic = RasCmdr.compute_parallel(\n",
        "    plan_number=specific_plans,\n",
        "    max_workers=max_workers,\n",
        "    num_cores=cores_per_worker,\n",
        "    dest_folder=dynamic_compute_folder,\n",
        "    overwrite_dest=True,\n",
        "    ras_object=source_project\n",
        ")\n",
        "\n",
        "# Record end time and calculate duration\n",
        "end_time = time.time()\n",
        "dynamic_duration = end_time - start_time\n",
        "\n",
        "print(f\"Parallel execution with dynamic worker allocation completed in {dynamic_duration:.2f} seconds\")\n",
        "\n",
        "# Create a DataFrame from the execution results for better visualization\n",
        "dynamic_results_df = pd.DataFrame([\n",
        "    {\"Plan\": plan, \"Success\": success}\n",
        "    for plan, success in results_dynamic.items()\n",
        "])\n",
        "\n",
        "# Sort by plan number\n",
        "dynamic_results_df = dynamic_results_df.sort_values(\"Plan\")\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nExecution Results:\")\n",
        "display.display(dynamic_results_df)\n",
        "\n",
        "# Initialize a RAS project in the dynamic compute folder\n",
        "dynamic_compute_project = RasPrj()\n",
        "init_ras_project(dynamic_compute_folder, \"6.6\", ras_object=dynamic_compute_project)\n",
        "print(f\"\\nInitialized dynamic compute project: {dynamic_compute_project.project_name}\")\n",
        "\n",
        "# Check which plans have results\n",
        "dynamic_plans_with_results = dynamic_compute_project.plan_df[dynamic_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "print(f\"Found {len(dynamic_plans_with_results)} plans with results:\")\n",
        "display.display(dynamic_plans_with_results[['plan_number', 'HDF_Results_Path']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "Let's compare the performance of the different parallel execution approaches we've tried. This will help us understand the impact of worker count and plan selection on execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame for individual plan runtimes\n",
        "plan_data = []\n",
        "\n",
        "# Define the approaches with more descriptive labels including worker and core counts\n",
        "approach_labels = {\n",
        "    \"all_plans\": \"All Plans (2 workers \u00d7 2 cores = 4 cores total)\",\n",
        "    \"specific_plans\": \"Specific Plans (1 worker \u00d7 2 cores = 2 cores total)\",\n",
        "    \"dynamic_workers\": f\"Dynamic Workers (1 worker \u00d7 4 cores = 4 cores total)\"\n",
        "}\n",
        "\n",
        "# Extract runtimes from the log messages\n",
        "# For all plans approach\n",
        "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"01\", \"Runtime\": 35.72})\n",
        "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"03\", \"Runtime\": 82.70})\n",
        "# Omitting plan 04 as it's a 1D model\n",
        "\n",
        "# For specific plans approach (plans 01 and 03 were run)\n",
        "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"01\", \"Runtime\": 29.10})\n",
        "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"03\", \"Runtime\": 36.09})\n",
        "\n",
        "# For dynamic worker approach (plans 01 and 03 were run)\n",
        "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"01\", \"Runtime\": 28.48})\n",
        "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"03\", \"Runtime\": 49.43})\n",
        "\n",
        "# Create a DataFrame\n",
        "plan_runtime_df = pd.DataFrame(plan_data)\n",
        "\n",
        "# Create a grouped bar chart for plan runtimes\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Get all unique plan numbers and ensure they're sorted\n",
        "plans = sorted(plan_runtime_df[\"Plan\"].unique())\n",
        "\n",
        "# Create x positions for the bars\n",
        "x = np.arange(len(plans))\n",
        "width = 0.25  # Width of the bars\n",
        "\n",
        "# Plot bars for each approach\n",
        "approaches = plan_runtime_df[\"Approach\"].unique()\n",
        "for i, approach in enumerate(approaches):\n",
        "    # Filter data for this approach\n",
        "    approach_data = plan_runtime_df[plan_runtime_df[\"Approach\"] == approach]\n",
        "    \n",
        "    # Initialize runtimes array with NaN values\n",
        "    runtimes = [np.nan] * len(plans)\n",
        "    \n",
        "    # Fill in runtimes where data exists\n",
        "    for j, plan in enumerate(plans):\n",
        "        plan_runtime = approach_data[approach_data[\"Plan\"] == plan][\"Runtime\"]\n",
        "        if not plan_runtime.empty:\n",
        "            runtimes[j] = plan_runtime.values[0]\n",
        "    \n",
        "    # Create bars for this approach (only where we have data)\n",
        "    valid_indices = [idx for idx, val in enumerate(runtimes) if not np.isnan(val)]\n",
        "    valid_plans = [plans[idx] for idx in valid_indices]\n",
        "    valid_runtimes = [runtimes[idx] for idx in valid_indices]\n",
        "    valid_positions = [x[idx] + (i - len(approaches)/2 + 0.5) * width for idx in valid_indices]\n",
        "    \n",
        "    # Plot the bars\n",
        "    bars = plt.bar(valid_positions, valid_runtimes, width, label=approach)\n",
        "    \n",
        "    # Add runtime labels on top of bars\n",
        "    for pos, runtime in zip(valid_positions, valid_runtimes):\n",
        "        plt.text(pos, runtime + 2, f\"{runtime:.1f}s\", ha='center', va='bottom')\n",
        "\n",
        "# Add labels, title, and custom x-axis tick labels\n",
        "plt.xlabel('Plan Number', fontsize=12)\n",
        "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
        "plt.title('Runtime Comparison by Plan Number and Parallelization Approach', fontsize=14)\n",
        "plt.xticks(x, plans, fontsize=11)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add note about omitting Plan 04\n",
        "plt.figtext(0.5, 0.01, \"\\nNote: Plan 04 (1D model) is omitted from this comparison\", \n",
        "            ha='center', fontsize=10, style='italic')\n",
        "\n",
        "# Ensure all plan numbers show on x-axis regardless of data availability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Parallel Plan Execution\n",
        "\n",
        "In this notebook, we've explored how to execute HEC-RAS plans in parallel using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
        "\n",
        "1. **Basic Parallel Execution**: Using `RasCmdr.compute_parallel()` to run all plans in a project simultaneously\n",
        "2. **Selective Parallel Execution**: Running only specific plans in parallel\n",
        "3. **Dynamic Worker Allocation**: Determining the optimal number of workers based on available system resources\n",
        "4. **Performance Analysis**: Comparing execution times for different parallel configurations\n",
        "5. **Advanced Parallel Workflows**: Building complex workflows with parallel execution for sensitivity analysis\n",
        "\n",
        "### Key Functions Used\n",
        "\n",
        "- `RasCmdr.compute_parallel()`: Execute multiple plans in parallel\n",
        "- `RasPlan.clone_plan()`: Create a new plan based on an existing one\n",
        "- `RasPlan.update_plan_description()`: Update the description of a plan\n",
        "- `RasPlan.set_num_cores()`: Set the number of cores for a plan to use\n",
        "- `RasPlan.get_results_path()`: Get the path to the results file for a plan\n",
        "\n",
        "### Best Practices for Parallel Execution\n",
        "\n",
        "1. **Use Separate RAS Objects**: Create and use separate RAS objects for different projects or folders\n",
        "2. **Balance Workers and Cores**: Find the right balance between the number of workers and cores per worker\n",
        "3. **Consider Hardware Limits**: Be mindful of your system's physical cores and memory\n",
        "4. **Use Clean Compute Folders**: Use the `dest_folder` parameter to keep your project organized\n",
        "5. **Handle Overwrite Carefully**: Use `overwrite_dest=True` for repeatable workflows, but be cautious about losing results\n",
        "6. **Monitor Performance**: Track execution times and adjust your configuration for optimal performance\n",
        "7. **Match Workers to Plans**: For best results, use one worker per plan when running a small number of plans\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\09_plan_parameter_operations.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS Commander: Plan Key Operations\n",
        "\n",
        "This notebook demonstrates how to perform key operations on HEC-RAS plan files using the RAS Commander library. Plan files in HEC-RAS (`.p*` files) control the simulation settings and parameters, making them essential for hydraulic modeling workflows.\n",
        "\n",
        "## Operations Covered\n",
        "\n",
        "1. **Project Initialization**: Set up a HEC-RAS project for automation\n",
        "2. **Plan Values**: Retrieve specific values from plan files\n",
        "3. **Run Flags**: Configure which components (geometry preprocessor, unsteady flow, etc.) will run\n",
        "4. **Plan Intervals**: Set computation and output time intervals\n",
        "5. **Plan Descriptions**: Read and update plan descriptions\n",
        "6. **Simulation Dates**: Modify simulation start and end dates\n",
        "\n",
        "These operations allow you to programmatically control and customize HEC-RAS simulations without opening the GUI, which is especially useful for batch processing, sensitivity analysis, and model calibration.\n",
        "\n",
        "Let's begin by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install --upgrade ras-commander\n",
        "# This installs latest version of ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "\n",
        "# Import all ras-commander modules\n",
        "from ras_commander import *\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Bald Eagle Creek example project\n",
        "# The extract_project method downloads the project from GitHub if not already present,\n",
        "# and extracts it to the example_projects folder\n",
        "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
        "\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Plan Files in HEC-RAS\n",
        "\n",
        "Before we dive into the operations, let's understand what HEC-RAS plan files are and why they're important:\n",
        "\n",
        "### What is a Plan File?\n",
        "\n",
        "A HEC-RAS plan file (`.p*`) is a configuration file that defines how a hydraulic simulation will run. It links together:\n",
        "\n",
        "1. **Geometry**: River channel and floodplain physical characteristics (`.g*` files)\n",
        "2. **Flow Data**: Inflow conditions, either steady (`.f*`) or unsteady (`.u*`)\n",
        "3. **Simulation Parameters**: Time steps, computational methods, and output settings\n",
        "\n",
        "### Key Components of Plan Files\n",
        "\n",
        "Plan files contain many parameters that control simulation behavior:\n",
        "\n",
        "- **Simulation Type**: Steady, unsteady, sediment transport, water quality\n",
        "- **Computation Intervals**: Time steps for calculations\n",
        "- **Output Intervals**: How frequently results are saved\n",
        "- **Run Flags**: Which modules to execute (preprocessor, postprocessor, etc.)\n",
        "- **Simulation Period**: Start and end dates for unsteady simulations\n",
        "- **Computation Methods**: Numerical schemes and solver settings\n",
        "- **Resource Allocation**: Number of CPU cores to use\n",
        "\n",
        "### Why Automate Plan Operations?\n",
        "\n",
        "Automating plan operations with RAS Commander allows you to:\n",
        "\n",
        "1. **Batch Processing**: Run multiple scenarios with different parameters\n",
        "2. **Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
        "3. **Calibration**: Adjust parameters to match observed data\n",
        "4. **Consistency**: Ensure standardized settings across multiple models\n",
        "5. **Documentation**: Programmatically track simulation configurations\n",
        "\n",
        "Now, let's download and extract an example project to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a RasExamples instance\n",
        "ras_examples = RasExamples()\n",
        "\n",
        "# Extract the Bald Eagle Creek example project\n",
        "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\"])\n",
        "print(f\"Extracted project to: {extracted_paths}\")\n",
        "\n",
        "# Verify the path exists\n",
        "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Project Initialization\n",
        "\n",
        "The first step in any RAS Commander workflow is initializing the HEC-RAS project. This connects the Python environment to the HEC-RAS project files.\n",
        "\n",
        "The `init_ras_project()` function does the following:\n",
        "\n",
        "1. Locates the main project file (`.prj`)\n",
        "2. Reads all associated files (plans, geometries, flows)\n",
        "3. Creates dataframes containing project components\n",
        "4. Sets up the connection to the HEC-RAS executable\n",
        "\n",
        "Let's initialize our project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the project (using the default global ras object)\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "print(f\"Initialized project: {ras.project_name}\")\n",
        "\n",
        "# Display basic project information\n",
        "print(\"\\nProject Overview:\")\n",
        "print(f\"Project Folder: {ras.project_folder}\")\n",
        "print(f\"Project File: {ras.prj_file}\")\n",
        "print(f\"Number of Plan Files: {len(ras.plan_df)}\")\n",
        "print(f\"Number of Geometry Files: {len(ras.geom_df)}\")\n",
        "print(f\"Number of Flow Files: {len(ras.flow_df)}\")\n",
        "print(f\"Number of Unsteady Files: {len(ras.unsteady_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's also take a look at the plan files in this project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the plan files\n",
        "print(\"Plan Files in Project:\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first plan number for our examples\n",
        "plan_number = ras.plan_df['plan_number'].iloc[0]\n",
        "print(f\"\\nWe'll work with Plan: {plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Updating Run Flags\n",
        "\n",
        "Run flags in HEC-RAS control which components of the simulation are executed. The `RasPlan.update_run_flags()` method allows you to modify these flags programmatically.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `geometry_preprocessor` (bool, optional): Whether to run the geometry preprocessor\n",
        "- `unsteady_flow_simulation` (bool, optional): Whether to run the unsteady flow simulation\n",
        "- `run_sediment` (bool, optional): Whether to run sediment transport calculations\n",
        "- `post_processor` (bool, optional): Whether to run the post-processor\n",
        "- `floodplain_mapping` (bool, optional): Whether to run floodplain mapping\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Common Run Flags\n",
        "\n",
        "1. **Geometry Preprocessor**: Computes hydraulic tables from geometry data\n",
        "   - `True`: Recompute tables (useful after geometry changes)\n",
        "   - `False`: Use existing tables (faster but may be outdated)\n",
        "\n",
        "2. **Unsteady Flow Simulation**: The main hydraulic calculations\n",
        "   - `True`: Run unsteady flow calculations\n",
        "   - `False`: Skip unsteady flow calculations\n",
        "\n",
        "3. **Sediment Transport**: Simulates erosion and deposition\n",
        "   - `True`: Calculate sediment transport\n",
        "   - `False`: Skip sediment transport\n",
        "\n",
        "4. **Post-Processor**: Calculates additional variables from results\n",
        "   - `True`: Run post-processing (recommended)\n",
        "   - `False`: Skip post-processing (faster but fewer outputs)\n",
        "\n",
        "5. **Floodplain Mapping**: Generates inundation maps\n",
        "   - `True`: Generate maps (requires terrain data)\n",
        "   - `False`: Skip mapping (faster)\n",
        "\n",
        "Let's update the run flags for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update run flags for the plan\n",
        "print(f\"Updating run flags for plan {plan_number}...\")\n",
        "RasPlan.update_run_flags(\n",
        "    \"01\",\n",
        "    geometry_preprocessor=False,     # This may result in a popup if preprocessor files are not present\n",
        "    unsteady_flow_simulation=False,   # Run the main hydraulic calculations\n",
        "    run_sediment=False,              # Skip sediment transport calculations\n",
        "    post_processor=False,             # Run post-processing for additional outputs\n",
        "    floodplain_mapping=True,        # Skip floodplain mapping\n",
        ")\n",
        "print(\"Run flags updated successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The dataframes won't automatically update with changes, so re-init to ensure you are reading the latest version\n",
        "init_ras_project(bald_eagle_path, \"6.6\")\n",
        "\n",
        "# Display the plan dataframe again to show changes were effective\n",
        "print(\"Plan Files in Project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_path = RasPlan.get_plan_path(\"01\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(plan_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the Plan file's contents to confirm the change\n",
        "\n",
        "# Print the plan file contents to verify the run flag changes\n",
        "with open(plan_path, 'r') as f:\n",
        "    print(f.read())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Updating Plan Intervals\n",
        "\n",
        "Time intervals in HEC-RAS control the temporal resolution of simulations and outputs. The `RasPlan.update_plan_intervals()` method allows you to modify these intervals.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `computation_interval` (str, optional): Time step for calculations\n",
        "- `output_interval` (str, optional): Time step for saving detailed results\n",
        "- `instantaneous_interval` (str, optional): Time step for peak value calculations\n",
        "- `mapping_interval` (str, optional): Time step for map outputs\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Valid Interval Values\n",
        "\n",
        "Time intervals must be specified in HEC-RAS format:\n",
        "- Seconds: `1SEC`, `2SEC`, `3SEC`, `4SEC`, `5SEC`, `6SEC`, `10SEC`, `15SEC`, `20SEC`, `30SEC`\n",
        "- Minutes: `1MIN`, `2MIN`, `3MIN`, `4MIN`, `5MIN`, `6MIN`, `10MIN`, `15MIN`, `20MIN`, `30MIN`\n",
        "- Hours: `1HOUR`, `2HOUR`, `3HOUR`, `4HOUR`, `6HOUR`, `8HOUR`, `12HOUR`\n",
        "- Days: `1DAY`\n",
        "\n",
        "### Interval Types\n",
        "\n",
        "1. **Computation Interval**: Time step used for hydraulic calculations\n",
        "   - Smaller intervals: More accurate but slower\n",
        "   - Larger intervals: Faster but may introduce numerical errors\n",
        "   - Rule of thumb: Should be small enough to capture flow changes\n",
        "\n",
        "2. **Output Interval**: How frequently detailed results are saved\n",
        "   - Smaller intervals: More detailed results but larger files\n",
        "   - Larger intervals: Smaller files but less temporal resolution\n",
        "   - Usually larger than computation interval\n",
        "\n",
        "3. **Instantaneous Interval**: Time step for peak value calculations\n",
        "   - Affects when max/min values are checked\n",
        "   - Usually equal to output interval\n",
        "\n",
        "4. **Mapping Interval**: How frequently map data is saved\n",
        "   - Affects animation smoothness and file size\n",
        "   - Usually larger than output interval\n",
        "\n",
        "Let's update the intervals for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update plan intervals\n",
        "print(f\"Updating intervals for plan {plan_number}...\")\n",
        "RasPlan.update_plan_intervals(\n",
        "    plan_number,\n",
        "    computation_interval=\"5SEC\",    # 5-second time step for calculations\n",
        "    output_interval=\"1MIN\",         # Save detailed results every minute\n",
        "    instantaneous_interval=\"5MIN\",  # Check for max/min values every 5 minutes\n",
        "    mapping_interval=\"15MIN\",       # Save map data every 15 minutes\n",
        "\n",
        ")\n",
        "print(\"Plan intervals updated successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Managing Plan Descriptions\n",
        "\n",
        "Plan descriptions provide documentation for simulation configurations. The RAS Commander library offers methods to read and update these descriptions.\n",
        "\n",
        "### Reading Descriptions\n",
        "\n",
        "The `RasPlan.read_plan_description()` method retrieves the current description from a plan file.\n",
        "\n",
        "#### Parameters\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Updating Descriptions\n",
        "\n",
        "The `RasPlan.update_plan_description()` method sets a new description for a plan file.\n",
        "\n",
        "#### Parameters\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `description` (str): The new description text\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Best Practices for Plan Descriptions\n",
        "\n",
        "Effective plan descriptions should include:\n",
        "1. Purpose of the simulation\n",
        "2. Key parameters and settings\n",
        "3. Date of creation or modification\n",
        "4. Author or organization\n",
        "5. Any special considerations or notes\n",
        "\n",
        "Let's read the current description and then update it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the current plan description\n",
        "current_description = RasPlan.read_plan_description(plan_number)\n",
        "print(f\"Current plan description:\\n{current_description}\")\n",
        "\n",
        "# Create a new description with detailed information\n",
        "new_description = f\"\"\"Modified Plan for RAS Commander Testing\n",
        "Date: {datetime.now().strftime('%Y-%m-%d')}\n",
        "Purpose: Demonstrating RAS Commander plan operations\n",
        "Settings:\n",
        "- Computation Interval: 5SEC\n",
        "- Output Interval: 1MIN\n",
        "- Mapping Interval: 15MIN\n",
        "- Geometry Preprocessor: Enabled\n",
        "- Post-Processor: Enabled\n",
        "Notes: This plan was automatically modified using ras-commander.\"\"\"\n",
        "\n",
        "# Update the plan description\n",
        "print(\"\\nUpdating plan description...\")\n",
        "RasPlan.update_plan_description(plan_number, new_description)\n",
        "print(\"Plan description updated successfully\")\n",
        "\n",
        "# Verify the updated description\n",
        "updated_description = RasPlan.read_plan_description(plan_number)\n",
        "print(f\"\\nUpdated plan description:\\n{updated_description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Updating Simulation Dates\n",
        "\n",
        "For unsteady flow simulations, the simulation period defines the time window for the analysis. The `RasPlan.update_simulation_date()` method allows you to modify this period.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
        "- `start_date` (datetime): The start date and time for the simulation\n",
        "- `end_date` (datetime): The end date and time for the simulation\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "\n",
        "### Considerations for Simulation Dates\n",
        "\n",
        "1. **Hydrograph Coverage**: The simulation period should fully encompass your hydrographs\n",
        "2. **Warm-Up Period**: Include time before the main event for model stabilization\n",
        "3. **Cool-Down Period**: Include time after the main event for complete drainage\n",
        "4. **Computational Efficiency**: Avoid unnecessarily long periods to reduce runtime\n",
        "5. **Consistency**: Ensure dates match available boundary condition data\n",
        "\n",
        "Let's update the simulation dates for our plan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current simulation date\n",
        "current_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\")\n",
        "print(f\"Current simulation date: {current_sim_date}\")\n",
        "\n",
        "# Parse the current simulation date string\n",
        "current_dates = current_sim_date.split(\",\")\n",
        "current_start = datetime.strptime(f\"{current_dates[0]},{current_dates[1]}\", \"%d%b%Y,%H%M\")\n",
        "current_end = datetime.strptime(f\"{current_dates[2]},{current_dates[3]}\", \"%d%b%Y,%H%M\")\n",
        "\n",
        "# Define new simulation period - adjust by 1 hour from current dates\n",
        "start_date = current_start + timedelta(hours=1)  # Current start + 1 hour\n",
        "end_date = current_end - timedelta(hours=1)      # Current end - 1 hour\n",
        "\n",
        "# Update the simulation date\n",
        "print(f\"\\nUpdating simulation period to: {start_date.strftime('%d%b%Y,%H%M')} - {end_date.strftime('%d%b%Y,%H%M')}\")\n",
        "RasPlan.update_simulation_date(plan_number, start_date, end_date)\n",
        "print(\"Simulation dates updated successfully\")\n",
        "\n",
        "# Verify the updated simulation date\n",
        "updated_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\")\n",
        "print(f\"\\nUpdated simulation date: {updated_sim_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verifying Updated Plan Values\n",
        "\n",
        "After making multiple changes to a plan, it's a good practice to verify that all updates were applied correctly. Let's check the updated values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the Plan file's contents to confirm the change\n",
        "\n",
        "# Print the plan file contents to verify the run flag changes\n",
        "with open(plan_path, 'r') as f:\n",
        "    print(f.read())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Computing the Plan (Optional)\n",
        "\n",
        "After making changes to a plan, you might want to run the simulation to see the effects. The `RasCmdr.compute_plan()` method executes a HEC-RAS simulation with the specified plan.\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "- `plan_number` (str): The plan number to execute\n",
        "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
        "- `rasect` (RasPrj, optional): The RAS project object\n",
        "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files\n",
        "- `num_cores` (int, optional): Number of processor cores to use\n",
        "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder\n",
        "\n",
        "If you want to run the simulation, you can uncomment the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RasCmdr.compute_plan(plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to run the simulation with the updated plan\n",
        "\n",
        "# # Define a destination folder for the computation\n",
        "# dest_folder = script_dir / \"compute_results\"\n",
        "# print(f\"Computing plan {plan_number}...\")\n",
        "# print(f\"Results will be saved to: {dest_folder}\")\n",
        "\n",
        "# # Execute the plan\n",
        "# success = RasCmdr.compute_plan(\n",
        "#     plan_number,\n",
        "#     dest_folder=dest_folder,\n",
        "#     clear_geompre=True,    # Clear preprocessor files to ensure clean results\n",
        "#     num_cores=2,           # Use 2 processor cores\n",
        "#     overwrite_dest=True,   # Overwrite existing destination folder\n",
        "#     rasect=ras\n",
        "# )\n",
        "\n",
        "# if success:\n",
        "#     print(f\"Plan {plan_number} computed successfully\")\n",
        "#     # Check for results file\n",
        "#     results_path = RasPlan.get_results_path(plan_number)\n",
        "#     if results_path:\n",
        "#         print(f\"Results saved to: {results_path}\")\n",
        "# else:\n",
        "#     print(f\"Failed to compute plan {plan_number}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Plan Key Operations\n",
        "\n",
        "In this notebook, we've covered the essential operations for manipulating HEC-RAS plan files programmatically using RAS Commander:\n",
        "\n",
        "1. **Project Initialization**: We initialized a HEC-RAS project using `init_ras_project()`\n",
        "2. **Plan Values**: We retrieved plan values with `RasPlan.get_plan_value()`\n",
        "3. **Run Flags**: We updated simulation components with `RasPlan.update_run_flags()`\n",
        "4. **Plan Intervals**: We modified time steps with `RasPlan.update_plan_intervals()`\n",
        "5. **Plan Descriptions**: We managed documentation with `RasPlan.read_plan_description()` and `RasPlan.update_plan_description()`\n",
        "6. **Simulation Dates**: We changed the analysis period with `RasPlan.update_simulation_date()`\n",
        "7. **Verification**: We verified our changes by comparing initial and updated values\n",
        "\n",
        "### Key Classes and Functions Used\n",
        "\n",
        "- `RasPlan`: The main class for plan operations\n",
        "  - `get_plan_value()`: Retrieve specific values from plan files\n",
        "  - `update_run_flags()`: Configure which components will run\n",
        "  - `update_plan_intervals()`: Set computation and output time intervals\n",
        "  - `read_plan_description()`: Get the current plan description\n",
        "  - `update_plan_description()`: Set a new plan description\n",
        "  - `update_simulation_date()`: Modify the simulation period\n",
        "  - `get_results_path()`: Get the path to results files\n",
        "\n",
        "- `RasCmdr`: The class for executing HEC-RAS simulations\n",
        "  - `compute_plan()`: Run a single plan simulation\n",
        "\n",
        "### Best Practices for Plan Operations\n",
        "\n",
        "1. **Verify Before Updating**: Always check current values before making changes\n",
        "2. **Document Changes**: Use descriptive plan descriptions to track modifications\n",
        "3. **Maintain Consistency**: Ensure flow data matches simulation dates\n",
        "4. **Use Appropriate Intervals**: Balance accuracy and computational efficiency\n",
        "5. **Backup Original Files**: Use destination folders when running simulations\n",
        "6. **Verify After Updates**: Confirm that all changes were applied correctly\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "With these plan operations, you can now:\n",
        "\n",
        "1. **Create Batch Workflows**: Process multiple scenarios with different parameters\n",
        "2. **Perform Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
        "3. **Automate Calibration**: Adjust parameters to match observed data\n",
        "4. **Build Model Ensembles**: Run multiple configurations for uncertainty analysis\n",
        "5. **Integrate with Other Tools**: Connect HEC-RAS to broader modeling frameworks\n",
        "\n",
        "These operations form the foundation for advanced HEC-RAS automation using the RAS Commander library."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
==================================================

File: c:\GH\ras-commander\examples\101_Core_Sensitivity.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14_Core_Sensitivity.ipynb\n",
        "Testing Core Sensitivity for RAS using the Bald Eagle Creek Multi-Gage 2D project.  \n",
        "\n",
        "\n",
        "This should take around 15-45 minutes to run depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ras_commander import RasExamples, init_ras_project, RasCmdr, RasPlan, RasGeo\n",
        "\n",
        "# Step 1: Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "\n",
        "RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "# Use Path.cwd() to get the current working directory in a Jupyter Notebook\n",
        "current_directory = Path.cwd()\n",
        "project_path = current_directory / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "\n",
        "# Step 2: Initialize the RAS Project Folder using init_ras_project (from ras_commander)\n",
        "init_ras_project(project_path, \"6.6\")\n",
        "\n",
        "# Step 3: Initialize a DataFrame to store execution results\n",
        "results = []\n",
        "\n",
        "# Step 4: Run sensitivity analysis for Plan 03 with core counts 1-8\n",
        "plan_number = '03'\n",
        "print(f\"Running sensitivity analysis for Plan {plan_number}\")\n",
        "\n",
        "# Clear geompre files before running the plan\n",
        "plan_path = RasPlan.get_plan_path(plan_number)\n",
        "RasGeo.clear_geompre_files(plan_path)\n",
        "\n",
        "for cores in range(1, 5):\n",
        "    print(f\"Running with {cores} core(s)\")\n",
        "    # Set core count for this plan\n",
        "    RasPlan.set_num_cores(plan_number, cores)\n",
        "    \n",
        "    # Time the execution of the plan\n",
        "    start_time = time.time()\n",
        "    RasCmdr.compute_plan(plan_number)\n",
        "    execution_time = time.time() - start_time\n",
        "    \n",
        "    # Store the results\n",
        "    results.append({\n",
        "        \"plan_number\": plan_number,\n",
        "        \"cores\": cores,\n",
        "        \"execution_time\": execution_time\n",
        "    })\n",
        "    \n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "print(\"Sensitivity analysis complete\")\n",
        "\n",
        "# Step 5: Convert results into a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Optionally, save the results to a CSV file\n",
        "results_df.to_csv(\"core_sensitivity_results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTES FOR REVISIONS:\n",
        "- Use HDF compute summary to show the time for each preprocesS/unsteady compute/postprocess step. \n",
        "- First, run preprocessor and then toggle options to only run unsteady compute and postprocess. \n",
        "- Plot each step separately. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optionally, load the results from a CSV file\n",
        "results_df = pd.read_csv(\"core_sensitivity_results.csv\")\n",
        "\n",
        "# Display the results dataframe for verification\n",
        "print(\"results_df DataFrame (time is in seconds):\")\n",
        "display(results_df)\n",
        "\n",
        "# Step 6: Calculate unit runtime (based on 1 core execution time)\n",
        "results_df['unit_runtime'] = results_df.groupby('plan_number')['execution_time'].transform(lambda x: x / x.iloc[0])\n",
        "\n",
        "# Get the project name from the ras object\n",
        "project_name = ras.project_name\n",
        "\n",
        "# Step 7: Plot a line chart for unit runtime vs. cores for each plan\n",
        "plt.figure(figsize=(10, 6))\n",
        "for plan in results_df['plan_number'].unique():\n",
        "    plan_data = results_df[results_df['plan_number'] == plan]\n",
        "    plt.plot(plan_data['cores'], plan_data['unit_runtime'], label=f\"Plan {plan}\")\n",
        "\n",
        "plt.xlabel(\"Number of Cores\")\n",
        "plt.ylabel(\"Unit Runtime (Relative to 1 Core)\")\n",
        "plt.title(f\"{project_name} (HEC Example Project)\\nCore Count Sensitivity Analysis\")\n",
        "plt.legend(title=\"Plan Number\")\n",
        "plt.grid(False)\n",
        "plt.vlines([1,2,3,4], ymin=0, ymax=1.2, linestyles='dotted', alpha=0.3)\n",
        "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdrpip4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\102_benchmarking_versions_6.1_to_6.6.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAS-Commander standard code cells 1-3: Install Packages and Prepare the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define versions to compare\n",
        "versions = ['6.6', '6.5', '6.4.1', '6.3.1', '6.3', '6.2', \"6.1\", \"6.0\"] # NOTE: ras-commander does not support versions prior to 6.2 due to HDF5 file format changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract BaldEagleCrkMulti2D project\n",
        "project_path = RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init the ras_project with ras-commander to read all HEC-RAS project information \n",
        "init_ras_project(project_path, \"6.5\")\n",
        "print(ras)\n",
        "# If no ras object is defined in init_ras_project, it defaults to \"ras\" (useful for single project scripts)\n",
        "# Display plan dataframe\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export Plan Numbers to List and Print\n",
        "plan_numbers = ras.plan_df['plan_number'].tolist()\n",
        "print(plan_numbers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define run_simulation function for\n",
        "import time\n",
        "from ras_commander import RasGeo\n",
        "\n",
        "def run_simulation(version, plan_number):\n",
        "    # Initialize project for the specific version\n",
        "    ras_project = init_ras_project(project_path, str(version))\n",
        "    \n",
        "    # Clear geometry preprocessor files for the plan\n",
        "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_project)\n",
        "    RasGeo.clear_geompre_files(plan_path, ras_object=ras_project)\n",
        "    \n",
        "    # Set the number of cores to 4\n",
        "    RasPlan.set_num_cores(plan_number, \"4\", ras_object=ras_project)\n",
        "    \n",
        "    # Update plan run flags \u2013 setting \"Run HTab\" flag to 1 to force geometry preprocessing\n",
        "    RasPlan.update_run_flags(plan_number, {\"Run HTab\": 1}, ras_object=ras_project)\n",
        "    \n",
        "    # Compute the plan\n",
        "    start_time = time.time()\n",
        "    success = RasCmdr.compute_plan(plan_number, ras_object=ras_project)\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    if success:\n",
        "        # Get the HDF file path for the plan results\n",
        "        hdf_path = RasPlan.get_results_path(plan_number, ras_object=ras_project)\n",
        "        \n",
        "        # Extract runtime data from the HDF file\n",
        "        runtime_data = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "        \n",
        "        # Extract required information from the runtime data\n",
        "        preprocessor_time = runtime_data['Preprocessing Geometry (hr)'].values[0]\n",
        "        unsteady_compute_time = runtime_data['Unsteady Flow Computations (hr)'].values[0]\n",
        "        \n",
        "        # Get volume accounting data from the HDF file\n",
        "        volume_accounting = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
        "        # Extract Error Percent from the DataFrame\n",
        "        volume_error = volume_accounting['Error Percent'].values[0] if not volume_accounting.empty else None\n",
        "        \n",
        "        # Print the extracted data\n",
        "        print(f\"\\nExtracted Data for Plan {plan_number} in Version {version}:\")\n",
        "        print(f\"Preprocessor Time: {preprocessor_time:.3f} hr\")\n",
        "        print(f\"Unsteady Compute Time: {unsteady_compute_time:.3f} hr\") \n",
        "        print(f\"Volume Error: {volume_error:.3f}%\" if volume_error is not None else \"Volume Error: None\")\n",
        "        print(f\"Total Time: {total_time/3600:.3f} hr\\n\")\n",
        "        \n",
        "        return {\n",
        "            'Version': version,\n",
        "            'Plan': plan_number,\n",
        "            'Preprocessor Time (hr)': preprocessor_time,\n",
        "            'Unsteady Compute Time (hr)': unsteady_compute_time,\n",
        "            'Volume Error (%)': volume_error,\n",
        "            'Total Time (hr)': total_time / 3600  # convert seconds to hours\n",
        "        }\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OPTIONAL: Benchmark all plans in Version 6.6\n",
        "Change the following cell from Markdown to Code and delete \"```\" to Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "#### Initialize results list\n",
        "results = []\n",
        "#### Loop through each plan number\n",
        "for plan in plan_numbers:\n",
        "    print(f\"Running simulation for Version 6.6, Plan {plan}\")\n",
        "    result = run_simulation(\"6.6\", plan)\n",
        "    if result is not None:  #### Check if result is not None\n",
        "        results.append(result)\n",
        "\n",
        "#### Convert results list to DataFrame and save as CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('hecras_plan_comparison.csv', index=False)\n",
        "\n",
        "print(\"Results saved to 'hecras_plan_comparison.csv'\")\n",
        "\n",
        "#### Load and display the results dataframe\n",
        "df = pd.read_csv('hecras_plan_comparison.csv')\n",
        "\n",
        "#### Get plan titles from ras.plan_df and merge with results\n",
        "plan_titles = pd.DataFrame({\n",
        "    'Plan': ras.plan_df['plan_number'].str.zfill(2),  #### Ensure 2-digit format\n",
        "    'Short Identifier': ras.plan_df['Short Identifier']\n",
        "})\n",
        "#### Convert df's Plan column to 2-digit string format\n",
        "df['Plan'] = df['Plan'].astype(str).str.zfill(2)\n",
        "\n",
        "df = df.merge(plan_titles, on='Plan', how='left')\n",
        "\n",
        "print(\"Benchmarking Results:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "#### Create a more comprehensive visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "#### Function to create rotated labels\n",
        "def plot_with_rotated_labels(ax, data, values, color, title, ylabel):\n",
        "    bars = ax.bar(range(len(data)), values, color=color, alpha=0.7)\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.set_ylabel(ylabel, fontsize=10)\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    #### Set x-ticks at bar positions\n",
        "    ax.set_xticks(range(len(data)))\n",
        "    \n",
        "    #### Create labels with plan numbers and titles\n",
        "    labels = [f\"Plan {plan}\\n{title}\" for plan, title in zip(data['Plan'], data['Short Identifier'])]\n",
        "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "\n",
        "#### Plot 1: Unsteady Runtime\n",
        "plot_with_rotated_labels(ax1, df, df['Unsteady Compute Time (hr)'], 'blue', \n",
        "                        'Unsteady Runtime by Plan', 'Unsteady Runtime (hours)')\n",
        "\n",
        "#### Plot 2: Volume Error  \n",
        "plot_with_rotated_labels(ax2, df, df['Volume Error (%)'], 'red',\n",
        "                        'Volume Error by Plan', 'Volume Error (%)')\n",
        "\n",
        "#### Plot 3: Preprocessor Time\n",
        "plot_with_rotated_labels(ax3, df, df['Preprocessor Time (hr)'], 'green',\n",
        "                        'Preprocessor Time by Plan', 'Preprocessor Time (hours)')\n",
        "\n",
        "#### Plot 4: Total Runtime\n",
        "plot_with_rotated_labels(ax4, df, df['Total Time (hr)'], 'purple',\n",
        "                        'Total Runtime by Plan', 'Total Runtime (hours)')\n",
        "\n",
        "#### Adjust layout and display\n",
        "plt.tight_layout(pad=3.0)\n",
        "fig.suptitle('Plan Performance Comparison', fontsize=14, y=1.02)\n",
        "plt.show()\n",
        "\n",
        "#### Calculate and display summary statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "#### Calculate plan-to-plan performance changes\n",
        "print(\"\\nPlan-to-Plan Performance Changes:\")\n",
        "df['Unsteady Runtime Change (%)'] = df['Unsteady Compute Time (hr)'].pct_change() * 100\n",
        "print(df[['Plan', 'Short Identifier', 'Unsteady Runtime Change (%)']].to_string(index=False))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the plan number you want to run across all versions\n",
        "plan_number = '02'  # Make sure this is a string and include the leading zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run simulations for all versions with plan_number defined by user\n",
        "results = []\n",
        "for version in versions:\n",
        "    print(f\"Running simulation for Version {version}, Plan {plan_number}\")\n",
        "    result = run_simulation(version, plan_number) \n",
        "    if result is not None:  # Check if result is not None\n",
        "        results.append(result)\n",
        "        print(f\"Completed: Version {version}, Plan {plan_number}\")\n",
        "    else:\n",
        "        print(f\"Failed: Version {version}, Plan {plan_number}\")\n",
        "\n",
        "# Create DataFrame from results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Save initial results to CSV\n",
        "df.to_csv('save_initial_results.csv', index=False)\n",
        "\n",
        "print(\"Initial results saved to 'save_initial_results.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create line graphs\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Unsteady Runtime vs Version\n",
        "plt.subplot(1, 2, 1)\n",
        "# Convert Version to categorical type to handle string versions properly\n",
        "plt.plot(pd.Categorical(df['Version']), df['Unsteady Compute Time (hr)'], marker='o')\n",
        "plt.title(f'Unsteady Runtime vs HEC-RAS Version (Plan {plan_number})')\n",
        "plt.xlabel('HEC-RAS Version')\n",
        "plt.ylabel('Unsteady Runtime (hours)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Volume Error vs Version\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(pd.Categorical(df['Version']), df['Volume Error (%)'], marker='o')\n",
        "plt.title(f'Volume Error vs HEC-RAS Version (Plan {plan_number})')\n",
        "plt.xlabel('HEC-RAS Version')\n",
        "plt.ylabel('Volume Error (%)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdrpip4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\103_Running_AEP_Events_from_Atlas_14.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AEP Storm Analysis with RAS-Commander\n",
        "\n",
        "This notebook automates the end-to-end process of analyzing multiple storm events with different Annual Exceedance Probabilities (AEP) in HEC-RAS. It covers:\n",
        "\n",
        "1. Generating hyetographs from NOAA Atlas 14 precipitation data\n",
        "2. Creating HEC-RAS plan files for each AEP event\n",
        "3. Creating unsteady flow files with the generated hyetographs\n",
        "4. Executing multiple plans in parallel\n",
        "5. Analyzing and visualizing the results\n",
        "\n",
        "This automation is particularly useful for analyzing how a drainage system performs under different storm frequencies, from common events (e.g., 2-year) to rare events (e.g., 100-year)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Import Libraries\n",
        "\n",
        "First, we'll import all the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "\n",
        "# Install ras-commander if not already installed\n",
        "# Uncomment this line if you need to install the package\n",
        "# !pip install ras-commander\n",
        "\n",
        "# Import RAS-Commander modules\n",
        "from ras_commander import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RasExamples.extract_project([\"Davis\"])\n",
        "# This loads the project in fresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Hyetograph Generation Functions\n",
        "\n",
        "These functions handle reading precipitation frequency data from NOAA Atlas 14 and generating balanced storm hyetographs using the Alternating Block Method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_duration(duration_str):\n",
        "    \"\"\"\n",
        "    Parses a duration string and converts it to hours.\n",
        "    Examples: \"5-min:\" -> 0.0833 hours, \"2-hr:\" -> 2 hours, \"2-day:\" -> 48 hours\n",
        "    \"\"\"\n",
        "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
        "    if not match:\n",
        "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
        "    value, unit = match.groups()\n",
        "    value = int(value)\n",
        "    unit = unit.lower()\n",
        "    if unit in ['min', 'minute', 'minutes']:\n",
        "        hours = value / 60.0\n",
        "    elif unit in ['hr', 'hour', 'hours']:\n",
        "        hours = value\n",
        "    elif unit in ['day', 'days']:\n",
        "        hours = value * 24\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
        "    return hours\n",
        "\n",
        "def read_precipitation_data(csv_file):\n",
        "    \"\"\"\n",
        "    Reads the precipitation frequency CSV and returns a DataFrame\n",
        "    with durations in hours as the index and ARIs as columns.\n",
        "    \"\"\"\n",
        "    with open(csv_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    header_line_idx = None\n",
        "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
        "\n",
        "    # Locate the header line\n",
        "    for idx, line in enumerate(lines):\n",
        "        if header_pattern.match(line.strip().lower()):\n",
        "            header_line_idx = idx\n",
        "            break\n",
        "\n",
        "    if header_line_idx is None:\n",
        "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
        "\n",
        "    # Extract the ARI headers from the header line\n",
        "    header_line = lines[header_line_idx].strip()\n",
        "    headers = [item.strip() for item in header_line.split(',')]\n",
        "    \n",
        "    if len(headers) < 2:\n",
        "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
        "\n",
        "    aris = headers[1:]  # Exclude the first column which is the duration\n",
        "\n",
        "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
        "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
        "\n",
        "    # Initialize lists to store durations and corresponding depths\n",
        "    durations = []\n",
        "    depths = {ari: [] for ari in aris}\n",
        "\n",
        "    # Iterate over the lines following the header to extract data\n",
        "    for line in lines[header_line_idx + 1:]:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue  # Skip empty lines\n",
        "        if not duration_pattern.match(line):\n",
        "            break  # Stop if the line does not match the duration pattern\n",
        "        parts = [part.strip() for part in line.split(',')]\n",
        "        if len(parts) != len(headers):\n",
        "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
        "        duration_str = parts[0]\n",
        "        try:\n",
        "            duration_hours = parse_duration(duration_str)\n",
        "        except ValueError as ve:\n",
        "            print(f\"Skipping line due to error: {ve}\")\n",
        "            continue  # Skip lines with invalid duration formats\n",
        "        durations.append(duration_hours)\n",
        "        for ari, depth_str in zip(aris, parts[1:]):\n",
        "            try:\n",
        "                depth = float(depth_str)\n",
        "            except ValueError:\n",
        "                depth = np.nan  # Assign NaN for invalid depth values\n",
        "            depths[ari].append(depth)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(depths, index=durations)\n",
        "    df.index.name = 'Duration_hours'\n",
        "\n",
        "    # Drop any rows with NaN values\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "def interpolate_depths(df, total_duration):\n",
        "    \"\"\"\n",
        "    Interpolates precipitation depths for each ARI on a log-log scale\n",
        "    for each hour up to the total storm duration.\n",
        "    \"\"\"\n",
        "    T = total_duration\n",
        "    t_hours = np.arange(1, T+1)\n",
        "    D = {}\n",
        "    for ari in df.columns:\n",
        "        durations = df.index.values\n",
        "        depths = df[ari].values\n",
        "        # Ensure all depths are positive\n",
        "        if np.any(depths <= 0):\n",
        "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
        "        # Log-log interpolation\n",
        "        log_durations = np.log(durations)\n",
        "        log_depths = np.log(depths)\n",
        "        log_t = np.log(t_hours)\n",
        "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
        "        D_t = np.exp(log_D_t)\n",
        "        D[ari] = D_t\n",
        "    return D\n",
        "\n",
        "def compute_incremental_depths(D, total_duration):\n",
        "    \"\"\"\n",
        "    Computes incremental precipitation depths for each hour.\n",
        "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
        "    \"\"\"\n",
        "    incremental_depths = {}\n",
        "    for ari, D_t in D.items():\n",
        "        I_t = np.empty(total_duration)\n",
        "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
        "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
        "        incremental_depths[ari] = I_t\n",
        "    return incremental_depths\n",
        "\n",
        "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
        "    \"\"\"\n",
        "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
        "    \"\"\"\n",
        "    hyetograph = [0.0] * T\n",
        "    hyetograph[central_index] = max_depth\n",
        "    remaining_depths = sorted_depths.copy()\n",
        "    remaining_depths.remove(max_depth)\n",
        "    left = central_index - 1\n",
        "    right = central_index + 1\n",
        "    toggle = True  # Start assigning to the right\n",
        "    for depth in remaining_depths:\n",
        "        if toggle and right < T:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif not toggle and left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        elif right < T:\n",
        "            hyetograph[right] = depth\n",
        "            right += 1\n",
        "        elif left >= 0:\n",
        "            hyetograph[left] = depth\n",
        "            left -= 1\n",
        "        else:\n",
        "            print(\"Warning: Not all incremental depths assigned.\")\n",
        "            break\n",
        "        toggle = not toggle\n",
        "    return hyetograph\n",
        "\n",
        "def generate_hyetograph(incremental_depths, position_percent, T):\n",
        "    \"\"\"\n",
        "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
        "    \"\"\"\n",
        "    max_depth = np.max(incremental_depths)\n",
        "    incremental_depths_list = incremental_depths.tolist()\n",
        "    central_index = int(round(T * position_percent / 100)) - 1\n",
        "    central_index = max(0, min(central_index, T - 1))\n",
        "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
        "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
        "    return hyetograph\n",
        "\n",
        "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
        "    \"\"\"\n",
        "    Saves the hyetograph to a CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'Time_hour': np.arange(1, total_duration + 1),\n",
        "        'Precipitation_in': hyetograph\n",
        "    })\n",
        "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
        "    output_file = os.path.join(output_dir, filename)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
        "    \"\"\"\n",
        "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    for ari in aris:\n",
        "        # Ensure ARI is a string for consistent filename formatting\n",
        "        ari_str = str(ari)\n",
        "        \n",
        "        # Construct the filename based on the naming convention\n",
        "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        \n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Warning: File '{filename}' does not exist in the directory '{output_dir}'. Skipping this ARI.\")\n",
        "            continue\n",
        "        \n",
        "        # Read the hyetograph data\n",
        "        try:\n",
        "            hyetograph_df = pd.read_csv(filepath)\n",
        "            print(f\"Successfully read the hyetograph data from '{filename}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading the hyetograph CSV file '{filename}': {e}\")\n",
        "            continue\n",
        "        \n",
        "        # Plot the hyetograph\n",
        "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
        "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
        "    \n",
        "    # Customize the plot\n",
        "    plt.xlabel('Time (Hour)', fontsize=14)\n",
        "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
        "    plt.title(f'Comparison of Hyetographs for ARIs {aris}\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 24)))  # Adjust x-ticks based on duration\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate AEP Hydrographs\n",
        "\n",
        "This cell orchestrates the entire AEP analysis process, generating hyetographs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Main function to run the entire AEP analysis process.\n",
        "\"\"\"\n",
        "# Set the paths and parameters\n",
        "input_csv = 'data/PF_Depth_English_PDS_DavisCA.csv'  # Path to NOAA Atlas 14 data\n",
        "output_dir = 'hyetographs'  # Directory for saving hyetographs\n",
        "position_percent = 50  # Position percentage for the maximum incremental depth block\n",
        "total_duration = 24  # Storm duration in hours\n",
        "base_plan = \"02\"  # Base plan to clone\n",
        "\n",
        "# Set the AEP events (return periods in years)\n",
        "aep_events = [2, 5, 10, 25, 50, 100]\n",
        "\n",
        "# Ensure the output directory exists\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory is set to: {output_dir}\")\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "# Step 1: Generate hyetographs for each AEP event\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 1: Generating hyetographs for each AEP event...\")\n",
        "\n",
        "try:\n",
        "    # Read precipitation data\n",
        "    df = read_precipitation_data(input_csv)\n",
        "    print(\"Successfully read the input CSV file.\")\n",
        "    \n",
        "    # Display the first few rows of the DataFrame to verify\n",
        "    print(\"\\nPrecipitation Frequency Data from Atlas 14:\")\n",
        "    display.display(df.head())\n",
        "    \n",
        "    # Interpolate depths\n",
        "    D = interpolate_depths(df, total_duration)\n",
        "    print(\"Successfully interpolated precipitation depths.\")\n",
        "\n",
        "    print(\"Array D with interpolated depths\")\n",
        "    display.display(D)\n",
        "    \n",
        "    # Compute incremental depths\n",
        "    inc_depths = compute_incremental_depths(D, total_duration)\n",
        "    print(\"Successfully computed incremental depths.\")\n",
        "    \n",
        "    # Show Incremental Depths\n",
        "    print(\"Array inc_depths Contents \")\n",
        "    display.display(inc_depths)\n",
        "\n",
        "    # Generate and save hyetographs for each AEP\n",
        "    hyetograph_files = {}\n",
        "    for ari in aep_events:\n",
        "        ari_str = str(ari)\n",
        "        if ari_str in inc_depths:\n",
        "            hyetograph = generate_hyetograph(inc_depths[ari_str], position_percent, total_duration)\n",
        "            file_path = save_hyetograph(hyetograph, ari_str, output_dir, position_percent, total_duration)\n",
        "            hyetograph_files[ari_str] = file_path\n",
        "        else:\n",
        "            print(f\"Warning: ARI {ari_str} not found in the data. Skipping.\")\n",
        "    \n",
        "    print(\"\\nAll hyetographs have been generated and saved.\")\n",
        "    \n",
        "    # Plot the hyetographs for comparison\n",
        "    plot_multiple_hyetographs(aep_events, position_percent, total_duration, output_dir)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error generating hyetographs: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Initialize the HEC-RAS project\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 2: Initializing the HEC-RAS ras...\")\n",
        "\n",
        "# Define the path to the Davis project\n",
        "current_dir = Path.cwd()\n",
        "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
        "\n",
        "# Check if the project exists\n",
        "if not pipes_ex_path.exists():\n",
        "    # Extract the project if needed\n",
        "    RasExamples.extract_project([\"Davis\"])\n",
        "\n",
        "# Initialize the RAS project\n",
        "init_ras_project(pipes_ex_path, \"6.6\")\n",
        "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
        "\n",
        "# Display the existing plans\n",
        "print(\"\\nExisting plans in the project:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ras.boundaries_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"01\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"01\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsteady_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract boundary conditions and tables\n",
        "boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\n",
        "print(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "boundaries_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the contents of Unsteady File\n",
        "with open(unsteady_file, 'r') as f:\n",
        "    unsteady_contents = f.read()\n",
        "print(f\"Contents of unsteady flow file {unsteady_file}:\")\n",
        "print(\"-\" * 80)\n",
        "print(unsteady_contents)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To implement AEP event hydrographs, we will edit the Precipitation Hydrograph table\n",
        "We will need to edit both the number of values, as well as replacing the existing fixed-width table.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define HEC-RAS Plan and Unsteady Flow File Functions\n",
        "\n",
        "These functions handle creating HEC-RAS plan files and unsteady flow files for each AEP event. They apply the generated hyetographs to the boundary conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_plan_for_aep(base_plan, aep_years, duration_hours, hyetograph_file, project):\n",
        "    \"\"\"\n",
        "    Creates a new plan for a specific AEP event.\n",
        "    \"\"\"\n",
        "    # Create plan name and short ID\n",
        "    plan_name = f\"{aep_years}YR-{duration_hours}HR\"\n",
        "    \n",
        "    print(f\"Creating new plan '{plan_name}'...\")\n",
        "    \n",
        "    # Clone the base plan\n",
        "    new_plan_number = RasPlan.clone_plan(base_plan, new_plan_shortid=plan_name, ras_object=project)\n",
        "    print(f\"Created new plan: {new_plan_number}\")\n",
        "    \n",
        "    # Clone the unsteady flow file from the base plan\n",
        "    base_unsteady = None\n",
        "    for _, row in project.plan_df.iterrows():\n",
        "        if row['plan_number'] == base_plan:\n",
        "            base_unsteady = row.get('unsteady_number', None)\n",
        "            \n",
        "    if base_unsteady is None:\n",
        "        raise ValueError(f\"Could not find unsteady flow file for base plan {base_plan}\")\n",
        "\n",
        "    \n",
        "    new_unsteady_number = RasPlan.clone_unsteady(base_unsteady, ras_object=project)\n",
        "    print(f\"Created new unsteady flow file: {new_unsteady_number}\")\n",
        "    \n",
        "    # Update the unsteady flow file with the hyetograph data\n",
        "    unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object=project)\n",
        "    \n",
        "    \n",
        "    # Update the flow title to reflect the AEP event\n",
        "    new_title = f\"{aep_years}YR-{duration_hours}HR Storm\"\n",
        "    RasUnsteady.update_flow_title(unsteady_file_path, new_title, ras_object=project)\n",
        "    print(f\"Updated unsteady flow title to: {new_title}\")\n",
        "    \n",
        "    # Modify the unsteady flow file with the hyetograph data\n",
        "    success = modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, project)\n",
        "    if success:\n",
        "        print(f\"Successfully applied hyetograph data from {hyetograph_file} to unsteady flow file\")\n",
        "    else:\n",
        "        print(f\"Warning: Failed to apply hyetograph data. Unsteady flow file may need manual modification.\")\n",
        "    \n",
        "    # Assign the unsteady flow file to the plan\n",
        "    RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object=project)\n",
        "    print(f\"Assigned unsteady flow file {new_unsteady_number} to plan {new_plan_number}\")\n",
        "    '''\n",
        "    # Update the plan description\n",
        "    description = f\"AEP {aep_years}-year, {duration_hours}-hour storm\\n\"\n",
        "    description += f\"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "    description += f\"Based on plan {base_plan}\\n\"\n",
        "    description += f\"Hyetograph from: {os.path.basename(hyetograph_file)}\"\n",
        "    \n",
        "    RasPlan.update_plan_description(new_plan_number, description, ras_object=project)\n",
        "    print(f\"Updated plan description for plan {new_plan_number}\")\n",
        "    \n",
        "    return new_plan_number, new_unsteady_number\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modify_unsteady_flow_with_hyetograph(unsteady_file_path, hyetograph_file, project):\n",
        "    \"\"\"\n",
        "    Modifies an unsteady flow file to incorporate hyetograph data as precipitation.\n",
        "    \n",
        "    Parameters:\n",
        "    - unsteady_file_path: Path to the unsteady flow file\n",
        "    - hyetograph_file: Path to the hyetograph data CSV\n",
        "    - project: RAS project object\n",
        "    \n",
        "    Returns:\n",
        "    - Boolean indicating success\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the hyetograph data\n",
        "        hyetograph_df = pd.read_csv(hyetograph_file)\n",
        "        print(f\"Loaded hyetograph from {hyetograph_file} with {len(hyetograph_df)} values\")\n",
        "        \n",
        "        # Read the unsteady flow file\n",
        "        with open(unsteady_file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        \n",
        "        # Find the sections that need to be modified\n",
        "        precip_hydrograph_index = None\n",
        "        \n",
        "        for i, line in enumerate(lines):\n",
        "            if line.startswith(\"Precipitation Hydrograph=\"):\n",
        "                precip_hydrograph_index = i\n",
        "                break\n",
        "        \n",
        "        if precip_hydrograph_index is None:\n",
        "            print(\"Cannot find Precipitation Hydrograph section in unsteady file.\")\n",
        "            return False\n",
        "        \n",
        "        # Get the time interval from the hyetograph\n",
        "        time_interval = \"1HOUR\"  # Default\n",
        "        if \"Time_hour\" in hyetograph_df.columns and len(hyetograph_df) > 1:\n",
        "            hour_diff = hyetograph_df[\"Time_hour\"].iloc[1] - hyetograph_df[\"Time_hour\"].iloc[0]\n",
        "            time_interval = f\"{int(hour_diff)}HOUR\" if hour_diff >= 1 else f\"{int(hour_diff*60)}MIN\"\n",
        "        \n",
        "        # Format the precipitation values for the hydrograph\n",
        "        precipitation_values = hyetograph_df[\"Precipitation_in\"].values\n",
        "        \n",
        "        # Create the Precipitation Hydrograph line\n",
        "        precip_line = f\"Precipitation Hydrograph= {len(precipitation_values)} \\n\"\n",
        "        \n",
        "        # Format the values in groups of 10 per line\n",
        "        value_lines = []\n",
        "        for i in range(0, len(precipitation_values), 10):\n",
        "            row_values = precipitation_values[i:i+10]\n",
        "            row_line = \"\".join([f\"{value:8.2f}\" for value in row_values]) + \"\\n\"\n",
        "            value_lines.append(row_line)\n",
        "            \n",
        "        # Remove old hydrograph data - find end of current hydrograph\n",
        "        current_line = precip_hydrograph_index + 1\n",
        "        while current_line < len(lines) and not any(lines[current_line].startswith(prefix) for prefix in [\"DSS Path=\", \"Use DSS=\", \"Use Fixed Start Time=\"]):\n",
        "            current_line += 1\n",
        "            \n",
        "        # Replace the hydrograph section\n",
        "        lines[precip_hydrograph_index:current_line] = [precip_line] + value_lines\n",
        "            \n",
        "        # Write the modified file back\n",
        "        with open(unsteady_file_path, 'w') as file:\n",
        "            file.writelines(lines)\n",
        "            \n",
        "        print(f\"Successfully applied hyetograph data from {hyetograph_file} to unsteady flow file.\")\n",
        "        print(f\"Added {len(precipitation_values)} precipitation values with interval {time_interval}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error modifying unsteady flow file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Create new plans for each AEP event\n",
        "#-------------------------------------------------------------------------\n",
        "print(\"\\nStep 3: Creating new plans for each AEP event...\")\n",
        "\n",
        "new_plan_numbers = []\n",
        "\n",
        "for ari in aep_events:\n",
        "    ari_str = str(ari)\n",
        "    if ari_str in hyetograph_files:\n",
        "        try:\n",
        "            # Create a new plan for this AEP event\n",
        "            new_plan_number, _ = create_plan_for_aep(\n",
        "                base_plan=base_plan,\n",
        "                aep_years=ari_str,\n",
        "                duration_hours=total_duration,\n",
        "                hyetograph_file=hyetograph_files[ari_str],\n",
        "                project=ras\n",
        "            )\n",
        "            new_plan_numbers.append(new_plan_number)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plan for AEP {ari_str}: {e}\")\n",
        "\n",
        "# Display the updated plans\n",
        "print(\"\\nUpdated plans in the project:\")\n",
        "display.display(ras.plan_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"03\"\n",
        "unsteady_file_rev = RasPlan.get_unsteady_path(\"03\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file_rev}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsteady_file_rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(unsteady_file_rev, 'r') as f:\n",
        "    unsteady_contents_rev = f.read()\n",
        "print(f\"Contents of unsteady flow file for plan 03 ({unsteady_file_rev}):\")\n",
        "print(\"-\" * 80)\n",
        "print(unsteady_contents_rev)\n",
        "print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import difflib\n",
        "\n",
        "def show_file_diff(file1_path, file2_path, context_lines=3):\n",
        "    \"\"\"\n",
        "    Shows the differences between two files with context.\n",
        "    \n",
        "    Parameters:\n",
        "    - file1_path: Path to the first file\n",
        "    - file2_path: Path to the second file\n",
        "    - context_lines: Number of context lines to show around differences\n",
        "    \"\"\"\n",
        "    # Read the file contents\n",
        "    with open(file1_path, 'r') as file1:\n",
        "        file1_lines = file1.readlines()\n",
        "    \n",
        "    with open(file2_path, 'r') as file2:\n",
        "        file2_lines = file2.readlines()\n",
        "    \n",
        "    # Create a differ object\n",
        "    differ = difflib.unified_diff(\n",
        "        file1_lines, \n",
        "        file2_lines,\n",
        "        fromfile=str(file1_path),\n",
        "        tofile=str(file2_path),\n",
        "        n=context_lines\n",
        "    )\n",
        "    \n",
        "    # Convert differ output to a string\n",
        "    diff_text = ''.join(differ)\n",
        "    \n",
        "    # If no differences found\n",
        "    if not diff_text:\n",
        "        print(f\"No differences found between {file1_path} and {file2_path}\")\n",
        "        return\n",
        "    \n",
        "    # Print the differences\n",
        "    print(f\"Differences between {file1_path} and {file2_path}:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(diff_text)\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Count added, removed, and modified lines\n",
        "    added = sum(1 for line in diff_text.splitlines() if line.startswith('+') and not line.startswith('+++'))\n",
        "    removed = sum(1 for line in diff_text.splitlines() if line.startswith('-') and not line.startswith('---'))\n",
        "    \n",
        "    print(f\"Summary: {added} additions, {removed} removals\")\n",
        "\n",
        "# Show differences between the unsteady flow files\n",
        "if 'unsteady_file' in locals() and 'unsteady_file_rev' in locals():\n",
        "    show_file_diff(unsteady_file, unsteady_file_rev)\n",
        "else:\n",
        "    print(\"Error: One or both unsteady flow file variables not defined.\")\n",
        "    print(\"Please run the cells that define unsteady_file and unsteady_file_rev first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Parallel Execution and Results Analysis Functions\n",
        "\n",
        "These functions manage parallel plan execution with resource optimization and extract, analyze, and visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_plans_in_parallel(plan_numbers, project, max_workers=None, cores_per_worker=2):\n",
        "    \"\"\"\n",
        "    Executes multiple plans in parallel.\n",
        "    \"\"\"\n",
        "    # Calculate optimal number of workers if not provided\n",
        "    if max_workers is None:\n",
        "        physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
        "        max_workers = max(1, physical_cores // cores_per_worker)\n",
        "    \n",
        "    print(f\"Executing {len(plan_numbers)} plans in parallel with {max_workers} workers, \" + \n",
        "          f\"each using {cores_per_worker} cores...\")\n",
        "    \n",
        "    # Create compute folder\n",
        "    compute_folder = Path(project.project_folder) / \"compute_aep_parallel\"\n",
        "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Execute plans in parallel\n",
        "    start_time = time.time()\n",
        "    \n",
        "    results = RasCmdr.compute_parallel(\n",
        "        plan_number=plan_numbers,\n",
        "        max_workers=max_workers,\n",
        "        num_cores=cores_per_worker,\n",
        "        dest_folder=compute_folder,\n",
        "        clear_geompre=True,\n",
        "        overwrite_dest=True,\n",
        "        ras_object=project\n",
        "    )\n",
        "    \n",
        "    end_time = time.time()\n",
        "    total_duration = end_time - start_time\n",
        "    \n",
        "    print(f\"Parallel execution completed in {total_duration:.2f} seconds\")\n",
        "    \n",
        "    # Create a DataFrame from the execution results\n",
        "    results_df = pd.DataFrame([\n",
        "        {\"Plan\": plan, \"Success\": success}\n",
        "        for plan, success in results.items()\n",
        "    ])\n",
        "    \n",
        "    # Sort by plan number\n",
        "    results_df = results_df.sort_values(\"Plan\")\n",
        "    \n",
        "    print(\"\\nExecution Results:\")\n",
        "    display.display(results_df)\n",
        "    \n",
        "    return results, compute_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the path to unsteady flow file associated with Plan \"01\"\n",
        "unsteady_file = RasPlan.get_unsteady_path(\"01\")\n",
        "print(f\"Unsteady flow file path: {unsteady_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# Execute all plans in parallel\n",
        "#-------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set computation parameters for better performance\n",
        "for plan_number in new_plan_numbers:\n",
        "    RasPlan.set_num_cores(plan_number, 2, ras_object=ras)\n",
        "    RasPlan.update_plan_intervals(\n",
        "        plan_number,\n",
        "        computation_interval=\"15MIN\",\n",
        "        output_interval=\"30MIN\",\n",
        "        mapping_interval=\"1HOUR\",\n",
        "        ras_object=ras\n",
        "    )\n",
        "    print(f\"Updated computation settings for plan {plan_number}\")\n",
        "\n",
        "# Execute plans in parallel\n",
        "results, compute_folder = execute_plans_in_parallel(\n",
        "    plan_numbers=new_plan_numbers,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\nStep 4: Executing all plans in parallel...\")\n",
        "\n",
        "results, compute_folder = execute_plans_in_parallel(\n",
        "    plan_numbers=new_plan_numbers,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df = HdfResultsPlan.get_runtime_data(\"02\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(\"02\", variable=\"Pipes/Pipe Flow DS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws = HdfPipe.get_pipe_network_timeseries(\"02\", variable=\"Nodes/Water Surface\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_results(results, compute_folder, project):\n",
        "    \"\"\"\n",
        "    Analyzes the results from multiple plans.\n",
        "    \"\"\"\n",
        "    print(\"Analyzing results from parallel execution...\")\n",
        "    \n",
        "    # Initialize a RAS project in the compute folder\n",
        "    compute_project = RasPrj()\n",
        "    compute_project = init_ras_project(compute_folder, \"6.6\", ras_object=compute_project)\n",
        "    print(f\"Initialized compute project: {compute_project.project_name}\")\n",
        "    \n",
        "    # Check which plans have results\n",
        "    plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
        "    print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
        "    display.display(plans_with_results[['plan_number', 'Short Identifier', 'HDF_Results_Path']])\n",
        "    \n",
        "    # Initialize a dictionary to store analysis results\n",
        "    analysis_results = {}\n",
        "    \n",
        "    # Analyze each plan's results\n",
        "    for idx, row in plans_with_results.iterrows():\n",
        "        plan_number = row['plan_number']\n",
        "        plan_name = row['Short Identifier']\n",
        "        hdf_path = row['HDF_Results_Path']\n",
        "        \n",
        "        print(f\"\\nAnalyzing results for plan {plan_number} ({plan_name})...\")\n",
        "        \n",
        "        try:\n",
        "            # Get runtime data\n",
        "            runtime_df = HdfResultsPlan.get_runtime_data(hdf_path)\n",
        "            \n",
        "            if runtime_df is not None and not runtime_df.empty:\n",
        "                # Extract key metrics\n",
        "                sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
        "                compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
        "                compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
        "                \n",
        "                # Get pipe network results\n",
        "                try:\n",
        "                    # Get pipe flow data\n",
        "                    pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Pipes/Pipe Flow DS\")\n",
        "                    node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "                    \n",
        "                    # Convert xarray DataArrays to numpy arrays and compute statistics\n",
        "                    pipe_flow_array = pipe_flow_ds.values\n",
        "                    node_ws_array = node_ws.values\n",
        "                    \n",
        "                    # Calculate maximum flows and water surfaces\n",
        "                    max_flows = np.nanmax(pipe_flow_array, axis=0)  # Max over time for each location\n",
        "                    avg_max_flow = np.nanmean(max_flows)\n",
        "                    max_max_flow = np.nanmax(max_flows)\n",
        "                    \n",
        "                    max_ws = np.nanmax(node_ws_array, axis=0)  # Max over time for each node\n",
        "                    avg_max_ws = np.nanmean(max_ws)\n",
        "                    max_max_ws = np.nanmax(max_ws)\n",
        "                    \n",
        "                    # Store results in the dictionary\n",
        "                    analysis_results[plan_name] = {\n",
        "                        'Plan Number': plan_number,\n",
        "                        'Simulation Duration (s)': sim_duration,\n",
        "                        'Compute Time (hr)': compute_time,\n",
        "                        'Compute Speed (hr/hr)': compute_speed,\n",
        "                        'Average Max Pipe Flow (cfs)': avg_max_flow,\n",
        "                        'Maximum Pipe Flow (cfs)': max_max_flow,\n",
        "                        'Average Max Node Water Surface (ft)': avg_max_ws,\n",
        "                        'Maximum Node Water Surface (ft)': max_max_ws,\n",
        "                        'HDF Path': hdf_path\n",
        "                    }\n",
        "                    \n",
        "                    print(f\"  Simulation Duration: {sim_duration:.2f} seconds\")\n",
        "                    print(f\"  Computation Time: {compute_time:.5f} hours\")\n",
        "                    print(f\"  Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
        "                    print(f\"  Average Max Pipe Flow: {avg_max_flow:.2f} cfs\")\n",
        "                    print(f\"  Maximum Pipe Flow: {max_max_flow:.2f} cfs\")\n",
        "                    print(f\"  Average Max Node Water Surface: {avg_max_ws:.2f} ft\")\n",
        "                    print(f\"  Maximum Node Water Surface: {max_max_ws:.2f} ft\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"  Error analyzing pipe network data: {str(e)}\")\n",
        "                    analysis_results[plan_name] = {\n",
        "                        'Plan Number': plan_number,\n",
        "                        'Simulation Duration (s)': sim_duration,\n",
        "                        'Compute Time (hr)': compute_time,\n",
        "                        'Compute Speed (hr/hr)': compute_speed,\n",
        "                        'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                        'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                        'Average Max Node Water Surface (ft)': np.nan,\n",
        "                        'Maximum Node Water Surface (ft)': np.nan,\n",
        "                        'HDF Path': hdf_path\n",
        "                    }\n",
        "            else:\n",
        "                print(\"  No runtime data found.\")\n",
        "                analysis_results[plan_name] = {\n",
        "                    'Plan Number': plan_number,\n",
        "                    'Simulation Duration (s)': np.nan,\n",
        "                    'Compute Time (hr)': np.nan,\n",
        "                    'Compute Speed (hr/hr)': np.nan,\n",
        "                    'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                    'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                    'Average Max Node Water Surface (ft)': np.nan,\n",
        "                    'Maximum Node Water Surface (ft)': np.nan,\n",
        "                    'HDF Path': hdf_path\n",
        "                }\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error analyzing plan {plan_number}: {str(e)}\")\n",
        "            analysis_results[plan_name] = {\n",
        "                'Plan Number': plan_number,\n",
        "                'Simulation Duration (s)': np.nan,\n",
        "                'Compute Time (hr)': np.nan,\n",
        "                'Compute Speed (hr/hr)': np.nan,\n",
        "                'Average Max Pipe Flow (cfs)': np.nan,\n",
        "                'Maximum Pipe Flow (cfs)': np.nan,\n",
        "                'Average Max Node Water Surface (ft)': np.nan,\n",
        "                'Maximum Node Water Surface (ft)': np.nan,\n",
        "                'HDF Path': hdf_path\n",
        "            }\n",
        "    \n",
        "    # Create a DataFrame from the analysis results\n",
        "    analysis_df = pd.DataFrame.from_dict(analysis_results, orient='index')\n",
        "    \n",
        "    # Extract AEP years and handle NaN values\n",
        "    analysis_df['AEP_Years'] = analysis_df.index.str.extract(r'(\\d+)YR').astype(float)\n",
        "    \n",
        "    # Sort by AEP years, handling the base plan\n",
        "    analysis_df = analysis_df.sort_values('AEP_Years', na_position='first')\n",
        "    \n",
        "    # Drop the temporary column used for sorting\n",
        "    analysis_df = analysis_df.drop(columns=['AEP_Years'])\n",
        "    \n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    display.display(analysis_df)\n",
        "    \n",
        "    return analysis_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------------\n",
        "# Step 5: Analyze the results\n",
        "#---------------------------------------------------------------------\n",
        "print(\"\\nStep 5: Analyzing the results...\")\n",
        "\n",
        "analysis_df = analyze_results(\n",
        "    results=results,\n",
        "    compute_folder=compute_folder,\n",
        "    project=ras\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_results(analysis_df):\n",
        "    \"\"\"\n",
        "    Plots the results from the analysis.\n",
        "    \"\"\"\n",
        "    # Extract AEP values from the index (plan names), skipping non-AEP plans\n",
        "    aep_values = []\n",
        "    aep_data = pd.DataFrame()\n",
        "    \n",
        "    for name in analysis_df.index:\n",
        "        if 'YR' in name:\n",
        "            try:\n",
        "                aep_year = int(name.split('YR')[0])\n",
        "                aep_values.append(aep_year)\n",
        "                aep_data = pd.concat([aep_data, analysis_df.loc[[name]]])\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    if len(aep_values) == 0:\n",
        "        print(\"No valid AEP plans found to plot\")\n",
        "        return\n",
        "        \n",
        "    # Create a figure with multiple subplots\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Plot 1: Maximum Pipe Flow vs AEP\n",
        "    axs[0].semilogx(aep_values, aep_data['Maximum Pipe Flow (cfs)'], 'o-', marker='o', markersize=8)\n",
        "    axs[0].set_title('Maximum Pipe Flow vs Return Period', fontsize=16)\n",
        "    axs[0].set_xlabel('Return Period (years)', fontsize=14)\n",
        "    axs[0].set_ylabel('Maximum Pipe Flow (cfs)', fontsize=14)\n",
        "    axs[0].grid(True)\n",
        "    \n",
        "    # Add data labels\n",
        "    for i, txt in enumerate(aep_values):\n",
        "        axs[0].annotate(f\"{txt} yr\", \n",
        "                      (aep_values[i], aep_data['Maximum Pipe Flow (cfs)'].iloc[i]),\n",
        "                      textcoords=\"offset points\", \n",
        "                      xytext=(0, 10), \n",
        "                      ha='center')\n",
        "    \n",
        "    # Plot 2: Maximum Node Water Surface vs AEP\n",
        "    axs[1].semilogx(aep_values, aep_data['Maximum Node Water Surface (ft)'], 'o-', marker='s', markersize=8, color='green')\n",
        "    axs[1].set_title('Maximum Node Water Surface vs Return Period', fontsize=16)\n",
        "    axs[1].set_xlabel('Return Period (years)', fontsize=14)\n",
        "    axs[1].set_ylabel('Maximum Node Water Surface (ft)', fontsize=14)\n",
        "    axs[1].grid(True)\n",
        "    \n",
        "    # Add data labels\n",
        "    for i, txt in enumerate(aep_values):\n",
        "        axs[1].annotate(f\"{txt} yr\", \n",
        "                      (aep_values[i], aep_data['Maximum Node Water Surface (ft)'].iloc[i]),\n",
        "                      textcoords=\"offset points\", \n",
        "                      xytext=(0, 10), \n",
        "                      ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Plot time series for each return period\n",
        "    try:\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        \n",
        "        # Create a color map for different return periods\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(aep_values)))\n",
        "        \n",
        "        # Plot each return period\n",
        "        for i, name in enumerate(aep_data.index):\n",
        "            # Get HDF path for this return period\n",
        "            hdf_path = aep_data.loc[name, 'HDF Path']\n",
        "            \n",
        "            # Get pipe network timeseries data\n",
        "            node_ws = HdfPipe.get_pipe_network_timeseries(hdf_path, variable=\"Nodes/Water Surface\")\n",
        "            \n",
        "            # Get data for location 61\n",
        "            loc_61_ws = node_ws.sel(location=61)\n",
        "            \n",
        "            # Plot the time series\n",
        "            plt.plot(loc_61_ws.time.values, loc_61_ws.values, \n",
        "                    label=f'{name}', \n",
        "                    color=colors[i],\n",
        "                    linewidth=2)\n",
        "        \n",
        "        plt.title('Water Surface Elevation Time Series by Return Period - Location 61', fontsize=16)\n",
        "        plt.xlabel('Time', fontsize=14)\n",
        "        plt.ylabel('Water Surface Elevation (ft)', fontsize=14)\n",
        "        plt.grid(True)\n",
        "        plt.legend(fontsize=12)\n",
        "        \n",
        "        # Format x-axis dates\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Could not create detailed heatmap: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------------\n",
        "# Step 6: Plot the results\n",
        "#---------------------------------------------------------------------\n",
        "print(\"\\nStep 6: Plotting the results...\")\n",
        "\n",
        "plot_results(analysis_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "node_ws = HdfPipe.get_pipe_network_timeseries(\"04\", variable=\"Nodes/Water Surface\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "node_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds = HdfPipe.get_pipe_network_timeseries(\"04\", variable=\"Pipes/Pipe Flow DS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_flow_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot time series for each event"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot time series for each AEP event at the same downstream location\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import pandas as pd\n",
        "from ras_commander import HdfResultsPlan\n",
        "\n",
        "# Set up the figure with a larger size for better visibility\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Set up color map for different ARI events\n",
        "cmap = plt.cm.viridis\n",
        "colors = [cmap(i) for i in np.linspace(0, 0.9, len(new_plan_numbers))]\n",
        "\n",
        "# Dictionary to store all dataframes for later use\n",
        "all_ts_data = {}\n",
        "\n",
        "# Loop through each plan we created\n",
        "for i, plan_number in enumerate(new_plan_numbers):\n",
        "    try:\n",
        "        # Get results HDF path for this plan\n",
        "        results_path = RasPlan.get_results_path(plan_number, ras_object=ras)\n",
        "        \n",
        "        if not results_path.exists():\n",
        "            print(f\"No results found for Plan {plan_number}. Skipping.\")\n",
        "            continue\n",
        "            \n",
        "        # Extract the AEP value from the plan name\n",
        "        plan_info = ras.plan_df.loc[int(plan_number)]\n",
        "        plan_name = plan_info[\"Title\"]\n",
        "        aep_label = f\"{plan_info['ShortID']} ({plan_name})\"\n",
        "        \n",
        "        # Get reference point time series (using the same 'pipe_flow_ds' point)\n",
        "        ref_ts = HdfResultsPlan.reference_points_timeseries_output(results_path)\n",
        "        \n",
        "        # Check if pipe_flow_ds exists in the results\n",
        "        if 'pipe_flow_ds' not in ref_ts:\n",
        "            print(f\"Warning: 'pipe_flow_ds' not found in results for Plan {plan_number}\")\n",
        "            continue\n",
        "        \n",
        "        # Get the time series data for pipe_flow_ds\n",
        "        ts_data = ref_ts['pipe_flow_ds']\n",
        "        \n",
        "        # Store data for potential further use\n",
        "        all_ts_data[plan_number] = ts_data\n",
        "        \n",
        "        # Plot the stage (water surface elevation)\n",
        "        if 'Stage' in ts_data.columns:\n",
        "            plt.plot(ts_data.index, ts_data['Stage'], \n",
        "                     label=aep_label, \n",
        "                     color=colors[i], \n",
        "                     linewidth=2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing Plan {plan_number}: {e}\")\n",
        "\n",
        "# Add grid, legend and labels\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.title('Water Surface Elevation Time Series at Downstream Point', fontsize=14)\n",
        "plt.xlabel('Time', fontsize=12)\n",
        "plt.ylabel('Stage (m)', fontsize=12)\n",
        "\n",
        "# Format the x-axis to show dates nicely\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjust y-axis to show reasonable number of ticks\n",
        "plt.gca().yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
        "\n",
        "# Add a secondary plot with flow if it exists in the data\n",
        "if all(('Flow' in df.columns) for df in all_ts_data.values() if not df.empty):\n",
        "    ax2 = plt.gca().twinx()\n",
        "    \n",
        "    for i, (plan_number, ts_data) in enumerate(all_ts_data.items()):\n",
        "        if 'Flow' in ts_data.columns:\n",
        "            ax2.plot(ts_data.index, ts_data['Flow'], \n",
        "                     linestyle='--', \n",
        "                     color=colors[i], \n",
        "                     alpha=0.5)\n",
        "    \n",
        "    ax2.set_ylabel('Flow (m\u00b3/s)', color='gray', fontsize=12)\n",
        "    ax2.tick_params(axis='y', labelcolor='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a table of peak values for each plan\n",
        "print(\"\\nPeak Values Summary:\")\n",
        "peak_data = []\n",
        "\n",
        "for plan_number, ts_data in all_ts_data.items():\n",
        "    plan_info = ras.plan_df.loc[int(plan_number)]\n",
        "    aep_label = f\"{plan_info['ShortID']} ({plan_info['Title']})\"\n",
        "    \n",
        "    if 'Stage' in ts_data.columns:\n",
        "        peak_stage = ts_data['Stage'].max()\n",
        "        peak_time = ts_data['Stage'].idxmax()\n",
        "        \n",
        "        peak_flow = None\n",
        "        if 'Flow' in ts_data.columns:\n",
        "            peak_flow = ts_data['Flow'].max()\n",
        "        \n",
        "        peak_data.append({\n",
        "            'Plan': aep_label,\n",
        "            'Peak Stage (m)': round(peak_stage, 2),\n",
        "            'Time of Peak': peak_time,\n",
        "            'Peak Flow (m\u00b3/s)': round(peak_flow, 2) if peak_flow is not None else 'N/A'\n",
        "        })\n",
        "\n",
        "if peak_data:\n",
        "    peak_df = pd.DataFrame(peak_data)\n",
        "    display(peak_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a comprehensive workflow for automated AEP storm analysis using RAS-Commander. The key benefits of this approach include:\n",
        "\n",
        "1. **Efficiency**: Automating repetitive tasks saves time and reduces errors\n",
        "2. **Consistency**: Ensures consistent methodology across all return periods\n",
        "3. **Parallel Execution**: Makes optimal use of computational resources\n",
        "4. **Comprehensive Analysis**: Extracts and visualizes key metrics across return periods\n",
        "5. **Reproducibility**: The entire workflow is documented and repeatable\n",
        "\n",
        "This approach can be extended to include additional analyses, such as:\n",
        "\n",
        "- Comparing different storm patterns (e.g., position of peak intensity)\n",
        "- Analyzing climate change scenarios by adjusting precipitation depths\n",
        "- Evaluating infrastructure improvements by comparing baseline and modified geometries\n",
        "- Generating frequency curves for key hydraulic parameters\n",
        "\n",
        "By leveraging the power of RAS-Commander, engineers can focus on interpreting results and making design decisions rather than managing model configurations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\10_1d_hdf_data_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS 1D HDF Data Analysis Notebook\n",
        "\n",
        "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "!pip install --upgrade ras-commander\n",
        "# This installs ras-commander and all dependencies\n",
        "\n",
        "# Set to false to disable plot generation for llm-friendly outputs\n",
        "generate_plots = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "from shapely.geometry import LineString\n",
        "\n",
        "\n",
        "# Set pandas display options to show only 7 rows by default\n",
        "pd.set_option('display.max_rows', 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Example Project or Load Your Own Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the Balde Eagle Creek 1D Example project from HEC and run plan 01\n",
        "\n",
        "# Define the path to the 1D Balde Eagle Creek project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"Balde Eagle Creek\"\n",
        "import logging\n",
        "\n",
        "# Check if BaldEagle.p01.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = bald_eagle_path / \"BaldEagle.p01.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "    RasExamples.extract_project(\"Balde Eagle Creek\")\n",
        "\n",
        "    # Initialize the RAS project using the custom ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    logging.info(f\"Balde Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Balde Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"01\"\n",
        "\n",
        "    # Execute Plan 01 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
        "    if success_bald_eagle:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"BaldEagle.p01.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the custom ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    plan_number = \"01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Plan DataFrame for the project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nGeometry DataFrame for the project:\")\n",
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUnsteady DataFrame for the project:\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBoundary Conditions DataFrame for the project:\")\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get HDF Results Entries (only present when results are present)\n",
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the geometry HDF path\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
        "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS-Commander's Decorators Allow for Flexible Function Calling\n",
        "You can call most of the functions in the HDF* Classes using any of the following:\n",
        "1. Plan/Geometry Number (with or without leading zeros):\n",
        "   - \"01\", \"1\" - Plan/geometry number as string\n",
        "   - 1 - Plan/geometry number as integer\n",
        "   - \"p01\", \"p1\" - Plan number with 'p' prefix\n",
        "2. Direct File Paths:\n",
        "   - pathlib.Path object pointing to HDF file\n",
        "   - String path to HDF file\n",
        "\n",
        "3. h5py.File Objects:\n",
        "   - Already opened HDF file object\n",
        "\n",
        "The @standardize_input decorator handles all these input types consistently:\n",
        "   - Validates the input exists and is accessible\n",
        "   - Converts to proper pathlib.Path object\n",
        "   - Handles RAS object references\n",
        "   - Provides logging and error handling\n",
        "\n",
        "This flexibility makes it easier to work with HDF files in different contexts while maintaining consistent behavior \n",
        "across the codebase, and helps prevent strict typing from introducing unnecessary friction for LLM Coding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1D HDF Data Extraction Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract runtime and compute time data as dataframe\n",
        "print(\"\\nExtracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "# This returns a string with the projection as EPSG code (e.g. \"EPSG:6556\"), or None if not found.\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)  \n",
        "# This projection is returned as EPSG to improve compatibility with geopandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projection\n",
        "### The example project we are using does not have a projection  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfPlan to Get Geometry Information (Base Geometry Attributes) as dataframes\n",
        "print(\"\\nExtracting Base Geometry Attributes\")\n",
        "geom_attrs_df = HdfPlan.get_geometry_information(\"01\")  \n",
        "# NOTE: Here we call the function using the plan number instead of the hdf path to demonstrate that the decorator will work with the plan number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_attrs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get geometry structures attributes as dataframe\n",
        "print(\"\\nGetting geometry structures attributes\")\n",
        "geom_structures_attrs_df = HdfStruc.get_geom_structures_attrs(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_structures_attrs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instead of hdf_input, USE plan_hdf_path or geom_hdf_path, or the plan number as \"8\" or \"08\" \n",
        "# Input decorators allow for flexible inputs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get structures as geodataframe\n",
        "structures_gdf = HdfStruc.get_structures(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "structures_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reference lines as geodataframe\n",
        "ref_lines_gdf = HdfBndry.get_reference_lines(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_lines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reference points as geodataframe\n",
        "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_points_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross sections as geodataframe\n",
        "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_sections_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Showing only cross sections with ineffective flow areas\n",
        "\n",
        "# Filter rows where ineffective_blocks is not empty\n",
        "ineffective_xs_gdf = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(len) > 0]\n",
        "print(\"\\nCross Sections with Ineffective Flow Areas:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ineffective_xs_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print first 5 cross sections data\n",
        "print(\"\\nCross Section Information:\")\n",
        "\n",
        "for idx, row in cross_sections_gdf.head(5).iterrows():\n",
        "    print(f\"\\nCross Section {idx + 1}:\")\n",
        "    print(f\"River: {row['River']}\")\n",
        "    print(f\"Reach: {row['Reach']}\")\n",
        "    print(\"\\nGeometry:\")\n",
        "    print(row['geometry'])\n",
        "    print(\"\\nStation-Elevation Points:\")\n",
        "    \n",
        "    # Print header\n",
        "    print(\"     #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation\")\n",
        "    print(\"-\" * 150)\n",
        "    \n",
        "    # Calculate number of rows needed\n",
        "    points = row['station_elevation']\n",
        "    num_rows = (len(points) + 4) // 5  # Round up division\n",
        "    \n",
        "    # Print points in 5 columns\n",
        "    for i in range(num_rows):\n",
        "        line = \"\"\n",
        "        for j in range(5):\n",
        "            point_idx = i + j * num_rows\n",
        "            if point_idx < len(points):\n",
        "                station, elevation = points[point_idx]\n",
        "                line += f\"{point_idx+1:6d} {station:10.2f} {elevation:10.2f}    \"\n",
        "        print(line)\n",
        "    print(\"-\" * 150)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross sections on map with matplotlib\n",
        "\n",
        "if generate_plots:\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(15,10))\n",
        "    \n",
        "    # Plot cross sections\n",
        "    cross_sections_gdf.plot(ax=ax, color='red', linewidth=1, label='Cross Sections')\n",
        "    \n",
        "    # Add river name and reach labels\n",
        "    #for idx, row in cross_sections_gdf.iterrows():\n",
        "    #    # Get midpoint of cross section line for label placement\n",
        "    #    midpoint = row.geometry.centroid\n",
        "    #    label = f\"{row['River']}\\n{row['Reach']}\\nRS: {row['RS']}\"\n",
        "    #    ax.annotate(label, (midpoint.x, midpoint.y), \n",
        "    #               xytext=(5, 5), textcoords='offset points',\n",
        "    #               fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
        "    \n",
        "    # Customize plot\n",
        "    ax.set_title('Cross Sections Location Map')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    \n",
        "    # Equal aspect ratio to preserve shape\n",
        "    ax.set_aspect('equal')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross sections with Manning's n values colored by value\n",
        "\n",
        "if generate_plots:\n",
        "    # Create figure\n",
        "    fig, ax1 = plt.subplots(figsize=(20,10))\n",
        "\n",
        "    # Create colormap\n",
        "    cmap = plt.cm.viridis\n",
        "    norm = plt.Normalize(vmin=0.02, vmax=0.08)  # Typical Manning's n range\n",
        "\n",
        "    # Plot cross sections colored by Manning's n\n",
        "    for idx, row in cross_sections_gdf.iterrows():\n",
        "        # Extract Manning's n values and stations\n",
        "        mannings = row['mannings_n']\n",
        "        n_values = mannings['Mann n']\n",
        "        stations = mannings['Station']\n",
        "        \n",
        "        # Get the full linestring coordinates\n",
        "        line_coords = list(row.geometry.coords)\n",
        "        \n",
        "        # Calculate total length of the cross section\n",
        "        total_length = row.geometry.length\n",
        "        \n",
        "        # For each Manning's n segment\n",
        "        for i in range(len(n_values)-1):\n",
        "            # Calculate the start and end proportions along the line\n",
        "            start_prop = stations[i] / stations[-1]\n",
        "            end_prop = stations[i+1] / stations[-1]\n",
        "            \n",
        "            # Get the start and end points for this segment\n",
        "            start_idx = int(start_prop * (len(line_coords)-1))\n",
        "            end_idx = int(end_prop * (len(line_coords)-1))\n",
        "            \n",
        "            # Extract the segment coordinates\n",
        "            segment_coords = line_coords[start_idx:end_idx+1]\n",
        "            \n",
        "            if len(segment_coords) >= 2:\n",
        "                # Create a line segment\n",
        "                segment = LineString(segment_coords)\n",
        "                \n",
        "                # Get color from colormap for this n value\n",
        "                color = cmap(norm(n_values[i]))\n",
        "                \n",
        "                # Plot the segment\n",
        "                ax1.plot(*segment.xy, color=color, linewidth=2)\n",
        "\n",
        "    # Add colorbar\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    plt.colorbar(sm, ax=ax1, label=\"Manning's n Value\")\n",
        "\n",
        "    ax1.set_title(\"Cross Sections Colored by Manning's n Values\")\n",
        "    ax1.grid(True)\n",
        "    ax1.set_aspect('equal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross sections with ineffective flow areas\n",
        "\n",
        "if generate_plots:\n",
        "    # Create figure\n",
        "    fig, ax2 = plt.subplots(figsize=(20,10))\n",
        "\n",
        "    # Plot all cross sections first\n",
        "    cross_sections_gdf.plot(ax=ax2, color='lightgray', linewidth=1, label='Cross Sections')\n",
        "\n",
        "    # Plot ineffective flow areas with thicker lines\n",
        "    ineffective_sections = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(lambda x: len(x) > 0)]\n",
        "    ineffective_sections.plot(ax=ax2, color='red', linewidth=3, label='Ineffective Flow Areas')\n",
        "\n",
        "    # Add ineffective flow area labels with offset to lower right\n",
        "    for idx, row in cross_sections_gdf.iterrows():\n",
        "        # Get midpoint of cross section line\n",
        "        midpoint = row.geometry.centroid\n",
        "        \n",
        "        # Extract ineffective flow blocks\n",
        "        ineff_blocks = row['ineffective_blocks']\n",
        "        \n",
        "        if ineff_blocks:  # Only label if there are ineffective blocks\n",
        "            label_parts = []\n",
        "            # Add RS to first line of label\n",
        "            label_parts.append(f\"RS: {row['RS']}\")\n",
        "            for block in ineff_blocks:\n",
        "                label_parts.append(\n",
        "                    f\"L:{block['Left Sta']:.0f}-R:{block['Right Sta']:.0f}\\n\"\n",
        "                    f\"Elev: {block['Elevation']:.2f}\\n\"\n",
        "                    f\"Permanent: {block['Permanent']}\"\n",
        "                )\n",
        "            \n",
        "            label = '\\n'.join(label_parts)\n",
        "            \n",
        "            ax2.annotate(label, (midpoint.x, midpoint.y),\n",
        "                        xytext=(15, -15),  # Offset to lower right\n",
        "                        textcoords='offset points',\n",
        "                        fontsize=8, \n",
        "                        bbox=dict(facecolor='white', alpha=0.7),\n",
        "                        arrowprops=dict(arrowstyle='->'),\n",
        "                        horizontalalignment='left',\n",
        "                        verticalalignment='top')\n",
        "\n",
        "    ax2.set_title('Cross Sections with Ineffective Flow Areas')\n",
        "    ax2.grid(True)\n",
        "    ax2.legend()\n",
        "    ax2.set_aspect('equal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cross section elevation for cross section 42\n",
        "if generate_plots:\n",
        "    # Get cross sections data\n",
        "    cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path)\n",
        "\n",
        "    if not cross_sections_gdf.empty:\n",
        "        # Get station-elevation data for cross section 42\n",
        "        station_elevation = cross_sections_gdf.iloc[42]['station_elevation']\n",
        "        \n",
        "        # Convert list of lists to numpy arrays for plotting\n",
        "        stations = np.array([point[0] for point in station_elevation])\n",
        "        elevations = np.array([point[1] for point in station_elevation])\n",
        "        \n",
        "        # Create figure and axis\n",
        "        fig, ax = plt.subplots(figsize=(12,8))\n",
        "        \n",
        "        # Plot cross section\n",
        "        ax.plot(stations, elevations, 'b-', linewidth=2)\n",
        "        \n",
        "        # Add labels and title\n",
        "        river = cross_sections_gdf.iloc[42]['River']\n",
        "        reach = cross_sections_gdf.iloc[42]['Reach'] \n",
        "        rs = cross_sections_gdf.iloc[42]['RS']\n",
        "        \n",
        "        # Show bank stations as dots\n",
        "        left_bank_station = cross_sections_gdf.iloc[42]['Left Bank']\n",
        "        right_bank_station = cross_sections_gdf.iloc[42]['Right Bank']\n",
        "        \n",
        "        # Get elevations at bank stations\n",
        "        left_bank_elev = elevations[np.searchsorted(stations, left_bank_station)]\n",
        "        right_bank_elev = elevations[np.searchsorted(stations, right_bank_station)]\n",
        "        \n",
        "        # Plot bank stations with dots\n",
        "        ax.plot(left_bank_station, left_bank_elev, 'ro')\n",
        "        ax.plot(right_bank_station, right_bank_elev, 'ro')\n",
        "        \n",
        "        # Add bank station labels with station and elevation\n",
        "        ax.annotate(f'Left Bank\\nStation: {left_bank_station:.1f}\\nElevation: {left_bank_elev:.1f}',\n",
        "                   (left_bank_station, left_bank_elev),\n",
        "                   xytext=(-50, 30),\n",
        "                   textcoords='offset points',\n",
        "                   bbox=dict(facecolor='white', alpha=0.8),\n",
        "                   arrowprops=dict(arrowstyle='->'))\n",
        "                   \n",
        "        ax.annotate(f'Right Bank\\nStation: {right_bank_station:.1f}\\nElevation: {right_bank_elev:.1f}',\n",
        "                   (right_bank_station, right_bank_elev), \n",
        "                   xytext=(50, 30),\n",
        "                   textcoords='offset points',\n",
        "                   bbox=dict(facecolor='white', alpha=0.8),\n",
        "                   arrowprops=dict(arrowstyle='->'))\n",
        "        \n",
        "        ax.set_title(f'Cross Section Profile\\nRiver: {river}, Reach: {reach}, RS: {rs}')\n",
        "        ax.set_xlabel('Station (ft)')\n",
        "        ax.set_ylabel('Elevation (ft)')\n",
        "        \n",
        "        # Add grid\n",
        "        ax.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get river centerlines as geodataframe\n",
        "centerlines_gdf = HdfXsec.get_river_centerlines(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRiver Centerlines:\")\n",
        "centerlines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot river centerlines with labels\n",
        "if generate_plots:\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    # Plot centerlines\n",
        "    centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerline')\n",
        "\n",
        "    # Add river/reach labels\n",
        "    for idx, row in centerlines_gdf.iterrows():\n",
        "        # Get midpoint of the line for label placement\n",
        "        midpoint = row.geometry.interpolate(0.5, normalized=True)\n",
        "        \n",
        "        # Create label text combining river and reach names\n",
        "        label = f\"{row['River Name']}\\n{row['Reach Name']}\"\n",
        "        \n",
        "        # Add text annotation\n",
        "        ax.annotate(label, \n",
        "                    xy=(midpoint.x, midpoint.y),\n",
        "                    xytext=(10, 10), # Offset text slightly\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_title('River Centerlines', fontsize=14)\n",
        "    ax.set_xlabel('Easting', fontsize=12)\n",
        "    ax.set_ylabel('Northing', fontsize=12)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get river edge lines as geodataframe\n",
        "edge_lines_gdf = HdfXsec.get_river_edge_lines(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRiver Edge Lines:\")\n",
        "edge_lines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get bank lines as geodataframe\n",
        "bank_lines_gdf = HdfXsec.get_river_bank_lines(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nRiver Bank Lines:\")\n",
        "bank_lines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create figure and axis\n",
        "\n",
        "if generate_plots:\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    # Plot river edge lines\n",
        "    edge_lines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Edge Lines')\n",
        "\n",
        "    # Plot centerlines for reference\n",
        "    centerlines_gdf.plot(ax=ax, color='red', linewidth=2, linestyle='--', label='River Centerline')\n",
        "\n",
        "    # Plot river bank lines\n",
        "    bank_lines_gdf.plot(ax=ax, color='green', linewidth=2, label='River Bank Lines')\n",
        "\n",
        "    # Add title and labels\n",
        "    ax.set_title('River Edge Lines, Centerline, and Bank Lines', fontsize=14)\n",
        "    ax.set_xlabel('Easting', fontsize=12)\n",
        "    ax.set_ylabel('Northing', fontsize=12)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract 1D Structures Geodataframe\n",
        "\n",
        "\n",
        "\n",
        "# Display basic information about the structures\n",
        "print(\"\\nStructures Summary:\")\n",
        "print(f\"Number of structures found: {len(structures_gdf)}\")\n",
        "structures_gdf\n",
        "\n",
        "# Display first few rows of key attributes\n",
        "print(\"\\nStructure Details:\")\n",
        "display_cols = ['Structure ID', 'Structure Type', 'River Name', 'Reach Name', 'Station']\n",
        "display_cols = [col for col in display_cols if col in structures_gdf.columns]\n",
        "if display_cols:\n",
        "    print(structures_gdf[display_cols].head())\n",
        "\n",
        "\n",
        "if generate_plots:\n",
        "\n",
        "    # Create visualization\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "    # Plot river centerlines\n",
        "    if not centerlines_gdf.empty:\n",
        "        centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerlines')\n",
        "\n",
        "    # Plot cross sections\n",
        "    if not cross_sections_gdf.empty:\n",
        "        cross_sections_gdf.plot(ax=ax, color='green', linewidth=1, label='Cross Sections')\n",
        "\n",
        "    # Plot structures\n",
        "    if not structures_gdf.empty:\n",
        "        structures_gdf.plot(ax=ax, color='red', marker='s', markersize=100, label='Structures')\n",
        "\n",
        "    # Add title and labels\n",
        "    ax.set_title('HEC-RAS Model Components', fontsize=14)\n",
        "    ax.set_xlabel('Easting', fontsize=12)\n",
        "    ax.set_ylabel('Northing', fontsize=12)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend(fontsize=12)\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "# Print summary of cross sections\n",
        "print(\"\\nCross Sections Summary:\")\n",
        "print(f\"Number of cross sections found: {len(cross_sections_gdf)}\")\n",
        "if not cross_sections_gdf.empty:\n",
        "    print(\"\\nCross Section Details:\")\n",
        "    xs_display_cols = ['River', 'Reach', 'Station']\n",
        "    xs_display_cols = [col for col in xs_display_cols if col in cross_sections_gdf.columns]\n",
        "    if xs_display_cols:\n",
        "        print(cross_sections_gdf[xs_display_cols].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Plan Parameters\n",
        "print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n",
        "\n",
        "plan_parameters_df = HdfPlan.get_plan_parameters(hdf_path=plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nPlan Parameters DataFrame:\")\n",
        "plan_parameters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract volume accounting data\n",
        "volume_accounting_df = HdfResultsPlan.get_volume_accounting(hdf_path=plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nVolume Accounting DataFrame:\")\n",
        "volume_accounting_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get simulation start time\n",
        "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
        "print(f\"Simulation start time: {start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get plan end time\n",
        "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
        "print(f\"Simulation end time: {end_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the time of maximum water surface elevation (WSEL) for cross sections\n",
        "\n",
        "# Get cross section results timeseries\n",
        "xsec_results_xr = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n",
        "print(\"\\nCross Section Results Shape:\", xsec_results_xr['Water_Surface'].shape)\n",
        "\n",
        "# Get cross section geometry data\n",
        "xsec_geom = HdfXsec.get_cross_sections(plan_hdf_path)\n",
        "print(\"\\nNumber of cross sections in geometry:\", len(xsec_geom))\n",
        "\n",
        "# Create dataframe with cross section locations and max WSEL times\n",
        "xs_data = []\n",
        "\n",
        "# Extract water surface data from xarray Dataset\n",
        "water_surface = xsec_results_xr['Water_Surface'].values\n",
        "times = pd.to_datetime(xsec_results_xr.time.values)\n",
        "\n",
        "# Debug print\n",
        "print(\"\\nFirst few cross section names:\")\n",
        "print(xsec_results_xr.cross_section.values[:5])\n",
        "\n",
        "# Iterate through cross sections\n",
        "for xs_idx in range(len(xsec_results_xr.cross_section)):\n",
        "    # Get WSEL timeseries for this cross section\n",
        "    wsel_series = water_surface[:, xs_idx]\n",
        "    \n",
        "    # Get cross section name and parse components\n",
        "    xs_name = xsec_results_xr.cross_section.values[xs_idx]\n",
        "    \n",
        "    # Split the string and remove empty strings\n",
        "    xs_parts = [part for part in xs_name.split() if part]\n",
        "    \n",
        "    if len(xs_parts) >= 3:\n",
        "        river = \"Bald Eagle\"  # Combine first two words\n",
        "        reach = \"Loc Hav\"     # Next two words\n",
        "        rs = xs_parts[-1]     # Last part is the station\n",
        "        \n",
        "        # Get geometry for this cross section\n",
        "        xs_match = xsec_geom[\n",
        "            (xsec_geom['River'] == river) & \n",
        "            (xsec_geom['Reach'] == reach) & \n",
        "            (xsec_geom['RS'] == rs)\n",
        "        ]\n",
        "        \n",
        "        if not xs_match.empty:\n",
        "            geom = xs_match.iloc[0]\n",
        "            # Use first point of cross section line for plotting\n",
        "            x = geom.geometry.coords[0][0]\n",
        "            y = geom.geometry.coords[0][1]\n",
        "            \n",
        "            # Find time of max WSEL\n",
        "            max_wsel_idx = np.argmax(wsel_series)\n",
        "            max_wsel = np.max(wsel_series)\n",
        "            max_time = times[max_wsel_idx]\n",
        "            \n",
        "            xs_data.append({\n",
        "                'xs_name': xs_name,\n",
        "                'x': x,\n",
        "                'y': y,\n",
        "                'max_wsel': max_wsel,\n",
        "                'time_of_max': max_time\n",
        "            })\n",
        "        else:\n",
        "            print(f\"\\nWarning: No geometry match found for {xs_name}\")\n",
        "            print(f\"River: {river}, Reach: {reach}, RS: {rs}\")\n",
        "    else:\n",
        "        print(f\"\\nWarning: Could not parse cross section name: {xs_name}\")\n",
        "\n",
        "# Create dataframe\n",
        "xs_df = pd.DataFrame(xs_data)\n",
        "\n",
        "# Debug print\n",
        "print(\"\\nNumber of cross sections processed:\", len(xs_df))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if generate_plots:\n",
        "    print(\"\\nColumns in xs_df:\", xs_df.columns.tolist())\n",
        "    print(\"\\nFirst row of xs_df:\")\n",
        "    print(xs_df.iloc[0])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Convert datetime to hours since start for colormap\n",
        "    min_time = min(xs_df['time_of_max'])\n",
        "    color_values = [(t - min_time).total_seconds() / 3600 for t in xs_df['time_of_max']]\n",
        "\n",
        "    # Plot cross section points\n",
        "    scatter = ax.scatter(xs_df['x'], xs_df['y'],\n",
        "                        c=color_values,\n",
        "                        cmap='viridis',\n",
        "                        s=50)\n",
        "\n",
        "    # Customize plot\n",
        "    ax.set_title('Time of Maximum Water Surface Elevation at Cross Sections')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter)\n",
        "    cbar.set_label('Hours since simulation start')\n",
        "\n",
        "    # Format colorbar ticks\n",
        "    max_hours = int(max(color_values))\n",
        "    tick_interval = max(1, max_hours // 6)  # Show ~6 ticks\n",
        "    cbar.set_ticks(range(0, max_hours + 1, tick_interval))\n",
        "    cbar.set_ticklabels([f'{h}h' for h in range(0, max_hours + 1, tick_interval)])\n",
        "\n",
        "    # Add grid and adjust styling\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    max_wsel_xs = xs_df.loc[xs_df['max_wsel'].idxmax()]\n",
        "    hours_since_start = (max_wsel_xs['time_of_max'] - min_time).total_seconds() / 3600\n",
        "\n",
        "    print(f\"\\nOverall Maximum WSEL: {max_wsel_xs['max_wsel']:.2f} ft\")\n",
        "    print(f\"Time of Overall Maximum WSEL: {max_wsel_xs['time_of_max']}\")\n",
        "    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
        "    print(f\"Location of Overall Maximum WSEL: X={max_wsel_xs['x']:.2f}, Y={max_wsel_xs['y']:.2f}\")\n",
        "    print(f\"Cross Section: {max_wsel_xs['xs_name']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unsteady attributes as dataframe\n",
        "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unsteady summary attributes as dataframe\n",
        "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_summary_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1D Cross Section Results as Xarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cross section results timeseries as xarray dataset\n",
        "xsec_results_xr = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xsec_results_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print time series for specific cross section\n",
        "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
        "\n",
        "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
        "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
        "    print(f\"\\n{var}:\")\n",
        "    print(xsec_results_xr[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
        "\n",
        "# Create time series plots\n",
        "\n",
        "if generate_plots:\n",
        "\n",
        "    # Create a figure for each variable\n",
        "    variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
        "\n",
        "    for var in variables:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        # Convert time values to datetime if needed\n",
        "        time_values = pd.to_datetime(xsec_results_xr.time.values)\n",
        "        values = xsec_results_xr[var].sel(cross_section=target_xs).values\n",
        "        \n",
        "        # Plot with explicit x and y values\n",
        "        plt.plot(time_values, values, '-', linewidth=2)\n",
        "        \n",
        "        plt.title(f'{var} at {target_xs}')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(var.replace('_', ' '))\n",
        "        plt.grid(True)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Force display\n",
        "        plt.draw()\n",
        "        plt.pause(0.1)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced HDF Data Extraction\n",
        "This section focuses on directly accessing the HDF file from a jupyter notebook for use cases not directly supported by the RAS-Commander libary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Compute Messages as String\n",
        "print(\"Extracting Compute Messages\")\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract string from HDF object at a given path\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    results_hdf_filename : str\n",
        "        Name of the HDF file\n",
        "    hdf_path : str\n",
        "        Path of the object in the HDF file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Extracted string from the specified HDF object\n",
        "    \"\"\"\n",
        "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
        "        try:\n",
        "            hdf_object = hdf_file[hdf_path]\n",
        "            if isinstance(hdf_object, h5py.Group):\n",
        "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
        "            elif isinstance(hdf_object, h5py.Dataset):\n",
        "                data = hdf_object[()]\n",
        "                if isinstance(data, bytes):\n",
        "                    return data.decode('utf-8')\n",
        "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
        "                    return [v.decode('utf-8') for v in data]\n",
        "                else:\n",
        "                    return str(data)\n",
        "            else:\n",
        "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
        "        except KeyError:\n",
        "            return f\"Path not found: {hdf_path}\"\n",
        "\n",
        "try:\n",
        "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
        "    print(\"Compute Messages:\")\n",
        "    \n",
        "    # Parse and print the compute messages in a more visually friendly way\n",
        "    messages = results_summary_string[0].split('\\r\\n')\n",
        "    \n",
        "    for message in messages:\n",
        "        if message.strip():  # Skip empty lines\n",
        "            if ':' in message:\n",
        "                key, value = message.split(':', 1)\n",
        "                print(f\"{key.strip():40} : {value.strip()}\")\n",
        "            else:\n",
        "                print(f\"\\n{message.strip()}\")\n",
        "    \n",
        "    # Print computation summary in a table format\n",
        "    print(\"\\nComputation Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
        "    print(\"-\" * 50)\n",
        "    for line in messages:\n",
        "        if 'Computation Task' in line:\n",
        "            task, time = line.split('\\t')\n",
        "            print(f\"{task:<30} {time:<20}\")\n",
        "    \n",
        "    print(\"\\nComputation Speed:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
        "    print(\"-\" * 50)\n",
        "    for line in messages:\n",
        "        if 'Computation Speed' in line:\n",
        "            task, speed = line.split('\\t')\n",
        "            print(f\"{task:<30} {speed:<20}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting compute messages: {str(e)}\")\n",
        "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring HDF Datasets with HdfBase.get_dataset_info\n",
        "This allows users to find HDF information that is not included in the ras-commander library.  Find the path in HDFView and set the group_path below to explore the HDF datasets and attributes.  Then, use the output to write your own function to extract the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get HDF Paths with Properties (For Exploring HDF Files)\n",
        "HdfBase.get_dataset_info(plan_number, group_path=\"/Geometry\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get dataset structure:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/River Bank Lines/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Structures\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Output/Output Blocks/Computation Block/Global/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use the get_hdf5_dataset_info function from HdfUtils to explore the Cross Sections structure in the geometry HDF file\n",
        "\n",
        "print(\"\\nExploring Cross Sections structure in geometry file:\")\n",
        "print(\"HDF Base Path: /Geometry/Cross Sections \")\n",
        "HdfBase.get_dataset_info(geom_hdf_path, group_path='/Geometry/Cross Sections')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"\\n=== HDF5 File Structure ===\\n\")\n",
        "print(plan_hdf_path)\n",
        "HdfBase.get_dataset_info(plan_hdf_path, group_path='/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For HDF datasets that are not supported by the RAS-Commadner library, provide the dataset path to HdfBase.get_dataset_info and provide the output to an LLM along with a relevent HDF* class(es) to generate new functions that extend the library's coverage.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\11_2d_hdf_data_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS 2D HDF Data Analysis Notebook\n",
        "\n",
        "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies\n",
        "\n",
        "# Use this setting to disable plot generation within the notebook\n",
        "generate_plots = True\n",
        "# Use this setting to disable map generation within the notebook\n",
        "generate_maps = True\n",
        "# Set both to false for llm-friendly outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "import logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will try to import the pip package, if it fails it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "# This allows a user's revisions to be tested locally without installing the package\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Flexible imports to allow for development without installation \n",
        "#  ** Use this version with Jupyter Notebooks **\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    import os\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "print(\"ras_commander imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Example Project or Load Your Own Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To Use the HEC Example Project:\n",
        "# Download the BaldEagleCrkMulti2D project from HEC and Run Plan 06\n",
        "\n",
        "# Define the path to the BaldEagleCrkMulti2D project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "the_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "import logging\n",
        "\n",
        "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = the_path / \"BaldEagleDamBrk.p06.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(the_path, \"6.6\")\n",
        "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"06\"\n",
        "\n",
        "    # Update run flags for the project\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,\n",
        "        unsteady_flow_simulation=True,\n",
        "        run_sediment=False,\n",
        "        post_processor=True,\n",
        "        floodplain_mapping=False\n",
        "    )\n",
        "\n",
        "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_the = RasCmdr.compute_plan(plan_number)\n",
        "    if success_the:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"Project already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(the_path, \"6.6\")\n",
        "    plan_number = \"06\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show ras object info\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate: Get the geometry HDF path if you are extracting geometry elements from the geometry HDF\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAS-Commander's Decorators Allow for Flexible Function Calling\n",
        "You can call most of the functions in the HDF* Classes using any of the following:\n",
        "1. Plan/Geometry Number (with or without leading zeros):\n",
        "   - \"01\", \"1\" - Plan/geometry number as string\n",
        "   - 1 - Plan/geometry number as integer\n",
        "   - \"p01\", \"p1\" - Plan number with 'p' prefix\n",
        "2. Direct File Paths:\n",
        "   - pathlib.Path object pointing to HDF file\n",
        "   - String path to HDF file\n",
        "\n",
        "3. h5py.File Objects:\n",
        "   - Already opened HDF file object\n",
        "\n",
        "The @standardize_input decorator handles all these input types consistently:\n",
        "   - Validates the input exists and is accessible\n",
        "   - Converts to proper pathlib.Path object\n",
        "   - Handles RAS object references\n",
        "   - Provides logging and error handling\n",
        "\n",
        "This flexibility makes it easier to work with HDF files in different contexts while maintaining consistent behavior \n",
        "across the codebase, and helps prevent strict typing from introducing unnecessary friction for LLM Coding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2D HDF Data Extraction Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract runtime and compute time data as dataframe\n",
        "print(\"\\nExtracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "runtime_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
        "print(geom_hdf_path)\n",
        "\n",
        "# For the example project, plan 06 is associated with geometry 09\n",
        "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
        "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfPlan for geometry-related operations\n",
        "print(\"\\nExtracting Geometry Information\")\n",
        "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfMesh for geometry-related operations\n",
        "print(\"\\nListing 2D Flow Area Names\")\n",
        "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"2D Flow Area Name (returned as list):\")\n",
        "flow_area_names\n",
        "# Note: this is returned as a list because it is used internally by other functions.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 2D Flow Area Attributes (get_mesh_area_attributes)\n",
        "print(\"\\nExtracting 2D Flow Area Attributes\")\n",
        "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flow_area_attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get 2D Flow Area Perimeter Polygons (get_mesh_areas)\n",
        "print(\"\\nExtracting 2D Flow Area Perimeter Polygons\")\n",
        "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Map of Mesh Areas\n",
        "if generate_plots:\n",
        "    # Plot the 2D Flow Area Perimeter Polygons\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none')\n",
        "\n",
        "    # Add labels for each polygon\n",
        "    for idx, row in mesh_areas.iterrows():\n",
        "        centroid = row.geometry.centroid\n",
        "        # Check if 'Name' column exists, otherwise use a default label\n",
        "        label = row.get('Name', f'Area {idx}')\n",
        "        ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
        "\n",
        "    plt.title('2D Flow Area Perimeter Polygons')\n",
        "    plt.xlabel('Easting')\n",
        "    plt.ylabel('Northing')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh cell faces as geodatframe\n",
        "mesh_cell_faces_gdf = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_cell_faces_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.collections import LineCollection\n",
        "import numpy as np\n",
        "\n",
        "# Calculate and display statistics\n",
        "print(\"\\nMesh Cell Faces Statistics:\")\n",
        "print(f\"Total number of cell faces: {len(mesh_cell_faces_gdf)}\")\n",
        "print(f\"Number of unique meshes: {mesh_cell_faces_gdf['mesh_name'].nunique()}\")\n",
        "\n",
        "if generate_maps:\n",
        "    # Plot the mesh cell faces more efficiently\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Convert all geometries to numpy arrays at once for faster plotting\n",
        "    lines = [list(zip(*line.xy)) for line in mesh_cell_faces_gdf.geometry]\n",
        "    lines_collection = LineCollection(lines, colors='blue', linewidth=0.5, alpha=0.5)\n",
        "    ax.add_collection(lines_collection)\n",
        "\n",
        "    # Set plot title and labels\n",
        "    plt.title('Mesh Cell Faces')\n",
        "    plt.xlabel('Easting')\n",
        "    plt.ylabel('Northing')\n",
        "\n",
        "    # Calculate centroids once and store as numpy arrays\n",
        "    centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in mesh_cell_faces_gdf.geometry])\n",
        "\n",
        "    # Create scatter plot with numpy arrays\n",
        "    scatter = ax.scatter(\n",
        "        centroids[:, 0],\n",
        "        centroids[:, 1], \n",
        "        c=mesh_cell_faces_gdf['face_id'],\n",
        "        cmap='viridis',\n",
        "        s=1,\n",
        "        alpha=0.5\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Face ID')\n",
        "\n",
        "    # Set axis limits based on data bounds\n",
        "    ax.set_xlim(centroids[:, 0].min(), centroids[:, 0].max())\n",
        "    ax.set_ylim(centroids[:, 1].min(), centroids[:, 1].max())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to find the nearest cell face to a given point\n",
        "def find_nearest_cell_face(point, cell_faces_df):\n",
        "    \"\"\"\n",
        "    Find the nearest cell face to a given point.\n",
        "\n",
        "    Args:\n",
        "        point (shapely.geometry.Point): The input point.\n",
        "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
        "\n",
        "    Returns:\n",
        "        int: The face_id of the nearest cell face.\n",
        "        float: The distance to the nearest cell face.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the input point to all cell faces\n",
        "    distances = cell_faces_df.geometry.distance(point)\n",
        "\n",
        "    # Find the index of the minimum distance\n",
        "    nearest_index = distances.idxmin()\n",
        "\n",
        "    # Get the face_id and distance of the nearest cell face\n",
        "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
        "    nearest_distance = distances[nearest_index]\n",
        "\n",
        "    return nearest_face_id, nearest_distance\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nFinding the nearest cell face to a given point\")\n",
        "\n",
        "# Create a sample point (you can replace this with any point of interest)\n",
        "from shapely.geometry import Point\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "# Get the centroid of the mesh cell faces\n",
        "print(\"Getting Centroid of 2D Mesh Polygon\")\n",
        "centroid = mesh_cell_faces_gdf.geometry.union_all().centroid\n",
        "\n",
        "# Create GeoDataFrame with the centroid point, using same CRS as mesh_cell_faces_gdf\n",
        "sample_point = GeoDataFrame(\n",
        "    {'geometry': [centroid]}, \n",
        "    crs=mesh_cell_faces_gdf.crs\n",
        ")\n",
        "\n",
        "if not mesh_cell_faces_gdf.empty and not sample_point.empty:\n",
        "    print(\"Searching Cell\")\n",
        "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces_gdf)\n",
        "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
        "    print(f\"Face ID: {nearest_face_id}\")\n",
        "    print(f\"Distance: {distance:.2f} units\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate map of cell faces with sample point and nearest cell face shown\n",
        "if generate_maps:\n",
        "    # Visualize the result\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Plot all cell faces\n",
        "    mesh_cell_faces_gdf.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
        "    \n",
        "    # Plot the sample point\n",
        "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
        "    \n",
        "    # Plot the nearest cell face\n",
        "    nearest_face = mesh_cell_faces_gdf[mesh_cell_faces_gdf['face_id'] == nearest_face_id]\n",
        "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
        "    \n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('Nearest Cell Face to Sample Point')\n",
        "    \n",
        "    # Add legend and grid\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    \n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Cell Polygons\n",
        "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
        "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cell_polygons_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cell polygons\n",
        "\n",
        "if generate_maps:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot cell polygons\n",
        "    cell_polygons_df.plot(ax=ax, edgecolor='blue', facecolor='none')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('2D Flow Area Cell Polygons')\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Cell Info\n",
        "print(\"\\nExample 5: Extracting Cell Info\")\n",
        "cell_info_df = HdfMesh.get_mesh_cell_points(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cell_info_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cell centers\n",
        "\n",
        "if generate_maps:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot cell centers\n",
        "    cell_info_df.plot(ax=ax, color='red', markersize=5)\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('2D Flow Area Cell Centers')\n",
        "\n",
        "    # Add grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to find the nearest cell center to a given point\n",
        "def find_nearest_cell(point, cell_centers_df):\n",
        "    \"\"\"\n",
        "    Find the nearest cell center to a given point.\n",
        "\n",
        "    Args:\n",
        "        point (shapely.geometry.Point): The input point.\n",
        "        cell_centers_df (GeoDataFrame): DataFrame containing cell center points.\n",
        "\n",
        "    Returns:\n",
        "        int: The cell_id of the nearest cell.\n",
        "        float: The distance to the nearest cell center.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the input point to all cell centers\n",
        "    distances = cell_centers_df.geometry.distance(point)\n",
        "\n",
        "    # Find the index of the minimum distance\n",
        "    nearest_index = distances.idxmin()\n",
        "\n",
        "    # Get the cell_id and distance of the nearest cell\n",
        "    nearest_cell_id = cell_centers_df.loc[nearest_index, 'cell_id']\n",
        "    nearest_distance = distances[nearest_index]\n",
        "\n",
        "    return nearest_cell_id, nearest_distance\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nFinding the nearest cell to a given point\")\n",
        "\n",
        "# Sample point was created in a previous code cell \n",
        "\n",
        "# Get the projection from the geometry file\n",
        "# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n",
        "if projection:\n",
        "    print(f\"Using projection: {projection}\")\n",
        "else:\n",
        "    print(\"No projection information found. Using default CRS.\")\n",
        "    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n",
        "\n",
        "\n",
        "\n",
        "# Ensure the CRS of the sample point matches the cell_info_df\n",
        "if sample_point.crs != cell_info_df.crs:\n",
        "    sample_point = sample_point.to_crs(cell_info_df.crs)\n",
        "\n",
        "nearest_cell_id, distance = find_nearest_cell(sample_point.geometry.iloc[0], cell_info_df)\n",
        "print(f\"Nearest cell to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
        "print(f\"Cell ID: {nearest_cell_id}\")\n",
        "print(f\"Distance: {distance:.2f} units\")\n",
        "\n",
        "if generate_maps:\n",
        "    # Visualize the result\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot all cell centers\n",
        "    cell_info_df.plot(ax=ax, color='blue', markersize=5, alpha=0.5, label='Cell Centers')\n",
        "\n",
        "    # Plot the sample point\n",
        "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
        "\n",
        "    # Plot the nearest cell center\n",
        "    nearest_cell = cell_info_df[cell_info_df['cell_id'] == nearest_cell_id]\n",
        "    nearest_cell.plot(ax=ax, color='green', markersize=100, alpha=0.7, label='Nearest Cell')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('Nearest Cell to Sample Point')\n",
        "\n",
        "    # Add legend and grid\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get geometry structures attributes\n",
        "print(\"\\nGetting geometry structures attributes as Dataframe\")\n",
        "geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_structures_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Paths and Functions for each type of structure: \n",
        "\n",
        "# Getting geometry structures attributes\n",
        "# Geometry structures attributes:\n",
        "# Bridge/Culvert Count: 0\n",
        "# Connection Count: 4\n",
        "# Has Bridge Opening (2D): 0\n",
        "# Inline Structure Count: 0\n",
        "# Lateral Structure Count: 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get boundary condition lines\n",
        "print(\"\\nExtracting Boundary Condition Lines as Geodataframe\")\n",
        "bc_lines_df = HdfBndry.get_bc_lines(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bc_lines_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Boundary Condition Lines with Perimeter\n",
        "\n",
        "if generate_maps:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    if not mesh_areas.empty:\n",
        "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
        "        \n",
        "        # Add labels for each polygon\n",
        "        for idx, row in mesh_areas.iterrows():\n",
        "            centroid = row.geometry.centroid\n",
        "            label = row.get('Name', f'Area {idx}')\n",
        "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
        "\n",
        "    # Plot boundary condition lines\n",
        "    if not bc_lines_df.empty:\n",
        "        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Easting')\n",
        "    ax.set_ylabel('Northing')\n",
        "    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n",
        "\n",
        "    # Add grid and legend\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "# Plot 2D Flow Area Perimeter Polygons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Breaklines as Geodataframe\n",
        "print(\"\\nExtracting Breaklines\")\n",
        "breaklines_gdf = HdfBndry.get_breaklines(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "breaklines_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot breaklines and 2D Flow Area Perimeter Polygons\n",
        "\n",
        "if generate_plots:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Plot 2D Flow Area Perimeter Polygons\n",
        "    if not mesh_areas.empty:\n",
        "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
        "        \n",
        "        # Add labels for each polygon\n",
        "        for idx, row in mesh_areas.iterrows():\n",
        "            centroid = row.geometry.centroid\n",
        "            label = row.get('Name', f'Area {idx}')\n",
        "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
        "\n",
        "    # Plot breaklines\n",
        "    if not breaklines_gdf.empty:\n",
        "        breaklines_gdf.plot(ax=ax, color='blue', linewidth=2, label='Breaklines')\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Easting')\n",
        "    ax.set_ylabel('Northing')\n",
        "    ax.set_title('2D Flow Area Perimeter Polygons and Breaklines')\n",
        "\n",
        "    # Add grid and legend\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get structures as GeoDatframe\n",
        "structures_gdf = HdfStruc.get_structures(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "structures_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get boundary condition lines as GeoDatframe\n",
        "bc_lines_gdf = HdfBndry.get_bc_lines(geom_hdf_path)\n",
        "print(\"\\nBoundary Condition Lines:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bc_lines_gdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dev Note: Need to add function for Reference Lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reference points as Geodataframe\n",
        "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nReference Points:\")\n",
        "ref_points_gdf\n",
        "# There are no reference points in this example project (for demonstration only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Refinement Regions\n",
        "refinement_regions_df = HdfBndry.get_refinement_regions(geom_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "refinement_regions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Refinement Regions\n",
        "\n",
        "if not refinement_regions_df.empty:\n",
        "    print(\"Refinement Regions DataFrame:\")\n",
        "    display(refinement_regions_df.head())\n",
        "    \n",
        "    # Plot refinement regions\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    refinement_regions_df.plot(ax=ax, column='CellSize', legend=True, \n",
        "                               legend_kwds={'label': 'Cell Size', 'orientation': 'horizontal'},\n",
        "                               cmap='viridis')\n",
        "    ax.set_title('2D Mesh Area Refinement Regions')\n",
        "    ax.set_xlabel('Easting')\n",
        "    ax.set_ylabel('Northing')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No refinement regions found in the geometry file.\")\n",
        "\n",
        "# Analyze Refinement Regions\n",
        "if not refinement_regions_df.empty:\n",
        "    print(\"\\nRefinement Regions Analysis:\")\n",
        "    print(f\"Total number of refinement regions: {len(refinement_regions_df)}\")\n",
        "    print(\"\\nCell Size Statistics:\")\n",
        "    print(refinement_regions_df['CellSize'].describe())\n",
        "    \n",
        "    # Group by Shape Type\n",
        "    shape_type_counts = refinement_regions_df['ShapeType'].value_counts()\n",
        "    print(\"\\nRefinement Region Shape Types:\")\n",
        "    print(shape_type_counts)\n",
        "    \n",
        "    # Plot Shape Type distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    shape_type_counts.plot(kind='bar')\n",
        "    plt.title('Distribution of Refinement Region Shape Types')\n",
        "    plt.xlabel('Shape Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Plan Parameters \n",
        "plan_parameters_df = HdfPlan.get_plan_parameters(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_parameters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract volume accounting data\n",
        "volume_accounting_df = HdfResultsPlan.get_volume_accounting(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_accounting_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RasPlanHdf Class Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get plan start time as datetime object\n",
        "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
        "print(f\"Simulation start time: {start_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation start time: 2018-09-09 00:00:00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get plan end time as datetime object\n",
        "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
        "print(f\"Simulation end time: {end_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simulation end time: 2018-09-14 00:00:00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get maximum iteration count for mesh cells\n",
        "max_iter_gdf = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_iter_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cell coordinates \n",
        "cell_coords = HdfMesh.get_mesh_cell_points(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Mesh Max Iterations\n",
        "\n",
        "if generate_maps:\n",
        "    # Extract x and y coordinates from the geometry column\n",
        "    max_iter_gdf['x'] = max_iter_gdf['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
        "    max_iter_gdf['y'] = max_iter_gdf['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
        "\n",
        "    # Remove rows with None coordinates\n",
        "    max_iter_gdf = max_iter_gdf.dropna(subset=['x', 'y'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_iter_gdf['x'], max_iter_gdf['y'], \n",
        "                         c=max_iter_gdf['cell_last_iteration'], \n",
        "                         cmap='viridis', \n",
        "                         s=1)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Iterations per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Iterations')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")\n",
        "\n",
        "# Print the first few rows of the dataframe for verification\n",
        "print(\"\\nFirst few rows of the dataframe:\")\n",
        "max_iter_gdf[['mesh_name', 'cell_id', 'geometry']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List top 10 points for Max Iteration per Cell\n",
        "# Sort the dataframe by cell_last_iteration in descending order\n",
        "top_iterations = max_iter_gdf.sort_values(by='cell_last_iteration', ascending=False).head(10)\n",
        "\n",
        "# Create a more informative display with coordinates\n",
        "print(\"\\nTop 10 Cells with Highest Iteration Counts:\")\n",
        "top_iterations_display = top_iterations.copy()\n",
        "top_iterations_display['x_coord'] = top_iterations_display['geometry'].apply(lambda geom: round(geom.x, 2))\n",
        "top_iterations_display['y_coord'] = top_iterations_display['geometry'].apply(lambda geom: round(geom.y, 2))\n",
        "\n",
        "# Display the results in a formatted table\n",
        "print(top_iterations_display[['mesh_name', 'cell_id', 'cell_last_iteration', 'x_coord', 'y_coord']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh maximum water surface elevation as Geodataframe\n",
        "max_ws_gdf = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Dataframe Attributes (the HDF Attributes are also imported as Geoataframe Attributes)\n",
        "max_ws_gdf.attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_ws_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the max water surface as a map\n",
        "if generate_maps:\n",
        "    # Extract x and y coordinates from the geometry column\n",
        "    max_ws_gdf['x'] = max_ws_gdf['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
        "    max_ws_gdf['y'] = max_ws_gdf['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
        "\n",
        "    # Remove rows with None coordinates\n",
        "    max_ws_gdf = max_ws_gdf.dropna(subset=['x', 'y'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_ws_gdf['x'], max_ws_gdf['y'], \n",
        "                         c=max_ws_gdf['maximum_water_surface'], \n",
        "                         cmap='viridis', \n",
        "                         s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Water Surface per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Water Surface (ft)')\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"generate_maps is set to False\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the time of the max water surface elevation (WSEL)\n",
        "if generate_maps:\n",
        "    import matplotlib.dates as mdates\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Convert the 'maximum_water_surface_time' to datetime objects\n",
        "    max_ws_gdf['max_wsel_time'] = pd.to_datetime(max_ws_gdf['maximum_water_surface_time'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Convert datetime to hours since the start for colormap\n",
        "    min_time = max_ws_gdf['max_wsel_time'].min()\n",
        "    color_values = (max_ws_gdf['max_wsel_time'] - min_time).dt.total_seconds() / 3600  # Convert to hours\n",
        "\n",
        "    scatter = ax.scatter(max_ws_gdf['x'], max_ws_gdf['y'], \n",
        "                        c=color_values, \n",
        "                        cmap='viridis', \n",
        "                        s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Time of Maximum Water Surface Elevation per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Set up the colorbar\n",
        "    cbar = plt.colorbar(scatter)\n",
        "    cbar.set_label('Hours since simulation start')\n",
        "\n",
        "    # Format the colorbar ticks to show hours\n",
        "    cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))  # Set ticks every 6 hours\n",
        "    cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Find the overall maximum WSEL and its time\n",
        "    max_wsel_row = max_ws_gdf.loc[max_ws_gdf['maximum_water_surface'].idxmax()]\n",
        "    hours_since_start = (max_wsel_row['max_wsel_time'] - min_time).total_seconds() / 3600\n",
        "    print(f\"\\nOverall Maximum WSEL: {max_wsel_row['maximum_water_surface']:.2f} ft\")\n",
        "    print(f\"Time of Overall Maximum WSEL: {max_wsel_row['max_wsel_time']}\")\n",
        "    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
        "    print(f\"Location of Overall Maximum WSEL: X={max_wsel_row['x']}, Y={max_wsel_row['y']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh minimum water surface elevation as geodataframe\n",
        "min_ws_gdf = HdfResultsMesh.get_mesh_min_ws(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_ws_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh maximum face velocity as geodataframe\n",
        "max_face_v_gdf = HdfResultsMesh.get_mesh_max_face_v(plan_hdf_path)\n",
        "print(\"\\nMesh Max Face Velocity:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_face_v_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract midpoint coordinates from the LineString geometries\n",
        "max_face_v_gdf['x'] = max_face_v_gdf['geometry'].apply(lambda geom: geom.centroid.x)\n",
        "max_face_v_gdf['y'] = max_face_v_gdf['geometry'].apply(lambda geom: geom.centroid.y)\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(max_face_v_gdf['x'], max_face_v_gdf['y'], \n",
        "                    c=max_face_v_gdf['maximum_face_velocity'].abs(),\n",
        "                    cmap='viridis',\n",
        "                    s=10)\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_title('Max Face Velocity per Face')\n",
        "ax.set_xlabel('X Coordinate') \n",
        "ax.set_ylabel('Y Coordinate')\n",
        "plt.colorbar(scatter, label='Max Face Velocity (ft/s)')\n",
        "\n",
        "# Add grid lines\n",
        "ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Increase font size for better readability\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "# Adjust layout to prevent cutting off labels\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh minimum face velocity as geodataframe\n",
        "min_face_v_gdf = HdfResultsMesh.get_mesh_min_face_v(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Min Face Velocity:\")\n",
        "min_face_v_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh max water surface error as geodataframe\n",
        "\n",
        "max_ws_err_gdf = HdfResultsMesh.get_mesh_max_ws_err(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Max Water Surface Error:\")\n",
        "max_ws_err_gdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot max water surface error\n",
        "\n",
        "if generate_maps:\n",
        "# Extract x and y coordinates from the geometry points, handling None values\n",
        "    max_ws_err_gdf['x'] = max_ws_err_gdf['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
        "    max_ws_err_gdf['y'] = max_ws_err_gdf['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
        "\n",
        "    # Remove any rows with None coordinates\n",
        "    max_ws_err_gdf = max_ws_err_gdf.dropna(subset=['x', 'y'])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_ws_err_gdf['x'], max_ws_err_gdf['y'],\n",
        "                        c=max_ws_err_gdf['cell_maximum_water_surface_error'],\n",
        "                        cmap='viridis',\n",
        "                        s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Water Surface Error per Cell')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Water Surface Error (ft)')\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort Dataframe to show top 10 maximum water surface errors:\n",
        "max_ws_err_gdf_sorted = max_ws_err_gdf.sort_values(by='cell_maximum_water_surface_error', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTop 10 maximum water surface errors:\")\n",
        "max_ws_err_gdf_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant) as geodataframe\n",
        "max_courant_gdf = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Maximum Courant):\")\n",
        "max_courant_gdf.attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_courant_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot max Courant number\n",
        "\n",
        "# Convert to GeoDataFrame if not empty\n",
        "if not max_courant_gdf.empty:\n",
        "    if generate_maps:\n",
        "        # Get centroids of line geometries for plotting\n",
        "        max_courant_gdf['centroid'] = max_courant_gdf.geometry.centroid\n",
        "        max_courant_gdf['x'] = max_courant_gdf.centroid.x\n",
        "        max_courant_gdf['y'] = max_courant_gdf.centroid.y\n",
        "\n",
        "        # Create the plot\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        scatter = ax.scatter(max_courant_gdf['x'], max_courant_gdf['y'],\n",
        "                        c=max_courant_gdf['maximum_face_courant'],\n",
        "                        cmap='viridis',\n",
        "                        s=10)\n",
        "\n",
        "        # Customize the plot\n",
        "        ax.set_title('Max Courant Number per Face')\n",
        "        ax.set_xlabel('X Coordinate')\n",
        "        ax.set_ylabel('Y Coordinate')\n",
        "        plt.colorbar(scatter, label='Max Courant Number')\n",
        "\n",
        "        # Add grid lines\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Increase font size for better readability\n",
        "        plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "        # Adjust layout to prevent cutting off labels\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "# Print the first few rows of the dataframe for verification\n",
        "print(\"\\nFirst few rows of the Courant number dataframe:\")\n",
        "max_courant_gdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n",
        "\n",
        "max_face_shear_gdf = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Maximum Face Shear Stress:\")\n",
        "print(max_face_shear_gdf.attrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_face_shear_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot max face shear stress\n",
        "\n",
        "if generate_maps and not max_face_shear_gdf.empty:\n",
        "    # Calculate centroids of the line geometries and extract coordinates\n",
        "    max_face_shear_gdf['centroid'] = max_face_shear_gdf['geometry'].apply(lambda line: line.centroid)\n",
        "    max_face_shear_gdf['x'] = max_face_shear_gdf['centroid'].apply(lambda point: point.x)\n",
        "    max_face_shear_gdf['y'] = max_face_shear_gdf['centroid'].apply(lambda point: point.y)\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(max_face_shear_gdf['x'], max_face_shear_gdf['y'],\n",
        "                        c=max_face_shear_gdf['maximum_face_shear_stress'],\n",
        "                        cmap='viridis',\n",
        "                        s=10)\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Max Face Shear Stress per Face')\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, label='Max Face Shear Stress (PSF)')\n",
        "\n",
        "    # Add grid lines\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Increase font size for better readability\n",
        "    plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "    # Adjust layout to prevent cutting off labels\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for Minimum Water Surface as geodataframe\n",
        "summary_gdf_min_ws = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Water Surface\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Minimum Water Surface):\")\n",
        "summary_gdf_min_ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for Minimum Face Velocity as geodataframe\n",
        "summary_gdf_min_fv = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Face Velocity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Minimum Face Velocity):\")\n",
        "summary_gdf_min_fv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh summary output for Cell Cumulative Iteration as geodataframe\n",
        "summary_gdf_cum_iter = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Cell Cumulative Iteration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Summary Output (Cell Cumulative Iteration):\")\n",
        "summary_gdf_cum_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh timeseries output as xarray\n",
        "# The mesh name is part of the timeseries HDF path, so you must pass the mesh_name to retrieve it\n",
        "\n",
        "# Get mesh areas from previous code cell\n",
        "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mesh_areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the first mesh area name to extract mesh timeseries output as xarray\n",
        "timeseries_xr = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_areas[0], \"Water Surface\") # Use the first 2D flow area name for mesh_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timeseries_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Series Output Variables for Cells\n",
        "# \n",
        "# Variable Name: Description\n",
        "# Water Surface: Water surface elevation\n",
        "# Depth: Water depth\n",
        "# Velocity: Magnitude of velocity\n",
        "# Velocity X: X-component of velocity\n",
        "# Velocity Y: Y-component of velocity\n",
        "# Froude Number: Froude number\n",
        "# Courant Number: Courant number\n",
        "# Shear Stress: Shear stress on the bed\n",
        "# Bed Elevation: Elevation of the bed\n",
        "# Precipitation Rate: Rate of precipitation\n",
        "# Infiltration Rate: Rate of infiltration\n",
        "# Evaporation Rate: Rate of evaporation\n",
        "# Percolation Rate: Rate of percolation\n",
        "# Groundwater Elevation: Elevation of groundwater\n",
        "# Groundwater Depth: Depth to groundwater\n",
        "# Groundwater Flow: Groundwater flow rate\n",
        "# Groundwater Velocity: Magnitude of groundwater velocity\n",
        "# Groundwater Velocity X: X-component of groundwater velocity\n",
        "# Groundwater Velocity Y: Y-component of groundwater velocity\n",
        "# \n",
        "# These variables are available for time series output at the cell level in 2D flow areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh cells timeseries output as xarray\n",
        "cells_timeseries_xr = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_areas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cells_timeseries_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot WSE Time Series Data (Random Cell ID) \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if generate_plots:\n",
        "    import numpy as np\n",
        "    import random\n",
        "\n",
        "    # Extract Water Surface data\n",
        "    water_surface = cells_timeseries_xr[mesh_areas[0]]['Water Surface']\n",
        "\n",
        "    # Get the time values\n",
        "    time_values = water_surface.coords['time'].values\n",
        "\n",
        "    # Pick a random cell_id\n",
        "    random_cell_id = random.choice(water_surface.coords['cell_id'].values)\n",
        "\n",
        "    # Extract the water surface elevation time series for the random cell\n",
        "    wsel_timeseries = water_surface.sel(cell_id=random_cell_id)\n",
        "\n",
        "    # Find the peak value and its index\n",
        "    peak_value = wsel_timeseries.max().item()\n",
        "    peak_index = wsel_timeseries.argmax().item()\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(time_values, wsel_timeseries, label=f'Cell ID: {random_cell_id}')\n",
        "    plt.scatter(time_values[peak_index], peak_value, color='red', s=100, zorder=5)\n",
        "    plt.annotate(f'Peak: {peak_value:.2f} ft', \n",
        "                (time_values[peak_index], peak_value),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                ha='left', va='bottom',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
        "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "    plt.title(f'Water Surface Elevation Time Series for Random Cell (ID: {random_cell_id})')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Water Surface Elevation (ft)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Log the plotting action\n",
        "    logging.info(f\"Plotted water surface elevation time series for random cell ID: {random_cell_id}\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print some statistics\n",
        "    print(f\"Statistics for Cell ID {random_cell_id}:\")\n",
        "    print(f\"Minimum WSEL: {wsel_timeseries.min().item():.2f} ft\")\n",
        "    print(f\"Maximum WSEL: {peak_value:.2f} ft\")\n",
        "    print(f\"Mean WSEL: {wsel_timeseries.mean().item():.2f} ft\")\n",
        "    print(f\"Time of peak: {time_values[peak_index]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh faces timeseries output as xarray\n",
        "faces_timeseries_xr = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_areas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faces_timeseries_xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Random Face Results and Label Peak, Plus Map View\n",
        "\n",
        "if generate_maps:\n",
        "\n",
        "    # Select a random valid face ID number\n",
        "    random_face = np.random.randint(0, faces_timeseries_xr.sizes['face_id'])\n",
        "\n",
        "    # Extract time series data for the selected face\n",
        "    variable = 'face_velocity'  # We could also use 'face_flow'\n",
        "    face_data = faces_timeseries_xr[variable].sel(face_id=random_face)\n",
        "\n",
        "    # Find peak value and its corresponding time\n",
        "    peak_value = face_data.max().item()\n",
        "    peak_time = face_data.idxmax().values\n",
        "\n",
        "    # Plot time series\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(faces_timeseries_xr.time, face_data)\n",
        "    plt.title(f'{variable.capitalize()} Time Series for Face {random_face}')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(f'{variable.capitalize()} ({faces_timeseries_xr.attrs[\"units\"]})')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Annotate the peak point\n",
        "    plt.annotate(f'Peak: ({peak_time}, {peak_value:.2f})', \n",
        "                (peak_time, peak_value),\n",
        "                xytext=(10, 10), textcoords='offset points',\n",
        "                arrowprops=dict(arrowstyle=\"->\"))\n",
        "\n",
        "    # Check for negative values and label the minimum if present\n",
        "    min_value = face_data.min().item()\n",
        "    if min_value < 0:\n",
        "        min_time = face_data.idxmin().values\n",
        "        plt.annotate(f'Min: ({min_time}, {min_value:.2f})', \n",
        "                    (min_time, min_value),\n",
        "                    xytext=(10, -10), textcoords='offset points',\n",
        "                    arrowprops=dict(arrowstyle=\"->\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Create map view plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "\n",
        "    # Calculate mesh faces extents with 10% buffer\n",
        "    faces_bounds = mesh_cell_faces_gdf.total_bounds\n",
        "    x_min, y_min, x_max, y_max = faces_bounds\n",
        "    buffer_x = (x_max - x_min) * 0.1\n",
        "    buffer_y = (y_max - y_min) * 0.1\n",
        "    plot_xlim = [x_min - buffer_x, x_max + buffer_x]\n",
        "    plot_ylim = [y_min - buffer_y, y_max + buffer_y]\n",
        "\n",
        "    # Set plot limits before adding terrain\n",
        "    ax.set_xlim(plot_xlim)\n",
        "    ax.set_ylim(plot_ylim)\n",
        "\n",
        "    # Add the terrain TIFF to the map, clipped to our desired extent\n",
        "    tiff_path = Path.cwd() / 'example_projects' / 'BaldEagleCrkMulti2D' / 'Terrain' / 'Terrain50.baldeagledem.tif'\n",
        "    with rasterio.open(tiff_path) as src:\n",
        "        show(src, ax=ax, cmap='terrain', alpha=0.5)\n",
        "        \n",
        "    # Reset the limits after terrain plot\n",
        "    ax.set_xlim(plot_xlim)\n",
        "    ax.set_ylim(plot_ylim)\n",
        "\n",
        "    # Plot all faces in gray\n",
        "    mesh_cell_faces_gdf.plot(ax=ax, color='lightgray', alpha=0.5, zorder=2)\n",
        "\n",
        "    # Get the selected face geometry\n",
        "    selected_face = mesh_cell_faces_gdf[mesh_cell_faces_gdf['face_id'] == random_face]\n",
        "\n",
        "    # Highlight the selected face in red\n",
        "    selected_face.plot(\n",
        "        ax=ax, \n",
        "        color='red',\n",
        "        linewidth=2,\n",
        "        label=f'Selected Face (ID: {random_face})',\n",
        "        zorder=3\n",
        "    )\n",
        "\n",
        "    # Get bounds of selected face for zoomed inset\n",
        "    bounds = selected_face.geometry.bounds.iloc[0]\n",
        "    x_center = (bounds.iloc[0] + bounds.iloc[2]) / 2\n",
        "    y_center = (bounds.iloc[1] + bounds.iloc[3]) / 2\n",
        "    buffer = max(bounds.iloc[2] - bounds.iloc[0], bounds.iloc[3] - bounds.iloc[1]) * 2\n",
        "\n",
        "    # Create zoomed inset with a larger size, inside the map frame\n",
        "    axins = inset_axes(ax, width=\"70%\", height=\"70%\", loc='lower right',\n",
        "                    bbox_to_anchor=(0.65, 0.05, 0.35, 0.35),\n",
        "                    bbox_transform=ax.transAxes)\n",
        "\n",
        "    # Plot terrain and faces in inset\n",
        "    with rasterio.open(tiff_path) as src:\n",
        "        show(src, ax=axins, cmap='terrain', alpha=0.5)\n",
        "        \n",
        "    # Plot zoomed view in inset\n",
        "    mesh_cell_faces_gdf.plot(ax=axins, color='lightgray', alpha=0.5, zorder=2)\n",
        "    selected_face.plot(ax=axins, color='red', linewidth=2, zorder=3)\n",
        "\n",
        "    # Set inset limits with slightly more context\n",
        "    axins.set_xlim(x_center - buffer/1.5, x_center + buffer/1.5)\n",
        "    axins.set_ylim(y_center - buffer/1.5, y_center + buffer/1.5)\n",
        "\n",
        "    # Remove inset ticks for cleaner look\n",
        "    axins.set_xticks([])\n",
        "    axins.set_yticks([])\n",
        "\n",
        "    # Add a border to the inset\n",
        "    for spine in axins.spines.values():\n",
        "        spine.set_edgecolor('black')\n",
        "        spine.set_linewidth(1.5)\n",
        "\n",
        "    # Create connection lines between main plot and inset\n",
        "    # Get the selected face centroid for connection point\n",
        "    centroid = selected_face.geometry.centroid.iloc[0]\n",
        "    con1 = ConnectionPatch(\n",
        "        xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
        "        xyB=(0.02, 0.98), coordsB=axins.transAxes,\n",
        "        arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
        "    )\n",
        "    con2 = ConnectionPatch(\n",
        "        xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
        "        xyB=(0.98, 0.02), coordsB=axins.transAxes,\n",
        "        arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
        "    )\n",
        "\n",
        "    ax.add_artist(con1)\n",
        "    ax.add_artist(con2)\n",
        "\n",
        "    # Add title and legend to main plot\n",
        "    ax.set_title('Mesh Face Map View with Terrain')\n",
        "    ax.legend()\n",
        "\n",
        "    # Ensure equal aspect ratio while maintaining our desired extents\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary information\n",
        "    print(f\"Random Face: {random_face}\")\n",
        "    print(f\"Peak Value: {peak_value:.2f} {faces_timeseries_xr.attrs['units']} at {peak_time}\")\n",
        "    if min_value < 0:\n",
        "        print(f\"Minimum Value: {min_value:.2f} {faces_timeseries_xr.attrs['units']} at {min_time}\")\n",
        "\n",
        "    # Log the plotting action\n",
        "    logging.info(f\"Plotted mesh face time series and map view for random face ID: {random_face} with terrain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get meteorology precipitation attributes\n",
        "meteo_precip_attrs = HdfPlan.get_plan_met_precip(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "meteo_precip_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results unsteady attributes\n",
        "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results unsteady summary attributes\n",
        "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_unsteady_summary_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results volume accounting attributes\n",
        "volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_accounting_attrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract Compute Messages as String\n",
        "print(\"Extracting Compute Messages\")\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract string from HDF object at a given path\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    results_hdf_filename : str\n",
        "        Name of the HDF file\n",
        "    hdf_path : str\n",
        "        Path of the object in the HDF file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Extracted string from the specified HDF object\n",
        "    \"\"\"\n",
        "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
        "        try:\n",
        "            hdf_object = hdf_file[hdf_path]\n",
        "            if isinstance(hdf_object, h5py.Group):\n",
        "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
        "            elif isinstance(hdf_object, h5py.Dataset):\n",
        "                data = hdf_object[()]\n",
        "                if isinstance(data, bytes):\n",
        "                    return data.decode('utf-8')\n",
        "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
        "                    return [v.decode('utf-8') for v in data]\n",
        "                else:\n",
        "                    return str(data)\n",
        "            else:\n",
        "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
        "        except KeyError:\n",
        "            return f\"Path not found: {hdf_path}\"\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
        "    print(\"Compute Messages:\")\n",
        "    \n",
        "    # Parse and print the compute messages in a more visually friendly way\n",
        "    messages = results_summary_string[0].split('\\r\\n')\n",
        "    \n",
        "    for message in messages:\n",
        "        if message.strip():  # Skip empty lines\n",
        "            if ':' in message:\n",
        "                key, value = message.split(':', 1)\n",
        "                print(f\"{key.strip():40} : {value.strip()}\")\n",
        "            else:\n",
        "                print(f\"\\n{message.strip()}\")\n",
        "    \n",
        "    # Print computation summary in a table format\n",
        "    print(\"\\nComputation Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
        "    print(\"-\" * 50)\n",
        "    for line in messages:\n",
        "        if 'Computation Task' in line:\n",
        "            task, time = line.split('\\t')\n",
        "            print(f\"{task:<30} {time:<20}\")\n",
        "    \n",
        "    print(\"\\nComputation Speed:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
        "    print(\"-\" * 50)\n",
        "    for line in messages:\n",
        "        if 'Computation Speed' in line:\n",
        "            task, speed = line.split('\\t')\n",
        "            print(f\"{task:<30} {speed:<20}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting compute messages: {str(e)}\")\n",
        "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring HDF Datasets with HdfBase.get_dataset_info\n",
        "This allows users to find HDF information that is not included in the ras-commander library.  Find the path in HDFView and set the group_path below to explore the HDF datasets and attributes.  Then, use the output to write your own function to extract the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get HDF Paths with Properties (For Exploring HDF Files)\n",
        "HdfBase.get_dataset_info(plan_number, group_path=\"/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For HDF datasets that are not supported by the RAS-Commadner library, provide the dataset path to HdfBase.get_dataset_info and provide the output to an LLM along with a relevent HDF* class(es) to generate new functions that extend the library's coverage.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\12_2d_hdf_data_extraction pipes and pumps.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS Pipes, Conduits, and Pump Stations HDF Data Analysis Notebook\n",
        "\n",
        "This notebook demonstrates how to manipulate and analyze the new HEC-RAS Conduits, Pipes, and Pump Stations results using the ras-commander library. It leverages the HdfPipe and HdfPump classes to streamline data extraction, processing, and visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will try to import the pip package, if it fails it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "# This allows a user's revisions to be tested locally without installing the package\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Flexible imports to allow for development without installation \n",
        "#  ** Use this version with Jupyter Notebooks **\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    import os\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "print(\"ras_commander imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use Example Project or Load Your Own Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the Pipes Beta project from HEC and run plan 01\n",
        "\n",
        "# Define the path to the Pipes Beta project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
        "import logging\n",
        "\n",
        "# Check if Pipes Beta.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = pipes_ex_path / \"DavisStormSystem.p02.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the Pipes Beta project\n",
        "    RasExamples.extract_project([\"Davis\"])\n",
        "\n",
        "    # Initialize the RAS project using the ras. (Pipe Networks are only supported in versions 6.6 and above)\n",
        "    init_ras_project(pipes_ex_path, \"6.6\")\n",
        "    logging.info(f\"Pipes Beta project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Pipes Beta object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"02\"\n",
        "\n",
        "    # Update run flags for the project\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,\n",
        "        unsteady_flow_simulation=True,\n",
        "        run_sediment=False,\n",
        "        post_processor=True,\n",
        "        floodplain_mapping=False\n",
        "    )\n",
        "\n",
        "    # Execute Plan 06 using RasCmdr for Pipes Beta\n",
        "    print(f\"Executing Plan {plan_number} for the Pipes Beta Creek project...\")\n",
        "    success_pipes_ex = RasCmdr.compute_plan(plan_number)\n",
        "    if success_pipes_ex:\n",
        "        print(f\"Plan {plan_number} executed successfully for Pipes Beta.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Pipes Beta.\\n\")\n",
        "else:\n",
        "    print(\"Pipes Beta.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the ras.\n",
        "    init_ras_project(pipes_ex_path, \"6.6\")\n",
        "    plan_number = \"02\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Plan DataFrame for the project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUnsteady DataFrame for the project:\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBoundary Conditions DataFrame for the project:\")\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get HDF Results Entries (only present when results are present)\n",
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate: Get the geometry HDF path if you are extracting geometry elements from the geometry HDF\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract runtime and compute time data\n",
        "print(\"\\nExtracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)\n",
        "runtime_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2D Models with Pipe Networks: HDF Data Extraction Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pipe conduits\n",
        "pipe_conduits_gdf = HdfPipe.get_pipe_conduits(\"02\") # NOTE: Here we use the plan number instead of the path variable.  The library decorators ensure this maps correctly.  \n",
        "print(\"\\nPipe Conduits: pipe_conduits_gdf\")\n",
        "pipe_conduits_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the pipe conduit linestrings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a new figure with a specified size\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "# Plot each linestring from the GeoDataFrame\n",
        "for idx, row in pipe_conduits_gdf.iterrows():\n",
        "    # Extract coordinates from the linestring\n",
        "    x_coords, y_coords = row['Polyline'].xy\n",
        "    \n",
        "    # Plot the linestring\n",
        "    plt.plot(x_coords, y_coords, 'b-', linewidth=1, alpha=0.7)\n",
        "    \n",
        "    # Add vertical line markers at endpoints\n",
        "    plt.plot([x_coords[0]], [y_coords[0]], 'x', color='black', markersize=4)\n",
        "    plt.plot([x_coords[-1]], [y_coords[-1]], 'x', color='black', markersize=4)\n",
        "    \n",
        "    # Calculate center point of the line\n",
        "    center_x = (x_coords[0] + x_coords[-1]) / 2\n",
        "    center_y = (y_coords[0] + y_coords[-1]) / 2\n",
        "    \n",
        "    # Add pipe name label at center, oriented top-right\n",
        "    plt.text(center_x, center_y, f'{row[\"Name\"]}', fontsize=8, \n",
        "             verticalalignment='bottom', horizontalalignment='left',\n",
        "             rotation=45)  # 45 degree angle for top-right orientation\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Pipe Conduit Network Layout')\n",
        "plt.xlabel('Easting')\n",
        "plt.ylabel('Northing')\n",
        "\n",
        "# Add grid\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Adjust layout to prevent label clipping\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the first 2 terrain profiles\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract terrain profiles from the GeoDataFrame\n",
        "terrain_profiles = pipe_conduits_gdf['Terrain_Profiles'].tolist()\n",
        "\n",
        "# Create separate plots for the first 2 terrain profiles\n",
        "for i in range(2):\n",
        "    profile = terrain_profiles[i]\n",
        "    \n",
        "    # Unzip the profile into x and y coordinates\n",
        "    x_coords, y_coords = zip(*profile)\n",
        "    \n",
        "    # Create a new figure for each profile\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='g', alpha=0.7)\n",
        "    \n",
        "    # Add title and labels\n",
        "    plt.title(f'Terrain Profile {i + 1}')\n",
        "    plt.xlabel('Distance along profile (m)')\n",
        "    plt.ylabel('Elevation (m)')\n",
        "    \n",
        "    # Add grid\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    \n",
        "    # Adjust layout to prevent label clipping\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Nodes/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pipe nodes\n",
        "pipe_nodes_gdf = HdfPipe.get_pipe_nodes(plan_hdf_path)\n",
        "print(\"\\nPipe Nodes:\")\n",
        "pipe_nodes_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Networks/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pipe network data\n",
        "pipe_network_gdf = HdfPipe.get_pipe_network(plan_hdf_path)\n",
        "print(\"\\nPipe Network Data:\")\n",
        "pipe_network_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pump stations\n",
        "pump_stations_gdf = HdfPump.get_pump_stations(plan_hdf_path)\n",
        "print(\"\\nPump Stations:\")\n",
        "pump_stations_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get pump groups\n",
        "pump_groups_df = HdfPump.get_pump_groups(plan_hdf_path)\n",
        "print(\"\\nPump Groups:\")\n",
        "pump_groups_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
        "print(f\"Projection: {projection}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set CRS for GeoDataFrames\n",
        "if projection:\n",
        "    pipe_conduits_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
        "    pipe_nodes_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
        "\n",
        "print(\"Pipe Conduits GeoDataFrame columns:\")\n",
        "print(pipe_conduits_gdf.columns)\n",
        "\n",
        "print(\"\\nPipe Nodes GeoDataFrame columns:\")\n",
        "print(pipe_nodes_gdf.columns)\n",
        "\n",
        "perimeter_polygons = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
        "if projection:\n",
        "    perimeter_polygons.set_crs(projection, inplace=True, allow_override=True)\n",
        "    \n",
        "print(\"\\nPerimeter Polygons GeoDataFrame columns:\")\n",
        "print(perimeter_polygons.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from shapely import wkt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(28, 20))\n",
        "\n",
        "# Plot cell polygons with 50% transparency behind the pipe network\n",
        "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)\n",
        "if not cell_polygons_df.empty:\n",
        "    cell_polygons_df.plot(ax=ax, edgecolor='lightgray', facecolor='lightgray', alpha=0.5)\n",
        "\n",
        "# Plot pipe conduits - the Polyline column already contains LineString geometries\n",
        "pipe_conduits_gdf.set_geometry('Polyline', inplace=True)\n",
        "\n",
        "# Plot each pipe conduit individually to ensure all are shown\n",
        "for idx, row in pipe_conduits_gdf.iterrows():\n",
        "    ax.plot(*row.Polyline.xy, color='blue', linewidth=1)\n",
        "\n",
        "# Create a colormap for node elevations\n",
        "norm = plt.Normalize(pipe_nodes_gdf['Invert Elevation'].min(), \n",
        "                    pipe_nodes_gdf['Invert Elevation'].max())\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "# Plot pipe nodes colored by invert elevation\n",
        "scatter = ax.scatter(pipe_nodes_gdf.geometry.x, pipe_nodes_gdf.geometry.y,\n",
        "                    c=pipe_nodes_gdf['Invert Elevation'], \n",
        "                    cmap=cmap, norm=norm,\n",
        "                    s=100)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Invert Elevation (ft)', rotation=270, labelpad=15)\n",
        "\n",
        "# Add combined labels for invert and drop inlet elevations\n",
        "for idx, row in pipe_nodes_gdf.iterrows():\n",
        "    label_text = \"\"  # Initialize label_text for each node\n",
        "    # Add drop inlet elevation label if it exists and is not NaN\n",
        "    if 'Drop Inlet Elevation' in row and not np.isnan(row['Drop Inlet Elevation']):\n",
        "        label_text += f\"TOC: {row['Drop Inlet Elevation']:.2f}\\n\"\n",
        "    label_text += f\"INV: {row['Invert Elevation']:.2f}\"\n",
        "    \n",
        "    ax.annotate(label_text,\n",
        "                xy=(row.geometry.x, row.geometry.y),\n",
        "                xytext=(-10, -10), textcoords='offset points',\n",
        "                fontsize=8,\n",
        "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
        "\n",
        "# Add perimeter polygons \n",
        "if not perimeter_polygons.empty:\n",
        "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none')\n",
        "\n",
        "# Create proxy artists for legend\n",
        "conduit_line = mlines.Line2D([], [], color='blue', label='Conduits')\n",
        "node_point = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
        "                          markersize=10, label='Nodes')\n",
        "perimeter = mpatches.Patch(facecolor='none', edgecolor='black',\n",
        "                          label='Perimeter Polygons')\n",
        "\n",
        "ax.set_title('Pipe Network with Node Elevations')\n",
        "\n",
        "# Add legend with proxy artists\n",
        "ax.legend(handles=[conduit_line, node_point, perimeter])\n",
        "\n",
        "# Set aspect ratio to be equal and adjust limits\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "ax.autoscale_view()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize pump stations on a map\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "pump_stations_gdf.plot(ax=ax, color='green', markersize=50, label='Pump Station')\n",
        "\n",
        "# Add perimeter polygons\n",
        "if not perimeter_polygons.empty:\n",
        "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none', label='Perimeter Polygons')\n",
        "\n",
        "ax.set_title('Pump Station Location')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Get pipe network timeseries\n",
        "valid_variables = [\n",
        "    \"Cell Courant\", \"Cell Water Surface\", \"Face Flow\", \"Face Velocity\",\n",
        "    \"Face Water Surface\", \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\",\n",
        "    \"Pipes/Vel DS\", \"Pipes/Vel US\", \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\",\n",
        "    \"Nodes/Water Surface\"\n",
        "]\n",
        "\n",
        "print(\"Valid variables for pipe network timeseries:\")\n",
        "for var in valid_variables:\n",
        "    print(f\"- {var}\")\n",
        "\n",
        "# Extract pipe network timeseries for each valid pipe-related variable\n",
        "pipe_variables = [var for var in valid_variables if var.startswith(\"Pipes/\") or var.startswith(\"Nodes/\")]\n",
        "\n",
        "for variable in pipe_variables:\n",
        "    try:\n",
        "        pipe_timeseries = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
        "        print(f\"\\nPipe Network Timeseries ({variable}):\")\n",
        "        print(pipe_timeseries.head())  # Print first few rows to avoid overwhelming output\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {variable}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipe Network Timeseries Data Description\n",
        "\n",
        "The `get_pipe_network_timeseries` function returns an xarray DataArray for each variable. Here's a general description of the data structure:\n",
        "\n",
        "1. **Pipes/Pipe Flow DS and Pipes/Pipe Flow US**:\n",
        "   - Dimensions: time, location (pipe IDs)\n",
        "   - Units: ft^3/s (cubic feet per second)\n",
        "   - Description: Represents the flow rate at the downstream (DS) and upstream (US) ends of pipes over time.\n",
        "\n",
        "2. **Pipes/Vel DS and Pipes/Vel US**:\n",
        "   - Dimensions: time, location (pipe IDs)\n",
        "   - Units: ft/s (feet per second)\n",
        "   - Description: Shows the velocity at the downstream (DS) and upstream (US) ends of pipes over time.\n",
        "\n",
        "3. **Nodes/Depth**:\n",
        "   - Dimensions: time, location (node IDs)\n",
        "   - Units: ft (feet)\n",
        "   - Description: Indicates the depth of water at each node over time.\n",
        "\n",
        "4. **Nodes/Drop Inlet Flow**:\n",
        "   - Dimensions: time, location (node IDs)\n",
        "   - Units: cfs (cubic feet per second)\n",
        "   - Description: Represents the flow rate through drop inlets at each node over time.\n",
        "\n",
        "5. **Nodes/Water Surface**:\n",
        "   - Dimensions: time, location (node IDs)\n",
        "   - Units: ft (feet)\n",
        "   - Description: Shows the water surface elevation at each node over time.\n",
        "\n",
        "General notes:\n",
        "- The 'time' dimension represents the simulation timesteps.\n",
        "- The 'location' dimension represents either pipe IDs or node IDs, depending on the variable.\n",
        "- The number of timesteps and locations may vary depending on the specific dataset and simulation setup.\n",
        "- Negative values in flow variables may indicate reverse flow direction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the variables we want to plot\n",
        "variables = [\n",
        "    \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\", \"Pipes/Vel DS\", \"Pipes/Vel US\",\n",
        "    \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\", \"Nodes/Water Surface\"\n",
        "]\n",
        "\n",
        "# Create a separate plot for each variable\n",
        "for variable in variables:\n",
        "    try:\n",
        "        # Get the data for the current variable\n",
        "        data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
        "        \n",
        "        # Create a new figure\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        \n",
        "        # Pick one random location\n",
        "        random_location = random.choice(data.location.values)\n",
        "        \n",
        "        # Determine if it's a pipe or node variable\n",
        "        if variable.startswith(\"Pipes/\"):\n",
        "            location_type = \"Conduit ID\"\n",
        "        else:\n",
        "            location_type = \"Node ID\"\n",
        "        \n",
        "        # Plot the data for the randomly selected location\n",
        "        ax.plot(data.time, data.sel(location=random_location), label=f'{location_type} {random_location}')\n",
        "        \n",
        "        # Set the title and labels\n",
        "        ax.set_title(f'{variable} Over Time ({location_type} {random_location})')\n",
        "        ax.set_xlabel('Time')  # Corrected from ax.xlabel to ax.set_xlabel\n",
        "        ax.set_ylabel(f'{variable} ({data.attrs[\"units\"]})')  # Corrected from ax.ylabel to ax.set_ylabel\n",
        "        \n",
        "        # Format the x-axis to show dates nicely\n",
        "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
        "        plt.xticks(rotation=45)\n",
        "        \n",
        "        # Add a legend\n",
        "        ax.legend(title=location_type, loc='upper left')\n",
        "        \n",
        "        # Adjust the layout\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting {variable}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 8: Get pump station timeseries\n",
        "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
        "# Use the results_pump_station_timeseries method \n",
        "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
        "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
        "print(pump_timeseries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pump Stations/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the pump station timeseries data\n",
        "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
        "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
        "\n",
        "# Print the pump station timeseries\n",
        "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
        "print(pump_timeseries)\n",
        "\n",
        "# Create a new figure for plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# Plot each variable in the timeseries\n",
        "for variable in pump_timeseries.coords['variable'].values:\n",
        "    data = pump_timeseries.sel(variable=variable)\n",
        "    \n",
        "    # Decode units to strings\n",
        "    unit = pump_timeseries.attrs[\"units\"][list(pump_timeseries.coords[\"variable\"].values).index(variable)][1].decode('utf-8')\n",
        "    \n",
        "    # Check if the variable is 'Pumps on' to plot it differently\n",
        "    if variable == 'Pumps on':\n",
        "        # Plot with color based on the on/off status\n",
        "        colors = ['green' if val > 0 else 'red' for val in data.values.flatten()]\n",
        "        ax.scatter(pump_timeseries['time'], data, label=f'{variable} ({unit})', color=colors)\n",
        "    else:\n",
        "        ax.plot(pump_timeseries['time'], data, label=f'{variable} ({unit})')\n",
        "        \n",
        "        # Label the peak values\n",
        "        peak_time = pump_timeseries['time'][data.argmax()]\n",
        "        peak_value = data.max()\n",
        "        ax.annotate(f'Peak: {peak_value:.2f}', xy=(peak_time, peak_value), \n",
        "                    xytext=(peak_time, peak_value + 0.1 * peak_value), \n",
        "                    arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "                    fontsize=10, color='black', ha='center')\n",
        "\n",
        "# Set the title and labels\n",
        "ax.set_title(f'Timeseries Data for Pump Station: {pump_station_name}')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Values')\n",
        "\n",
        "# Format the x-axis to show dates nicely\n",
        "ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add a legend\n",
        "ax.legend(title='Variables', loc='upper left')\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring HDF Datasets with HdfBase.get_dataset_info\n",
        "This allows users to find HDF information that is not included in the ras-commander library.  Find the path in HDFView and set the group_path below to explore the HDF datasets and attributes.  Then, use the output to write your own function to extract the data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
        "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pipe Conduits/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For HDF datasets that are not supported by the RAS-Commander library, provide the dataset path to HdfBase.get_dataset_info and provide the output to an LLM along with a relevent HDF* class(es) to generate new functions that extend the library's coverage.   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\13_2d_detail_face_data_extraction.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEC-RAS 2D Detail Face Data Extraction Examples\n",
        "\n",
        "This notebook demonstrates how to extract detailed 2D face data, display individual cell face results and calculate a discharge weighted velocity using a user-provided profile line located where cell faces are perpendicular to flow. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "#from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import psutil  # For getting system CPU info\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will try to import the pip package, if it fails it will \n",
        "# add the parent directory to the Python path and try to import again\n",
        "# This assumes you are working in a subfolder of the ras-commander repository\n",
        "# This allows a user's revisions to be tested locally without installing the package\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Flexible imports to allow for development without installation \n",
        "#  ** Use this version with Jupyter Notebooks **\n",
        "try:\n",
        "    # Try to import from the installed package\n",
        "    from ras_commander import *\n",
        "except ImportError:\n",
        "    # If the import fails, add the parent directory to the Python path\n",
        "    import os\n",
        "    current_file = Path(os.getcwd()).resolve()\n",
        "    rascmdr_directory = current_file.parent\n",
        "    sys.path.append(str(rascmdr_directory))\n",
        "    print(\"Loading ras-commander from local dev copy\")\n",
        "    # Now try to import again\n",
        "    from ras_commander import *\n",
        "print(\"ras_commander imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: This notebook relies on the Chippewa 2D Project along with:\n",
        " - A user-generated GeoJSON containing the proposed profile lines\n",
        " - An example is provided in the \"data\" subfolder with name profile_lines_chippewa2D.geojson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the Chippewa_2D project from HEC and run plan 01\n",
        "\n",
        "# Define the path to the Chippewa_2D project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"Chippewa_2D\"\n",
        "import logging\n",
        "\n",
        "# Check if Chippewa_2D.p02.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = bald_eagle_path / \"Chippewa_2D.p02.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the Chippewa_2D project\n",
        "    RasExamples.extract_project([\"Chippewa_2D\"])\n",
        "\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"02\"\n",
        "\n",
        "    # Update run flags for the project\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,\n",
        "        unsteady_flow_simulation=True,\n",
        "        run_sediment=False,\n",
        "        post_processor=True,\n",
        "        floodplain_mapping=False\n",
        "    )\n",
        "\n",
        "    # Execute Plan 02 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
        "    if success_bald_eagle:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"Chippewa_2D.p02.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    plan_number = \"02\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show ras object info\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ras.get_hdf_entries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the HDF input path as Plan Number\n",
        "\n",
        "plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate: Get the geometry HDF path if you are extracting geometry elements from the geometry HDF\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract runtime and compute time data\n",
        "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
        "runtime_df = HdfResultsPlan.get_runtime_data(hdf_path=plan_number)\n",
        "if runtime_df is not None:\n",
        "    runtime_df\n",
        "else:\n",
        "    print(\"No runtime data found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
        "print(geom_hdf_path)\n",
        "\n",
        "# For the example project, plan 02 is associated with geometry 09\n",
        "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
        "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
        "if projection:\n",
        "    print(f\"Projection: {projection}\")\n",
        "else:\n",
        "    print(\"No projection information found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the  to USA Contiguous Albers Equal Area Conic (USGS version)\n",
        "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
        "projection = 'EPSG:5070'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfPlan for geometry-related operations\n",
        "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
        "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
        "\n",
        "if not geom_attrs.empty:\n",
        "    # Display the DataFrame directly\n",
        "    print(\"Base Geometry Attributes:\")\n",
        "    geom_attrs\n",
        "else:\n",
        "    print(\"No base geometry attributes found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfMesh for geometry-related operations\n",
        "print(\"\\nExample 3: Listing 2D Flow Area Names\")\n",
        "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
        "print(\"2D Flow Area Names:\", flow_area_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Get 2D Flow Area Attributes (get_geom_2d_flow_area_attrs)\n",
        "print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n",
        "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n",
        "flow_area_attributes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n",
        "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
        "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)  # Corrected function name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract mesh cell faces\n",
        "print(\"\\nExample: Extracting mesh cell faces\")\n",
        "\n",
        "# Get mesh cell faces using the standardize_input decorator for consistent file handling\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces GeoDataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the projection to USA Contiguous Albers Equal Area Conic (USGS version)\n",
        "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
        "projection = 'EPSG:5070'  # NAD83 / Conus Albers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Function: Find the nearest cell face to a given point\n",
        "# This provides enough basic information the face cell logic in the notebook\n",
        "\n",
        "def find_nearest_cell_face(point, cell_faces_df):\n",
        "    \"\"\"\n",
        "    Find the nearest cell face to a given point.\n",
        "\n",
        "    Args:\n",
        "        point (shapely.geometry.Point): The input point.\n",
        "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
        "\n",
        "    Returns:\n",
        "        int: The face_id of the nearest cell face.\n",
        "        float: The distance to the nearest cell face.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the input point to all cell faces\n",
        "    distances = cell_faces_df.geometry.distance(point)\n",
        "\n",
        "    # Find the index of the minimum distance\n",
        "    nearest_index = distances.idxmin()\n",
        "\n",
        "    # Get the face_id and distance of the nearest cell face\n",
        "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
        "    nearest_distance = distances[nearest_index]\n",
        "\n",
        "    return nearest_face_id, nearest_distance\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nExample: Finding the nearest cell face to a given point\")\n",
        "\n",
        "# Create a sample point (you can replace this with any point of interest)\n",
        "from shapely.geometry import Point\n",
        "from geopandas import GeoDataFrame\n",
        "\n",
        "# Create the sample point with the same CRS as mesh_cell_faces\n",
        "sample_point = GeoDataFrame(\n",
        "    {'geometry': [Point(1025677, 7853731)]}, \n",
        "    crs=mesh_cell_faces.crs\n",
        ")\n",
        "\n",
        "if not mesh_cell_faces.empty and not sample_point.empty:\n",
        "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n",
        "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
        "    print(f\"Face ID: {nearest_face_id}\")\n",
        "    print(f\"Distance: {distance:.2f} units\")\n",
        "\n",
        "    # Visualize the result\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    # Plot all cell faces\n",
        "    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
        "    \n",
        "    # Plot the sample point\n",
        "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
        "    \n",
        "    # Plot the nearest cell face\n",
        "    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n",
        "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
        "    \n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('X Coordinate')\n",
        "    ax.set_ylabel('Y Coordinate')\n",
        "    ax.set_title('Nearest Cell Face to Sample Point')\n",
        "    \n",
        "    # Add legend and grid\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    \n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Unable to perform nearest cell face search due to missing data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract mesh cell faces and plot with profile lines\n",
        "print(\"\\nExample: Extracting mesh cell faces and plotting with profile lines\")\n",
        "\n",
        "# Get mesh cell faces\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces DataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces\n",
        "\n",
        "# Load the GeoJSON file for profile lines\n",
        "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
        "profile_lines_gdf = gpd.read_file(geojson_path)\n",
        "\n",
        "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
        "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
        "\n",
        "# Plot the mesh cell faces and profile lines together\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "mesh_cell_faces.plot(ax=ax, color='blue', alpha=0.5, edgecolor='k', label='Mesh Cell Faces')\n",
        "profile_lines_gdf.plot(ax=ax, color='orange', linewidth=2, label='Profile Lines')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Easting')\n",
        "ax.set_ylabel('Northing')\n",
        "ax.set_title('Mesh Cell Faces and Profile Lines')\n",
        "\n",
        "# Add grid and legend\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extracting mesh cell faces near profile lines\n",
        "print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n",
        "\n",
        "# Get mesh cell faces using HdfMesh class\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces DataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces\n",
        "\n",
        "# Load the GeoJSON file for profile lines\n",
        "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
        "profile_lines_gdf = gpd.read_file(geojson_path)\n",
        "\n",
        "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
        "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
        "\n",
        "# Initialize a dictionary to store faces near each profile line\n",
        "faces_near_profile_lines = {}\n",
        "\n",
        "# Define distance threshold (10 ft converted to meters)\n",
        "distance_threshold = 10\n",
        "angle_threshold = 60  # degrees\n",
        "\n",
        "# Function to calculate the smallest angle between two lines or line segments.\n",
        "def calculate_angle(line):\n",
        "    if isinstance(line, LineString):\n",
        "        x_diff = line.xy[0][-1] - line.xy[0][0]\n",
        "        y_diff = line.xy[1][-1] - line.xy[1][0]\n",
        "    else:\n",
        "        x_diff = line[1][0] - line[0][0]\n",
        "        y_diff = line[1][1] - line[0][1]\n",
        "    \n",
        "    angle = np.degrees(np.arctan2(y_diff, x_diff))\n",
        "    return angle % 360 if angle >= 0 else (angle + 360) % 360\n",
        "\n",
        "# Function to break line into segments\n",
        "def break_line_into_segments(line, segment_length):\n",
        "    segments = []\n",
        "    segment_angles = []\n",
        "    \n",
        "    distances = np.arange(0, line.length, segment_length)\n",
        "    if distances[-1] != line.length:\n",
        "        distances = np.append(distances, line.length)\n",
        "        \n",
        "    for i in range(len(distances)-1):\n",
        "        point1 = line.interpolate(distances[i])\n",
        "        point2 = line.interpolate(distances[i+1])\n",
        "        segment = LineString([point1, point2])\n",
        "        segments.append(segment)\n",
        "        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n",
        "        \n",
        "    return segments, segment_angles\n",
        "\n",
        "# Function to calculate angle difference accounting for 180 degree equivalence\n",
        "def angle_difference(angle1, angle2):\n",
        "    diff = abs(angle1 - angle2) % 180\n",
        "    return min(diff, 180 - diff)\n",
        "\n",
        "# Function to order faces along profile line\n",
        "def order_faces_along_profile(profile_line, faces_gdf):\n",
        "    profile_start = Point(profile_line.coords[0])\n",
        "    \n",
        "    faces_with_dist = []\n",
        "    for idx, face in faces_gdf.iterrows():\n",
        "        face_start = Point(face.geometry.coords[0])\n",
        "        dist = profile_start.distance(face_start)\n",
        "        faces_with_dist.append((idx, dist))\n",
        "    \n",
        "    faces_with_dist.sort(key=lambda x: x[1])\n",
        "    return [x[0] for x in faces_with_dist]\n",
        "\n",
        "# Function to combine ordered faces into single linestring\n",
        "def combine_faces_to_linestring(ordered_faces_gdf):\n",
        "    coords = []\n",
        "    for _, face in ordered_faces_gdf.iterrows():\n",
        "        if not coords:  # First face - add all coordinates\n",
        "            coords.extend(list(face.geometry.coords))\n",
        "        else:  # Subsequent faces - add only end coordinate\n",
        "            coords.append(face.geometry.coords[-1])\n",
        "    return LineString(coords)\n",
        "\n",
        "# Initialize GeoDataFrame for final profile-to-faceline results\n",
        "profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n",
        "\n",
        "# Iterate through each profile line\n",
        "for index, profile_line in profile_lines_gdf.iterrows():\n",
        "    profile_geom = profile_line.geometry\n",
        "    \n",
        "    # Break profile line into segments\n",
        "    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n",
        "    \n",
        "    # Initialize set to store nearby faces\n",
        "    nearby_faces = set()\n",
        "    \n",
        "    # For each face, check distance to segments and angle difference\n",
        "    for face_idx, face in mesh_cell_faces.iterrows():\n",
        "        face_geom = face.geometry\n",
        "        \n",
        "        if isinstance(face_geom, LineString):\n",
        "            face_angle = calculate_angle(face_geom)\n",
        "            \n",
        "            for segment, segment_angle in zip(segments, segment_angles):\n",
        "                if face_geom.distance(segment) <= distance_threshold:\n",
        "                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n",
        "                        nearby_faces.add(face_idx)\n",
        "                        break\n",
        "    \n",
        "    # Convert the set of indices back to a GeoDataFrame\n",
        "    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n",
        "    \n",
        "    # Order faces along profile line\n",
        "    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n",
        "    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n",
        "    \n",
        "    # Combine ordered faces into single linestring\n",
        "    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n",
        "    \n",
        "    # Add to profile_to_faceline GeoDataFrame\n",
        "    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n",
        "                               'geometry': [combined_linestring]}, \n",
        "                              crs=profile_lines_gdf.crs)\n",
        "    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n",
        "    \n",
        "    # Store the ordered faces in the dictionary\n",
        "    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot all mesh cell faces in light blue\n",
        "mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n",
        "\n",
        "# Plot selected faces for each profile line with numbers\n",
        "colors = ['red', 'green', 'blue']\n",
        "for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n",
        "    if not faces.empty:\n",
        "        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n",
        "        \n",
        "        # Add numbers to faces\n",
        "        for i, (idx, face) in enumerate(faces.iterrows()):\n",
        "            midpoint = face.geometry.interpolate(0.5, normalized=True)\n",
        "            ax.text(midpoint.x, midpoint.y, str(i+1), \n",
        "                   color=color, fontweight='bold', ha='center', va='center')\n",
        "\n",
        "# Plot the combined linestrings\n",
        "profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n",
        "                        linestyle='--', label='Combined Face Lines')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Easting')\n",
        "ax.set_ylabel('Northing')\n",
        "ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile')\n",
        "\n",
        "# Add grid and legend\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nOriginal ordered faces near profile lines:\")\n",
        "faces_near_profile_lines\n",
        "\n",
        "print(\"\\nCombined profile-to-faceline results:\")\n",
        "profile_to_faceline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get face property tables with error handling\n",
        "face_property_tables = HdfMesh.get_mesh_face_property_tables(geom_hdf_path)\n",
        "face_property_tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the face property table for Face ID 4 and display it\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "face_id = 4\n",
        "face_properties = face_property_tables['Perimeter 1'][face_property_tables['Perimeter 1']['Face ID'] == face_id]\n",
        "\n",
        "# Create subplots arranged horizontally\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot Z vs Area\n",
        "axs[0].plot(face_properties['Z'], face_properties['Area'], marker='o', color='blue', label='Area')\n",
        "axs[0].set_title(f'Face ID {face_id}: Z vs Area')\n",
        "axs[0].set_xlabel('Z')\n",
        "axs[0].set_ylabel('Area')\n",
        "axs[0].grid(True)\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot Z vs Wetted Perimeter\n",
        "axs[1].plot(face_properties['Z'], face_properties['Wetted Perimeter'], marker='o', color='green', label='Wetted Perimeter')\n",
        "axs[1].set_title(f'Face ID {face_id}: Z vs Wetted Perimeter')\n",
        "axs[1].set_xlabel('Z')\n",
        "axs[1].set_ylabel('Wetted Perimeter')\n",
        "axs[1].grid(True)\n",
        "axs[1].legend()\n",
        "\n",
        "# Plot Z vs Manning's n\n",
        "axs[2].plot(face_properties['Z'], face_properties[\"Manning's n\"], marker='o', color='red', label=\"Manning's n\")\n",
        "axs[2].set_title(f'Face ID {face_id}: Z vs Manning\\'s n')\n",
        "axs[2].set_xlabel('Z')\n",
        "axs[2].set_ylabel(\"Manning's n\")\n",
        "axs[2].grid(True)\n",
        "axs[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh timeseries output\n",
        "# Get mesh areas from previous code cell\n",
        "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
        "\n",
        "mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n",
        "timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n",
        "timeseries_da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh cells timeseries output\n",
        "cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Cells Timeseries Output:\")\n",
        "cells_timeseries_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get mesh faces timeseries output\n",
        "faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMesh Faces Timeseries Output:\")\n",
        "faces_timeseries_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert all face velocities and face flow values to positive for further calculations\n",
        "# We have visually confirmed for this model that all flow is moving in the same direction\n",
        "\n",
        "# Function to process and convert face data to positive values\n",
        "def convert_to_positive_values(faces_timeseries_ds, cells_timeseries_ds):\n",
        "    \"\"\"\n",
        "    Convert face velocities and flows to positive values while maintaining their relationships.\n",
        "    \n",
        "    Args:\n",
        "        faces_timeseries_ds (xarray.Dataset): Dataset containing face timeseries data\n",
        "        cells_timeseries_ds (xarray.Dataset): Dataset containing cell timeseries data\n",
        "        \n",
        "    Returns:\n",
        "        xarray.Dataset: Modified dataset with positive values\n",
        "    \"\"\"\n",
        "    # Get the face velocity and flow variables\n",
        "    face_velocity = faces_timeseries_ds['face_velocity']\n",
        "    face_flow = faces_timeseries_ds['face_flow']\n",
        "    \n",
        "    # Calculate the sign of the velocity to maintain flow direction relationships\n",
        "    velocity_sign = xr.where(face_velocity >= 0, 1, -1)\n",
        "    \n",
        "    # Convert velocities and flows to absolute values while maintaining their relationship\n",
        "    faces_timeseries_ds['face_velocity'] = abs(face_velocity)\n",
        "    faces_timeseries_ds['face_flow'] = abs(face_flow)\n",
        "    \n",
        "    # Store the original sign as a new variable for reference\n",
        "    faces_timeseries_ds['velocity_direction'] = velocity_sign\n",
        "    \n",
        "    print(\"Conversion to positive values complete.\")\n",
        "    print(f\"Number of faces processed: {len(faces_timeseries_ds.face_id)}\")\n",
        "    \n",
        "    return faces_timeseries_ds, cells_timeseries_ds\n",
        "\n",
        "# Convert the values in our datasets\n",
        "faces_timeseries_ds_positive, cells_timeseries_ds_positive = convert_to_positive_values(\n",
        "    faces_timeseries_ds, \n",
        "    cells_timeseries_ds\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "# Function to process faces for a single profile line\n",
        "def process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds):\n",
        "    face_ids = faces['face_id'].tolist()\n",
        "    \n",
        "    # Extract relevant data for these faces\n",
        "    face_velocities = faces_timeseries_ds['face_velocity'].sel(face_id=face_ids)\n",
        "    face_flows = faces_timeseries_ds['face_flow'].sel(face_id=face_ids)\n",
        "    \n",
        "    # Create a new dataset with calculated results\n",
        "    results_ds = xr.Dataset({\n",
        "        'face_velocity': face_velocities,\n",
        "        'face_flow': face_flows\n",
        "    })\n",
        "    \n",
        "    # Convert to dataframe for easier manipulation\n",
        "    results_df = results_ds.to_dataframe().reset_index()\n",
        "    \n",
        "    # Add profile name and face order\n",
        "    results_df['profile_name'] = profile_name\n",
        "    results_df['face_order'] = results_df.groupby('time')['face_id'].transform(lambda x: pd.factorize(x)[0])\n",
        "    \n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Vave = Sum Qn / Sum An for each profile line\n",
        "where Vave = the summation of face flow / flow area for all the faces in the profile line\n",
        "\n",
        "Then, save the results to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all profile lines\n",
        "all_results = []\n",
        "for profile_name, faces in faces_near_profile_lines.items():\n",
        "    profile_results = process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds)\n",
        "    all_results.append(profile_results)\n",
        "\n",
        "# Combine results from all profile lines\n",
        "combined_results_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Display the first few rows of the combined results\n",
        "combined_results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profile_time_series = {}\n",
        "\n",
        "# Iterate through each profile line and extract its corresponding data\n",
        "for profile_name, faces_gdf in faces_near_profile_lines.items():\n",
        "    # Get the list of face_ids for this profile line\n",
        "    face_ids = faces_gdf['face_id'].tolist()\n",
        "    \n",
        "    # Filter the combined_results_df for these face_ids\n",
        "    profile_df = combined_results_df[combined_results_df['face_id'].isin(face_ids)].copy()\n",
        "    \n",
        "    # Add the profile name as a column\n",
        "    profile_df['profile_name'] = profile_name\n",
        "    \n",
        "    # Reset index for cleanliness\n",
        "    profile_df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # Store in the dictionary\n",
        "    profile_time_series[profile_name] = profile_df\n",
        "    \n",
        "    # Display a preview\n",
        "    print(f\"\\nTime Series DataFrame for {profile_name}:\")\n",
        "    profile_df\n",
        "\n",
        "# Optionally, display all profile names\n",
        "print(\"\\nProfile Lines Processed:\")\n",
        "profile_time_series\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_profiles_df = pd.concat(profile_time_series.values(), ignore_index=True)\n",
        "\n",
        "# Display the combined dataframe\n",
        "print(\"Combined Time Series DataFrame for All Profiles:\")\n",
        "all_profiles_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have the necessary variables\n",
        "print(\"Available variables:\")\n",
        "print(\"profile_time_series:\", 'profile_time_series' in locals())\n",
        "print(\"faces_near_profile_lines:\", 'faces_near_profile_lines' in locals())\n",
        "print(\"profile_averages:\", 'profile_averages' in locals())\n",
        "\n",
        "# Look at the structure of profile_time_series\n",
        "if 'profile_time_series' in locals():\n",
        "    for name, df in profile_time_series.items():\n",
        "        print(f\"\\nColumns in {name}:\")\n",
        "        print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_discharge_weighted_velocity(profile_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate discharge-weighted average velocity for a profile line\n",
        "    Vw = Sum(|Qi|*Vi)/Sum(|Qi|) where Qi is face flow and Vi is face velocity\n",
        "    \"\"\"\n",
        "    print(\"Calculating discharge-weighted velocity...\")\n",
        "    print(f\"Input DataFrame:\\n{profile_df.head()}\")\n",
        "\n",
        "    # Calculate weighted velocity for each timestep\n",
        "    weighted_velocities = []\n",
        "    for time in profile_df['time'].unique():\n",
        "        time_data = profile_df[profile_df['time'] == time]\n",
        "        abs_flows = np.abs(time_data['face_flow'])\n",
        "        abs_velocities = np.abs(time_data['face_velocity'])\n",
        "        weighted_vel = (abs_flows * abs_velocities).sum() / abs_flows.sum()\n",
        "        weighted_velocities.append({\n",
        "            'time': time,\n",
        "            'weighted_velocity': weighted_vel\n",
        "        })\n",
        "    \n",
        "    weighted_df = pd.DataFrame(weighted_velocities)\n",
        "    print(f\"Calculated weighted velocities:\\n{weighted_df.head()}\")\n",
        "    return weighted_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate for each profile line\n",
        "for profile_name, profile_df in profile_time_series.items():\n",
        "    print(f\"\\nProcessing profile: {profile_name}\")\n",
        "\n",
        "    # Calculate discharge-weighted velocity\n",
        "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
        "    \n",
        "    print(\"Weighted velocities calculated.\")\n",
        "    # Get ordered faces for this profile\n",
        "    ordered_faces = faces_near_profile_lines[profile_name]\n",
        "    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n",
        "    \n",
        "    print(\"Converted time to datetime format.\")\n",
        "\n",
        "    # Get ordered faces for this profile\n",
        "    ordered_faces = faces_near_profile_lines[profile_name]\n",
        "    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n",
        "    \n",
        "    # Save dataframes in the output directory\n",
        "    output_file = ras.project_folder / f\"{profile_name}_discharge_weighted_velocity.csv\"\n",
        "    weighted_velocities.to_csv(output_file, index=False)\n",
        "    print(f\"Saved weighted velocities to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create plots comparing discharge-weighted velocity and simple average for each profile line\n",
        "for profile_name, profile_df in profile_time_series.items():\n",
        "    \n",
        "    print(f\"\\nGenerating comparison plot for profile: {profile_name}\")\n",
        "    \n",
        "    # Calculate discharge-weighted velocity\n",
        "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
        "    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n",
        "    \n",
        "    # Calculate simple average velocity for each timestep\n",
        "    simple_averages = profile_df.groupby('time')['face_velocity'].mean().reset_index()\n",
        "    simple_averages['time'] = pd.to_datetime(simple_averages['time'])\n",
        "    \n",
        "    # Create figure for comparison plot\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    \n",
        "    # Plot individual face velocities with thin lines\n",
        "    for face_id in profile_df['face_id'].unique():\n",
        "        face_data = profile_df[profile_df['face_id'] == face_id]\n",
        "        plt.plot(face_data['time'], \n",
        "                face_data['face_velocity'], \n",
        "                alpha=0.8,  # More transparent\n",
        "                linewidth=0.3,  # Thinner line\n",
        "                color='gray',  # Consistent color\n",
        "                label=f'Face ID {face_id}')\n",
        "        \n",
        "        # Find and annotate peak value for each face\n",
        "        peak_idx = face_data['face_velocity'].idxmax()\n",
        "        peak_time = face_data.loc[peak_idx, 'time']\n",
        "        peak_vel = face_data.loc[peak_idx, 'face_velocity']\n",
        "        plt.annotate(f'{peak_vel:.2f} ({face_id})',\n",
        "                    xy=(peak_time, peak_vel),\n",
        "                    xytext=(10, 10),\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=8,\n",
        "                    alpha=0.5)\n",
        "    \n",
        "    # Plot discharge-weighted velocity\n",
        "    plt.plot(weighted_velocities['time'], \n",
        "            weighted_velocities['weighted_velocity'], \n",
        "            color='red', \n",
        "            alpha=1.0, \n",
        "            linewidth=2,\n",
        "            label='Discharge-Weighted Velocity')\n",
        "    \n",
        "    # Find and annotate peak weighted velocity\n",
        "    peak_idx = weighted_velocities['weighted_velocity'].idxmax()\n",
        "    peak_time = weighted_velocities.loc[peak_idx, 'time']\n",
        "    peak_vel = weighted_velocities.loc[peak_idx, 'weighted_velocity']\n",
        "    plt.annotate(f'Peak Weighted: {peak_vel:.2f}',\n",
        "                xy=(peak_time, peak_vel),\n",
        "                xytext=(10, 10),\n",
        "                textcoords='offset points',\n",
        "                color='red',\n",
        "                fontweight='bold')\n",
        "    \n",
        "    # Plot simple average\n",
        "    plt.plot(simple_averages['time'], \n",
        "            simple_averages['face_velocity'], \n",
        "            color='blue', \n",
        "            alpha=0.5, \n",
        "            linewidth=1,\n",
        "            linestyle='--',\n",
        "            label='Simple Average')\n",
        "    \n",
        "    # Find and annotate peak simple average\n",
        "    peak_idx = simple_averages['face_velocity'].idxmax()\n",
        "    peak_time = simple_averages.loc[peak_idx, 'time']\n",
        "    peak_vel = simple_averages.loc[peak_idx, 'face_velocity']\n",
        "    plt.annotate(f'Peak Average: {peak_vel:.2f}',\n",
        "                xy=(peak_time, peak_vel),\n",
        "                xytext=(10, -10),\n",
        "                textcoords='offset points',\n",
        "                color='blue',\n",
        "                fontweight='bold')\n",
        "    \n",
        "    # Configure plot\n",
        "    plt.title(f'Velocity Comparison for {profile_name} \\nIndividual Face Velocities vs Simple Average Velocity vs Discharge-Weighted Average Velocity')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Velocity (ft/s)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add legend with better placement\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    # Adjust layout to accommodate legend and stats\n",
        "    plt.subplots_adjust(right=0.8)\n",
        "    \n",
        "    # Save plot to file\n",
        "    plot_file = ras.project_folder / f\"{profile_name}_velocity_comparison.png\"\n",
        "    plt.savefig(plot_file, bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed comparison\n",
        "    print(f\"\\nVelocity Comparison for {profile_name} \\nIndividual Face Velocities vs Simple Average Velocity vs Discharge-Weighted Average Velocity\")\n",
        "    print(f\"Number of faces: {profile_df['face_id'].nunique()}\")\n",
        "    print(\"\\nDischarge-Weighted Velocity Statistics:\")\n",
        "    print(f\"Mean: {weighted_velocities['weighted_velocity'].mean():.2f} ft/s\")\n",
        "    print(f\"Max: {weighted_velocities['weighted_velocity'].max():.2f} ft/s\")\n",
        "    print(f\"Min: {weighted_velocities['weighted_velocity'].min():.2f} ft/s\")\n",
        "    print(\"\\nSimple Average Velocity Statistics:\")\n",
        "    print(f\"Mean: {simple_averages['face_velocity'].mean():.2f} ft/s\")\n",
        "    print(f\"Max: {simple_averages['face_velocity'].max():.2f} ft/s\")\n",
        "    print(f\"Min: {simple_averages['face_velocity'].min():.2f} ft/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extracting mesh cell faces near profile lines\n",
        "print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n",
        "\n",
        "# Get mesh cell faces using HdfMesh class\n",
        "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
        "\n",
        "# Display the first few rows of the mesh cell faces DataFrame\n",
        "print(\"First few rows of mesh cell faces:\")\n",
        "mesh_cell_faces\n",
        "\n",
        "# Load the GeoJSON file for profile lines\n",
        "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
        "profile_lines_gdf = gpd.read_file(geojson_path)\n",
        "\n",
        "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
        "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
        "\n",
        "# Initialize a dictionary to store faces near each profile line\n",
        "faces_near_profile_lines = {}\n",
        "\n",
        "# Define distance threshold (10 ft converted to meters)\n",
        "distance_threshold = 10\n",
        "angle_threshold = 60  # degrees\n",
        "\n",
        "# Function to calculate the smallest angle between two lines or line segments.\n",
        "def calculate_angle(line):\n",
        "    if isinstance(line, LineString):\n",
        "        x_diff = line.xy[0][-1] - line.xy[0][0]\n",
        "        y_diff = line.xy[1][-1] - line.xy[1][0]\n",
        "    else:\n",
        "        x_diff = line[1][0] - line[0][0]\n",
        "        y_diff = line[1][1] - line[0][1]\n",
        "    \n",
        "    angle = np.degrees(np.arctan2(y_diff, x_diff))\n",
        "    return angle % 360 if angle >= 0 else (angle + 360) % 360\n",
        "\n",
        "# Function to break line into segments\n",
        "def break_line_into_segments(line, segment_length):\n",
        "    segments = []\n",
        "    segment_angles = []\n",
        "    \n",
        "    distances = np.arange(0, line.length, segment_length)\n",
        "    if distances[-1] != line.length:\n",
        "        distances = np.append(distances, line.length)\n",
        "        \n",
        "    for i in range(len(distances)-1):\n",
        "        point1 = line.interpolate(distances[i])\n",
        "        point2 = line.interpolate(distances[i+1])\n",
        "        segment = LineString([point1, point2])\n",
        "        segments.append(segment)\n",
        "        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n",
        "        \n",
        "    return segments, segment_angles\n",
        "\n",
        "# Function to calculate angle difference accounting for 180 degree equivalence\n",
        "def angle_difference(angle1, angle2):\n",
        "    diff = abs(angle1 - angle2) % 180\n",
        "    return min(diff, 180 - diff)\n",
        "\n",
        "# Function to order faces along profile line\n",
        "def order_faces_along_profile(profile_line, faces_gdf):\n",
        "    profile_start = Point(profile_line.coords[0])\n",
        "    \n",
        "    faces_with_dist = []\n",
        "    for idx, face in faces_gdf.iterrows():\n",
        "        face_start = Point(face.geometry.coords[0])\n",
        "        dist = profile_start.distance(face_start)\n",
        "        faces_with_dist.append((idx, dist))\n",
        "    \n",
        "    faces_with_dist.sort(key=lambda x: x[1])\n",
        "    return [x[0] for x in faces_with_dist]\n",
        "\n",
        "# Function to combine ordered faces into single linestring\n",
        "def combine_faces_to_linestring(ordered_faces_gdf):\n",
        "    coords = []\n",
        "    for _, face in ordered_faces_gdf.iterrows():\n",
        "        if not coords:  # First face - add all coordinates\n",
        "            coords.extend(list(face.geometry.coords))\n",
        "        else:  # Subsequent faces - add only end coordinate\n",
        "            coords.append(face.geometry.coords[-1])\n",
        "    return LineString(coords)\n",
        "\n",
        "# Initialize GeoDataFrame for final profile-to-faceline results\n",
        "profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n",
        "\n",
        "# Iterate through each profile line\n",
        "for index, profile_line in profile_lines_gdf.iterrows():\n",
        "    profile_geom = profile_line.geometry\n",
        "    \n",
        "    # Break profile line into segments\n",
        "    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n",
        "    \n",
        "    # Initialize set to store nearby faces\n",
        "    nearby_faces = set()\n",
        "    \n",
        "    # For each face, check distance to segments and angle difference\n",
        "    for face_idx, face in mesh_cell_faces.iterrows():\n",
        "        face_geom = face.geometry\n",
        "        \n",
        "        if isinstance(face_geom, LineString):\n",
        "            face_angle = calculate_angle(face_geom)\n",
        "            \n",
        "            for segment, segment_angle in zip(segments, segment_angles):\n",
        "                if face_geom.distance(segment) <= distance_threshold:\n",
        "                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n",
        "                        nearby_faces.add(face_idx)\n",
        "                        break\n",
        "    \n",
        "    # Convert the set of indices back to a GeoDataFrame\n",
        "    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n",
        "    \n",
        "    # Order faces along profile line\n",
        "    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n",
        "    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n",
        "    \n",
        "    # Combine ordered faces into single linestring\n",
        "    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n",
        "    \n",
        "    # Add to profile_to_faceline GeoDataFrame\n",
        "    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n",
        "                               'geometry': [combined_linestring]}, \n",
        "                              crs=profile_lines_gdf.crs)\n",
        "    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n",
        "    \n",
        "    # Store the ordered faces in the dictionary\n",
        "    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(24, 16))\n",
        "\n",
        "# Plot all mesh cell faces in light blue\n",
        "mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n",
        "\n",
        "# Plot selected faces for each profile line with numbers and velocities\n",
        "colors = ['red', 'green', 'blue']\n",
        "for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n",
        "    if not faces.empty:\n",
        "        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n",
        "        \n",
        "        # Get velocity data for this profile from profile_time_series\n",
        "        profile_data = profile_time_series[profile_name]\n",
        "        \n",
        "        # Add face_id above and peak velocity below for each face\n",
        "        for idx, face in faces.iterrows():\n",
        "            midpoint = face.geometry.interpolate(0.5, normalized=True)\n",
        "            \n",
        "            # Get peak velocity for this face\n",
        "            face_velocities = profile_data[profile_data['face_id'] == face['face_id']]['face_velocity']\n",
        "            peak_velocity = face_velocities.max() if not face_velocities.empty else 0.0\n",
        "            # Add face_id above the face\n",
        "            ax.text(midpoint.x, midpoint.y + 50,  # Adjust the +50 offset as needed\n",
        "                   f\"{face['face_id']}\", \n",
        "                   color=color, \n",
        "                   fontweight='bold',\n",
        "                   fontsize=8,\n",
        "                   ha='center', \n",
        "                   va='bottom')\n",
        "            \n",
        "            # Add peak velocity below the face\n",
        "            ax.text(midpoint.x, midpoint.y - 50,  # Adjust the -50 offset as needed\n",
        "                   f\"{peak_velocity:.2f}fps\", \n",
        "                   color=color, \n",
        "                   fontweight='bold',\n",
        "                   fontsize=6,\n",
        "                   ha='center', \n",
        "                   va='top')\n",
        "\n",
        "\n",
        "# Plot the combined linestrings\n",
        "profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n",
        "                        linestyle='--', label='Combined Face Lines')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Easting')\n",
        "ax.set_ylabel('Northing')\n",
        "ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile\\nFace ID and Peak Face Velocity Shown')\n",
        "\n",
        "# Add grid and legend\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nOriginal ordered faces near profile lines:\")\n",
        "faces_near_profile_lines\n",
        "\n",
        "print(\"\\nCombined profile-to-faceline results:\")\n",
        "profile_to_faceline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE:  We are using the face normal velocity that is available in the HDF.  This will only be accurate if you pick cell faces that are perpendicular to flow.  Depending on the application, a more robust calculation may be required. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\14_fluvial_pluvial_delineation.ipynb
==================================================
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delineate Fluvial and Pluvial Areas using RAS-Commander\n",
        "\n",
        "We will leverage the HEC RAS Summary Outputs to delineate the Fluvial and Pluvial Areas\n",
        "\n",
        "Maximum Water Surface Elevation (WSEL) for each cell is recorded, along with the timestamps of when the maximum WSEL occurs.\n",
        "\n",
        "By locating adjacent cells with dissimilar timestamps, we can delineate the Fluvial and Pluvial Areas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A note about datframe types: \n",
        "\n",
        "Information from the HEC-RAS plan files are generally dataframes.  The text file interface is for the 32-bit side of HEC-RAS and all spatial data is most easily accessed in the HDF files.  This includes plan_df, geom_df, hdf_paths_df\n",
        "\n",
        "Geometry elements (Mesh Faces and Nodes) are provided as Geodataframes (cell_polygons_gdf, boundary_gdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation and Environment Setup\n",
        "Uncomment and run package installation commands if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ras-commander from pip (uncomment to install if needed)\n",
        "#!pip install ras-commander\n",
        "# This installs ras-commander and all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required modules\n",
        "from ras_commander import *  # Import all ras-commander modules\n",
        "\n",
        "# Import the required libraries for this notebook\n",
        "import h5py\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pyproj\n",
        "from shapely.geometry import Point, LineString, Polygon\n",
        "import xarray as xr\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the BaldEagleCrkMulti2D project from HEC and run plan 06\n",
        "\n",
        "# Define the path to the BaldEagleCrkMulti2D project\n",
        "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
        "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
        "import logging\n",
        "\n",
        "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
        "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
        "\n",
        "if not hdf_file.exists():\n",
        "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
        "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
        "\n",
        "\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    logging.info(f\"Bald Eagle project initialized with folder: {ras.project_folder}\")\n",
        "    \n",
        "    logging.info(f\"Bald Eagle object id: {id(ras)}\")\n",
        "    \n",
        "    # Define the plan number to execute\n",
        "    plan_number = \"06\"\n",
        "\n",
        "    # Update the run flags in the plan file\n",
        "    RasPlan.update_run_flags(\n",
        "        plan_number,\n",
        "        geometry_preprocessor=True,  # Run HTab\n",
        "        unsteady_flow_simulation=True,  # Run UNet\n",
        "        post_processor=True,  # Run PostProcess\n",
        "        floodplain_mapping=False,  # Run RASMapper\n",
        "    )\n",
        "\n",
        "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
        "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
        "    success_bald_eagle = RasCmdr.compute_plan(plan_number)\n",
        "    if success_bald_eagle:\n",
        "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
        "    else:\n",
        "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
        "else:\n",
        "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
        "    # Initialize the RAS project using the default global ras object\n",
        "    init_ras_project(bald_eagle_path, \"6.6\")\n",
        "    plan_number = \"06\"\n",
        "your_project_path = bald_eagle_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  OPTIONAL: Use your own project instead\n",
        "\n",
        "your_project_path = Path(r\"D:\\yourprojectpath\")\n",
        "\n",
        "init_ras_project(your_project_path, \"6.6\")\n",
        "plan_number = \"01\"  # Plan number to use for this notebook \n",
        "\n",
        "\n",
        "\n",
        "### If you use this code cell, don't run the previous cell or change to markdown\n",
        "### NOTE: Ensure the HDF Results file was generated by HEC-RAS Version 6.x or above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Project Dataframes using 'ras' Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
        "\n",
        "# Display plan_df for bald_eagle project\n",
        "print(\"Plan DataFrame for bald_eagle project:\")\n",
        "ras.plan_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nGeometry DataFrame for the project:\")\n",
        "ras.geom_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nUnsteady DataFrame for the project:\")\n",
        "ras.unsteady_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBoundary Conditions DataFrame for the project:\")\n",
        "ras.boundaries_df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find Paths for Results and Geometry HDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the plan HDF path for the plan_number defined above\n",
        "plan_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plan_hdf_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the geometry HDF path\n",
        "geom_hdf_path = ras.plan_df.loc[ras.plan_df['plan_number'] == plan_number, 'Geom Path'].values[0] + '.hdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "geom_hdf_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
        "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fluvial Pluvial Delineation using RAS-Commander \n",
        "\n",
        "Using the Maximum WSE Results layer, which contains the maximum water surface and time stamp of the maximum water surface, mesh cell faces are categorized.  If the difference in time (delta_t) in hours is greater than the (user defined, default 12) duration specified, that mesh cell face is added to the fluvial-pluvial boundary dataset. \n",
        "\n",
        "This is meant to provide a draft fluvial-pluvial boundary for floodplain analysis, to the extent it can be derived directly from the HEC-RAS results files. \n",
        "\n",
        "The function attempts to combine adjacent line segments to simplify the resulting geometry, but GIS cleanup and manual interpolation will be required to create a closed polygon boundary that could be used for further processing steps.  However, this approach does provide an efficient method for providing a draft boundary that is based on HEC-RAS's direct computations and mesh cell faces. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using mesh_max_ws, get the cell coordinates and plot the max water surface as a map\n",
        "import matplotlib.pyplot as plt\n",
        "from ras_commander.HdfMesh import HdfMesh\n",
        "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Get mesh max water surface\n",
        "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path)\n",
        "\n",
        "print(\"max_ws_df\")\n",
        "print(max_ws_df)\n",
        "\n",
        "# If you get an error here, you may have a pre-6.0 HDF.  Re-run in 6.x to generate a new results file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the function to plot\n",
        "HdfResultsPlot.plot_results_max_wsel(max_ws_df)\n",
        "\n",
        "# Plot the time of maximum water surface elevation\n",
        "HdfResultsPlot.plot_results_max_wsel_time(max_ws_df)\n",
        "\n",
        "# Print the first few rows of the merged dataframe for verification\n",
        "print(\"\\nFirst few rows of the merged dataframe:\")\n",
        "max_ws_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use HdfUtils for extracting projection\n",
        "print(\"\\nExtracting Projection from HDF\")\n",
        "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
        "if projection:\n",
        "    print(f\"Projection: {projection}\")\n",
        "else:\n",
        "    print(\"No projection information found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Extract Cell Polygons\n",
        "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
        "cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(geom_hdf_path)\n",
        "\n",
        "\n",
        "# Call the function to plot cell polygons\n",
        "#cell_polygons_gdf = HdfFluvialPluvial.plot_cell_polygons(cell_polygons_gdf, projection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from shapely.geometry import LineString, Polygon, MultiLineString\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from rtree import index\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(plan_hdf_path, delta_t=72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics about the boundary line lengths\n",
        "boundary_lengths = boundary_gdf.geometry.length\n",
        "\n",
        "print(\"Boundary line length statistics:\")\n",
        "print(f\"Max length: {boundary_lengths.max():.2f}\")\n",
        "print(f\"Min length: {boundary_lengths.min():.2f}\")\n",
        "print(f\"Average length: {boundary_lengths.mean():.2f}\")\n",
        "print(f\"Median length: {boundary_lengths.median():.2f}\")\n",
        "\n",
        "# Print general information about the boundary GeoDataFrame\n",
        "print(\"\\nBoundary GeoDataFrame info:\")\n",
        "print(boundary_gdf.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
        "boundary_gdf.plot(ax=ax, color='red', linewidth=2)\n",
        "plt.title('Fluvial-Pluvial Boundary')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "length_threshold = 3000 #in same units as X and Y coordinates\n",
        "\n",
        "# Filter out boundary lines below the length threshold\n",
        "filtered_boundary_gdf = boundary_gdf[boundary_lengths >= length_threshold]\n",
        "highlighted_boundary_gdf = boundary_gdf[boundary_lengths < length_threshold]\n",
        "\n",
        "# Visualize the results with highlighted boundaries below the threshold\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
        "filtered_boundary_gdf.plot(ax=ax, color='red', linewidth=2, label='Valid Boundaries')\n",
        "highlighted_boundary_gdf.plot(ax=ax, color='blue', linewidth=2, linestyle='--', label='Highlighted Boundaries Below Threshold')\n",
        "plt.title('Fluvial-Pluvial Boundary with Length Threshold')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fluvial_pluvial_boundary subfolder\n",
        "output_dir = your_project_path / \"fluvial_pluvial_boundary\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "print(f\"Output directory created/verified at: {output_dir}\")\n",
        "\n",
        "# Save to GeoJSON in output directory\n",
        "boundary_gdf.to_file(output_dir / 'fluvial_pluvial_boundary.geojson', driver='GeoJSON')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmdr_pip_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
==================================================

File: c:\GH\ras-commander\examples\README.md
==================================================
# RAS Commander Examples

This directory contains example notebooks demonstrating how to use the `ras-commander` library for automating HEC-RAS operations. These examples cover basic to advanced usage scenarios and provide a practical guide for hydraulic modelers looking to automate their workflows.

## Overview

HEC-RAS (Hydrologic Engineering Center's River Analysis System) is widely used for hydraulic modeling. The `ras-commander` library provides a Python interface to automate HEC-RAS operations without using the graphical user interface. This enables batch processing, sensitivity analysis, and integration with other Python tools for water resources engineering.

These example notebooks are designed to:
- Demonstrate key functionalities of the `ras-commander` library
- Provide practical use cases for automation
- Guide users from basic to advanced operations
- Serve as templates for your own automation scripts

## Examples

### [00_Using_RasExamples.ipynb](00_Using_RasExamples.ipynb)

This notebook introduces the `RasExamples` class, which provides easy access to HEC-RAS example projects for testing and demonstration purposes.

**Key contents:**
- Installing `ras-commander` from pip
- Using flexible imports for development without installation
- Extracting specific HEC-RAS example projects by folder name
- Advanced usage options for managing example projects
- Listing available example projects and categories
- Working with the new pipes and conduits examples (version 6.6)

### [01_project_initialization.ipynb](01_project_initialization.ipynb)

This notebook covers initializing and working with HEC-RAS projects using the `ras-commander` library.

**Key contents:**
- Setting up and configuring the RAS Commander environment
- Downloading and extracting example HEC-RAS projects
- Initializing HEC-RAS projects using the global `ras` object
- Initializing multiple HEC-RAS projects using custom RAS objects
- Accessing various project components (plans, geometries, flows, boundaries)
- Understanding the RAS object structure and its components
- Working with boundary conditions
- Comparing multiple projects

### [02_plan_and_geometry_operations.ipynb](02_plan_and_geometry_operations.ipynb)

This notebook demonstrates operations on HEC-RAS plan and geometry files using the RAS Commander library.

**Key contents:**
- Project initialization and understanding plan/geometry files
- Cloning plans to create new simulation scenarios
- Cloning geometry files for modified versions
- Setting geometry files for plans
- Clearing geometry preprocessor files
- Configuring simulation parameters and intervals
- Setting run flags and updating descriptions
- Cloning and configuring unsteady flow files
- Computing plans and verifying results
- Working with advanced HDF data
- Best practices for plan and geometry operations

### [03_unsteady_flow_operations.ipynb](03_unsteady_flow_operations.ipynb)

This notebook demonstrates operations on unsteady flow files using the RAS Commander library.

**Key contents:**
- Understanding unsteady flow files in HEC-RAS
- Extracting boundary conditions and tables from unsteady flow files
- Inspecting and analyzing boundary condition structures
- Working with different boundary condition types (flow hydrographs, stage hydrographs, etc.)
- Modifying flow titles in unsteady flow files
- Configuring restart settings for continuing simulations
- Extracting and working with flow tables
- Modifying flow tables and writing them back to files
- Applying updated unsteady flow to a plan and computing results

### [04_multiple_project_operations.ipynb](04_multiple_project_operations.ipynb)

This notebook demonstrates how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library.

**Key contents:**
- Initializing and managing multiple HEC-RAS projects
- Cloning and modifying plans across different projects
- Running computations for multiple projects in parallel
- Optimizing computing resources when working with multiple models
- Analyzing and comparing results from different projects
- Building comprehensive multi-project workflows
- Best practices for multiple project management
- Setting up compute folders for multiple projects
- Comparing project structures and results

### [05_single_plan_execution.ipynb](05_single_plan_execution.ipynb)

This notebook focuses specifically on executing a single HEC-RAS plan with various configuration options.

**Key contents:**
- Understanding the `RasCmdr.compute_plan` method and its parameters
- Executing a plan with a specified number of processor cores
- Creating and managing destination folders for computations
- Overwriting existing destination folders
- Verifying computation results
- Options for single plan execution (basic execution, destination folder, number of cores, etc.)
- Best practices for single plan execution

### [06_executing_plan_sets.ipynb](06_executing_plan_sets.ipynb)

This notebook demonstrates different ways to specify and execute HEC-RAS plans using the RAS Commander library.

**Key contents:**
- Understanding plan specification in HEC-RAS
- Sequential execution of specific plans
- Selective plan execution based on criteria
- Running only plans without HDF results
- Verifying execution results
- Best practices for plan specification
- Choosing appropriate execution methods based on scenario
- Understanding the importance of plan selection for efficiency

### [07_sequential_plan_execution.ipynb](07_sequential_plan_execution.ipynb)

This notebook demonstrates how to sequentially execute multiple HEC-RAS plans using the RAS Commander library.

**Key contents:**
- Understanding sequential execution in HEC-RAS
- Using the `RasCmdr.compute_test_mode` method
- Executing all plans in a project sequentially
- Analyzing the test folder after sequential execution
- Executing specific plans with geometry preprocessor clearing
- Best practices for sequential execution
- Environment setup and test folder management
- Benefits of sequential execution (controlled resource usage, dependency management, etc.)

### [08_parallel_execution.ipynb](08_parallel_execution.ipynb)

This notebook demonstrates how to execute multiple HEC-RAS plans in parallel to maximize computational efficiency.

**Key contents:**
- Understanding parallel execution in HEC-RAS
- Setting up a working environment for parallel execution
- Checking system resources for optimal parallel execution
- Executing all plans in a project in parallel
- Executing specific plans in parallel
- Dynamic worker allocation based on available resources
- Balancing workers and cores per worker
- Analyzing parallel execution results
- Performance comparison between different parallel configurations
- Best practices for parallel execution

### [09_plan_parameter_operations.ipynb](09_plan_parameter_operations.ipynb)

This notebook demonstrates how to perform key operations on HEC-RAS plan files, focusing on modifying simulation parameters.

**Key contents:**
- Understanding plan files in HEC-RAS
- Retrieving specific values from plan files
- Updating run flags to control which components will run
- Modifying computation and output time intervals
- Reading and updating plan descriptions
- Changing simulation start and end dates
- Verifying updated plan values
- Best practices for plan operations
- Automating parameter adjustments for sensitivity analysis
- Managing documentation through plan descriptions

### [10_1d_hdf_data_extraction.ipynb](10_1d_hdf_data_extraction.ipynb)

This notebook demonstrates how to extract and analyze 1D data from HEC-RAS HDF files using the RAS Commander library.

**Key contents:**
- Accessing and extracting base geometry attributes from HDF files
- Working with 1D cross-section data, including station-elevation profiles
- Visualizing cross-section properties like Manning's n values
- Extracting river centerlines, bank lines, and edge lines
- Analyzing runtime data and compute messages
- Processing and visualizing ineffective flow areas
- Extracting time series data for 1D cross sections
- Plotting cross-section elevation profiles with bank stations

### [11_2d_hdf_data_extraction.ipynb](11_2d_hdf_data_extraction.ipynb)

This notebook shows how to extract and analyze 2D data from HEC-RAS HDF files using the RAS Commander library.

**Key contents:**
- Working with 2D flow area attributes and perimeter polygons
- Extracting and visualizing mesh cell faces, polygons, and points
- Finding nearest faces and cells to specific points
- Extracting boundary condition lines and breaklines
- Analyzing maximum water surface elevations and timing
- Working with maximum face velocities and water surface errors
- Visualizing 2D model results with terrain data
- Extracting and interpreting cell and face time series data

### [12_2d_hdf_data_extraction_pipes_and_pumps.ipynb](12_2d_hdf_data_extraction_pipes_and_pumps.ipynb)

This notebook focuses on extracting and analyzing data related to pipes, conduits, and pump stations from HEC-RAS HDF files.

**Key contents:**
- Working with pipe conduits and associated geometries
- Extracting pipe node information and properties
- Analyzing pipe network connectivity and structures
- Visualizing pipe networks with node elevations
- Working with pump stations and pump groups
- Extracting pipe and node time series data
- Analyzing face flow, velocity, and water surface values
- Processing and visualizing pump station operation data

### [13_2d_detail_face_data_extraction.ipynb](13_2d_detail_face_data_extraction.ipynb)

This notebook demonstrates techniques for detailed face data extraction from 2D HEC-RAS models.

**Key contents:**
- Extracting and analyzing detailed face property tables
- Working with profile lines to identify cell faces
- Finding faces perpendicular to flow for discharge calculations
- Converting face velocities and flows to positive values
- Calculating discharge-weighted velocities for profile lines
- Comparing discharge-weighted and simple average velocities
- Visualizing time series data for selected faces
- Creating profile-specific result datasets for analysis

### [14_fluvial_pluvial_delineation.ipynb](14_fluvial_pluvial_delineation.ipynb)

This notebook demonstrates how to delineate fluvial and pluvial flooding areas based on the timing of maximum water surface elevations.

**Key contents:**
- Extracting maximum water surface elevation and timing data
- Identifying adjacent cells with dissimilar flood timing
- Calculating boundaries between fluvial and pluvial flooding
- Filtering boundaries based on length thresholds
- Visualizing the fluvial-pluvial boundary on a map
- Exporting boundaries to GeoJSON format
- Understanding the difference between river-driven and rainfall-driven flooding
- Using cell polygon geometry for spatial analysis

### [101_Core_Sensitivity.ipynb](101_Core_Sensitivity.ipynb)

This notebook tests HEC-RAS performance with different CPU core configurations to optimize computational efficiency.

**Key contents:**
- Setting up a controlled testing environment
- Running the same plan with varying core counts
- Measuring execution time for each configuration
- Analyzing performance scaling with increased cores
- Creating visualization of performance metrics
- Calculating unit runtime based on single-core performance
- Understanding diminishing returns with multiple cores
- Identifying optimal core count for specific models

### [102_benchmarking_versions_6.1_to_6.6.ipynb](102_benchmarking_versions_6.1_to_6.6.ipynb)

This notebook compares performance across different versions of HEC-RAS by running the same plan across multiple software versions.

**Key contents:**
- Running the same model across multiple HEC-RAS versions
- Measuring preprocessing, computation, and postprocessing times
- Analyzing volume error changes between versions
- Creating visualizations of performance trends
- Identifying performance improvements between versions
- Understanding version-specific computational differences
- Setting up flexible testing environments for multiple versions
- Interpreting HEC-RAS version performance evolution

### [103_Generating_AEP_Events_from_Atlas_14.ipynb](103_Generating_AEP_Events_from_Atlas_14.ipynb)

This notebook demonstrates an end-to-end workflow for generating and analyzing multiple Annual Exceedance Probability events.

**Key contents:**
- Generating hyetographs from NOAA Atlas 14 precipitation frequency data
- Parsing duration strings and interpolating precipitation depths
- Applying the Alternating Block Method for hyetograph creation
- Cloning and configuring HEC-RAS plans for different AEP events
- Executing multiple plans in parallel with resource optimization
- Extracting and visualizing results for multiple AEP scenarios
- Creating a complete workflow from data to flood analysis
- Comparing results across different return period events


## Contributing

If you have suggestions for additional examples or improvements to existing ones, please feel free to contribute by submitting pull requests or opening issues in the repository.
==================================================

