File: c:\GH\ras-commander\.cursorrules
==================================================
# RAS Commander (ras-commander) Coding Assistant

## Overview

This Assistant helps you write efficient Python code for HEC-RAS projects using the RAS Commander library. It automates tasks, provides a Pythonic interface, supports flexible execution modes, and offers built-in examples.

**Core Concepts:** RAS Objects, Project Initialization, File Handling (pathlib.Path), Data Management (Pandas), Execution Modes, Utility Functions.

## Classes, Functions and Arguments




Certainly! I'll summarize the decorators, provide tables for each class showing the decorators used and arguments, and give a summary of each class's function.

Decorator Summaries:

1. @log_call: Logs function calls, including entry and exit times, and any exceptions raised.
2. @standardize_input: Standardizes input for HDF file operations, handling different input types and ensuring consistent file paths.
3. @hdf_operation: Handles opening and closing of HDF files, and manages error handling for HDF operations.

Now, lets go through each class:


1. RasPrj Class:

| Function Name | @log_call | @standardize_input | @hdf_operation | Arguments |
|---------------|-----------|--------------------|--------------------|-----------|
| initialize | X | | | project_folder, ras_exe_path |
| _load_project_data | X | | | |
| _get_geom_file_for_plan | X | | | plan_number |
| _parse_plan_file | X | | | plan_file_path |
| _get_prj_entries | X | | | entry_type |
| _parse_unsteady_file | X | | | unsteady_file_path |
| check_initialized | X | | | |
| find_ras_prj | X | | | folder_path |
| get_project_name | X | | | |
| get_prj_entries | X | | | entry_type |
| get_plan_entries | X | | | |
| get_flow_entries | X |
1. RasPrj Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| initialize | X | | project_folder, ras_exe_path |
| _load_project_data | X | | |
| _get_geom_file_for_plan | X | | plan_number |
| _parse_plan_file | X | | plan_file_path |
| _get_prj_entries | X | | entry_type |
| _parse_unsteady_file | X | | unsteady_file_path |
| check_initialized | X | | |
| find_ras_prj | X | | folder_path |
| get_project_name | X | | |
| get_prj_entries | X | | entry_type |
| get_plan_entries | X | | |
| get_flow_entries | X | | |
| get_unsteady_entries | X | | |
| get_geom_entries | X | | |
| get_hdf_entries | X | | |
| print_data | X | | |
| get_plan_value | X | X | plan_number_or_path, key, ras_object |
| get_boundary_conditions | X | | |
| _parse_boundary_condition | X | | block, unsteady_number, bc_number |

2. RasPlan Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| set_geom | X | | plan_number, new_geom, ras_object |
| set_steady | X | | plan_number, new_steady_flow_number, ras_object |
| set_unsteady | X | | plan_number, new_unsteady_flow_number, ras_object |
| set_num_cores | X | | plan_number, num_cores, ras_object |
| set_geom_preprocessor | X | | file_path, run_htab, use_ib_tables, ras_object |
| get_results_path | X | X | plan_number, ras_object |
| get_plan_path | X | X | plan_number, ras_object |
| get_flow_path | X | X | flow_number, ras_object |
| get_unsteady_path | X | X | unsteady_number, ras_object |
| get_geom_path | X | X | geom_number, ras_object |
| clone_plan | X | | template_plan, new_plan_shortid, ras_object |
| clone_unsteady | X | | template_unsteady, ras_object |
| clone_steady | X | | template_flow, ras_object |
| clone_geom | X | | template_geom, ras_object |
| get_next_number | X | | existing_numbers |
| get_plan_value | X | X | plan_number_or_path, key, ras_object |
| update_plan_value | X | X | plan_number_or_path, key, value, ras_object |

3. RasGeo Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| clear_geompre_files | X | | plan_files, ras_object |

4. RasUnsteady Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| update_unsteady_parameters | X | | unsteady_file, modifications, ras_object |

5. RasCmdr Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| compute_plan | X | | plan_number, dest_folder, ras_object, clear_geompre, num_cores, overwrite_dest |
| compute_parallel | X | | plan_number, max_workers, num_cores, clear_geompre, ras_object, dest_folder, overwrite_dest |
| compute_test_mode | X | | plan_number, dest_folder_suffix, clear_geompre, num_cores, ras_object, overwrite_dest |

6. RasUtils Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| create_directory | X | | directory_path, ras_object |
| find_files_by_extension | X | | extension, ras_object |
| get_file_size | X | | file_path, ras_object |
| get_file_modification_time | X | | file_path, ras_object |
| get_plan_path | X | | current_plan_number_or_path, ras_object |
| remove_with_retry | X | | path, max_attempts, initial_delay, is_folder, ras_object |
| update_plan_file | X | | plan_number_or_path, file_type, entry_number, ras_object |
| check_file_access | X | | file_path, mode |
| convert_to_dataframe | X | | data_source, **kwargs |
| save_to_excel | X | | dataframe, excel_path, **kwargs |
| calculate_rmse | X | | observed_values, predicted_values, normalized |
| calculate_percent_bias | X | | observed_values, predicted_values, as_percentage |
| calculate_error_metrics | X | | observed_values, predicted_values |
| update_file | X | | file_path, update_function, *args |
| get_next_number | X | | existing_numbers |
| clone_file | X | | template_path, new_path, update_function, *args |
| update_project_file | X | | prj_file, file_type, new_num, ras_object |
| decode_byte_strings | X | | dataframe |
| perform_kdtree_query | X | | reference_points, query_points, max_distance |
| find_nearest_neighbors | X | | points, max_distance |
| consolidate_dataframe | X | | dataframe, group_by, pivot_columns, level, n_dimensional, aggregation_method |
| find_nearest_value | X | | array, target_value |
| horizontal_distance | X | | coord1, coord2 |

7. HdfBase Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| _get_simulation_start_time | | | hdf_file |
| _get_unsteady_datetimes | | | hdf_file |
| _get_2d_flow_area_names_and_counts | | | hdf_file |
| _parse_ras_datetime | | | datetime_str |
| _parse_ras_simulation_window_datetime | | | datetime_str |
| _parse_duration | | | duration_str |
| _parse_ras_datetime_ms | | | datetime_str |
| _convert_ras_hdf_string | | | value |

8. HdfBndry Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| bc_lines | | X (plan_hdf) | hdf_path |
| breaklines | | X (plan_hdf) | hdf_path |
| refinement_regions | | X (plan_hdf) | hdf_path |
| reference_lines_names | | X (plan_hdf) | hdf_path, mesh_name |
| reference_points_names | | X (plan_hdf) | hdf_path, mesh_name |
| reference_lines | | X (plan_hdf) | hdf_path |
| reference_points | | X (plan_hdf) | hdf_path |
| get_boundary_attributes | | X (plan_hdf) | hdf_path, boundary_type |
| get_boundary_count | | X (plan_hdf) | hdf_path, boundary_type |
| get_boundary_names | | X (plan_hdf) | hdf_path, boundary_type |

9. HdfMesh Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| mesh_area_names | | X (plan_hdf) | hdf_path |
| mesh_areas | | X (geom_hdf) | hdf_path |
| mesh_cell_polygons | | X (geom_hdf) | hdf_path |
| mesh_cell_points | | X (plan_hdf) | hdf_path |
| mesh_cell_faces | | X (plan_hdf) | hdf_path |
| get_geom_2d_flow_area_attrs | | X (geom_hdf) | hdf_path |

10. HdfPlan Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| get_simulation_start_time | X | X (plan_hdf) | hdf_path |
| get_simulation_end_time | X | X (plan_hdf) | hdf_path |
| get_unsteady_datetimes | X | X (plan_hdf) | hdf_path |
| get_plan_info_attrs | X | X (plan_hdf) | hdf_path |
| get_plan_param_attrs | X | X (plan_hdf) | hdf_path |
| get_meteorology_precip_attrs | X | X (plan_hdf) | hdf_path |
| get_geom_attrs | X | X (plan_hdf) | hdf_path |

11. HdfResultsMesh Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| mesh_summary_output | X | X (plan_hdf) | hdf_path, var, round_to |
| mesh_timeseries_output | X | X (plan_hdf) | hdf_path, mesh_name, var, truncate |
| mesh_faces_timeseries_output | X | X (plan_hdf) | hdf_path, mesh_name |
| mesh_cells_timeseries_output | X | X (plan_hdf) | hdf_path, mesh_names, var, truncate, ras_object |
| mesh_last_iter | X | X (plan_hdf) | hdf_path |
| mesh_max_ws | X | X (plan_hdf) | hdf_path, round_to |
| mesh_min_ws | X | X (plan_hdf) | hdf_path, round_to |
| mesh_max_face_v | X | X (plan_hdf) | hdf_path, round_to |
| mesh_min_face_v | X | X (plan_hdf) | hdf_path, round_to |
| mesh_max_ws_err | X | X (plan_hdf) | hdf_path, round_to |
| mesh_max_iter | X | X (plan_hdf) | hdf_path, round_to |

12. HdfResultsPlan Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| get_results_unsteady_attrs | X | X (plan_hdf) | hdf_path |
| get_results_unsteady_summary_attrs | X | X (plan_hdf) | hdf_path |
| get_results_volume_accounting_attrs | X | X (plan_hdf) | hdf_path |
| get_runtime_data | | X (plan_hdf) | hdf_path |
| reference_timeseries_output | X | X (plan_hdf) | hdf_path, reftype |
| reference_lines_timeseries_output | X | X (plan_hdf) | hdf_path |
| reference_points_timeseries_output | X | X (plan_hdf) | hdf_path |
| reference_summary_output | X | X (plan_hdf) | hdf_path, reftype |

13. HdfResultsXsec Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| steady_profile_xs_output | | X (plan_hdf) | hdf_path, var, round_to |
| cross_sections_wsel | | X (plan_hdf) | hdf_path |
| cross_sections_flow | | X (plan_hdf) | hdf_path |
| cross_sections_energy_grade | | X (plan_hdf) | hdf_path |
| cross_sections_additional_enc_station_left | | X (plan_hdf) | hdf_path |
| cross_sections_additional_enc_station_right | | X (plan_hdf) | hdf_path |
| cross_sections_additional_area_total | | X (plan_hdf) | hdf_path |
| cross_sections_additional_velocity_total | | X (plan_hdf) | hdf_path |

14. HdfStruc Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| structures | X | X (geom_hdf) | hdf_path, datetime_to_str |
| get_geom_structures_attrs | X | X (geom_hdf) | hdf_path |

15. HdfUtils Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| get_hdf_filename | | X (plan_hdf) | hdf_input, ras_object |
| get_root_attrs | | X (plan_hdf) | hdf_path |
| get_attrs | | X (plan_hdf) | hdf_path, attr_path |
| get_hdf_paths_with_properties | | X (plan_hdf) | hdf_path |
| get_group_attributes_as_df | | X (plan_hdf) | hdf_path, group_path |
| get_2d_flow_area_names_and_counts | | X (plan_hdf) | hdf_path |
| projection | | X (plan_hdf) | hdf_path |

16. HdfXsec Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| cross_sections | X | X (geom_hdf) | hdf_path, datetime_to_str |
| cross_sections_elevations | X | X (geom_hdf) | hdf_path, round_to |
| river_reaches | X | X (geom_hdf) | hdf_path, datetime_to_str |

17. RasExamples Class:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| __init__ | X | | |
| get_example_projects | X | | version_number |
| _load_project_data | X | | |
| _find_zip_file | X | | |
| _extract_folder_structure | X | | |
| _save_to_csv | X | | |
| list_categories | X | | |
| list_projects | X | | category |
| extract_project | X | | project_names |
| is_project_extracted | X | | project_name |
| clean_projects_directory | X | | |
| download_fema_ble_model | X | | huc8, output_dir |
| _make_safe_folder_name | X | | name |
| _download_file_with_progress | X | | url, dest_folder, file_size |
| _convert_size_to_bytes | X | | size_str |

18. RasGpt Class:

This class is mentioned in the code but has no implemented methods yet.

19. Standalone functions:

| Function Name | @log_call | @standardize_input | Arguments |
|---------------|-----------|--------------------|--------------------|
| init_ras_project | X | | ras_project_folder, ras_version, ras_instance |
| get_ras_exe | X | | ras_version |




Overall, the ras-commander library provides a comprehensive set of tools for working with HEC-RAS projects, including project management, file operations, data extraction, and simulation execution. The library makes extensive use of logging and input standardization through decorators, ensuring consistent behavior and traceability across its various components.


## Coding Assistance Rules:

1. Use default libraries, especially pathlib for file operations.
2. Use r-strings for paths, f-strings for formatting.
3. Always use pathlib over os for file/directory operations.
4. Include comments and use logging for output.
5. Follow PEP 8 conventions.
6. Provide clear error handling and user feedback.
7. Explain RAS Commander function purposes and key arguments.
8. Use either global 'ras' object or custom instances consistently.
9. Highlight parallel execution best practices.
10. Suggest RasExamples for testing when appropriate.
11. Utilize RasHdf for HDF file operations and data extraction.
12. Use type hints for function arguments and return values.
13. Apply the @log_call decorator for automatic function logging.
14. Emphasize proper error handling and logging in all functions.
15. When working with RasHdfGeom, always use the @standardize_input decorator for methods that interact with HDF files.
16. Remember that RasHdfGeom methods often return GeoDataFrames, which combine geometric data with attribute information.
17. When dealing with cross-sections or river reaches, consider using the datetime_to_str parameter to convert datetime objects to strings if needed.
18. For methods that accept a mesh_name parameter, remember that they can return either a dictionary of lists or a single list depending on whether a specific mesh is specified.
19. Use 'union_all()' for geodataframes. For pandas >= 2.0, use pd.concat instead of append.
20. Provide full code segments or scripts with no elides.
21. When importing from the Decorators module, use:
    ```python
    from .Decorators import standardize_input, log_call
    ```
22. When importing from the LoggingConfig module, use:
    ```python
    from .LoggingConfig import setup_logging, get_logger
    ```
23. Be aware that while the code will work with capitalized module names (Decorators.py and LoggingConfig.py), it's generally recommended to stick to lowercase names for modules as per PEP 8.
24. When revising code, label planning steps as:
    ## Explicit Planning and Reasoning for Revisions

25. Always consider the implications of file renaming on import statements throughout the project.
26. When working with GeoDataFrames, remember to use appropriate geometric operations and consider spatial relationships.
27. For HDF file operations, always use the standardize_input decorator to ensure consistent handling of file paths.
28. When dealing with large datasets, consider using chunking or iterative processing to manage memory usage.
29. Utilize the RasExamples class for testing and demonstrating functionality with sample projects.
30. When working with the RasGpt class, be aware that it's mentioned but currently has no implemented methods.
==================================================

Folder: c:\GH\ras-commander\.gitignore
==================================================

Folder: c:\GH\ras-commander\examples
==================================================

File: c:\GH\ras-commander\LICENSE
==================================================
MIT License

Copyright (c) 2024 William M. Katzenmeyer

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so.

==================================================

File: c:\GH\ras-commander\pyproject.toml
==================================================
[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta:__legacy__"

==================================================

Folder: c:\GH\ras-commander\ras_commander
==================================================

File: c:\GH\ras-commander\requirements.txt
==================================================
aider-chat @ git+https://github.com/paul-gauthier/aider.git@00d5348ee6295662c78a8ece31d71632145d9746
alabaster==0.7.16
annotated-types==0.6.0
anyio==3.7.1
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work
attrs==23.1.0
babel==2.16.0
backoff==2.2.1
backports.tarfile==1.2.0
black==24.8.0
boto3==1.35.25
botocore==1.35.25
certifi==2023.11.17
cffi==1.16.0
charset-normalizer==3.3.2
click==8.1.7
colorama==0.4.6
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1710320294760/work
ConfigArgParse==1.7
contourpy==1.3.0
cycler==0.12.1
debugpy @ file:///D:/bld/debugpy_1725269345345/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
diff-match-patch==20230430
diskcache==5.6.3
distro==1.8.0
docutils==0.20.1
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1720869315914/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1725214404607/work
flake8==7.1.1
fonttools==4.53.1
geopandas==1.0.1
gitdb==4.0.11
GitPython==3.1.40
grep-ast==0.2.4
h11==0.14.0
h5py==3.11.0
httpcore==1.0.2
idna==3.6
imagesize==1.4.1
importlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1726082825846/work
iniconfig==2.0.0
ipykernel @ file:///D:/bld/ipykernel_1719845595208/work
ipython @ file:///D:/bld/ipython_1725050320818/work
jaraco.classes==3.4.0
jaraco.context==6.0.1
jaraco.functools==4.1.0
jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work
Jinja2==3.1.4
jmespath==1.0.1
jsonschema==4.20.0
jsonschema-specifications==2023.11.2
jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1726610684920/work
jupyter_core @ file:///D:/bld/jupyter_core_1710257313664/work
keyring==25.4.1
kiwisolver==1.4.7
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.9.2
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1713250518406/work
mccabe==0.7.0
mdurl==0.1.2
more-itertools==10.5.0
mypy-extensions==1.0.0
nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1705850609492/work
networkx==3.2.1
nh3==0.2.18
numpy==1.26.2
packaging==23.2
pandas==2.2.3
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1712320355065/work
pathlib==1.0.1
pathspec==0.11.2
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pillow==10.4.0
pkginfo==1.10.0
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1726613481435/work
pluggy==1.5.0
prompt-toolkit==3.0.41
psutil @ file:///D:/bld/psutil_1725737996000/work
pure_eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1721585709575/work
pycodestyle==2.12.1
pycparser==2.21
pydantic==2.5.2
pydantic_core==2.14.5
pyflakes==3.2.0
Pygments==2.17.2
pyogrio==0.9.0
pyparsing==3.1.4
pyproj==3.6.1
pytest==8.3.3
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work
pytz==2024.2
pywin32==306
pywin32-ctypes==0.2.3
PyYAML==6.0.1
pyzmq @ file:///D:/bld/pyzmq_1725449086441/work
readme_renderer==43.0
referencing==0.31.1
regex==2023.10.3
requests==2.32.3
requests-toolbelt==1.0.0
rfc3986==2.0.0
rich==13.7.0
rpds-py==0.13.2
s3transfer==0.10.2
scipy==1.11.4
shapely==2.0.6
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
smmap==5.0.1
sniffio==1.3.0
snowballstemmer==2.2.0
sounddevice==0.4.6
soundfile==0.12.1
Sphinx==7.4.7
sphinx-rtd-theme==2.0.0
sphinxcontrib-applehelp==2.0.0
sphinxcontrib-devhelp==2.0.0
sphinxcontrib-htmlhelp==2.1.0
sphinxcontrib-jquery==4.1
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==2.0.0
sphinxcontrib-serializinghtml==2.0.0
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
tornado @ file:///D:/bld/tornado_1724956185692/work
tqdm==4.66.1
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1713535121073/work
tree-sitter==0.20.4
tree-sitter-languages==1.8.0
twine==5.1.1
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1717802530399/work
tzdata==2024.1
urllib3==2.2.3
wcwidth==0.2.12
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1726248574750/work

==================================================

File: c:\GH\ras-commander\settings.db
==================================================
SQLite format 3   @                                                                     .v  n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            N)eindexix_settings_idsettingsCREATE INDEX ix_settings_id ON settings (id)VtablesettingssettingsCREATE TABLE settings (
	id VARCHAR NOT NULL, 
	anthropic_api_key TEXT, 
	openai_api_key TEXT, 
	selected_model VARCHAR, 
	context_mode VARCHAR, 
	omit_folders TEXT, 
	omit_extensions TEXT, 
	omit_files TEXT, 
	chunk_level VARCHAR, 
	initial_chunk_size INTEGER, 
	followup_chunk_size INTEGER, 
	PRIMARY KEY (id)
)/C indexsqlite_autoindex_settings_1settings                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
==================================================

File: c:\GH\ras-commander\setup.py
==================================================
from setuptools import setup, find_packages
from setuptools.command.build_py import build_py
import subprocess
from pathlib import Path

class CustomBuildPy(build_py):
    def run(self):
        # Clean up __pycache__ folders
        root_dir = Path(__file__).parent
        for pycache_dir in root_dir.rglob('__pycache__'):
            if pycache_dir.is_dir():
                for cache_file in pycache_dir.iterdir():
                    cache_file.unlink()  # Delete each file
                pycache_dir.rmdir()      # Delete the empty directory
                print(f"Cleaned up: {pycache_dir}")

        # Run the summary_knowledge_bases.py script
        script_path = Path(__file__).parent / 'ai_tools' / 'generate_llm_knowledge_bases.py'
        try:
            subprocess.run(['python', str(script_path)], check=True)
        except subprocess.CalledProcessError:
            print("Warning: Knowledge base generation script failed, continuing with build")
        except FileNotFoundError:
            print("Warning: Knowledge base generation script not found, continuing with build")
        
        # Continue with the regular build process
        super().run()

setup(
    name="ras-commander",
    version="0.58.0",
    packages=find_packages(),
    include_package_data=True,
    python_requires='>=3.10',
    author="William M. Katzenmeyer, P.E., C.F.M.",
    author_email="heccommander@gmail.com",
    description="A Python library for automating HEC-RAS 6.x operations",
    long_description=open('README.md').read(),
    long_description_content_type="text/markdown",
    url="https://github.com/gpt-cmdr/ras-commander",
    cmdclass={
        'build_py': CustomBuildPy,
    },
    install_requires=[
        'h5py',
        'numpy',
        'pandas',
        'requests',
        'tqdm',
        'scipy',
        'xarray',
        'geopandas',
        'matplotlib',
        'shapely',
        'pathlib',
        'rasterstats',
        'rtree',
    ])

"""
ras-commander setup.py

This file is used to build and publish the ras-commander package to PyPI.

To build and publish this package, follow these steps:

1. Ensure you have the latest versions of setuptools, wheel, and twine installed:
   pip install --upgrade setuptools wheel twine

2. Update the version number in ras_commander/__init__.py (if not using automatic versioning)

3. Create source distribution and wheel:
   python setup.py sdist bdist_wheel

4. Check the distribution:
   twine check dist/*

5. Upload to Test PyPI (optional):
   twine upload --repository testpypi dist/*

6. Install from Test PyPI to verify (optional):
   pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple ras-commander

7. Upload to PyPI:
   twine upload dist/* --username __token__ --password <your_api_key>


8. Install from PyPI to verify:
   pip install ras-commander

Note: Ensure you have the necessary credentials and access rights to upload to PyPI.
For more information, visit: https://packaging.python.org/tutorials/packaging-projects/

"""

==================================================

File: c:\GH\ras-commander\.gitignore\.gitignore
==================================================
# Ignore the example_projects folder and all its subfolders
examples/example_projects/

# Ignore workspace, projects, and my_projects folders
workspace/
projects/
my_projects/

# Ignore FEMA BLE Models
examples/FEMA_BLE_Models/
examples/hdf_example_data/

# ignore tools/stored_map_assistant build folders
tools/stored_map_assistant/build/
tools/stored_map_assistant/dist/

# Ignore library assistant config
library_assistant/config/

# Ignore Python egg info
*.egg-info/
.eggs/

# Ignore the Example_Projects_6_5.zip file
Example_Projects_6_5.zip

# Ignore the misc folder and all its subfolders
misc/

# Ignore Python cache files
__pycache__/
*.py[cod]

# Ignore compiled Python files
*.so

# Ignore distribution / packaging
dist/
build/

# Ignore test cache
.pytest_cache/

# Ignore virtual environments
.venv/
venv/

# Ignore IDE-specific files (optional, uncomment if needed)
# .vscode/
# .idea/

# Ignore OS-specific files
.DS_Store
Thumbs.db
==================================================

File: c:\GH\ras-commander\examples\00_Using_RasExamples.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install RAS-Commander from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imports (if using the pip package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ras_commander import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible Imports (for active development of the library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will try to import the pip package, if it fails it will \n",
    "# add the parent directory to the Python path and try to import again\n",
    "# This assumes you are working in a subfolder of the ras-commander repository\n",
    "# This allows a user's revisions to be tested locally without installing the package\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Flexible imports to allow for development without installation \n",
    "#  ** Use this version with Jupyter Notebooks **\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    from ras_commander import *\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    import os\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    parent_directory = current_file.parent\n",
    "    sys.path.append(str(parent_directory))\n",
    "    \n",
    "    # Now try to import again\n",
    "    from ras_commander import *\n",
    "print(\"ras_commander imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RASExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Method for Calling HEC-RAS Example Projects by Folder Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Code Cell is All You Need\n",
    "# This is what this Class was intended to do: Help me make repeatable workflows around HEC-RAS Example Projects for testing and demonstration purposes. \n",
    "\n",
    "# Extract specific projects\n",
    "RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\", \"Davis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasExamples will not download a new .zip file if one already exists, this allows you to replace the Example_Projects_6_x.zip with your own zip file (with the same folder format as the HEC-RAS examples) and you will be able to load them by folder name for repeatable Test Driven Development\n",
    "\n",
    "Just make sure all project folders have unique folder names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if example projects are already downloaded\n",
    "if RasExamples.projects_dir.exists():\n",
    "    print(\"Example projects are already downloaded.\")\n",
    "    print(\"RasExamples.folder_df:\")\n",
    "    display(RasExamples.folder_df)\n",
    "else:\n",
    "    print(\"Downloading example projects...\")\n",
    "    RasExamples.get_example_projects()\n",
    "    print(\"RasExamples.folder_df:\")\n",
    "    display(RasExamples.folder_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all categories\n",
    "categories = RasExamples.list_categories()\n",
    "print(\"\\nAvailable categories:\")\n",
    "for category in categories:\n",
    "    print(f\"- {category}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List projects in a specific category\n",
    "category = \"1D Unsteady Flow Hydraulics\"\n",
    "projects = RasExamples.list_projects(category)\n",
    "print(f\"\\nProjects in '{category}':\")\n",
    "for project in projects:\n",
    "    print(f\"- {project}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all projects\n",
    "all_projects = RasExamples.list_projects()\n",
    "print(\"\\nAll available projects:\")\n",
    "for project in all_projects:\n",
    "    print(f\"- {project}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects\n",
    "projects_to_extract = [\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"]\n",
    "extracted_paths = RasExamples.extract_project(projects_to_extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about New Pipes and Conduits Version 6.6 Example Project\n",
    "\n",
    "Use project name \"Davis\" to explore pipes and conduits (introduced in version 6.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\01_project_initialization.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander Project Initialization\n",
    "\n",
    "This notebook demonstrates how to initialize and work with HEC-RAS projects using the `ras-commander` library. You'll learn how to:\n",
    "\n",
    "1. Set up and configure the RAS Commander environment\n",
    "2. Download and extract example HEC-RAS projects\n",
    "3. Initialize HEC-RAS projects using the global `ras` object\n",
    "4. Initialize multiple HEC-RAS projects using custom RAS objects\n",
    "5. Access various project components (plans, geometries, flows, boundaries)\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **RasPrj Objects**: Represent HEC-RAS projects with access to plans, geometries, flows, etc.\n",
    "- **Global `ras` object**: A singleton instance for simple, single-project scripts\n",
    "- **Custom RAS Objects**: Independent instances for multi-project workflows\n",
    "- **Project Initialization**: Process of connecting to HEC-RAS projects\n",
    "- **Project Components**: Structured access to plans, geometries, and flow files\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Import required Libraries and install if missing\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def install_module(module_name):\n",
    "    try:\n",
    "        if module_name.lower() == 'ipython':\n",
    "            # Try importing IPython instead of ipython\n",
    "            __import__('IPython') \n",
    "        else:\n",
    "            __import__(module_name)\n",
    "    except ImportError:\n",
    "        print(f\"{module_name} not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", module_name])\n",
    "\n",
    "# List of modules to check and install if necessary\n",
    "modules = [\n",
    "    'h5py', 'numpy', 'pandas', 'requests', 'tqdm', 'scipy', 'xarray',\n",
    "    'geopandas', 'matplotlib', 'ipython', 'tqdm', 'psutil', 'shapely', \n",
    "    'fiona', 'pathlib', 'rtree', 'rasterstats'\n",
    "]\n",
    "for module in modules:\n",
    "    install_module(module)\n",
    "\n",
    "# Import the required libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import rtree\n",
    "import rasterstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Let's define our working directory and ensure we're in a consistent environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAS Commander: Core Concepts\n",
    "\n",
    "RAS Commander is a Python library that provides tools for automating HEC-RAS tasks. It's built with several key design principles:\n",
    "\n",
    "1. **Project-Centric Architecture**: Everything revolves around HEC-RAS projects\n",
    "2. **Two RAS Object Approaches**:\n",
    "   - **Global `ras` Object**: A singleton for simple scripts\n",
    "   - **Custom RAS Objects**: Multiple ras project instances for complex workflows\n",
    "3. **Comprehensive Project Representation**: Each RAS object includes DataFrames for plans, geometries, flows, and boundaries\n",
    "4. **Logging**: Built-in logging to track operations and debug issues\n",
    "5. **HDF Support**: Specialized functions for HDF file access (plan results, geometry, etc.)\n",
    "\n",
    "Let's explore these concepts in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Example HEC-RAS Projects\n",
    "\n",
    "RAS Commander includes a utility to download and extract example HEC-RAS projects. These are useful for learning and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects we'll use in this tutorial\n",
    "# This will download them if not present and extract them to the example_projects folder\n",
    "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
    "print(extracted_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Paths for Extracted Example Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the first extracted path as our examples directory\n",
    "examples_dir = extracted_paths[0].parent\n",
    "print(f\"Examples directory: {examples_dir}\")\n",
    "\n",
    "\n",
    "# Define paths to the extracted projects\n",
    "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
    "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
    "muncie_path = examples_dir / \"Muncie\"\n",
    "\n",
    "# Verify the paths exist\n",
    "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
    "    print(f\"Path {path} exists: {path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function to Print RAS Object Data\n",
    "\n",
    "Let's create a utility function to help us explore the contents of RAS objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ras_object_data(ras_obj, project_name):\n",
    "    \"\"\"Prints comprehensive information about a RAS object\"\"\"\n",
    "    print(f\"\\n{project_name} Data:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Project Name: {ras_obj.get_project_name()}\")\n",
    "    print(f\"Project Folder: {ras_obj.project_folder}\")\n",
    "    print(f\"PRJ File: {ras_obj.prj_file}\")\n",
    "    print(f\"HEC-RAS Executable Path: {ras_obj.ras_exe_path}\")\n",
    "    \n",
    "    print(\"\\nPlan Files DataFrame:\")\n",
    "    display.display(ras_obj.plan_df)\n",
    "    \n",
    "    print(\"\\nFlow Files DataFrame:\")\n",
    "    display.display(ras_obj.flow_df)\n",
    "    \n",
    "    print(\"\\nUnsteady Flow Files DataFrame:\")\n",
    "    display.display(ras_obj.unsteady_df)\n",
    "    \n",
    "    print(\"\\nGeometry Files DataFrame:\")\n",
    "    display.display(ras_obj.geom_df)\n",
    "    \n",
    "    print(\"\\nHDF Entries DataFrame:\")\n",
    "    display.display(ras_obj.get_hdf_entries())\n",
    "    \n",
    "    print(\"\\nBoundary Conditions DataFrame:\")\n",
    "    display.display(ras_obj.boundaries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using the Global `ras` Object\n",
    "\n",
    "The global `ras` object is a singleton instance that persists throughout your script. It's ideal for simple scripts working with a single project.\n",
    "\n",
    "Key characteristics:\n",
    "- It's available as `ras` immediately after import\n",
    "- It's initialized via `init_ras_project()` without saving the return value\n",
    "- It provides access to all project data through the global `ras` variable\n",
    "- It's simple to use but can be problematic in complex scenarios\n",
    "\n",
    "Let's initialize it with the Bald Eagle Creek project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the global ras object with Bald Eagle Creek project\n",
    "# Note: This updates the global 'ras' object visible throughout the script\n",
    "# Parameters:\n",
    "#   - project_folder: Path to the HEC-RAS project folder (required)\n",
    "#   - ras_version: HEC-RAS version (e.g. \"6.5\") or path to Ras.exe (required first time)\n",
    "\n",
    "init_ras_project(bald_eagle_path, \"6.5\")\n",
    "print(f\"The global 'ras' object is now initialized with the {ras.project_name} project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the global ras object with our utility function\n",
    "print_ras_object_data(ras, \"Global RAS Object (Bald Eagle Creek)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the RAS Object Structure\n",
    "\n",
    "Each RAS object contains several important components:\n",
    "\n",
    "1. **Project Metadata**:\n",
    "   - `project_name`: Name of the HEC-RAS project\n",
    "   - `project_folder`: Directory containing project files\n",
    "   - `prj_file`: Path to the main .prj file\n",
    "   - `ras_exe_path`: Path to the HEC-RAS executable\n",
    "\n",
    "2. **Project DataFrames**:\n",
    "   - `plan_df`: Information about all plan files (.p*)\n",
    "   - `flow_df`: Information about all steady flow files (.f*)\n",
    "   - `unsteady_df`: Information about all unsteady flow files (.u*)\n",
    "   - `geom_df`: Information about all geometry files (.g*)\n",
    "   - `boundaries_df`: Information about all boundary conditions\n",
    "\n",
    "3. **Methods for Data Access**:\n",
    "   - `get_plan_entries()`: Get plan file information\n",
    "   - `get_flow_entries()`: Get flow file information\n",
    "   - `get_unsteady_entries()`: Get unsteady flow file information \n",
    "   - `get_geom_entries()`: Get geometry file information\n",
    "   - `get_hdf_entries()`: Get HDF file paths for result files\n",
    "   - `get_boundary_conditions()`: Get boundary condition details\n",
    "\n",
    "Let's see how to access specific information from these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first plan's details\n",
    "if not ras.plan_df.empty:\n",
    "    first_plan = ras.plan_df.iloc[0]\n",
    "    print(f\"First plan number: {first_plan['plan_number']}\")\n",
    "    print(f\"Plan path: {first_plan['full_path']}\")\n",
    "    \n",
    "    # Get the geometry file for this plan\n",
    "    geom_id = first_plan.get('Geom File', '').replace('g', '')\n",
    "    if geom_id:\n",
    "        geom_info = ras.geom_df[ras.geom_df['geom_number'] == geom_id]\n",
    "        if not geom_info.empty:\n",
    "            print(f\"Geometry file: {geom_info.iloc[0]['full_path']}\")\n",
    "    \n",
    "    # Get the HDF results file for this plan (if exists)\n",
    "    if 'HDF_Results_Path' in first_plan and first_plan['HDF_Results_Path']:\n",
    "        print(f\"Results file: {first_plan['HDF_Results_Path']}\")\n",
    "else:\n",
    "    print(\"No plans found in the project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Boundary Conditions\n",
    "\n",
    "Boundary conditions define the inputs and outputs of your model. Let's see how to access boundary condition information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the boundary conditions DataFrame\n",
    "ras.boundaries_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using Custom RAS Objects\n",
    "\n",
    "For more complex scripts or when working with multiple projects, it's better to create and use separate RAS objects. This approach:\n",
    "\n",
    "- Creates independent RAS objects for each project\n",
    "- Avoids overwriting the global `ras` object\n",
    "- Provides clearer separation between projects\n",
    "- Allows working with multiple projects simultaneously\n",
    "- Requires saving the return value from `init_ras_project()`\n",
    "\n",
    "Let's initialize multiple projects with custom RAS objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple project instances with custom RAS objects\n",
    "# Note: This also updates the global 'ras' object each time, but we'll use the custom instances\n",
    "# Parameters remain the same as before\n",
    "\n",
    "multi_2d_project = init_ras_project(multi_2d_path, \"6.5\")\n",
    "print(f\"\\nMulti2D project initialized with its own RAS object\")\n",
    "\n",
    "muncie_project = init_ras_project(muncie_path, \"6.5\")\n",
    "print(f\"\\nMuncie project initialized with its own RAS object\")\n",
    "\n",
    "# Note that the global 'ras' object now points to the Muncie project\n",
    "# The global 'ras' object gets overwritten every time a project is initialized ,\n",
    "print(f\"\\nGlobal 'ras' object now points to: {ras.project_name} since it was the last one initialized.  Avoid the global object when using multiple projects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Multiple Projects\n",
    "\n",
    "Now we have three RAS objects:\n",
    "- `multi_2d_project`: Our custom object for the Multi2D project\n",
    "- `muncie_project`: Our custom object for the Muncie project\n",
    "- `ras`: The global object (which now points to Muncie)\n",
    "\n",
    "Let's examine the Multi2D project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Multi2D project\n",
    "print_ras_object_data(multi_2d_project, \"Multi2D Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Muncie project\n",
    "print_ras_object_data(muncie_project, \"Muncie Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Projects\n",
    "\n",
    "Let's compare some key metrics of the two projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table of the two projects\n",
    "comparison_data = {\n",
    "    'Project Name': [multi_2d_project.project_name, muncie_project.project_name],\n",
    "    'Number of Plans': [len(multi_2d_project.plan_df), len(muncie_project.plan_df)],\n",
    "    'Number of Geometries': [len(multi_2d_project.geom_df), len(muncie_project.geom_df)],\n",
    "    'Number of Flow Files': [len(multi_2d_project.flow_df), len(muncie_project.flow_df)],\n",
    "    'Number of Unsteady Files': [len(multi_2d_project.unsteady_df), len(muncie_project.unsteady_df)],\n",
    "    'Number of Boundary Conditions': [len(multi_2d_project.boundaries_df) if hasattr(multi_2d_project, 'boundaries_df') else 0, \n",
    "                                     len(muncie_project.boundaries_df) if hasattr(muncie_project, 'boundaries_df') else 0],\n",
    "    'HDF Results Available': [len(multi_2d_project.get_hdf_entries()) > 0, len(muncie_project.get_hdf_entries()) > 0]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display.display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAS Commander: Best Practices\n",
    "\n",
    "After exploring both approaches, here are some best practices for using RAS Commander:\n",
    "\n",
    "1. **Choose Your Approach Based on Complexity**:\n",
    "   - **Simple Scripts** (one project): Use the global `ras` object\n",
    "   - **Complex Scripts** (multiple projects): Use custom RAS objects\n",
    "\n",
    "2. **Be Consistent**:\n",
    "   - Don't mix global and custom approaches in the same script\n",
    "   - Use descriptive names for custom RAS objects\n",
    "\n",
    "3. **Working with Project Files**:\n",
    "   - Access project files through the RAS object's DataFrames\n",
    "   - Use helper functions like `get_plan_path()` to resolve paths\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Always check for empty DataFrames before accessing their contents\n",
    "   - Use the built-in logging to track operations\n",
    "\n",
    "5. **Performance Considerations**:\n",
    "   - For large projects, consider using the HDF classes directly\n",
    "   - Cache results of expensive operations when possible\n",
    "\n",
    "## Summary of Key Functions\n",
    "\n",
    "- `init_ras_project(project_folder, ras_version)`: Initialize a RAS project\n",
    "- `RasExamples().extract_project(project_name)`: Extract example projects\n",
    "- `RasPrj.get_project_name()`: Get the name of the project\n",
    "- `RasPrj.get_plan_entries()`: Get plan file information\n",
    "- `RasPrj.get_flow_entries()`: Get flow file information\n",
    "- `RasPrj.get_unsteady_entries()`: Get unsteady flow file information\n",
    "- `RasPrj.get_geom_entries()`: Get geometry file information\n",
    "- `RasPrj.get_hdf_entries()`: Get HDF result file information\n",
    "- `RasPrj.get_boundary_conditions()`: Get boundary condition details\n",
    "- `RasPlan.get_plan_path(plan_number)`: Get the path to a plan file\n",
    "- `RasPlan.get_geom_path(geom_number)`: Get the path to a geometry file\n",
    "- `RasPlan.get_flow_path(flow_number)`: Get the path to a flow file\n",
    "- `RasPlan.get_unsteady_path(unsteady_number)`: Get the path to an unsteady flow file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of project initialization in RAS Commander, you can explore more advanced topics:\n",
    "\n",
    "1. Working with HDF files for result analysis\n",
    "2. Modifying plan, geometry, and flow files\n",
    "3. Running HEC-RAS simulations\n",
    "4. Extracting and visualizing results\n",
    "5. Automating model calibration\n",
    "\n",
    "These topics are covered in other examples and notebooks in the RAS Commander documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\02_plan_and_geometry_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Plan and Geometry Operations\n",
    "\n",
    "This notebook demonstrates how to perform operations on HEC-RAS plan and geometry files using the RAS Commander library. We'll explore how to initialize projects, clone plans and geometries, configure parameters, execute plans, and analyze results.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Plan Operations**:\n",
    "   - Clone an existing plan to create a new one\n",
    "   - Configure simulation parameters and intervals\n",
    "   - Set run flags and update descriptions\n",
    "3. **Geometry Operations**:\n",
    "   - Clone a geometry file to create a modified version\n",
    "   - Set the geometry for a plan\n",
    "   - Clear geometry preprocessor files to ensure clean results\n",
    "4. **Flow Operations**:\n",
    "   - Clone unsteady flow files\n",
    "   - Configure flow parameters\n",
    "5. **Plan Computation**: Run the plan with specified settings\n",
    "6. **Results Verification**: Check HDF entries to confirm results were written\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries for this notebook\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from datetime import datetime  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Flexible imports to allow for development without installation\n",
    "try:\n",
    "    # Try to import from the installed package\n",
    "    print(\"Importing ras-commander modules from pip package, if available\")\n",
    "    from ras_commander import *\n",
    "except ImportError:\n",
    "    # If the import fails, add the parent directory to the Python path\n",
    "    print(\"ras-commander pip package not found. Using local ras-commander library.\")\n",
    "    current_file = Path(os.getcwd()).resolve()\n",
    "    parent_directory = current_file.parent  # Adjust as needed for your directory structure\n",
    "    print(f\"Adding path to sys.path: {parent_directory}\")\n",
    "    sys.path.append(str(parent_directory))\n",
    "    \n",
    "    # Now try to import again\n",
    "    from ras_commander import *\n",
    "\n",
    "print(\"ras_commander imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Projects\n",
    "\n",
    "We'll use the `RasExamples` class to download and extract an example HEC-RAS project. For this notebook, we'll use the \"Balde Eagle Creek\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects we'll use in this tutorial\n",
    "# This will download them if not present and extract them to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(bald_eagle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Initialization\n",
    "\n",
    "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the project folder path and HEC-RAS version as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nHEC-RAS Project Plan Data (plan_df):\")\n",
    "display.display(ras.plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Plan and Geometry Operations in HEC-RAS\n",
    "\n",
    "Before diving into the operations, let's understand what plan and geometry files are in HEC-RAS:\n",
    "\n",
    "- **Plan Files** (`.p*`): Define the simulation parameters including the reference to geometry and flow files, as well as computational settings.\n",
    "- **Geometry Files** (`.g*`): Define the physical characteristics of the river/channel system including cross-sections, 2D areas, and structures.\n",
    "\n",
    "The `RasPlan` and `RasGeo` classes provide methods for working with these files, including:\n",
    "\n",
    "1. Creating new plans and geometries by cloning existing ones\n",
    "2. Modifying simulation parameters and settings\n",
    "3. Associating geometries with plans\n",
    "4. Managing preprocessor files\n",
    "5. Retrieving information from plans and geometries\n",
    "\n",
    "In the following sections, we'll explore these operations in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloning Plans and Geometries\n",
    "\n",
    "Let's start by cloning a plan to create a new simulation scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone plan \"01\" to create a new plan\n",
    "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Combined Test Plan\")\n",
    "print(f\"New plan created: {new_plan_number}\")\n",
    "\n",
    "# Display updated plan files\n",
    "print(\"\\nUpdated plan files:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Get the path to the new plan file\n",
    "plan_path = RasPlan.get_plan_path(new_plan_number)\n",
    "print(f\"\\nNew plan file path: {plan_path}\")\n",
    "\n",
    "# Let's examine the new plan's details\n",
    "new_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
    "print(f\"\\nNew plan details:\")\n",
    "print(f\"Plan number: {new_plan_number}\")\n",
    "print(f\"Description: {new_plan.get('description', 'No description')}\")\n",
    "print(f\"Short Identifier: {new_plan.get('Short Identifier', 'Not available')}\")\n",
    "print(f\"Geometry file: {new_plan.get('Geom File', 'None')}\")\n",
    "print(f\"File path: {new_plan['full_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clone a geometry file. This allows us to make modifications to a geometry without affecting the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone geometry \"01\" to create a new geometry file\n",
    "new_geom_number = RasPlan.clone_geom(\"01\")\n",
    "print(f\"New geometry created: {new_geom_number}\")\n",
    "\n",
    "# Display updated geometry files\n",
    "print(\"\\nUpdated geometry files:\")\n",
    "display.display(ras.geom_df)\n",
    "\n",
    "# Get the path to the new geometry file\n",
    "geom_path = RasPlan.get_geom_path(new_geom_number)\n",
    "print(f\"\\nNew geometry file path: {geom_path}\")\n",
    "\n",
    "# Examine the new geometry's details\n",
    "new_geom = ras.geom_df.loc[ras.geom_df['geom_number'] == new_geom_number].squeeze()\n",
    "print(f\"\\nNew geometry details:\")\n",
    "print(f\"Geometry number: {new_geom_number}\")\n",
    "print(f\"Geometry file: {new_geom.get('geom_file', 'Not available')}\")\n",
    "print(f\"File path: {new_geom.get('full_path', 'Not available')}\")\n",
    "print(f\"HDF path: {new_geom.get('hdf_path', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also clone an unsteady flow file to complete our new simulation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
    "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
    "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
    "\n",
    "# Display updated unsteady flow files\n",
    "print(\"\\nUpdated unsteady flow files:\")\n",
    "display.display(ras.unsteady_df)\n",
    "\n",
    "# Examine the new unsteady flow's details\n",
    "new_unsteady = ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number].iloc[0]\n",
    "print(f\"\\nNew unsteady flow details:\")\n",
    "print(f\"Unsteady number: {new_unsteady_number}\")\n",
    "print(f\"File path: {new_unsteady['full_path']}\")\n",
    "print(f\"Flow Title: {new_unsteady.get('Flow Title', 'Not available')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associating Files and Setting Parameters\n",
    "\n",
    "Now that we have cloned our plan, geometry, and unsteady flow files, we need to associate them with each other and set various parameters.\n",
    "\n",
    "### Setting Geometry for a Plan\n",
    "\n",
    "Let's associate our new geometry with our new plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new geometry for the cloned plan\n",
    "updated_geom_df = RasPlan.set_geom(new_plan_number, new_geom_number)\n",
    "plan_path = RasPlan.get_plan_path(new_plan_number, ras_object=ras)\n",
    "print(f\"Updated geometry for plan {new_plan_number} to geometry {new_geom_number}\")\n",
    "print(f\"Plan file path: {plan_path}\")\n",
    "\n",
    "# Let's verify the change\n",
    "updated_plan = ras.plan_df[ras.plan_df['plan_number'] == new_plan_number].iloc[0]\n",
    "print(f\"\\nVerified that plan {new_plan_number} now uses geometry file: {updated_plan.get('Geom File', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Unsteady Flow for a Plan\n",
    "\n",
    "Similarly, let's associate our new unsteady flow file with our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unsteady flow for the cloned plan\n",
    "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
    "print(f\"Updated unsteady flow for plan {new_plan_number} to unsteady flow {new_unsteady_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing Geometry Preprocessor Files\n",
    "\n",
    "When working with geometry files, it's important to clear the preprocessor files to ensure clean results. These files (with `.c*` extension) contain computed hydraulic properties that should be recomputed when the geometry changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear geometry preprocessor files for the cloned plan\n",
    "RasGeo.clear_geompre_files(plan_path)\n",
    "print(f\"Cleared geometry preprocessor files for plan {new_plan_number}\")\n",
    "\n",
    "# Check if preprocessor file exists after clearing\n",
    "geom_preprocessor_suffix = '.c' + ''.join(Path(plan_path).suffixes[1:])\n",
    "geom_preprocessor_file = Path(plan_path).with_suffix(geom_preprocessor_suffix)\n",
    "print(f\"Preprocessor file exists after clearing: {geom_preprocessor_file.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Computation Parameters\n",
    "\n",
    "Let's set the computation parameters for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of cores to use for the computation\n",
    "RasPlan.set_num_cores(new_plan_number, 2)\n",
    "print(f\"Updated number of cores for plan {new_plan_number} to 2\")\n",
    "\n",
    "# Verify by extracting the value from the plan file\n",
    "cores_value = RasPlan.get_plan_value(new_plan_number, \"UNET D1 Cores\")\n",
    "print(f\"\\nVerified that UNET D1 Cores is set to: {cores_value}\")\n",
    "\n",
    "# Set geometry preprocessor options\n",
    "RasPlan.set_geom_preprocessor(plan_path, run_htab=-1, use_ib_tables=-1)\n",
    "print(f\"Updated geometry preprocessor options for plan {new_plan_number}\")\n",
    "print(f\"- Run HTab: -1 (Force recomputation of geometry tables)\")\n",
    "print(f\"- Use Existing IB Tables: -1 (Force recomputation of interpolation/boundary tables)\")\n",
    "\n",
    "# Verify by extracting the values from the plan file\n",
    "run_htab_value = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
    "ib_tables_value = RasPlan.get_plan_value(new_plan_number, \"UNET Use Existing IB Tables\")\n",
    "print(f\"\\nVerified setting values:\")\n",
    "print(f\"- Run HTab: {run_htab_value}\")\n",
    "print(f\"- UNET Use Existing IB Tables: {ib_tables_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Simulation Parameters\n",
    "\n",
    "Now, let's update various simulation parameters for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Update simulation date\n",
    "start_date = datetime(2023, 1, 1, 0, 0)  # January 1, 2023, 00:00\n",
    "end_date = datetime(2023, 1, 5, 23, 59)  # January 5, 2023, 23:59\n",
    "\n",
    "RasPlan.update_simulation_date(new_plan_number, start_date, end_date)\n",
    "print(f\"Updated simulation date for plan {new_plan_number}:\")\n",
    "print(f\"- Start Date: {start_date}\")\n",
    "print(f\"- End Date: {end_date}\")\n",
    "\n",
    "# Verify the update\n",
    "sim_date = RasPlan.get_plan_value(new_plan_number, \"Simulation Date\")\n",
    "print(f\"Verified Simulation Date value: {sim_date}\")\n",
    "\n",
    "# 2. Update plan intervals\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_plan_number,\n",
    "    computation_interval=\"1MIN\",  # Computational time step\n",
    "    output_interval=\"15MIN\",      # How often results are written\n",
    "    mapping_interval=\"30MIN\"      # How often mapping outputs are created\n",
    ")\n",
    "print(f\"\\nUpdated plan intervals for plan {new_plan_number}:\")\n",
    "print(f\"- Computation Interval: 1MIN\")\n",
    "print(f\"- Output Interval: 15MIN\")\n",
    "print(f\"- Mapping Interval: 30MIN\")\n",
    "\n",
    "# Verify the updates\n",
    "comp_interval = RasPlan.get_plan_value(new_plan_number, \"Computation Interval\")\n",
    "mapping_interval = RasPlan.get_plan_value(new_plan_number, \"Mapping Interval\")\n",
    "print(f\"Verified interval values:\")\n",
    "print(f\"- Computation Interval: {comp_interval}\")\n",
    "print(f\"- Mapping Interval: {mapping_interval}\")\n",
    "\n",
    "# 3. Update run flags\n",
    "RasPlan.update_run_flags(\n",
    "    new_plan_number,\n",
    "    geometry_preprocessor=True,   # Run the geometry preprocessor\n",
    "    unsteady_flow_simulation=True, # Run unsteady flow simulation\n",
    "    post_processor=True,          # Run post-processing\n",
    "    floodplain_mapping=True       # Generate floodplain mapping outputs\n",
    ")\n",
    "print(f\"\\nUpdated run flags for plan {new_plan_number}:\")\n",
    "print(f\"- Geometry Preprocessor: True\")\n",
    "print(f\"- Unsteady Flow Simulation: True\")\n",
    "print(f\"- Post Processor: True\")\n",
    "print(f\"- Floodplain Mapping: True\")\n",
    "\n",
    "# Verify the updates\n",
    "run_htab = RasPlan.get_plan_value(new_plan_number, \"Run HTab\")\n",
    "run_unet = RasPlan.get_plan_value(new_plan_number, \"Run UNet\")\n",
    "print(f\"Verified run flag values:\")\n",
    "print(f\"- Run HTab (Geometry Preprocessor): {run_htab}\")\n",
    "print(f\"- Run UNet (Unsteady Flow): {run_unet}\")\n",
    "\n",
    "# 4. Update plan description\n",
    "new_description = \"Combined plan with modified geometry and unsteady flow\\nJanuary 2023 simulation\\n1-minute computation interval\\nGeometry and unsteady flow from cloned files\"\n",
    "RasPlan.update_plan_description(new_plan_number, new_description)\n",
    "print(f\"\\nUpdated description for plan {new_plan_number}\")\n",
    "\n",
    "# Read back the description\n",
    "current_description = RasPlan.read_plan_description(new_plan_number)\n",
    "print(f\"Current plan description:\\n{current_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Plan\n",
    "\n",
    "Now that we have set up all the parameters, let's compute the plan using RasCmdr.compute_plan():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the plan with our configured settings\n",
    "# Note: This may take several minutes depending on the complexity of the model\n",
    "print(f\"Computing plan {new_plan_number}...\")\n",
    "success = RasCmdr.compute_plan(new_plan_number, clear_geompre=True)\n",
    "\n",
    "if success:\n",
    "    print(f\"Plan {new_plan_number} computed successfully\")\n",
    "else:\n",
    "    print(f\"Failed to compute plan {new_plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Results\n",
    "\n",
    "After computation, we should check if results were written correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the plan entries to ensure we have the latest data\n",
    "ras.plan_df = ras.get_plan_entries()\n",
    "hdf_entries = ras.get_hdf_entries()\n",
    "\n",
    "if not hdf_entries.empty:\n",
    "    print(\"HDF entries for the project:\")\n",
    "    display.display(hdf_entries)\n",
    "    \n",
    "    # Check if our new plan has an HDF file\n",
    "    new_plan_hdf = hdf_entries[hdf_entries['plan_number'] == new_plan_number]\n",
    "    if not new_plan_hdf.empty:\n",
    "        print(f\"\\nPlan {new_plan_number} has a valid HDF results file:\")\n",
    "        print(f\"HDF Path: {new_plan_hdf.iloc[0]['HDF_Results_Path']}\")\n",
    "    else:\n",
    "        print(f\"\\nNo HDF entry found for plan {new_plan_number}\")\n",
    "else:\n",
    "    print(\"No HDF entries found. This could mean the plan hasn't been computed successfully or the results haven't been written yet.\")\n",
    "\n",
    "# Display all plan entries to see their HDF paths\n",
    "print(\"\\nAll plan entries with their HDF paths:\")\n",
    "plan_hdf_info = ras.plan_df[['plan_number', 'HDF_Results_Path']]\n",
    "display.display(plan_hdf_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the plan was computed successfully, we can examine the runtime data and volume accounting from the HDF results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get computation runtime data from HDF\n",
    "print(\"Checking computation runtime data...\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(new_plan_number)\n",
    "\n",
    "if runtime_df is not None and not runtime_df.empty:\n",
    "    print(\"\\nSimulation Runtime Statistics:\")\n",
    "    display.display(runtime_df)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    sim_duration = runtime_df['Simulation Duration (s)'].iloc[0]\n",
    "    compute_time = runtime_df['Complete Process (hr)'].iloc[0]\n",
    "    compute_speed = runtime_df['Complete Process Speed (hr/hr)'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nSimulation Duration: {sim_duration:.2f} seconds\")\n",
    "    print(f\"Computation Time: {compute_time:.5f} hours\")\n",
    "    print(f\"Computation Speed: {compute_speed:.2f} (simulation hours/compute hours)\")\n",
    "else:\n",
    "    print(\"No runtime data found. This may indicate the simulation didn't complete successfully.\")\n",
    "\n",
    "# Get volume accounting data\n",
    "print(\"\\nChecking volume accounting...\")\n",
    "volume_df = HdfResultsPlan.get_volume_accounting(new_plan_number)\n",
    "\n",
    "if volume_df is not None and not isinstance(volume_df, bool):\n",
    "    # Handle volume_df as a dictionary\n",
    "    if isinstance(volume_df, dict):\n",
    "        error_percent = volume_df.get('Error Percent')\n",
    "        if error_percent is not None:\n",
    "            print(f\"\\nFinal Volume Balance Error: {float(error_percent):.8f}%\")\n",
    "            \n",
    "        # Print other key statistics\n",
    "        print(\"\\nDetailed Volume Statistics:\")\n",
    "        print(f\"Volume Starting: {float(volume_df['Volume Starting']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "        print(f\"Volume Ending: {float(volume_df['Volume Ending']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "        print(f\"Total Inflow: {float(volume_df['Total Boundary Flux of Water In']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "        print(f\"Total Outflow: {float(volume_df['Total Boundary Flux of Water Out']):.2f} {volume_df['Vol Accounting in'].decode()}\")\n",
    "else:\n",
    "    print(\"No volume accounting data found. This may indicate the simulation didn't complete successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Advanced HDF Data\n",
    "\n",
    "Let's explore how to access more detailed geometry data from the HDF files. When working with HEC-RAS, the geometric information is stored in HDF files (`.g*.hdf`) which can be accessed using the HDF classes in RAS Commander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh geometry information\n",
    "ras.geom_df = ras.get_geom_entries()\n",
    "\n",
    "# Get HDF path for the new geometry\n",
    "geom_info = ras.geom_df[ras.geom_df['geom_number'] == new_geom_number]\n",
    "if not geom_info.empty and 'hdf_path' in geom_info.columns:\n",
    "    geom_hdf_path = geom_info.iloc[0]['hdf_path']\n",
    "    print(f\"Geometry HDF path: {geom_hdf_path}\")\n",
    "    \n",
    "    # Check if the HDF file exists\n",
    "    geom_hdf_file = Path(geom_hdf_path)\n",
    "    if geom_hdf_file.exists():\n",
    "        print(f\"Geometry HDF file exists: {geom_hdf_file.exists()}\")\n",
    "        \n",
    "        # If it exists, try to extract some information from it\n",
    "        try:\n",
    "            # Get cross-sections if this is a 1D or combined 1D/2D model\n",
    "            xs_data = HdfXsec.get_cross_sections(geom_hdf_path)\n",
    "            if not xs_data.empty:\n",
    "                print(f\"\\nFound {len(xs_data)} cross-sections in the geometry:\")\n",
    "                display.display(xs_data.head())\n",
    "            else:\n",
    "                print(\"No cross-sections found in the geometry.\")\n",
    "                \n",
    "            # Get 2D flow areas if this is a 2D or combined 1D/2D model\n",
    "            mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)\n",
    "            if not mesh_areas.empty:\n",
    "                print(f\"\\nFound {len(mesh_areas)} 2D flow areas in the geometry:\")\n",
    "                display.display(mesh_areas.head())\n",
    "            else:\n",
    "                print(\"No 2D flow areas found in the geometry.\")\n",
    "                \n",
    "            # Get structures if any exist\n",
    "            strucs = HdfStruc.get_structures(geom_hdf_path)\n",
    "            if not strucs.empty:\n",
    "                print(f\"\\nFound {len(strucs)} structures in the geometry:\")\n",
    "                display.display(strucs.head())\n",
    "            else:\n",
    "                print(\"No structures found in the geometry.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing geometry HDF data: {e}\")\n",
    "    else:\n",
    "        print(\"Geometry HDF file does not exist.\")\n",
    "else:\n",
    "    print(\"Could not find HDF path for the new geometry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Plan and Geometry Operations\n",
    "\n",
    "In this notebook, we've covered a comprehensive range of operations on HEC-RAS plan and geometry files using the RAS Commander library:\n",
    "\n",
    "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
    "2. **Plan Operations**:\n",
    "   - Created a new plan by cloning an existing one\n",
    "   - Updated simulation parameters (dates, intervals, etc.)\n",
    "   - Set run flags for different components\n",
    "   - Updated the plan description\n",
    "3. **Geometry Operations**:\n",
    "   - Created a new geometry by cloning an existing one\n",
    "   - Associated the new geometry with our plan\n",
    "   - Cleared geometry preprocessor files\n",
    "4. **Unsteady Flow Operations**:\n",
    "   - Created a new unsteady flow file by cloning an existing one\n",
    "   - Associated it with our plan\n",
    "5. **Computation and Verification**:\n",
    "   - Computed our plan with the specified settings\n",
    "   - Verified the results using HDF entries\n",
    "   - Analyzed runtime statistics and volume accounting\n",
    "6. **Advanced HDF Operations**:\n",
    "   - Accessed detailed geometry information from HDF files\n",
    "   - Explored cross-sections, mesh areas, and structures\n",
    "\n",
    "### Key Classes and Functions Used\n",
    "\n",
    "- `RasPlan`: For plan operations (cloning, setting components, and modifying parameters)\n",
    "- `RasGeo`: For geometry operations (cloning, clearing preprocessor files)\n",
    "- `RasCmdr`: For executing HEC-RAS simulations\n",
    "- `HdfResultsPlan`: For accessing plan-level results\n",
    "- `HdfXsec`, `HdfMesh`, `HdfStruc`: For accessing geometry details\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further enhance your HEC-RAS automation, consider exploring:\n",
    "\n",
    "1. **Parameter Sweeps**: Create and run multiple plans with varying parameters\n",
    "2. **Parallel Computations**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
    "3. **Advanced Results Analysis**: Use the HDF classes to extract and analyze specific model results\n",
    "4. **Spatial Visualization**: Create maps and plots of simulation results\n",
    "5. **Model Calibration**: Automate comparison between model results and observations\n",
    "\n",
    "The RAS Commander library provides a powerful framework for automating and streamlining your HEC-RAS workflows, enabling more efficient hydraulic modeling and analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\03_unsteady_flow_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Unsteady Flow Operations\n",
    "\n",
    "This notebook demonstrates operations on unsteady flow files using the RAS Commander library. Unsteady flow files in HEC-RAS (`.u*` files) define the time-varying boundary conditions used in dynamic simulations.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Boundary Extraction**: Extract boundary conditions and tables from unsteady flow files\n",
    "3. **Boundary Analysis**: Inspect and understand boundary condition structures\n",
    "4. **Flow Title Updates**: Modify the title of unsteady flow files\n",
    "5. **Restart Settings**: Configure restart file settings for continuing simulations\n",
    "6. **Table Modification**: Extract, modify, and update flow tables\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Unsteady Flow Files in HEC-RAS\n",
    "\n",
    "Unsteady flow files (`.u*` files) in HEC-RAS define the time-varying boundary conditions that drive dynamic simulations. These include:\n",
    "\n",
    "- **Flow Hydrographs**: Time-series of flow values at model boundaries\n",
    "- **Stage Hydrographs**: Time-series of water surface elevations\n",
    "- **Lateral Inflows**: Distributed inflows along a reach\n",
    "- **Gate Operations**: Time-series of gate settings\n",
    "- **Meteorological Data**: Rainfall, evaporation, and other meteorological inputs\n",
    "\n",
    "The `RasUnsteady` class in RAS Commander provides methods for working with these files, including extracting boundaries, reading tables, and modifying parameters.\n",
    "\n",
    "Let's set up our working directory and define paths to example projects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Projects\n",
    "\n",
    "We'll use the `RasExamples` class to download and extract an example HEC-RAS project with unsteady flow files. For this notebook, we'll use the \"Balde Eagle Creek\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "The first step is to initialize the HEC-RAS project. This is done using the `init_ras_project()` function, which takes the following parameters:\n",
    "\n",
    "- `ras_project_folder`: Path to the HEC-RAS project folder (required)\n",
    "- `ras_version`: HEC-RAS version (e.g., \"6.6\") or path to Ras.exe (required first time)\n",
    "\n",
    "This function initializes the global `ras` object that we'll use for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "# This function returns a RAS object, but also updates the global 'ras' object\n",
    "# Parameters:\n",
    "#   - ras_project_folder: Path to the HEC-RAS project folder\n",
    "#   - ras_version: HEC-RAS version or path to Ras.exe\n",
    "\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the unsteady flow files in the project\n",
    "print(\"\\nHEC-RAS Project Unsteady Flow Data (unsteady_df):\")\n",
    "display.display(ras.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasUnsteady Class\n",
    "\n",
    "The `RasUnsteady` class provides functionality for working with HEC-RAS unsteady flow files (`.u*` files). Key operations include:\n",
    "\n",
    "1. **Extracting Boundary Conditions**: Read and parse boundary conditions from unsteady flow files\n",
    "2. **Modifying Flow Titles**: Update descriptive titles for unsteady flow scenarios\n",
    "3. **Managing Restart Settings**: Configure restart file options for continuing simulations\n",
    "4. **Working with Tables**: Extract, modify, and update flow tables\n",
    "\n",
    "Most methods in this class are static and work with the global `ras` object by default, though you can also pass in a custom RAS object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Boundary Conditions and Tables\n",
    "\n",
    "The `extract_boundary_and_tables()` method from the `RasUnsteady` class allows us to extract boundary conditions and their associated tables from an unsteady flow file.\n",
    "\n",
    "Parameters for `RasUnsteady.extract_boundary_and_tables()`:\n",
    "- `unsteady_file` (str): Path to the unsteady flow file\n",
    "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
    "\n",
    "Returns:\n",
    "- `pd.DataFrame`: DataFrame containing boundary conditions and their associated tables\n",
    "\n",
    "Let's see how this works with our example project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to unsteady flow file \"02\"\n",
    "unsteady_file = RasPlan.get_unsteady_path(\"02\")\n",
    "print(f\"Unsteady flow file path: {unsteady_file}\")\n",
    "\n",
    "# Extract boundary conditions and tables\n",
    "boundaries_df = RasUnsteady.extract_boundary_and_tables(unsteady_file)\n",
    "print(f\"Extracted {len(boundaries_df)} boundary conditions from the unsteady flow file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Print Boundaries and Tables\n",
    "\n",
    "The `print_boundaries_and_tables()` method provides a formatted display of the boundary conditions and their associated tables. This method doesn't return anything; it just prints the information in a readable format.\n",
    "\n",
    "Parameters for `RasUnsteady.print_boundaries_and_tables()`:\n",
    "- `boundaries_df` (pd.DataFrame): DataFrame containing boundary conditions from `extract_boundary_and_tables()`\n",
    "\n",
    "Let's use this method to get a better understanding of our boundary conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the boundaries and tables in a formatted way\n",
    "print(\"Detailed boundary conditions and tables:\")\n",
    "RasUnsteady.print_boundaries_and_tables(boundaries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Boundary Condition Types\n",
    "\n",
    "The output above shows the different types of boundary conditions in our unsteady flow file. Let's understand what each type means:\n",
    "\n",
    "1. **Flow Hydrograph**: A time series of flow values (typically in cfs or cms) entering the model at a specific location. These are used at upstream boundaries or internal points where flow enters the system.\n",
    "\n",
    "2. **Stage Hydrograph**: A time series of water surface elevations (typically in ft or m) that define the downstream boundary condition.\n",
    "\n",
    "3. **Gate Openings**: Time series of gate settings (typically height in ft or m) for hydraulic structures such as spillways, sluice gates, or other control structures.\n",
    "\n",
    "4. **Lateral Inflow Hydrograph**: Flow entering the system along a reach, not at a specific point. This can represent tributary inflows, overland flow, or other distributed inputs.\n",
    "\n",
    "5. **Normal Depth**: A boundary condition where the water surface slope is assumed to equal the bed slope. This is represented by a friction slope value.\n",
    "\n",
    "Let's look at a specific boundary condition in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the first boundary condition in more detail\n",
    "if not boundaries_df.empty:\n",
    "    first_boundary = boundaries_df.iloc[0]\n",
    "    print(f\"Detailed look at boundary condition {1}:\")\n",
    "    \n",
    "    # Print boundary location components\n",
    "    print(f\"\\nBoundary Location:\")\n",
    "    print(f\"  River Name: {first_boundary.get('River Name', 'N/A')}\")\n",
    "    print(f\"  Reach Name: {first_boundary.get('Reach Name', 'N/A')}\")\n",
    "    print(f\"  River Station: {first_boundary.get('River Station', 'N/A')}\")\n",
    "    print(f\"  Storage Area Name: {first_boundary.get('Storage Area Name', 'N/A')}\")\n",
    "    \n",
    "    # Print boundary condition type and other properties\n",
    "    print(f\"\\nBoundary Properties:\")\n",
    "    print(f\"  Boundary Type: {first_boundary.get('bc_type', 'N/A')}\")\n",
    "    print(f\"  DSS File: {first_boundary.get('DSS File', 'N/A')}\")\n",
    "    print(f\"  Use DSS: {first_boundary.get('Use DSS', 'N/A')}\")\n",
    "    \n",
    "    # Print table statistics if available\n",
    "    if 'Tables' in first_boundary and isinstance(first_boundary['Tables'], dict):\n",
    "        print(f\"\\nTable Information:\")\n",
    "        for table_name, table_df in first_boundary['Tables'].items():\n",
    "            print(f\"  {table_name}: {len(table_df)} values\")\n",
    "            if not table_df.empty:\n",
    "                print(f\"    Min Value: {table_df['Value'].min()}\")\n",
    "                print(f\"    Max Value: {table_df['Value'].max()}\")\n",
    "                print(f\"    First 5 Values: {table_df['Value'].head(5).tolist()}\")\n",
    "else:\n",
    "    print(\"No boundary conditions found in the unsteady flow file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update Flow Title\n",
    "\n",
    "The flow title in an unsteady flow file provides a description of the simulation scenario. The `update_flow_title()` method allows us to modify this title.\n",
    "\n",
    "Parameters for `RasUnsteady.update_flow_title()`:\n",
    "- `unsteady_file` (str): Full path to the unsteady flow file\n",
    "- `new_title` (str): New flow title (max 24 characters)\n",
    "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
    "\n",
    "Let's clone an unsteady flow file and update its title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone unsteady flow \"02\" to create a new unsteady flow file\n",
    "new_unsteady_number = RasPlan.clone_unsteady(\"02\")\n",
    "print(f\"New unsteady flow created: {new_unsteady_number}\")\n",
    "\n",
    "# Get the path to the new unsteady flow file\n",
    "new_unsteady_file = RasPlan.get_unsteady_path(new_unsteady_number)\n",
    "print(f\"New unsteady flow file path: {new_unsteady_file}\")\n",
    "\n",
    "# Get the current flow title\n",
    "current_title = None\n",
    "for _, row in ras.unsteady_df.iterrows():\n",
    "    if row['unsteady_number'] == new_unsteady_number and 'Flow Title' in row:\n",
    "        current_title = row['Flow Title']\n",
    "        break\n",
    "print(f\"Current flow title: {current_title}\")\n",
    "\n",
    "# Update the flow title\n",
    "new_title = \"Modified Flow Scenario\"\n",
    "RasUnsteady.update_flow_title(new_unsteady_file, new_title)\n",
    "print(f\"Updated flow title to: {new_title}\")\n",
    "\n",
    "# Refresh unsteady flow information to see the change\n",
    "ras.unsteady_df = ras.get_unsteady_entries()\n",
    "display.display(ras.unsteady_df[ras.unsteady_df['unsteady_number'] == new_unsteady_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update Restart Settings\n",
    "\n",
    "Restart files in HEC-RAS allow you to continue a simulation from a specific point in time, which can save computational resources. The `update_restart_settings()` method allows you to configure restart options.\n",
    "\n",
    "Parameters for `RasUnsteady.update_restart_settings()`:\n",
    "- `unsteady_file` (str): Full path to the unsteady flow file\n",
    "- `use_restart` (bool): Whether to use restart (True) or not (False)\n",
    "- `restart_filename` (str, optional): Name of the restart file (required if use_restart is True)\n",
    "- `ras_object` (optional): Custom RAS object to use instead of the global one\n",
    "\n",
    "Let's update the restart settings for our new unsteady flow file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update restart settings\n",
    "restart_filename = f\"{ras.project_name}.rst\"\n",
    "RasUnsteady.update_restart_settings(new_unsteady_file, use_restart=True, restart_filename=restart_filename)\n",
    "print(f\"Updated restart settings to use restart file: {restart_filename}\")\n",
    "\n",
    "# Let's extract the boundaries again to see the updated settings\n",
    "updated_boundaries_df = RasUnsteady.extract_boundary_and_tables(new_unsteady_file)\n",
    "\n",
    "# Check if there's a \"Use Restart\" property in the updated file\n",
    "use_restart_value = None\n",
    "restart_filename_value = None\n",
    "with open(new_unsteady_file, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\"Use Restart=\"):\n",
    "            use_restart_value = line.strip().split('=')[1].strip()\n",
    "        elif line.startswith(\"Restart Filename=\"):\n",
    "            restart_filename_value = line.strip().split('=')[1].strip()\n",
    "\n",
    "print(f\"\\nVerified restart settings:\")\n",
    "print(f\"Use Restart: {use_restart_value}\")\n",
    "print(f\"Restart Filename: {restart_filename_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOES NOT WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Working with Flow Tables\n",
    "\n",
    "Flow tables in unsteady flow files contain the time-series data for boundary conditions. Let's explore how to extract and work with these tables using some of the advanced methods from the `RasUnsteady` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific tables from the unsteady flow file\n",
    "all_tables = RasUnsteady.extract_tables(new_unsteady_file)\n",
    "print(f\"Extracted {len(all_tables)} tables from the unsteady flow file.\")\n",
    "\n",
    "# Let's look at the available table names\n",
    "print(\"\\nAvailable tables:\")\n",
    "for table_name in all_tables.keys():\n",
    "    print(f\"  {table_name}\")\n",
    "\n",
    "# Select the first table for detailed analysis\n",
    "if all_tables and len(all_tables) > 0:\n",
    "    first_table_name = list(all_tables.keys())[0]\n",
    "    first_table = all_tables[first_table_name]\n",
    "    \n",
    "    print(f\"\\nDetailed look at table '{first_table_name}':\")\n",
    "    print(f\"  Number of values: {len(first_table)}\")\n",
    "    print(f\"  Min value: {first_table['Value'].min()}\")\n",
    "    print(f\"  Max value: {first_table['Value'].max()}\")\n",
    "    print(f\"  Mean value: {first_table['Value'].mean():.2f}\")\n",
    "    print(f\"  First 10 values: {first_table['Value'].head(10).tolist()}\")\n",
    "    \n",
    "    # Create a visualization of the table values\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(first_table['Value'].values)\n",
    "        plt.title(f\"{first_table_name} Values\")\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create visualization: {e}\")\n",
    "else:\n",
    "    print(\"No tables found in the unsteady flow file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Modifying Flow Tables\n",
    "\n",
    "Now let's demonstrate how to modify a flow table and write it back to the unsteady flow file. For this example, we'll scale all the values in a table by a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, identify tables in the unsteady flow file\n",
    "tables = RasUnsteady.identify_tables(open(new_unsteady_file, 'r').readlines())\n",
    "print(f\"Identified {len(tables)} tables in the unsteady flow file.\")\n",
    "\n",
    "# Let's look at the first flow hydrograph table\n",
    "flow_hydrograph_tables = [t for t in tables if t[0] == 'Flow Hydrograph=']\n",
    "if flow_hydrograph_tables:\n",
    "    table_name, start_line, end_line = flow_hydrograph_tables[0]\n",
    "    print(f\"\\nSelected table: {table_name}\")\n",
    "    print(f\"  Start line: {start_line}\")\n",
    "    print(f\"  End line: {end_line}\")\n",
    "    \n",
    "    # Parse the table\n",
    "    lines = open(new_unsteady_file, 'r').readlines()\n",
    "    table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
    "    print(f\"\\nOriginal table statistics:\")\n",
    "    print(f\"  Number of values: {len(table_df)}\")\n",
    "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
    "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
    "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
    "    \n",
    "    # Modify the table - let's scale all values by 75%\n",
    "    scale_factor = 0.75\n",
    "    table_df['Value'] = table_df['Value'] * scale_factor\n",
    "    print(f\"\\nModified table statistics (scaled by {scale_factor}):\")\n",
    "    print(f\"  Number of values: {len(table_df)}\")\n",
    "    print(f\"  Min value: {table_df['Value'].min()}\")\n",
    "    print(f\"  Max value: {table_df['Value'].max()}\")\n",
    "    print(f\"  First 5 values: {table_df['Value'].head(5).tolist()}\")\n",
    "    \n",
    "    # Write the modified table back to the file\n",
    "    RasUnsteady.write_table_to_file(new_unsteady_file, table_name, table_df, start_line)\n",
    "    print(f\"\\nUpdated table written back to the unsteady flow file.\")\n",
    "    \n",
    "    # Re-read the table to verify changes\n",
    "    lines = open(new_unsteady_file, 'r').readlines()\n",
    "    updated_table_df = RasUnsteady.parse_fixed_width_table(lines, start_line, end_line)\n",
    "    print(f\"\\nVerified updated table statistics:\")\n",
    "    print(f\"  Number of values: {len(updated_table_df)}\")\n",
    "    print(f\"  Min value: {updated_table_df['Value'].min()}\")\n",
    "    print(f\"  Max value: {updated_table_df['Value'].max()}\")\n",
    "    print(f\"  First 5 values: {updated_table_df['Value'].head(5).tolist()}\")\n",
    "else:\n",
    "    print(\"No flow hydrograph tables found in the unsteady flow file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Applying the Updated Unsteady Flow to a Plan\n",
    "\n",
    "Now that we've modified an unsteady flow file, let's create a plan that uses it, and compute the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone an existing plan\n",
    "new_plan_number = RasPlan.clone_plan(\"01\", new_plan_shortid=\"Modified Flow Test\")\n",
    "print(f\"New plan created: {new_plan_number}\")\n",
    "\n",
    "# Set the modified unsteady flow for the new plan\n",
    "RasPlan.set_unsteady(new_plan_number, new_unsteady_number)\n",
    "print(f\"Set unsteady flow {new_unsteady_number} for plan {new_plan_number}\")\n",
    "\n",
    "# Update the plan description\n",
    "new_description = \"Test plan using modified unsteady flow\\nFlow scaled to 75% of original\\nWith restart file enabled\"\n",
    "RasPlan.update_plan_description(new_plan_number, new_description)\n",
    "print(f\"Updated plan description for plan {new_plan_number}\")\n",
    "\n",
    "# Set computation options\n",
    "RasPlan.set_num_cores(new_plan_number, 2)\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_plan_number,\n",
    "    computation_interval=\"1MIN\",\n",
    "    output_interval=\"15MIN\",\n",
    "    mapping_interval=\"1HOUR\"\n",
    ")\n",
    "print(f\"Updated computation settings for plan {new_plan_number}\")\n",
    "\n",
    "# Compute the plan\n",
    "print(f\"\\nComputing plan {new_plan_number} with modified unsteady flow...\")\n",
    "success = RasCmdr.compute_plan(new_plan_number)\n",
    "\n",
    "if success:\n",
    "    print(f\"Plan {new_plan_number} computed successfully\")\n",
    "    \n",
    "    # Check the results path\n",
    "    results_path = RasPlan.get_results_path(new_plan_number)\n",
    "    if results_path:\n",
    "        print(f\"Results available at: {results_path}\")\n",
    "        \n",
    "        # If it exists, get its size\n",
    "        results_file = Path(results_path)\n",
    "        if results_file.exists():\n",
    "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "else:\n",
    "    print(f\"Failed to compute plan {new_plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Unsteady Flow Operations\n",
    "\n",
    "In this notebook, we've covered the following unsteady flow operations using RAS Commander:\n",
    "\n",
    "1. **Project Initialization**: We initialized a HEC-RAS project to work with\n",
    "2. **Boundary Extraction**: We extracted boundary conditions and tables from unsteady flow files\n",
    "3. **Boundary Analysis**: We inspected and understood boundary condition structures\n",
    "4. **Flow Title Updates**: We modified the title of an unsteady flow file\n",
    "5. **Restart Settings**: We configured restart file settings for continuing simulations\n",
    "6. **Table Extraction**: We extracted flow tables for analysis\n",
    "7. **Table Modification**: We modified a flow table and wrote it back to the file\n",
    "8. **Application**: We created a plan using our modified unsteady flow and computed results\n",
    "\n",
    "### Key Classes and Functions Used\n",
    "\n",
    "- `RasUnsteady.extract_boundary_and_tables()`: Extract boundary conditions and tables\n",
    "- `RasUnsteady.print_boundaries_and_tables()`: Display formatted boundary information\n",
    "- `RasUnsteady.update_flow_title()`: Modify the flow title\n",
    "- `RasUnsteady.update_restart_settings()`: Configure restart options\n",
    "- `RasUnsteady.extract_tables()`: Extract tables from unsteady flow files\n",
    "- `RasUnsteady.identify_tables()`: Identify table locations in file\n",
    "- `RasUnsteady.parse_fixed_width_table()`: Parse fixed-width tables\n",
    "- `RasUnsteady.write_table_to_file()`: Write modified tables back to file\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further explore unsteady flow operations with RAS Commander, consider:\n",
    "\n",
    "1. **Advanced Flow Modifications**: Create scripts that systematically modify flow hydrographs\n",
    "2. **Sensitivity Analysis**: Create variations of unsteady flows to assess model sensitivity\n",
    "3. **Batch Processing**: Process multiple unsteady flow files for scenario analysis\n",
    "4. **Custom Boundary Conditions**: Create unsteady flows from external data sources\n",
    "5. **Results Analysis**: Compare results from different unsteady flow scenarios\n",
    "\n",
    "These advanced topics can be explored by building on the foundation established in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\04_multiple_project_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Multiple Project Operations\n",
    "\n",
    "This notebook demonstrates how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. This advanced workflow is useful for comparing different river systems, running scenario analyses across multiple watersheds, or managing a suite of related models.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Multiple Project Initialization**: Initialize and manage multiple HEC-RAS projects simultaneously\n",
    "2. **Cross-Project Operations**: Clone and modify plans across different projects\n",
    "3. **Parallel Execution**: Run computations for multiple projects in parallel\n",
    "4. **Resource Management**: Optimize computing resources when working with multiple models\n",
    "5. **Results Comparison**: Analyze and compare results from different projects\n",
    "6. **Advanced Project Workflow**: Build a comprehensive multi-project workflow\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Working Environment\n",
    "\n",
    "Let's set up our working directory and check the number of available CPU cores. Since we'll be running multiple projects in parallel, it's important to understand our system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific projects we'll use in this tutorial\n",
    "# This will download them if not present and extract them to the example_projects folder\n",
    "extracted_paths = RasExamples.extract_project([\"Balde Eagle Creek\", \"BaldEagleCrkMulti2D\", \"Muncie\"])\n",
    "print(extracted_paths)\n",
    "\n",
    "# Get the parent directory of the first extracted path as our examples directory\n",
    "examples_dir = extracted_paths[0].parent\n",
    "print(f\"Examples directory: {examples_dir}\")\n",
    "\n",
    "\n",
    "# Define paths to the extracted projects\n",
    "bald_eagle_path = examples_dir / \"Balde Eagle Creek\"\n",
    "multi_2d_path = examples_dir / \"BaldEagleCrkMulti2D\"\n",
    "muncie_path = examples_dir / \"Muncie\"\n",
    "\n",
    "# Verify the paths exist\n",
    "for path in [bald_eagle_path, multi_2d_path, muncie_path]:\n",
    "    print(f\"Path {path} exists: {path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define computation output paths\n",
    "bald_eagle_compute_folder = examples_dir / \"compute_bald_eagle\"\n",
    "muncie_compute_folder = examples_dir / \"compute_muncie\"\n",
    "\n",
    "# Check system resources\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "physical_cpu_count = psutil.cpu_count(logical=False)\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print(f\"System Resources:\")\n",
    "print(f\"- {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
    "print(f\"- {available_memory_gb:.1f} GB available memory\")\n",
    "print(f\"For multiple HEC-RAS projects, a good rule of thumb is:\")\n",
    "print(f\"- Assign 2-4 cores per project\")\n",
    "print(f\"- Allocate at least 2-4 GB of RAM per project\")\n",
    "print(f\"Based on your system, you could reasonably run {min(physical_cpu_count//2, int(available_memory_gb//3))} projects simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Multiple RAS Project Management\n",
    "\n",
    "When working with multiple HEC-RAS projects in RAS Commander, there are two important concepts to understand:\n",
    "\n",
    "1. **The Global 'ras' Object**: By default, RAS Commander maintains a global `ras` object that represents the currently active project. This is convenient for simple scripts.\n",
    "\n",
    "2. **Custom RAS Objects**: For multiple projects, you'll create separate RAS objects for each project. These custom objects store project-specific data and are passed to RAS Commander functions using the `ras_object` parameter.\n",
    "\n",
    "### Best Practices for Multiple Project Management\n",
    "\n",
    "- **Name Your Objects Clearly**: Use descriptive variable names for your RAS objects (e.g., `bald_eagle_ras`, `muncie_ras`)\n",
    "- **Be Consistent**: Always pass the appropriate RAS object to functions when working with multiple projects\n",
    "- **Avoid Using Global 'ras'**: When working with multiple projects, avoid using the global `ras` object to prevent confusion\n",
    "- **Separate Compute Folders**: Use separate computation folders for each project\n",
    "- **Manage Resources**: Be mindful of CPU and memory usage when running multiple projects in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Projects\n",
    "\n",
    "We'll use the `RasExamples` class to download and extract two example HEC-RAS projects: \"Balde Eagle Creek\" and \"Muncie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing project if it exists to ensure a clean start\n",
    "if examples_dir.exists():\n",
    "    shutil.rmtree(examples_dir)\n",
    "    print(f\"Removed existing example projects directory: {examples_dir}\")\n",
    "\n",
    "# Create a RasExamples instance\n",
    "ras_examples = RasExamples()\n",
    "\n",
    "# Extract the example projects\n",
    "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\", \"Muncie\"])\n",
    "print(f\"Extracted projects to:\")\n",
    "for path in extracted_paths:\n",
    "    print(f\"- {path}\")\n",
    "\n",
    "# Verify the paths exist\n",
    "print(f\"\\nBald Eagle Creek project exists: {bald_eagle_path.exists()}\")\n",
    "print(f\"Muncie project exists: {muncie_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Multiple Projects\n",
    "\n",
    "Let's initialize both HEC-RAS projects. Instead of using the global `ras` object, we'll create separate RAS objects for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both projects\n",
    "bald_eagle_ras = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized Bald Eagle Creek project: {bald_eagle_ras.project_name}\")\n",
    "\n",
    "muncie_ras = init_ras_project(muncie_path, \"6.6\")\n",
    "print(f\"Initialized Muncie project: {muncie_ras.project_name}\")\n",
    "\n",
    "# Display available plans in each project\n",
    "print(\"\\nAvailable plans in Bald Eagle Creek project:\")\n",
    "display.display(bald_eagle_ras.plan_df)\n",
    "\n",
    "print(\"\\nAvailable plans in Muncie project:\")\n",
    "display.display(muncie_ras.plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Plans in Each Project\n",
    "\n",
    "Now, let's clone a plan in each project, giving them custom short identifiers. This demonstrates how to perform operations on multiple projects independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone plans with custom short identifiers\n",
    "new_bald_eagle_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=bald_eagle_ras)\n",
    "print(f\"Created new plan {new_bald_eagle_plan} in Bald Eagle Creek project\")\n",
    "\n",
    "new_muncie_plan = RasPlan.clone_plan(\"01\", new_plan_shortid=\"MultiProjDemo\", ras_object=muncie_ras)\n",
    "print(f\"Created new plan {new_muncie_plan} in Muncie project\")\n",
    "\n",
    "# Display the updated plan dataframes\n",
    "print(\"\\nUpdated plans in Bald Eagle Creek project:\")\n",
    "bald_eagle_ras.plan_df = bald_eagle_ras.get_plan_entries()  # Refresh the plan dataframe\n",
    "display.display(bald_eagle_ras.plan_df)\n",
    "\n",
    "print(\"\\nUpdated plans in Muncie project:\")\n",
    "muncie_ras.plan_df = muncie_ras.get_plan_entries()  # Refresh the plan dataframe\n",
    "display.display(muncie_ras.plan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Plans for Both Projects\n",
    "\n",
    "Let's configure the plans for both projects, setting geometry, number of cores, and other parameters. This demonstrates how to customize plans for different projects using the same code structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Bald Eagle Creek plan\n",
    "print(\"Configuring Bald Eagle Creek plan:\")\n",
    "RasPlan.set_geom(new_bald_eagle_plan, \"01\", ras_object=bald_eagle_ras)\n",
    "RasPlan.set_num_cores(new_bald_eagle_plan, 2, ras_object=bald_eagle_ras)\n",
    "\n",
    "# Update description and intervals\n",
    "description = \"Multi-project demonstration plan\\nBald Eagle Creek project\\nConfigured for parallel execution\"\n",
    "RasPlan.update_plan_description(new_bald_eagle_plan, description, ras_object=bald_eagle_ras)\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_bald_eagle_plan, \n",
    "    computation_interval=\"10SEC\", \n",
    "    output_interval=\"5MIN\", \n",
    "    ras_object=bald_eagle_ras\n",
    ")\n",
    "print(\"Successfully configured Bald Eagle Creek plan\")\n",
    "\n",
    "# Configure the Muncie plan\n",
    "print(\"\\nConfiguring Muncie plan:\")\n",
    "RasPlan.set_geom(new_muncie_plan, \"01\", ras_object=muncie_ras)\n",
    "RasPlan.set_num_cores(new_muncie_plan, 2, ras_object=muncie_ras)\n",
    "\n",
    "# Update description and intervals\n",
    "description = \"Multi-project demonstration plan\\nMuncie project\\nConfigured for parallel execution\"\n",
    "RasPlan.update_plan_description(new_muncie_plan, description, ras_object=muncie_ras)\n",
    "RasPlan.update_plan_intervals(\n",
    "    new_muncie_plan, \n",
    "    computation_interval=\"10SEC\", \n",
    "    output_interval=\"5MIN\", \n",
    "    ras_object=muncie_ras\n",
    ")\n",
    "print(\"Successfully configured Muncie plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Compute Folders for Both Projects\n",
    "\n",
    "Now, let's create separate compute folders for each project. This allows us to run the computations separately and in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute folders or clean them if they already exist\n",
    "for folder in [bald_eagle_compute_folder, muncie_compute_folder]:\n",
    "    if folder.exists():\n",
    "        shutil.rmtree(folder)\n",
    "        print(f\"Removed existing compute folder: {folder}\")\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created compute folder: {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Project Execution Function\n",
    "\n",
    "Let's define a function to execute plans for each project, which we can run in parallel. This function will handle plan execution, timing, and provide detailed status updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_plan(plan_number, ras_object, compute_folder, project_name):\n",
    "    \"\"\"\n",
    "    Execute a HEC-RAS plan and return detailed information about the execution.\n",
    "    \n",
    "    Args:\n",
    "        plan_number (str): The plan number to execute\n",
    "        ras_object: The RAS project object\n",
    "        compute_folder (Path): Folder where computation will be performed\n",
    "        project_name (str): A descriptive name for the project\n",
    "        \n",
    "    Returns:\n",
    "        dict: Detailed information about the execution\n",
    "    \"\"\"\n",
    "    print(f\"Starting execution of plan {plan_number} for {project_name}...\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the plan in the compute folder\n",
    "    success = RasCmdr.compute_plan(\n",
    "        plan_number=plan_number, \n",
    "        ras_object=ras_object, \n",
    "        dest_folder=compute_folder,\n",
    "        clear_geompre=True\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Determine if results were created\n",
    "    result_path = None\n",
    "    result_size = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize a temporary RAS object in the compute folder to check results\n",
    "        compute_ras = init_ras_project(compute_folder, ras_object.ras_exe_path)\n",
    "        result_path = RasPlan.get_results_path(plan_number, ras_object=compute_ras)\n",
    "        \n",
    "        if result_path:\n",
    "            result_file = Path(result_path)\n",
    "            if result_file.exists():\n",
    "                result_size = result_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking results for {project_name}: {e}\")\n",
    "    \n",
    "    # Build result information\n",
    "    result_info = {\n",
    "        \"project_name\": project_name,\n",
    "        \"plan_number\": plan_number,\n",
    "        \"success\": success,\n",
    "        \"duration\": duration,\n",
    "        \"compute_folder\": str(compute_folder),\n",
    "        \"result_path\": str(result_path) if result_path else None,\n",
    "        \"result_size_mb\": result_size,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    print(f\"Completed execution of plan {plan_number} for {project_name} in {duration:.2f} seconds\")\n",
    "    return result_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute Plans for Both Projects in Parallel\n",
    "\n",
    "Now, let's run both projects in parallel using a `ThreadPoolExecutor`. This allows us to utilize our system resources efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing plans for both projects in parallel...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Define the execution tasks\n",
    "execution_tasks = [\n",
    "    (new_bald_eagle_plan, bald_eagle_ras, bald_eagle_compute_folder, \"Bald Eagle Creek\"),\n",
    "    (new_muncie_plan, muncie_ras, muncie_compute_folder, \"Muncie\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Execute the plans in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [\n",
    "        executor.submit(execute_plan, *task)\n",
    "        for task in execution_tasks\n",
    "    ]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Execution error: {e}\")\n",
    "\n",
    "print(\"\\nAll executions complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results\n",
    "\n",
    "Let's analyze the results from both project executions, comparing execution times, result sizes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Execution Results Summary:\")\n",
    "display.display(results_df[['project_name', 'plan_number', 'success', 'duration', 'result_size_mb']])\n",
    "\n",
    "# Create a bar chart for execution times\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['project_name'], results_df['duration'], color=['blue', 'green'])\n",
    "plt.title('Execution Time by Project')\n",
    "plt.xlabel('Project')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add duration values on top of the bars\n",
    "for i, duration in enumerate(results_df['duration']):\n",
    "    plt.text(i, duration + 5, f\"{duration:.1f}s\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If we have result sizes, create a chart for those as well\n",
    "if results_df['result_size_mb'].notna().any():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(results_df['project_name'], results_df['result_size_mb'], color=['orange', 'purple'])\n",
    "    plt.title('Result File Size by Project')\n",
    "    plt.xlabel('Project')\n",
    "    plt.ylabel('Result Size (MB)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add size values on top of the bars\n",
    "    for i, size in enumerate(results_df['result_size_mb']):\n",
    "        if pd.notna(size):\n",
    "            plt.text(i, size + 2, f\"{size:.1f} MB\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Two HEC-RAS Projects\n",
    "\n",
    "Let's create a utility function to compare the structures of the two HEC-RAS projects. This helps us understand the differences between the projects we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_project_structures(ras_object1, name1, ras_object2, name2):\n",
    "    \"\"\"\n",
    "    Compare the structures of two HEC-RAS projects and display differences.\n",
    "    \"\"\"\n",
    "    # Refresh all dataframes to ensure we have the latest data\n",
    "    ras_object1.plan_df = ras_object1.get_plan_entries()\n",
    "    ras_object1.geom_df = ras_object1.get_geom_entries()\n",
    "    ras_object1.flow_df = ras_object1.get_flow_entries()\n",
    "    ras_object1.unsteady_df = ras_object1.get_unsteady_entries()\n",
    "    \n",
    "    ras_object2.plan_df = ras_object2.get_plan_entries()\n",
    "    ras_object2.geom_df = ras_object2.get_geom_entries()\n",
    "    ras_object2.flow_df = ras_object2.get_flow_entries()\n",
    "    ras_object2.unsteady_df = ras_object2.get_unsteady_entries()\n",
    "    \n",
    "    # Create a comparison dictionary\n",
    "    comparison = {\n",
    "        'Project Name': [ras_object1.project_name, ras_object2.project_name],\n",
    "        'Plan Count': [len(ras_object1.plan_df), len(ras_object2.plan_df)],\n",
    "        'Geometry Count': [len(ras_object1.geom_df), len(ras_object2.geom_df)],\n",
    "        'Flow Count': [len(ras_object1.flow_df), len(ras_object2.flow_df)],\n",
    "        'Unsteady Count': [len(ras_object1.unsteady_df), len(ras_object2.unsteady_df)]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame for the comparison\n",
    "    comparison_df = pd.DataFrame(comparison, index=[name1, name2])\n",
    "    \n",
    "\n",
    "    # Display the comparison\n",
    "    print(\"Project Structure Comparison:\")\n",
    "    display.display(comparison_df)\n",
    "    \n",
    "    # Create a bar chart to visualize the comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    comparison_df.iloc[:, 1:].plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Project Structure Comparison')\n",
    "    plt.xlabel('Project')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Component')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set y-axis to only show whole numbers (integers)\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare the structures of the two projects\n",
    "comparison_df = compare_project_structures(\n",
    "    bald_eagle_ras, \"Bald Eagle Creek\", \n",
    "    muncie_ras, \"Muncie\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Multiple Project Operations\n",
    "\n",
    "In this notebook, we've demonstrated how to work with multiple HEC-RAS projects simultaneously using the RAS Commander library. We've covered the following key operations:\n",
    "\n",
    "1. **Initializing Multiple Projects**: Creating separate RAS objects for different projects\n",
    "2. **Independent Configuration**: Configuring plans with project-specific parameters\n",
    "3. **Parallel Execution**: Running computations from different projects simultaneously\n",
    "4. **Resource Management**: Organizing compute folders and tracking execution statistics\n",
    "5. **Results Comparison**: Analyzing and comparing results from different projects\n",
    "6. **Advanced Workflows**: Creating sensitivity plans and batch processing pipelines\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "When working with multiple HEC-RAS projects in RAS Commander, remember these key concepts:\n",
    "\n",
    "- **Custom RAS Objects**: Create and use separate RAS objects for each project\n",
    "- **Always Specify ras_object**: Use the `ras_object` parameter in all function calls\n",
    "- **Separate Compute Folders**: Use separate folders for each project's computations\n",
    "- **Resource Management**: Be mindful of CPU and memory usage when running in parallel\n",
    "- **Project Tracking**: Keep track of which results belong to which project\n",
    "\n",
    "### Multiple Project Applications\n",
    "\n",
    "Working with multiple projects unlocks advanced applications such as:\n",
    "\n",
    "1. **Model Comparison**: Compare results from different river systems\n",
    "2. **Basin-wide Analysis**: Analyze connected river systems in parallel\n",
    "3. **Parameter Sweep**: Test a range of parameters across multiple models\n",
    "4. **Model Development**: Develop and test models simultaneously\n",
    "5. **Batch Processing**: Process large sets of models in an automated pipeline\n",
    "\n",
    "These capabilities make RAS Commander a powerful tool for large-scale hydraulic modeling and water resources management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\05_single_plan_execution.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Single Plan Execution\n",
    "\n",
    "This notebook demonstrates how to execute a single HEC-RAS plan using the RAS Commander library. We'll focus specifically on running a plan with a specified number of processor cores while overwriting an existing computation folder.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Plan Overview**: Explore the available plans in the project\n",
    "3. **Core Execution Configuration**: Set the number of processor cores to use during computation\n",
    "4. **Destination Folder Management**: Use and overwrite computation folders \n",
    "5. **Results Verification**: Check the results paths after computation\n",
    "6. **Performance Considerations**: Understand the impact of core count on performance\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Working Environment\n",
    "\n",
    "Let's set up our working directory and paths to example projects. We'll also check the number of available CPU cores on this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to example projects\n",
    "examples_dir = bald_eagle_path.parent\n",
    "\n",
    "# Define computation output paths\n",
    "compute_dest_folder = examples_dir / \"compute_test\"\n",
    "\n",
    "# Check system resources\n",
    "cpu_count = psutil.cpu_count(logical=True)\n",
    "physical_cpu_count = psutil.cpu_count(logical=False)\n",
    "print(f\"System has {physical_cpu_count} physical CPU cores ({cpu_count} logical cores)\")\n",
    "print(f\"For HEC-RAS computation, it's often most efficient to use 2-8 cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasCmdr.compute_plan Method\n",
    "\n",
    "Before we dive into execution, let's understand the `compute_plan` method from the `RasCmdr` class, which is the core function for running HEC-RAS simulations.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (str, Path): The plan number to execute or the full path to the plan file\n",
    "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
    "- `ras_object` (RasPrj, optional): Specific RAS object to use (defaults to global `ras`)\n",
    "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files (default: False)\n",
    "- `num_cores` (int, optional): Number of processor cores to use (default: None, uses plan settings)\n",
    "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists (default: False)\n",
    "\n",
    "### Returns\n",
    "- `bool`: True if the execution was successful, False otherwise\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Destination Folder**: By default, the simulation runs in the original project folder. Specifying a destination folder creates a copy of the project in that location for execution, leaving the original project untouched.\n",
    "\n",
    "2. **Number of Cores**: HEC-RAS can use multiple processor cores to speed up computation. The optimal number depends on the model complexity and your computer's specifications. Generally:\n",
    "   - 1-2 cores: Good for small models, highest efficiency per core\n",
    "   - 3-8 cores: Good balance for most models\n",
    "   - >8 cores: Diminishing returns, may actually be slower due to overhead\n",
    "\n",
    "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties, which is useful after making geometry changes.\n",
    "\n",
    "4. **Overwrite Destination**: Controls whether an existing destination folder should be overwritten. This is a safety feature to prevent accidental deletion of important results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore Available Plans\n",
    "\n",
    "Let's examine the available plans in the project to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the available plans in the project\n",
    "print(\"Available plans in the project:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Let's check the current setting for number of cores in the plans\n",
    "print(\"\\nCurrent core settings for plans:\")\n",
    "for plan_num in ras.plan_df['plan_number']:\n",
    "    # Check all three core parameters\n",
    "    d1_cores = RasPlan.get_plan_value(plan_num, \"UNET D1 Cores\")\n",
    "    d2_cores = RasPlan.get_plan_value(plan_num, \"UNET D2 Cores\") \n",
    "    ps_cores = RasPlan.get_plan_value(plan_num, \"PS Cores\")\n",
    "    \n",
    "    print(f\"Plan {plan_num}'s Existing Settings:\")\n",
    "    print(f\"  1D Cores: {d1_cores}\")\n",
    "    print(f\"  2D Cores: {d2_cores}\")\n",
    "    print(f\"  Pump Station Cores: {ps_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Destination Folder Structure\n",
    "\n",
    "Now, let's prepare a destination folder for our computation. This allows us to run simulations without modifying the original project files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a destination folder path\n",
    "dest_folder = examples_dir / \"compute_test_cores\"\n",
    "\n",
    "# Check if the destination folder already exists\n",
    "if dest_folder.exists():\n",
    "    print(f\"Destination folder already exists: {dest_folder}\")\n",
    "    print(\"We'll use overwrite_dest=True to replace it\")\n",
    "else:\n",
    "    print(f\"Destination folder will be created: {dest_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Execute a Plan with a Specified Number of Cores\n",
    "\n",
    "Now we're ready to execute a plan with a specified number of cores, overwriting the destination folder if it exists. This is the core functionality demonstrated in Example 5 of the original script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a plan and number of cores\n",
    "plan_number = \"01\"\n",
    "num_cores = 2  # Specify the number of cores to use\n",
    "\n",
    "print(f\"Executing plan {plan_number} with {num_cores} cores...\")\n",
    "print(f\"Destination folder: {dest_folder}\")\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the plan with specified parameters\n",
    "success = RasCmdr.compute_plan(\n",
    "    plan_number,              # The plan to execute\n",
    "    dest_folder=dest_folder,  # Where to run the simulation\n",
    "    num_cores=num_cores,      # Number of processor cores to use\n",
    "    overwrite_dest=True       # Overwrite destination folder if it exists\n",
    ")\n",
    "\n",
    "# Record the end time and calculate duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Report results\n",
    "if success:\n",
    "    print(f\" Plan {plan_number} executed successfully using {num_cores} cores\")\n",
    "    print(f\"Execution time: {duration:.2f} seconds\")\n",
    "else:\n",
    "    print(f\" Plan {plan_number} execution failed\")\n",
    "    print(f\"Time elapsed: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Results\n",
    "\n",
    "After execution, let's verify the results by checking the results paths and examining the destination folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the destination folder exists and contains the expected files\n",
    "if dest_folder.exists():\n",
    "    print(f\"Destination folder exists: {dest_folder}\")\n",
    "    \n",
    "    # List the key files in the destination folder\n",
    "    print(\"\\nKey files in destination folder:\")\n",
    "    project_files = list(dest_folder.glob(f\"{ras.project_name}.*\"))\n",
    "    for file in project_files[:10]:  # Show first 10 files\n",
    "        file_size = file.stat().st_size / 1024  # Size in KB\n",
    "        print(f\"  {file.name}: {file_size:.1f} KB\")\n",
    "    \n",
    "    if len(project_files) > 10:\n",
    "        print(f\"  ... and {len(project_files) - 10} more files\")\n",
    "    \n",
    "    # Check for HDF result files\n",
    "    print(\"\\nHDF result files:\")\n",
    "    hdf_files = list(dest_folder.glob(f\"*.hdf\"))\n",
    "    for file in hdf_files:\n",
    "        file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "        print(f\"  {file.name}: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"Destination folder does not exist: {dest_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results path using the RasPlan.get_results_path method\n",
    "# First, initialize a RAS object using the destination folder\n",
    "try:\n",
    "    dest_ras = init_ras_project(dest_folder, \"6.6\")\n",
    "    \n",
    "    # Get the results path for the plan we just executed\n",
    "    results_path = RasPlan.get_results_path(plan_number, ras_object=dest_ras)\n",
    "    \n",
    "    if results_path:\n",
    "        print(f\"Results for plan {plan_number} are located at: {results_path}\")\n",
    "        \n",
    "        # Check if the file exists and get its size\n",
    "        results_file = Path(results_path)\n",
    "        if results_file.exists():\n",
    "            size_mb = results_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"Results file size: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"No results found for plan {plan_number} in the destination folder\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Single Plan Execution Options\n",
    "\n",
    "The `RasCmdr.compute_plan()` method provides a flexible way to execute HEC-RAS plans with various options. Here's a summary of the key parameters we've explored:\n",
    "\n",
    "1. **Basic Execution**: Simply provide a plan number\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\")\n",
    "   ```\n",
    "\n",
    "2. **Destination Folder**: Run in a separate folder to preserve the original project\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\")\n",
    "   ```\n",
    "\n",
    "3. **Number of Cores**: Control the CPU resources used\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", num_cores=2)\n",
    "   ```\n",
    "\n",
    "4. **Overwrite Destination**: Replace existing computation folders\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", dest_folder=\"path/to/folder\", overwrite_dest=True)\n",
    "   ```\n",
    "\n",
    "5. **Clear Geometry Preprocessor**: Force recalculation of geometric properties\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\"01\", clear_geompre=True)\n",
    "   ```\n",
    "\n",
    "6. **Combined Options**: Use multiple options together\n",
    "   ```python\n",
    "   RasCmdr.compute_plan(\n",
    "       \"01\",\n",
    "       dest_folder=\"path/to/folder\",\n",
    "       num_cores=2,\n",
    "       clear_geompre=True,\n",
    "       overwrite_dest=True\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further enhance your HEC-RAS automation, consider exploring:\n",
    "\n",
    "1. **Parallel Execution**: Use `RasCmdr.compute_parallel()` to run multiple plans simultaneously\n",
    "2. **Test Mode**: Use `RasCmdr.compute_test_mode()` for testing purposes\n",
    "3. **Pre-Processing**: Modify plans, geometries, and unsteady flows before execution\n",
    "4. **Post-Processing**: Analyze results after computation\n",
    "5. **Batch Processing**: Create scripts for parameter sweeps or scenario analysis\n",
    "\n",
    "These advanced topics are covered in other examples and documentation for the RAS Commander library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\06_executing_plan_sets.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Executing Plan Sets \n",
    "\n",
    "This notebook demonstrates different ways to specify and execute HEC-RAS plans using the RAS Commander library. Proper plan specification is essential for efficient model execution, especially when working with large projects containing multiple plans.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project and explore available plans\n",
    "2. **Sequential Execution of Specific Plans**: Select and run particular plans in sequence\n",
    "3. **Parallel Execution of Specific Plans**: Run selected plans simultaneously\n",
    "4. **Executing All Plans**: Run every plan in a project\n",
    "5. **Filtered Plan Selection**: Select plans based on criteria or patterns\n",
    "6. **Conditional Execution**: Run plans based on results of previous executions\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Plan Specification in HEC-RAS\n",
    "\n",
    "In HEC-RAS, each plan (`.p*` file) represents a specific hydraulic model simulation scenario. When working with RAS Commander, you can specify plans for execution in several ways:\n",
    "\n",
    "1. **Single Plan**: Specify one plan by its number (e.g., \"01\")\n",
    "2. **List of Plans**: Specify multiple plans as a list (e.g., [\"01\", \"03\", \"05\"])\n",
    "3. **All Plans**: Execute all plans in a project by not specifying any plan or passing `None`\n",
    "4. **Filtered Plans**: Select plans based on criteria (e.g., plans with specific flow conditions)\n",
    "5. **Plan Path**: Specify the full path to a plan file instead of just the number\n",
    "\n",
    "### Why Plan Specification Matters\n",
    "\n",
    "- **Efficiency**: Run only the plans you need rather than recomputing everything\n",
    "- **Organization**: Group related plans for batch processing\n",
    "- **Automation**: Create workflows that process plans in a specific order\n",
    "- **Resource Management**: Optimize hardware utilization for specific plans\n",
    "\n",
    "### Best Practices for Plan Specification\n",
    "\n",
    "- Use consistent formatting for plan numbers (e.g., always use two-digit strings like \"01\" instead of 1)\n",
    "- Check available plans before attempting to execute them\n",
    "- Organize plans by purpose to make selection easier\n",
    "- Use descriptive short identifiers and plan titles to aid in selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function and explore the available plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nAvailable plans in the project:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Check plan details to understand what each plan represents\n",
    "plan_details = []\n",
    "for index, row in ras.plan_df.iterrows():\n",
    "    plan_number = row['plan_number']\n",
    "    \n",
    "    # Get plan description if available\n",
    "    description = None\n",
    "    if 'description' in row:\n",
    "        description = row['description']\n",
    "    else:\n",
    "        try:\n",
    "            description = RasPlan.read_plan_description(plan_number)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Get short identifier if available\n",
    "    short_id = None\n",
    "    if 'Short Identifier' in row:\n",
    "        short_id = row['Short Identifier']\n",
    "    \n",
    "    # Get geometry file\n",
    "    geom_file = None\n",
    "    if 'Geom File' in row:\n",
    "        geom_file = row['Geom File']\n",
    "    \n",
    "    # Check if the plan has results\n",
    "    has_results = False\n",
    "    if 'HDF_Results_Path' in row and row['HDF_Results_Path']:\n",
    "        has_results = True\n",
    "    \n",
    "    plan_details.append({\n",
    "        'Plan Number': plan_number,\n",
    "        'Short ID': short_id,\n",
    "        'Description': description[:50] + '...' if description and len(description) > 50 else description,\n",
    "        'Geometry': geom_file,\n",
    "        'Has Results': has_results\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with the plan details\n",
    "plan_details_df = pd.DataFrame(plan_details)\n",
    "print(\"\\nPlan details:\")\n",
    "display.display(plan_details_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sequential Execution of Specific Plans\n",
    "\n",
    "Let's execute specific plans in sequence using `RasCmdr.compute_test_mode()` with a list of plan numbers. This approach allows us to run only the plans we need, in the order we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing specific plans sequentially...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Define the plans to execute\n",
    "specific_plans = [\"01\", \"03\"]\n",
    "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute specific plans sequentially\n",
    "execution_results = RasCmdr.compute_test_mode(\n",
    "    plan_number=specific_plans,\n",
    "    dest_folder_suffix=\"[SpecificSequential]\",\n",
    "    num_cores=6, \n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "sequential_duration = end_time - start_time\n",
    "\n",
    "print(f\"Sequential execution of specific plans completed in {sequential_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "sequential_results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Sequential\"}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "sequential_results_df \n",
    "\n",
    "# Ensure the 'Plan' column exists before sorting\n",
    "if 'Plan' in sequential_results_df.columns:\n",
    "    sequential_results_df = sequential_results_df.sort_values(\"Plan\")\n",
    "else:\n",
    "    print(\"Warning: 'Plan' column not found in execution results.\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nSequential Execution Results:\")\n",
    "display.display(sequential_results_df)\n",
    "\n",
    "# Check the test folder\n",
    "test_folder = bald_eagle_path.parent / f\"{ras.project_name} [SpecificSequential]\"\n",
    "if test_folder.exists():\n",
    "    print(f\"\\nTest folder exists: {test_folder}\")\n",
    "    \n",
    "    # Check for results\n",
    "    hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
    "    if hdf_files:\n",
    "        print(f\"Found {len(hdf_files)} HDF result files:\")\n",
    "        for file in hdf_files:\n",
    "            file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "            print(f\"  {file.name}: {file_size:.1f} MB\")\n",
    "    else:\n",
    "        print(\"No HDF result files found in the test folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running Only Plans Without HDF Results\n",
    "An important use case is to identify and execute only those plans that have no existing HDF results. This approach can save time by avoiding redundant computations, especially useful when adding new plans to an existing project or after making limited changes.\n",
    "\n",
    "Let's demonstrate how to:\n",
    "\n",
    "- Use the `ras` object to identify plans without results\n",
    "- Create a filtered list of these plans\n",
    "- Execute only the missing plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifying and executing plans without HDF results...\")\n",
    "\n",
    "# Use the ras object to determine which plans don't have results\n",
    "plans_no_results = ras.plan_df[ras.plan_df['HDF_Results_Path'].isna()]['plan_number'].tolist()\n",
    "\n",
    "if not plans_no_results:\n",
    "    print(\"All plans already have HDF results. Creating a test scenario...\")\n",
    "    # For demonstration purposes, pretend some plans don't have results\n",
    "    plans_no_results = [\"04\", \"05\"]\n",
    "    print(f\"Simulating no results for plans: {', '.join(plans_no_results)}\")\n",
    "else:\n",
    "    print(f\"Found {len(plans_no_results)} plans without HDF results: {', '.join(plans_no_results)}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute only the plans without results\n",
    "if plans_no_results:\n",
    "    print(f\"\\nExecuting {len(plans_no_results)} plans without results...\")\n",
    "    execution_results = RasCmdr.compute_test_mode(\n",
    "        plan_number=plans_no_results,\n",
    "        dest_folder_suffix=\"[MissingPlans]\",\n",
    "        num_cores=6\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution completed in {duration:.2f} seconds\")\n",
    "    \n",
    "    # Create a DataFrame from the execution results\n",
    "    missing_results_df = pd.DataFrame([\n",
    "        {\"Plan\": plan, \"Success\": success, \"Execution Type\": \"Missing Plans\"}\n",
    "        for plan, success in execution_results.items()\n",
    "    ])\n",
    "    \n",
    "    # Sort by plan number\n",
    "    missing_results_df = missing_results_df.sort_values(\"Plan\")\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\nExecution Results for Plans Without HDF Results:\")\n",
    "    display.display(missing_results_df)\n",
    "    \n",
    "    # Check the test folder\n",
    "    test_folder = script_dir / f\"{ras.project_name} [MissingPlans]\"\n",
    "    if test_folder.exists():\n",
    "        print(f\"\\nTest folder exists: {test_folder}\")\n",
    "        \n",
    "        # Check for results\n",
    "        hdf_files = list(test_folder.glob(\"*.p*.hdf\"))\n",
    "        if hdf_files:\n",
    "            print(f\"Found {len(hdf_files)} HDF result files:\")\n",
    "            for file in hdf_files:\n",
    "                file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "                print(f\"  {file.name}: {file_size:.1f} MB\")\n",
    "        else:\n",
    "            print(\"No HDF result files found in the test folder\")\n",
    "else:\n",
    "    print(\"No plans without results to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Results\n",
    "After executing the plans that were missing HDF results, it's important to verify that the results were properly generated. Let's check if the execution actually created the expected output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the project with the test folder to see updated results\n",
    "missing_plans_folder = script_dir / f\"{ras.project_name} [MissingPlans]\"\n",
    "\n",
    "if missing_plans_folder.exists():\n",
    "    # Initialize the project from the test folder\n",
    "    test_ras = init_ras_project(missing_plans_folder, \"6.6\")\n",
    "    \n",
    "    # Check which plans now have results\n",
    "    plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]['plan_number'].tolist()\n",
    "    \n",
    "    print(f\"Plans with results after execution: {', '.join(plans_with_results)}\")\n",
    "    \n",
    "    # Verify if all previously missing plans now have results\n",
    "    all_generated = all(plan in plans_with_results for plan in plans_no_results)\n",
    "    \n",
    "    if all_generated:\n",
    "        print(\" Successfully generated results for all missing plans\")\n",
    "    else:\n",
    "        print(\" Some plans still don't have results after execution\")\n",
    "        missing_after = [plan for plan in plans_no_results if plan not in plans_with_results]\n",
    "        print(f\"Plans still missing results: {', '.join(missing_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Plan Specification Techniques\n",
    "\n",
    "In this notebook, we've explored different ways to specify and execute HEC-RAS plans using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
    "\n",
    "1. **Basic Plan Specification**\n",
    "   - Single plan by number: `\"01\"`\n",
    "   - List of specific plans: `[\"01\", \"03\"]`\n",
    "   - All plans: `ras.plan_df['plan_number'].tolist()`\n",
    "\n",
    "2. **Advanced Selection**\n",
    "   - Categorization: Grouping plans by purpose or type\n",
    "   - Dependencies: Ensuring prerequisite plans are run first\n",
    "   - Ordered execution: Running plans in a specific sequence\n",
    "\n",
    "3. **Run Plans with Missing Results (HDF)**\n",
    "   - Using ras object to determine which plans have results\n",
    "   - Creating a list of plans with no results\n",
    "   - Running those plans sequentially\n",
    "\n",
    "4. NOTE: run_parallel can also run a list of plans, but compute_plan is only made for single plan execution.  \n",
    "\n",
    "\n",
    "### Best Practices for Plan Specification\n",
    "\n",
    "1. **Consistent Formatting**: Use two-digit strings for plan numbers (\"01\" instead of 1)\n",
    "2. **Descriptive Naming**: Use meaningful short identifiers that describe the plan's purpose\n",
    "3. **Verify Availability**: Check that specified plans exist before trying to execute them\n",
    "4. **Document Dependencies**: Keep track of which plans depend on others\n",
    "5. **Use Appropriate Execution Method**: Choose sequential or parallel based on dependencies and resources\n",
    "6. **Monitor Performance**: Track execution times to identify optimization opportunities\n",
    "\n",
    "By applying these techniques, you can create efficient and organized workflows for executing HEC-RAS plans, from simple batch processing to complex dependency-based execution sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\07_sequential_plan_execution.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Sequential Plan Execution\n",
    "\n",
    "This notebook demonstrates how to sequentially execute multiple HEC-RAS plans using the RAS Commander library. Sequential execution is useful for batch processing plans that need to be run in a specific order or when you want to ensure consistent resource usage across multiple runs.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project by specifying the project path and version\n",
    "2. **Sequential Execution of All Plans**: Run all plans in a project sequentially in a test folder\n",
    "3. **Selective Plan Execution**: Run only specific plans in sequence\n",
    "4. **Geometry Preprocessor Management**: Clear geometry preprocessor files before execution\n",
    "5. **Execution Result Analysis**: Track and analyze the results of sequential executions\n",
    "6. **Performance Monitoring**: Monitor and compare execution times across different runs\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define examples_dir as parent of bald_eagle_path\n",
    "examples_dir = bald_eagle_path.parent\n",
    "print(f\"Examples directory set to: {examples_dir}\")\n",
    "\n",
    "    \n",
    "# Remove any compute test folders from previous runs\n",
    "for folder in examples_dir.glob(\"*[[]AllSequential[]]*\"):\n",
    "    if folder.is_dir():\n",
    "        print(f\"Removing existing test folder: {folder}\")\n",
    "        shutil.rmtree(folder)\n",
    "        \n",
    "for folder in examples_dir.glob(\"*[[]SpecificSequential*[]]*\"):\n",
    "    if folder.is_dir():\n",
    "        print(f\"Removing existing test folder: {folder}\")\n",
    "        shutil.rmtree(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Sequential Execution in HEC-RAS\n",
    "\n",
    "HEC-RAS simulations can be executed in several ways:\n",
    "\n",
    "1. **Single Plan Execution**: Run one plan at a time using `RasCmdr.compute_plan()`\n",
    "2. **Sequential Execution**: Run multiple plans one after another using `RasCmdr.compute_test_mode()`\n",
    "3. **Parallel Execution**: Run multiple plans simultaneously using `RasCmdr.compute_parallel()`\n",
    "\n",
    "This notebook focuses on the second approach: **Sequential Execution**. Here are the key benefits of sequential execution:\n",
    "\n",
    "- **Controlled Resource Usage**: By running plans one at a time, you ensure consistent resource usage\n",
    "- **Dependency Management**: When later plans depend on results from earlier plans\n",
    "- **Simplified Debugging**: Easier to identify which plan is causing an issue when they run sequentially\n",
    "- **Consistent Test Environment**: All plans run in the same isolated folder\n",
    "\n",
    "The `compute_test_mode()` function from `RasCmdr` is specifically designed for this purpose. It creates a separate test folder, copies the project there, and executes the specified plans in sequential order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Project\n",
    "\n",
    "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HEC-RAS project\n",
    "init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized HEC-RAS project: {ras.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nHEC-RAS Project Plan Data:\")\n",
    "display.display(ras.plan_df)\n",
    "\n",
    "# Check how many plans we have\n",
    "plan_count = len(ras.plan_df)\n",
    "print(f\"Found {plan_count} plans in the project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasCmdr.compute_test_mode Method\n",
    "\n",
    "Before we start executing plans, let's understand the `compute_test_mode()` method from the `RasCmdr` class, which we'll use for sequential execution.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (str, list[str], optional): Plan number or list of plan numbers to execute. If None, all plans will be executed.\n",
    "- `dest_folder_suffix` (str, optional): Suffix to append to the test folder name. Defaults to \"[Test]\".\n",
    "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.\n",
    "- `num_cores` (int, optional): Maximum number of cores to use for each plan. If None, the current setting is not changed.\n",
    "- `ras_object` (RasPrj, optional): Specific RAS object to use. If None, uses the global ras object.\n",
    "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder if it exists. Defaults to False.\n",
    "\n",
    "### Return Value\n",
    "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Test Folder**: The function creates a separate folder with the specified suffix, copying the project there for execution.\n",
    "2. **Sequential Execution**: Plans are executed one after another in the specified order.\n",
    "3. **Geometry Preprocessor Files**: These files store precomputed hydraulic properties. Clearing them forces HEC-RAS to recompute these properties.\n",
    "4. **Destination Folder Option**: The suffix determines the name of the test folder. Unlike `compute_plan()`, you can't specify an arbitrary destination folder.\n",
    "5. **Overwrite Option**: Controls whether an existing test folder should be overwritten.\n",
    "\n",
    "Now, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sequential Execution of All Plans\n",
    "\n",
    "Let's execute all plans in the project sequentially. This will create a test folder with the suffix \"[AllSequential]\" and run all plans one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing all plans sequentially...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute all plans sequentially\n",
    "# - dest_folder_suffix: Suffix to append to the test folder name\n",
    "# - overwrite_dest: Overwrite the destination folder if it exists\n",
    "# - no ras object is specified, it will use the default \"ras\" object\n",
    "execution_results = RasCmdr.compute_test_mode(\n",
    "    dest_folder_suffix=\"[AllSequential]\",\n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Sequential execution of all plans completed in {total_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examining the Test Folder\n",
    "\n",
    "Let's examine the test folder created by `compute_test_mode()` to better understand what happened during sequential execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test folder path\n",
    "test_folder = script_dir / f\"{ras.project_name} [AllSequential]\"\n",
    "\n",
    "if test_folder.exists():\n",
    "    print(f\"Test folder exists: {test_folder}\")\n",
    "    \n",
    "    # List the key files in the test folder\n",
    "    print(\"\\nKey files in test folder:\")\n",
    "    \n",
    "    # First, list the project file and all plan files\n",
    "    prj_files = list(test_folder.glob(\"*.prj\"))\n",
    "    plan_files = list(test_folder.glob(\"*.p*\"))\n",
    "    plan_files.sort()\n",
    "    \n",
    "    if prj_files:\n",
    "        print(f\"Project file: {prj_files[0].name}\")\n",
    "    \n",
    "    print(\"Plan files:\")\n",
    "    for plan_file in plan_files:\n",
    "        file_size = plan_file.stat().st_size / 1024  # Size in KB\n",
    "        print(f\"  {plan_file.name}: {file_size:.1f} KB\")\n",
    "    \n",
    "    # Look for HDF result files\n",
    "    hdf_files = list(test_folder.glob(\"*.hdf\"))\n",
    "    hdf_files.sort()\n",
    "    \n",
    "    print(\"\\nHDF files:\")\n",
    "    for hdf_file in hdf_files:\n",
    "        file_size = hdf_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "        print(f\"  {hdf_file.name}: {file_size:.1f} MB\")\n",
    "    \n",
    "    # Geometry preprocessor files (if any)\n",
    "    geompre_files = list(test_folder.glob(\"*.c*\"))\n",
    "    geompre_files.sort()\n",
    "    \n",
    "    if geompre_files:\n",
    "        print(\"\\nGeometry preprocessor files:\")\n",
    "        for geompre_file in geompre_files:\n",
    "            file_size = geompre_file.stat().st_size / 1024  # Size in KB\n",
    "            print(f\"  {geompre_file.name}: {file_size:.1f} KB\")\n",
    "    else:\n",
    "        print(\"\\nNo geometry preprocessor files found\")\n",
    "        \n",
    "    # Initialize a RAS project in the test folder to inspect results\n",
    "    try:\n",
    "        test_ras = init_ras_project(test_folder, ras.ras_exe_path)\n",
    "        print(\"\\nPlans with results in the test folder:\")\n",
    "        test_plans_with_results = test_ras.plan_df[test_ras.plan_df['HDF_Results_Path'].notna()]\n",
    "        display.display(test_plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing test folder as a RAS project: {e}\")\n",
    "else:\n",
    "    print(f\"Test folder not found: {test_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sequential Execution of Specific Plans\n",
    "\n",
    "Now, let's execute only specific plans in the project. We'll select plans \"01\" and \"02\" and run them sequentially with the `clear_geompre` option set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing specific plans sequentially with clearing geometry preprocessor files...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Define the plans to execute\n",
    "selected_plans = [\"01\", \"02\"]\n",
    "print(f\"Selected plans: {', '.join(selected_plans)}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute specific plans sequentially\n",
    "# - plan_number: List of plan numbers to execute\n",
    "# - dest_folder_suffix: Suffix to append to the test folder name\n",
    "# - clear_geompre: Clear geometry preprocessor files before execution\n",
    "# - overwrite_dest: Overwrite the destination folder if it exists\n",
    "execution_results = RasCmdr.compute_test_mode(\n",
    "    plan_number=selected_plans,\n",
    "    dest_folder_suffix=\"[SpecificSequentialClearGeompre]\",\n",
    "    clear_geompre=True,\n",
    "    overwrite_dest=True\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Sequential execution of specific plans completed in {total_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Sequential Plan Execution\n",
    "\n",
    "In this notebook, we've explored how to execute HEC-RAS plans sequentially using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
    "\n",
    "1. **Project Setup and Initialization**: Setting up the environment and initializing a HEC-RAS project\n",
    "2. **Example Project Management**: Using `RasExamples` to download and extract sample projects\n",
    "3. **Basic Sequential Execution**: Using `RasCmdr.compute_test_mode()` to run all plans in a project\n",
    "4. **Test Folder Analysis**: Examining the contents and results of sequential execution\n",
    "5. **Selective Plan Execution**: Running specific plans with geometry preprocessor clearing\n",
    "\n",
    "### Key Functions Used\n",
    "\n",
    "- `init_ras_project()`: Initialize a HEC-RAS project\n",
    "- `RasExamples.extract_project()`: Extract example projects for testing\n",
    "- `RasCmdr.compute_test_mode()`: Run plans sequentially in a test folder\n",
    "- `Path.glob()`: Examine test folder contents and results\n",
    "- `RasCmdr.compute_test_mode(clear_geompre=True)`: Execute plans with preprocessor clearing\n",
    "\n",
    "### Best Practices for Sequential Execution\n",
    "\n",
    "1. **Environment Setup**: Ensure all required libraries are installed and properly imported\n",
    "2. **Project Organization**: Clean up existing test folders before new executions\n",
    "3. **Resource Management**: Monitor system resources (CPU cores, memory) for optimal performance\n",
    "4. **Test Folder Naming**: Use meaningful suffixes to distinguish different execution runs\n",
    "5. **Performance Tracking**: Monitor execution times for each sequential run\n",
    "6. **Results Verification**: Check test folders for successful plan execution and result files\n",
    "7. **Selective Execution**: Use plan filtering when only specific plans need to be run\n",
    "\n",
    "With these techniques, you can effectively manage and execute HEC-RAS simulations sequentially, whether running all plans or a selected subset with specific configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\08_parallel_execution.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Parallel Plan Execution\n",
    "\n",
    "This notebook demonstrates how to execute multiple HEC-RAS plans in parallel using the RAS Commander library. Parallel execution allows you to make better use of your computer's processing power by running multiple plans simultaneously.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Initialize a HEC-RAS project and prepare it for parallel execution\n",
    "2. **Parallel Execution of All Plans**: Run all plans in a project simultaneously\n",
    "3. **Selective Parallel Execution**: Run only specific plans in parallel\n",
    "4. **Dynamic Worker Allocation**: Automatically determine the optimal number of parallel workers\n",
    "5. **Resource Management**: Optimize CPU core utilization for parallel runs\n",
    "6. **Results Comparison**: Analyze and visualize execution performance\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import math  # Import math to avoid NameError in get_optimal_worker_count function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Working Environment\n",
    "\n",
    "Let's set up our working directory and check the system resources available for parallel execution. This will help us make informed decisions about how many workers to use.\n",
    "\n",
    "For this notebook we will be using the \"Muncie\" HEC Example Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Muncie example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "muncie_path = RasExamples.extract_project(\"Muncie\")\n",
    "print(f\"Extracted project to: {muncie_path}\")  \n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {muncie_path.exists()}\")\n",
    "\n",
    "\n",
    "# Create compute folders\n",
    "compute_folder = muncie_path.parent / \"compute_test_parallel\"\n",
    "specific_compute_folder = muncie_path.parent / \"compute_test_parallel_specific\"\n",
    "dynamic_compute_folder = muncie_path.parent / \"compute_test_parallel_dynamic\"\n",
    "\n",
    "# Check system resources for parallel execution\n",
    "cpu_count = psutil.cpu_count(logical=True)  # Logical cores (including hyper-threading)\n",
    "physical_cores = psutil.cpu_count(logical=False)  # Physical cores only\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)  # Total RAM in GB\n",
    "available_memory_gb = psutil.virtual_memory().available / (1024**3)  # Available RAM in GB\n",
    "\n",
    "print(f\"System Resources:\")\n",
    "print(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores with hyper-threading)\")\n",
    "print(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
    "\n",
    "# Functions to help with resource management\n",
    "def get_optimal_worker_count(cores_per_worker=2):\n",
    "    \"\"\"Calculate the optimal number of workers based on available physical cores.\"\"\"\n",
    "    optimal_workers = math.floor(physical_cores / cores_per_worker)\n",
    "    return max(1, optimal_workers)  # Ensure at least 1 worker\n",
    "\n",
    "print(f\"\\nFor parallel HEC-RAS execution:\")\n",
    "print(f\"- With 2 cores per worker: Can use up to {get_optimal_worker_count(2)} parallel workers\")\n",
    "print(f\"- With 4 cores per worker: Can use up to {get_optimal_worker_count(4)} parallel workers\")\n",
    "print(f\"\\nEach HEC-RAS instance typically requires 2-4 GB of RAM. Based on your available memory,\")\n",
    "print(f\"you could reasonably run {math.floor(available_memory_gb / 3)} instances simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Parallel Execution in HEC-RAS\n",
    "\n",
    "HEC-RAS simulations can be computationally intensive, especially for large models or long simulation periods. Parallel execution allows you to run multiple plans simultaneously, making better use of your computer's processing power.\n",
    "\n",
    "### Key Concepts in Parallel Execution\n",
    "\n",
    "1. **Workers**: Each worker is a separate process that can execute a HEC-RAS plan. The `max_workers` parameter determines how many plans can be executed simultaneously.\n",
    "\n",
    "2. **Cores per Worker**: Each worker (HEC-RAS instance) can utilize multiple CPU cores. The `num_cores` parameter sets how many cores each worker uses.\n",
    "\n",
    "3. **Resource Balancing**: Effective parallel execution requires balancing the number of workers with the cores per worker. Too many workers or too many cores per worker can lead to resource contention and slower overall performance.\n",
    "\n",
    "4. **Worker Folders**: Each worker gets its own folder with a copy of the project, allowing for isolated execution.\n",
    "\n",
    "### Parallel vs. Sequential Execution\n",
    "\n",
    "- **Parallel**: Multiple plans run simultaneously (good for independent plans, faster overall completion)\n",
    "- **Sequential**: Plans run one after another (good for dependent plans, consistent resource usage)\n",
    "\n",
    "### Optimal Configuration\n",
    "\n",
    "The optimal configuration depends on your hardware and the specific plans you're running:\n",
    "\n",
    "- For most models, 2-4 cores per worker provides good performance\n",
    "- Set `max_workers` based on available physical cores: `max_workers = floor(physical_cores / cores_per_worker)`\n",
    "- Ensure you have enough memory: each worker typically needs 2-4 GB of RAM\n",
    "\n",
    "Now, let's download and extract our example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting Example HEC-RAS Project\n",
    "\n",
    "Let's use the `RasExamples` class to download and extract the \"Balde Eagle Creek\" example project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "Let's initialize the HEC-RAS project using the `init_ras_project()` function. We'll store the initialized object in a variable to use later, rather than relying on the global `ras` object. This approach is more suitable for working with multiple projects or compute folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the source project\n",
    "source_project = init_ras_project(muncie_path, \"6.6\")\n",
    "print(f\"Initialized source project: {source_project.project_name}\")\n",
    "\n",
    "# Display the current plan files in the project\n",
    "print(\"\\nAvailable plans in the project:\")\n",
    "display.display(source_project.plan_df)\n",
    "\n",
    "# Check how many plans we have\n",
    "plan_count = len(source_project.plan_df)\n",
    "print(f\"Found {plan_count} plans in the project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RasCmdr.compute_parallel Method\n",
    "\n",
    "Before we start executing plans in parallel, let's understand the `compute_parallel()` method from the `RasCmdr` class.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (Union[str, List[str], None]): Plan number(s) to compute. If None, all plans are computed.\n",
    "- `max_workers` (int): Maximum number of parallel workers (default: 2).\n",
    "- `num_cores` (int): Number of cores to use per plan computation (default: 2).\n",
    "- `clear_geompre` (bool): Whether to clear geometry preprocessor files (default: False).\n",
    "- `ras_object` (Optional[RasPrj]): Specific RAS object to use. If None, uses global ras instance.\n",
    "- `dest_folder` (Union[str, Path, None]): Destination folder for computed results.\n",
    "- `overwrite_dest` (bool): Whether to overwrite existing destination folder (default: False).\n",
    "\n",
    "### Return Value\n",
    "- `Dict[str, bool]`: Dictionary of plan numbers and their execution success status.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Worker Assignment**: Plans are assigned to workers in a round-robin fashion. For example, with 3 workers and 5 plans, workers would be assigned as follows: Worker 1: Plans 1 & 4, Worker 2: Plans 2 & 5, Worker 3: Plan 3.\n",
    "\n",
    "2. **Worker Folders**: Each worker gets its own folder (a subdirectory of the destination folder) for isolated execution.\n",
    "\n",
    "3. **Result Consolidation**: After all plans are executed, results are consolidated into the destination folder.\n",
    "\n",
    "4. **Resource Management**: Each worker can use multiple cores as specified by `num_cores`.\n",
    "\n",
    "Now, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parallel Execution of All Plans\n",
    "\n",
    "Let's execute all plans in the project in parallel. We'll use 3 workers, with 2 cores per worker. This approach is good when you have multiple plans that are independent of each other and you want to complete them as quickly as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing all plans in parallel...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Create compute folder if it doesn't exist\n",
    "compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the parameters for parallel execution\n",
    "max_workers = 4\n",
    "cores_per_worker = 1\n",
    "\n",
    "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
    "print(f\"Destination folder: {compute_folder}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute all plans in parallel\n",
    "results_all = RasCmdr.compute_parallel(\n",
    "    max_workers=max_workers,\n",
    "    num_cores=cores_per_worker,\n",
    "    dest_folder=compute_folder,\n",
    "    overwrite_dest=True,\n",
    "    ras_object=source_project\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "print(f\"Parallel execution of all plans completed in {total_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in results_all.items()\n",
    "])\n",
    "\n",
    "# Sort by plan number\n",
    "results_df = results_df.sort_values(\"Plan\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examining the Parallel Execution Results\n",
    "\n",
    "Let's initialize a RAS project in the compute folder and examine the results of the parallel execution. This will help us understand what happened during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a RAS project in the compute folder\n",
    "compute_project = init_ras_project(compute_folder, \"6.6\")\n",
    "print(f\"Initialized compute project: {compute_project.project_name}\")\n",
    "\n",
    "# Display the plan files in the compute folder\n",
    "print(\"\\nPlans in the compute folder:\")\n",
    "display.display(compute_project.plan_df)\n",
    "\n",
    "# Check which plans have results\n",
    "plans_with_results = compute_project.plan_df[compute_project.plan_df['HDF_Results_Path'].notna()]\n",
    "print(f\"\\nFound {len(plans_with_results)} plans with results:\")\n",
    "display.display(plans_with_results[['plan_number', 'HDF_Results_Path']])\n",
    "\n",
    "# List the worker folders (they should have been removed during results consolidation)\n",
    "worker_folders = list(compute_folder.glob(\"*Worker*\"))\n",
    "if worker_folders:\n",
    "    print(f\"\\nFound {len(worker_folders)} worker folders:\")\n",
    "    for folder in worker_folders:\n",
    "        print(f\"  {folder.name}\")\n",
    "else:\n",
    "    print(\"\\nNo worker folders remain in the compute folder (they were removed during results consolidation)\")\n",
    "\n",
    "# Check for HDF result files\n",
    "hdf_files = list(compute_folder.glob(\"*.hdf\"))\n",
    "hdf_files.sort()\n",
    "\n",
    "print(f\"\\nFound {len(hdf_files)} HDF files in the compute folder:\")\n",
    "for file in hdf_files:\n",
    "    file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  {file.name}: {file_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Examples: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution of Specific Plans\n",
    "\n",
    "Now, let's execute only specific plans in the project in parallel. This approach is useful when you only want to run a subset of the available plans, perhaps for testing or comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing specific plans in parallel...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Create specific compute folder if it doesn't exist\n",
    "specific_compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the plans to execute\n",
    "specific_plans = [\"01\", \"03\"]\n",
    "print(f\"Selected plans: {', '.join(specific_plans)}\")\n",
    "\n",
    "# Define the parameters for parallel execution\n",
    "max_workers = 2  # One for each plan\n",
    "cores_per_worker = 2\n",
    "\n",
    "print(f\"Using {max_workers} parallel workers, each with {cores_per_worker} cores\")\n",
    "print(f\"Destination folder: {specific_compute_folder}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute specific plans in parallel\n",
    "results_specific = RasCmdr.compute_parallel(\n",
    "    plan_number=specific_plans,\n",
    "    max_workers=max_workers,\n",
    "    num_cores=cores_per_worker,\n",
    "    dest_folder=specific_compute_folder,\n",
    "    overwrite_dest=True,\n",
    "    ras_object=source_project\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "specific_duration = end_time - start_time\n",
    "\n",
    "print(f\"Parallel execution of specific plans completed in {specific_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "specific_results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in results_specific.items()\n",
    "])\n",
    "\n",
    "# Sort by plan number\n",
    "specific_results_df = specific_results_df.sort_values(\"Plan\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(specific_results_df)\n",
    "\n",
    "# Initialize a RAS project in the specific compute folder\n",
    "specific_compute_project = init_ras_project(specific_compute_folder, \"6.6\")\n",
    "print(f\"\\nInitialized specific compute project: {specific_compute_project.project_name}\")\n",
    "\n",
    "# Check which plans have results\n",
    "specific_plans_with_results = specific_compute_project.plan_df[specific_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
    "print(f\"Found {len(specific_plans_with_results)} plans with results:\")\n",
    "display.display(specific_plans_with_results[['plan_number', 'HDF_Results_Path']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution with Max Workers Defined by Physical Cores (\"Dynamic Worker Allocation\") \n",
    "\n",
    "In this step, we'll determine the optimal number of workers based on the physical cores available on the system. This approach ensures that we make efficient use of the available hardware without overcommitting resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing plans with dynamic worker allocation...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Create dynamic compute folder if it doesn't exist\n",
    "dynamic_compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the cores per worker\n",
    "cores_per_worker = 4\n",
    "# 2 cores per worker is the efficiency point for most CPU's, due to L2/L3 cache being shared by 2 cores in most x86 CPU's\n",
    "# 4-8 cores per worker is the maximum performance point for most CPU's, using more compute power to marginally lower runtime \n",
    "# when using parallel compute, 2 cores per worker is typically optimal as it is assumed you are maximizing throughput (efficency) over single-plan runtime (performance)\n",
    "\n",
    "# Calculate the optimal number of workers based on physical cores\n",
    "max_workers = get_optimal_worker_count(cores_per_worker)\n",
    "print(f\"System has {physical_cores} physical cores\")\n",
    "print(f\"With {cores_per_worker} cores per worker, optimal worker count is {max_workers}\")\n",
    "print(f\"Destination folder: {dynamic_compute_folder}\")\n",
    "\n",
    "# Record start time for performance measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute all plans with dynamic worker allocation\n",
    "results_dynamic = RasCmdr.compute_parallel(\n",
    "    plan_number=specific_plans,\n",
    "    max_workers=max_workers,\n",
    "    num_cores=cores_per_worker,\n",
    "    dest_folder=dynamic_compute_folder,\n",
    "    overwrite_dest=True,\n",
    "    ras_object=source_project\n",
    ")\n",
    "\n",
    "# Record end time and calculate duration\n",
    "end_time = time.time()\n",
    "dynamic_duration = end_time - start_time\n",
    "\n",
    "print(f\"Parallel execution with dynamic worker allocation completed in {dynamic_duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "dynamic_results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success}\n",
    "    for plan, success in results_dynamic.items()\n",
    "])\n",
    "\n",
    "# Sort by plan number\n",
    "dynamic_results_df = dynamic_results_df.sort_values(\"Plan\")\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(dynamic_results_df)\n",
    "\n",
    "# Initialize a RAS project in the dynamic compute folder\n",
    "dynamic_compute_project = init_ras_project(dynamic_compute_folder, \"6.6\")\n",
    "print(f\"\\nInitialized dynamic compute project: {dynamic_compute_project.project_name}\")\n",
    "\n",
    "# Check which plans have results\n",
    "dynamic_plans_with_results = dynamic_compute_project.plan_df[dynamic_compute_project.plan_df['HDF_Results_Path'].notna()]\n",
    "print(f\"Found {len(dynamic_plans_with_results)} plans with results:\")\n",
    "display.display(dynamic_plans_with_results[['plan_number', 'HDF_Results_Path']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of the different parallel execution approaches we've tried. This will help us understand the impact of worker count and plan selection on execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for individual plan runtimes\n",
    "plan_data = []\n",
    "\n",
    "# Define the approaches with more descriptive labels including worker and core counts\n",
    "approach_labels = {\n",
    "    \"all_plans\": \"All Plans (2 workers  2 cores = 4 cores total)\",\n",
    "    \"specific_plans\": \"Specific Plans (1 worker  2 cores = 2 cores total)\",\n",
    "    \"dynamic_workers\": f\"Dynamic Workers (1 worker  4 cores = 4 cores total)\"\n",
    "}\n",
    "\n",
    "# Extract runtimes from the log messages\n",
    "# For all plans approach\n",
    "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"01\", \"Runtime\": 35.72})\n",
    "plan_data.append({\"Approach\": approach_labels[\"all_plans\"], \"Plan\": \"03\", \"Runtime\": 82.70})\n",
    "# Omitting plan 04 as it's a 1D model\n",
    "\n",
    "# For specific plans approach (plans 01 and 03 were run)\n",
    "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"01\", \"Runtime\": 29.10})\n",
    "plan_data.append({\"Approach\": approach_labels[\"specific_plans\"], \"Plan\": \"03\", \"Runtime\": 36.09})\n",
    "\n",
    "# For dynamic worker approach (plans 01 and 03 were run)\n",
    "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"01\", \"Runtime\": 28.48})\n",
    "plan_data.append({\"Approach\": approach_labels[\"dynamic_workers\"], \"Plan\": \"03\", \"Runtime\": 49.43})\n",
    "\n",
    "# Create a DataFrame\n",
    "plan_runtime_df = pd.DataFrame(plan_data)\n",
    "\n",
    "# Create a grouped bar chart for plan runtimes\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Get all unique plan numbers and ensure they're sorted\n",
    "plans = sorted(plan_runtime_df[\"Plan\"].unique())\n",
    "\n",
    "# Create x positions for the bars\n",
    "x = np.arange(len(plans))\n",
    "width = 0.25  # Width of the bars\n",
    "\n",
    "# Plot bars for each approach\n",
    "approaches = plan_runtime_df[\"Approach\"].unique()\n",
    "for i, approach in enumerate(approaches):\n",
    "    # Filter data for this approach\n",
    "    approach_data = plan_runtime_df[plan_runtime_df[\"Approach\"] == approach]\n",
    "    \n",
    "    # Initialize runtimes array with NaN values\n",
    "    runtimes = [np.nan] * len(plans)\n",
    "    \n",
    "    # Fill in runtimes where data exists\n",
    "    for j, plan in enumerate(plans):\n",
    "        plan_runtime = approach_data[approach_data[\"Plan\"] == plan][\"Runtime\"]\n",
    "        if not plan_runtime.empty:\n",
    "            runtimes[j] = plan_runtime.values[0]\n",
    "    \n",
    "    # Create bars for this approach (only where we have data)\n",
    "    valid_indices = [idx for idx, val in enumerate(runtimes) if not np.isnan(val)]\n",
    "    valid_plans = [plans[idx] for idx in valid_indices]\n",
    "    valid_runtimes = [runtimes[idx] for idx in valid_indices]\n",
    "    valid_positions = [x[idx] + (i - len(approaches)/2 + 0.5) * width for idx in valid_indices]\n",
    "    \n",
    "    # Plot the bars\n",
    "    bars = plt.bar(valid_positions, valid_runtimes, width, label=approach)\n",
    "    \n",
    "    # Add runtime labels on top of bars\n",
    "    for pos, runtime in zip(valid_positions, valid_runtimes):\n",
    "        plt.text(pos, runtime + 2, f\"{runtime:.1f}s\", ha='center', va='bottom')\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels\n",
    "plt.xlabel('Plan Number', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.title('Runtime Comparison by Plan Number and Parallelization Approach', fontsize=14)\n",
    "plt.xticks(x, plans, fontsize=11)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add note about omitting Plan 04\n",
    "plt.figtext(0.5, 0.01, \"\\nNote: Plan 04 (1D model) is omitted from this comparison\", \n",
    "            ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# Ensure all plan numbers show on x-axis regardless of data availability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Parallel Plan Execution\n",
    "\n",
    "In this notebook, we've explored how to execute HEC-RAS plans in parallel using the RAS Commander library. Here's a summary of the key techniques we've covered:\n",
    "\n",
    "1. **Basic Parallel Execution**: Using `RasCmdr.compute_parallel()` to run all plans in a project simultaneously\n",
    "2. **Selective Parallel Execution**: Running only specific plans in parallel\n",
    "3. **Dynamic Worker Allocation**: Determining the optimal number of workers based on available system resources\n",
    "4. **Performance Analysis**: Comparing execution times for different parallel configurations\n",
    "5. **Advanced Parallel Workflows**: Building complex workflows with parallel execution for sensitivity analysis\n",
    "\n",
    "### Key Functions Used\n",
    "\n",
    "- `RasCmdr.compute_parallel()`: Execute multiple plans in parallel\n",
    "- `RasPlan.clone_plan()`: Create a new plan based on an existing one\n",
    "- `RasPlan.update_plan_description()`: Update the description of a plan\n",
    "- `RasPlan.set_num_cores()`: Set the number of cores for a plan to use\n",
    "- `RasPlan.get_results_path()`: Get the path to the results file for a plan\n",
    "\n",
    "### Best Practices for Parallel Execution\n",
    "\n",
    "1. **Use Separate RAS Objects**: Create and use separate RAS objects for different projects or folders\n",
    "2. **Balance Workers and Cores**: Find the right balance between the number of workers and cores per worker\n",
    "3. **Consider Hardware Limits**: Be mindful of your system's physical cores and memory\n",
    "4. **Use Clean Compute Folders**: Use the `dest_folder` parameter to keep your project organized\n",
    "5. **Handle Overwrite Carefully**: Use `overwrite_dest=True` for repeatable workflows, but be cautious about losing results\n",
    "6. **Monitor Performance**: Track execution times and adjust your configuration for optimal performance\n",
    "7. **Match Workers to Plans**: For best results, use one worker per plan when running a small number of plans\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\09_plan_parameter_operations.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS Commander: Plan Key Operations\n",
    "\n",
    "This notebook demonstrates how to perform key operations on HEC-RAS plan files using the RAS Commander library. Plan files in HEC-RAS (`.p*` files) control the simulation settings and parameters, making them essential for hydraulic modeling workflows.\n",
    "\n",
    "## Operations Covered\n",
    "\n",
    "1. **Project Initialization**: Set up a HEC-RAS project for automation\n",
    "2. **Plan Values**: Retrieve specific values from plan files\n",
    "3. **Run Flags**: Configure which components (geometry preprocessor, unsteady flow, etc.) will run\n",
    "4. **Plan Intervals**: Set computation and output time intervals\n",
    "5. **Plan Descriptions**: Read and update plan descriptions\n",
    "6. **Simulation Dates**: Modify simulation start and end dates\n",
    "\n",
    "These operations allow you to programmatically control and customize HEC-RAS simulations without opening the GUI, which is especially useful for batch processing, sensitivity analysis, and model calibration.\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "\n",
    "# Import all ras-commander modules\n",
    "from ras_commander import *\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Bald Eagle Creek example project\n",
    "# The extract_project method downloads the project from GitHub if not already present,\n",
    "# and extracts it to the example_projects folder\n",
    "bald_eagle_path = RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "print(f\"Extracted project to: {bald_eagle_path}\")  \n",
    "\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Plan Files in HEC-RAS\n",
    "\n",
    "Before we dive into the operations, let's understand what HEC-RAS plan files are and why they're important:\n",
    "\n",
    "### What is a Plan File?\n",
    "\n",
    "A HEC-RAS plan file (`.p*`) is a configuration file that defines how a hydraulic simulation will run. It links together:\n",
    "\n",
    "1. **Geometry**: River channel and floodplain physical characteristics (`.g*` files)\n",
    "2. **Flow Data**: Inflow conditions, either steady (`.f*`) or unsteady (`.u*`)\n",
    "3. **Simulation Parameters**: Time steps, computational methods, and output settings\n",
    "\n",
    "### Key Components of Plan Files\n",
    "\n",
    "Plan files contain many parameters that control simulation behavior:\n",
    "\n",
    "- **Simulation Type**: Steady, unsteady, sediment transport, water quality\n",
    "- **Computation Intervals**: Time steps for calculations\n",
    "- **Output Intervals**: How frequently results are saved\n",
    "- **Run Flags**: Which modules to execute (preprocessor, postprocessor, etc.)\n",
    "- **Simulation Period**: Start and end dates for unsteady simulations\n",
    "- **Computation Methods**: Numerical schemes and solver settings\n",
    "- **Resource Allocation**: Number of CPU cores to use\n",
    "\n",
    "### Why Automate Plan Operations?\n",
    "\n",
    "Automating plan operations with RAS Commander allows you to:\n",
    "\n",
    "1. **Batch Processing**: Run multiple scenarios with different parameters\n",
    "2. **Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
    "3. **Calibration**: Adjust parameters to match observed data\n",
    "4. **Consistency**: Ensure standardized settings across multiple models\n",
    "5. **Documentation**: Programmatically track simulation configurations\n",
    "\n",
    "Now, let's download and extract an example project to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RasExamples instance\n",
    "ras_examples = RasExamples()\n",
    "\n",
    "# Extract the Bald Eagle Creek example project\n",
    "extracted_paths = ras_examples.extract_project([\"Balde Eagle Creek\"])\n",
    "print(f\"Extracted project to: {extracted_paths}\")\n",
    "\n",
    "# Verify the path exists\n",
    "print(f\"Bald Eagle Creek project exists: {bald_eagle_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Initialization\n",
    "\n",
    "The first step in any RAS Commander workflow is initializing the HEC-RAS project. This connects the Python environment to the HEC-RAS project files.\n",
    "\n",
    "The `init_ras_project()` function does the following:\n",
    "\n",
    "1. Locates the main project file (`.prj`)\n",
    "2. Reads all associated files (plans, geometries, flows)\n",
    "3. Creates dataframes containing project components\n",
    "4. Sets up the connection to the HEC-RAS executable\n",
    "\n",
    "Let's initialize our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the project\n",
    "ras_obj = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "print(f\"Initialized project: {ras_obj.project_name}\")\n",
    "\n",
    "# Display basic project information\n",
    "print(\"\\nProject Overview:\")\n",
    "print(f\"Project Folder: {ras_obj.project_folder}\")\n",
    "print(f\"Project File: {ras_obj.prj_file}\")\n",
    "print(f\"Number of Plan Files: {len(ras_obj.plan_df)}\")\n",
    "print(f\"Number of Geometry Files: {len(ras_obj.geom_df)}\")\n",
    "print(f\"Number of Flow Files: {len(ras_obj.flow_df)}\")\n",
    "print(f\"Number of Unsteady Files: {len(ras_obj.unsteady_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the plan files in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the plan files\n",
    "print(\"Plan Files in Project:\")\n",
    "display.display(ras_obj.plan_df[['plan_number', 'Plan Title', 'Short Identifier', 'Geom File']])\n",
    "\n",
    "# Get the first plan number for our examples\n",
    "plan_number = ras_obj.plan_df['plan_number'].iloc[0]\n",
    "print(f\"\\nWe'll work with Plan: {plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Retrieving Plan Values\n",
    "\n",
    "The `RasPlan.get_plan_value()` method allows you to retrieve specific values from a plan file. This is useful for checking current settings before making changes or for extracting information for analysis.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `key` (str): The specific parameter to retrieve (e.g., \"Computation Interval\")\n",
    "- `ras_object` (RasPrj, optional): The RAS project object (defaults to global `ras`)\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- `Computation Interval`: Time step for calculations (e.g., \"5SEC\", \"1MIN\")\n",
    "- `Short Identifier`: Brief name/ID for the plan\n",
    "- `Simulation Date`: Start and end dates for simulation\n",
    "- `UNET D1 Cores`: Number of processor cores to use\n",
    "- `Plan Title`: Full title of the plan\n",
    "- `Geom File`: Associated geometry file\n",
    "- `Flow File`: Associated flow file (for steady flow)\n",
    "- `Unsteady File`: Associated unsteady flow file\n",
    "- `Friction Slope Method`: Method for calculating friction slopes\n",
    "- `Run HTab`: Whether to run the geometry preprocessor\n",
    "- `UNET Use Existing IB Tables`: Whether to use existing internal boundary tables\n",
    "\n",
    "Let's retrieve some key values from our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keys to check\n",
    "keys_to_check = [\n",
    "    'Short Identifier', \n",
    "    'Plan Title',\n",
    "    'Computation Interval', \n",
    "    'Simulation Date', \n",
    "    'UNET D1 Cores',\n",
    "    'Geom File',\n",
    "    'Friction Slope Method'\n",
    "]\n",
    "\n",
    "# Retrieve and display the initial values\n",
    "print(\"Initial Plan Values:\")\n",
    "initial_values = {}\n",
    "for key in keys_to_check:\n",
    "    value = RasPlan.get_plan_value(plan_number, key, ras_object=ras_obj)\n",
    "    initial_values[key] = value\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV NOTE: NEED TO REVIEW # OF CORES LOGIC TO ENSURE IT IS UPDATED AND APPLIES TO 1D/2D and PIPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Updating Run Flags\n",
    "\n",
    "Run flags in HEC-RAS control which components of the simulation are executed. The `RasPlan.update_run_flags()` method allows you to modify these flags programmatically.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `geometry_preprocessor` (bool, optional): Whether to run the geometry preprocessor\n",
    "- `unsteady_flow_simulation` (bool, optional): Whether to run the unsteady flow simulation\n",
    "- `run_sediment` (bool, optional): Whether to run sediment transport calculations\n",
    "- `post_processor` (bool, optional): Whether to run the post-processor\n",
    "- `floodplain_mapping` (bool, optional): Whether to run floodplain mapping\n",
    "- `ras_object` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Common Run Flags\n",
    "\n",
    "1. **Geometry Preprocessor**: Computes hydraulic tables from geometry data\n",
    "   - `True`: Recompute tables (useful after geometry changes)\n",
    "   - `False`: Use existing tables (faster but may be outdated)\n",
    "\n",
    "2. **Unsteady Flow Simulation**: The main hydraulic calculations\n",
    "   - `True`: Run unsteady flow calculations\n",
    "   - `False`: Skip unsteady flow calculations\n",
    "\n",
    "3. **Sediment Transport**: Simulates erosion and deposition\n",
    "   - `True`: Calculate sediment transport\n",
    "   - `False`: Skip sediment transport\n",
    "\n",
    "4. **Post-Processor**: Calculates additional variables from results\n",
    "   - `True`: Run post-processing (recommended)\n",
    "   - `False`: Skip post-processing (faster but fewer outputs)\n",
    "\n",
    "5. **Floodplain Mapping**: Generates inundation maps\n",
    "   - `True`: Generate maps (requires terrain data)\n",
    "   - `False`: Skip mapping (faster)\n",
    "\n",
    "Let's update the run flags for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update run flags for the plan\n",
    "print(f\"Updating run flags for plan {plan_number}...\")\n",
    "RasPlan.update_run_flags(\n",
    "    plan_number,\n",
    "    geometry_preprocessor=True,      # Force recalculation of hydraulic tables\n",
    "    unsteady_flow_simulation=True,   # Run the main hydraulic calculations\n",
    "    run_sediment=False,              # Skip sediment transport calculations\n",
    "    post_processor=True,             # Run post-processing for additional outputs\n",
    "    floodplain_mapping=False,        # Skip floodplain mapping\n",
    "    ras_object=ras_obj\n",
    ")\n",
    "print(\"Run flags updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Updating Plan Intervals\n",
    "\n",
    "Time intervals in HEC-RAS control the temporal resolution of simulations and outputs. The `RasPlan.update_plan_intervals()` method allows you to modify these intervals.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `computation_interval` (str, optional): Time step for calculations\n",
    "- `output_interval` (str, optional): Time step for saving detailed results\n",
    "- `instantaneous_interval` (str, optional): Time step for peak value calculations\n",
    "- `mapping_interval` (str, optional): Time step for map outputs\n",
    "- `ras_object` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Valid Interval Values\n",
    "\n",
    "Time intervals must be specified in HEC-RAS format:\n",
    "- Seconds: `1SEC`, `2SEC`, `3SEC`, `4SEC`, `5SEC`, `6SEC`, `10SEC`, `15SEC`, `20SEC`, `30SEC`\n",
    "- Minutes: `1MIN`, `2MIN`, `3MIN`, `4MIN`, `5MIN`, `6MIN`, `10MIN`, `15MIN`, `20MIN`, `30MIN`\n",
    "- Hours: `1HOUR`, `2HOUR`, `3HOUR`, `4HOUR`, `6HOUR`, `8HOUR`, `12HOUR`\n",
    "- Days: `1DAY`\n",
    "\n",
    "### Interval Types\n",
    "\n",
    "1. **Computation Interval**: Time step used for hydraulic calculations\n",
    "   - Smaller intervals: More accurate but slower\n",
    "   - Larger intervals: Faster but may introduce numerical errors\n",
    "   - Rule of thumb: Should be small enough to capture flow changes\n",
    "\n",
    "2. **Output Interval**: How frequently detailed results are saved\n",
    "   - Smaller intervals: More detailed results but larger files\n",
    "   - Larger intervals: Smaller files but less temporal resolution\n",
    "   - Usually larger than computation interval\n",
    "\n",
    "3. **Instantaneous Interval**: Time step for peak value calculations\n",
    "   - Affects when max/min values are checked\n",
    "   - Usually equal to output interval\n",
    "\n",
    "4. **Mapping Interval**: How frequently map data is saved\n",
    "   - Affects animation smoothness and file size\n",
    "   - Usually larger than output interval\n",
    "\n",
    "Let's update the intervals for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update plan intervals\n",
    "print(f\"Updating intervals for plan {plan_number}...\")\n",
    "RasPlan.update_plan_intervals(\n",
    "    plan_number,\n",
    "    computation_interval=\"5SEC\",    # 5-second time step for calculations\n",
    "    output_interval=\"1MIN\",         # Save detailed results every minute\n",
    "    instantaneous_interval=\"5MIN\",  # Check for max/min values every 5 minutes\n",
    "    mapping_interval=\"15MIN\",       # Save map data every 15 minutes\n",
    "    ras_object=ras_obj\n",
    ")\n",
    "print(\"Plan intervals updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Managing Plan Descriptions\n",
    "\n",
    "Plan descriptions provide documentation for simulation configurations. The RAS Commander library offers methods to read and update these descriptions.\n",
    "\n",
    "### Reading Descriptions\n",
    "\n",
    "The `RasPlan.read_plan_description()` method retrieves the current description from a plan file.\n",
    "\n",
    "#### Parameters\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `ras_object` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Updating Descriptions\n",
    "\n",
    "The `RasPlan.update_plan_description()` method sets a new description for a plan file.\n",
    "\n",
    "#### Parameters\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `description` (str): The new description text\n",
    "- `ras_object` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Best Practices for Plan Descriptions\n",
    "\n",
    "Effective plan descriptions should include:\n",
    "1. Purpose of the simulation\n",
    "2. Key parameters and settings\n",
    "3. Date of creation or modification\n",
    "4. Author or organization\n",
    "5. Any special considerations or notes\n",
    "\n",
    "Let's read the current description and then update it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the current plan description\n",
    "current_description = RasPlan.read_plan_description(plan_number, ras_object=ras_obj)\n",
    "print(f\"Current plan description:\\n{current_description}\")\n",
    "\n",
    "# Create a new description with detailed information\n",
    "new_description = f\"\"\"Modified Plan for RAS Commander Testing\n",
    "Date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "Purpose: Demonstrating RAS Commander plan operations\n",
    "Settings:\n",
    "- Computation Interval: 5SEC\n",
    "- Output Interval: 1MIN\n",
    "- Mapping Interval: 15MIN\n",
    "- Geometry Preprocessor: Enabled\n",
    "- Post-Processor: Enabled\n",
    "Notes: This plan was automatically modified using ras-commander.\"\"\"\n",
    "\n",
    "# Update the plan description\n",
    "print(\"\\nUpdating plan description...\")\n",
    "RasPlan.update_plan_description(plan_number, new_description, ras_object=ras_obj)\n",
    "print(\"Plan description updated successfully\")\n",
    "\n",
    "# Verify the updated description\n",
    "updated_description = RasPlan.read_plan_description(plan_number, ras_object=ras_obj)\n",
    "print(f\"\\nUpdated plan description:\\n{updated_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Updating Simulation Dates\n",
    "\n",
    "For unsteady flow simulations, the simulation period defines the time window for the analysis. The `RasPlan.update_simulation_date()` method allows you to modify this period.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number_or_path` (str or Path): The plan number or full path to the plan file\n",
    "- `start_date` (datetime): The start date and time for the simulation\n",
    "- `end_date` (datetime): The end date and time for the simulation\n",
    "- `ras_object` (RasPrj, optional): The RAS project object\n",
    "\n",
    "### Considerations for Simulation Dates\n",
    "\n",
    "1. **Hydrograph Coverage**: The simulation period should fully encompass your hydrographs\n",
    "2. **Warm-Up Period**: Include time before the main event for model stabilization\n",
    "3. **Cool-Down Period**: Include time after the main event for complete drainage\n",
    "4. **Computational Efficiency**: Avoid unnecessarily long periods to reduce runtime\n",
    "5. **Consistency**: Ensure dates match available boundary condition data\n",
    "\n",
    "Let's update the simulation dates for our plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current simulation date\n",
    "current_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\", ras_object=ras_obj)\n",
    "print(f\"Current simulation date: {current_sim_date}\")\n",
    "\n",
    "# Parse the current simulation date string\n",
    "current_dates = current_sim_date.split(\",\")\n",
    "current_start = datetime.strptime(f\"{current_dates[0]},{current_dates[1]}\", \"%d%b%Y,%H%M\")\n",
    "current_end = datetime.strptime(f\"{current_dates[2]},{current_dates[3]}\", \"%d%b%Y,%H%M\")\n",
    "\n",
    "# Define new simulation period - adjust by 1 hour from current dates\n",
    "start_date = current_start + timedelta(hours=1)  # Current start + 1 hour\n",
    "end_date = current_end - timedelta(hours=1)      # Current end - 1 hour\n",
    "\n",
    "# Update the simulation date\n",
    "print(f\"\\nUpdating simulation period to: {start_date.strftime('%d%b%Y,%H%M')} - {end_date.strftime('%d%b%Y,%H%M')}\")\n",
    "RasPlan.update_simulation_date(plan_number, start_date, end_date, ras_object=ras_obj)\n",
    "print(\"Simulation dates updated successfully\")\n",
    "\n",
    "# Verify the updated simulation date\n",
    "updated_sim_date = RasPlan.get_plan_value(plan_number, \"Simulation Date\", ras_object=ras_obj)\n",
    "print(f\"\\nUpdated simulation date: {updated_sim_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verifying Updated Plan Values\n",
    "\n",
    "After making multiple changes to a plan, it's a good practice to verify that all updates were applied correctly. Let's check the updated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and display the updated values\n",
    "print(\"Updated Plan Values:\")\n",
    "updated_values = {}\n",
    "for key in keys_to_check:\n",
    "    value = RasPlan.get_plan_value(plan_number, key, ras_object=ras_obj)\n",
    "    updated_values[key] = value\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Parameter': keys_to_check,\n",
    "    'Initial Value': [initial_values.get(k, 'N/A') for k in keys_to_check],\n",
    "    'Updated Value': [updated_values.get(k, 'N/A') for k in keys_to_check]\n",
    "})\n",
    "\n",
    "print(\"\\nChanges Summary:\")\n",
    "display.display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Computing the Plan (Optional)\n",
    "\n",
    "After making changes to a plan, you might want to run the simulation to see the effects. The `RasCmdr.compute_plan()` method executes a HEC-RAS simulation with the specified plan.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- `plan_number` (str): The plan number to execute\n",
    "- `dest_folder` (str, Path, optional): Destination folder for computation\n",
    "- `ras_object` (RasPrj, optional): The RAS project object\n",
    "- `clear_geompre` (bool, optional): Whether to clear geometry preprocessor files\n",
    "- `num_cores` (int, optional): Number of processor cores to use\n",
    "- `overwrite_dest` (bool, optional): Whether to overwrite the destination folder\n",
    "\n",
    "If you want to run the simulation, you can uncomment the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEV NOTE THIS SHOULD RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the simulation with the updated plan\n",
    "\n",
    "# # Define a destination folder for the computation\n",
    "# dest_folder = script_dir / \"compute_results\"\n",
    "# print(f\"Computing plan {plan_number}...\")\n",
    "# print(f\"Results will be saved to: {dest_folder}\")\n",
    "\n",
    "# # Execute the plan\n",
    "# success = RasCmdr.compute_plan(\n",
    "#     plan_number,\n",
    "#     dest_folder=dest_folder,\n",
    "#     clear_geompre=True,    # Clear preprocessor files to ensure clean results\n",
    "#     num_cores=2,           # Use 2 processor cores\n",
    "#     overwrite_dest=True,   # Overwrite existing destination folder\n",
    "#     ras_object=ras_obj\n",
    "# )\n",
    "\n",
    "# if success:\n",
    "#     print(f\"Plan {plan_number} computed successfully\")\n",
    "#     # Check for results file\n",
    "#     results_path = RasPlan.get_results_path(plan_number, ras_object=ras_obj)\n",
    "#     if results_path:\n",
    "#         print(f\"Results saved to: {results_path}\")\n",
    "# else:\n",
    "#     print(f\"Failed to compute plan {plan_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Plan Key Operations\n",
    "\n",
    "In this notebook, we've covered the essential operations for manipulating HEC-RAS plan files programmatically using RAS Commander:\n",
    "\n",
    "1. **Project Initialization**: We initialized a HEC-RAS project using `init_ras_project()`\n",
    "2. **Plan Values**: We retrieved plan values with `RasPlan.get_plan_value()`\n",
    "3. **Run Flags**: We updated simulation components with `RasPlan.update_run_flags()`\n",
    "4. **Plan Intervals**: We modified time steps with `RasPlan.update_plan_intervals()`\n",
    "5. **Plan Descriptions**: We managed documentation with `RasPlan.read_plan_description()` and `RasPlan.update_plan_description()`\n",
    "6. **Simulation Dates**: We changed the analysis period with `RasPlan.update_simulation_date()`\n",
    "7. **Verification**: We verified our changes by comparing initial and updated values\n",
    "\n",
    "### Key Classes and Functions Used\n",
    "\n",
    "- `RasPlan`: The main class for plan operations\n",
    "  - `get_plan_value()`: Retrieve specific values from plan files\n",
    "  - `update_run_flags()`: Configure which components will run\n",
    "  - `update_plan_intervals()`: Set computation and output time intervals\n",
    "  - `read_plan_description()`: Get the current plan description\n",
    "  - `update_plan_description()`: Set a new plan description\n",
    "  - `update_simulation_date()`: Modify the simulation period\n",
    "  - `get_results_path()`: Get the path to results files\n",
    "\n",
    "- `RasCmdr`: The class for executing HEC-RAS simulations\n",
    "  - `compute_plan()`: Run a single plan simulation\n",
    "\n",
    "### Best Practices for Plan Operations\n",
    "\n",
    "1. **Verify Before Updating**: Always check current values before making changes\n",
    "2. **Document Changes**: Use descriptive plan descriptions to track modifications\n",
    "3. **Maintain Consistency**: Ensure flow data matches simulation dates\n",
    "4. **Use Appropriate Intervals**: Balance accuracy and computational efficiency\n",
    "5. **Backup Original Files**: Use destination folders when running simulations\n",
    "6. **Verify After Updates**: Confirm that all changes were applied correctly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With these plan operations, you can now:\n",
    "\n",
    "1. **Create Batch Workflows**: Process multiple scenarios with different parameters\n",
    "2. **Perform Sensitivity Analysis**: Systematically vary parameters to assess their impact\n",
    "3. **Automate Calibration**: Adjust parameters to match observed data\n",
    "4. **Build Model Ensembles**: Run multiple configurations for uncertainty analysis\n",
    "5. **Integrate with Other Tools**: Connect HEC-RAS to broader modeling frameworks\n",
    "\n",
    "These operations form the foundation for advanced HEC-RAS automation using the RAS Commander library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\101_Core_Sensitivity.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14_Core_Sensitivity.ipynb\n",
    "Testing Core Sensitivity for RAS using the Bald Eagle Creek Multi-Gage 2D project.  \n",
    "\n",
    "\n",
    "This should take around 15-45 minutes to run depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ras_commander import RasExamples, init_ras_project, RasCmdr, RasPlan, RasGeo\n",
    "\n",
    "# Step 1: Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "\n",
    "RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "# Use Path.cwd() to get the current working directory in a Jupyter Notebook\n",
    "current_directory = Path.cwd()\n",
    "project_path = current_directory / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "\n",
    "# Step 2: Initialize the Muncie Project using init_ras_project (from ras_commander)\n",
    "muncie_project = init_ras_project(project_path, \"6.6\")\n",
    "\n",
    "# Step 3: Initialize a DataFrame to store execution results\n",
    "results = []\n",
    "\n",
    "# Step 4: Run sensitivity analysis for Plan 03 with core counts 1-8\n",
    "plan_number = '03'\n",
    "print(f\"Running sensitivity analysis for Plan {plan_number}\")\n",
    "\n",
    "# Clear geompre files before running the plan\n",
    "plan_path = RasPlan.get_plan_path(plan_number)\n",
    "RasGeo.clear_geompre_files(plan_path)\n",
    "\n",
    "for cores in range(1, 5):\n",
    "    print(f\"Running with {cores} core(s)\")\n",
    "    # Set core count for this plan\n",
    "    RasPlan.set_num_cores(plan_number, cores)\n",
    "    \n",
    "    # Time the execution of the plan\n",
    "    start_time = time.time()\n",
    "    RasCmdr.compute_plan(plan_number)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"plan_number\": plan_number,\n",
    "        \"cores\": cores,\n",
    "        \"execution_time\": execution_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "print(\"Sensitivity analysis complete\")\n",
    "\n",
    "# Step 5: Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv(\"core_sensitivity_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES FOR REVISIONS:\n",
    "- Use HDF compute summary to show the time for each preproces/unsteady compute/postprocess step. \n",
    "- First, run preprocessor and then toggle options to only run unsteady compute and postprocess. \n",
    "- Plot each step separately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load the results from a CSV file\n",
    "results_df = pd.read_csv(\"core_sensitivity_results.csv\")\n",
    "\n",
    "# Display the results dataframe for verification\n",
    "print(\"results_df DataFrame (time is in seconds):\")\n",
    "display(results_df)\n",
    "\n",
    "# Step 6: Calculate unit runtime (based on 1 core execution time)\n",
    "results_df['unit_runtime'] = results_df.groupby('plan_number')['execution_time'].transform(lambda x: x / x.iloc[0])\n",
    "\n",
    "# Get the project name from the ras object\n",
    "project_name = ras.project_name\n",
    "\n",
    "# Step 7: Plot a line chart for unit runtime vs. cores for each plan\n",
    "plt.figure(figsize=(10, 6))\n",
    "for plan in results_df['plan_number'].unique():\n",
    "    plan_data = results_df[results_df['plan_number'] == plan]\n",
    "    plt.plot(plan_data['cores'], plan_data['unit_runtime'], label=f\"Plan {plan}\")\n",
    "\n",
    "plt.xlabel(\"Number of Cores\")\n",
    "plt.ylabel(\"Unit Runtime (Relative to 1 Core)\")\n",
    "plt.title(f\"{project_name} (HEC Example Project)\\nCore Count Sensitivity Analysis\")\n",
    "plt.legend(title=\"Plan Number\")\n",
    "plt.grid(False)\n",
    "plt.vlines([1,2,3,4], ymin=0, ymax=1.2, linestyles='dotted', alpha=0.3)\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\102_benchmarking_versions_6.1_to_6.6.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS-Commander standard code cells 1-3: Install Packages and Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define versions to compare\n",
    "versions = ['6.6', '6.5', '6.4.1', '6.3.1', '6.3', '6.2', \"6.1\", \"6.0\"] # NOTE: ras-commander does not support versions prior to 6.2 due to HDF5 file format changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BaldEagleCrkMulti2D project\n",
    "project_path = RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the ras_project with ras-commander to read all HEC-RAS project information \n",
    "init_ras_project(project_path, \"6.5\")\n",
    "print(ras)\n",
    "# If no ras object is defined in init_ras_project, it defaults to \"ras\" (useful for single project scripts)\n",
    "# Display plan dataframe\n",
    "ras.plan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Plan Numbers to List and Print\n",
    "plan_numbers = ras.plan_df['plan_number'].tolist()\n",
    "print(plan_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define run_simulation function for\n",
    "import time\n",
    "from ras_commander import RasGeo\n",
    "\n",
    "def run_simulation(version, plan_number):\n",
    "    # Initialize project for the specific version\n",
    "    ras_project = init_ras_project(project_path, str(version))\n",
    "    \n",
    "    # Clear geometry preprocessor files for the plan\n",
    "    plan_path = RasPlan.get_plan_path(plan_number, ras_object=ras_project)\n",
    "    RasGeo.clear_geompre_files(plan_path, ras_object=ras_project)\n",
    "    \n",
    "    # Set the number of cores to 4\n",
    "    RasPlan.set_num_cores(plan_number, \"4\", ras_object=ras_project)\n",
    "    \n",
    "    # Update plan run flags  setting \"Run HTab\" flag to 1 to force geometry preprocessing\n",
    "    RasPlan.update_run_flags(plan_number, {\"Run HTab\": 1}, ras_object=ras_project)\n",
    "    \n",
    "    # Compute the plan\n",
    "    start_time = time.time()\n",
    "    success = RasCmdr.compute_plan(plan_number, ras_object=ras_project)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if success:\n",
    "        # Get the HDF file path for the plan results\n",
    "        hdf_path = RasPlan.get_results_path(plan_number, ras_object=ras_project)\n",
    "        \n",
    "        # Extract runtime data from the HDF file\n",
    "        runtime_data = HdfResultsPlan.get_runtime_data(hdf_path)\n",
    "        \n",
    "        # Extract required information from the runtime data\n",
    "        preprocessor_time = runtime_data['Preprocessing Geometry (hr)'].values[0]\n",
    "        unsteady_compute_time = runtime_data['Unsteady Flow Computations (hr)'].values[0]\n",
    "        \n",
    "        # Get volume accounting data from the HDF file\n",
    "        volume_accounting = HdfResultsPlan.get_volume_accounting(hdf_path)\n",
    "        # Extract Error Percent from the DataFrame\n",
    "        volume_error = volume_accounting['Error Percent'].values[0] if not volume_accounting.empty else None\n",
    "        \n",
    "        # Print the extracted data\n",
    "        print(f\"\\nExtracted Data for Plan {plan_number} in Version {version}:\")\n",
    "        print(f\"Preprocessor Time: {preprocessor_time:.3f} hr\")\n",
    "        print(f\"Unsteady Compute Time: {unsteady_compute_time:.3f} hr\") \n",
    "        print(f\"Volume Error: {volume_error:.3f}%\" if volume_error is not None else \"Volume Error: None\")\n",
    "        print(f\"Total Time: {total_time/3600:.3f} hr\\n\")\n",
    "        \n",
    "        return {\n",
    "            'Version': version,\n",
    "            'Plan': plan_number,\n",
    "            'Preprocessor Time (hr)': preprocessor_time,\n",
    "            'Unsteady Compute Time (hr)': unsteady_compute_time,\n",
    "            'Volume Error (%)': volume_error,\n",
    "            'Total Time (hr)': total_time / 3600  # convert seconds to hours\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, benchmark all plans in Version 6.6\n",
    "results = []\n",
    "for plan in plan_numbers:\n",
    "    print(f\"Running simulation for Version 6.6, Plan {plan}\")\n",
    "    result = run_simulation(\"6.6\", plan)\n",
    "    if result is not None:  # Check if result is not None\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results list to DataFrame and save as CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('hecras_plan_comparison.csv', index=False)\n",
    "\n",
    "print(\"Results saved to 'hecras_plan_comparison.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe of results, and plot the results\n",
    "\n",
    "# Load and display the results dataframe\n",
    "df = pd.read_csv('hecras_plan_comparison.csv')\n",
    "\n",
    "# Get plan titles from ras.plan_df and merge with results\n",
    "plan_titles = pd.DataFrame({\n",
    "    'Plan': ras.plan_df['plan_number'].str.zfill(2),  # Ensure 2-digit format\n",
    "    'Short Identifier': ras.plan_df['Short Identifier']\n",
    "})\n",
    "# Convert df's Plan column to 2-digit string format\n",
    "df['Plan'] = df['Plan'].astype(str).str.zfill(2)\n",
    "\n",
    "df = df.merge(plan_titles, on='Plan', how='left')\n",
    "\n",
    "print(\"Benchmarking Results:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Create a more comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Function to create rotated labels\n",
    "def plot_with_rotated_labels(ax, data, values, color, title, ylabel):\n",
    "    bars = ax.bar(range(len(data)), values, color=color, alpha=0.7)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set x-ticks at bar positions\n",
    "    ax.set_xticks(range(len(data)))\n",
    "    \n",
    "    # Create labels with plan numbers and titles\n",
    "    labels = [f\"Plan {plan}\\n{title}\" for plan, title in zip(data['Plan'], data['Short Identifier'])]\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "# Plot 1: Unsteady Runtime\n",
    "plot_with_rotated_labels(ax1, df, df['Unsteady Compute Time (hr)'], 'blue', \n",
    "                        'Unsteady Runtime by Plan', 'Unsteady Runtime (hours)')\n",
    "\n",
    "# Plot 2: Volume Error  \n",
    "plot_with_rotated_labels(ax2, df, df['Volume Error (%)'], 'red',\n",
    "                        'Volume Error by Plan', 'Volume Error (%)')\n",
    "\n",
    "# Plot 3: Preprocessor Time\n",
    "plot_with_rotated_labels(ax3, df, df['Preprocessor Time (hr)'], 'green',\n",
    "                        'Preprocessor Time by Plan', 'Preprocessor Time (hours)')\n",
    "\n",
    "# Plot 4: Total Runtime\n",
    "plot_with_rotated_labels(ax4, df, df['Total Time (hr)'], 'purple',\n",
    "                        'Total Runtime by Plan', 'Total Runtime (hours)')\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout(pad=3.0)\n",
    "fig.suptitle('Plan Performance Comparison', fontsize=14, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Calculate plan-to-plan performance changes\n",
    "print(\"\\nPlan-to-Plan Performance Changes:\")\n",
    "df['Unsteady Runtime Change (%)'] = df['Unsteady Compute Time (hr)'].pct_change() * 100\n",
    "print(df[['Plan', 'Short Identifier', 'Unsteady Runtime Change (%)']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the plan number you want to run across all versions\n",
    "plan_number = '02'  # Make sure this is a string and include the leading zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations for all versions with plan_number defined by user\n",
    "results = []\n",
    "for version in versions:\n",
    "    print(f\"Running simulation for Version {version}, Plan {plan_number}\")\n",
    "    result = run_simulation(version, plan_number) \n",
    "    if result is not None:  # Check if result is not None\n",
    "        results.append(result)\n",
    "        print(f\"Completed: Version {version}, Plan {plan_number}\")\n",
    "    else:\n",
    "        print(f\"Failed: Version {version}, Plan {plan_number}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save initial results to CSV\n",
    "df.to_csv('save_initial_results.csv', index=False)\n",
    "\n",
    "print(\"Initial results saved to 'save_initial_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line graphs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Unsteady Runtime vs Version\n",
    "plt.subplot(1, 2, 1)\n",
    "# Convert Version to categorical type to handle string versions properly\n",
    "plt.plot(pd.Categorical(df['Version']), df['Unsteady Compute Time (hr)'], marker='o')\n",
    "plt.title(f'Unsteady Runtime vs HEC-RAS Version (Plan {plan_number})')\n",
    "plt.xlabel('HEC-RAS Version')\n",
    "plt.ylabel('Unsteady Runtime (hours)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Volume Error vs Version\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(pd.Categorical(df['Version']), df['Volume Error (%)'], marker='o')\n",
    "plt.title(f'Volume Error vs HEC-RAS Version (Plan {plan_number})')\n",
    "plt.xlabel('HEC-RAS Version')\n",
    "plt.ylabel('Volume Error (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\103_Generating AEP Events from Atlas 14.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated HEC-RAS Analysis for Multiple AEP Events\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow for performing flood analysis with different Annual Exceedance Probability (AEP) events. We'll automate the following steps:\n",
    "\n",
    "1. Generate hyetographs from NOAA Atlas 14 data for different AEP events\n",
    "2. Download the Davis HEC-RAS project\n",
    "3. Clone and configure HEC-RAS plans and unsteady flow files for each AEP event\n",
    "4. Execute all plans in parallel\n",
    "5. Extract and visualize results\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "We'll use the following libraries:\n",
    "- `ras-commander`: For HEC-RAS automation\n",
    "- `pandas`, `numpy`: For data manipulation\n",
    "- `matplotlib`: For visualization\n",
    "- Standard libraries: `os`, `re`, `pathlib`, etc.\n",
    "\n",
    "Let's start by installing ras-commander (if needed) and importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander if needed (uncomment to run)\n",
    "# !pip install ras-commander\n",
    "\n",
    "# Import necessary libraries\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from math import log, exp\n",
    "from pathlib import Path\n",
    "import time\n",
    "import psutil  # For getting system CPU info\n",
    "from IPython import display\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Hyetographs from NOAA Atlas 14 Data\n",
    "\n",
    "First, let's define functions to generate hyetographs from NOAA Atlas 14 precipitation frequency data. These functions will:\n",
    "1. Parse duration strings from the CSV file\n",
    "2. Read and process precipitation frequency data\n",
    "3. Interpolate depths for each ARI (Annual Recurrence Interval)\n",
    "4. Compute incremental depths\n",
    "5. Apply the Alternating Block Method to generate hyetographs\n",
    "6. Save the hyetographs to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse duration strings and convert them to hours\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a duration string and converts it to hours.\n",
    "    Examples:\n",
    "        \"5-min:\" -> 0.0833 hours\n",
    "        \"2-hr:\" -> 2 hours\n",
    "        \"2-day:\" -> 48 hours\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "    value, unit = match.groups()\n",
    "    value = int(value)\n",
    "    unit = unit.lower()\n",
    "    if unit in ['min', 'minute', 'minutes']:\n",
    "        hours = value / 60.0\n",
    "    elif unit in ['hr', 'hour', 'hours']:\n",
    "        hours = value\n",
    "    elif unit in ['day', 'days']:\n",
    "        hours = value * 24\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
    "    return hours\n",
    "\n",
    "# Function to read and process the precipitation frequency CSV\n",
    "def read_precipitation_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the precipitation frequency CSV and returns a DataFrame\n",
    "    with durations in hours as the index and ARIs as columns.\n",
    "    This function dynamically locates the header line for the data table.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_line_idx = None\n",
    "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
    "\n",
    "    # Locate the header line\n",
    "    for idx, line in enumerate(lines):\n",
    "        if header_pattern.match(line.strip().lower()):\n",
    "            header_line_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_line_idx is None:\n",
    "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
    "\n",
    "    # Extract the ARI headers from the header line\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    headers = [item.strip() for item in header_line.split(',')]\n",
    "    \n",
    "    if len(headers) < 2:\n",
    "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
    "\n",
    "    aris = headers[1:]  # Exclude the first column which is the duration\n",
    "\n",
    "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
    "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
    "\n",
    "    # Initialize lists to store durations and corresponding depths\n",
    "    durations = []\n",
    "    depths = {ari: [] for ari in aris}\n",
    "\n",
    "    # Iterate over the lines following the header to extract data\n",
    "    for line in lines[header_line_idx + 1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if not duration_pattern.match(line):\n",
    "            break  # Stop if the line does not match the duration pattern\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if len(parts) != len(headers):\n",
    "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
    "        duration_str = parts[0]\n",
    "        try:\n",
    "            duration_hours = parse_duration(duration_str)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Skipping line due to error: {ve}\")\n",
    "            continue  # Skip lines with invalid duration formats\n",
    "        durations.append(duration_hours)\n",
    "        for ari, depth_str in zip(aris, parts[1:]):\n",
    "            try:\n",
    "                depth = float(depth_str)\n",
    "            except ValueError:\n",
    "                depth = np.nan  # Assign NaN for invalid depth values\n",
    "            depths[ari].append(depth)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(depths, index=durations)\n",
    "    df.index.name = 'Duration_hours'\n",
    "\n",
    "    # Drop any rows with NaN values (optional, based on data quality)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to perform log-log linear interpolation for each ARI\n",
    "def interpolate_depths(df, total_duration):\n",
    "    \"\"\"\n",
    "    Interpolates precipitation depths for each ARI on a log-log scale\n",
    "    for each hour up to the total storm duration.\n",
    "    \"\"\"\n",
    "    T = total_duration\n",
    "    t_hours = np.arange(1, T+1)\n",
    "    D = {}\n",
    "    for ari in df.columns:\n",
    "        durations = df.index.values\n",
    "        depths = df[ari].values\n",
    "        # Ensure all depths are positive\n",
    "        if np.any(depths <= 0):\n",
    "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
    "        # Log-log interpolation\n",
    "        log_durations = np.log(durations)\n",
    "        log_depths = np.log(depths)\n",
    "        log_t = np.log(t_hours)\n",
    "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
    "        D_t = np.exp(log_D_t)\n",
    "        D[ari] = D_t\n",
    "    return D\n",
    "\n",
    "# Function to compute incremental precipitation depths\n",
    "def compute_incremental_depths(D, total_duration):\n",
    "    \"\"\"\n",
    "    Computes incremental precipitation depths for each hour.\n",
    "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
    "    \"\"\"\n",
    "    incremental_depths = {}\n",
    "    for ari, D_t in D.items():\n",
    "        I_t = np.empty(total_duration)\n",
    "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
    "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
    "        incremental_depths[ari] = I_t\n",
    "    return incremental_depths\n",
    "\n",
    "# Function to assign incremental depths using the Alternating Block Method\n",
    "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
    "    \"\"\"\n",
    "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    hyetograph = [0.0] * T\n",
    "    hyetograph[central_index] = max_depth\n",
    "    remaining_depths = sorted_depths.copy()\n",
    "    remaining_depths.remove(max_depth)\n",
    "    left = central_index - 1\n",
    "    right = central_index + 1\n",
    "    toggle = True  # Start assigning to the right\n",
    "    for depth in remaining_depths:\n",
    "        if toggle and right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif not toggle and left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        elif right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        else:\n",
    "            print(\"Warning: Not all incremental depths assigned.\")\n",
    "            break\n",
    "        toggle = not toggle\n",
    "    return hyetograph\n",
    "\n",
    "# Function to generate the hyetograph for a given ARI\n",
    "def generate_hyetograph(incremental_depths, position_percent, T):\n",
    "    \"\"\"\n",
    "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    max_depth = np.max(incremental_depths)\n",
    "    incremental_depths_list = incremental_depths.tolist()\n",
    "    central_index = int(round(T * position_percent / 100)) - 1\n",
    "    central_index = max(0, min(central_index, T - 1))\n",
    "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
    "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
    "    return hyetograph\n",
    "\n",
    "# Function to save the hyetograph to a CSV file\n",
    "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
    "    \"\"\"\n",
    "    Saves the hyetograph to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Time_hour': np.arange(1, total_duration + 1),\n",
    "        'Precipitation_in': hyetograph\n",
    "    })\n",
    "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hyetographs for Different AEP Events\n",
    "\n",
    "Now, let's use the functions defined above to generate hyetographs for different AEP events.\n",
    "We'll download the NOAA Atlas 14 data for Davis, CA and generate hyetographs for various ARI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open NOAA Atlas 14 data from CSV file\n",
    "def open_noaa_atlas14_csv():\n",
    "    \"\"\"\n",
    "    Opens NOAA Atlas 14 data from CSV file in data directory.\n",
    "    \"\"\"\n",
    "    # Create data directory if it doesn't exist\n",
    "    data_dir = Path('data')\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    input_file = data_dir / 'PF_Depth_English_PDS_DavisCA.csv'\n",
    "    if input_file.exists():\n",
    "        logging.info(f\"NOAA Atlas 14 data file found: {input_file}\")\n",
    "        return str(input_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"NOAA Atlas 14 data file not found at {input_file}\")\n",
    "\n",
    "# Generate hyetographs\n",
    "def generate_all_hyetographs(input_csv, output_dir, ari_values, position_percent=50, total_duration=24):\n",
    "    \"\"\"\n",
    "    Generates hyetographs for specified ARI values.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_csv: Path to NOAA Atlas 14 CSV file\n",
    "    - output_dir: Directory to save hyetographs\n",
    "    - ari_values: List of ARI values to generate hyetographs for\n",
    "    - position_percent: Position percentage for peak intensity (default: 50%)\n",
    "    - total_duration: Total storm duration in hours (default: 24 hours)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping ARI values to hyetograph file paths\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Output directory is set to: {output_dir}\")\n",
    "    \n",
    "    # Read precipitation data\n",
    "    try:\n",
    "        df = read_precipitation_data(input_csv)\n",
    "        logging.info(\"Successfully read the input CSV file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading input CSV: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Display the first few rows of the DataFrame to verify\n",
    "    logging.info(\"\\nPrecipitation Frequency Data (first few rows):\")\n",
    "    display.display(df.head())\n",
    "    \n",
    "    # Interpolate depths\n",
    "    try:\n",
    "        D = interpolate_depths(df, total_duration)\n",
    "        logging.info(\"Successfully interpolated precipitation depths.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during interpolation: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Compute incremental depths\n",
    "    I = compute_incremental_depths(D, total_duration)\n",
    "    logging.info(\"Successfully computed incremental depths.\")\n",
    "    \n",
    "    # Generate and save hyetographs for each ARI\n",
    "    hyetograph_files = {}\n",
    "    for ari in ari_values:\n",
    "        ari_str = str(ari)\n",
    "        if ari_str in I:\n",
    "            incremental_depths = I[ari_str]\n",
    "            hyetograph = generate_hyetograph(incremental_depths, position_percent, total_duration)\n",
    "            hyetograph_file = save_hyetograph(hyetograph, ari_str, output_dir, position_percent, total_duration)\n",
    "            hyetograph_files[ari_str] = hyetograph_file\n",
    "        else:\n",
    "            logging.warning(f\"ARI {ari} not found in the input data. Skipping.\")\n",
    "    \n",
    "    logging.info(f\"\\nGenerated {len(hyetograph_files)} hyetographs.\")\n",
    "    return hyetograph_files\n",
    "\n",
    "# Plot multiple hyetographs\n",
    "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
    "    \"\"\"\n",
    "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    - aris (list of str or int): List of Annual Recurrence Intervals to plot\n",
    "    - position_percent (int): Position percentage for the maximum intensity\n",
    "    - total_duration (int): Total storm duration in hours\n",
    "    - output_dir (str): Directory where hyetograph CSV files are saved\n",
    "    \n",
    "    Returns:\n",
    "    - Plot object\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    for ari in aris:\n",
    "        # Ensure ARI is a string for consistent filename formatting\n",
    "        ari_str = str(ari)\n",
    "        \n",
    "        # Construct the filename based on the naming convention\n",
    "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            logging.warning(f\"File '{filename}' does not exist in '{output_dir}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        try:\n",
    "            hyetograph_df = pd.read_csv(filepath)\n",
    "            logging.info(f\"Successfully read hyetograph data from '{filename}'.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading CSV file '{filename}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot the hyetograph\n",
    "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
    "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Time (Hour)', fontsize=14)\n",
    "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
    "    plt.title(f'Comparison of Hyetographs for Different ARIs\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 12)))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt_filename = f\"hyetograph_comparison_pos{position_percent}pct_{total_duration}hr.png\"\n",
    "    plt_filepath = os.path.join(output_dir, plt_filename)\n",
    "    plt.savefig(plt_filepath, dpi=300)\n",
    "    logging.info(f\"Saved comparison plot to {plt_filepath}\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Run the hyetograph generation process\n",
    "ari_values = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "position_percent = 50  # Position of peak intensity (50% = center of storm)\n",
    "total_duration = 24    # Total storm duration in hours\n",
    "\n",
    "# Open NOAA Atlas 14 data\n",
    "input_csv = open_noaa_atlas14_csv()\n",
    "\n",
    "# Generate hyetographs\n",
    "output_dir = 'hyetographs'\n",
    "hyetograph_files = generate_all_hyetographs(\n",
    "    input_csv=input_csv,\n",
    "    output_dir=output_dir,\n",
    "    ari_values=ari_values,\n",
    "    position_percent=position_percent,\n",
    "    total_duration=total_duration\n",
    ")\n",
    "\n",
    "# Plot the hyetographs\n",
    "plot = plot_multiple_hyetographs(\n",
    "    aris=ari_values,\n",
    "    position_percent=position_percent,\n",
    "    total_duration=total_duration,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Download and Prepare the Davis HEC-RAS Project\n",
    "\n",
    "Now, let's download the Davis project using RasExamples.extract_project() and prepare it for our analysis. \n",
    "We'll then create a new folder for our AEP analysis to keep the original project intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davis_path = RasExamples.extract_project(\"Davis\")\n",
    "\n",
    "# Create a new project folder for our AEP analysis\n",
    "def create_aep_project_folder(original_project_folder, new_project_name):\n",
    "    \"\"\"\n",
    "    Creates a new project folder for our AEP analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - original_project_folder: Path to the original project folder\n",
    "    - new_project_name: Name for the new project folder\n",
    "    \n",
    "    Returns:\n",
    "    - Path to the new project folder\n",
    "    \"\"\"\n",
    "    # Create path for the new project folder\n",
    "    new_project_folder = original_project_folder.parent / new_project_name\n",
    "    \n",
    "    # Remove the new project folder if it already exists\n",
    "    if new_project_folder.exists():\n",
    "        logging.info(f\"Removing existing folder: {new_project_folder}\")\n",
    "        shutil.rmtree(new_project_folder)\n",
    "    \n",
    "    # Copy the original project to the new folder\n",
    "    logging.info(f\"Copying project from {original_project_folder} to {new_project_folder}\")\n",
    "    shutil.copytree(original_project_folder, new_project_folder)\n",
    "    \n",
    "    logging.info(f\"Created new project folder: {new_project_folder}\")\n",
    "    return new_project_folder\n",
    "\n",
    "# Initialize the RAS project\n",
    "def initialize_ras_project(project_folder, ras_version=\"6.6\"):\n",
    "    \"\"\"\n",
    "    Initializes the RAS project and returns the RAS object.\n",
    "    \n",
    "    Parameters:\n",
    "    - project_folder: Path to the project folder\n",
    "    - ras_version: HEC-RAS version (default: \"6.6\")\n",
    "    \n",
    "    Returns:\n",
    "    - Initialized RAS object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ras_object = init_ras_project(project_folder, ras_version)\n",
    "        logging.info(f\"Initialized RAS project: {ras_object.project_name}\")\n",
    "        return ras_object\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing RAS project: {e}\")\n",
    "        raise\n",
    "\n",
    "# Download the Davis project\n",
    "davis_path = download_davis_project()\n",
    "\n",
    "# Create a new project folder for our AEP analysis\n",
    "aep_project_name = \"Davis_AEP_Analysis\"\n",
    "aep_project_folder = create_aep_project_folder(davis_path, aep_project_name)\n",
    "\n",
    "# Initialize the RAS project\n",
    "ras_object = initialize_ras_project(aep_project_folder)\n",
    "\n",
    "# Display project information\n",
    "print(\"\\nHEC-RAS Project Information:\")\n",
    "print(f\"Project Name: {ras_object.project_name}\")\n",
    "print(f\"Project Folder: {ras_object.project_folder}\")\n",
    "\n",
    "# Display available plans\n",
    "print(\"\\nAvailable Plans:\")\n",
    "display.display(ras_object.plan_df)\n",
    "\n",
    "# Display available unsteady flow files\n",
    "print(\"\\nAvailable Unsteady Flow Files:\")\n",
    "display.display(ras_object.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Clone Plans and Unsteady Flow Files for Each AEP Event\n",
    "\n",
    "Now, let's clone Plan 02 for each AEP event and update the plan and unsteady flow files with the appropriate hyetograph data. This will allow us to simulate different AEP events with HEC-RAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template plan and unsteady flow file numbers\n",
    "template_plan = \"02\"  # Use plan 02 as the template\n",
    "template_unsteady = \"01\"  # Use unsteady file 01 as the template\n",
    "\n",
    "# Function to create plans and unsteady flow files for each AEP event\n",
    "def create_aep_plans_and_unsteady_files(ras_object, template_plan, template_unsteady, ari_values, hyetograph_files):\n",
    "    \"\"\"\n",
    "    Creates plans and unsteady flow files for each AEP event.\n",
    "    \n",
    "    Parameters:\n",
    "    - ras_object: Initialized RAS object\n",
    "    - template_plan: Plan number to use as template\n",
    "    - template_unsteady: Unsteady flow file number to use as template\n",
    "    - ari_values: List of ARI values to create plans for\n",
    "    - hyetograph_files: Dictionary mapping ARI values to hyetograph file paths\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping ARI values to plan numbers\n",
    "    - Dictionary mapping ARI values to unsteady flow file numbers\n",
    "    \"\"\"\n",
    "    project_name = ras_object.project_name\n",
    "    project_folder = ras_object.project_folder\n",
    "    prj_file = ras_object.prj_file\n",
    "    \n",
    "    new_plan_numbers = {}\n",
    "    new_unsteady_numbers = {}\n",
    "    \n",
    "    for ari in ari_values:\n",
    "        ari_str = str(ari)\n",
    "        if ari_str not in hyetograph_files:\n",
    "            logging.warning(f\"ARI {ari} does not have a hyetograph file. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Create new plan file by cloning template\n",
    "        new_plan_number = RasPlan.clone_plan(template_plan, f\"ARI_{ari}_years\", ras_object)\n",
    "        logging.info(f\"Created new plan: {new_plan_number} for ARI {ari} years\")\n",
    "        new_plan_numbers[ari_str] = new_plan_number\n",
    "        \n",
    "        # Update plan description\n",
    "        plan_description = f\"Annual Exceedance Probability (AEP) Event\\nAnnual Recurrence Interval (ARI): {ari} years\\nExceedance Probability: {100/float(ari):.2f}%\"\n",
    "        # Get plan file path\n",
    "        plan_file_path = RasPlan.get_plan_path(new_plan_number, ras_object)\n",
    "        \n",
    "        # Update the plan file directly since update_plan_value doesn't exist\n",
    "        def update_description(lines):\n",
    "            updated_lines = []\n",
    "            in_description = False\n",
    "            description_updated = False\n",
    "            \n",
    "            for line in lines:\n",
    "                if line.strip().startswith(\"Description=\"):\n",
    "                    updated_lines.append(f\"Description={plan_description}\\n\")\n",
    "                    description_updated = True\n",
    "                    in_description = True\n",
    "                elif in_description and line.strip() and not line.strip().startswith(\"Description=\"):\n",
    "                    in_description = False\n",
    "                    if not description_updated:\n",
    "                        updated_lines.append(line)\n",
    "                else:\n",
    "                    updated_lines.append(line)\n",
    "                    \n",
    "            return updated_lines\n",
    "        \n",
    "        # Use RasUtils to update the file\n",
    "        RasUtils.update_file(plan_file_path, update_description)\n",
    "        logging.info(f\"Updated plan description for ARI {ari} years\")\n",
    "        \n",
    "        # Create new unsteady flow file by cloning template\n",
    "        new_unsteady_number = RasPlan.clone_unsteady(template_unsteady, ras_object)\n",
    "        logging.info(f\"Created new unsteady flow file: {new_unsteady_number} for ARI {ari} years\")\n",
    "        new_unsteady_numbers[ari_str] = new_unsteady_number\n",
    "        \n",
    "        # Update unsteady flow file with hyetograph data\n",
    "        unsteady_file_path = RasPlan.get_unsteady_path(new_unsteady_number, ras_object)\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        hyetograph_file = hyetograph_files[ari_str]\n",
    "        hyetograph_data = pd.read_csv(hyetograph_file)\n",
    "        \n",
    "        # Update flow title in unsteady file\n",
    "        flow_title = f\"ARI_{ari}_years\"\n",
    "        RasUnsteady.update_flow_title(unsteady_file_path, flow_title, ras_object)\n",
    "        logging.info(f\"Updated flow title for ARI {ari} years\")\n",
    "        \n",
    "        # Apply the new unsteady file to the new plan\n",
    "        RasPlan.set_unsteady(new_plan_number, new_unsteady_number, ras_object)\n",
    "        logging.info(f\"Applied unsteady flow file {new_unsteady_number} to plan {new_plan_number}\")\n",
    "    \n",
    "    logging.info(f\"Created {len(new_plan_numbers)} plans and {len(new_unsteady_numbers)} unsteady flow files.\")\n",
    "    return new_plan_numbers, new_unsteady_numbers\n",
    "\n",
    "# Create plans and unsteady flow files for each AEP event\n",
    "new_plan_numbers, new_unsteady_numbers = create_aep_plans_and_unsteady_files(\n",
    "    ras_object=ras_object,\n",
    "    template_plan=template_plan,\n",
    "    template_unsteady=template_unsteady,\n",
    "    ari_values=ari_values,\n",
    "    hyetograph_files=hyetograph_files\n",
    ")\n",
    "\n",
    "# Display the new plans and unsteady flow files\n",
    "print(\"\\nNew Plans for AEP Events:\")\n",
    "for ari, plan_number in new_plan_numbers.items():\n",
    "    print(f\"ARI {ari} years: Plan {plan_number}\")\n",
    "\n",
    "print(\"\\nNew Unsteady Flow Files for AEP Events:\")\n",
    "for ari, unsteady_number in new_unsteady_numbers.items():\n",
    "    print(f\"ARI {ari} years: Unsteady Flow {unsteady_number}\")\n",
    "\n",
    "# Refresh the RAS object to see the new plans and unsteady flow files\n",
    "ras_object = initialize_ras_project(aep_project_folder)\n",
    "\n",
    "# Display all plans\n",
    "print(\"\\nAll Plans:\")\n",
    "display.display(ras_object.plan_df)\n",
    "\n",
    "# Display all unsteady flow files\n",
    "print(\"\\nAll Unsteady Flow Files:\")\n",
    "display.display(ras_object.unsteady_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Execute All Plans in Parallel\n",
    "\n",
    "Now, let's execute all the plans we created in parallel using the RasCmdr.compute_parallel() function.\n",
    "This will allow us to efficiently run multiple HEC-RAS simulations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the optimal number of workers based on system resources\n",
    "def get_optimal_worker_count(cores_per_worker=2):\n",
    "    \"\"\"\n",
    "    Calculate the optimal number of workers based on available physical cores.\n",
    "    \n",
    "    Parameters:\n",
    "    - cores_per_worker: Number of cores to allocate to each worker (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Optimal number of workers\n",
    "    \"\"\"\n",
    "    # Get physical CPU cores\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    if physical_cores is None:\n",
    "        physical_cores = psutil.cpu_count(logical=True) // 2  # Fallback estimate\n",
    "    \n",
    "    # Calculate optimal workers based on physical cores\n",
    "    optimal_workers = physical_cores // cores_per_worker\n",
    "    \n",
    "    # Ensure at least 1 worker\n",
    "    return max(1, optimal_workers)\n",
    "\n",
    "# Execute plans in parallel\n",
    "def execute_plans_in_parallel(ras_object, plan_numbers, compute_folder, cores_per_worker=2):\n",
    "    \"\"\"\n",
    "    Executes multiple HEC-RAS plans in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    - ras_object: Initialized RAS object\n",
    "    - plan_numbers: List of plan numbers to execute\n",
    "    - compute_folder: Folder to store computation results\n",
    "    - cores_per_worker: Number of cores to allocate to each worker (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of execution results\n",
    "    \"\"\"\n",
    "    # Check system resources\n",
    "    cpu_count = psutil.cpu_count(logical=True)\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    \n",
    "    logging.info(f\"System Resources:\")\n",
    "    logging.info(f\"- {physical_cores} physical CPU cores ({cpu_count} logical cores)\")\n",
    "    logging.info(f\"- {memory_gb:.1f} GB total memory ({available_memory_gb:.1f} GB available)\")\n",
    "    \n",
    "    # Calculate optimal number of workers\n",
    "    max_workers = get_optimal_worker_count(cores_per_worker)\n",
    "    logging.info(f\"Using {max_workers} workers with {cores_per_worker} cores per worker\")\n",
    "    \n",
    "    # Create compute folder if it doesn't exist\n",
    "    compute_folder = Path(compute_folder)\n",
    "    compute_folder.mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(f\"Compute folder: {compute_folder}\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute plans in parallel\n",
    "    logging.info(f\"Executing {len(plan_numbers)} plans in parallel...\")\n",
    "    results = RasCmdr.compute_parallel(\n",
    "        plan_number=plan_numbers,\n",
    "        max_workers=max_workers,\n",
    "        num_cores=cores_per_worker,\n",
    "        dest_folder=compute_folder,\n",
    "        overwrite_dest=True,\n",
    "        ras_object=ras_object\n",
    "    )\n",
    "    \n",
    "    # Record end time and calculate duration\n",
    "    end_time = time.time()\n",
    "    total_duration = end_time - start_time\n",
    "    \n",
    "    logging.info(f\"Parallel execution completed in {total_duration:.2f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create compute folder\n",
    "compute_folder = aep_project_folder.parent / \"Davis_AEP_Compute\"\n",
    "\n",
    "# Get plan numbers to execute\n",
    "plan_numbers_to_execute = list(new_plan_numbers.values())\n",
    "print(f\"Executing {len(plan_numbers_to_execute)} plans: {plan_numbers_to_execute}\")\n",
    "\n",
    "# Execute plans in parallel\n",
    "execution_results = execute_plans_in_parallel(\n",
    "    ras_object=ras_object,\n",
    "    plan_numbers=plan_numbers_to_execute,\n",
    "    compute_folder=compute_folder,\n",
    "    cores_per_worker=2\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the execution results for better visualization\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Plan\": plan, \"Success\": success, \"ARI\": next((ari for ari, p in new_plan_numbers.items() if p == plan), None)}\n",
    "    for plan, success in execution_results.items()\n",
    "])\n",
    "\n",
    "# Sort by ARI\n",
    "results_df = results_df.sort_values(\"ARI\", key=lambda x: x.astype(float))\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nExecution Results:\")\n",
    "display.display(results_df)\n",
    "\n",
    "# Initialize a RAS project in the compute folder\n",
    "compute_ras_object = initialize_ras_project(compute_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Extract and Visualize Results\n",
    "\n",
    "Finally, let's extract and visualize the results from the HEC-RAS simulations. We'll use the ras-commander library to extract water surface elevation (WSEL) data for a specific cell in the 2D mesh and create comparison plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated an end-to-end workflow for performing flood analysis with different Annual Exceedance Probability (AEP) events using HEC-RAS and the ras-commander library. The key steps were:\n",
    "\n",
    "1. **Generate Hyetographs**: Created precipitation hyetographs for different ARI values using NOAA Atlas 14 data\n",
    "2. **Download and Prepare the HEC-RAS Project**: Downloaded the Davis project and created a new project folder for our analysis\n",
    "3. **Clone and Configure Plans**: Created new plans and unsteady flow files for each AEP event and updated them with the appropriate hyetographs\n",
    "4. **Execute Plans in Parallel**: Used parallel processing to efficiently run multiple HEC-RAS simulations simultaneously\n",
    "5. **Extract and Visualize Results**: Extracted water surface elevation data for a specific cell and created comparison plots\n",
    "\n",
    "This workflow demonstrates how the ras-commander library can be used to automate HEC-RAS workflows, making it easier to perform complex analyses with multiple scenarios. By using parallel processing, we can significantly reduce the overall computation time required for multi-scenario analyses.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To build on this analysis, you could:\n",
    "- Extract results for multiple cells or cross-sections to analyze spatial patterns of flooding\n",
    "- Create flood extent maps for different AEP events\n",
    "- Perform sensitivity analysis by varying parameters such as roughness or infiltration\n",
    "- Compare results with observed flood data for model calibration\n",
    "- Create animated visualizations of the flooding process\n",
    "\n",
    "These extensions would further enhance the value of the analysis and provide more comprehensive insights into flood behavior and risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

==================================================

File: c:\GH\ras-commander\examples\103_generating_aep_hyetographs_from_atlas_14_r.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from math import log, exp\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to parse duration strings and convert them to hours\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"\n",
    "    Parses a duration string and converts it to hours.\n",
    "    Examples:\n",
    "        \"5-min:\" -> 0.0833 hours\n",
    "        \"2-hr:\" -> 2 hours\n",
    "        \"2-day:\" -> 48 hours\n",
    "    \"\"\"\n",
    "    match = re.match(r'(\\d+)-(\\w+):', duration_str.strip())\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration_str}\")\n",
    "    value, unit = match.groups()\n",
    "    value = int(value)\n",
    "    unit = unit.lower()\n",
    "    if unit in ['min', 'minute', 'minutes']:\n",
    "        hours = value / 60.0\n",
    "    elif unit in ['hr', 'hour', 'hours']:\n",
    "        hours = value\n",
    "    elif unit in ['day', 'days']:\n",
    "        hours = value * 24\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown time unit in duration: {unit}\")\n",
    "    return hours\n",
    "\n",
    "# Function to read and process the precipitation frequency CSV\n",
    "def read_precipitation_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the precipitation frequency CSV and returns a DataFrame\n",
    "    with durations in hours as the index and ARIs as columns.\n",
    "    This function dynamically locates the header line for the data table.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header_line_idx = None\n",
    "    header_pattern = re.compile(r'^by duration for ari', re.IGNORECASE)\n",
    "\n",
    "    # Locate the header line\n",
    "    for idx, line in enumerate(lines):\n",
    "        if header_pattern.match(line.strip().lower()):\n",
    "            header_line_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_line_idx is None:\n",
    "        raise ValueError('Header line for precipitation frequency estimates not found in CSV file.')\n",
    "\n",
    "    # Extract the ARI headers from the header line\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    headers = [item.strip() for item in header_line.split(',')]\n",
    "    \n",
    "    if len(headers) < 2:\n",
    "        raise ValueError('Insufficient number of ARI columns found in the header line.')\n",
    "\n",
    "    aris = headers[1:]  # Exclude the first column which is the duration\n",
    "\n",
    "    # Define the pattern for data lines (e.g., \"5-min:\", \"10-min:\", etc.)\n",
    "    duration_pattern = re.compile(r'^\\d+-(min|hr|day):')\n",
    "\n",
    "    # Initialize lists to store durations and corresponding depths\n",
    "    durations = []\n",
    "    depths = {ari: [] for ari in aris}\n",
    "\n",
    "    # Iterate over the lines following the header to extract data\n",
    "    for line in lines[header_line_idx + 1:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        if not duration_pattern.match(line):\n",
    "            break  # Stop if the line does not match the duration pattern\n",
    "        parts = [part.strip() for part in line.split(',')]\n",
    "        if len(parts) != len(headers):\n",
    "            raise ValueError(f\"Data row does not match header columns: {line}\")\n",
    "        duration_str = parts[0]\n",
    "        try:\n",
    "            duration_hours = parse_duration(duration_str)\n",
    "        except ValueError as ve:\n",
    "            print(f\"Skipping line due to error: {ve}\")\n",
    "            continue  # Skip lines with invalid duration formats\n",
    "        durations.append(duration_hours)\n",
    "        for ari, depth_str in zip(aris, parts[1:]):\n",
    "            try:\n",
    "                depth = float(depth_str)\n",
    "            except ValueError:\n",
    "                depth = np.nan  # Assign NaN for invalid depth values\n",
    "            depths[ari].append(depth)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(depths, index=durations)\n",
    "    df.index.name = 'Duration_hours'\n",
    "\n",
    "    # Drop any rows with NaN values (optional, based on data quality)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to perform log-log linear interpolation for each ARI\n",
    "def interpolate_depths(df, total_duration):\n",
    "    \"\"\"\n",
    "    Interpolates precipitation depths for each ARI on a log-log scale\n",
    "    for each hour up to the total storm duration.\n",
    "    \"\"\"\n",
    "    T = total_duration\n",
    "    t_hours = np.arange(1, T+1)\n",
    "    D = {}\n",
    "    for ari in df.columns:\n",
    "        durations = df.index.values\n",
    "        depths = df[ari].values\n",
    "        # Ensure all depths are positive\n",
    "        if np.any(depths <= 0):\n",
    "            raise ValueError(f\"Non-positive depth value in ARI {ari}\")\n",
    "        # Log-log interpolation\n",
    "        log_durations = np.log(durations)\n",
    "        log_depths = np.log(depths)\n",
    "        log_t = np.log(t_hours)\n",
    "        log_D_t = np.interp(log_t, log_durations, log_depths)\n",
    "        D_t = np.exp(log_D_t)\n",
    "        D[ari] = D_t\n",
    "    return D\n",
    "\n",
    "# Function to compute incremental precipitation depths\n",
    "def compute_incremental_depths(D, total_duration):\n",
    "    \"\"\"\n",
    "    Computes incremental precipitation depths for each hour.\n",
    "    I(t) = D(t) - D(t-1), with D(0) = 0.\n",
    "    \"\"\"\n",
    "    incremental_depths = {}\n",
    "    for ari, D_t in D.items():\n",
    "        I_t = np.empty(total_duration)\n",
    "        I_t[0] = D_t[0]  # I(1) = D(1) - D(0) = D(1)\n",
    "        I_t[1:] = D_t[1:] - D_t[:-1]\n",
    "        incremental_depths[ari] = I_t\n",
    "    return incremental_depths\n",
    "\n",
    "# Function to assign incremental depths using the Alternating Block Method\n",
    "def assign_alternating_block(sorted_depths, max_depth, central_index, T):\n",
    "    \"\"\"\n",
    "    Assigns incremental depths to the hyetograph using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    hyetograph = [0.0] * T\n",
    "    hyetograph[central_index] = max_depth\n",
    "    remaining_depths = sorted_depths.copy()\n",
    "    remaining_depths.remove(max_depth)\n",
    "    left = central_index - 1\n",
    "    right = central_index + 1\n",
    "    toggle = True  # Start assigning to the right\n",
    "    for depth in remaining_depths:\n",
    "        if toggle and right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif not toggle and left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        elif right < T:\n",
    "            hyetograph[right] = depth\n",
    "            right += 1\n",
    "        elif left >= 0:\n",
    "            hyetograph[left] = depth\n",
    "            left -= 1\n",
    "        else:\n",
    "            print(\"Warning: Not all incremental depths assigned.\")\n",
    "            break\n",
    "        toggle = not toggle\n",
    "    return hyetograph\n",
    "\n",
    "# Function to generate the hyetograph for a given ARI\n",
    "def generate_hyetograph(incremental_depths, position_percent, T):\n",
    "    \"\"\"\n",
    "    Generates the hyetograph for a given ARI using the Alternating Block Method.\n",
    "    \"\"\"\n",
    "    max_depth = np.max(incremental_depths)\n",
    "    incremental_depths_list = incremental_depths.tolist()\n",
    "    central_index = int(round(T * position_percent / 100)) - 1\n",
    "    central_index = max(0, min(central_index, T - 1))\n",
    "    sorted_depths = sorted(incremental_depths_list, reverse=True)\n",
    "    hyetograph = assign_alternating_block(sorted_depths, max_depth, central_index, T)\n",
    "    return hyetograph\n",
    "\n",
    "# Function to save the hyetograph to a CSV file\n",
    "def save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration):\n",
    "    \"\"\"\n",
    "    Saves the hyetograph to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Time_hour': np.arange(1, total_duration + 1),\n",
    "        'Precipitation_in': hyetograph\n",
    "    })\n",
    "    filename = f'hyetograph_ARI_{ari}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "    output_file = os.path.join(output_dir, filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Hyetograph for ARI {ari} years saved to {output_file}\")\n",
    "\n",
    "# User Inputs\n",
    "# --------------------\n",
    "# Set the path to your input CSV file from NOAA Atlas 14\n",
    "input_csv = 'data\\PF_Depth_English_PDS_DavisCA.csv'  # Update this path if necessary\n",
    "\n",
    "# Set the output directory where hyetograph CSV files will be saved\n",
    "output_dir = 'hyetographs'\n",
    "\n",
    "# Set the position percentage for the maximum incremental depth block\n",
    "# Choose from 25, 33, 50, 67, or 75\n",
    "position_percent = 50  # Default is 50\n",
    "\n",
    "# Set the total storm duration in hours\n",
    "total_duration = 24  # Default is 24 hours\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory is set to: {output_dir}\")\n",
    "\n",
    "# Read precipitation data\n",
    "try:\n",
    "    df = read_precipitation_data(input_csv)\n",
    "    print(\"Successfully read the input CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading input CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(\"\\nPrecipitation Frequency Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# Interpolate depths\n",
    "try:\n",
    "    D = interpolate_depths(df, total_duration)\n",
    "    print(\"Successfully interpolated precipitation depths.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during interpolation: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display interpolated depths for the first ARI\n",
    "first_ari = df.columns[0]\n",
    "print(f\"\\nInterpolated Depths for ARI {first_ari} years:\")\n",
    "print(D[first_ari])\n",
    "\n",
    "# Compute incremental depths\n",
    "I = compute_incremental_depths(D, total_duration)\n",
    "print(\"Successfully computed incremental depths.\")\n",
    "\n",
    "# Generate and save hyetographs for each ARI\n",
    "for ari, incremental_depths in I.items():\n",
    "    hyetograph = generate_hyetograph(incremental_depths, position_percent, total_duration)\n",
    "    save_hyetograph(hyetograph, ari, output_dir, position_percent, total_duration)\n",
    "\n",
    "print(\"\\nAll hyetographs have been generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the hyetographs (final request from o1-mini)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot multiple hyetographs on the same plot\n",
    "def plot_multiple_hyetographs(aris, position_percent, total_duration, output_dir='hyetographs'):\n",
    "    \"\"\"\n",
    "    Plots multiple hyetographs for specified ARIs on the same figure for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    - aris (list of str or int): List of Annual Recurrence Intervals to plot (e.g., [1, 2, 5, 10])\n",
    "    - position_percent (int): Position percentage for the maximum incremental depth block (25, 33, 50, 67, or 75)\n",
    "    - total_duration (int): Total storm duration in hours\n",
    "    - output_dir (str): Directory where hyetograph CSV files are saved\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    for ari in aris:\n",
    "        # Ensure ARI is a string for consistent filename formatting\n",
    "        ari_str = str(ari)\n",
    "        \n",
    "        # Construct the filename based on the naming convention\n",
    "        filename = f'hyetograph_ARI_{ari_str}_years_pos{position_percent}pct_{total_duration}hr.csv'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: File '{filename}' does not exist in the directory '{output_dir}'. Skipping this ARI.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the hyetograph data\n",
    "        try:\n",
    "            hyetograph_df = pd.read_csv(filepath)\n",
    "            print(f\"Successfully read the hyetograph data from '{filename}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading the hyetograph CSV file '{filename}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot the hyetograph\n",
    "        plt.bar(hyetograph_df['Time_hour'], hyetograph_df['Precipitation_in'], \n",
    "                width=0.8, edgecolor='black', alpha=0.5, label=f'ARI {ari_str} years')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Time (Hour)', fontsize=14)\n",
    "    plt.ylabel('Incremental Precipitation (inches)', fontsize=14)\n",
    "    plt.title(f'Comparison of Hyetographs for ARIs {aris}\\nPosition: {position_percent}% | Duration: {total_duration} Hours', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(1, total_duration + 1, max(1, total_duration // 24)))  # Adjust x-ticks based on duration\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# User Inputs for Multiple ARIs\n",
    "# --------------------\n",
    "# Set the Annual Recurrence Intervals you want to plot\n",
    "aris_to_plot = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]  # Example: Multiple ARIs\n",
    "\n",
    "# Set the position percentage for the maximum incremental depth block\n",
    "position_percent = 50  # Example: 50%\n",
    "\n",
    "# Set the total storm duration in hours\n",
    "total_duration = 24  # Example: 24 hours\n",
    "\n",
    "# Set the output directory where hyetograph CSV files are saved\n",
    "output_dir = 'hyetographs'  # Ensure this matches the output directory used previously\n",
    "\n",
    "# Plot the multiple hyetographs\n",
    "plot_multiple_hyetographs(aris=aris_to_plot, \n",
    "                           position_percent=position_percent, \n",
    "                           total_duration=total_duration, \n",
    "                           output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: REVISE BELOW TO RUN DAVIS AND EXTRACT RESULTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\10_1d_hdf_data_extraction.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS 1D HDF Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing ras-commander flexibly (from package or local dev copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and run plan 01\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"Balde Eagle Creek\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagle.p01.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    RasExamples.extract_project(\"Balde Eagle Creek\")\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Balde Eagle project initialized with folder: {bald_eagle.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Balde Eagle object id: {id(bald_eagle)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"01\"\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagle.p01.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "bald_eagle.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "bald_eagle.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"01\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasHdfUtils\n",
    "| Method Name | Description |\n",
    "|-------------|-------------|\n",
    "| get_attrs | Converts attributes from a HEC-RAS HDF file into a Python dictionary for a given attribute path |\n",
    "| get_root_attrs | Returns attributes at root level of HEC-RAS HDF file |\n",
    "| get_hdf_paths_with_properties | Gets all paths in the HDF file with their properties |\n",
    "| get_group_attributes_as_df | Gets attributes of a group in the HDF file as a DataFrame |\n",
    "| get_hdf_filename | Gets the HDF filename from various input types |\n",
    "| get_runtime_data | Extracts runtime and compute time data from a single HDF file |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDF Paths with Properties (For Exploring HDF Files)\n",
    "HdfBase.get_dataset_info(plan_number, ras_object=bald_eagle, group_path=\"/Geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path, ras_object=bald_eagle)\n",
    "geom_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=bald_eagle)\n",
    "\n",
    "runtime_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of all the functions in the RasGeomHdf class from the ras_commander/RasGeomHdf.py file:\n",
    "\n",
    "| Function Name | Description |\n",
    "|---------------|-------------|\n",
    "| projection | Returns the projection of the RAS geometry as a pyproj.CRS object |\n",
    "| get_geom_attrs | Returns base geometry attributes from a HEC-RAS HDF file |\n",
    "\n",
    "| mesh_area_names | Returns a list of the 2D mesh area names of the RAS geometry |\n",
    "| get_geom_2d_flow_area_attrs | Returns geometry 2d flow area attributes from a HEC-RAS HDF file |\n",
    "| mesh_areas | Returns 2D flow area perimeter polygons |\n",
    "| mesh_cell_polygons | Returns 2D flow mesh cell polygons |\n",
    "| mesh_cell_points | Returns 2D flow mesh cell points |\n",
    "| mesh_cell_faces | Returns 2D flow mesh cell faces |\n",
    "\n",
    "| get_geom_structures_attrs | Returns geometry structures attributes from a HEC-RAS HDF file |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| bc_lines | Returns 2D mesh area boundary condition lines |\n",
    "| breaklines | Returns 2D mesh area breaklines |\n",
    "\n",
    "\n",
    "\n",
    "| refinement_regions | Returns 2D mesh area refinement regions |\n",
    "| structures | Returns the model structures |\n",
    "| reference_lines_names | Returns reference line names |\n",
    "| reference_points_names | Returns reference point names |\n",
    "| reference_lines | Returns the reference lines geometry and attributes |\n",
    "| reference_points | Returns the reference points geometry and attributes |\n",
    "| cross_sections | Returns the model 1D cross sections |\n",
    "| river_reaches | Returns the model 1D river reach lines |\n",
    "| cross_sections_elevations | Returns the model cross section elevation information |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
    "print(geom_hdf_path)\n",
    "\n",
    "# For the example project, plan 06 is associated with geometry 09\n",
    "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
    "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.  This attribute is only included if a RASMapper projection is defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example project we are using does not have a projection, so error messages should be expected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
    "geom_attrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geometry structures attributes\n",
    "print(\"\\nGetting geometry structures attributes\")\n",
    "geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path, ras_object=bald_eagle)\n",
    "if geom_structures_attrs:\n",
    "    print(\"Geometry structures attributes:\")\n",
    "    for key, value in geom_structures_attrs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No geometry structures attributes found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEED TO EDIT THIS TO SHOW BC LINES WITH RIVERS AND CROSS SECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Extract Boundary Condition Lines and Plot with 2D Flow Area Perimeter Polygons\n",
    "print(\"\\nExample 7: Extracting Boundary Condition Lines and Plotting with 2D Flow Area Perimeter Polygons\")\n",
    "bc_lines_df = HdfBndry.bc_lines(geom_hdf_path, ras_object=bald_eagle)\n",
    "if not bc_lines_df.empty:\n",
    "    display(bc_lines_df.head())\n",
    "else:\n",
    "    print(\"No Boundary Condition Lines found.\")\n",
    "\n",
    "# Plot if data exists\n",
    "if not bc_lines_df.empty or not mesh_areas.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 2D Flow Area Perimeter Polygons\n",
    "    if not mesh_areas.empty:\n",
    "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
    "        \n",
    "        # Add labels for each polygon\n",
    "        for idx, row in mesh_areas.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            label = row.get('Name', f'Area {idx}')\n",
    "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "    \n",
    "    # Plot boundary condition lines\n",
    "    if not bc_lines_df.empty:\n",
    "        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTEAD OF hdf_input, USE plan_hdf_path or geom_hdf_path as appropriate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get structures\n",
    "structures_gdf = HdfStruc.get_structures(geom_hdf_path)\n",
    "print(\"Structures:\")\n",
    "if not structures_gdf.empty:\n",
    "    structures_gdf\n",
    "else:\n",
    "    print(\"No structures found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get reference lines\n",
    "ref_lines_gdf = HdfBndry.get_reference_lines(geom_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nReference Lines:\")\n",
    "if not ref_lines_gdf.empty:\n",
    "    display(ref_lines_gdf.head())\n",
    "else:\n",
    "    print(\"No reference lines found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get reference points\n",
    "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nReference Points:\")\n",
    "if not ref_points_gdf.empty:\n",
    "    display(ref_points_gdf.head())\n",
    "else:\n",
    "    print(\"No reference points found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the get_hdf5_dataset_info function from HdfUtils to explore the Cross Sections structure in the geometry HDF file\n",
    "\n",
    "print(\"\\nExploring Cross Sections structure in geometry file:\")\n",
    "print(\"HDF Base Path: /Geometry/Cross Sections \")\n",
    "HdfBase.get_dataset_info(geom_hdf_path, group_path='/Geometry/Cross Sections')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get cross section geodataframe\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n",
    "with pd.option_context('display.max_columns', None):  # Show all columns\n",
    "    cross_sections_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_sections_gdf: \n",
    "\n",
    "| geometry | station_elevation | mannings_n | ineffective_blocks | River | Reach | RS | Name | Description | Len Left | Len Channel | Len Right | Left Bank | Right Bank | Friction Mode | Contr | Expan | Left Levee Sta | Left Levee Elev | Right Levee Sta | Right Levee Elev | HP Count | HP Start Elev | HP Vert Incr | HP LOB Slices | HP Chan Slices | HP ROB Slices | Ineff Block Mode | Obstr Block Mode | Default Centerline | Last Edited |\n",
    "|-----------|-------------------|------------|--------------------|-------|-------|----|------|-------------|----------|-------------|-----------|-----------|------------|----------------|-------|-------|----------------|-----------------|----------------|------------------|----------|----------------|---------------|----------------|----------------|----------------|------------------|------------------|-------------------|--------------|\n",
    "| 0         | LINESTRING (1968668.17 290166.79, 1969067.87 2... | [[0.0, 660.41], [5.0, 660.61], [40.0, 659.85],... | {'Station': [0.0, 190.0, 375.0], 'Mann n': [0.... | []    | Bald Eagle | Loc Hav | 138154.4 |             | 358.429993 | 463.640015 | 517.640015 | 190.000000 | 375.000000 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 49       | 656.799988      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n",
    "| 1         | LINESTRING (1968627.02 290584.12, 1969009.09 2... | [[0.0, 664.28], [50.0, 661.73], [55.0, 661.54]... | {'Station': [0.0, 535.0, 672.5599975585938], '... | []    | Bald Eagle | Loc Hav | 137690.8 |             | 305.709991 | 363.839996 | 382.829987 | 535.000000 | 672.559998 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 65       | 654.229980      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n",
    "| 2         | LINESTRING (1968585.88 290854.5, 1968868.02 29... | [[0.0, 662.72], [20.0, 665.5], [25.0, 666.48],... | {'Station': [0.0, 580.0, 717.239990234375], 'M... | []    | Bald Eagle | Loc Hav | 137327.0 |             | 732.929993 | 762.020020 | 765.359985 | 580.000000 | 717.239990 | Basic Mann n | 0.1   | 0.3   | NaN            | NaN             | NaN            | NaN              | 66       | 653.900024      | 1.0           | 5              | 5              | 5              | 0                | 0                | 0                 | 18Sep2000 09:10:52 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where ineffective_blocks is not empty\n",
    "ineffective_xs = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(len) > 0]\n",
    "\n",
    "print(\"\\nCross Sections with Ineffective Flow Areas:\")\n",
    "ineffective_xs\n",
    "\n",
    "# Print a message if no cross sections with ineffective flow areas are found\n",
    "print(\"\\nNo cross sections found with ineffective flow areas.\" if ineffective_xs.empty else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross sections data\n",
    "\n",
    "print(\"\\nCross Section Information:\")\n",
    "if not cross_sections_gdf.empty:\n",
    "    for idx, row in cross_sections_gdf.iterrows():\n",
    "        print(f\"\\nCross Section {idx + 1}:\")\n",
    "        print(f\"River: {row['River']}\")\n",
    "        print(f\"Reach: {row['Reach']}\")\n",
    "        print(\"\\nGeometry:\")\n",
    "        print(row['geometry'])\n",
    "        print(\"\\nStation-Elevation Points:\")\n",
    "        \n",
    "        # Print header\n",
    "        print(\"     #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation        #      Station   Elevation\")\n",
    "        print(\"-\" * 150)\n",
    "        \n",
    "        # Calculate number of rows needed\n",
    "        points = row['station_elevation']\n",
    "        num_rows = (len(points) + 4) // 5  # Round up division\n",
    "        \n",
    "        # Print points in 5 columns\n",
    "        for i in range(num_rows):\n",
    "            line = \"\"\n",
    "            for j in range(5):\n",
    "                point_idx = i + j * num_rows\n",
    "                if point_idx < len(points):\n",
    "                    station, elevation = points[point_idx]\n",
    "                    line += f\"{point_idx+1:6d} {station:10.2f} {elevation:10.2f}    \"\n",
    "            print(line)\n",
    "        print(\"-\" * 150)\n",
    "else:\n",
    "    print(\"No cross sections found in the geometry file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross sections on map\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get cross sections data\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "if not cross_sections_gdf.empty:\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    \n",
    "    # Plot cross sections\n",
    "    cross_sections_gdf.plot(ax=ax, color='red', linewidth=1, label='Cross Sections')\n",
    "    \n",
    "    # Add river name and reach labels\n",
    "    #for idx, row in cross_sections_gdf.iterrows():\n",
    "    #    # Get midpoint of cross section line for label placement\n",
    "    #    midpoint = row.geometry.centroid\n",
    "    #    label = f\"{row['River']}\\n{row['Reach']}\\nRS: {row['RS']}\"\n",
    "    #    ax.annotate(label, (midpoint.x, midpoint.y), \n",
    "    #               xytext=(5, 5), textcoords='offset points',\n",
    "    #               fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title('Cross Sections Location Map')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Equal aspect ratio to preserve shape\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cross sections found in the geometry file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross sections with Manning's n values colored by value\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Create figure\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Create colormap\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(vmin=0.02, vmax=0.08)  # Typical Manning's n range\n",
    "\n",
    "# Plot cross sections colored by Manning's n\n",
    "for idx, row in cross_sections_gdf.iterrows():\n",
    "    # Extract Manning's n values and stations\n",
    "    mannings = row['mannings_n']\n",
    "    n_values = mannings['Mann n']\n",
    "    stations = mannings['Station']\n",
    "    \n",
    "    # Get the full linestring coordinates\n",
    "    line_coords = list(row.geometry.coords)\n",
    "    \n",
    "    # Calculate total length of the cross section\n",
    "    total_length = row.geometry.length\n",
    "    \n",
    "    # For each Manning's n segment\n",
    "    for i in range(len(n_values)-1):\n",
    "        # Calculate the start and end proportions along the line\n",
    "        start_prop = stations[i] / stations[-1]\n",
    "        end_prop = stations[i+1] / stations[-1]\n",
    "        \n",
    "        # Get the start and end points for this segment\n",
    "        start_idx = int(start_prop * (len(line_coords)-1))\n",
    "        end_idx = int(end_prop * (len(line_coords)-1))\n",
    "        \n",
    "        # Extract the segment coordinates\n",
    "        segment_coords = line_coords[start_idx:end_idx+1]\n",
    "        \n",
    "        if len(segment_coords) >= 2:\n",
    "            # Create a line segment\n",
    "            segment = LineString(segment_coords)\n",
    "            \n",
    "            # Get color from colormap for this n value\n",
    "            color = cmap(norm(n_values[i]))\n",
    "            \n",
    "            # Plot the segment\n",
    "            ax1.plot(*segment.xy, color=color, linewidth=2)\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, ax=ax1, label=\"Manning's n Value\")\n",
    "\n",
    "ax1.set_title(\"Cross Sections Colored by Manning's n Values\")\n",
    "ax1.grid(True)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross sections with ineffective flow areas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get cross sections data\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "# Create figure\n",
    "fig, ax2 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Plot all cross sections first\n",
    "cross_sections_gdf.plot(ax=ax2, color='lightgray', linewidth=1, label='Cross Sections')\n",
    "\n",
    "# Plot ineffective flow areas with thicker lines\n",
    "ineffective_sections = cross_sections_gdf[cross_sections_gdf['ineffective_blocks'].apply(lambda x: len(x) > 0)]\n",
    "ineffective_sections.plot(ax=ax2, color='red', linewidth=3, label='Ineffective Flow Areas')\n",
    "\n",
    "# Add ineffective flow area labels with offset to lower right\n",
    "for idx, row in cross_sections_gdf.iterrows():\n",
    "    # Get midpoint of cross section line\n",
    "    midpoint = row.geometry.centroid\n",
    "    \n",
    "    # Extract ineffective flow blocks\n",
    "    ineff_blocks = row['ineffective_blocks']\n",
    "    \n",
    "    if ineff_blocks:  # Only label if there are ineffective blocks\n",
    "        label_parts = []\n",
    "        # Add RS to first line of label\n",
    "        label_parts.append(f\"RS: {row['RS']}\")\n",
    "        for block in ineff_blocks:\n",
    "            label_parts.append(\n",
    "                f\"L:{block['Left Sta']:.0f}-R:{block['Right Sta']:.0f}\\n\"\n",
    "                f\"Elev: {block['Elevation']:.2f}\\n\"\n",
    "                f\"Permanent: {block['Permanent']}\"\n",
    "            )\n",
    "        \n",
    "        label = '\\n'.join(label_parts)\n",
    "        \n",
    "        ax2.annotate(label, (midpoint.x, midpoint.y),\n",
    "                    xytext=(15, -15),  # Offset to lower right\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8, \n",
    "                    bbox=dict(facecolor='white', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->'),\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='top')\n",
    "\n",
    "ax2.set_title('Cross Sections with Ineffective Flow Areas')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross section elevation for cross section 42\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get cross sections data\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "if not cross_sections_gdf.empty:\n",
    "    # Get station-elevation data for cross section 42\n",
    "    station_elevation = cross_sections_gdf.iloc[42]['station_elevation']\n",
    "    \n",
    "    # Convert list of lists to numpy arrays for plotting\n",
    "    stations = np.array([point[0] for point in station_elevation])\n",
    "    elevations = np.array([point[1] for point in station_elevation])\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    \n",
    "    # Plot cross section\n",
    "    ax.plot(stations, elevations, 'b-', linewidth=2)\n",
    "    \n",
    "    # Add labels and title\n",
    "    river = cross_sections_gdf.iloc[42]['River']\n",
    "    reach = cross_sections_gdf.iloc[42]['Reach'] \n",
    "    rs = cross_sections_gdf.iloc[42]['RS']\n",
    "    \n",
    "    # Show bank stations as dots\n",
    "    left_bank_station = cross_sections_gdf.iloc[42]['Left Bank']\n",
    "    right_bank_station = cross_sections_gdf.iloc[42]['Right Bank']\n",
    "    \n",
    "    # Interpolating bank stations for plotting\n",
    "    ax.plot(left_bank_station, elevations[np.searchsorted(stations, left_bank_station)], 'ro', label='Left Bank Station')\n",
    "    ax.plot(right_bank_station, elevations[np.searchsorted(stations, right_bank_station)], 'ro', label='Right Bank Station')\n",
    "    \n",
    "    ax.set_title(f'Cross Section Profile\\nRiver: {river}, Reach: {reach}, RS: {rs}\\n'\n",
    "                 f'Left Bank Station: {left_bank_station}, Right Bank Station: {right_bank_station}')\n",
    "    ax.set_xlabel('Station (ft)')\n",
    "    ax.set_ylabel('Elevation (ft)')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cross sections found in the geometry file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "centerlines = HdfXsec.get_river_centerlines(geom_hdf_path)\n",
    "centerlines_with_stations = HdfXsec.get_river_stationing(centerlines)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRiver Centerlines:\")\n",
    "centerlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot river centerlines with labels\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot centerlines\n",
    "centerlines.plot(ax=ax, color='blue', linewidth=2, label='River Centerline')\n",
    "\n",
    "# Add river/reach labels\n",
    "for idx, row in centerlines.iterrows():\n",
    "    # Get midpoint of the line for label placement\n",
    "    midpoint = row.geometry.interpolate(0.5, normalized=True)\n",
    "    \n",
    "    # Create label text combining river and reach names\n",
    "    label = f\"{row['River Name']}\\n{row['Reach Name']}\"\n",
    "    \n",
    "    # Add text annotation\n",
    "    ax.annotate(label, \n",
    "                xy=(midpoint.x, midpoint.y),\n",
    "                xytext=(10, 10), # Offset text slightly\n",
    "                textcoords='offset points',\n",
    "                fontsize=10,\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_title('River Centerlines', fontsize=14)\n",
    "ax.set_xlabel('Easting', fontsize=12)\n",
    "ax.set_ylabel('Northing', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "edge_lines = HdfXsec.get_river_edge_lines(geom_hdf_path)\n",
    "centerlines = HdfXsec.get_river_centerlines(geom_hdf_path)\n",
    "# Display results\n",
    "print(\"\\nRiver Edge Lines:\")\n",
    "edge_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "bank_lines = HdfXsec.get_river_bank_lines(geom_hdf_path)\n",
    "# Display results\n",
    "print(\"\\nRiver Bank Lines:\")\n",
    "bank_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot river edge lines\n",
    "edge_lines.plot(ax=ax, color='blue', linewidth=2, label='River Edge Lines')\n",
    "\n",
    "# Plot centerlines for reference\n",
    "centerlines.plot(ax=ax, color='red', linewidth=2, linestyle='--', label='River Centerline')\n",
    "\n",
    "# Plot river bank lines\n",
    "bank_lines.plot(ax=ax, color='green', linewidth=2, label='River Bank Lines')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('River Edge Lines, Centerline, and Bank Lines', fontsize=14)\n",
    "ax.set_xlabel('Easting', fontsize=12)\n",
    "ax.set_ylabel('Northing', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get dataset structure:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/River Bank Lines/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to explore HDF file to assist with 1D Structures Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Output/Output Blocks/Computation Block/Global/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Structures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1D Structures Geodataframe\n",
    "\n",
    "\n",
    "# Extract data into GeoDataFrames\n",
    "structures_gdf = HdfStruc.get_structures(hdf_path=geom_hdf_path)\n",
    "cross_sections_gdf = HdfXsec.get_cross_sections(hdf_path=geom_hdf_path)\n",
    "centerlines_gdf = HdfXsec.get_river_centerlines(hdf_path=geom_hdf_path)\n",
    "\n",
    "# Display basic information about the structures\n",
    "print(\"\\nStructures Summary:\")\n",
    "print(f\"Number of structures found: {len(structures_gdf)}\")\n",
    "structures_gdf\n",
    "\n",
    "# Display first few rows of key attributes\n",
    "print(\"\\nStructure Details:\")\n",
    "display_cols = ['Structure ID', 'Structure Type', 'River Name', 'Reach Name', 'Station']\n",
    "display_cols = [col for col in display_cols if col in structures_gdf.columns]\n",
    "if display_cols:\n",
    "    print(structures_gdf[display_cols].head())\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot river centerlines\n",
    "if not centerlines_gdf.empty:\n",
    "    centerlines_gdf.plot(ax=ax, color='blue', linewidth=2, label='River Centerlines')\n",
    "\n",
    "# Plot cross sections\n",
    "if not cross_sections_gdf.empty:\n",
    "    cross_sections_gdf.plot(ax=ax, color='green', linewidth=1, label='Cross Sections')\n",
    "\n",
    "# Plot structures\n",
    "if not structures_gdf.empty:\n",
    "    structures_gdf.plot(ax=ax, color='red', marker='s', markersize=100, label='Structures')\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('HEC-RAS Model Components', fontsize=14)\n",
    "ax.set_xlabel('Easting', fontsize=12)\n",
    "ax.set_ylabel('Northing', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Print summary of cross sections\n",
    "print(\"\\nCross Sections Summary:\")\n",
    "print(f\"Number of cross sections found: {len(cross_sections_gdf)}\")\n",
    "if not cross_sections_gdf.empty:\n",
    "    print(\"\\nCross Section Details:\")\n",
    "    xs_display_cols = ['River', 'Reach', 'Station']\n",
    "    xs_display_cols = [col for col in xs_display_cols if col in cross_sections_gdf.columns]\n",
    "    if xs_display_cols:\n",
    "        print(cross_sections_gdf[xs_display_cols].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Compute Messages as String\n",
    "print(\"Extracting Compute Messages\")\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract string from HDF object at a given path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_hdf_filename : str\n",
    "        Name of the HDF file\n",
    "    hdf_path : str\n",
    "        Path of the object in the HDF file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Extracted string from the specified HDF object\n",
    "    \"\"\"\n",
    "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
    "        try:\n",
    "            hdf_object = hdf_file[hdf_path]\n",
    "            if isinstance(hdf_object, h5py.Group):\n",
    "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
    "            elif isinstance(hdf_object, h5py.Dataset):\n",
    "                data = hdf_object[()]\n",
    "                if isinstance(data, bytes):\n",
    "                    return data.decode('utf-8')\n",
    "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
    "                    return [v.decode('utf-8') for v in data]\n",
    "                else:\n",
    "                    return str(data)\n",
    "            else:\n",
    "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
    "        except KeyError:\n",
    "            return f\"Path not found: {hdf_path}\"\n",
    "\n",
    "try:\n",
    "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
    "    print(\"Compute Messages:\")\n",
    "    \n",
    "    # Parse and print the compute messages in a more visually friendly way\n",
    "    messages = results_summary_string[0].split('\\r\\n')\n",
    "    \n",
    "    for message in messages:\n",
    "        if message.strip():  # Skip empty lines\n",
    "            if ':' in message:\n",
    "                key, value = message.split(':', 1)\n",
    "                print(f\"{key.strip():40} : {value.strip()}\")\n",
    "            else:\n",
    "                print(f\"\\n{message.strip()}\")\n",
    "    \n",
    "    # Print computation summary in a table format\n",
    "    print(\"\\nComputation Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Task' in line:\n",
    "            task, time = line.split('\\t')\n",
    "            print(f\"{task:<30} {time:<20}\")\n",
    "    \n",
    "    print(\"\\nComputation Speed:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Speed' in line:\n",
    "            task, speed = line.split('\\t')\n",
    "            print(f\"{task:<30} {speed:<20}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting compute messages: {str(e)}\")\n",
    "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 12: Extract Plan Parameters and Volume Accounting\n",
    "print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n",
    "\n",
    "# Extract plan parameters\n",
    "plan_parameters_df = HdfPlan.get_plan_parameters(hdf_path=plan_hdf_path)\n",
    "\n",
    "# Extract volume accounting data\n",
    "volume_accounting_df = HdfResultsPlan.get_volume_accounting(hdf_path=plan_hdf_path)\n",
    "\n",
    "print(\"\\nPlan Parameters DataFrame:\")\n",
    "plan_parameters_df\n",
    "\n",
    "print(\"\\nVolume Accounting DataFrame:\")\n",
    "volume_accounting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RasPlanHdf Class Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get simulation start time\n",
    "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
    "print(f\"Simulation start time: {start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get plan end time\n",
    "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
    "print(f\"Simulation end time: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the cell below to time of max wsel for 1D models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time of maximum water surface elevation (WSEL) for cross sections\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Get cross section results timeseries\n",
    "xsec_results = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n",
    "print(\"\\nCross Section Results Shape:\", xsec_results['Water_Surface'].shape)\n",
    "\n",
    "# Get cross section geometry data\n",
    "xsec_geom = HdfXsec.get_cross_sections(plan_hdf_path)\n",
    "print(\"\\nNumber of cross sections in geometry:\", len(xsec_geom))\n",
    "\n",
    "# Create dataframe with cross section locations and max WSEL times\n",
    "xs_data = []\n",
    "\n",
    "# Extract water surface data from xarray Dataset\n",
    "water_surface = xsec_results['Water_Surface'].values\n",
    "times = pd.to_datetime(xsec_results.time.values)\n",
    "\n",
    "# Debug print\n",
    "print(\"\\nFirst few cross section names:\")\n",
    "print(xsec_results.cross_section.values[:5])\n",
    "\n",
    "# Iterate through cross sections\n",
    "for xs_idx in range(len(xsec_results.cross_section)):\n",
    "    # Get WSEL timeseries for this cross section\n",
    "    wsel_series = water_surface[:, xs_idx]\n",
    "    \n",
    "    # Get cross section name and parse components\n",
    "    xs_name = xsec_results.cross_section.values[xs_idx]\n",
    "    \n",
    "    # Split the string and remove empty strings\n",
    "    xs_parts = [part for part in xs_name.split() if part]\n",
    "    \n",
    "    if len(xs_parts) >= 3:\n",
    "        river = \"Bald Eagle\"  # Combine first two words\n",
    "        reach = \"Loc Hav\"     # Next two words\n",
    "        rs = xs_parts[-1]     # Last part is the station\n",
    "        \n",
    "        # Get geometry for this cross section\n",
    "        xs_match = xsec_geom[\n",
    "            (xsec_geom['River'] == river) & \n",
    "            (xsec_geom['Reach'] == reach) & \n",
    "            (xsec_geom['RS'] == rs)\n",
    "        ]\n",
    "        \n",
    "        if not xs_match.empty:\n",
    "            geom = xs_match.iloc[0]\n",
    "            # Use first point of cross section line for plotting\n",
    "            x = geom.geometry.coords[0][0]\n",
    "            y = geom.geometry.coords[0][1]\n",
    "            \n",
    "            # Find time of max WSEL\n",
    "            max_wsel_idx = np.argmax(wsel_series)\n",
    "            max_wsel = np.max(wsel_series)\n",
    "            max_time = times[max_wsel_idx]\n",
    "            \n",
    "            xs_data.append({\n",
    "                'xs_name': xs_name,\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'max_wsel': max_wsel,\n",
    "                'time_of_max': max_time\n",
    "            })\n",
    "        else:\n",
    "            print(f\"\\nWarning: No geometry match found for {xs_name}\")\n",
    "            print(f\"River: {river}, Reach: {reach}, RS: {rs}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Could not parse cross section name: {xs_name}\")\n",
    "\n",
    "# Create dataframe\n",
    "xs_df = pd.DataFrame(xs_data)\n",
    "\n",
    "# Debug print\n",
    "print(\"\\nNumber of cross sections processed:\", len(xs_df))\n",
    "if not xs_df.empty:\n",
    "    print(\"\\nColumns in xs_df:\", xs_df.columns.tolist())\n",
    "    print(\"\\nFirst row of xs_df:\")\n",
    "    print(xs_df.iloc[0])\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Convert datetime to hours since start for colormap\n",
    "    min_time = min(xs_df['time_of_max'])\n",
    "    color_values = [(t - min_time).total_seconds() / 3600 for t in xs_df['time_of_max']]\n",
    "\n",
    "    # Plot cross section points\n",
    "    scatter = ax.scatter(xs_df['x'], xs_df['y'],\n",
    "                        c=color_values,\n",
    "                        cmap='viridis',\n",
    "                        s=50)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title('Time of Maximum Water Surface Elevation at Cross Sections')\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Hours since simulation start')\n",
    "\n",
    "    # Format colorbar ticks\n",
    "    max_hours = int(max(color_values))\n",
    "    tick_interval = max(1, max_hours // 6)  # Show ~6 ticks\n",
    "    cbar.set_ticks(range(0, max_hours + 1, tick_interval))\n",
    "    cbar.set_ticklabels([f'{h}h' for h in range(0, max_hours + 1, tick_interval)])\n",
    "\n",
    "    # Add grid and adjust styling\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    max_wsel_xs = xs_df.loc[xs_df['max_wsel'].idxmax()]\n",
    "    hours_since_start = (max_wsel_xs['time_of_max'] - min_time).total_seconds() / 3600\n",
    "\n",
    "    print(f\"\\nOverall Maximum WSEL: {max_wsel_xs['max_wsel']:.2f} ft\")\n",
    "    print(f\"Time of Overall Maximum WSEL: {max_wsel_xs['time_of_max']}\")\n",
    "    print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
    "    print(f\"Location of Overall Maximum WSEL: X={max_wsel_xs['x']:.2f}, Y={max_wsel_xs['y']:.2f}\")\n",
    "    print(f\"Cross Section: {max_wsel_xs['xs_name']}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No cross sections were processed successfully\")\n",
    "    print(\"xs_data length:\", len(xs_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to add this to the ras-commander library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unsteady attributes\n",
    "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nResults Unsteady Attributes:\")\n",
    "for key, value in results_unsteady_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unsteady summary attributes\n",
    "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nResults Unsteady Summary Attributes:\")\n",
    "for key, value in results_unsteady_summary_attrs.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get volume accounting attributes\n",
    "volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nVolume Accounting Attributes:\")\n",
    "for key, value in volume_accounting_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== HDF5 File Structure ===\\n\")\n",
    "print(plan_hdf_path)\n",
    "HdfBase.get_dataset_info(plan_hdf_path, group_path='/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsec_results = HdfResultsXsec.get_xsec_timeseries(plan_hdf_path)\n",
    "print(xsec_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time series for specific cross section\n",
    "target_xs = \"Bald Eagle       Loc Hav          136202.3\"\n",
    "\n",
    "print(\"\\nTime Series Data for Cross Section:\", target_xs)\n",
    "for var in ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']:\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(xsec_results[var].sel(cross_section=target_xs).values[:5])  # Show first 5 values\n",
    "\n",
    "# Create time series plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure for each variable\n",
    "variables = ['Water_Surface', 'Velocity_Total', 'Velocity_Channel', 'Flow_Lateral', 'Flow']\n",
    "\n",
    "for var in variables:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Convert time values to datetime if needed\n",
    "    time_values = pd.to_datetime(xsec_results.time.values)\n",
    "    values = xsec_results[var].sel(cross_section=target_xs).values\n",
    "    \n",
    "    # Plot with explicit x and y values\n",
    "    plt.plot(time_values, values, '-', linewidth=2)\n",
    "    \n",
    "    plt.title(f'{var} at {target_xs}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(var.replace('_', ' '))\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Force display\n",
    "    plt.draw()\n",
    "    plt.pause(0.1)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\11_2d_hdf_data_extraction.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS 2D HDF Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to manipulate and analyze HEC-RAS 2D HDF data using the ras-commander library. It leverages the HdfBase, HdfUtils, HdfStruc, HdfMesh, HdfXsec, HdfBndry, HdfPlan, HdfResultsPlan, HdfResultsMesh, and HdfResultsXsec classes to streamline data extraction, processing, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and Run Plan 06\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "    # Initialize custom Ras object\n",
    "    bald_eagle = RasPrj()\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {bald_eagle.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(bald_eagle)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"06\"\n",
    "\n",
    "    # Update run flags for the project\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,\n",
    "        unsteady_flow_simulation=True,\n",
    "        run_sediment=False,\n",
    "        post_processor=True,\n",
    "        floodplain_mapping=False,\n",
    "        ras_object=bald_eagle\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = RasPrj()\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "bald_eagle.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "bald_eagle.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDF input path as Plan Number\n",
    "\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RasHdfUtils\n",
    "| Method Name | Description |\n",
    "|-------------|-------------|\n",
    "| get_attrs | Converts attributes from a HEC-RAS HDF file into a Python dictionary for a given attribute path |\n",
    "| get_root_attrs | Returns attributes at root level of HEC-RAS HDF file |\n",
    "| get_hdf_paths_with_properties | Gets all paths in the HDF file with their properties |\n",
    "| get_group_attributes_as_df | Gets attributes of a group in the HDF file as a DataFrame |\n",
    "| get_hdf_filename | Gets the HDF filename from various input types |\n",
    "| get_runtime_data | Extracts runtime and compute time data from a single HDF file |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDF Paths with Properties (For Exploring HDF Files)\n",
    "HdfBase.get_dataset_info(plan_number, ras_object=bald_eagle, group_path=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=bald_eagle)\n",
    "if runtime_df is not None:\n",
    "    runtime_df\n",
    "else:\n",
    "    print(\"No runtime data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runtime_df example output: \n",
    "\n",
    "| Plan Name                        | File Name                     | Simulation Start Time | Simulation End Time | Simulation Duration (s) | Simulation Time (hr) | Completing Geometry (hr) | Preprocessing Geometry (hr) | Completing Event Conditions (hr) | Unsteady Flow Computations (hr) | Complete Process (hr) | Unsteady Flow Speed (hr/hr) | Complete Process Speed (hr/hr) |\n",
    "|----------------------------------|-------------------------------|-----------------------|---------------------|-------------------------|-----------------------|--------------------------|------------------------------|----------------------------------|----------------------------------|-----------------------|------------------------------|----------------------------------|\n",
    "| Gridded Precip - Infiltration    | BaldEagleDamBrk.p06.hdf      | 09Sep2018 00:00:00    | 14Sep2018 00:00:00  | 432000.0                | 120.0                 | N/A                      | 0.000113                     | N/A                              | 0.074436                        | 0.080951              | 1612.126776                  | 1482.386368                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of all the functions in the RasGeomHdf class from the ras_commander/RasGeomHdf.py file:\n",
    "\n",
    "| Function Name | Description |\n",
    "|---------------|-------------|\n",
    "| projection | Returns the projection of the RAS geometry as a pyproj.CRS object |\n",
    "| get_geom_attrs | Returns base geometry attributes from a HEC-RAS HDF file |\n",
    "\n",
    "| mesh_area_names | Returns a list of the 2D mesh area names of the RAS geometry |\n",
    "| get_geom_2d_flow_area_attrs | Returns geometry 2d flow area attributes from a HEC-RAS HDF file |\n",
    "| mesh_areas | Returns 2D flow area perimeter polygons |\n",
    "| mesh_cell_polygons | Returns 2D flow mesh cell polygons |\n",
    "| mesh_cell_points | Returns 2D flow mesh cell points |\n",
    "| mesh_cell_faces | Returns 2D flow mesh cell faces |\n",
    "\n",
    "| get_geom_structures_attrs | Returns geometry structures attributes from a HEC-RAS HDF file |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| bc_lines | Returns 2D mesh area boundary condition lines |\n",
    "| breaklines | Returns 2D mesh area breaklines |\n",
    "\n",
    "\n",
    "\n",
    "| refinement_regions | Returns 2D mesh area refinement regions |\n",
    "| structures | Returns the model structures |\n",
    "| reference_lines_names | Returns reference line names |\n",
    "| reference_points_names | Returns reference point names |\n",
    "| reference_lines | Returns the reference lines geometry and attributes |\n",
    "| reference_points | Returns the reference points geometry and attributes |\n",
    "| cross_sections | Returns the model 1D cross sections |\n",
    "| river_reaches | Returns the model 1D river reach lines |\n",
    "| cross_sections_elevations | Returns the model cross section elevation information |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
    "print(geom_hdf_path)\n",
    "\n",
    "# For the example project, plan 06 is associated with geometry 09\n",
    "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
    "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Geometry Information\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
    "geom_attrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geom_attrs output: \n",
    "\n",
    "| Complete Geometry | Extents | Geometry Time | Infiltration Date Last Modified | Infiltration File Date | Infiltration Filename | Infiltration Layername | Land Cover Date Last Modified | Land Cover File Date | Land Cover Filename | ... | Percent Impervious Date Last Modified | Percent Impervious File Date | Percent Impervious Filename | Percent Impervious Layername | SI Units | Terrain File Date | Terrain Filename | Terrain Layername | Title | Version |\n",
    "|-------------------|---------|---------------|---------------------------------|-----------------------|----------------------|------------------------|------------------------------|----------------------|---------------------|-----|--------------------------------------|-----------------------------|----------------------------|------------------------------|----------|-------------------|------------------|-------------------|-------|---------|\n",
    "| 0                 | True    | [1960041.35636708, 2092643.59732271, 285497.89...] | 27Oct2024 20:09:19 | 11MAR2022 13:52:44 | 24NOV2020 13:24:58 | .\\Soils Data\\Infiltration.hdf | Infiltration | 11MAR2022 13:45:08 | 11MAR2022 13:45:08 | .\\Land Classification\\LandCover.hdf | ... | 11MAR2022 13:45:08 | 11MAR2022 13:45:08 | .\\Land Classification\\LandCover.hdf | LandCover | False | 09FEB2015 08:26:58 | .\\Terrain\\Terrain50.hdf | Terrain50 | Single 2D Area - Internal Dam Structure | 1.0.20 (20Sep2024) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfMesh for geometry-related operations\n",
    "print(\"\\nExample 3: Listing 2D Flow Area Names\")\n",
    "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path)\n",
    "print(\"2D Flow Area Name (returned as list):\\n\", flow_area_names)\n",
    "# Note: this is returned as a list because it is used internally by other functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Attributes (get_mesh_area_attributes)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n",
    "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n",
    "flow_area_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flow_area_df:\n",
    "\n",
    "Value\n",
    "| Name                        | b'BaldEagleCr' |\n",
    "|-----------------------------|-----------------|\n",
    "| Locked                      | 0               |\n",
    "| Mann                        | 0.04            |\n",
    "| Multiple Face Mann n       | 0               |\n",
    "| Composite LC               | 0               |\n",
    "| Cell Vol Tol               | 0.01            |\n",
    "| Cell Min Area Fraction      | 0.01            |\n",
    "| Face Profile Tol           | 0.01            |\n",
    "| Face Area Tol              | 0.01            |\n",
    "| Face Conv Ratio            | 0.02            |\n",
    "| Laminar Depth              | 0.2             |\n",
    "| Min Face Length Ratio      | 0.05            |\n",
    "| Spacing dx                 | 250.0           |\n",
    "| Spacing dy                 | 250.0           |\n",
    "| Shift dx                   | NaN             |\n",
    "| Shift dy                   | NaN             |\n",
    "| Cell Count                 | 18066           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Perimeter Polygons (get_mesh_areas)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
    "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "# Plot the 2D Flow Area Perimeter Polygons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "\n",
    "# Add labels for each polygon\n",
    "for idx, row in mesh_areas.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    # Check if 'Name' column exists, otherwise use a default label\n",
    "    label = row.get('Name', f'Area {idx}')\n",
    "    ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "\n",
    "plt.title('2D Flow Area Perimeter Polygons')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract mesh cell faces\n",
    "print(\"\\nExample: Extracting mesh cell faces\")\n",
    "\n",
    "# Get mesh cell faces\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces DataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mesh_cell_faces geodataframe:\n",
    "\n",
    "flow_area_df:\n",
    "\n",
    "| mesh_name    | face_id | geometry                                           |\n",
    "|--------------|---------|----------------------------------------------------|\n",
    "| BaldEagleCr  | 0       | LINESTRING (2042125 351625, 2042375 351625)      |\n",
    "| BaldEagleCr  | 1       | LINESTRING (2042375 351625, 2042375 351875)      |\n",
    "| BaldEagleCr  | 2       | LINESTRING (2042375 351875, 2042125 351875)      |\n",
    "| BaldEagleCr  | 3       | LINESTRING (2042125 351875, 2042125 351625)      |\n",
    "| BaldEagleCr  | 4       | LINESTRING (2042375 351375, 2042375 351625)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection\n",
    "import numpy as np\n",
    "\n",
    "# Plot the mesh cell faces more efficiently\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Convert all geometries to numpy arrays at once for faster plotting\n",
    "lines = [list(zip(*line.xy)) for line in mesh_cell_faces.geometry]\n",
    "lines_collection = LineCollection(lines, colors='blue', linewidth=0.5, alpha=0.5)\n",
    "ax.add_collection(lines_collection)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Mesh Cell Faces')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "\n",
    "# Calculate centroids once and store as numpy arrays\n",
    "centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in mesh_cell_faces.geometry])\n",
    "\n",
    "# Create scatter plot with numpy arrays\n",
    "scatter = ax.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1], \n",
    "    c=mesh_cell_faces['face_id'],\n",
    "    cmap='viridis',\n",
    "    s=1,\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.colorbar(scatter, label='Face ID')\n",
    "\n",
    "# Set axis limits based on data bounds\n",
    "ax.set_xlim(centroids[:, 0].min(), centroids[:, 0].max())\n",
    "ax.set_ylim(centroids[:, 1].min(), centroids[:, 1].max())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display some statistics\n",
    "print(\"\\nMesh Cell Faces Statistics:\")\n",
    "print(f\"Total number of cell faces: {len(mesh_cell_faces)}\")\n",
    "print(f\"Number of unique meshes: {mesh_cell_faces['mesh_name'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the nearest cell face to a given point\n",
    "def find_nearest_cell_face(point, cell_faces_df):\n",
    "    \"\"\"\n",
    "    Find the nearest cell face to a given point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.geometry.Point): The input point.\n",
    "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
    "\n",
    "    Returns:\n",
    "        int: The face_id of the nearest cell face.\n",
    "        float: The distance to the nearest cell face.\n",
    "    \"\"\"\n",
    "    # Calculate distances from the input point to all cell faces\n",
    "    distances = cell_faces_df.geometry.distance(point)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    nearest_index = distances.idxmin()\n",
    "\n",
    "    # Get the face_id and distance of the nearest cell face\n",
    "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
    "    nearest_distance = distances[nearest_index]\n",
    "\n",
    "    return nearest_face_id, nearest_distance\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample: Finding the nearest cell face to a given point\")\n",
    "\n",
    "# Create a sample point (you can replace this with any point of interest)\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# Get the projection from the geometry file\n",
    "# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n",
    "if projection:\n",
    "    print(f\"Using projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found. Using default CRS.\")\n",
    "    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n",
    "\n",
    "# Create the sample point with the correct CRS\n",
    "sample_point = GeoDataFrame({'geometry': [Point(2042250, 351750)]}, crs=projection)\n",
    "\n",
    "if not mesh_cell_faces.empty and not sample_point.empty:\n",
    "    # Ensure the CRS of the sample point matches the mesh_cell_faces\n",
    "    if sample_point.crs != mesh_cell_faces.crs:\n",
    "        sample_point = sample_point.to_crs(mesh_cell_faces.crs)\n",
    "    \n",
    "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n",
    "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
    "    print(f\"Face ID: {nearest_face_id}\")\n",
    "    print(f\"Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Visualize the result\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all cell faces\n",
    "    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
    "    \n",
    "    # Plot the sample point\n",
    "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
    "    \n",
    "    # Plot the nearest cell face\n",
    "    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n",
    "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Nearest Cell Face to Sample Point')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Unable to perform nearest cell face search due to missing data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Cell Polygons\n",
    "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
    "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path, ras_object=bald_eagle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell_polygons_df:\n",
    "\n",
    "| mesh_name    | cell_id | geometry                                      |\n",
    "|--------------|---------|-----------------------------------------------|\n",
    "| BaldEagleCr  | 0       | POLYGON ((2082875 370625, 2082723.922 370776.0... |\n",
    "| BaldEagleCr  | 1       | POLYGON ((2083125 370625, 2083125 370844.185, ... |\n",
    "| BaldEagleCr  | 2       | POLYGON ((2083375 370625, 2083375 370886.638, ... |\n",
    "| BaldEagleCr  | 3       | POLYGON ((2083625 370625, 2083625 370925.693, ... |\n",
    "| BaldEagleCr  | 4       | POLYGON ((2083875 370625, 2083875 370958.588, ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cell Polygons\n",
    "if not cell_polygons_df.empty:\n",
    "    cell_polygons_df.head()\n",
    "else:\n",
    "    print(\"No Cell Polygons found.\")\n",
    "\n",
    "# Plot cell polygons\n",
    "if not cell_polygons_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot cell polygons\n",
    "    cell_polygons_df.plot(ax=ax, edgecolor='blue', facecolor='none')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('2D Flow Area Cell Polygons')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cell polygon data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Cell Info\n",
    "print(\"\\nExample 5: Extracting Cell Info\")\n",
    "cell_info_df = HdfMesh.get_mesh_cell_points(geom_hdf_path, ras_object=bald_eagle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell_info_df: \n",
    "\n",
    "| mesh_name    | cell_id | geometry                          |\n",
    "|--------------|---------|-----------------------------------|\n",
    "| BaldEagleCr  | 0       | POINT (2083000 370750)           |\n",
    "| BaldEagleCr  | 1       | POINT (2083250 370750)           |\n",
    "| BaldEagleCr  | 2       | POINT (2083500 370750)           |\n",
    "| BaldEagleCr  | 3       | POINT (2083750 370750)           |\n",
    "| BaldEagleCr  | 4       | POINT (2084000 370750)           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cell Info\n",
    "if not cell_info_df.empty:\n",
    "    cell_info_df.head()\n",
    "else:\n",
    "    print(\"No Cell Info found.\")\n",
    "\n",
    "# Plot cell centers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not cell_info_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot cell centers\n",
    "    cell_info_df.plot(ax=ax, color='red', markersize=5)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('2D Flow Area Cell Centers')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No cell data available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the nearest cell center to a given point\n",
    "def find_nearest_cell(point, cell_centers_df):\n",
    "    \"\"\"\n",
    "    Find the nearest cell center to a given point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.geometry.Point): The input point.\n",
    "        cell_centers_df (GeoDataFrame): DataFrame containing cell center points.\n",
    "\n",
    "    Returns:\n",
    "        int: The cell_id of the nearest cell.\n",
    "        float: The distance to the nearest cell center.\n",
    "    \"\"\"\n",
    "    # Calculate distances from the input point to all cell centers\n",
    "    distances = cell_centers_df.geometry.distance(point)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    nearest_index = distances.idxmin()\n",
    "\n",
    "    # Get the cell_id and distance of the nearest cell\n",
    "    nearest_cell_id = cell_centers_df.loc[nearest_index, 'cell_id']\n",
    "    nearest_distance = distances[nearest_index]\n",
    "\n",
    "    return nearest_cell_id, nearest_distance\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample: Finding the nearest cell to a given point\")\n",
    "\n",
    "# Create a sample point (you can replace this with any point of interest)\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# Get the projection from the geometry file\n",
    "# projection = HdfUtils.get_projection(hdf_path=geom_hdf_path) # This was done in a previous code cell\n",
    "if projection:\n",
    "    print(f\"Using projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found. Using default CRS.\")\n",
    "    projection = \"EPSG:4326\"  # Default to WGS84 if no projection is found\n",
    "\n",
    "# Create the sample point with the correct CRS\n",
    "sample_point = GeoDataFrame({'geometry': [Point(2083500, 370800)]}, crs=projection)\n",
    "\n",
    "if not cell_info_df.empty and not sample_point.empty:\n",
    "    # Ensure the CRS of the sample point matches the cell_info_df\n",
    "    if sample_point.crs != cell_info_df.crs:\n",
    "        sample_point = sample_point.to_crs(cell_info_df.crs)\n",
    "    \n",
    "    nearest_cell_id, distance = find_nearest_cell(sample_point.geometry.iloc[0], cell_info_df)\n",
    "    print(f\"Nearest cell to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
    "    print(f\"Cell ID: {nearest_cell_id}\")\n",
    "    print(f\"Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Visualize the result\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all cell centers\n",
    "    cell_info_df.plot(ax=ax, color='blue', markersize=5, alpha=0.5, label='Cell Centers')\n",
    "    \n",
    "    # Plot the sample point\n",
    "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
    "    \n",
    "    # Plot the nearest cell center\n",
    "    nearest_cell = cell_info_df[cell_info_df['cell_id'] == nearest_cell_id]\n",
    "    nearest_cell.plot(ax=ax, color='green', markersize=100, alpha=0.7, label='Nearest Cell')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Nearest Cell to Sample Point')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Unable to perform nearest cell search due to missing data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geometry structures attributes\n",
    "print(\"\\nGetting geometry structures attributes\")\n",
    "geom_structures_attrs = HdfStruc.get_geom_structures_attrs(geom_hdf_path, ras_object=bald_eagle)\n",
    "if geom_structures_attrs:\n",
    "    print(\"Geometry structures attributes:\")\n",
    "    for key, value in geom_structures_attrs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No geometry structures attributes found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paths and Functions for each type of structure: \n",
    "\n",
    "# Getting geometry structures attributes\n",
    "# Geometry structures attributes:\n",
    "# Bridge/Culvert Count: 0\n",
    "# Connection Count: 4\n",
    "# Has Bridge Opening (2D): 0\n",
    "# Inline Structure Count: 0\n",
    "# Lateral Structure Count: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Boundary Condition Lines and Plot with 2D Flow Area Perimeter Polygons\n",
    "print(\"\\nExample 7: Extracting Boundary Condition Lines and Plotting with 2D Flow Area Perimeter Polygons\")\n",
    "bc_lines_df = HdfBndry.get_bc_lines(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "if not bc_lines_df.empty:\n",
    "    bc_lines_df.head()\n",
    "else:\n",
    "    print(\"No Boundary Condition Lines found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| bc_line_id |         name         |    mesh_name    |    type    |                                           geometry                                            |\n",
    "|-------------|----------------------|------------------|------------|------------------------------------------------------------------------------------------------|\n",
    "|      0      |     DSNormalDepth    |   BaldEagleCr    |  External  | LINESTRING (2082004.235 364024.82, 2083193.546...)                                          |\n",
    "|      1      |       DS2NormalD     |   BaldEagleCr    |  External  | LINESTRING (2084425.804 365392.892, 2084354.64...)                                          |\n",
    "|      2      |   Upstream Inflow    |   BaldEagleCr    |  External  | LINESTRING (1967473.737 290973.629, 1969582.89...)                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Boundary Condition Lines with Perimeter\n",
    "# Plot if data exists\n",
    "if not bc_lines_df.empty or not mesh_areas.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 2D Flow Area Perimeter Polygons\n",
    "    if not mesh_areas.empty:\n",
    "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
    "        \n",
    "        # Add labels for each polygon\n",
    "        for idx, row in mesh_areas.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            label = row.get('Name', f'Area {idx}')\n",
    "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "    \n",
    "    # Plot boundary condition lines\n",
    "    if not bc_lines_df.empty:\n",
    "        bc_lines_df.plot(ax=ax, color='red', linewidth=2, label='Boundary Condition Lines')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    ax.set_title('2D Flow Area Perimeter Polygons and Boundary Condition Lines')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Breaklines and Plot with 2D Flow Area Perimeter Polygons\n",
    "print(\"\\nExample 8: Extracting Breaklines and Plotting with 2D Flow Area Perimeter Polygons\")\n",
    "breaklines_gdf = HdfBndry.get_breaklines(geom_hdf_path, ras_object=bald_eagle)\n",
    "if not breaklines_gdf.empty:\n",
    "    breaklines_gdf\n",
    "else:\n",
    "    print(\"No Breaklines found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breaklines_gdf:\n",
    "\n",
    "\n",
    "| bl_id | Name      | geometry |\n",
    "|-------|-----------|----------|\n",
    "| 0     | SayersDam | LINESTRING (2002361.246 323707.927, 2002741.35...) |\n",
    "| 1     | Lower     | LINESTRING (2060356.422 351786.819, 2060316.47...) |\n",
    "| 2     | Middle    | LINESTRING (2052757.788 348470.547, 2052785.84...) |\n",
    "| 3     | Upper     | LINESTRING (2045597.199 348412.994, 2045638.91...) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot breaklines and 2D Flow Area Perimeter Polygons if they exist\n",
    "if not breaklines_gdf.empty or not mesh_areas.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot 2D Flow Area Perimeter Polygons\n",
    "    if not mesh_areas.empty:\n",
    "        mesh_areas.plot(ax=ax, edgecolor='black', facecolor='none', alpha=0.7, label='2D Flow Area')\n",
    "        \n",
    "        # Add labels for each polygon\n",
    "        for idx, row in mesh_areas.iterrows():\n",
    "            centroid = row.geometry.centroid\n",
    "            label = row.get('Name', f'Area {idx}')\n",
    "            ax.annotate(label, (centroid.x, centroid.y), ha='center', va='center')\n",
    "    \n",
    "    # Plot breaklines\n",
    "    if not breaklines_gdf.empty:\n",
    "        breaklines_gdf.plot(ax=ax, color='blue', linewidth=2, label='Breaklines')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    ax.set_title('2D Flow Area Perimeter Polygons and Breaklines')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get structures\n",
    "structures_gdf = HdfStruc.get_structures(geom_hdf_path, ras_object=bald_eagle)\n",
    "print(\"Structures:\")\n",
    "if not structures_gdf.empty:\n",
    "    structures_gdf.head()\n",
    "else:\n",
    "    print(\"No structures found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structures_gdf: \n",
    "\n",
    "| Type | Mode | River | Reach | RS | Connection | Groupname | US Type | US River | US Reach | ... | US XS Mann (Count) | US BR Mann (Index) | US BR Mann (Count) | DS XS Mann (Index) | DS XS Mann (Count) | DS BR Mann (Index) | DS BR Mann (Count) | RC (Index) | RC (Count) | Profile_Data |\n",
    "|------|------|-------|-------|-------|------------|-----------|----------|-----------|-----------|-----|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|------------|------------|--------------|\n",
    "| Connection | Weir/Gate/Culverts | | | | Sayers Dam | BaldEagleCr, Sayers Dam | 2D | | | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | [{'Station': 0.0, 'Elevation': 683.0}, {'Stati... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get boundary condition lines\n",
    "bc_lines_gdf = HdfBndry.get_bc_lines(geom_hdf_path)\n",
    "print(\"\\nBoundary Condition Lines:\")\n",
    "\n",
    "bc_lines_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get reference points\n",
    "ref_points_gdf = HdfBndry.get_reference_points(geom_hdf_path)\n",
    "print(\"\\nReference Points:\")\n",
    "ref_points_gdf\n",
    "\n",
    "# There are no reference points in this example project (for demonstration only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Refinement Regions\n",
    "print(\"\\nExample: Extracting Refinement Regions\")\n",
    "\n",
    "# Make sure to pass the bald_eagle object as the ras_object parameter\n",
    "refinement_regions_df = HdfBndry.get_refinement_regions(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "if not refinement_regions_df.empty:\n",
    "    print(\"Refinement Regions DataFrame:\")\n",
    "    display(refinement_regions_df.head())\n",
    "    \n",
    "    # Plot refinement regions\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    refinement_regions_df.plot(ax=ax, column='CellSize', legend=True, \n",
    "                               legend_kwds={'label': 'Cell Size', 'orientation': 'horizontal'},\n",
    "                               cmap='viridis')\n",
    "    ax.set_title('2D Mesh Area Refinement Regions')\n",
    "    ax.set_xlabel('Easting')\n",
    "    ax.set_ylabel('Northing')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No refinement regions found in the geometry file.\")\n",
    "\n",
    "# Example: Analyze Refinement Regions\n",
    "if not refinement_regions_df.empty:\n",
    "    print(\"\\nRefinement Regions Analysis:\")\n",
    "    print(f\"Total number of refinement regions: {len(refinement_regions_df)}\")\n",
    "    print(\"\\nCell Size Statistics:\")\n",
    "    print(refinement_regions_df['CellSize'].describe())\n",
    "    \n",
    "    # Group by Shape Type\n",
    "    shape_type_counts = refinement_regions_df['ShapeType'].value_counts()\n",
    "    print(\"\\nRefinement Region Shape Types:\")\n",
    "    print(shape_type_counts)\n",
    "    \n",
    "    # Plot Shape Type distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shape_type_counts.plot(kind='bar')\n",
    "    plt.title('Distribution of Refinement Region Shape Types')\n",
    "    plt.xlabel('Shape Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Compute Messages as String\n",
    "print(\"Extracting Compute Messages\")\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extract_string_from_hdf(results_hdf_filename: str, hdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract string from HDF object at a given path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_hdf_filename : str\n",
    "        Name of the HDF file\n",
    "    hdf_path : str\n",
    "        Path of the object in the HDF file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Extracted string from the specified HDF object\n",
    "    \"\"\"\n",
    "    with h5py.File(results_hdf_filename, 'r') as hdf_file:\n",
    "        try:\n",
    "            hdf_object = hdf_file[hdf_path]\n",
    "            if isinstance(hdf_object, h5py.Group):\n",
    "                return f\"Group: {hdf_path}\\nContents: {list(hdf_object.keys())}\"\n",
    "            elif isinstance(hdf_object, h5py.Dataset):\n",
    "                data = hdf_object[()]\n",
    "                if isinstance(data, bytes):\n",
    "                    return data.decode('utf-8')\n",
    "                elif isinstance(data, np.ndarray) and data.dtype.kind == 'S':\n",
    "                    return [v.decode('utf-8') for v in data]\n",
    "                else:\n",
    "                    return str(data)\n",
    "            else:\n",
    "                return f\"Unsupported object type: {type(hdf_object)}\"\n",
    "        except KeyError:\n",
    "            return f\"Path not found: {hdf_path}\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    results_summary_string = extract_string_from_hdf(plan_hdf_path, '/Results/Summary/Compute Messages (text)')\n",
    "    print(\"Compute Messages:\")\n",
    "    \n",
    "    # Parse and print the compute messages in a more visually friendly way\n",
    "    messages = results_summary_string[0].split('\\r\\n')\n",
    "    \n",
    "    for message in messages:\n",
    "        if message.strip():  # Skip empty lines\n",
    "            if ':' in message:\n",
    "                key, value = message.split(':', 1)\n",
    "                print(f\"{key.strip():40} : {value.strip()}\")\n",
    "            else:\n",
    "                print(f\"\\n{message.strip()}\")\n",
    "    \n",
    "    # Print computation summary in a table format\n",
    "    print(\"\\nComputation Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Computation Task':<30} {'Time':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Task' in line:\n",
    "            task, time = line.split('\\t')\n",
    "            print(f\"{task:<30} {time:<20}\")\n",
    "    \n",
    "    print(\"\\nComputation Speed:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Task':<30} {'Simulation/Runtime':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in messages:\n",
    "        if 'Computation Speed' in line:\n",
    "            task, speed = line.split('\\t')\n",
    "            print(f\"{task:<30} {speed:<20}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting compute messages: {str(e)}\")\n",
    "    print(\"\\nNote: If 'Results/Summary Output' is not in the file structure, it might indicate that the simulation didn't complete successfully or the results weren't saved properly.\")\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Plan 06 does not have any errors, so this dataframe will be empty (for demonstration purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Compute Messages Example \n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def parse_2d_compute_messages(compute_messages):\n",
    "    \"\"\"\n",
    "    Parse 2D compute messages to extract data lines, clean the data, \n",
    "    and retrieve top 20 cells with the highest error.\n",
    "\n",
    "    Parameters:\n",
    "        compute_messages (list or str): The raw compute messages.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the parsed compute messages string and the main DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle both list and string inputs\n",
    "        if isinstance(compute_messages, list):\n",
    "            compute_messages = '\\n'.join(compute_messages)\n",
    "        elif not isinstance(compute_messages, str):\n",
    "            logging.error(f\"Unexpected type for compute_messages: {type(compute_messages)}\")\n",
    "            return \"\", pd.DataFrame()\n",
    "\n",
    "        # Split the message into lines\n",
    "        lines = compute_messages.split('\\n')\n",
    "        logging.info(\"Successfully split compute messages into lines.\")\n",
    "        \n",
    "        # Initialize lists to store parsed data\n",
    "        data_lines = []\n",
    "        print(\"Data lines:\", data_lines)\n",
    "        header_lines = []\n",
    "        print(\"Header lines:\", header_lines) \n",
    "        footer_lines = []\n",
    "        print(\"Footer lines:\", footer_lines)\n",
    "\n",
    "        \n",
    "        # More flexible timestamp pattern that includes various message types\n",
    "        timestamp_pattern = re.compile(r'^\\d{2}[A-Z]{3}\\d{4}\\s+\\d{2}:\\d{2}:\\d{2}')\n",
    "        logging.debug(\"Compiled timestamp regular expression.\")\n",
    "        \n",
    "        data_started = False\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            if timestamp_pattern.match(stripped_line):\n",
    "                data_started = True\n",
    "                # Split the line and add to data_lines\n",
    "                parts = stripped_line.split()\n",
    "                if len(parts) >= 8:  # Ensure we have all expected columns\n",
    "                    # Combine Date and Time into 'Date and Time'\n",
    "                    date_time = f\"{parts[0]} {parts[1]}\"\n",
    "                    location = parts[2]\n",
    "                    cell_type = f\"{parts[3]} {parts[4]}\"\n",
    "                    cell_number = parts[5]\n",
    "                    wsel = parts[6]\n",
    "                    error = parts[7]\n",
    "                    iterations = parts[8] if len(parts) > 8 else None\n",
    "                    data_lines.append([date_time, location, cell_type, cell_number, wsel, error, iterations])\n",
    "                    logging.debug(f\"Parsed data line: {data_lines[-1]}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Line skipped due to insufficient parts: {stripped_line}\")\n",
    "            elif not data_started:\n",
    "                header_lines.append(stripped_line)\n",
    "            elif data_started and not stripped_line:\n",
    "                data_started = False\n",
    "            elif not data_started:\n",
    "                footer_lines.append(stripped_line)\n",
    "        \n",
    "        # Create DataFrame from data lines\n",
    "        df = pd.DataFrame(\n",
    "            data_lines, \n",
    "            columns=['Date and Time', 'Location', 'Cell Type', 'Cell Number', 'WSEL', 'ERROR', 'ITERATIONS']\n",
    "        )\n",
    "        logging.info(\"Created DataFrame from parsed data lines.\")\n",
    "        \n",
    "        # Clean and convert columns to appropriate types\n",
    "        df['Cell Number'] = (\n",
    "            pd.to_numeric(df['Cell Number'].replace('#', pd.NA), errors='coerce')\n",
    "            .fillna(-1)\n",
    "            .astype('Int64')\n",
    "        )\n",
    "        df['WSEL'] = pd.to_numeric(df['WSEL'], errors='coerce')\n",
    "        df['ERROR'] = pd.to_numeric(df['ERROR'], errors='coerce')\n",
    "        df['ITERATIONS'] = pd.to_numeric(df['ITERATIONS'], errors='coerce').astype('Int64')\n",
    "        logging.info(\"Converted DataFrame columns to appropriate types.\")\n",
    "        \n",
    "        # Get top 20 cells with highest error\n",
    "        top_20_cells = (\n",
    "            df.sort_values('ERROR', ascending=False)\n",
    "            .drop_duplicates('Cell Number')\n",
    "            .head(20)\n",
    "        )\n",
    "        \n",
    "        # Construct the reordered message\n",
    "        reordered_message = '\\n'.join(header_lines + \n",
    "                                      ['\\nTop 20 Cells with Highest Error:'] + \n",
    "                                      [' '.join(map(str, row)) for row in top_20_cells.values] + \n",
    "                                      ['\\n'] + footer_lines)\n",
    "        \n",
    "        logging.info(\"Reordered compute messages.\")\n",
    "        \n",
    "        return reordered_message, df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing compute messages: {e}\")\n",
    "        return \"\", pd.DataFrame()\n",
    "\n",
    "# Use the function to parse compute messages\n",
    "parsed_messages, df = parse_2d_compute_messages(results_summary_string)\n",
    "\n",
    "print(parsed_messages)\n",
    "print(df)\n",
    "\n",
    "# Get top 20 cells with highest error\n",
    "if not df.empty and 'ERROR' in df.columns:\n",
    "    top_20_cells = (\n",
    "        df.sort_values('ERROR', ascending=False)\n",
    "        .drop_duplicates('Cell Number')\n",
    "        .head(20)\n",
    "    )\n",
    "else:\n",
    "    logging.warning(\"Unable to get top 20 cells with highest error. DataFrame is empty or 'ERROR' column is missing.\")\n",
    "    top_20_cells = pd.DataFrame()\n",
    "\n",
    "# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
    "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "print(\"\\n2D Flow Area Groups and Perimeters:\")\n",
    "if not mesh_areas.empty:\n",
    "    print(\"Available columns:\", mesh_areas.columns.tolist())\n",
    "    \n",
    "    # Display the first few rows of the mesh_areas DataFrame\n",
    "    print(\"\\nFirst few rows of mesh_areas DataFrame:\")\n",
    "    mesh_areas.head()\n",
    "else:\n",
    "    print(\"No 2D Flow Area groups found in the HDF file.\")\n",
    "\n",
    "# Use the previously extracted cell_polygons_df\n",
    "print(\"\\nTop 20 Cell Polygons:\")\n",
    "if 'cell_polygons_df' in locals() and not cell_polygons_df.empty and not top_20_cells.empty:\n",
    "    # Get the cell numbers from top_20_cells\n",
    "    top_20_cell_numbers = top_20_cells['Cell Number'].tolist()\n",
    "    \n",
    "    # Filter cell_polygons_df to only include top 20 cells\n",
    "    top_20_cell_polygons = cell_polygons_df[cell_polygons_df['cell_id'].isin(top_20_cell_numbers)]\n",
    "    \n",
    "    display(top_20_cell_polygons)\n",
    "\n",
    "    # Plot top 20 cell polygons and mesh areas\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot mesh areas\n",
    "    mesh_areas.plot(ax=ax, edgecolor='red', facecolor='none', alpha=0.5, label='Mesh Areas')\n",
    "    \n",
    "    # Plot top 20 cell polygons\n",
    "    top_20_cell_polygons.plot(ax=ax, edgecolor='blue', facecolor='none', alpha=0.7, label='Top 20 Error Cells')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('2D Flow Area Perimeters and Top 20 Cell Polygons')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Cell Polygons found or no top 20 cells with highest error available.\")\n",
    "    print(\"Unable to plot cell polygons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Example for Debugging or New Features: List all paths, groups, and attributes under \"/Results/Unsteady/Summary/Volume Accounting\"\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Results/Unsteady/Summary/Volume Accounting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 12: Extract Plan Parameters and Volume Accounting\n",
    "print(\"\\nExample 12: Extracting Plan Parameters and Volume Accounting Data\")\n",
    "\n",
    "# Extract plan parameters\n",
    "plan_parameters_df = HdfPlan.get_plan_parameters(plan_hdf_path)\n",
    "\n",
    "# Extract volume accounting data\n",
    "volume_accounting_df = HdfResultsPlan.get_volume_accounting(plan_hdf_path)\n",
    "\n",
    "print(\"\\nPlan Parameters DataFrame:\")\n",
    "plan_parameters_df\n",
    "\n",
    "print(\"\\nVolume Accounting DataFrame:\")\n",
    "volume_accounting_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RasPlanHdf Class Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get plan start time\n",
    "start_time = HdfPlan.get_plan_start_time(plan_hdf_path)\n",
    "print(f\"Simulation start time: {start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation start time: 2018-09-09 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get plan end time\n",
    "end_time = HdfPlan.get_plan_end_time(plan_hdf_path)\n",
    "print(f\"Simulation end time: {end_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation end time: 2018-09-14 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get maximum iteration count for mesh cells\n",
    "max_iter_df = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)\n",
    "print(\"\\nMesh Max Iterations:\")\n",
    "print(max_iter_df.attrs)\n",
    "max_iter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_iter_df:\n",
    "\n",
    "| mesh_name | cell_id | cell_last_iteration | geometry |\n",
    "|-----------|---------|--------------------| ---------|\n",
    "| BaldEagleCr | 0 | 0 | POINT (2083000 370750) |\n",
    "| BaldEagleCr | 1 | 0 | POINT (2083250 370750) |\n",
    "| BaldEagleCr | 2 | 0 | POINT (2083500 370750) |\n",
    "| BaldEagleCr | 3 | 2 | POINT (2083750 370750) |\n",
    "| BaldEagleCr | 4 | 0 | POINT (2084000 370750) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum iteration count for mesh cells\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "\n",
    "max_iter_gdf = HdfResultsMesh.get_mesh_max_iter(plan_hdf_path)\n",
    "\n",
    "print(\"max_iter_df\")\n",
    "print(max_iter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mesh_max_iter_df:\n",
    "\n",
    "| mesh_name | cell_id | cell_last_iteration | geometry |\n",
    "|-----------|---------|--------------------| ---------|\n",
    "| BaldEagleCr | 0 | 0 | POINT (2083000 370750) |\n",
    "| ... | ... | ... | ... |\n",
    "| BaldEagleCr | 19592 | 0 | POINT (1978423.032 300718.897) |\n",
    "\n",
    "\n",
    "[19597 rows x 4 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cell coordinates \n",
    "cell_coords = HdfMesh.get_mesh_cell_points(plan_hdf_path)\n",
    "cell_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mesh Max Iterations\n",
    "\n",
    "# Extract x and y coordinates from the geometry column\n",
    "max_iter_df['x'] = max_iter_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
    "max_iter_df['y'] = max_iter_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
    "\n",
    "# Remove rows with None coordinates\n",
    "max_iter_df = max_iter_df.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_iter_df['x'], max_iter_df['y'], \n",
    "                     c=max_iter_df['cell_last_iteration'], \n",
    "                     cmap='viridis', \n",
    "                     s=1)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Iterations per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Iterations')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "max_iter_df[['mesh_name', 'cell_id', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh maximum water surface elevation\n",
    "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Maximum Water Surface Elevation:\")\n",
    "print(max_ws_df.attrs)\n",
    "max_ws_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_ws_df:\n",
    "\n",
    "| mesh_name | cell_id | maximum_water_surface | maximum_water_surface_time | geometry |\n",
    "|-----------|---------|---------------------|--------------------------|-----------|\n",
    "| BaldEagleCr | 0 | 704.054443 | 2018-09-10 18:00:00 | POINT (2083000 370750) |\n",
    "| BaldEagleCr | 1 | 692.377991 | 2018-09-10 18:04:00 | POINT (2083250 370750) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the max water surface as a map\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "\n",
    "# Extract x and y coordinates from the geometry column\n",
    "max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
    "max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
    "\n",
    "# Remove rows with None coordinates\n",
    "max_ws_df = max_ws_df.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], \n",
    "                     c=max_ws_df['maximum_water_surface'], \n",
    "                     cmap='viridis', \n",
    "                     s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Water Surface per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Water Surface (ft)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "max_ws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time of the max water surface elevation (WSEL)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert the 'maximum_water_surface_time' to datetime objects\n",
    "max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Convert datetime to hours since the start for colormap\n",
    "min_time = max_ws_df['max_wsel_time'].min()\n",
    "color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600  # Convert to hours\n",
    "\n",
    "scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], \n",
    "                     c=color_values, \n",
    "                     cmap='viridis', \n",
    "                     s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Time of Maximum Water Surface Elevation per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "# Set up the colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Hours since simulation start')\n",
    "\n",
    "# Format the colorbar ticks to show hours\n",
    "cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))  # Set ticks every 6 hours\n",
    "cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Find the overall maximum WSEL and its time\n",
    "max_wsel_row = max_ws_df.loc[max_ws_df['maximum_water_surface'].idxmax()]\n",
    "hours_since_start = (max_wsel_row['max_wsel_time'] - min_time).total_seconds() / 3600\n",
    "print(f\"\\nOverall Maximum WSEL: {max_wsel_row['maximum_water_surface']:.2f} ft\")\n",
    "print(f\"Time of Overall Maximum WSEL: {max_wsel_row['max_wsel_time']}\")\n",
    "print(f\"Hours since simulation start: {hours_since_start:.2f} hours\")\n",
    "print(f\"Location of Overall Maximum WSEL: X={max_wsel_row['x']}, Y={max_wsel_row['y']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh minimum water surface elevation\n",
    "min_ws_df = HdfResultsMesh.get_mesh_min_ws(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Minimum Water Surface Elevation:\")\n",
    "min_ws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh maximum face velocity\n",
    "max_face_v_df = HdfResultsMesh.get_mesh_max_face_v(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Max Face Velocity:\")\n",
    "max_face_v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract midpoint coordinates from the LineString geometries\n",
    "max_face_v_df['x'] = max_face_v_df['geometry'].apply(lambda geom: geom.centroid.x)\n",
    "max_face_v_df['y'] = max_face_v_df['geometry'].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_face_v_df['x'], max_face_v_df['y'], \n",
    "                    c=max_face_v_df['maximum_face_velocity'].abs(),\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Face Velocity per Face')\n",
    "ax.set_xlabel('X Coordinate') \n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Face Velocity (ft/s)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the face velocity dataframe:\")\n",
    "max_face_v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh minimum face velocity\n",
    "min_face_v_df = HdfResultsMesh.get_mesh_min_face_v(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Min Face Velocity:\")\n",
    "min_face_v_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh max water surface error\n",
    "\n",
    "max_ws_err_df = HdfResultsMesh.get_mesh_max_ws_err(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Max Water Surface Error:\")\n",
    "max_ws_err_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max water surface error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract x and y coordinates from the geometry points, handling None values\n",
    "max_ws_err_df['x'] = max_ws_err_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)\n",
    "max_ws_err_df['y'] = max_ws_err_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)\n",
    "\n",
    "# Remove any rows with None coordinates\n",
    "max_ws_err_df = max_ws_err_df.dropna(subset=['x', 'y'])\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_ws_err_df['x'], max_ws_err_df['y'],\n",
    "                    c=max_ws_err_df['cell_maximum_water_surface_error'],\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Water Surface Error per Cell')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Water Surface Error (ft)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the water surface error dataframe:\")\n",
    "max_ws_err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to add this to the ras-commander library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n",
    "max_courant_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\", ras_object=bald_eagle)\n",
    "print(\"\\nMesh Summary Output (Maximum Courant):\")\n",
    "print(max_courant_df.attrs)\n",
    "max_courant_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max Courant number\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get mesh max Courant number\n",
    "max_courant_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Courant\", ras_object=bald_eagle)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "max_courant_gdf = gpd.GeoDataFrame(max_courant_df)\n",
    "\n",
    "# Get centroids of line geometries for plotting\n",
    "max_courant_gdf['centroid'] = max_courant_gdf.geometry.centroid\n",
    "max_courant_gdf['x'] = max_courant_gdf.centroid.x\n",
    "max_courant_gdf['y'] = max_courant_gdf.centroid.y\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_courant_gdf['x'], max_courant_gdf['y'],\n",
    "                    c=max_courant_gdf['maximum_face_courant'],\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Courant Number per Face')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Courant Number')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the Courant number dataframe:\")\n",
    "max_courant_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh summary output for other Datasets (here we retrieve Maximum Face Courant)\n",
    "\n",
    "max_face_shear_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\", ras_object=bald_eagle)\n",
    "print(\"\\nMesh Summary Output (Maximum Face Shear Stress:\")\n",
    "print(max_face_shear_df.attrs)\n",
    "max_face_shear_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max face shear stress\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import Point, LineString\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get mesh max face shear stress\n",
    "max_shear_df = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Maximum Face Shear Stress\", ras_object=bald_eagle)\n",
    "\n",
    "# Calculate centroids of the line geometries and extract coordinates\n",
    "max_shear_df['centroid'] = max_shear_df['geometry'].apply(lambda line: line.centroid)\n",
    "max_shear_df['x'] = max_shear_df['centroid'].apply(lambda point: point.x)\n",
    "max_shear_df['y'] = max_shear_df['centroid'].apply(lambda point: point.y)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(max_shear_df['x'], max_shear_df['y'],\n",
    "                    c=max_shear_df['maximum_face_shear_stress'],\n",
    "                    cmap='viridis',\n",
    "                    s=10)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Max Face Shear Stress per Face')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "plt.colorbar(scatter, label='Max Face Shear Stress (PSF)')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Increase font size for better readability\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the first few rows of the dataframe for verification\n",
    "print(\"\\nFirst few rows of the shear stress dataframe:\")\n",
    "max_shear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get mesh summary output for Minimum Water Surface\n",
    "summary_df_min_ws = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Water Surface\", ras_object=bald_eagle)\n",
    "print(\"\\nMesh Summary Output (Minimum Water Surface):\")\n",
    "summary_df_min_ws\n",
    "\n",
    "# Example: Get mesh summary output for Minimum Face Velocity\n",
    "summary_df_min_fv = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Minimum Face Velocity\", ras_object=bald_eagle)\n",
    "print(\"\\nMesh Summary Output (Minimum Face Velocity):\")\n",
    "summary_df_min_fv\n",
    "\n",
    "# Example: Get mesh summary output for Cell Cumulative Iteration\n",
    "summary_df_cum_iter = HdfResultsMesh.get_mesh_summary(plan_hdf_path, var=\"Cell Cumulative Iteration\", ras_object=bald_eagle)\n",
    "print(\"\\nMesh Summary Output (Cell Cumulative Iteration):\")\n",
    "summary_df_cum_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh timeseries output\n",
    "\n",
    "# Get mesh areas from previous code cell\n",
    "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "if mesh_areas:\n",
    "    mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n",
    "    timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\", ras_object=bald_eagle)\n",
    "    print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n",
    "    print(timeseries_da)\n",
    "else:\n",
    "    print(\"No mesh areas found in the geometry file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Output Variables for Cells\n",
    "# \n",
    "# Variable Name: Description\n",
    "# Water Surface: Water surface elevation\n",
    "# Depth: Water depth\n",
    "# Velocity: Magnitude of velocity\n",
    "# Velocity X: X-component of velocity\n",
    "# Velocity Y: Y-component of velocity\n",
    "# Froude Number: Froude number\n",
    "# Courant Number: Courant number\n",
    "# Shear Stress: Shear stress on the bed\n",
    "# Bed Elevation: Elevation of the bed\n",
    "# Precipitation Rate: Rate of precipitation\n",
    "# Infiltration Rate: Rate of infiltration\n",
    "# Evaporation Rate: Rate of evaporation\n",
    "# Percolation Rate: Rate of percolation\n",
    "# Groundwater Elevation: Elevation of groundwater\n",
    "# Groundwater Depth: Depth to groundwater\n",
    "# Groundwater Flow: Groundwater flow rate\n",
    "# Groundwater Velocity: Magnitude of groundwater velocity\n",
    "# Groundwater Velocity X: X-component of groundwater velocity\n",
    "# Groundwater Velocity Y: Y-component of groundwater velocity\n",
    "# \n",
    "# These variables are available for time series output at the cell level in 2D flow areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh cells timeseries output\n",
    "cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Cells Timeseries Output:\")\n",
    "print(cells_timeseries_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WSE Time Series Data (Random Cell ID)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Extract Water Surface data\n",
    "water_surface = cells_timeseries_ds['BaldEagleCr']['Water Surface']\n",
    "\n",
    "# Get the time values\n",
    "time_values = water_surface.coords['time'].values\n",
    "\n",
    "# Pick a random cell_id\n",
    "random_cell_id = random.choice(water_surface.coords['cell_id'].values)\n",
    "\n",
    "# Extract the water surface elevation time series for the random cell\n",
    "wsel_timeseries = water_surface.sel(cell_id=random_cell_id)\n",
    "\n",
    "# Find the peak value and its index\n",
    "peak_value = wsel_timeseries.max().item()\n",
    "peak_index = wsel_timeseries.argmax().item()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_values, wsel_timeseries, label=f'Cell ID: {random_cell_id}')\n",
    "plt.scatter(time_values[peak_index], peak_value, color='red', s=100, zorder=5)\n",
    "plt.annotate(f'Peak: {peak_value:.2f} ft', \n",
    "             (time_values[peak_index], peak_value),\n",
    "             xytext=(10, 10), textcoords='offset points',\n",
    "             ha='left', va='bottom',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.title(f'Water Surface Elevation Time Series for Random Cell (ID: {random_cell_id})')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Water Surface Elevation (ft)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Log the plotting action\n",
    "logging.info(f\"Plotted water surface elevation time series for random cell ID: {random_cell_id}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Statistics for Cell ID {random_cell_id}:\")\n",
    "print(f\"Minimum WSEL: {wsel_timeseries.min().item():.2f} ft\")\n",
    "print(f\"Maximum WSEL: {peak_value:.2f} ft\")\n",
    "print(f\"Mean WSEL: {wsel_timeseries.mean().item():.2f} ft\")\n",
    "print(f\"Time of peak: {time_values[peak_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh faces timeseries output\n",
    "faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Faces Timeseries Output:\")\n",
    "print(faces_timeseries_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Face Results and Label Peak, Plus Map View\n",
    "\n",
    "# Step 1: Import necessary libraries \n",
    "# In notebook cell at top of notebook\n",
    "\n",
    "# Step 2: Select a random valid face ID number\n",
    "random_face = np.random.randint(0, faces_timeseries_ds.sizes['face_id'])\n",
    "\n",
    "# Step 3: Extract time series data for the selected face\n",
    "variable = 'face_velocity'  # We could also use 'face_flow'\n",
    "face_data = faces_timeseries_ds[variable].sel(face_id=random_face)\n",
    "\n",
    "# Step 4: Find peak value and its corresponding time\n",
    "peak_value = face_data.max().item()\n",
    "peak_time = face_data.idxmax().values\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(faces_timeseries_ds.time, face_data)\n",
    "plt.title(f'{variable.capitalize()} Time Series for Face {random_face}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(f'{variable.capitalize()} ({faces_timeseries_ds.attrs[\"units\"]})')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate the peak point\n",
    "plt.annotate(f'Peak: ({peak_time}, {peak_value:.2f})', \n",
    "            (peak_time, peak_value),\n",
    "            xytext=(10, 10), textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "# Check for negative values and label the minimum if present\n",
    "min_value = face_data.min().item()\n",
    "if min_value < 0:\n",
    "    min_time = face_data.idxmin().values\n",
    "    plt.annotate(f'Min: ({min_time}, {min_value:.2f})', \n",
    "                (min_time, min_value),\n",
    "                xytext=(10, -10), textcoords='offset points',\n",
    "                arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create map view plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Get mesh faces for map view\n",
    "mesh_faces = HdfMesh.get_mesh_cell_faces(plan_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "# Calculate mesh faces extents with 10% buffer\n",
    "faces_bounds = mesh_faces.total_bounds\n",
    "x_min, y_min, x_max, y_max = faces_bounds\n",
    "buffer_x = (x_max - x_min) * 0.1\n",
    "buffer_y = (y_max - y_min) * 0.1\n",
    "plot_xlim = [x_min - buffer_x, x_max + buffer_x]\n",
    "plot_ylim = [y_min - buffer_y, y_max + buffer_y]\n",
    "\n",
    "# Set plot limits before adding terrain\n",
    "ax.set_xlim(plot_xlim)\n",
    "ax.set_ylim(plot_ylim)\n",
    "\n",
    "# Add the terrain TIFF to the map, clipped to our desired extent\n",
    "tiff_path = Path.cwd() / 'example_projects' / 'BaldEagleCrkMulti2D' / 'Terrain' / 'Terrain50.baldeagledem.tif'\n",
    "with rasterio.open(tiff_path) as src:\n",
    "    show(src, ax=ax, cmap='terrain', alpha=0.5)\n",
    "    \n",
    "# Reset the limits after terrain plot\n",
    "ax.set_xlim(plot_xlim)\n",
    "ax.set_ylim(plot_ylim)\n",
    "\n",
    "# Plot all faces in gray\n",
    "mesh_faces.plot(ax=ax, color='lightgray', alpha=0.5, zorder=2)\n",
    "\n",
    "# Get the selected face geometry\n",
    "selected_face = mesh_faces[mesh_faces['face_id'] == random_face]\n",
    "\n",
    "# Highlight the selected face in red\n",
    "selected_face.plot(\n",
    "    ax=ax, \n",
    "    color='red',\n",
    "    linewidth=2,\n",
    "    label=f'Selected Face (ID: {random_face})',\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Get bounds of selected face for zoomed inset\n",
    "bounds = selected_face.geometry.bounds.iloc[0]\n",
    "x_center = (bounds.iloc[0] + bounds.iloc[2]) / 2\n",
    "y_center = (bounds.iloc[1] + bounds.iloc[3]) / 2\n",
    "buffer = max(bounds.iloc[2] - bounds.iloc[0], bounds.iloc[3] - bounds.iloc[1]) * 2\n",
    "\n",
    "# Create zoomed inset with a larger size, inside the map frame\n",
    "axins = inset_axes(ax, width=\"70%\", height=\"70%\", loc='lower right',\n",
    "                  bbox_to_anchor=(0.65, 0.05, 0.35, 0.35),\n",
    "                  bbox_transform=ax.transAxes)\n",
    "\n",
    "# Plot terrain and faces in inset\n",
    "with rasterio.open(tiff_path) as src:\n",
    "    show(src, ax=axins, cmap='terrain', alpha=0.5)\n",
    "    \n",
    "# Plot zoomed view in inset\n",
    "mesh_faces.plot(ax=axins, color='lightgray', alpha=0.5, zorder=2)\n",
    "selected_face.plot(ax=axins, color='red', linewidth=2, zorder=3)\n",
    "\n",
    "# Set inset limits with slightly more context\n",
    "axins.set_xlim(x_center - buffer/1.5, x_center + buffer/1.5)\n",
    "axins.set_ylim(y_center - buffer/1.5, y_center + buffer/1.5)\n",
    "\n",
    "# Remove inset ticks for cleaner look\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "\n",
    "# Add a border to the inset\n",
    "for spine in axins.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "# Create connection lines between main plot and inset\n",
    "# Get the selected face centroid for connection point\n",
    "centroid = selected_face.geometry.centroid.iloc[0]\n",
    "con1 = ConnectionPatch(\n",
    "    xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
    "    xyB=(0.02, 0.98), coordsB=axins.transAxes,\n",
    "    arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
    ")\n",
    "con2 = ConnectionPatch(\n",
    "    xyA=(centroid.x, centroid.y), coordsA=ax.transData,\n",
    "    xyB=(0.98, 0.02), coordsB=axins.transAxes,\n",
    "    arrowstyle=\"-\", linestyle=\"--\", color=\"gray\", alpha=0.6\n",
    ")\n",
    "\n",
    "ax.add_artist(con1)\n",
    "ax.add_artist(con2)\n",
    "\n",
    "# Add title and legend to main plot\n",
    "ax.set_title('Mesh Face Map View with Terrain')\n",
    "ax.legend()\n",
    "\n",
    "# Ensure equal aspect ratio while maintaining our desired extents\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary information\n",
    "print(f\"Random Face: {random_face}\")\n",
    "print(f\"Peak Value: {peak_value:.2f} {faces_timeseries_ds.attrs['units']} at {peak_time}\")\n",
    "if min_value < 0:\n",
    "    print(f\"Minimum Value: {min_value:.2f} {faces_timeseries_ds.attrs['units']} at {min_time}\")\n",
    "\n",
    "# Log the plotting action\n",
    "logging.info(f\"Plotted mesh face time series and map view for random face ID: {random_face} with terrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meteorology precipitation attributes\n",
    "meteo_precip_attrs = HdfPlan.get_plan_met_precip(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nMeteorology Precipitation Attributes:\")\n",
    "for key, value in meteo_precip_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results unsteady attributes\n",
    "results_unsteady_attrs = HdfResultsPlan.get_unsteady_info(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nResults Unsteady Attributes:\")\n",
    "for key, value in results_unsteady_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results unsteady summary attributes\n",
    "results_unsteady_summary_attrs = HdfResultsPlan.get_unsteady_summary(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nResults Unsteady Summary Attributes:\")\n",
    "for key, value in results_unsteady_summary_attrs.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Get results volume accounting attributes\n",
    "volume_accounting_attrs = HdfResultsPlan.get_volume_accounting(plan_hdf_path, ras_object=bald_eagle)\n",
    "print(\"\\nVolume Accounting Attributes:\")\n",
    "for key, value in volume_accounting_attrs.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\12_2d_hdf_data_extraction pipes and pumps.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS Pipes, Conduits, and Pump Stations HDF Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to manipulate and analyze the new HEC-RAS Conduits, Pipes, and Pump Stations results using the ras-commander library. It leverages the HdfPipe and HdfPump classes to streamline data extraction, processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Pipes Beta project from HEC and run plan 01\n",
    "\n",
    "# Define the path to the Pipes Beta project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "pipes_ex_path = current_dir / \"example_projects\" / \"Davis\"\n",
    "import logging\n",
    "\n",
    "# Check if Pipes Beta.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = pipes_ex_path / \"DavisStormSystem.p02.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the Pipes Beta project\n",
    "    RasExamples.extract_project([\"Davis\"])\n",
    "\n",
    "    # Initialize custom Ras object\n",
    "    pipes_ex = RasPrj()\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    pipes_ex = init_ras_project(pipes_ex_path, \"6.6\")\n",
    "    logging.info(f\"Pipes Beta project initialized with folder: {pipes_ex.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Pipes Beta object id: {id(pipes_ex)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"02\"\n",
    "\n",
    "    # Update run flags for the project\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,\n",
    "        unsteady_flow_simulation=True,\n",
    "        run_sediment=False,\n",
    "        post_processor=True,\n",
    "        floodplain_mapping=False,\n",
    "        ras_object=pipes_ex\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Pipes Beta\n",
    "    print(f\"Executing Plan {plan_number} for the Pipes Beta Creek project...\")\n",
    "    success_pipes_ex = RasCmdr.compute_plan(plan_number, ras_object=pipes_ex)\n",
    "    if success_pipes_ex:\n",
    "        print(f\"Plan {plan_number} executed successfully for Pipes Beta.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Pipes Beta.\\n\")\n",
    "else:\n",
    "    print(\"Pipes Beta.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    pipes_ex = RasPrj()\n",
    "    pipes_ex = init_ras_project(pipes_ex_path, \"6.6\")\n",
    "    plan_number = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for pipes_ex project\n",
    "print(\"Plan DataFrame for pipes_ex project:\")\n",
    "pipes_ex.plan_df\n",
    "\n",
    "# Display geom_df for pipes_ex project\n",
    "print(\"\\nGeometry DataFrame for pipes_ex project:\")\n",
    "pipes_ex.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_hdf_path = pipes_ex.plan_df.loc[pipes_ex.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = pipes_ex.plan_df.loc[pipes_ex.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = pipes_ex.geom_df.loc[pipes_ex.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=pipes_ex)\n",
    "runtime_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pipe Conduits/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pipe conduits\n",
    "pipe_conduits_gdf = HdfPipe.get_pipe_conduits(plan_hdf_path)\n",
    "print(\"\\nPipe Conduits: pipe_conduits_gdf\")\n",
    "pipe_conduits_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_conduits_gdf:  \n",
    "\n",
    "| Name | System Name | US Node | DS Node | Modeling Approach | Conduit Length | Max Cell Length | Shape | Rise | Span | ... | Slope | US Entrance Loss Coefficient | DS Exit Loss Coefficient | US Backflow Loss Coefficient | DS Backflow Loss Coefficient | DS Flap Gate | Major Group | Minor Group | Polyline | Terrain_Profiles |\n",
    "|------|-------------|---------|---------|-------------------|----------------|------------------|-------|------|------|-----|-------|------------------------------|--------------------------|------------------------------|------------------------------|--------------|-------------|-------------|----------|-------------------|\n",
    "| 0    | 134         | Davis   | O13-DMH007 | O13-DMH006 | hydraulic        | 443.740020      | 40.0             | circular | 6.0  | 6.0  | ... | 0.002723 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 40.819695), (21.217846, 40.642994), (35... |\n",
    "| 1    | 133         | Davis   | O13-DMH024 | O13-DMH009 | hydraulic        | 800.000024      | 40.0             | circular | 6.0  | 6.0  | ... | 0.001904 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 40.530186), (21.1467, 40.44057), (50.88... |\n",
    "| 2    | 132         | Davis   | O13-DMH006 | O13-SDS03 | hydraulic        | 443.740070      | 40.0             | circular | 6.0  | 6.0  | ... | 0.002816 | 0.2                        | 0.4                      | 0.2                          | 0.4                          | 0            | Major Group 2 |             | LINESTRING (6635295.441 1965214.2465, 6635196.... | [(0.0, 41.700996), (26.817467, 41.552666), (83... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install adjusttext #No longer required - optional to help with labels overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pipe conduit linestrings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new figure with a specified size\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# Plot each linestring from the GeoDataFrame\n",
    "for idx, row in pipe_conduits_gdf.iterrows():\n",
    "    # Extract coordinates from the linestring\n",
    "    x_coords, y_coords = row['Polyline'].xy\n",
    "    \n",
    "    # Plot the linestring\n",
    "    plt.plot(x_coords, y_coords, 'b-', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Add vertical line markers at endpoints\n",
    "    plt.plot([x_coords[0]], [y_coords[0]], 'x', color='black', markersize=4)\n",
    "    plt.plot([x_coords[-1]], [y_coords[-1]], 'x', color='black', markersize=4)\n",
    "    \n",
    "    # Calculate center point of the line\n",
    "    center_x = (x_coords[0] + x_coords[-1]) / 2\n",
    "    center_y = (y_coords[0] + y_coords[-1]) / 2\n",
    "    \n",
    "    # Add pipe name label at center, oriented top-right\n",
    "    plt.text(center_x, center_y, f'{row[\"Name\"]}', fontsize=8, \n",
    "             verticalalignment='bottom', horizontalalignment='left',\n",
    "             rotation=45)  # 45 degree angle for top-right orientation\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Pipe Conduit Network Layout')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout to prevent label clipping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 terrain profiles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract terrain profiles from the GeoDataFrame\n",
    "terrain_profiles = pipe_conduits_gdf['Terrain_Profiles'].tolist()\n",
    "\n",
    "# Create separate plots for the first 2 terrain profiles\n",
    "for i in range(2):\n",
    "    profile = terrain_profiles[i]\n",
    "    \n",
    "    # Unzip the profile into x and y coordinates\n",
    "    x_coords, y_coords = zip(*profile)\n",
    "    \n",
    "    # Create a new figure for each profile\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_coords, y_coords, marker='o', linestyle='-', color='g', alpha=0.7)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f'Terrain Profile {i + 1}')\n",
    "    plt.xlabel('Distance along profile (m)')\n",
    "    plt.ylabel('Elevation (m)')\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Adjust layout to prevent label clipping\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Nodes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pipe nodes\n",
    "pipe_nodes_gdf = HdfPipe.get_pipe_nodes(plan_hdf_path)\n",
    "print(\"\\nPipe Nodes:\")\n",
    "pipe_nodes_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_nodes_gdf:\n",
    "\n",
    "\n",
    "| Name         | System Name | Node Type | Node Status                     | Condtui Connections (US:DS) | Invert Elevation | Base Area | Terrain Elevation | Terrain Elevation Override | Depth     | Drop Inlet Elevation | Drop Inlet Weir Length | Drop Inlet Weir Coefficient | Drop Inlet Orifice Area | Drop Inlet Orifice Coefficient | Total Connection Count | geometry                             |\n",
    "|--------------|-------------|-----------|----------------------------------|------------------------------|------------------|-----------|-------------------|---------------------------|-----------|----------------------|-------------------------|-----------------------------|-------------------------|-------------------------------|------------------------|-------------------------------------|\n",
    "| O14-di027   | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 36.060001        | 36.0     | 39.860001         | NaN                       | 3.799999  | 39.863369           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6637926.81 1964917.32)     |\n",
    "| P11-DMH004  | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 38.169998        | 36.0     | 48.720001         | NaN                       | 10.550003 | 48.718811           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6629444.634 1963504.411)   |\n",
    "| O14-DMH005  | Davis       | Junction   | Junction with drop inlet        | 1:1                          | 31.559999        | 36.0     | 40.840000         | NaN                       | 9.280001  | 40.843731           | 3.0                     | 3.3                         | 1.0                     | 0.67                          | 2                      | POINT (6637368.497 1966084.574)   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "#HdfUtils.get_hdf5_dataset_info(plan_hdf_path, \"/Geometry/Pipe Networks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pipe network data\n",
    "pipe_network_gdf = HdfPipe.get_pipe_network(plan_hdf_path)\n",
    "print(\"\\nPipe Network Data:\")\n",
    "pipe_network_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_network_gdf:\n",
    "| Cell_ID | Conduit_ID | Node_ID | Minimum_Elevation | DS_Face_Indices | Face_Indices | US_Face_Indices | Cell_Property_Info_Index | US Face Elevation | DS Face Elevation | Min Elevation | Area | Info Index | Cell_Polygon | Face_Polylines | Node_Point |\n",
    "|---------|------------|---------|-------------------|------------------|--------------|------------------|--------------------------|-------------------|-------------------|---------------|------|------------|---------------|----------------|------------|\n",
    "| 0       | 0          | 0       | -1                | 26.824432        | [1]          | [0, 1]           | [0]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |\n",
    "| 1       | 1          | 0       | -1                | 26.714573        | [2]          | [1, 2]           | [1]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |\n",
    "| 2       | 2          | 0       | -1                | 26.604715        | [3]          | [2, 3]           | [2]                      | 0                 | 26.93429          | 26.824432     | 242.040024 | 0          | POLYGON ((6635288.02154 1965233.24073, 6635279... | [LINESTRING (6635288.021542038 1965233.2407260... | None       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pump stations\n",
    "pump_stations_gdf = HdfPump.get_pump_stations(plan_hdf_path)\n",
    "print(\"\\nPump Stations:\")\n",
    "pump_stations_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pump Stations:\n",
    "| geometry                          | station_id | Name             | Inlet River | Inlet Reach | Inlet RS | Inlet RS Distance | Inlet SA/2D | Inlet Pipe Node | Outlet River | ... | Outlet Pipe Node | Reference River | Reference Reach | Reference RS | Reference RS Distance | Reference SA/2D | Reference Point | Reference Pipe Node | Highest Pump Line Elevation | Pump Groups |\n",
    "|-----------------------------------|------------|------------------|-------------|-------------|----------|-------------------|--------------|------------------|--------------|-----|------------------|------------------|-----------------|--------------|-----------------------|------------------|-----------------|----------------------|------------------------------|-------------|\n",
    "| POINT (6635027.027 1966080.07)   | 0          | Pump Station #1   |             |             | NaN      |                   |              | Davis [O13-SDS03] |              | ... |                  | NaN              |                 | NaN          |                       | NaN              |                 |                      | NaN                          | 1           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get pump groups\n",
    "pump_groups_df = HdfPump.get_pump_groups(plan_hdf_path)\n",
    "print(\"\\nPump Groups:\")\n",
    "pump_groups_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pump Groups:\n",
    "| Pump Station ID | Name             | Bias On | Start Up Time | Shut Down Time | Width | Pumps | efficiency_curve_start | efficiency_curve_count | efficiency_curve |\n",
    "|------------------|------------------|---------|----------------|----------------|-------|-------|------------------------|-----------------------|------------------|\n",
    "| 0                | Pump Station #1   | 0       | 5.0            | NaN            | 5.0   | 1     | 0                      | 6                     | [[2.0, 70.0], [4.0, 60.0], [6.0, 55.0], [8.0, ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "print(f\"Projection: {projection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CRS for GeoDataFrames\n",
    "if projection:\n",
    "    pipe_conduits_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
    "    pipe_nodes_gdf.set_crs(projection, inplace=True, allow_override=True)\n",
    "\n",
    "print(\"Pipe Conduits GeoDataFrame columns:\")\n",
    "print(pipe_conduits_gdf.columns)\n",
    "\n",
    "print(\"\\nPipe Nodes GeoDataFrame columns:\")\n",
    "print(pipe_nodes_gdf.columns)\n",
    "\n",
    "perimeter_polygons = HdfMesh.get_mesh_areas(geom_hdf_path, ras_object=pipes_ex)\n",
    "if projection:\n",
    "    perimeter_polygons.set_crs(projection, inplace=True, allow_override=True)\n",
    "    \n",
    "print(\"\\nPerimeter Polygons GeoDataFrame columns:\")\n",
    "print(perimeter_polygons.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from shapely import wkt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(28, 20))\n",
    "\n",
    "# Plot cell polygons with 50% transparency behind the pipe network\n",
    "cell_polygons_df = HdfMesh.get_mesh_cell_polygons(geom_hdf_path, ras_object=pipes_ex)\n",
    "if not cell_polygons_df.empty:\n",
    "    cell_polygons_df.plot(ax=ax, edgecolor='lightgray', facecolor='lightgray', alpha=0.5)\n",
    "\n",
    "# Plot pipe conduits - the Polyline column already contains LineString geometries\n",
    "pipe_conduits_gdf.set_geometry('Polyline', inplace=True)\n",
    "\n",
    "# Plot each pipe conduit individually to ensure all are shown\n",
    "for idx, row in pipe_conduits_gdf.iterrows():\n",
    "    ax.plot(*row.Polyline.xy, color='blue', linewidth=1)\n",
    "\n",
    "# Create a colormap for node elevations\n",
    "norm = plt.Normalize(pipe_nodes_gdf['Invert Elevation'].min(), \n",
    "                    pipe_nodes_gdf['Invert Elevation'].max())\n",
    "cmap = plt.cm.viridis\n",
    "\n",
    "# Plot pipe nodes colored by invert elevation\n",
    "scatter = ax.scatter(pipe_nodes_gdf.geometry.x, pipe_nodes_gdf.geometry.y,\n",
    "                    c=pipe_nodes_gdf['Invert Elevation'], \n",
    "                    cmap=cmap, norm=norm,\n",
    "                    s=100)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Invert Elevation (ft)', rotation=270, labelpad=15)\n",
    "\n",
    "# Add combined labels for invert and drop inlet elevations\n",
    "for idx, row in pipe_nodes_gdf.iterrows():\n",
    "    label_text = \"\"  # Initialize label_text for each node\n",
    "    # Add drop inlet elevation label if it exists and is not NaN\n",
    "    if 'Drop Inlet Elevation' in row and not np.isnan(row['Drop Inlet Elevation']):\n",
    "        label_text += f\"TOC: {row['Drop Inlet Elevation']:.2f}\\n\"\n",
    "    label_text += f\"INV: {row['Invert Elevation']:.2f}\"\n",
    "    \n",
    "    ax.annotate(label_text,\n",
    "                xy=(row.geometry.x, row.geometry.y),\n",
    "                xytext=(-10, -10), textcoords='offset points',\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Add perimeter polygons \n",
    "if not perimeter_polygons.empty:\n",
    "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "\n",
    "# Create proxy artists for legend\n",
    "conduit_line = mlines.Line2D([], [], color='blue', label='Conduits')\n",
    "node_point = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                          markersize=10, label='Nodes')\n",
    "perimeter = mpatches.Patch(facecolor='none', edgecolor='black',\n",
    "                          label='Perimeter Polygons')\n",
    "\n",
    "ax.set_title('Pipe Network with Node Elevations')\n",
    "\n",
    "# Add legend with proxy artists\n",
    "ax.legend(handles=[conduit_line, node_point, perimeter])\n",
    "\n",
    "# Set aspect ratio to be equal and adjust limits\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pump stations on a map\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "pump_stations_gdf.plot(ax=ax, color='green', markersize=50, label='Pump Station')\n",
    "\n",
    "# Add perimeter polygons\n",
    "if not perimeter_polygons.empty:\n",
    "    perimeter_polygons.plot(ax=ax, edgecolor='black', facecolor='none', label='Perimeter Polygons')\n",
    "\n",
    "ax.set_title('Pump Station Location')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Get pipe network timeseries\n",
    "valid_variables = [\n",
    "    \"Cell Courant\", \"Cell Water Surface\", \"Face Flow\", \"Face Velocity\",\n",
    "    \"Face Water Surface\", \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\",\n",
    "    \"Pipes/Vel DS\", \"Pipes/Vel US\", \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\",\n",
    "    \"Nodes/Water Surface\"\n",
    "]\n",
    "\n",
    "print(\"Valid variables for pipe network timeseries:\")\n",
    "for var in valid_variables:\n",
    "    print(f\"- {var}\")\n",
    "\n",
    "# Extract pipe network timeseries for each valid pipe-related variable\n",
    "pipe_variables = [var for var in valid_variables if var.startswith(\"Pipes/\") or var.startswith(\"Nodes/\")]\n",
    "\n",
    "for variable in pipe_variables:\n",
    "    try:\n",
    "        pipe_timeseries = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
    "        print(f\"\\nPipe Network Timeseries ({variable}):\")\n",
    "        print(pipe_timeseries.head())  # Print first few rows to avoid overwhelming output\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {variable}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe Network Timeseries Data Description\n",
    "\n",
    "The `get_pipe_network_timeseries` function returns an xarray DataArray for each variable. Here's a general description of the data structure:\n",
    "\n",
    "1. **Pipes/Pipe Flow DS and Pipes/Pipe Flow US**:\n",
    "   - Dimensions: time, location (pipe IDs)\n",
    "   - Units: ft^3/s (cubic feet per second)\n",
    "   - Description: Represents the flow rate at the downstream (DS) and upstream (US) ends of pipes over time.\n",
    "\n",
    "2. **Pipes/Vel DS and Pipes/Vel US**:\n",
    "   - Dimensions: time, location (pipe IDs)\n",
    "   - Units: ft/s (feet per second)\n",
    "   - Description: Shows the velocity at the downstream (DS) and upstream (US) ends of pipes over time.\n",
    "\n",
    "3. **Nodes/Depth**:\n",
    "   - Dimensions: time, location (node IDs)\n",
    "   - Units: ft (feet)\n",
    "   - Description: Indicates the depth of water at each node over time.\n",
    "\n",
    "4. **Nodes/Drop Inlet Flow**:\n",
    "   - Dimensions: time, location (node IDs)\n",
    "   - Units: cfs (cubic feet per second)\n",
    "   - Description: Represents the flow rate through drop inlets at each node over time.\n",
    "\n",
    "5. **Nodes/Water Surface**:\n",
    "   - Dimensions: time, location (node IDs)\n",
    "   - Units: ft (feet)\n",
    "   - Description: Shows the water surface elevation at each node over time.\n",
    "\n",
    "General notes:\n",
    "- The 'time' dimension represents the simulation timesteps.\n",
    "- The 'location' dimension represents either pipe IDs or node IDs, depending on the variable.\n",
    "- The number of timesteps and locations may vary depending on the specific dataset and simulation setup.\n",
    "- Negative values in flow variables may indicate reverse flow direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the variables we want to plot\n",
    "variables = [\n",
    "    \"Pipes/Pipe Flow DS\", \"Pipes/Pipe Flow US\", \"Pipes/Vel DS\", \"Pipes/Vel US\",\n",
    "    \"Nodes/Depth\", \"Nodes/Drop Inlet Flow\", \"Nodes/Water Surface\"\n",
    "]\n",
    "\n",
    "# Create a separate plot for each variable\n",
    "for variable in variables:\n",
    "    try:\n",
    "        # Get the data for the current variable\n",
    "        data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)\n",
    "        \n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Pick one random location\n",
    "        random_location = random.choice(data.location.values)\n",
    "        \n",
    "        # Determine if it's a pipe or node variable\n",
    "        if variable.startswith(\"Pipes/\"):\n",
    "            location_type = \"Conduit ID\"\n",
    "        else:\n",
    "            location_type = \"Node ID\"\n",
    "        \n",
    "        # Plot the data for the randomly selected location\n",
    "        ax.plot(data.time, data.sel(location=random_location), label=f'{location_type} {random_location}')\n",
    "        \n",
    "        # Set the title and labels\n",
    "        ax.set_title(f'{variable} Over Time ({location_type} {random_location})')\n",
    "        ax.set_xlabel('Time')  # Corrected from ax.xlabel to ax.set_xlabel\n",
    "        ax.set_ylabel(f'{variable} ({data.attrs[\"units\"]})')  # Corrected from ax.ylabel to ax.set_ylabel\n",
    "        \n",
    "        # Format the x-axis to show dates nicely\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add a legend\n",
    "        ax.legend(title=location_type, loc='upper left')\n",
    "        \n",
    "        # Adjust the layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {variable}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Get pump station timeseries\n",
    "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
    "# Use the results_pump_station_timeseries method \n",
    "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
    "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
    "print(pump_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_hdf5_dataset_info function to get Pipe Conduits data:\n",
    "HdfBase.get_dataset_info(plan_hdf_path, \"/Geometry/Pump Stations/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the pump station timeseries data\n",
    "pump_station_name = pump_stations_gdf.iloc[0]['Name']  # Get the first pump station name\n",
    "pump_timeseries = HdfPump.get_pump_station_timeseries(plan_hdf_path, pump_station=pump_station_name)\n",
    "\n",
    "# Print the pump station timeseries\n",
    "print(f\"\\nPump Station Timeseries ({pump_station_name}):\")\n",
    "print(pump_timeseries)\n",
    "\n",
    "# Create a new figure for plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Plot each variable in the timeseries\n",
    "for variable in pump_timeseries.coords['variable'].values:\n",
    "    data = pump_timeseries.sel(variable=variable)\n",
    "    \n",
    "    # Decode units to strings\n",
    "    unit = pump_timeseries.attrs[\"units\"][list(pump_timeseries.coords[\"variable\"].values).index(variable)][1].decode('utf-8')\n",
    "    \n",
    "    # Check if the variable is 'Pumps on' to plot it differently\n",
    "    if variable == 'Pumps on':\n",
    "        # Plot with color based on the on/off status\n",
    "        colors = ['green' if val > 0 else 'red' for val in data.values.flatten()]\n",
    "        ax.scatter(pump_timeseries['time'], data, label=f'{variable} ({unit})', color=colors)\n",
    "    else:\n",
    "        ax.plot(pump_timeseries['time'], data, label=f'{variable} ({unit})')\n",
    "        \n",
    "        # Label the peak values\n",
    "        peak_time = pump_timeseries['time'][data.argmax()]\n",
    "        peak_value = data.max()\n",
    "        ax.annotate(f'Peak: {peak_value:.2f}', xy=(peak_time, peak_value), \n",
    "                    xytext=(peak_time, peak_value + 0.1 * peak_value), \n",
    "                    arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                    fontsize=10, color='black', ha='center')\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title(f'Timeseries Data for Pump Station: {pump_station_name}')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Values')\n",
    "\n",
    "# Format the x-axis to show dates nicely\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(title='Variables', loc='upper left')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\13_2d_detail_face_data_extraction.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEC-RAS 2D Detail Face Data Extraction Examples\n",
    "\n",
    "This notebook demonstrates how to extract detailed 2D face data, display individual cell face results and calculate a discharge weighted velocity using a user-provided profile line located where cell faces are perpendicular to flow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import psutil  # For getting system CPU info\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path  # Ensure pathlib is imported for file operations\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Chippewa_2D project from HEC and run plan 01\n",
    "\n",
    "# Define the path to the Chippewa_2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"Chippewa_2D\"\n",
    "import logging\n",
    "\n",
    "# Check if Chippewa_2D.p02.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"Chippewa_2D.p02.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the Chippewa_2D project\n",
    "    RasExamples.extract_project([\"Chippewa_2D\"])\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {bald_eagle.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(bald_eagle)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"02\"\n",
    "\n",
    "    # Update run flags for the project\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,\n",
    "        unsteady_flow_simulation=True,\n",
    "        run_sediment=False,\n",
    "        post_processor=True,\n",
    "        floodplain_mapping=False,\n",
    "        ras_object=bald_eagle\n",
    "    )\n",
    "\n",
    "    # Execute Plan 02 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"Chippewa_2D.p02.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = RasPrj()\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "output_dir = bald_eagle_path / \"detail_face_data_analysis\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory created/verified at: {output_dir}\")\n",
    "\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "bald_eagle.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "bald_eagle.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDF input path as Plan Number\n",
    "\n",
    "plan_number = \"02\"  # Assuming we're using plan 01 as in the previous code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract runtime and compute time data\n",
    "print(\"\\nExample 2: Extracting runtime and compute time data\")\n",
    "runtime_df = HdfResultsPlan.get_runtime_data(hdf_input=plan_number, ras_object=bald_eagle)\n",
    "if runtime_df is not None:\n",
    "    runtime_df\n",
    "else:\n",
    "    print(\"No runtime data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the RasGeomHdf Class Functions, we will use geom_hdf_path\n",
    "print(geom_hdf_path)\n",
    "\n",
    "# For the example project, plan 02 is associated with geometry 09\n",
    "# If you want to call the geometry by number, call RasHdfGeom functions with a number\n",
    "# Otherwise, if you want to look up geometry hdf path by plan number, follow the logic in the previous code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the  to USA Contiguous Albers Equal Area Conic (USGS version)\n",
    "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
    "projection = 'EPSG:5070'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfPlan for geometry-related operations\n",
    "print(\"\\nExample: Extracting Base Geometry Attributes\")\n",
    "geom_attrs = HdfPlan.get_geometry_information(geom_hdf_path)\n",
    "\n",
    "if not geom_attrs.empty:\n",
    "    # Display the DataFrame directly\n",
    "    print(\"Base Geometry Attributes:\")\n",
    "    geom_attrs\n",
    "else:\n",
    "    print(\"No base geometry attributes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfMesh for geometry-related operations\n",
    "print(\"\\nExample 3: Listing 2D Flow Area Names\")\n",
    "flow_area_names = HdfMesh.get_mesh_area_names(geom_hdf_path, ras_object=bald_eagle)\n",
    "print(\"2D Flow Area Names:\", flow_area_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Attributes (get_geom_2d_flow_area_attrs)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Attributes\")\n",
    "flow_area_attributes = HdfMesh.get_mesh_area_attributes(geom_hdf_path)\n",
    "flow_area_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get 2D Flow Area Perimeter Polygons (mesh_areas)\n",
    "print(\"\\nExample: Extracting 2D Flow Area Perimeter Polygons\")\n",
    "mesh_areas = HdfMesh.get_mesh_areas(geom_hdf_path)  # Corrected function name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract mesh cell faces\n",
    "print(\"\\nExample: Extracting mesh cell faces\")\n",
    "\n",
    "# Get mesh cell faces using the standardize_input decorator for consistent file handling\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces GeoDataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the projection to USA Contiguous Albers Equal Area Conic (USGS version)\n",
    "# Note, we would usually call the projection function in HdfMesh but the projection is not set in this example project\n",
    "projection = 'EPSG:5070'  # NAD83 / Conus Albers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Function: Find the nearest cell face to a given point\n",
    "# This provides enough basic information the face cell logic in the notebook\n",
    "\n",
    "def find_nearest_cell_face(point, cell_faces_df):\n",
    "    \"\"\"\n",
    "    Find the nearest cell face to a given point.\n",
    "\n",
    "    Args:\n",
    "        point (shapely.geometry.Point): The input point.\n",
    "        cell_faces_df (GeoDataFrame): DataFrame containing cell face linestrings.\n",
    "\n",
    "    Returns:\n",
    "        int: The face_id of the nearest cell face.\n",
    "        float: The distance to the nearest cell face.\n",
    "    \"\"\"\n",
    "    # Calculate distances from the input point to all cell faces\n",
    "    distances = cell_faces_df.geometry.distance(point)\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    nearest_index = distances.idxmin()\n",
    "\n",
    "    # Get the face_id and distance of the nearest cell face\n",
    "    nearest_face_id = cell_faces_df.loc[nearest_index, 'face_id']\n",
    "    nearest_distance = distances[nearest_index]\n",
    "\n",
    "    return nearest_face_id, nearest_distance\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample: Finding the nearest cell face to a given point\")\n",
    "\n",
    "# Create a sample point (you can replace this with any point of interest)\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# Create the sample point with the same CRS as mesh_cell_faces\n",
    "sample_point = GeoDataFrame(\n",
    "    {'geometry': [Point(1025677, 7853731)]}, \n",
    "    crs=mesh_cell_faces.crs\n",
    ")\n",
    "\n",
    "if not mesh_cell_faces.empty and not sample_point.empty:\n",
    "    nearest_face_id, distance = find_nearest_cell_face(sample_point.geometry.iloc[0], mesh_cell_faces)\n",
    "    print(f\"Nearest cell face to point {sample_point.geometry.iloc[0].coords[0]}:\")\n",
    "    print(f\"Face ID: {nearest_face_id}\")\n",
    "    print(f\"Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Visualize the result\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all cell faces\n",
    "    mesh_cell_faces.plot(ax=ax, color='blue', linewidth=0.5, alpha=0.5, label='Cell Faces')\n",
    "    \n",
    "    # Plot the sample point\n",
    "    sample_point.plot(ax=ax, color='red', markersize=100, alpha=0.7, label='Sample Point')\n",
    "    \n",
    "    # Plot the nearest cell face\n",
    "    nearest_face = mesh_cell_faces[mesh_cell_faces['face_id'] == nearest_face_id]\n",
    "    nearest_face.plot(ax=ax, color='green', linewidth=2, alpha=0.7, label='Nearest Face')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Nearest Cell Face to Sample Point')\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Unable to perform nearest cell face search due to missing data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract mesh cell faces and plot with profile lines\n",
    "print(\"\\nExample: Extracting mesh cell faces and plotting with profile lines\")\n",
    "\n",
    "# Get mesh cell faces\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces DataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces\n",
    "\n",
    "# Load the GeoJSON file for profile lines\n",
    "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
    "profile_lines_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
    "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
    "\n",
    "# Plot the mesh cell faces and profile lines together\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "mesh_cell_faces.plot(ax=ax, color='blue', alpha=0.5, edgecolor='k', label='Mesh Cell Faces')\n",
    "profile_lines_gdf.plot(ax=ax, color='orange', linewidth=2, label='Profile Lines')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Easting')\n",
    "ax.set_ylabel('Northing')\n",
    "ax.set_title('Mesh Cell Faces and Profile Lines')\n",
    "\n",
    "# Add grid and legend\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracting mesh cell faces near profile lines\n",
    "print(\"\\nExample: Extracting mesh cell faces near profile lines\")\n",
    "\n",
    "# Get mesh cell faces using HdfMesh class\n",
    "mesh_cell_faces = HdfMesh.get_mesh_cell_faces(geom_hdf_path)\n",
    "\n",
    "# Display the first few rows of the mesh cell faces DataFrame\n",
    "print(\"First few rows of mesh cell faces:\")\n",
    "mesh_cell_faces\n",
    "\n",
    "# Load the GeoJSON file for profile lines\n",
    "geojson_path = Path(r'data/profile_lines_chippewa2D.geojson')  # Update with the correct path\n",
    "profile_lines_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Set the Coordinate Reference System (CRS) to EPSG:5070\n",
    "profile_lines_gdf = profile_lines_gdf.set_crs(epsg=5070, allow_override=True)\n",
    "\n",
    "# Initialize a dictionary to store faces near each profile line\n",
    "faces_near_profile_lines = {}\n",
    "\n",
    "# Define distance threshold (10 ft converted to meters)\n",
    "distance_threshold = 10\n",
    "angle_threshold = 60  # degrees\n",
    "\n",
    "# Function to calculate the smallest angle between two lines or line segments.\n",
    "def calculate_angle(line):\n",
    "    if isinstance(line, LineString):\n",
    "        x_diff = line.xy[0][-1] - line.xy[0][0]\n",
    "        y_diff = line.xy[1][-1] - line.xy[1][0]\n",
    "    else:\n",
    "        x_diff = line[1][0] - line[0][0]\n",
    "        y_diff = line[1][1] - line[0][1]\n",
    "    \n",
    "    angle = np.degrees(np.arctan2(y_diff, x_diff))\n",
    "    return angle % 360 if angle >= 0 else (angle + 360) % 360\n",
    "\n",
    "# Function to break line into segments\n",
    "def break_line_into_segments(line, segment_length):\n",
    "    segments = []\n",
    "    segment_angles = []\n",
    "    \n",
    "    distances = np.arange(0, line.length, segment_length)\n",
    "    if distances[-1] != line.length:\n",
    "        distances = np.append(distances, line.length)\n",
    "        \n",
    "    for i in range(len(distances)-1):\n",
    "        point1 = line.interpolate(distances[i])\n",
    "        point2 = line.interpolate(distances[i+1])\n",
    "        segment = LineString([point1, point2])\n",
    "        segments.append(segment)\n",
    "        segment_angles.append(calculate_angle([point1.coords[0], point2.coords[0]]))\n",
    "        \n",
    "    return segments, segment_angles\n",
    "\n",
    "# Function to calculate angle difference accounting for 180 degree equivalence\n",
    "def angle_difference(angle1, angle2):\n",
    "    diff = abs(angle1 - angle2) % 180\n",
    "    return min(diff, 180 - diff)\n",
    "\n",
    "# Function to order faces along profile line\n",
    "def order_faces_along_profile(profile_line, faces_gdf):\n",
    "    profile_start = Point(profile_line.coords[0])\n",
    "    \n",
    "    faces_with_dist = []\n",
    "    for idx, face in faces_gdf.iterrows():\n",
    "        face_start = Point(face.geometry.coords[0])\n",
    "        dist = profile_start.distance(face_start)\n",
    "        faces_with_dist.append((idx, dist))\n",
    "    \n",
    "    faces_with_dist.sort(key=lambda x: x[1])\n",
    "    return [x[0] for x in faces_with_dist]\n",
    "\n",
    "# Function to combine ordered faces into single linestring\n",
    "def combine_faces_to_linestring(ordered_faces_gdf):\n",
    "    coords = []\n",
    "    for _, face in ordered_faces_gdf.iterrows():\n",
    "        if not coords:  # First face - add all coordinates\n",
    "            coords.extend(list(face.geometry.coords))\n",
    "        else:  # Subsequent faces - add only end coordinate\n",
    "            coords.append(face.geometry.coords[-1])\n",
    "    return LineString(coords)\n",
    "\n",
    "# Initialize GeoDataFrame for final profile-to-faceline results\n",
    "profile_to_faceline = gpd.GeoDataFrame(columns=['profile_name', 'geometry'], crs=profile_lines_gdf.crs)\n",
    "\n",
    "# Iterate through each profile line\n",
    "for index, profile_line in profile_lines_gdf.iterrows():\n",
    "    profile_geom = profile_line.geometry\n",
    "    \n",
    "    # Break profile line into segments\n",
    "    segments, segment_angles = break_line_into_segments(profile_geom, distance_threshold)\n",
    "    \n",
    "    # Initialize set to store nearby faces\n",
    "    nearby_faces = set()\n",
    "    \n",
    "    # For each face, check distance to segments and angle difference\n",
    "    for face_idx, face in mesh_cell_faces.iterrows():\n",
    "        face_geom = face.geometry\n",
    "        \n",
    "        if isinstance(face_geom, LineString):\n",
    "            face_angle = calculate_angle(face_geom)\n",
    "            \n",
    "            for segment, segment_angle in zip(segments, segment_angles):\n",
    "                if face_geom.distance(segment) <= distance_threshold:\n",
    "                    if angle_difference(face_angle, segment_angle) <= angle_threshold:\n",
    "                        nearby_faces.add(face_idx)\n",
    "                        break\n",
    "    \n",
    "    # Convert the set of indices back to a GeoDataFrame\n",
    "    nearby_faces_gdf = mesh_cell_faces.loc[list(nearby_faces)]\n",
    "    \n",
    "    # Order faces along profile line\n",
    "    ordered_indices = order_faces_along_profile(profile_geom, nearby_faces_gdf)\n",
    "    ordered_faces_gdf = nearby_faces_gdf.loc[ordered_indices]\n",
    "    \n",
    "    # Combine ordered faces into single linestring\n",
    "    combined_linestring = combine_faces_to_linestring(ordered_faces_gdf)\n",
    "    \n",
    "    # Add to profile_to_faceline GeoDataFrame\n",
    "    new_row = gpd.GeoDataFrame({'profile_name': [profile_line['Name']], \n",
    "                               'geometry': [combined_linestring]}, \n",
    "                              crs=profile_lines_gdf.crs)\n",
    "    profile_to_faceline = pd.concat([profile_to_faceline, new_row], ignore_index=True)\n",
    "    \n",
    "    # Store the ordered faces in the dictionary\n",
    "    faces_near_profile_lines[profile_line['Name']] = ordered_faces_gdf\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot all mesh cell faces in light blue\n",
    "mesh_cell_faces.plot(ax=ax, color='lightblue', alpha=0.3, edgecolor='k', label='All Mesh Faces')\n",
    "\n",
    "# Plot selected faces for each profile line with numbers\n",
    "colors = ['red', 'green', 'blue']\n",
    "for (profile_name, faces), color in zip(faces_near_profile_lines.items(), colors):\n",
    "    if not faces.empty:\n",
    "        faces.plot(ax=ax, color=color, alpha=0.6, label=f'Faces near {profile_name}')\n",
    "        \n",
    "        # Add numbers to faces\n",
    "        for i, (idx, face) in enumerate(faces.iterrows()):\n",
    "            midpoint = face.geometry.interpolate(0.5, normalized=True)\n",
    "            ax.text(midpoint.x, midpoint.y, str(i+1), \n",
    "                   color=color, fontweight='bold', ha='center', va='center')\n",
    "\n",
    "# Plot the combined linestrings\n",
    "profile_to_faceline.plot(ax=ax, color='black', linewidth=2, \n",
    "                        linestyle='--', label='Combined Face Lines')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Easting')\n",
    "ax.set_ylabel('Northing')\n",
    "ax.set_title('Mesh Cell Faces and Profile Lines\\nNumbered in order along profile')\n",
    "\n",
    "# Add grid and legend\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nOriginal ordered faces near profile lines:\")\n",
    "faces_near_profile_lines\n",
    "\n",
    "print(\"\\nCombined profile-to-faceline results:\")\n",
    "profile_to_faceline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get face property tables with error handling\n",
    "face_property_tables = HdfMesh.get_mesh_face_property_tables(geom_hdf_path)\n",
    "face_property_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the face property table for Face ID 4 and display it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "face_id = 4\n",
    "face_properties = face_property_tables['Perimeter 1'][face_property_tables['Perimeter 1']['Face ID'] == face_id]\n",
    "\n",
    "# Create subplots arranged horizontally\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot Z vs Area\n",
    "axs[0].plot(face_properties['Z'], face_properties['Area'], marker='o', color='blue', label='Area')\n",
    "axs[0].set_title(f'Face ID {face_id}: Z vs Area')\n",
    "axs[0].set_xlabel('Z')\n",
    "axs[0].set_ylabel('Area')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Z vs Wetted Perimeter\n",
    "axs[1].plot(face_properties['Z'], face_properties['Wetted Perimeter'], marker='o', color='green', label='Wetted Perimeter')\n",
    "axs[1].set_title(f'Face ID {face_id}: Z vs Wetted Perimeter')\n",
    "axs[1].set_xlabel('Z')\n",
    "axs[1].set_ylabel('Wetted Perimeter')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot Z vs Manning's n\n",
    "axs[2].plot(face_properties['Z'], face_properties[\"Manning's n\"], marker='o', color='red', label=\"Manning's n\")\n",
    "axs[2].set_title(f'Face ID {face_id}: Z vs Manning\\'s n')\n",
    "axs[2].set_xlabel('Z')\n",
    "axs[2].set_ylabel(\"Manning's n\")\n",
    "axs[2].grid(True)\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mesh timeseries output\n",
    "\n",
    "# Get mesh areas from previous code cell\n",
    "mesh_areas = HdfMesh.get_mesh_area_names(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "if mesh_areas:\n",
    "    mesh_name = mesh_areas[0]  # Use the first 2D flow area name\n",
    "    timeseries_da = HdfResultsMesh.get_mesh_timeseries(plan_hdf_path, mesh_name, \"Water Surface\", ras_object=bald_eagle)\n",
    "    print(f\"\\nMesh Timeseries Output (Water Surface) for {mesh_name}:\")\n",
    "    print(timeseries_da)\n",
    "else:\n",
    "    print(\"No mesh areas found in the geometry file.\")\n",
    "\n",
    "# Get mesh cells timeseries output\n",
    "cells_timeseries_ds = HdfResultsMesh.get_mesh_cells_timeseries(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Cells Timeseries Output:\")\n",
    "print(cells_timeseries_ds)\n",
    "\n",
    "# Get mesh faces timeseries output\n",
    "faces_timeseries_ds = HdfResultsMesh.get_mesh_faces_timeseries(plan_hdf_path, mesh_name, ras_object=bald_eagle)\n",
    "print(\"\\nMesh Faces Timeseries Output:\")\n",
    "print(faces_timeseries_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all face velocities and face flow values to positive\n",
    "\n",
    "# Function to process and convert face data to positive values\n",
    "def convert_to_positive_values(faces_timeseries_ds, cells_timeseries_ds):\n",
    "    \"\"\"\n",
    "    Convert face velocities and flows to positive values while maintaining their relationships.\n",
    "    \n",
    "    Args:\n",
    "        faces_timeseries_ds (xarray.Dataset): Dataset containing face timeseries data\n",
    "        cells_timeseries_ds (xarray.Dataset): Dataset containing cell timeseries data\n",
    "        \n",
    "    Returns:\n",
    "        xarray.Dataset: Modified dataset with positive values\n",
    "    \"\"\"\n",
    "    # Get the face velocity and flow variables\n",
    "    face_velocity = faces_timeseries_ds['face_velocity']\n",
    "    face_flow = faces_timeseries_ds['face_flow']\n",
    "    \n",
    "    # Calculate the sign of the velocity to maintain flow direction relationships\n",
    "    velocity_sign = xr.where(face_velocity >= 0, 1, -1)\n",
    "    \n",
    "    # Convert velocities and flows to absolute values while maintaining their relationship\n",
    "    faces_timeseries_ds['face_velocity'] = abs(face_velocity)\n",
    "    faces_timeseries_ds['face_flow'] = abs(face_flow)\n",
    "    \n",
    "    # Store the original sign as a new variable for reference\n",
    "    faces_timeseries_ds['velocity_direction'] = velocity_sign\n",
    "    \n",
    "    print(\"Conversion to positive values complete.\")\n",
    "    print(f\"Number of faces processed: {len(faces_timeseries_ds.face_id)}\")\n",
    "    \n",
    "    return faces_timeseries_ds, cells_timeseries_ds\n",
    "\n",
    "# Convert the values in our datasets\n",
    "faces_timeseries_ds_positive, cells_timeseries_ds_positive = convert_to_positive_values(\n",
    "    faces_timeseries_ds, \n",
    "    cells_timeseries_ds\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Function to process faces for a single profile line\n",
    "def process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds):\n",
    "    face_ids = faces['face_id'].tolist()\n",
    "    \n",
    "    # Extract relevant data for these faces\n",
    "    face_velocities = faces_timeseries_ds['face_velocity'].sel(face_id=face_ids)\n",
    "    face_flows = faces_timeseries_ds['face_flow'].sel(face_id=face_ids)\n",
    "    \n",
    "    # Create a new dataset with calculated results\n",
    "    results_ds = xr.Dataset({\n",
    "        'face_velocity': face_velocities,\n",
    "        'face_flow': face_flows\n",
    "    })\n",
    "    \n",
    "    # Convert to dataframe for easier manipulation\n",
    "    results_df = results_ds.to_dataframe().reset_index()\n",
    "    \n",
    "    # Add profile name and face order\n",
    "    results_df['profile_name'] = profile_name\n",
    "    results_df['face_order'] = results_df.groupby('time')['face_id'].transform(lambda x: pd.factorize(x)[0])\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Vave = Sum Qn / Sum An for each profile line\n",
    "# where Vave = the summation of face flow / flow area for all the faces in the profile line\n",
    "\n",
    "# Then, save the results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all profile lines\n",
    "all_results = []\n",
    "for profile_name, faces in faces_near_profile_lines.items():\n",
    "    profile_results = process_profile_line(profile_name, faces, cells_timeseries_ds, faces_timeseries_ds)\n",
    "    all_results.append(profile_results)\n",
    "\n",
    "# Combine results from all profile lines\n",
    "combined_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined results\n",
    "print(combined_results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_time_series = {}\n",
    "\n",
    "# Iterate through each profile line and extract its corresponding data\n",
    "for profile_name, faces_gdf in faces_near_profile_lines.items():\n",
    "    # Get the list of face_ids for this profile line\n",
    "    face_ids = faces_gdf['face_id'].tolist()\n",
    "    \n",
    "    # Filter the combined_results_df for these face_ids\n",
    "    profile_df = combined_results_df[combined_results_df['face_id'].isin(face_ids)].copy()\n",
    "    \n",
    "    # Add the profile name as a column\n",
    "    profile_df['profile_name'] = profile_name\n",
    "    \n",
    "    # Reset index for cleanliness\n",
    "    profile_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    profile_time_series[profile_name] = profile_df\n",
    "    \n",
    "    # Display a preview\n",
    "    print(f\"\\nTime Series DataFrame for {profile_name}:\")\n",
    "    profile_df\n",
    "\n",
    "# Optionally, display all profile names\n",
    "print(\"\\nProfile Lines Processed:\")\n",
    "print(list(profile_time_series.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n",
    "|------------|---------|---------------|-------------|----------------|------------|\n",
    "| 2019-04-02 | 370     | 1.543974      | 961.118225  | Profile Line 1 | 0          |\n",
    "| 2019-04-02 | 232     | 2.738194      | 5103.555176 | Profile Line 1 | 1          |\n",
    "| 2019-04-02 | 747     | 3.109769      | 4777.513672 | Profile Line 1 | 2          |\n",
    "| 2019-04-02 | 216     | 2.974400      | 5120.266113 | Profile Line 1 | 3          |\n",
    "| 2019-04-02 | 184     | 0.924792      | 700.676697  | Profile Line 1 | 4          |  \n",
    "  \n",
    "\n",
    "\n",
    "| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n",
    "|------------|---------|---------------|-------------|----------------|------------|\n",
    "| 2019-04-02 | 52      | 0.000000      | 0.000000    | Profile Line 2 | 0          |\n",
    "| 2019-04-02 | 92      | 0.000000      | 0.000000    | Profile Line 2 | 1          |\n",
    "| 2019-04-02 | 548     | 1.018038      | 353.129822  | Profile Line 2 | 2          |\n",
    "| 2019-04-02 | 691     | 2.106394      | 2195.409912 | Profile Line 2 | 3          |\n",
    "| 2019-04-02 | 78      | 2.376904      | 3600.228760 | Profile Line 2 | 4          |  \n",
    "  \n",
    "\n",
    "\n",
    "| Time       | face_id | face_velocity | face_flow   | profile_name   | face_order |\n",
    "|------------|---------|---------------|-------------|----------------|------------|\n",
    "| 2019-04-02 | 532     | 0.000000      | 0.000000    | Profile Line 3 | 0          |\n",
    "| 2019-04-02 | 341     | 0.000000      | 0.000000    | Profile Line 3 | 1          |\n",
    "| 2019-04-02 | 349     | 1.962641      | 2601.644287 | Profile Line 3 | 2          |\n",
    "| 2019-04-02 | 455     | 2.367594      | 4148.870605 | Profile Line 3 | 3          |\n",
    "| 2019-04-02 | 469     | 2.515510      | 4458.292480 | Profile Line 3 | 4          |  \n",
    "  \n",
    "  \n",
    " \n",
    "Profile Lines Processed:\n",
    "['Profile Line 1', 'Profile Line 2', 'Profile Line 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_profiles_df = pd.concat(profile_time_series.values(), ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(\"Combined Time Series DataFrame for All Profiles:\")\n",
    "all_profiles_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively explore the 2D Flow Areas structure in the geometry HDF file\n",
    "import h5py\n",
    "\n",
    "def print_hdf_structure(name, obj):\n",
    "    \"\"\"Print information about HDF5 object\"\"\"\n",
    "    print(f\"\\nPath: {name}\")\n",
    "    print(f\"Type: {type(obj).__name__}\")\n",
    "    \n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"Shape: {obj.shape}\")\n",
    "        print(f\"Dtype: {obj.dtype}\")\n",
    "        print(\"Attributes:\")\n",
    "        for key, value in obj.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "def explore_flow_areas(file_path):\n",
    "    \"\"\"\n",
    "    Recursively explore and print 2D Flow Areas structure in HDF5 file\n",
    "    \n",
    "    :param file_path: Path to the HDF5 file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as hdf_file:\n",
    "            if '/Geometry/2D Flow Areas' in hdf_file:\n",
    "                flow_areas_group = hdf_file['/Geometry/2D Flow Areas']\n",
    "                flow_areas_group.visititems(print_hdf_structure)\n",
    "            else:\n",
    "                print(\"2D Flow Areas group not found in geometry file\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring HDF file: {e}\")\n",
    "\n",
    "print(\"\\nExploring 2D Flow Areas structure in geometry file:\")\n",
    "print(\"HDF Base Path: /Geometry/2D Flow Areas \")\n",
    "explore_flow_areas(geom_hdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the necessary variables\n",
    "print(\"Available variables:\")\n",
    "print(\"profile_time_series:\", 'profile_time_series' in locals())\n",
    "print(\"faces_near_profile_lines:\", 'faces_near_profile_lines' in locals())\n",
    "print(\"profile_averages:\", 'profile_averages' in locals())\n",
    "\n",
    "# Look at the structure of profile_time_series\n",
    "if 'profile_time_series' in locals():\n",
    "    for name, df in profile_time_series.items():\n",
    "        print(f\"\\nColumns in {name}:\")\n",
    "        print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discharge_weighted_velocity(profile_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate discharge-weighted average velocity for a profile line\n",
    "    Vw = Sum(|Qi|*Vi)/Sum(|Qi|) where Qi is face flow and Vi is face velocity\n",
    "    \"\"\"\n",
    "    print(\"Calculating discharge-weighted velocity...\")\n",
    "    print(f\"Input DataFrame:\\n{profile_df.head()}\")\n",
    "\n",
    "    # Calculate weighted velocity for each timestep\n",
    "    weighted_velocities = []\n",
    "    for time in profile_df['time'].unique():\n",
    "        time_data = profile_df[profile_df['time'] == time]\n",
    "        abs_flows = np.abs(time_data['face_flow'])\n",
    "        abs_velocities = np.abs(time_data['face_velocity'])\n",
    "        weighted_vel = (abs_flows * abs_velocities).sum() / abs_flows.sum()\n",
    "        weighted_velocities.append({\n",
    "            'time': time,\n",
    "            'weighted_velocity': weighted_vel\n",
    "        })\n",
    "    \n",
    "    weighted_df = pd.DataFrame(weighted_velocities)\n",
    "    print(f\"Calculated weighted velocities:\\n{weighted_df.head()}\")\n",
    "    return weighted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for each profile line\n",
    "for profile_name, profile_df in profile_time_series.items():\n",
    "    print(f\"\\nProcessing profile: {profile_name}\")\n",
    "\n",
    "    # Calculate discharge-weighted velocity\n",
    "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
    "    \n",
    "    print(\"Weighted velocities calculated.\")\n",
    "    weighted_velocities\n",
    "    \n",
    "    # Convert time to datetime if it isn't already\n",
    "    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n",
    "    print(\"Converted time to datetime format.\")\n",
    "\n",
    "    # Get ordered faces for this profile\n",
    "    ordered_faces = faces_near_profile_lines[profile_name]\n",
    "    print(f\"Number of ordered faces: {len(ordered_faces)}\")\n",
    "    \n",
    "    # Save dataframes in the output directory\n",
    "    output_file = output_dir / f\"{profile_name}_discharge_weighted_velocity.csv\"\n",
    "    weighted_velocities.to_csv(output_file, index=False)\n",
    "    print(f\"Saved weighted velocities to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots comparing discharge-weighted velocity and simple average for each profile line\n",
    "for profile_name, profile_df in profile_time_series.items():\n",
    "    \n",
    "    print(f\"\\nGenerating comparison plot for profile: {profile_name}\")\n",
    "    \n",
    "    # Calculate discharge-weighted velocity\n",
    "    weighted_velocities = calculate_discharge_weighted_velocity(profile_df)\n",
    "    weighted_velocities['time'] = pd.to_datetime(weighted_velocities['time'])\n",
    "    \n",
    "    # Calculate simple average velocity for each timestep\n",
    "    simple_averages = profile_df.groupby('time')['face_velocity'].mean().reset_index()\n",
    "    simple_averages['time'] = pd.to_datetime(simple_averages['time'])\n",
    "    \n",
    "    # Create figure for comparison plot\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    \n",
    "    # Plot individual face velocities with thin lines\n",
    "    for face_id in profile_df['face_id'].unique():\n",
    "        face_data = profile_df[profile_df['face_id'] == face_id]\n",
    "        plt.plot(face_data['time'], \n",
    "                face_data['face_velocity'], \n",
    "                alpha=0.8,  # More transparent\n",
    "                linewidth=0.3,  # Thinner line\n",
    "                color='gray',  # Consistent color\n",
    "                label=f'Face ID {face_id}' if face_id == profile_df['face_id'].iloc[0] else \"\")\n",
    "        \n",
    "        # Find and annotate peak value for each face\n",
    "        peak_idx = face_data['face_velocity'].idxmax()\n",
    "        peak_time = face_data.loc[peak_idx, 'time']\n",
    "        peak_vel = face_data.loc[peak_idx, 'face_velocity']\n",
    "        plt.annotate(f'{peak_vel:.2f}',\n",
    "                    xy=(peak_time, peak_vel),\n",
    "                    xytext=(10, 10),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8,\n",
    "                    alpha=0.5)\n",
    "    \n",
    "    # Plot discharge-weighted velocity\n",
    "    plt.plot(weighted_velocities['time'], \n",
    "            weighted_velocities['weighted_velocity'], \n",
    "            color='red', \n",
    "            alpha=1.0, \n",
    "            linewidth=2,\n",
    "            label='Discharge-Weighted Velocity')\n",
    "    \n",
    "    # Find and annotate peak weighted velocity\n",
    "    peak_idx = weighted_velocities['weighted_velocity'].idxmax()\n",
    "    peak_time = weighted_velocities.loc[peak_idx, 'time']\n",
    "    peak_vel = weighted_velocities.loc[peak_idx, 'weighted_velocity']\n",
    "    plt.annotate(f'Peak Weighted: {peak_vel:.2f}',\n",
    "                xy=(peak_time, peak_vel),\n",
    "                xytext=(10, 10),\n",
    "                textcoords='offset points',\n",
    "                color='red',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    # Plot simple average\n",
    "    plt.plot(simple_averages['time'], \n",
    "            simple_averages['face_velocity'], \n",
    "            color='blue', \n",
    "            alpha=0.5, \n",
    "            linewidth=1,\n",
    "            linestyle='--',\n",
    "            label='Simple Average')\n",
    "    \n",
    "    # Find and annotate peak simple average\n",
    "    peak_idx = simple_averages['face_velocity'].idxmax()\n",
    "    peak_time = simple_averages.loc[peak_idx, 'time']\n",
    "    peak_vel = simple_averages.loc[peak_idx, 'face_velocity']\n",
    "    plt.annotate(f'Peak Average: {peak_vel:.2f}',\n",
    "                xy=(peak_time, peak_vel),\n",
    "                xytext=(10, -10),\n",
    "                textcoords='offset points',\n",
    "                color='blue',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    # Configure plot\n",
    "    plt.title(f'Velocity Comparison - {profile_name}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Velocity (ft/s)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend with better placement\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout to accommodate legend and stats\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    \n",
    "    # Save plot to file\n",
    "    plot_file = output_dir / f\"{profile_name}_velocity_comparison.png\"\n",
    "    plt.savefig(plot_file, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(f\"\\nVelocity Comparison for {profile_name}:\")\n",
    "    print(f\"Number of faces: {profile_df['face_id'].nunique()}\")\n",
    "    print(\"\\nDischarge-Weighted Velocity Statistics:\")\n",
    "    print(f\"Mean: {weighted_velocities['weighted_velocity'].mean():.2f} ft/s\")\n",
    "    print(f\"Max: {weighted_velocities['weighted_velocity'].max():.2f} ft/s\")\n",
    "    print(f\"Min: {weighted_velocities['weighted_velocity'].min():.2f} ft/s\")\n",
    "    print(\"\\nSimple Average Velocity Statistics:\")\n",
    "    print(f\"Mean: {simple_averages['face_velocity'].mean():.2f} ft/s\")\n",
    "    print(f\"Max: {simple_averages['face_velocity'].max():.2f} ft/s\")\n",
    "    print(f\"Min: {simple_averages['face_velocity'].min():.2f} ft/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  We are using the face normal velocity that is available in the HDF.  This will only be accurate if you pick cell faces that are perpendicular to flow.  Depending on the application, a more robust calculation may be required. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\examples\14_fluvial_pluvial_delineation.ipynb
==================================================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delineate Fluvial and Pluvial Areas using RAS-Commander\n",
    "\n",
    "We will leverage the HEC RAS Summary Outputs to delineate the Fluvial and Pluvial Areas\n",
    "\n",
    "Maximum Water Surface Elevation (WSEL) for each cell is recorded, along with the timestamps of when the maximum WSEL occurs.\n",
    "\n",
    "By locating adjacent cells with dissimilar timestamps, we can delineate the Fluvial and Pluvial Areas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about datframe types: \n",
    "\n",
    "Information from the HEC-RAS plan files are generally dataframes.  The text file interface is for the 32-bit side of HEC-RAS and all spatial data is most easily accessed in the HDF files.  This includes plan_df, geom_df, hdf_paths_df\n",
    "\n",
    "Geometry elements (Mesh Faces and Nodes) are provided as Geodataframes (cell_polygons_gdf, boundary_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ras-commander from pip (uncomment to install if needed)\n",
    "#!pip install ras-commander\n",
    "# This installs ras-commander and all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules\n",
    "from ras_commander import *  # Import all ras-commander modules\n",
    "\n",
    "# Import the required libraries for this notebook\n",
    "import h5py\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import xarray as xr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the BaldEagleCrkMulti2D project from HEC and run plan 06\n",
    "\n",
    "# Define the path to the BaldEagleCrkMulti2D project\n",
    "current_dir = Path.cwd()  # Adjust if your notebook is in a different directory\n",
    "bald_eagle_path = current_dir / \"example_projects\" / \"BaldEagleCrkMulti2D\"\n",
    "import logging\n",
    "\n",
    "# Check if BaldEagleCrkMulti2D.p06.hdf exists (so we don't have to re-run the simulation when re-running or debugging)\n",
    "hdf_file = bald_eagle_path / \"BaldEagleDamBrk.p06.hdf\"\n",
    "\n",
    "if not hdf_file.exists():\n",
    "    # Initialize RasExamples and extract the BaldEagleCrkMulti2D project\n",
    "    RasExamples.extract_project([\"BaldEagleCrkMulti2D\"])\n",
    "\n",
    "\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    logging.info(f\"Bald Eagle project initialized with folder: {bald_eagle.project_folder}\")\n",
    "    \n",
    "    logging.info(f\"Bald Eagle object id: {id(bald_eagle)}\")\n",
    "    \n",
    "    # Define the plan number to execute\n",
    "    plan_number = \"06\"\n",
    "\n",
    "    # Update the run flags in the plan file\n",
    "    RasPlan.update_run_flags(\n",
    "        plan_number,\n",
    "        geometry_preprocessor=True,  # Run HTab\n",
    "        unsteady_flow_simulation=True,  # Run UNet\n",
    "        post_processor=True,  # Run PostProcess\n",
    "        floodplain_mapping=False,  # Run RASMapper\n",
    "        ras_object=bald_eagle\n",
    "    )\n",
    "\n",
    "    # Execute Plan 06 using RasCmdr for Bald Eagle\n",
    "    print(f\"Executing Plan {plan_number} for the Bald Eagle Creek project...\")\n",
    "    success_bald_eagle = RasCmdr.compute_plan(plan_number, ras_object=bald_eagle)\n",
    "    if success_bald_eagle:\n",
    "        print(f\"Plan {plan_number} executed successfully for Bald Eagle.\\n\")\n",
    "    else:\n",
    "        print(f\"Plan {plan_number} execution failed for Bald Eagle.\\n\")\n",
    "else:\n",
    "    print(\"BaldEagleCrkMulti2D.p06.hdf already exists. Skipping project extraction and plan execution.\")\n",
    "    # Initialize the RAS project using the custom ras object\n",
    "    bald_eagle = RasPrj()\n",
    "    bald_eagle = init_ras_project(bald_eagle_path, \"6.6\")\n",
    "    plan_number = \"06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Plan and Geometry Dataframes and find Plan and Geometry HDF Paths\n",
    "\n",
    "# Display plan_df for bald_eagle project\n",
    "print(\"Plan DataFrame for bald_eagle project:\")\n",
    "bald_eagle.plan_df\n",
    "\n",
    "# Display geom_df for bald_eagle project\n",
    "print(\"\\nGeometry DataFrame for bald_eagle project:\")\n",
    "bald_eagle.geom_df\n",
    "\n",
    "# Get the plan HDF path\n",
    "plan_number = \"06\"  # Assuming we're using plan 01 as in the previous code\n",
    "plan_hdf_path = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'HDF_Results_Path'].values[0]\n",
    "\n",
    "# Get the geometry file number from the plan DataFrame\n",
    "geom_file = bald_eagle.plan_df.loc[bald_eagle.plan_df['plan_number'] == plan_number, 'Geom File'].values[0]\n",
    "geom_number = geom_file[1:]  # Remove the 'g' prefix\n",
    "\n",
    "# Get the geometry HDF path\n",
    "geom_hdf_path = bald_eagle.geom_df.loc[bald_eagle.geom_df['geom_number'] == geom_number, 'hdf_path'].values[0]\n",
    "\n",
    "print(f\"\\nPlan HDF path for Plan {plan_number}: {plan_hdf_path}\")\n",
    "print(f\"Geometry HDF path for Plan {plan_number}: {geom_hdf_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mesh_max_ws, get the cell coordinates and plot the max water surface as a map\n",
    "import matplotlib.pyplot as plt\n",
    "from ras_commander.HdfMesh import HdfMesh\n",
    "from ras_commander.HdfResultsMesh import HdfResultsMesh\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Get mesh max water surface\n",
    "max_ws_df = HdfResultsMesh.get_mesh_max_ws(plan_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "print(\"max_ws_df\")\n",
    "print(max_ws_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot\n",
    "HdfResultsPlot.plot_results_max_wsel(max_ws_df)\n",
    "\n",
    "# Plot the time of maximum water surface elevation\n",
    "HdfResultsPlot.plot_results_max_wsel_time(max_ws_df)\n",
    "\n",
    "# Print the first few rows of the merged dataframe for verification\n",
    "print(\"\\nFirst few rows of the merged dataframe:\")\n",
    "max_ws_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HdfUtils for extracting projection\n",
    "print(\"\\nExtracting Projection from HDF\")\n",
    "projection = HdfBase.get_projection(hdf_path=geom_hdf_path)\n",
    "if projection:\n",
    "    print(f\"Projection: {projection}\")\n",
    "else:\n",
    "    print(\"No projection information found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract Cell Polygons\n",
    "print(\"\\nExample 6: Extracting Cell Polygons\")\n",
    "cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(geom_hdf_path, ras_object=bald_eagle)\n",
    "\n",
    "\n",
    "# Call the function to plot cell polygons\n",
    "#cell_polygons_gdf = HdfFluvialPluvial.plot_cell_polygons(cell_polygons_gdf, projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString, Polygon, MultiLineString\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from rtree import index\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(plan_hdf_path, delta_t=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics about the boundary line lengths\n",
    "boundary_lengths = boundary_gdf.geometry.length\n",
    "\n",
    "print(\"Boundary line length statistics:\")\n",
    "print(f\"Max length: {boundary_lengths.max():.2f}\")\n",
    "print(f\"Min length: {boundary_lengths.min():.2f}\")\n",
    "print(f\"Average length: {boundary_lengths.mean():.2f}\")\n",
    "print(f\"Median length: {boundary_lengths.median():.2f}\")\n",
    "\n",
    "# Print general information about the boundary GeoDataFrame\n",
    "print(\"\\nBoundary GeoDataFrame info:\")\n",
    "print(boundary_gdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
    "boundary_gdf.plot(ax=ax, color='red', linewidth=2)\n",
    "plt.title('Fluvial-Pluvial Boundary')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_threshold = 3000 #in same units as X and Y coordinates\n",
    "\n",
    "# Filter out boundary lines below the length threshold\n",
    "filtered_boundary_gdf = boundary_gdf[boundary_lengths >= length_threshold]\n",
    "highlighted_boundary_gdf = boundary_gdf[boundary_lengths < length_threshold]\n",
    "\n",
    "# Visualize the results with highlighted boundaries below the threshold\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cell_polygons_gdf.plot(ax=ax, edgecolor='gray', facecolor='none', alpha=0.5)\n",
    "filtered_boundary_gdf.plot(ax=ax, color='red', linewidth=2, label='Valid Boundaries')\n",
    "highlighted_boundary_gdf.plot(ax=ax, color='blue', linewidth=2, linestyle='--', label='Highlighted Boundaries Below Threshold')\n",
    "plt.title('Fluvial-Pluvial Boundary with Length Threshold')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fluvial_pluvial_boundary subfolder\n",
    "output_dir = bald_eagle_path / \"fluvial_pluvial_boundary\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Output directory created/verified at: {output_dir}\")\n",
    "\n",
    "# Save to GeoJSON in output directory\n",
    "boundary_gdf.to_file(output_dir / 'fluvial_pluvial_boundary.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdrpip4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

==================================================

File: c:\GH\ras-commander\ras_commander\Decorators.py
==================================================
from functools import wraps
from pathlib import Path
from typing import Union
import logging
import h5py
import inspect


def log_call(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(func.__module__)
        logger.debug(f"Calling {func.__name__}")
        result = func(*args, **kwargs)
        logger.debug(f"Finished {func.__name__}")
        return result
    return wrapper

def standardize_input(file_type: str = 'plan_hdf'):
    """
    Decorator to standardize input for HDF file operations.
    
    This decorator processes various input types and converts them to a Path object
    pointing to the correct HDF file. It handles the following input types:
    - h5py.File objects
    - pathlib.Path objects
    - Strings (file paths or plan/geom numbers)
    - Integers (interpreted as plan/geom numbers)
    
    The decorator also manages RAS object references and logging.
    
    Args:
        file_type (str): Specifies whether to look for 'plan_hdf' or 'geom_hdf' files.
    
    Returns:
        A decorator that wraps the function to standardize its input to a Path object.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(func.__module__)
            
            # Check if the function expects an hdf_path parameter
            sig = inspect.signature(func)
            param_names = list(sig.parameters.keys())
            
            # If first parameter is 'hdf_file', skip path processing
            if param_names and param_names[0] == 'hdf_file':
                return func(*args, **kwargs)
                
            # Handle both static method calls and regular function calls
            if args and isinstance(args[0], type):
                # Static method call, remove the class argument
                args = args[1:]
            
            hdf_input = kwargs.pop('hdf_path', None) or kwargs.pop('hdf_input', None) or (args[0] if args else None)
            
            # Import ras here to ensure we get the most current instance
            from .RasPrj import ras as ras
            ras_object = kwargs.pop('ras_object', None) or (args[1] if len(args) > 1 else None)
            ras_obj = ras_object or ras

            # If no hdf_input provided, return the function unmodified
            if hdf_input is None:
                return func(*args, **kwargs)

            # NEW: If input is already a Path and exists, use it directly regardless of file_type
            if isinstance(hdf_input, Path) and hdf_input.is_file():
                logger.info(f"Using existing HDF file: {hdf_input}")
                new_args = (hdf_input,) + args[1:]
                return func(*new_args, **kwargs)

            hdf_path = None

            # If hdf_input is already an h5py.File object, use its filename
            if isinstance(hdf_input, h5py.File):
                hdf_path = Path(hdf_input.filename)
            # Handle Path objects
            elif isinstance(hdf_input, Path):
                if hdf_input.is_file():
                    hdf_path = hdf_input
            # Handle string inputs
            elif isinstance(hdf_input, str):
                # Check if it's a file path
                if Path(hdf_input).is_file():
                    hdf_path = Path(hdf_input)
                # Check if it's a number (with or without 'p' prefix)
                elif hdf_input.isdigit() or (len(hdf_input) == 3 and hdf_input[0] == 'p' and hdf_input[1:].isdigit()):
                    try:
                        ras_obj.check_initialized()
                    except Exception as e:
                        raise ValueError(f"RAS object is not initialized: {str(e)}")
                        
                    number = hdf_input if hdf_input.isdigit() else hdf_input[1:]
                    
                    if file_type == 'plan_hdf':
                        plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'] == number]
                        if not plan_info.empty:
                            hdf_path = Path(plan_info.iloc[0]['HDF_Results_Path'])
                    elif file_type == 'geom_hdf':
                        geom_info = ras_obj.geom_df[ras_obj.geom_df['geom_number'] == number]
                        if not geom_info.empty:
                            hdf_path = Path(geom_info.iloc[0]['HDF_Path'])
                    else:
                        raise ValueError(f"Invalid file type: {file_type}")
            # Handle integer inputs (assuming they're plan or geom numbers)
            elif isinstance(hdf_input, int):
                try:
                    ras_obj.check_initialized()
                except Exception as e:
                    raise ValueError(f"RAS object is not initialized: {str(e)}")
                    
                number = f"{hdf_input:02d}"
                
                if file_type == 'plan_hdf':
                    plan_info = ras_obj.plan_df[ras_obj.plan_df['plan_number'] == number]
                    if not plan_info.empty:
                        hdf_path = Path(plan_info.iloc[0]['HDF_Results_Path'])
                elif file_type == 'geom_hdf':
                    geom_info = ras_obj.geom_df[ras_obj.geom_df['geom_number'] == number]
                    if not geom_info.empty:
                        hdf_path = Path(geom_info.iloc[0]['HDF_Path'])
                else:
                    raise ValueError(f"Invalid file type: {file_type}")

            if hdf_path is None or not hdf_path.is_file():
                error_msg = f"HDF file not found: {hdf_input}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)

            logger.info(f"Using HDF file: {hdf_path}")
            
            # Pass all original arguments and keywords, replacing hdf_input with standardized hdf_path
            new_args = (hdf_path,) + args[1:]
            return func(*new_args, **kwargs)

        return wrapper
    return decorator
==================================================

File: c:\GH\ras-commander\ras_commander\HdfBase.py
==================================================
"""
HdfBase: Core HDF File Operations for HEC-RAS

This module provides fundamental methods for interacting with HEC-RAS HDF files.
It serves as a foundation for more specialized HDF classes.

Attribution:
    Derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    Copyright (c) 2024 fema-ffrd - MIT License

Features:
    - Time parsing and conversion utilities
    - HDF attribute and dataset access
    - Geometric data extraction
    - 2D flow area information retrieval

Classes:
    HdfBase: Base class containing static methods for HDF operations

Key Methods:
    Time Operations:
        - get_simulation_start_time(): Get simulation start datetime
        - get_unsteady_timestamps(): Get unsteady output timestamps
        - parse_ras_datetime(): Parse RAS datetime strings
    
    Data Access:
        - get_2d_flow_area_names_and_counts(): Get 2D flow area info
        - get_projection(): Get spatial projection
        - get_attrs(): Access HDF attributes
        - get_dataset_info(): Explore HDF structure
        - get_polylines_from_parts(): Extract geometric polylines

Example:
    ```python
    from ras_commander import HdfBase
    
    with h5py.File('model.hdf', 'r') as hdf:
        start_time = HdfBase.get_simulation_start_time(hdf)
        timestamps = HdfBase.get_unsteady_timestamps(hdf)
    ```
"""
import re
from datetime import datetime, timedelta
import h5py
import numpy as np
import pandas as pd
import xarray as xr
from typing import List, Tuple, Union, Optional, Dict, Any
from pathlib import Path
import logging
from shapely.geometry import LineString, MultiLineString

from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfBase:
    """
    Base class for HEC-RAS HDF file operations.

    This class provides static methods for fundamental HDF file operations,
    including time parsing, attribute access, and geometric data extraction.
    All methods are designed to work with h5py.File objects or pathlib.Path
    inputs.

    Note:
        This class is not meant to be instantiated. All methods are static
        and should be called directly from the class.
    """

    @staticmethod
    def get_simulation_start_time(hdf_file: h5py.File) -> datetime:
        """
        Extract the simulation start time from the HDF file.

        Args:
            hdf_file: Open HDF file object containing RAS simulation data.

        Returns:
            datetime: Simulation start time as a datetime object.

        Raises:
            ValueError: If Plan Information is not found or start time cannot be parsed.
        
        Note:
            Expects 'Plan Data/Plan Information' group with 'Simulation Start Time' attribute.
        """
        plan_info = hdf_file.get("Plan Data/Plan Information")
        if plan_info is None:
            raise ValueError("Plan Information not found in HDF file")
        time_str = plan_info.attrs.get('Simulation Start Time')
        return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))

    @staticmethod
    def get_unsteady_timestamps(hdf_file: h5py.File) -> List[datetime]:
        """
        Extract the list of unsteady timestamps from the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.

        Returns:
            List[datetime]: A list of datetime objects representing the unsteady timestamps.
        """
        group_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
        raw_datetimes = hdf_file[group_path][:]
        return [HdfUtils.parse_ras_datetime_ms(x.decode("utf-8")) for x in raw_datetimes]

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_2d_flow_area_names_and_counts(hdf_path: Path) -> List[Tuple[str, int]]:
        """
        Get the names and cell counts of 2D flow areas from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            List[Tuple[str, int]]: A list of tuples containing the name and cell count of each 2D flow area.

        Raises:
            ValueError: If there's an error reading the HDF file or accessing the required data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                flow_area_2d_path = "Geometry/2D Flow Areas"
                if flow_area_2d_path not in hdf_file:
                    return []
                
                attributes = hdf_file[f"{flow_area_2d_path}/Attributes"][()]
                names = [HdfUtils.convert_ras_string(name) for name in attributes["Name"]]
                
                cell_info = hdf_file[f"{flow_area_2d_path}/Cell Info"][()]
                cell_counts = [info[1] for info in cell_info]
                
                return list(zip(names, cell_counts))
        except Exception as e:
            logger.error(f"Error reading 2D flow area names and counts from {hdf_path}: {str(e)}")
            raise ValueError(f"Failed to get 2D flow area names and counts: {str(e)}")


    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_projection(hdf_path: Path) -> Optional[str]:
        """
        Get projection information from HDF file or RASMapper project file.
        Converts WKT projection to EPSG code for GeoDataFrame compatibility.
        
        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            Optional[str]: The projection as EPSG code (e.g. "EPSG:6556"), or None if not found.
        """
        from pyproj import CRS

        project_folder = hdf_path.parent
        wkt = None
        proj_file = None  # Initialize proj_file variable
        
        # Try HDF file
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                proj_wkt = hdf_file.attrs.get("Projection")
                if proj_wkt is not None:
                    if isinstance(proj_wkt, (bytes, np.bytes_)):
                        wkt = proj_wkt.decode("utf-8")
                        logger.info(f"Found projection in HDF file: {hdf_path}")
                        return wkt
        except Exception as e:
            logger.error(f"Error reading projection from HDF file {hdf_path}: {str(e)}")

        # Try RASMapper file if no HDF projection
        if not wkt:
            try:
                rasmap_files = list(project_folder.glob("*.rasmap"))
                if rasmap_files:
                    with open(rasmap_files[0], 'r') as f:
                        content = f.read()
                        
                    proj_match = re.search(r'<RASProjectionFilename Filename="(.*?)"', content)
                    if proj_match:
                        proj_file = project_folder / proj_match.group(1).replace('.\\', '')
                        if proj_file.exists():
                            with open(proj_file, 'r') as f:
                                wkt = f.read().strip()
                                logger.info(f"Found projection in RASMapper file: {proj_file}")
                                return wkt
            except Exception as e:
                logger.error(f"Error reading RASMapper projection file: {str(e)}")
        
        # Customize error message based on whether proj_file was found
        if proj_file:
            error_msg = (
                "No valid projection found. Checked:\n"
                f"1. HDF file projection attribute: {hdf_path}\n"
                f"2. RASMapper projection file {proj_file} found in RASMapper file, but was invalid"
            )
        else:
            error_msg = (
                "No valid projection found. Checked:\n"
                f"1. HDF file projection attribute: {hdf_path}\n was checked and no projection attribute found"
                "2. No RASMapper projection file found"
            )

        error_msg += (
            "\nTo fix this:\n"
            "1. Open RASMapper\n"
            "2. Click Map > Set Projection\n" 
            "3. Select an appropriate projection file or coordinate system\n"
            "4. Save the RASMapper project"
        )
        
        logger.critical(error_msg)
        return None

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_attrs(hdf_file: h5py.File, attr_path: str) -> Dict[str, Any]:
        """
        Get attributes from an HDF file at a specified path.

        Args:
            hdf_file (h5py.File): The opened HDF file.
            attr_path (str): Path to the attributes in the HDF file.

        Returns:
            Dict[str, Any]: Dictionary of attributes.
        """
        try:
            if attr_path not in hdf_file:
                logger.warning(f"Path {attr_path} not found in HDF file")
                return {}
            
            return HdfUtils.convert_hdf5_attrs_to_dict(hdf_file[attr_path].attrs)
        except Exception as e:
            logger.error(f"Error getting attributes from {attr_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_dataset_info(file_path: Path, group_path: str = '/') -> None:
        """
        Recursively explore and print the structure of an HDF5 file.

        Displays detailed information about groups, datasets, and their attributes
        in a hierarchical format.

        Args:
            file_path: Path to the HDF5 file.
            group_path: Starting group path to explore (default: root '/').

        Prints:
            - Group and dataset names with hierarchical indentation
            - Dataset shapes and data types
            - All attributes for groups and datasets
        """
        def recurse(name, obj, indent=0):
            spacer = "    " * indent
            if isinstance(obj, h5py.Group):
                print(f"{spacer}Group: {name}")
                HdfBase.print_attrs(name, obj)
                for key in obj:
                    recurse(f"{name}/{key}", obj[key], indent+1)
            elif isinstance(obj, h5py.Dataset):
                print(f"{spacer}Dataset: {name}")
                print(f"{spacer}    Shape: {obj.shape}")
                print(f"{spacer}    Dtype: {obj.dtype}")
                HdfBase.print_attrs(name, obj)
            else:
                print(f"{spacer}Unknown object: {name}")

        try:
            with h5py.File(file_path, 'r') as hdf_file:
                if group_path in hdf_file:
                    print("")
                    print(f"Exploring group: {group_path}\n")
                    group = hdf_file[group_path]
                    for key in group:
                        print("")
                        recurse(f"{group_path}/{key}", group[key], indent=1)
                else:
                    print(f"Group path '{group_path}' not found in the HDF5 file.")
        except Exception as e:
            print(f"Error exploring HDF5 file: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_polylines_from_parts(hdf_path: Path, path: str, info_name: str = "Polyline Info", 
                              parts_name: str = "Polyline Parts", 
                              points_name: str = "Polyline Points") -> List[LineString]:
        """
        Extract polylines from HDF file parts data.

        Args:
            hdf_path: Path to the HDF file.
            path: Internal HDF path to polyline data.
            info_name: Name of polyline info dataset.
            parts_name: Name of polyline parts dataset.
            points_name: Name of polyline points dataset.

        Returns:
            List of Shapely LineString/MultiLineString geometries.

        Note:
            Expects HDF datasets containing:
            - Polyline information (start points and counts)
            - Parts information for multi-part lines
            - Point coordinates
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                polyline_info_path = f"{path}/{info_name}"
                polyline_parts_path = f"{path}/{parts_name}"
                polyline_points_path = f"{path}/{points_name}"

                polyline_info = hdf_file[polyline_info_path][()]
                polyline_parts = hdf_file[polyline_parts_path][()]
                polyline_points = hdf_file[polyline_points_path][()]

                geoms = []
                for pnt_start, pnt_cnt, part_start, part_cnt in polyline_info:
                    points = polyline_points[pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(LineString(points))
                    else:
                        parts = polyline_parts[part_start : part_start + part_cnt]
                        geoms.append(
                            MultiLineString(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return geoms
        except Exception as e:
            logger.error(f"Error getting polylines: {str(e)}")
            return []

    @staticmethod
    def print_attrs(name: str, obj: Union[h5py.Dataset, h5py.Group]) -> None:
        """
        Print the attributes of an HDF5 object (Dataset or Group).

        Args:
            name (str): Name of the object
            obj (Union[h5py.Dataset, h5py.Group]): HDF5 object whose attributes are to be printed
        """
        if len(obj.attrs) > 0:
            print(f"    Attributes for {name}:")
            for key, value in obj.attrs.items():
                print(f"        {key}: {value}")




==================================================

File: c:\GH\ras-commander\ras_commander\HdfBndry.py
==================================================
"""
Class: HdfBndry

A utility class for extracting and processing boundary-related features from HEC-RAS HDF files,
including boundary conditions, breaklines, refinement regions, and reference features.

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfBndry:
- get_bc_lines()           # Returns boundary condition lines as a GeoDataFrame.
- get_breaklines()         # Returns 2D mesh area breaklines as a GeoDataFrame.
- get_refinement_regions() # Returns refinement regions as a GeoDataFrame.
- get_reference_lines()    # Returns reference lines as a GeoDataFrame.
- get_reference_points()   # Returns reference points as a GeoDataFrame.



"""
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .HdfMesh import HdfMesh
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfBndry:
    """
    A class for handling boundary-related data from HEC-RAS HDF files.

    This class provides methods to extract and process various boundary elements
    such as boundary condition lines, breaklines, refinement regions, and reference
    lines/points from HEC-RAS geometry HDF files.

    Methods in this class return data primarily as GeoDataFrames, making it easy
    to work with spatial data in a geospatial context.

    Note:
        This class relies on the HdfBase and HdfUtils classes for some of its
        functionality. Ensure these classes are available in the same package.
    """
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_bc_lines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area boundary condition lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the boundary condition lines and their attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                bc_lines_path = "Geometry/Boundary Condition Lines"
                if bc_lines_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                # Get geometries
                bc_line_data = hdf_file[bc_lines_path]
                geoms = HdfBase.get_polylines_from_parts(hdf_path, bc_lines_path)
                
                # Get attributes
                attributes = pd.DataFrame(bc_line_data["Attributes"][()])
                
                # Convert string columns
                str_columns = ['Name', 'SA-2D', 'Type']
                for col in str_columns:
                    if col in attributes.columns:
                        attributes[col] = attributes[col].apply(HdfUtils.convert_ras_string)
                
                # Create GeoDataFrame with all attributes
                gdf = gpd.GeoDataFrame(
                    attributes,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_file)
                )
                
                # Add ID column if not present
                if 'bc_line_id' not in gdf.columns:
                    gdf['bc_line_id'] = range(len(gdf))
                    
                return gdf

        except Exception as e:
            logger.error(f"Error reading boundary condition lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_breaklines(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area breaklines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the breaklines.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                breaklines_path = "Geometry/2D Flow Area Break Lines"
                if breaklines_path not in hdf_file:
                    logger.warning(f"Breaklines path '{breaklines_path}' not found in HDF file.")
                    return gpd.GeoDataFrame()
                bl_line_data = hdf_file[breaklines_path]
                bl_line_ids = range(bl_line_data["Attributes"][()].shape[0])
                names = np.vectorize(HdfUtils.convert_ras_string)(
                    bl_line_data["Attributes"][()]["Name"]
                )
                geoms = HdfBase.get_polylines_from_parts(hdf_path, breaklines_path)
                return gpd.GeoDataFrame(
                    {"bl_id": bl_line_ids, "Name": names, "geometry": geoms},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading breaklines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_refinement_regions(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Return 2D mesh area refinement regions.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the refinement regions.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                refinement_regions_path = "/Geometry/2D Flow Area Refinement Regions"
                if refinement_regions_path not in hdf_file:
                    return gpd.GeoDataFrame()
                rr_data = hdf_file[refinement_regions_path]
                rr_ids = range(rr_data["Attributes"][()].shape[0])
                names = np.vectorize(HdfUtils.convert_ras_string)(rr_data["Attributes"][()]["Name"])
                geoms = list()
                for pnt_start, pnt_cnt, part_start, part_cnt in rr_data["Polygon Info"][()]:
                    points = rr_data["Polygon Points"][()][pnt_start : pnt_start + pnt_cnt]
                    if part_cnt == 1:
                        geoms.append(Polygon(points))
                    else:
                        parts = rr_data["Polygon Parts"][()][part_start : part_start + part_cnt]
                        geoms.append(
                            MultiPolygon(
                                list(
                                    points[part_pnt_start : part_pnt_start + part_pnt_cnt]
                                    for part_pnt_start, part_pnt_cnt in parts
                                )
                            )
                        )
                return gpd.GeoDataFrame(
                    {"rr_id": rr_ids, "Name": names, "geometry": geoms},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading refinement regions: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_lines(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference lines geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference lines. If mesh_name is provided,
            returns only lines for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_lines_path = "Geometry/Reference Lines"
                attributes_path = f"{reference_lines_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                attributes = hdf_file[attributes_path][()]
                refline_ids = range(attributes.shape[0])
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA-2D"])
                
                try:
                    types = v_conv_str(attributes["Type"])
                except ValueError:
                    types = np.array([""] * attributes.shape[0])
                
                geoms = HdfBase.get_polylines_from_parts(hdf_path, reference_lines_path)
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refln_id": refline_ids,
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Type": types,
                        "geometry": geoms,
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference lines: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_reference_points(hdf_path: Path, mesh_name: Optional[str] = None) -> gpd.GeoDataFrame:
        """
        Return the reference points geometry and attributes.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        mesh_name : Optional[str], optional
            Name of the mesh to filter by. Default is None.

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the reference points. If mesh_name is provided,
            returns only points for that mesh.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                reference_points_path = "Geometry/Reference Points"
                attributes_path = f"{reference_points_path}/Attributes"
                if attributes_path not in hdf_file:
                    return gpd.GeoDataFrame()
                
                ref_points_group = hdf_file[reference_points_path]
                attributes = ref_points_group["Attributes"][:]
                v_conv_str = np.vectorize(HdfUtils.convert_ras_string)
                names = v_conv_str(attributes["Name"])
                mesh_names = v_conv_str(attributes["SA/2D"])
                cell_id = attributes["Cell Index"]
                points = ref_points_group["Points"][()]
                
                gdf = gpd.GeoDataFrame(
                    {
                        "refpt_id": range(attributes.shape[0]),
                        "Name": names,
                        "mesh_name": mesh_names,
                        "Cell Index": cell_id,
                        "geometry": list(map(Point, points)),
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
                
                # Filter by mesh_name if provided
                if mesh_name is not None:
                    gdf = gdf[gdf['mesh_name'] == mesh_name]
                
                return gdf
                
        except Exception as e:
            logger.error(f"Error reading reference points: {str(e)}")
            return gpd.GeoDataFrame()

    

==================================================

File: c:\GH\ras-commander\ras_commander\HdfFluvialPluvial.py
==================================================
"""
Class: HdfFluvialPluvial

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfFluvialPluvial:
- calculate_fluvial_pluvial_boundary()
- _process_cell_adjacencies()
- _identify_boundary_edges()

"""

from typing import Dict, List, Tuple
import pandas as pd
import geopandas as gpd
from collections import defaultdict
from shapely.geometry import LineString, MultiLineString  # Added MultiLineString import
from tqdm import tqdm
from .HdfMesh import HdfMesh
from .HdfUtils import HdfUtils
from .Decorators import standardize_input
from .HdfResultsMesh import HdfResultsMesh
from .LoggingConfig import get_logger
from pathlib import Path

logger = get_logger(__name__)

class HdfFluvialPluvial:
    """
    A class for analyzing and visualizing fluvial-pluvial boundaries in HEC-RAS 2D model results.

    This class provides methods to process and visualize HEC-RAS 2D model outputs,
    specifically focusing on the delineation of fluvial and pluvial flood areas.
    It includes functionality for calculating fluvial-pluvial boundaries based on
    the timing of maximum water surface elevations.

    Key Concepts:
    - Fluvial flooding: Flooding from rivers/streams
    - Pluvial flooding: Flooding from rainfall/surface water
    - Delta_t: Time threshold (in hours) used to distinguish between fluvial and pluvial cells.
               Cells with max WSE time differences greater than delta_t are considered boundaries.

    Data Requirements:
    - HEC-RAS plan HDF file containing:
        - 2D mesh cell geometry (accessed via HdfMesh)
        - Maximum water surface elevation times (accessed via HdfResultsMesh)

    Usage Example:
        >>> ras = init_ras_project(project_path, ras_version)
        >>> hdf_path = Path("path/to/plan.hdf")
        >>> boundary_gdf = HdfFluvialPluvial.calculate_fluvial_pluvial_boundary(
        ...     hdf_path, 
        ...     delta_t=12
        ... )
    """
    def __init__(self):
        self.logger = get_logger(__name__)  # Initialize logger with module name
    
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def calculate_fluvial_pluvial_boundary(hdf_path: Path, delta_t: float = 12) -> gpd.GeoDataFrame:
        """
        Calculate the fluvial-pluvial boundary based on cell polygons and maximum water surface elevation times.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file
            delta_t (float): Threshold time difference in hours. Cells with time differences
                        greater than this value are considered boundaries. Default is 12 hours.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing the fluvial-pluvial boundaries with:
                - geometry: LineString features representing boundaries
                - CRS: Coordinate reference system matching the input HDF file

        Raises:
            ValueError: If no cell polygons or maximum water surface data found in HDF file
            Exception: If there are errors during boundary calculation

        Note:
            The returned boundaries represent locations where the timing of maximum water surface
            elevation changes significantly (> delta_t), indicating potential transitions between
            fluvial and pluvial flooding mechanisms.
        """
        try:
            # Get cell polygons from HdfMesh
            logger.info("Getting cell polygons from HDF file...")
            cell_polygons_gdf = HdfMesh.get_mesh_cell_polygons(hdf_path)
            if cell_polygons_gdf.empty:
                raise ValueError("No cell polygons found in HDF file")

            # Get max water surface data from HdfResultsMesh
            logger.info("Getting maximum water surface data from HDF file...")
            max_ws_df = HdfResultsMesh.get_mesh_max_ws(hdf_path)
            if max_ws_df.empty:
                raise ValueError("No maximum water surface data found in HDF file")

            # Convert timestamps using the renamed utility function
            logger.info("Converting maximum water surface timestamps...")
            if 'maximum_water_surface_time' in max_ws_df.columns:
                max_ws_df['maximum_water_surface_time'] = max_ws_df['maximum_water_surface_time'].apply(
                    lambda x: HdfUtils.parse_ras_datetime(x) if isinstance(x, str) else x
                )

            # Process cell adjacencies
            logger.info("Processing cell adjacencies...")
            cell_adjacency, common_edges = HdfFluvialPluvial._process_cell_adjacencies(cell_polygons_gdf)
            
            # Get cell times from max_ws_df
            logger.info("Extracting cell times from maximum water surface data...")
            cell_times = max_ws_df.set_index('cell_id')['maximum_water_surface_time'].to_dict()
            
            # Identify boundary edges
            logger.info("Identifying boundary edges...")
            boundary_edges = HdfFluvialPluvial._identify_boundary_edges(
                cell_adjacency, common_edges, cell_times, delta_t
            )

            # FOCUS YOUR REVISIONS HERE: 
            # Join adjacent LineStrings into simple LineStrings by connecting them at shared endpoints
            logger.info("Joining adjacent LineStrings into simple LineStrings...")
            
            def get_coords(geom):
                """Helper function to extract coordinates from geometry objects
                
                Args:
                    geom: A Shapely LineString or MultiLineString geometry
                
                Returns:
                    tuple: Tuple containing:
                        - list of original coordinates [(x1,y1), (x2,y2),...]
                        - list of rounded coordinates for comparison
                        - None if invalid geometry
                """
                if isinstance(geom, LineString):
                    orig_coords = list(geom.coords)
                    # Round coordinates to 0.01 for comparison
                    rounded_coords = [(round(x, 2), round(y, 2)) for x, y in orig_coords]
                    return orig_coords, rounded_coords
                elif isinstance(geom, MultiLineString):
                    orig_coords = list(geom.geoms[0].coords)
                    rounded_coords = [(round(x, 2), round(y, 2)) for x, y in orig_coords]
                    return orig_coords, rounded_coords
                return None, None

            def find_connecting_line(current_end, unused_lines, endpoint_counts, rounded_endpoints):
                """Find a line that connects to the current endpoint
                
                Args:
                    current_end: Tuple of (x, y) coordinates
                    unused_lines: Set of unused line indices
                    endpoint_counts: Dict of endpoint occurrence counts
                    rounded_endpoints: Dict of rounded endpoint coordinates
                
                Returns:
                    tuple: (line_index, should_reverse, found) or (None, None, False)
                """
                rounded_end = (round(current_end[0], 2), round(current_end[1], 2))
                
                # Skip if current endpoint is connected to more than 2 lines
                if endpoint_counts.get(rounded_end, 0) > 2:
                    return None, None, False
                
                for i in unused_lines:
                    start, end = rounded_endpoints[i]
                    if start == rounded_end and endpoint_counts.get(start, 0) <= 2:
                        return i, False, True
                    elif end == rounded_end and endpoint_counts.get(end, 0) <= 2:
                        return i, True, True
                return None, None, False

            # Initialize data structures
            joined_lines = []
            unused_lines = set(range(len(boundary_edges)))
            
            # Create endpoint lookup dictionaries
            line_endpoints = {}
            rounded_endpoints = {}
            for i, edge in enumerate(boundary_edges):
                coords_result = get_coords(edge)
                if coords_result:
                    orig_coords, rounded_coords = coords_result
                    line_endpoints[i] = (orig_coords[0], orig_coords[-1])
                    rounded_endpoints[i] = (rounded_coords[0], rounded_coords[-1])

            # Count endpoint occurrences
            endpoint_counts = {}
            for start, end in rounded_endpoints.values():
                endpoint_counts[start] = endpoint_counts.get(start, 0) + 1
                endpoint_counts[end] = endpoint_counts.get(end, 0) + 1

            # Iteratively join lines
            while unused_lines:
                # Start a new line chain
                current_points = []
                
                # Find first unused line
                start_idx = unused_lines.pop()
                start_coords, _ = get_coords(boundary_edges[start_idx])
                if start_coords:
                    current_points.extend(start_coords)
                
                # Try to extend in both directions
                continue_joining = True
                while continue_joining:
                    continue_joining = False
                    
                    # Try to extend forward
                    next_idx, should_reverse, found = find_connecting_line(
                        current_points[-1], 
                        unused_lines,
                        endpoint_counts,
                        rounded_endpoints
                    )
                    
                    if found:
                        unused_lines.remove(next_idx)
                        next_coords, _ = get_coords(boundary_edges[next_idx])
                        if next_coords:
                            if should_reverse:
                                current_points.extend(reversed(next_coords[:-1]))
                            else:
                                current_points.extend(next_coords[1:])
                        continue_joining = True
                        continue
                    
                    # Try to extend backward
                    prev_idx, should_reverse, found = find_connecting_line(
                        current_points[0], 
                        unused_lines,
                        endpoint_counts,
                        rounded_endpoints
                    )
                    
                    if found:
                        unused_lines.remove(prev_idx)
                        prev_coords, _ = get_coords(boundary_edges[prev_idx])
                        if prev_coords:
                            if should_reverse:
                                current_points[0:0] = reversed(prev_coords[:-1])
                            else:
                                current_points[0:0] = prev_coords[:-1]
                        continue_joining = True
                
                # Create final LineString from collected points
                if current_points:
                    joined_lines.append(LineString(current_points))

            # FILL GAPS BETWEEN JOINED LINES
            logger.info(f"Starting gap analysis for {len(joined_lines)} line segments...")
            
            def find_endpoints(lines):
                """Get all endpoints of the lines with their indices"""
                endpoints = []
                for i, line in enumerate(lines):
                    coords = list(line.coords)
                    endpoints.append((coords[0], i, 'start'))
                    endpoints.append((coords[-1], i, 'end'))
                return endpoints
            
            def find_nearby_points(point1, point2, tolerance=0.01):
                """Check if two points are within tolerance distance"""
                return (abs(point1[0] - point2[0]) <= tolerance and 
                       abs(point1[1] - point2[1]) <= tolerance)
            
            def find_gaps(lines, tolerance=0.01):
                """Find gaps between line endpoints"""
                logger.info("Analyzing line endpoints to identify gaps...")
                endpoints = []
                for i, line in enumerate(lines):
                    coords = list(line.coords)
                    start = coords[0]
                    end = coords[-1]
                    endpoints.append({
                        'point': start,
                        'line_idx': i,
                        'position': 'start',
                        'coords': coords
                    })
                    endpoints.append({
                        'point': end,
                        'line_idx': i,
                        'position': 'end',
                        'coords': coords
                    })
                
                logger.info(f"Found {len(endpoints)} endpoints to analyze")
                gaps = []
                
                # Compare each endpoint with all others
                for i, ep1 in enumerate(endpoints):
                    for ep2 in endpoints[i+1:]:
                        # Skip if endpoints are from same line
                        if ep1['line_idx'] == ep2['line_idx']:
                            continue
                            
                        point1 = ep1['point']
                        point2 = ep2['point']
                        
                        # Skip if points are too close (already connected)
                        if find_nearby_points(point1, point2):
                            continue
                            
                        # Check if this could be a gap
                        dist = LineString([point1, point2]).length
                        if dist < 10.0:  # Maximum gap distance threshold
                            gaps.append({
                                'start': ep1,
                                'end': ep2,
                                'distance': dist
                            })
                
                logger.info(f"Identified {len(gaps)} potential gaps to fill")
                return sorted(gaps, key=lambda x: x['distance'])

            def join_lines_with_gap(line1_coords, line2_coords, gap_start_pos, gap_end_pos):
                """Join two lines maintaining correct point order based on gap positions"""
                if gap_start_pos == 'end' and gap_end_pos == 'start':
                    # line1 end connects to line2 start
                    return line1_coords + line2_coords
                elif gap_start_pos == 'start' and gap_end_pos == 'end':
                    # line1 start connects to line2 end
                    return list(reversed(line2_coords)) + line1_coords
                elif gap_start_pos == 'end' and gap_end_pos == 'end':
                    # line1 end connects to line2 end
                    return line1_coords + list(reversed(line2_coords))
                else:  # start to start
                    # line1 start connects to line2 start
                    return list(reversed(line1_coords)) + line2_coords

            # Process gaps and join lines
            processed_lines = joined_lines.copy()
            line_groups = [[i] for i in range(len(processed_lines))]
            gaps = find_gaps(processed_lines)
            
            filled_gap_count = 0
            for gap_idx, gap in enumerate(gaps, 1):
                logger.info(f"Processing gap {gap_idx}/{len(gaps)} (distance: {gap['distance']:.3f})")
                
                line1_idx = gap['start']['line_idx']
                line2_idx = gap['end']['line_idx']
                
                # Find the groups containing these lines
                group1 = next(g for g in line_groups if line1_idx in g)
                group2 = next(g for g in line_groups if line2_idx in g)
                
                # Skip if lines are already in the same group
                if group1 == group2:
                    continue
                
                # Get the coordinates for both lines
                line1_coords = gap['start']['coords']
                line2_coords = gap['end']['coords']
                
                # Join the lines in correct order
                joined_coords = join_lines_with_gap(
                    line1_coords,
                    line2_coords,
                    gap['start']['position'],
                    gap['end']['position']
                )
                
                # Create new joined line
                new_line = LineString(joined_coords)
                
                # Update processed_lines and line_groups
                new_idx = len(processed_lines)
                processed_lines.append(new_line)
                
                # Merge groups and remove old ones
                new_group = group1 + group2
                line_groups.remove(group1)
                line_groups.remove(group2)
                line_groups.append(new_group + [new_idx])
                
                filled_gap_count += 1
                logger.info(f"Successfully joined lines {line1_idx} and {line2_idx}")
            
            logger.info(f"Gap filling complete. Filled {filled_gap_count} out of {len(gaps)} gaps")
            
            # Get final lines (take the last line from each group)
            final_lines = [processed_lines[group[-1]] for group in line_groups]
            
            logger.info(f"Final cleanup complete. Resulting in {len(final_lines)} line segments")
            joined_lines = final_lines

            # Create final GeoDataFrame with CRS from cell_polygons_gdf
            logger.info("Creating final GeoDataFrame for boundaries...")
            boundary_gdf = gpd.GeoDataFrame(
                geometry=joined_lines, 
                crs=cell_polygons_gdf.crs
            )

            # Clean up intermediate dataframes
            logger.info("Cleaning up intermediate dataframes...")
            del cell_polygons_gdf
            del max_ws_df

            logger.info("Fluvial-pluvial boundary calculation completed successfully.")
            return boundary_gdf

        except Exception as e:
            self.logger.error(f"Error calculating fluvial-pluvial boundary: {str(e)}")
            return None
        
        
    @staticmethod
    def _process_cell_adjacencies(cell_polygons_gdf: gpd.GeoDataFrame) -> Tuple[Dict[int, List[int]], Dict[int, Dict[int, LineString]]]:
        """
        Optimized method to process cell adjacencies by extracting shared edges directly.
        
        Args:
            cell_polygons_gdf (gpd.GeoDataFrame): GeoDataFrame containing 2D mesh cell polygons
                                                   with 'cell_id' and 'geometry' columns.

        Returns:
            Tuple containing:
                - Dict[int, List[int]]: Dictionary mapping cell IDs to lists of adjacent cell IDs.
                - Dict[int, Dict[int, LineString]]: Nested dictionary storing common edges between cells,
                                                    where common_edges[cell1][cell2] gives the shared boundary.
        """
        cell_adjacency = defaultdict(list)
        common_edges = defaultdict(dict)

        # Build an edge to cells mapping
        edge_to_cells = defaultdict(set)

        # Function to generate edge keys
        def edge_key(coords1, coords2, precision=8):
            # Round coordinates
            coords1 = tuple(round(coord, precision) for coord in coords1)
            coords2 = tuple(round(coord, precision) for coord in coords2)
            # Create sorted key to handle edge direction
            return tuple(sorted([coords1, coords2]))

        # For each polygon, extract edges
        for idx, row in cell_polygons_gdf.iterrows():
            cell_id = row['cell_id']
            geom = row['geometry']
            if geom.is_empty or not geom.is_valid:
                continue
            # Get exterior coordinates
            coords = list(geom.exterior.coords)
            num_coords = len(coords)
            for i in range(num_coords - 1):
                coord1 = coords[i]
                coord2 = coords[i + 1]
                key = edge_key(coord1, coord2)
                edge_to_cells[key].add(cell_id)

        # Now, process edge_to_cells to build adjacency
        for edge, cells in edge_to_cells.items():
            cells = list(cells)
            if len(cells) >= 2:
                # For all pairs of cells sharing this edge
                for i in range(len(cells)):
                    for j in range(i + 1, len(cells)):
                        cell1 = cells[i]
                        cell2 = cells[j]
                        # Update adjacency
                        if cell2 not in cell_adjacency[cell1]:
                            cell_adjacency[cell1].append(cell2)
                        if cell1 not in cell_adjacency[cell2]:
                            cell_adjacency[cell2].append(cell1)
                        # Store common edge
                        common_edge = LineString([edge[0], edge[1]])
                        common_edges[cell1][cell2] = common_edge
                        common_edges[cell2][cell1] = common_edge

        logger.info("Cell adjacencies processed successfully.")
        return cell_adjacency, common_edges

    @staticmethod
    def _identify_boundary_edges(cell_adjacency: Dict[int, List[int]], 
                               common_edges: Dict[int, Dict[int, LineString]], 
                               cell_times: Dict[int, pd.Timestamp], 
                               delta_t: float) -> List[LineString]:
        """
        Identify boundary edges between cells with significant time differences.

        Args:
            cell_adjacency (Dict[int, List[int]]): Dictionary of cell adjacencies
            common_edges (Dict[int, Dict[int, LineString]]): Dictionary of shared edges between cells
            cell_times (Dict[int, pd.Timestamp]): Dictionary mapping cell IDs to their max WSE times
            delta_t (float): Time threshold in hours

        Returns:
            List[LineString]: List of LineString geometries representing boundaries
        """
        # Validate cell_times data
        valid_times = {k: v for k, v in cell_times.items() if pd.notna(v)}
        if len(valid_times) < len(cell_times):
            logger.warning(f"Found {len(cell_times) - len(valid_times)} cells with invalid timestamps")
            cell_times = valid_times

        # Use a set to store processed cell pairs and avoid duplicates
        processed_pairs = set()
        boundary_edges = []
        
        # Track time differences for debugging
        time_diffs = []

        with tqdm(total=len(cell_adjacency), desc="Processing cell adjacencies") as pbar:
            for cell_id, neighbors in cell_adjacency.items():
                if cell_id not in cell_times:
                    logger.debug(f"Skipping cell {cell_id} - no timestamp data")
                    pbar.update(1)
                    continue
                    
                cell_time = cell_times[cell_id]

                for neighbor_id in neighbors:
                    if neighbor_id not in cell_times:
                        logger.debug(f"Skipping neighbor {neighbor_id} of cell {cell_id} - no timestamp data")
                        continue
                        
                    # Create a sorted tuple of the cell pair to ensure uniqueness
                    cell_pair = tuple(sorted([cell_id, neighbor_id]))
                    
                    # Skip if we've already processed this pair
                    if cell_pair in processed_pairs:
                        continue
                        
                    neighbor_time = cell_times[neighbor_id]
                    
                    # Ensure both timestamps are valid
                    if pd.isna(cell_time) or pd.isna(neighbor_time):
                        continue
                    
                    # Calculate time difference in hours
                    time_diff = abs((cell_time - neighbor_time).total_seconds() / 3600)
                    time_diffs.append(time_diff)
                    
                    logger.debug(f"Time difference between cells {cell_id} and {neighbor_id}: {time_diff:.2f} hours")

                    if time_diff >= delta_t:
                        logger.debug(f"Found boundary edge between cells {cell_id} and {neighbor_id} "
                                   f"(time diff: {time_diff:.2f} hours)")
                        boundary_edges.append(common_edges[cell_id][neighbor_id])
                    
                    # Mark this pair as processed
                    processed_pairs.add(cell_pair)

                pbar.update(1)

        # Log summary statistics
        if time_diffs:
            logger.info(f"Time difference statistics:")
            logger.info(f"  Min: {min(time_diffs):.2f} hours")
            logger.info(f"  Max: {max(time_diffs):.2f} hours")
            logger.info(f"  Mean: {sum(time_diffs)/len(time_diffs):.2f} hours")
            logger.info(f"  Number of boundaries found: {len(boundary_edges)}")
            logger.info(f"  Delta-t threshold: {delta_t} hours")

        return boundary_edges
==================================================

File: c:\GH\ras-commander\ras_commander\HdfInfiltration.py
==================================================
"""
Class: HdfInfiltration

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfInfiltration:
- scale_infiltration_data(): Updates infiltration parameters in HDF file with scaling factors
- get_infiltration_data(): Retrieves current infiltration parameters from HDF file
- get_infiltration_map(): Reads the infiltration raster map from HDF file
- calculate_soil_statistics(): Calculates soil statistics from zonal statistics
- get_significant_mukeys(): Gets mukeys with percentage greater than threshold
- calculate_total_significant_percentage(): Calculates total percentage covered by significant mukeys
- save_statistics(): Saves soil statistics to CSV
- get_infiltration_parameters(): Gets infiltration parameters for a specific mukey
- calculate_weighted_parameters(): Calculates weighted infiltration parameters based on soil statistics

Each function is decorated with @standardize_input to ensure consistent handling of HDF file paths
and @log_call for logging function calls and errors. Functions return various data types including
DataFrames, dictionaries, and floating-point values depending on their purpose.

The class provides comprehensive functionality for analyzing and modifying infiltration-related
data in HEC-RAS HDF files, including parameter scaling, soil statistics calculation, and
weighted parameter computation.
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)
        
from pathlib import Path
import pandas as pd
import geopandas as gpd
import h5py
from rasterstats import zonal_stats
from .Decorators import log_call, standardize_input

class HdfInfiltration:
        
    """
    A class for handling infiltration-related operations on HEC-RAS HDF files.

    This class provides methods to extract and modify infiltration data from HEC-RAS HDF files,
    including base overrides and infiltration parameters.
    """

    # Constants for unit conversion
    SQM_TO_ACRE = 0.000247105
    SQM_TO_SQMILE = 3.861e-7
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    @log_call
    def scale_infiltration_data(
        hdf_path: Path,
        infiltration_df: pd.DataFrame,
        scale_md: float = 1.0,
        scale_id: float = 1.0,
        scale_pr: float = 1.0
    ) -> Optional[pd.DataFrame]:
        """
        Update infiltration parameters in the HDF file with optional scaling factors.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        infiltration_df : pd.DataFrame
            DataFrame containing infiltration parameters with columns:
            ['Name', 'Maximum Deficit', 'Initial Deficit', 'Potential Percolation Rate']
        scale_md : float, optional
            Scaling factor for Maximum Deficit, by default 1.0
        scale_id : float, optional
            Scaling factor for Initial Deficit, by default 1.0
        scale_pr : float, optional
            Scaling factor for Potential Percolation Rate, by default 1.0

        Returns
        -------
        Optional[pd.DataFrame]
            The updated infiltration DataFrame if successful, None if operation fails
        """
        try:
            hdf_path_to_overwrite = '/Geometry/Infiltration/Base Overrides'
            
            # Apply scaling factors
            infiltration_df = infiltration_df.copy()
            infiltration_df['Maximum Deficit'] *= scale_md
            infiltration_df['Initial Deficit'] *= scale_id
            infiltration_df['Potential Percolation Rate'] *= scale_pr

            with h5py.File(hdf_path, 'a') as hdf_file:
                # Delete existing dataset if it exists
                if hdf_path_to_overwrite in hdf_file:
                    del hdf_file[hdf_path_to_overwrite]

                # Define dtype for structured array
                dt = np.dtype([
                    ('Land Cover Name', 'S7'),
                    ('Maximum Deficit', 'f4'),
                    ('Initial Deficit', 'f4'),
                    ('Potential Percolation Rate', 'f4')
                ])

                # Create structured array
                structured_array = np.zeros(infiltration_df.shape[0], dtype=dt)
                structured_array['Land Cover Name'] = np.array(infiltration_df['Name'].astype(str).values.astype('|S7'))
                structured_array['Maximum Deficit'] = infiltration_df['Maximum Deficit'].values.astype(np.float32)
                structured_array['Initial Deficit'] = infiltration_df['Initial Deficit'].values.astype(np.float32)
                structured_array['Potential Percolation Rate'] = infiltration_df['Potential Percolation Rate'].values.astype(np.float32)

                # Create new dataset
                hdf_file.create_dataset(
                    hdf_path_to_overwrite,
                    data=structured_array,  
                    dtype=dt,
                    compression='gzip',
                    compression_opts=1,
                    chunks=(100,),
                    maxshape=(None,)
                )

            return infiltration_df

        except Exception as e:
            logger.error(f"Error updating infiltration data in {hdf_path}: {str(e)}")
            return None

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    @log_call
    def get_infiltration_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Retrieve current infiltration parameters from the HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        Optional[pd.DataFrame]
            DataFrame containing infiltration parameters if successful, None if operation fails
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if '/Geometry/Infiltration/Base Overrides' not in hdf_file:
                    logger.warning(f"No infiltration data found in {hdf_path}")
                    return None

                data = hdf_file['/Geometry/Infiltration/Base Overrides'][()]
                
                # Convert structured array to DataFrame
                df = pd.DataFrame({
                    'Name': [name.decode('utf-8').strip() for name in data['Land Cover Name']],
                    'Maximum Deficit': data['Maximum Deficit'],
                    'Initial Deficit': data['Initial Deficit'],
                    'Potential Percolation Rate': data['Potential Percolation Rate']
                })
                
                return df

        except Exception as e:
            logger.error(f"Error reading infiltration data from {hdf_path}: {str(e)}")
            return None
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        


    @staticmethod
    @log_call
    @standardize_input
    def get_infiltration_map(hdf_path: Path) -> dict:
        """Read the infiltration raster map from HDF file
        
        Args:
            hdf_path: Path to the HDF file
            
        Returns:
            Dictionary mapping raster values to mukeys
        """
        with h5py.File(hdf_path, 'r') as hdf:
            raster_map_data = hdf['Raster Map'][:]
            return {int(item[0]): item[1].decode('utf-8') for item in raster_map_data}

    @staticmethod
    @log_call
    def calculate_soil_statistics(zonal_stats: list, raster_map: dict) -> pd.DataFrame:
        """Calculate soil statistics from zonal statistics
        
        Args:
            zonal_stats: List of zonal statistics
            raster_map: Dictionary mapping raster values to mukeys
            
        Returns:
            DataFrame with soil statistics including percentages and areas
        """
        # Initialize areas dictionary
        mukey_areas = {mukey: 0 for mukey in raster_map.values()}
        
        # Calculate total area and mukey areas
        total_area_sqm = 0
        for stat in zonal_stats:
            for raster_val, area in stat.items():
                mukey = raster_map.get(raster_val)
                if mukey:
                    mukey_areas[mukey] += area
                total_area_sqm += area

        # Create DataFrame rows
        rows = []
        for mukey, area_sqm in mukey_areas.items():
            if area_sqm > 0:
                rows.append({
                    'mukey': mukey,
                    'Percentage': (area_sqm / total_area_sqm) * 100,
                    'Area in Acres': area_sqm * HdfInfiltration.SQM_TO_ACRE,
                    'Area in Square Miles': area_sqm * HdfInfiltration.SQM_TO_SQMILE
                })
        
        return pd.DataFrame(rows)

    @staticmethod
    @log_call
    def get_significant_mukeys(soil_stats: pd.DataFrame, 
                             threshold: float = 1.0) -> pd.DataFrame:
        """Get mukeys with percentage greater than threshold
        
        Args:
            soil_stats: DataFrame with soil statistics
            threshold: Minimum percentage threshold (default 1.0)
            
        Returns:
            DataFrame with significant mukeys and their statistics
        """
        significant = soil_stats[soil_stats['Percentage'] > threshold].copy()
        significant.sort_values('Percentage', ascending=False, inplace=True)
        return significant

    @staticmethod
    @log_call
    def calculate_total_significant_percentage(significant_mukeys: pd.DataFrame) -> float:
        """Calculate total percentage covered by significant mukeys
        
        Args:
            significant_mukeys: DataFrame of significant mukeys
            
        Returns:
            Total percentage covered by significant mukeys
        """
        return significant_mukeys['Percentage'].sum()

    @staticmethod
    @log_call
    def save_statistics(soil_stats: pd.DataFrame, output_path: Path, 
                       include_timestamp: bool = True):
        """Save soil statistics to CSV
        
        Args:
            soil_stats: DataFrame with soil statistics
            output_path: Path to save CSV file
            include_timestamp: Whether to include timestamp in filename
        """
        if include_timestamp:
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            output_path = output_path.with_name(
                f"{output_path.stem}_{timestamp}{output_path.suffix}")
        
        soil_stats.to_csv(output_path, index=False)

    @staticmethod
    @log_call
    @standardize_input
    def get_infiltration_parameters(hdf_path: Path, mukey: str) -> dict:
        """Get infiltration parameters for a specific mukey from HDF file
        
        Args:
            hdf_path: Path to the HDF file
            mukey: Mukey identifier
            
        Returns:
            Dictionary of infiltration parameters
        """
        with h5py.File(hdf_path, 'r') as hdf:
            if 'Infiltration Parameters' not in hdf:
                raise KeyError("No infiltration parameters found in HDF file")
                
            params = hdf['Infiltration Parameters'][:]
            for row in params:
                if row[0].decode('utf-8') == mukey:
                    return {
                        'Initial Loss (in)': float(row[1]),
                        'Constant Loss Rate (in/hr)': float(row[2]),
                        'Impervious Area (%)': float(row[3])
                    }
        return None

    @staticmethod
    @log_call
    def calculate_weighted_parameters(soil_stats: pd.DataFrame, 
                                   infiltration_params: dict) -> dict:
        """Calculate weighted infiltration parameters based on soil statistics
        
        Args:
            soil_stats: DataFrame with soil statistics
            infiltration_params: Dictionary of infiltration parameters by mukey
            
        Returns:
            Dictionary of weighted average infiltration parameters
        """
        total_weight = soil_stats['Percentage'].sum()
        
        weighted_params = {
            'Initial Loss (in)': 0.0,
            'Constant Loss Rate (in/hr)': 0.0,
            'Impervious Area (%)': 0.0
        }
        
        for _, row in soil_stats.iterrows():
            mukey = row['mukey']
            weight = row['Percentage'] / total_weight
            
            if mukey in infiltration_params:
                for param in weighted_params:
                    weighted_params[param] += (
                        infiltration_params[mukey][param] * weight
                    )
        
        return weighted_params

# Example usage:
"""
from pathlib import Path

# Initialize paths
raster_path = Path('input_files/gSSURGO_InfiltrationDC.tif')
boundary_path = Path('input_files/WF_Boundary_Simple.shp')
hdf_path = raster_path.with_suffix('.hdf')

# Get infiltration mapping
infil_map = HdfInfiltration.get_infiltration_map(hdf_path)

# Get zonal statistics (using RasMapper class)
clipped_data, transform, nodata = RasMapper.clip_raster_with_boundary(
    raster_path, boundary_path)
stats = RasMapper.calculate_zonal_stats(
    boundary_path, clipped_data, transform, nodata)

# Calculate soil statistics
soil_stats = HdfInfiltration.calculate_soil_statistics(stats, infil_map)

# Get significant mukeys (>1%)
significant = HdfInfiltration.get_significant_mukeys(soil_stats, threshold=1.0)

# Calculate total percentage of significant mukeys
total_significant = HdfInfiltration.calculate_total_significant_percentage(significant)
print(f"Total percentage of significant mukeys: {total_significant}%")

# Get infiltration parameters for each significant mukey
infiltration_params = {}
for mukey in significant['mukey']:
    params = HdfInfiltration.get_infiltration_parameters(hdf_path, mukey)
    if params:
        infiltration_params[mukey] = params

# Calculate weighted parameters
weighted_params = HdfInfiltration.calculate_weighted_parameters(
    significant, infiltration_params)
print("Weighted infiltration parameters:", weighted_params)

# Save results
HdfInfiltration.save_statistics(soil_stats, Path('soil_statistics.csv'))
"""
==================================================

File: c:\GH\ras-commander\ras_commander\HdfMesh.py
==================================================
"""
A static class for handling mesh-related operations on HEC-RAS HDF files.

This class provides static methods to extract and analyze mesh data from HEC-RAS HDF files,
including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
2D flow area attributes. No instantiation is required to use these methods.

All methods are designed to work with the mesh geometry data stored in
HEC-RAS HDF files, providing functionality to retrieve and process various aspects
of the 2D flow areas and their associated mesh structures.


List of Functions:
-----------------
get_mesh_area_names()
    Returns list of 2D mesh area names
get_mesh_areas()
    Returns 2D flow area perimeter polygons
get_mesh_cell_polygons()
    Returns 2D flow mesh cell polygons
get_mesh_cell_points()
    Returns 2D flow mesh cell center points
get_mesh_cell_faces()
    Returns 2D flow mesh cell faces
get_mesh_area_attributes()
    Returns geometry 2D flow area attributes
get_mesh_face_property_tables()
    Returns Face Property Tables for each Face in all 2D Flow Areas
get_mesh_cell_property_tables()
    Returns Cell Property Tables for each Cell in all 2D Flow Areas

Each function is decorated with @standardize_input and @log_call for consistent
input handling and logging functionality.
"""
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import Polygon, Point, LineString, MultiLineString, MultiPolygon
from shapely.ops import polygonize  # Importing polygonize to resolve the undefined name error
from typing import List, Tuple, Optional, Dict, Any
import logging
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfMesh:
    """
    A class for handling mesh-related operations on HEC-RAS HDF files.

    This class provides methods to extract and analyze mesh data from HEC-RAS HDF files,
    including mesh area names, mesh areas, cell polygons, cell points, cell faces, and
    2D flow area attributes.

    Methods in this class are designed to work with the mesh geometry data stored in
    HEC-RAS HDF files, providing functionality to retrieve and process various aspects
    of the 2D flow areas and their associated mesh structures.

    Note: This class relies on HdfBase and HdfUtils for some underlying operations.
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_area_names(hdf_path: Path) -> List[str]:
        """
        Return a list of the 2D mesh area names from the RAS geometry.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        List[str]
            A list of the 2D mesh area names within the RAS geometry.
            Returns an empty list if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/2D Flow Areas" not in hdf_file:
                    return list()
                return list(
                    [
                        HdfUtils.convert_ras_string(n.decode('utf-8'))
                        for n in hdf_file["Geometry/2D Flow Areas/Attributes"][()]["Name"]
                    ]
                )
        except Exception as e:
            logger.error(f"Error reading mesh area names from {hdf_path}: {str(e)}")
            return list()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_areas(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow area perimeter polygons.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow area perimeter polygons if 2D areas exist.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()
                mesh_area_polygons = [
                    Polygon(hdf_file["Geometry/2D Flow Areas/{}/Perimeter".format(n)][()])
                    for n in mesh_area_names
                ]
                return GeoDataFrame(
                    {"mesh_name": mesh_area_names, "geometry": mesh_area_polygons},
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file),
                )
        except Exception as e:
            logger.error(f"Error reading mesh areas from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_cell_polygons(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow mesh cell polygons.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell polygons with columns:
            - mesh_name: name of the mesh area
            - cell_id: unique identifier for each cell
            - geometry: polygon geometry of the cell
            Returns an empty GeoDataFrame if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Get face geometries once
                face_gdf = HdfMesh.get_mesh_cell_faces(hdf_path)
                
                # Pre-allocate lists for better memory efficiency
                all_mesh_names = []
                all_cell_ids = []
                all_geometries = []

                for mesh_name in mesh_area_names:
                    # Get cell face info in one read
                    cell_face_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Face and Orientation Info"][()]
                    cell_face_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Face and Orientation Values"][()][:, 0]
                    
                    # Create face lookup dictionary for this mesh
                    mesh_faces_dict = dict(face_gdf[face_gdf.mesh_name == mesh_name][["face_id", "geometry"]].values)

                    # Process each cell
                    for cell_id, (start, length) in enumerate(cell_face_info[:, :2]):
                        face_ids = cell_face_values[start:start + length]
                        face_geoms = [mesh_faces_dict[face_id] for face_id in face_ids]
                        
                        # Create polygon
                        polygons = list(polygonize(face_geoms))
                        if polygons:
                            all_mesh_names.append(mesh_name)
                            all_cell_ids.append(cell_id)
                            all_geometries.append(Polygon(polygons[0]))

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "cell_id": all_cell_ids,
                        "geometry": all_geometries
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell polygons from {hdf_path}: {str(e)}")
            return GeoDataFrame()
        
    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cell_points(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow mesh cell center points.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell center points.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()
                
                # Pre-allocate lists
                all_mesh_names = []
                all_cell_ids = []
                all_points = []

                for mesh_name in mesh_area_names:
                    # Get all cell centers in one read
                    cell_centers = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"][()]
                    cell_count = len(cell_centers)
                    
                    # Extend lists efficiently
                    all_mesh_names.extend([mesh_name] * cell_count)
                    all_cell_ids.extend(range(cell_count))
                    all_points.extend(Point(coords) for coords in cell_centers)

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "cell_id": all_cell_ids,
                        "geometry": all_points
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell points from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cell_faces(hdf_path: Path) -> GeoDataFrame:
        """
        Return 2D flow mesh cell faces.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the 2D flow mesh cell faces.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return GeoDataFrame()

                # Pre-allocate lists
                all_mesh_names = []
                all_face_ids = []
                all_geometries = []

                for mesh_name in mesh_area_names:
                    # Read all data at once
                    facepoints_index = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Perimeter Values"][()]

                    # Process each face
                    for face_id, ((pnt_a_idx, pnt_b_idx), (start_row, count)) in enumerate(zip(facepoints_index, faces_perim_info)):
                        coords = [facepoints_coords[pnt_a_idx]]
                        
                        if count > 0:
                            coords.extend(faces_perim_values[start_row:start_row + count])
                            
                        coords.append(facepoints_coords[pnt_b_idx])
                        
                        all_mesh_names.append(mesh_name)
                        all_face_ids.append(face_id)
                        all_geometries.append(LineString(coords))

                # Create GeoDataFrame in one go
                return GeoDataFrame(
                    {
                        "mesh_name": all_mesh_names,
                        "face_id": all_face_ids,
                        "geometry": all_geometries
                    },
                    geometry="geometry",
                    crs=HdfBase.get_projection(hdf_file)
                )

        except Exception as e:
            logger.error(f"Error reading mesh cell faces from {hdf_path}: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_area_attributes(hdf_path: Path) -> pd.DataFrame:
        """
        Return geometry 2D flow area attributes from a HEC-RAS HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing the 2D flow area attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                d2_flow_area = hdf_file.get("Geometry/2D Flow Areas/Attributes")
                if d2_flow_area is not None and isinstance(d2_flow_area, h5py.Dataset):
                    result = {}
                    for name in d2_flow_area.dtype.names:
                        try:
                            value = d2_flow_area[name][()]
                            if isinstance(value, bytes):
                                value = value.decode('utf-8')  # Decode as UTF-8
                            result[name] = value if not isinstance(value, bytes) else value.decode('utf-8')
                        except Exception as e:
                            logger.warning(f"Error converting attribute '{name}': {str(e)}")
                    return pd.DataFrame.from_dict(result, orient='index', columns=['Value'])
                else:
                    logger.info("No 2D Flow Area attributes found or invalid dataset.")
                    return pd.DataFrame()  # Return an empty DataFrame
        except Exception as e:
            logger.error(f"Error reading 2D flow area attributes from {hdf_path}: {str(e)}")
            return pd.DataFrame()  # Return an empty DataFrame

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_face_property_tables(hdf_path: Path) -> Dict[str, pd.DataFrame]:
        """
        Extract Face Property Tables for each Face in all 2D Flow Areas.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        Dict[str, pd.DataFrame]
            A dictionary where:
            - keys: mesh area names (str)
            - values: DataFrames with columns:
                - Face ID: unique identifier for each face
                - Z: elevation
                - Area: face area
                - Wetted Perimeter: wetted perimeter length
                - Manning's n: Manning's roughness coefficient
            Returns an empty dictionary if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return {}

                result = {}
                for mesh_name in mesh_area_names:
                    area_elevation_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Area Elevation Info"][()]
                    area_elevation_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Faces Area Elevation Values"][()]
                    
                    face_data = []
                    for face_id, (start_index, count) in enumerate(area_elevation_info):
                        face_values = area_elevation_values[start_index:start_index+count]
                        for z, area, wetted_perimeter, mannings_n in face_values:
                            face_data.append({
                                'Face ID': face_id,
                                'Z': str(z),
                                'Area': str(area), 
                                'Wetted Perimeter': str(wetted_perimeter),
                                "Manning's n": str(mannings_n)
                            })
                    
                    result[mesh_name] = pd.DataFrame(face_data)
                
                return result

        except Exception as e:
            logger.error(f"Error extracting face property tables from {hdf_path}: {str(e)}")
            return {}

    @staticmethod
    @standardize_input(file_type='geom_hdf')
    def get_mesh_cell_property_tables(hdf_path: Path) -> Dict[str, pd.DataFrame]:
        """
        Extract Cell Property Tables for each Cell in all 2D Flow Areas.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.

        Returns
        -------
        Dict[str, pd.DataFrame]
            A dictionary where:
            - keys: mesh area names (str)
            - values: DataFrames with columns:
                - Cell ID: unique identifier for each cell
                - Z: elevation
                - Volume: cell volume
                - Surface Area: cell surface area
            Returns an empty dictionary if no 2D areas exist or if there's an error.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                mesh_area_names = HdfMesh.get_mesh_area_names(hdf_path)
                if not mesh_area_names:
                    return {}

                result = {}
                for mesh_name in mesh_area_names:
                    cell_elevation_info = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Elevation Volume Info"][()]
                    cell_elevation_values = hdf_file[f"Geometry/2D Flow Areas/{mesh_name}/Cells Elevation Volume Values"][()]
                    
                    cell_data = []
                    for cell_id, (start_index, count) in enumerate(cell_elevation_info):
                        cell_values = cell_elevation_values[start_index:start_index+count]
                        for z, volume, surface_area in cell_values:
                            cell_data.append({
                                'Cell ID': cell_id,
                                'Z': str(z),
                                'Volume': str(volume),
                                'Surface Area': str(surface_area)
                            })
                    
                    result[mesh_name] = pd.DataFrame(cell_data)
                
                return result

        except Exception as e:
            logger.error(f"Error extracting cell property tables from {hdf_path}: {str(e)}")
            return {}

==================================================

File: c:\GH\ras-commander\ras_commander\HdfPipe.py
==================================================
"""
Class: HdfPipe

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPipe:
Geometry Retrieval Functions:
- get_pipe_conduits() - Get pipe conduit geometries and attributes
- get_pipe_nodes() - Get pipe node geometries and attributes
- get_pipe_network() - Get complete pipe network data
- get_pipe_profile() - Get elevation profile for a specific conduit
- extract_pipe_network_data() - Extract both nodes and conduits data

Results Retrieval Functions:
- get_pipe_network_timeseries() - Get timeseries data for pipe network variables
- get_pipe_network_summary() - Get summary statistics for pipe networks
- get_pipe_node_timeseries() - Get timeseries data for a specific node
- get_pipe_conduit_timeseries() - Get timeseries data for a specific conduit

Note: All functions use the @standardize_input decorator to validate input paths
and the @log_call decorator for logging function calls.
"""
import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import LineString, Point, MultiLineString, Polygon, MultiPolygon
from typing import List, Dict, Any, Optional, Union, Tuple
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger
from .HdfResultsMesh import HdfResultsMesh
import logging  

logger = get_logger(__name__)

class HdfPipe:
    """
    Static methods for handling pipe network data from HEC-RAS HDF files.

    Contains methods for:
    - Geometry retrieval (nodes, conduits, networks, profiles)
    - Results retrieval (timeseries and summary data)

    All methods use @standardize_input for path validation and @log_call
    """

    # Geometry Retrieval Functions
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_conduits(hdf_path: Path, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Extracts pipe conduit geometries and attributes from HDF5 file.

        Parameters:
            hdf_path: Path to the HDF5 file
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame with columns:
            - Attributes from HDF5
            - Polyline: LineString geometries
            - Terrain_Profiles: List of (station, elevation) tuples
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Conduits/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Polyline Data ---
            polyline_info = group['Polyline Info'][:]  # Shape (132,4) - point_start_idx, point_count, part_start_idx, part_count
            polyline_points = group['Polyline Points'][:]  # Shape (396,2) - x,y coordinates
            
            polyline_geometries = []
            for info in polyline_info:
                point_start_idx = info[0]
                point_count = info[1]
                
                # Extract coordinates for this polyline directly using start index and count
                coords = polyline_points[point_start_idx:point_start_idx + point_count]
                
                if len(coords) < 2:
                    polyline_geometries.append(None)
                else:
                    polyline_geometries.append(LineString(coords))
            
            # --- Read Terrain Profiles Data ---
            terrain_info = group['Terrain Profiles Info'][:]
            terrain_values = group['Terrain Profiles Values'][:]
            
            # Create a list of (Station, Elevation) tuples for Terrain Profiles
            terrain_coords = list(zip(terrain_values[:, 0], terrain_values[:, 1]))
            
            terrain_profiles_list: List[List[Tuple[float, float]]] = []
            
            for i in range(len(terrain_info)):
                info = terrain_info[i]
                start_idx = info[0]
                count = info[1]
                
                # Extract (Station, Elevation) pairs
                segment = terrain_coords[start_idx : start_idx + count]
                
                terrain_profiles_list.append(segment)  # Store the list of (Station, Elevation) tuples
            
            # --- Combine Data into GeoDataFrame ---
            attr_df['Polyline'] = polyline_geometries
            attr_df['Terrain_Profiles'] = terrain_profiles_list
            
            # Initialize GeoDataFrame with Polyline geometries
            gdf = gpd.GeoDataFrame(attr_df, geometry='Polyline', crs=crs)
            
            return gdf


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_nodes(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for Pipe Node points and their attributes from an HDF5 file.
        
        Parameters:
        - hdf_path: Path to the HDF5 file.
        
        Returns:
        - A GeoDataFrame containing pipe node attributes and their geometries.
        """
        with h5py.File(hdf_path, 'r') as f:
            group = f['/Geometry/Pipe Nodes/']
            
            # --- Read and Process Attributes ---
            attributes = group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode byte string fields to UTF-8 strings
            string_columns = attr_df.select_dtypes([object]).columns  # Changed 'S' to object
            for col in string_columns:
                attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            # --- Read Points Data ---
            points = group['Points'][:]
            # Create Shapely Point geometries
            geometries = [Point(xy) for xy in points]
            
            # --- Combine Attributes and Geometries into GeoDataFrame ---
            gdf = gpd.GeoDataFrame(attr_df, geometry=geometries)
            
            return gdf
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network(hdf_path: Path, pipe_network_name: Optional[str] = None, crs: Optional[str] = "EPSG:4326") -> gpd.GeoDataFrame:
        """
        Creates a GeoDataFrame for a pipe network's geometry.

        Parameters:
            hdf_path: Path to the HDF5 file
            pipe_network_name: Name of network (uses first if None)
            crs: Coordinate Reference System (default: "EPSG:4326")

        Returns:
            GeoDataFrame containing:
            - Cell polygons (primary geometry)
            - Face polylines
            - Node points
            - Associated attributes
        """
        with h5py.File(hdf_path, 'r') as f:
            pipe_networks_group = f['/Geometry/Pipe Networks/']
            
            # --- Determine Pipe Network to Use ---
            attributes = pipe_networks_group['Attributes'][:]
            attr_df = pd.DataFrame(attributes)
            
            # Decode 'Name' from byte strings to UTF-8
            attr_df['Name'] = attr_df['Name'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
            
            if pipe_network_name:
                if pipe_network_name not in attr_df['Name'].values:
                    raise ValueError(f"Pipe network '{pipe_network_name}' not found in the HDF5 file.")
                network_idx = attr_df.index[attr_df['Name'] == pipe_network_name][0]
            else:
                network_idx = 0  # Default to first network
            
            # Get the name of the selected pipe network
            selected_network_name = attr_df.at[network_idx, 'Name']
            logging.info(f"Selected Pipe Network: {selected_network_name}")
            
            # Access the selected pipe network group
            network_group_path = f"/Geometry/Pipe Networks/{selected_network_name}/"
            network_group = f[network_group_path]
            
            # --- Helper Functions ---
            def decode_bytes(df: pd.DataFrame) -> pd.DataFrame:
                """Decode byte string columns to UTF-8."""
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                return df
            
            def build_polygons(info, parts, points) -> List[Optional[Polygon or MultiPolygon]]:
                """Build Shapely Polygon or MultiPolygon geometries from HDF5 datasets."""
                poly_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    cell_info = info[i]
                    point_start_idx = cell_info[0]
                    point_count = cell_info[1]
                    part_start_idx = cell_info[2]
                    part_count = cell_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = poly_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 3:
                            continue  # Not a valid polygon part
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        try:
                            geometries.append(Polygon(parts_list[0]))
                        except ValueError:
                            geometries.append(None)
                    else:
                        try:
                            geometries.append(MultiPolygon([Polygon(p) for p in parts_list if len(p) >= 3]))
                        except ValueError:
                            geometries.append(None)
                return geometries
            
            def build_multilinestring(info, parts, points) -> List[Optional[LineString or MultiLineString]]:
                """Build Shapely LineString or MultiLineString geometries from HDF5 datasets."""
                line_coords = list(zip(points[:, 0], points[:, 1]))
                geometries = []
                for i in range(len(info)):
                    face_info = info[i]
                    point_start_idx = face_info[0]
                    point_count = face_info[1]
                    part_start_idx = face_info[2]
                    part_count = face_info[3]
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(parts):
                            continue  # Prevent index out of range
                        part_info = parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        coords = line_coords[part_point_start : part_point_start + part_point_count]
                        if len(coords) < 2:
                            continue  # Cannot form LineString with fewer than 2 points
                        parts_list.append(coords)
                    
                    if not parts_list:
                        geometries.append(None)
                    elif len(parts_list) == 1:
                        geometries.append(LineString(parts_list[0]))
                    else:
                        geometries.append(MultiLineString(parts_list))
                return geometries
            
            # --- Read and Process Cell Polygons ---
            cell_polygons_info = network_group['Cell Polygons Info'][:]
            cell_polygons_parts = network_group['Cell Polygons Parts'][:]
            cell_polygons_points = network_group['Cell Polygons Points'][:]
            
            cell_polygons_geometries = build_polygons(cell_polygons_info, cell_polygons_parts, cell_polygons_points)
            
            # --- Read and Process Face Polylines ---
            face_polylines_info = network_group['Face Polylines Info'][:]
            face_polylines_parts = network_group['Face Polylines Parts'][:]
            face_polylines_points = network_group['Face Polylines Points'][:]
            
            face_polylines_geometries = build_multilinestring(face_polylines_info, face_polylines_parts, face_polylines_points)
            
            # --- Read and Process Node Points ---
            node_surface_connectivity_group = network_group.get('Node Surface Connectivity', None)
            if node_surface_connectivity_group is not None:
                node_surface_connectivity = node_surface_connectivity_group[:]
            else:
                node_surface_connectivity = None
            
            # Assuming Node Connectivity Info and Values contain node coordinates
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            node_indices = network_group['Node Indices'][:]
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            
            # For simplicity, assuming that node connectivity includes X and Y coordinates
            # This may need to be adjusted based on actual data structure
            # Here, we'll create dummy points as placeholder
            # Replace with actual coordinate extraction logic as per data structure
            # For demonstration, we'll create random points
            # You should replace this with actual data extraction
            # Example:
            # node_points = network_group['Node Coordinates'][:]
            # node_geometries = [Point(x, y) for x, y in node_points]
            
            # Placeholder for node geometries
            # Assuming node_indices contains Node IDs and coordinates
            # Adjust based on actual dataset structure
            # Here, we assume that node_indices has columns: [Node ID, X, Y]
            # But based on the log, Node Surface Connectivity has ['Node ID', 'Layer', 'Layer ID', 'Sublayer ID']
            # No coordinates are provided, so we cannot create Point geometries unless coordinates are available elsewhere
            # Therefore, this part may need to be adapted based on actual data
            # For now, we'll skip node points geometries
            node_geometries = [None] * len(node_indices)  # Placeholder
            
            # --- Read and Process Cell Property Table ---
            cell_property_table = network_group['Cell Property Table'][:]
            cell_property_df = pd.DataFrame(cell_property_table)
            
            # Decode byte strings if any
            cell_property_df = decode_bytes(cell_property_df)
            
            # --- Read and Process Cells DS Face Indices ---
            cells_ds_face_info = network_group['Cells DS Face Indices Info'][:]
            cells_ds_face_values = network_group['Cells DS Face Indices Values'][:]
            
            # Create lists of DS Face Indices per cell
            cells_ds_face_indices = []
            for i in range(len(cells_ds_face_info)):
                info = cells_ds_face_info[i]
                start_idx, count = info
                indices = cells_ds_face_values[start_idx : start_idx + count]
                cells_ds_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Face Indices ---
            cells_face_info = network_group['Cells Face Indices Info'][:]
            cells_face_values = network_group['Cells Face Indices Values'][:]
            
            # Create lists of Face Indices per cell
            cells_face_indices = []
            for i in range(len(cells_face_info)):
                info = cells_face_info[i]
                start_idx, count = info
                indices = cells_face_values[start_idx : start_idx + count]
                cells_face_indices.append(indices.tolist())
            
            # --- Read and Process Cells Minimum Elevations ---
            cells_min_elevations = network_group['Cells Minimum Elevations'][:]
            cells_min_elevations_df = pd.DataFrame(cells_min_elevations, columns=['Minimum_Elevation'])
            
            # --- Read and Process Cells Node and Conduit IDs ---
            cells_node_conduit_ids = network_group['Cells Node and Conduit IDs'][:]
            cells_node_conduit_df = pd.DataFrame(cells_node_conduit_ids, columns=['Node_ID', 'Conduit_ID'])
            
            # --- Read and Process Cells US Face Indices ---
            cells_us_face_info = network_group['Cells US Face Indices Info'][:]
            cells_us_face_values = network_group['Cells US Face Indices Values'][:]
            
            # Create lists of US Face Indices per cell
            cells_us_face_indices = []
            for i in range(len(cells_us_face_info)):
                info = cells_us_face_info[i]
                start_idx, count = info
                indices = cells_us_face_values[start_idx : start_idx + count]
                cells_us_face_indices.append(indices.tolist())
            
            # --- Read and Process Conduit Indices ---
            conduit_indices = network_group['Conduit Indices'][:]
            conduit_indices_df = pd.DataFrame(conduit_indices, columns=['Conduit_ID'])
            
            # --- Read and Process Face Property Table ---
            face_property_table = network_group['Face Property Table'][:]
            face_property_df = pd.DataFrame(face_property_table)
            
            # Decode byte strings if any
            face_property_df = decode_bytes(face_property_df)
            
            # --- Read and Process Face Conduit ID and Stations ---
            faces_conduit_id_stations = network_group['Faces Conduit ID and Stations'][:]
            faces_conduit_df = pd.DataFrame(faces_conduit_id_stations, columns=['ConduitID', 'ConduitStation', 'CellUS', 'CellDS', 'Elevation'])
            
            # --- Read and Process Node Connectivity Info and Values ---
            node_connectivity_info = network_group['Node Connectivity Info'][:]
            node_connectivity_values = network_group['Node Connectivity Values'][:]
            
            # Create lists of connected nodes per node
            node_connectivity = []
            for i in range(len(node_connectivity_info)):
                info = node_connectivity_info[i]
                start_idx, count = info
                connections = node_connectivity_values[start_idx : start_idx + count]
                node_connectivity.append(connections.tolist())
            
            # --- Read and Process Node Indices ---
            node_indices = network_group['Node Indices'][:]
            node_indices_df = pd.DataFrame(node_indices, columns=['Node_ID'])
            
            # --- Read and Process Node Surface Connectivity ---
            node_surface_connectivity = network_group['Node Surface Connectivity'][:]
            node_surface_connectivity_df = pd.DataFrame(node_surface_connectivity, columns=['Node_ID', 'Layer', 'Layer_ID', 'Sublayer_ID'])
            
            # --- Combine All Cell-Related Data ---
            cells_df = pd.DataFrame({
                'Cell_ID': range(len(cell_polygons_geometries)),
                'Conduit_ID': cells_node_conduit_df['Conduit_ID'],
                'Node_ID': cells_node_conduit_df['Node_ID'],
                'Minimum_Elevation': cells_min_elevations_df['Minimum_Elevation'],
                'DS_Face_Indices': cells_ds_face_indices,
                'Face_Indices': cells_face_indices,
                'US_Face_Indices': cells_us_face_indices,
                'Cell_Property_Info_Index': cell_property_df['Info Index'],
                # Add other cell properties as needed
            })
            
            # Merge with cell property table
            cells_df = cells_df.merge(cell_property_df, left_on='Cell_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Face-Related Data ---
            faces_df = pd.DataFrame({
                'Face_ID': range(len(face_polylines_geometries)),
                'Conduit_ID': faces_conduit_df['ConduitID'],
                'Conduit_Station': faces_conduit_df['ConduitStation'],
                'Cell_US': faces_conduit_df['CellUS'],
                'Cell_DS': faces_conduit_df['CellDS'],
                'Elevation': faces_conduit_df['Elevation'],
                'Face_Property_Info_Index': face_property_df['Info Index'],
                # Add other face properties as needed
            })
            
            # Merge with face property table
            faces_df = faces_df.merge(face_property_df, left_on='Face_Property_Info_Index', right_index=True, how='left')
            
            # --- Combine All Node-Related Data ---
            nodes_df = pd.DataFrame({
                'Node_ID': node_indices_df['Node_ID'],
                'Connected_Nodes': node_connectivity,
                # Add other node properties as needed
            })
            
            # Merge with node surface connectivity
            nodes_df = nodes_df.merge(node_surface_connectivity_df, on='Node_ID', how='left')
            
            # --- Create GeoDataFrame ---
            # Main DataFrame will be cells with their polygons
            cells_df['Cell_Polygon'] = cell_polygons_geometries
            
            # Add face polylines as a separate column (list of geometries)
            cells_df['Face_Polylines'] = cells_df['Face_Indices'].apply(lambda indices: [face_polylines_geometries[i] for i in indices if i < len(face_polylines_geometries)])
            
            # Add node points if geometries are available
            # Currently, node_geometries are placeholders (None). Replace with actual geometries if available.
            cells_df['Node_Point'] = cells_df['Node_ID'].apply(lambda nid: node_geometries[nid] if nid < len(node_geometries) else None)
            
            # Initialize GeoDataFrame with Cell Polygons
            gdf = gpd.GeoDataFrame(cells_df, geometry='Cell_Polygon', crs=crs)
            
            # Optionally, add Face Polylines and Node Points as separate columns
            # Note: GeoPandas primarily supports one geometry column, so these are stored as object columns
            gdf['Face_Polylines'] = cells_df['Face_Polylines']
            gdf['Node_Point'] = cells_df['Node_Point']
            
            # You can further expand this GeoDataFrame by merging with faces_df and nodes_df if needed
            
            return gdf
        
        
        
        
        


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_profile(hdf_path: Path, conduit_id: int) -> pd.DataFrame:
        """
        Extract the profile data for a specific pipe conduit.

        Args:
            hdf_path (Path): Path to the HDF file.
            conduit_id (int): ID of the conduit to extract profile for.

        Returns:
            pd.DataFrame: DataFrame containing the pipe profile data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
            IndexError: If the specified conduit_id is out of range.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Get conduit info
                terrain_profiles_info = hdf['/Geometry/Pipe Conduits/Terrain Profiles Info'][()]
                
                if conduit_id >= len(terrain_profiles_info):
                    raise IndexError(f"conduit_id {conduit_id} is out of range")

                start, count = terrain_profiles_info[conduit_id]

                # Extract profile data
                profile_values = hdf['/Geometry/Pipe Conduits/Terrain Profiles Values'][start:start+count]

                # Create DataFrame
                df = pd.DataFrame(profile_values, columns=['Station', 'Elevation'])

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except IndexError as e:
            logger.error(f"Invalid conduit_id: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe profile data: {e}")
            raise
        
        
   









# RESULTS FUNCTIONS: 

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract results summary data for pipe networks from the HDF file.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pipe network summary data.

        Raises:
            KeyError: If the required datasets are not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pipe Network"
                if summary_path not in hdf:
                    logger.warning("Pipe Network summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame
                df = pd.DataFrame(summary_data)

                # Convert column names
                df.columns = [col.decode('utf-8') for col in df.columns]

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network summary data: {e}")
            raise




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_node(plan_hdf_path: Path, node_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific node.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        node_id : int
            ID of the node to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Depth
            - Drop Inlet Flow
            - Water Surface
        """
        try:
            node_variables = ["Nodes/Depth", "Nodes/Drop Inlet Flow", "Nodes/Water Surface"]
            node_data = {}

            for variable in node_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                node_data[variable] = data.sel(location=node_id)
            
            return node_data
        except Exception as e:
            logger.error(f"Error extracting time series data for node {node_id}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def extract_timeseries_for_conduit(plan_hdf_path: Path, conduit_id: int) -> Dict[str, xr.DataArray]:
        """
        Extract time series data for a specific conduit.
        
        Parameters:
        -----------
        plan_hdf_path : Path
            Path to HEC-RAS results HDF file
        conduit_id : int
            ID of the conduit to extract data for
            
        Returns:
        --------
        Dict[str, xr.DataArray]: Dictionary containing time series data for:
            - Pipe Flow (US/DS)
            - Velocity (US/DS)
        """
        try:
            conduit_variables = ["Pipes/Pipe Flow DS", "Pipes/Pipe Flow US", 
                                "Pipes/Vel DS", "Pipes/Vel US"]
            conduit_data = {}

            for variable in conduit_variables:
                data = HdfPipe.get_pipe_network_timeseries(plan_hdf_path, variable=variable)
                conduit_data[variable] = data.sel(location=conduit_id)
            
            return conduit_data
        except Exception as e:
            logger.error(f"Error extracting time series data for conduit {conduit_id}: {str(e)}")
            raise


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pipe_network_timeseries(hdf_path: Path, variable: str) -> xr.DataArray:
        """
        Extracts timeseries data for a pipe network variable.

        Parameters:
            hdf_path: Path to the HDF5 file
            variable: Variable name to extract. Valid options:
                - Cell: Courant, Water Surface
                - Face: Flow, Velocity, Water Surface
                - Pipes: Pipe Flow (DS/US), Vel (DS/US)
                - Nodes: Depth, Drop Inlet Flow, Water Surface

        Returns:
            xarray.DataArray with dimensions (time, location)
        """
        valid_variables = [
            "Cell Courant", "Cell Water Surface", "Face Flow", "Face Velocity",
            "Face Water Surface", "Pipes/Pipe Flow DS", "Pipes/Pipe Flow US",
            "Pipes/Vel DS", "Pipes/Vel US", "Nodes/Depth", "Nodes/Drop Inlet Flow",
            "Nodes/Water Surface"
        ]

        if variable not in valid_variables:
            raise ValueError(f"Invalid variable. Must be one of: {', '.join(valid_variables)}")

        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract timeseries data
                data_path = f"/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pipe Networks/Davis/{variable}"
                data = hdf[data_path][()]

                # Extract time information using the correct method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'location'],
                    coords={'time': time, 'location': range(data.shape[1])},
                    name=variable
                )

                # Add attributes
                da.attrs['units'] = hdf[data_path].attrs.get('Units', b'').decode('utf-8')
                da.attrs['variable'] = variable

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pipe network timeseries data: {e}")
            raise





==================================================

File: c:\GH\ras-commander\ras_commander\HdfPlan.py
==================================================
"""
Class: HdfPlan

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPlan:
- get_simulation_start_time()
- get_simulation_end_time()
- get_unsteady_datetimes()
- get_plan_info_attrs()
- get_plan_parameters()
- get_meteorology_precip_attrs()
- get_geom_attrs()


REVISIONS NEEDED: 

Use get_ prefix for functions that return data.  
Since we are extracting plan data, we should use get_plan_...
BUT, we will never set results data, so we should use results_

We need to shorten names where possible.

List of Revised Functions in HdfPlan:
- get_plan_start_time()
- get_plan_end_time()
- get_plan_timestamps_list()     
- get_plan_information()
- get_plan_parameters()
- get_plan_met_precip()
- get_geometry_information()






"""

import h5py
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)


class HdfPlan:
    """
    A class for handling HEC-RAS plan HDF files.

    Provides static methods for extracting data from HEC-RAS plan HDF files including 
    simulation times, plan information, and geometry attributes. All methods use 
    @standardize_input for handling different input types and @log_call for logging.

    Note: This code is partially derived from the rashdf library (https://github.com/fema-ffrd/rashdf)
    under MIT license.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_start_time(hdf_path: Path) -> datetime:
        """
        Get the plan start time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan start time in UTC format.

        Raises:
            ValueError: If there's an error reading the plan start time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_simulation_start_time(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan start time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_end_time(hdf_path: Path) -> datetime:
        """
        Get the plan end time from the plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            datetime: The plan end time.

        Raises:
            ValueError: If there's an error reading the plan end time.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info = hdf_file.get('Plan Data/Plan Information')
                if plan_info is None:
                    raise ValueError("Plan Information not found in HDF file")
                time_str = plan_info.attrs.get('Simulation End Time')
                return HdfUtils.parse_ras_datetime(time_str.decode('utf-8'))
        except Exception as e:
            raise ValueError(f"Failed to get plan end time: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_timestamps_list(hdf_path: Path) -> List[datetime]:
        """
        Get the list of output timestamps from the plan simulation.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            List[datetime]: Chronological list of simulation output timestamps in UTC.

        Raises:
            ValueError: If there's an error retrieving the plan timestamps.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfBase.get_unsteady_timestamps(hdf_file)
        except Exception as e:
            raise ValueError(f"Failed to get plan timestamps: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_information(hdf_path: Path) -> Dict:
        """
        Get plan information from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Plan information including simulation times, flow regime, 
                computation settings, etc.

        Raises:
            ValueError: If there's an error retrieving the plan information.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_info_path = "Plan Data/Plan Information"
                if plan_info_path not in hdf_file:
                    raise ValueError(f"Plan Information not found in {hdf_path}")
                
                attrs = {}
                for key in hdf_file[plan_info_path].attrs.keys():
                    value = hdf_file[plan_info_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            raise ValueError(f"Failed to get plan information attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_parameters(hdf_path: Path) -> Dict:
        """
        Get plan parameter attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: A dictionary containing the plan parameter attributes.

        Raises:
            ValueError: If there's an error retrieving the plan parameter attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                plan_params_path = "Plan Data/Plan Parameters"
                if plan_params_path not in hdf_file:
                    raise ValueError(f"Plan Parameters not found in {hdf_path}")
                
                attrs = {}
                for key in hdf_file[plan_params_path].attrs.keys():
                    value = hdf_file[plan_params_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            raise ValueError(f"Failed to get plan parameter attributes: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_plan_met_precip(hdf_path: Path) -> Dict:
        """
        Get precipitation attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            Dict: Precipitation attributes including method, time series data,
                and spatial distribution if available. Returns empty dict if
                no precipitation data exists.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                precip_path = "Event Conditions/Meteorology/Precipitation"
                if precip_path not in hdf_file:
                    logger.error(f"Precipitation data not found in {hdf_path}")
                    return {}
                
                attrs = {}
                for key in hdf_file[precip_path].attrs.keys():
                    value = hdf_file[precip_path].attrs[key]
                    if isinstance(value, bytes):
                        value = HdfUtils.convert_ras_string(value)
                    attrs[key] = value
                
                return attrs
        except Exception as e:
            logger.error(f"Failed to get precipitation attributes: {str(e)}")
            return {}
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_geometry_information(hdf_path: Path) -> pd.DataFrame:
        """
        Get root level geometry attributes from the HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.

        Returns:
            pd.DataFrame: DataFrame with geometry attributes including Creation Date/Time,
                        Version, Units, and Projection information.

        Raises:
            ValueError: If Geometry group is missing or there's an error reading attributes.
        """
        print(f"Getting geometry attributes from {hdf_path}")
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                geom_attrs_path = "Geometry"
                print(f"Checking for Geometry group in {hdf_path}")
                if geom_attrs_path not in hdf_file:
                    raise ValueError(f"Geometry group not found in {hdf_path}")

                attrs = {}
                geom_group = hdf_file[geom_attrs_path]
                print("Getting root level geometry attributes")
                # Get root level geometry attributes only
                for key, value in geom_group.attrs.items():
                    if isinstance(value, bytes):
                        try:
                            value = HdfUtils.convert_ras_string(value)
                        except UnicodeDecodeError:
                            logger.warning(f"Failed to decode byte string for root attribute {key}")
                            continue
                    attrs[key] = value

                print("Successfully extracted root level geometry attributes")
                return pd.DataFrame.from_dict(attrs, orient='index', columns=['Value'])

        except (OSError, RuntimeError) as e:
            raise ValueError(f"Failed to read HDF file {hdf_path}: {str(e)}")
        except Exception as e:
            raise ValueError(f"Failed to get geometry attributes: {str(e)}")



==================================================

File: c:\GH\ras-commander\ras_commander\HdfPlot.py
==================================================
"""
Class: HdfPlot

A collection of static methods for plotting general HDF data from HEC-RAS models.
"""

import matplotlib.pyplot as plt
import pandas as pd
import geopandas as gpd
from typing import Optional, Union, Tuple
from .Decorators import log_call, standardize_input
from .HdfUtils import HdfUtils

class HdfPlot:
    """
    A class containing static methods for plotting general HDF data from HEC-RAS models.
    
    This class provides plotting functionality for HDF data, focusing on
    geometric elements like cell polygons and time series data.
    """

    @staticmethod
    @log_call
    def plot_mesh_cells(
        cell_polygons_df: pd.DataFrame, ## THIS IS A GEODATAFRAME - NEED TO EDIT BOTH ARGUMENT AND USAGE
        projection: str,
        title: str = '2D Flow Area Mesh Cells',
        figsize: Tuple[int, int] = (12, 8)
    ) -> Optional[gpd.GeoDataFrame]:
        """
        Plots the mesh cells from the provided DataFrame and returns the GeoDataFrame.

        Args:
            cell_polygons_df (pd.DataFrame): DataFrame containing cell polygons.
            projection (str): The coordinate reference system to assign to the GeoDataFrame.
            title (str, optional): Plot title. Defaults to '2D Flow Area Mesh Cells'.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 8).

        Returns:
            Optional[gpd.GeoDataFrame]: GeoDataFrame containing the mesh cells, or None if no cells found.
        """
        if cell_polygons_df.empty:
            print("No Cell Polygons found.")
            return None

        # Convert any datetime columns to strings using HdfUtils
        cell_polygons_df = HdfUtils.convert_df_datetimes_to_str(cell_polygons_df)
        
        cell_polygons_gdf = gpd.GeoDataFrame(cell_polygons_df, crs=projection)

        print("Cell Polygons CRS:", cell_polygons_gdf.crs)
        display(cell_polygons_gdf.head())

        fig, ax = plt.subplots(figsize=figsize)
        cell_polygons_gdf.plot(ax=ax, edgecolor='blue', facecolor='none')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()

        return cell_polygons_gdf

    @staticmethod
    @log_call
    def plot_time_series(
        df: pd.DataFrame,
        x_col: str,
        y_col: str,
        title: str = None,
        figsize: Tuple[int, int] = (12, 6)
    ) -> None:
        """
        Plots time series data from HDF results.

        Args:
            df (pd.DataFrame): DataFrame containing the time series data
            x_col (str): Name of the column containing x-axis data (usually time)
            y_col (str): Name of the column containing y-axis data
            title (str, optional): Plot title. Defaults to None.
            figsize (Tuple[int, int], optional): Figure size. Defaults to (12, 6).
        """
        # Convert any datetime columns to strings
        df = HdfUtils.convert_df_datetimes_to_str(df)
        
        fig, ax = plt.subplots(figsize=figsize)
        df.plot(x=x_col, y=y_col, ax=ax)
        
        if title:
            ax.set_title(title)
        ax.grid(True)
        plt.tight_layout()
        plt.show()
    
    
    
    
    
    
    
    
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\HdfPump.py
==================================================
"""
Class: HdfPump

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfPump:
- get_pump_stations()
- get_pump_groups()
- get_pump_station_timeseries()
- get_pump_station_summary()
- get_pump_operation_timeseries()


"""


import h5py
import numpy as np
import pandas as pd
import geopandas as gpd
import xarray as xr
from pathlib import Path
from shapely.geometry import Point
from typing import List, Dict, Any, Optional, Union
from .HdfUtils import HdfUtils
from .HdfBase import HdfBase
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfPump:
    """
    A class for handling pump station related data from HEC-RAS HDF files.

    This class provides static methods to extract and process pump station data, including:
    - Pump station locations and attributes
    - Pump group configurations and efficiency curves
    - Time series results for pump operations
    - Summary statistics for pump stations

    All methods are static and designed to work with HEC-RAS HDF files containing pump data.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_stations(hdf_path: Path) -> gpd.GeoDataFrame:
        """
        Extract pump station data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing pump station data with columns:
                - geometry: Point geometry of pump station location
                - station_id: Unique identifier for each pump station
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump station datasets are not found in the HDF file.
            Exception: If there are errors processing the pump station data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump station data
                attributes = hdf['/Geometry/Pump Stations/Attributes'][()]
                points = hdf['/Geometry/Pump Stations/Points'][()]

                # Create geometries
                geometries = [Point(x, y) for x, y in points]

                # Create GeoDataFrame
                gdf = gpd.GeoDataFrame(geometry=geometries)
                gdf['station_id'] = range(len(gdf))

                # Add attributes and decode byte strings
                attr_df = pd.DataFrame(attributes)
                string_columns = attr_df.select_dtypes([object]).columns
                for col in string_columns:
                    attr_df[col] = attr_df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                
                for col in attr_df.columns:
                    gdf[col] = attr_df[col]

                # Set CRS if available
                crs = HdfBase.get_projection(hdf_path)
                if crs:
                    gdf.set_crs(crs, inplace=True)

                return gdf

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_groups(hdf_path: Path) -> pd.DataFrame:
        """
        Extract pump group data from the HDF file.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump group data with columns:
                - efficiency_curve_start: Starting index of efficiency curve data
                - efficiency_curve_count: Number of points in efficiency curve
                - efficiency_curve: List of efficiency curve values
                - Additional attributes from the HDF file

        Raises:
            KeyError: If pump group datasets are not found in the HDF file.
            Exception: If there are errors processing the pump group data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract pump group data
                attributes = hdf['/Geometry/Pump Stations/Pump Groups/Attributes'][()]
                efficiency_curves_info = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Info'][()]
                efficiency_curves_values = hdf['/Geometry/Pump Stations/Pump Groups/Efficiency Curves Values'][()]

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(attributes)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                # Add efficiency curve data
                df['efficiency_curve_start'] = efficiency_curves_info[:, 0]
                df['efficiency_curve_count'] = efficiency_curves_info[:, 1]

                # Process efficiency curves
                def get_efficiency_curve(start, count):
                    return efficiency_curves_values[start:start+count].tolist()

                df['efficiency_curve'] = df.apply(lambda row: get_efficiency_curve(row['efficiency_curve_start'], row['efficiency_curve_count']), axis=1)

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump group data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_station_timeseries(hdf_path: Path, pump_station: str) -> xr.DataArray:
        """
        Extract timeseries results data for a specific pump station.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.
            pump_station (str): Name or identifier of the pump station.

        Returns:
            xr.DataArray: DataArray containing the timeseries data with dimensions:
                - time: Timestamps of simulation
                - variable: Variables including ['Flow', 'Stage HW', 'Stage TW', 
                           'Pump Station', 'Pumps on']
            Attributes include units and pump station name.

        Raises:
            KeyError: If required datasets are not found in the HDF file.
            ValueError: If the specified pump station name is not found.
            Exception: If there are errors processing the timeseries data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Check if the pump station exists
                pumping_stations_path = "/Results/Unsteady/Output/Output Blocks/DSS Hydrograph Output/Unsteady Time Series/Pumping Stations"
                if pump_station not in hdf[pumping_stations_path]:
                    raise ValueError(f"Pump station '{pump_station}' not found in HDF file")

                # Extract timeseries data
                data_path = f"{pumping_stations_path}/{pump_station}/Structure Variables"
                data = hdf[data_path][()]

                # Extract time information - Updated to use new method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataArray
                da = xr.DataArray(
                    data=data,
                    dims=['time', 'variable'],
                    coords={'time': time, 'variable': ['Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on']},
                    name=pump_station
                )

                # Add attributes and decode byte strings
                units = hdf[data_path].attrs.get('Variable_Unit', b'')
                da.attrs['units'] = units.decode('utf-8') if isinstance(units, bytes) else units
                da.attrs['pump_station'] = pump_station

                return da

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except ValueError as e:
            logger.error(str(e))
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station timeseries data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_station_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Extract summary statistics and performance data for all pump stations.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.

        Returns:
            pd.DataFrame: DataFrame containing pump station summary data including
                operational statistics and performance metrics. Returns empty DataFrame
                if no summary data is found.

        Raises:
            KeyError: If the summary dataset is not found in the HDF file.
            Exception: If there are errors processing the summary data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract summary data
                summary_path = "/Results/Unsteady/Summary/Pump Station"
                if summary_path not in hdf:
                    logger.warning("Pump Station summary data not found in HDF file")
                    return pd.DataFrame()

                summary_data = hdf[summary_path][()]
                
                # Create DataFrame and decode byte strings
                df = pd.DataFrame(summary_data)
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting pump station summary data: {e}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_pump_operation_timeseries(hdf_path: Path, pump_station: str) -> pd.DataFrame:
        """
        Extract detailed pump operation results data for a specific pump station.

        Args:
            hdf_path (Path): Path to the HEC-RAS HDF file.
            pump_station (str): Name or identifier of the pump station.

        Returns:
            pd.DataFrame: DataFrame containing pump operation data with columns:
                - Time: Simulation timestamps
                - Flow: Pump flow rate
                - Stage HW: Headwater stage
                - Stage TW: Tailwater stage
                - Pump Station: Station identifier
                - Pumps on: Number of active pumps

        Raises:
            KeyError: If required datasets are not found in the HDF file.
            ValueError: If the specified pump station name is not found.
            Exception: If there are errors processing the operation data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Check if the pump station exists
                pump_stations_path = "/Results/Unsteady/Output/Output Blocks/DSS Profile Output/Unsteady Time Series/Pumping Stations"
                if pump_station not in hdf[pump_stations_path]:
                    raise ValueError(f"Pump station '{pump_station}' not found in HDF file")

                # Extract pump operation data
                data_path = f"{pump_stations_path}/{pump_station}/Structure Variables"
                data = hdf[data_path][()]

                # Extract time information - Updated to use new method name
                time = HdfBase.get_unsteady_timestamps(hdf)

                # Create DataFrame and decode byte strings
                df = pd.DataFrame(data, columns=['Flow', 'Stage HW', 'Stage TW', 'Pump Station', 'Pumps on'])
                string_columns = df.select_dtypes([object]).columns
                for col in string_columns:
                    df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)
                    
                df['Time'] = time

                return df

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except ValueError as e:
            logger.error(str(e))
            raise
        except Exception as e:
            logger.error(f"Error extracting pump operation data: {e}")
            raise
==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsMesh.py
==================================================
"""
Class: HdfResultsMesh

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All methods in this class are static and designed to be used without instantiation.

Public Functions:
- get_mesh_summary(): Get summary output data for a variable 
- get_mesh_timeseries(): Get timeseries output for a mesh and variable  
- get_mesh_faces_timeseries(): Get timeseries for all face-based variables
- get_mesh_cells_timeseries(): Get timeseries for mesh cells
- get_mesh_last_iter(): Get last iteration count for cells
- get_mesh_max_ws(): Get maximum water surface elevation at each cell   
- get_mesh_min_ws(): Get minimum water surface elevation at each cell
- get_mesh_max_face_v(): Get maximum face velocity at each face
- get_mesh_min_face_v(): Get minimum face velocity at each face
- get_mesh_max_ws_err(): Get maximum water surface error at each cell
- get_mesh_max_iter(): Get maximum iteration count at each cell

Private Functions:
- _get_mesh_timeseries_output_path(): Get HDF path for timeseries output  #REDUNDANT??
- _get_mesh_cells_timeseries_output(): Internal handler for cell timeseries   #REDUNDANT??
- _get_mesh_timeseries_output(): Internal handler for mesh timeseries       # FACES?? 
- _get_mesh_timeseries_output_values_units(): Get values and units for timeseries
- _get_available_meshes(): Get list of available meshes in HDF            #USE HDFBASE OR HDFUTIL
- get_mesh_summary_output(): Internal handler for summary output        
- get_mesh_summary_output_group(): Get HDF group for summary output         #REDUNDANT??  Include in Above

The class works with HEC-RAS version 6.0+ plan HDF files and uses HdfBase and 
HdfUtils for common operations. Methods use @log_call decorator for logging and 
@standardize_input decorator to handle different input types.






REVISIONS MADE:

Use get_ prefix for functions that return data.  
BUT, we will never set results data, so we should use get_ for results data.

Renamed functions:
- mesh_summary_output() to get_mesh_summary()
- mesh_timeseries_output() to get_mesh_timeseries()
- mesh_faces_timeseries_output() to get_mesh_faces_timeseries()
- mesh_cells_timeseries_output() to get_mesh_cells_timeseries()
- mesh_last_iter() to get_mesh_last_iter()
- mesh_max_ws() to get_mesh_max_ws()







"""

import numpy as np
import pandas as pd
import xarray as xr
from pathlib import Path
import h5py
from typing import Union, List, Optional, Dict, Any, Tuple
from .HdfMesh import HdfMesh
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import log_call, standardize_input
from .LoggingConfig import setup_logging, get_logger
import geopandas as gpd

logger = get_logger(__name__)

class HdfResultsMesh:
    """
    Handles mesh-related results from HEC-RAS HDF files.

    Provides methods to extract and analyze:
    - Mesh summary outputs
    - Timeseries data
    - Water surface elevations
    - Velocities
    - Error metrics

    Works with HEC-RAS 6.0+ plan HDF files.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary(hdf_path: Path, var: str, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, var, round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_summary: {str(e)}")
            logger.error(f"Variable: {var}")
            raise ValueError(f"Failed to get summary output: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_timeseries(hdf_path: Path, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_path (Path): Path to the HDF file
            mesh_name (str): Name of the mesh
            var (str): Variable to retrieve (see valid options below)
            truncate (bool): Whether to truncate trailing zeros (default True)

        Returns:
            xr.DataArray: DataArray with dimensions:
                - time: Timestamps
                - face_id/cell_id: IDs for faces/cells
                And attributes:
                - units: Variable units
                - mesh_name: Name of mesh
                - variable: Variable name

        Valid variables include:
            "Water Surface", "Face Velocity", "Cell Velocity X"...
        """
        with h5py.File(hdf_path, 'r') as hdf_file:
            return HdfResultsMesh._get_mesh_timeseries_output(hdf_file, mesh_name, var, truncate)

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_faces_timeseries(hdf_path: Path, mesh_name: str) -> xr.Dataset:
        """
        Get timeseries output for all face-based variables of a specific mesh.

        Args:
            hdf_path (Path): Path to the HDF file.
            mesh_name (str): Name of the mesh.

        Returns:
            xr.Dataset: Dataset containing the timeseries output for all face-based variables.
        """
        face_vars = ["Face Velocity", "Face Flow"]
        datasets = []
        
        for var in face_vars:
            try:
                da = HdfResultsMesh.get_mesh_timeseries(hdf_path, mesh_name, var)
                # Assign the variable name as the DataArray name
                da.name = var.lower().replace(' ', '_')
                datasets.append(da)
            except Exception as e:
                logger.warning(f"Failed to process {var} for mesh {mesh_name}: {str(e)}")
        
        if not datasets:
            logger.error(f"No valid data found for mesh {mesh_name}")
            return xr.Dataset()
        
        try:
            return xr.merge(datasets)
        except Exception as e:
            logger.error(f"Failed to merge datasets: {str(e)}")
            return xr.Dataset()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_cells_timeseries(hdf_path: Path, mesh_names: Optional[Union[str, List[str]]] = None, var: Optional[str] = None, truncate: bool = False, ras_object: Optional[Any] = None) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output.

        Args:
            hdf_path (Path): Path to HDF file
            mesh_names (str|List[str], optional): Mesh name(s). If None, processes all meshes
            var (str, optional): Variable name. If None, retrieves all variables
            truncate (bool): Remove trailing zeros if True
            ras_object (Any, optional): RAS object if available

        Returns:
            Dict[str, xr.Dataset]: Dictionary mapping mesh names to datasets containing:
                - Time-indexed variables
                - Cell/face IDs
                - Variable metadata
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh._get_mesh_cells_timeseries_output(hdf_file, mesh_names, var, truncate)
        except Exception as e:
            logger.error(f"Error in get_mesh_cells_timeseries: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_last_iter(hdf_path: Path) -> pd.DataFrame:
        """
        Get last iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.

        Returns:
            pd.DataFrame: DataFrame containing last iteration counts.
        """
        return HdfResultsMesh.get_mesh_summary_output(hdf_path, "Cell Last Iteration")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get maximum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing maximum water surface elevations with geometry.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface: {str(e)}")
        




    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_ws(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get minimum water surface elevation for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing minimum water surface elevations with geometry.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Water Surface", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_ws: {str(e)}")
            raise ValueError(f"Failed to get minimum water surface: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum face velocity for each mesh face.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum face velocities.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Maximum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_face_v: {str(e)}")
            raise ValueError(f"Failed to get maximum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_min_face_v(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get minimum face velocity for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing minimum face velocities.

        Raises:
            ValueError: If there's an error processing the minimum face velocity data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Minimum Face Velocity", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_min_face_v: {str(e)}")
            raise ValueError(f"Failed to get minimum face velocity: {str(e)}")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_ws_err(hdf_path: Path, round_to: str = "100ms") -> pd.DataFrame:
        """
        Get maximum water surface error for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            pd.DataFrame: DataFrame containing maximum water surface errors.

        Raises:
            ValueError: If there's an error processing the maximum water surface error data.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Maximum Water Surface Error", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_ws_err: {str(e)}")
            raise ValueError(f"Failed to get maximum water surface error: {str(e)}")


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_max_iter(hdf_path: Path, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get maximum iteration count for each mesh cell.

        Args:
            hdf_path (Path): Path to the HDF file.
            round_to (str): Time rounding specification (default "100ms").

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing maximum iteration counts with geometry.
                Includes columns:
                - mesh_name: Name of the mesh
                - cell_id: ID of the cell
                - cell_last_iteration: Maximum number of iterations
                - cell_last_iteration_time: Time when max iterations occurred
                - geometry: Point geometry representing cell center
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                return HdfResultsMesh.get_mesh_summary_output(hdf_file, "Cell Last Iteration", round_to)
        except Exception as e:
            logger.error(f"Error in get_mesh_max_iter: {str(e)}")
            raise ValueError(f"Failed to get maximum iteration count: {str(e)}")
        
        


    @staticmethod
    def _get_mesh_timeseries_output_path(mesh_name: str, var_name: str) -> str:
        """
        Get the HDF path for mesh timeseries output.

        Args:
            mesh_name (str): Name of the mesh.
            var_name (str): Name of the variable.

        Returns:
            str: The HDF path for the specified mesh and variable.
        """
        return f"Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas/{mesh_name}/{var_name}"


    @staticmethod
    def _get_mesh_cells_timeseries_output(hdf_file: h5py.File, 
                                         mesh_names: Optional[Union[str, List[str]]] = None,
                                         var: Optional[str] = None, 
                                         truncate: bool = False) -> Dict[str, xr.Dataset]:
        """
        Get mesh cells timeseries output for specified meshes and variables.
        
        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_names (Optional[Union[str, List[str]]]): Name(s) of the mesh(es). If None, processes all available meshes.
            var (Optional[str]): Name of the variable to retrieve. If None, retrieves all variables.
            truncate (bool): If True, truncates the output to remove trailing zeros.

        Returns:
            Dict[str, xr.Dataset]: A dictionary of xarray Datasets, one for each mesh, containing the mesh cells timeseries output.

        Raises:
            ValueError: If there's an error processing the timeseries output data.
        """
        TIME_SERIES_OUTPUT_VARS = {
            "cell": [
                "Water Surface", "Depth", "Velocity", "Velocity X", "Velocity Y",
                "Froude Number", "Courant Number", "Shear Stress", "Bed Elevation",
                "Precipitation Rate", "Infiltration Rate", "Evaporation Rate",
                "Percolation Rate", "Groundwater Elevation", "Groundwater Depth",
                "Groundwater Flow", "Groundwater Velocity", "Groundwater Velocity X",
                "Groundwater Velocity Y"
            ],
            "face": [
                "Face Velocity", "Face Flow", "Face Water Surface", "Face Courant",
                "Face Cumulative Volume", "Face Eddy Viscosity", "Face Flow Period Average",
                "Face Friction Term", "Face Pressure Gradient Term", "Face Shear Stress",
                "Face Tangential Velocity"
            ]
        }

        try:
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            time_stamps = HdfBase.get_unsteady_timestamps(hdf_file)

            if mesh_names is None:
                mesh_names = HdfResultsMesh._get_available_meshes(hdf_file)
            elif isinstance(mesh_names, str):
                mesh_names = [mesh_names]

            if var:
                variables = [var]
            else:
                variables = TIME_SERIES_OUTPUT_VARS["cell"] + TIME_SERIES_OUTPUT_VARS["face"]

            datasets = {}
            for mesh_name in mesh_names:
                data_vars = {}
                for variable in variables:
                    try:
                        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, variable)
                        dataset = hdf_file[path]
                        values = dataset[:]
                        units = dataset.attrs.get("Units", "").decode("utf-8")

                        if truncate:
                            last_nonzero = np.max(np.nonzero(values)[1]) + 1 if values.size > 0 else 0
                            values = values[:, :last_nonzero]
                            truncated_time_stamps = time_stamps[:last_nonzero]
                        else:
                            truncated_time_stamps = time_stamps

                        if values.shape[0] != len(truncated_time_stamps):
                            logger.warning(f"Mismatch between time steps ({len(truncated_time_stamps)}) and data shape ({values.shape}) for variable {variable}")
                            continue

                        # Determine if this is a face-based or cell-based variable
                        id_dim = "face_id" if any(face_var in variable for face_var in TIME_SERIES_OUTPUT_VARS["face"]) else "cell_id"

                        data_vars[variable] = xr.DataArray(
                            data=values,
                            dims=['time', id_dim],
                            coords={'time': truncated_time_stamps, id_dim: np.arange(values.shape[1])},
                            attrs={'units': units}
                        )
                    except KeyError:
                        logger.warning(f"Variable '{variable}' not found in the HDF file for mesh '{mesh_name}'. Skipping.")
                    except Exception as e:
                        logger.error(f"Error processing variable '{variable}' for mesh '{mesh_name}': {str(e)}")

                if data_vars:
                    datasets[mesh_name] = xr.Dataset(
                        data_vars=data_vars,
                        attrs={'mesh_name': mesh_name, 'start_time': start_time}
                    )
                else:
                    logger.warning(f"No valid data variables found for mesh '{mesh_name}'")

            return datasets
        except Exception as e:
            logger.error(f"Error in _mesh_cells_timeseries_output: {str(e)}")
            raise ValueError(f"Error processing timeseries output data: {e}")



    @staticmethod
    def _get_mesh_timeseries_output(hdf_file: h5py.File, mesh_name: str, var: str, truncate: bool = True) -> xr.DataArray:
        """
        Get timeseries output for a specific mesh and variable.

        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.
            truncate (bool): Whether to truncate the output to remove trailing zeros (default True).

        Returns:
            xr.DataArray: DataArray containing the timeseries output.

        Raises:
            ValueError: If the specified path is not found in the HDF file or if there's an error processing the data.
        """
        try:
            path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
            
            if path not in hdf_file:
                raise ValueError(f"Path {path} not found in HDF file")

            dataset = hdf_file[path]
            values = dataset[:]
            units = dataset.attrs.get("Units", "").decode("utf-8")
            
            # Get start time and timesteps
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            # Updated to use the new function name from HdfUtils
            timesteps = HdfUtils.convert_timesteps_to_datetimes(
                np.array(hdf_file["Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time"][:]),
                start_time
            )

            if truncate:
                non_zero = np.nonzero(values)[0]
                if len(non_zero) > 0:
                    start, end = non_zero[0], non_zero[-1] + 1
                    values = values[start:end]
                    timesteps = timesteps[start:end]

            # Determine if this is a face-based or cell-based variable
            id_dim = "face_id" if "Face" in var else "cell_id"
            dims = ["time", id_dim] if values.ndim == 2 else ["time"]
            coords = {"time": timesteps}
            if values.ndim == 2:
                coords[id_dim] = np.arange(values.shape[1])

            return xr.DataArray(
                values,
                coords=coords,
                dims=dims,
                attrs={"units": units, "mesh_name": mesh_name, "variable": var},
            )
        except Exception as e:
            logger.error(f"Error in get_mesh_timeseries_output: {str(e)}")
            raise ValueError(f"Failed to get timeseries output: {str(e)}")


    @staticmethod
    def _get_mesh_timeseries_output_values_units(hdf_file: h5py.File, mesh_name: str, var: str) -> Tuple[np.ndarray, str]:
        """
        Get the mesh timeseries output values and units for a specific variable from the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Variable name to retrieve.

        Returns:
            Tuple[np.ndarray, str]: A tuple containing the output values and units.
        """
        path = HdfResultsMesh._get_mesh_timeseries_output_path(mesh_name, var)
        group = hdf_file[path]
        values = group[:]
        units = group.attrs.get("Units")
        if units is not None:
            units = units.decode("utf-8")
        return values, units


    @staticmethod
    def _get_available_meshes(hdf_file: h5py.File) -> List[str]:
        """
        Get the names of all available meshes in the HDF file.

        Args:
            hdf_file (h5py.File): Open HDF file object.

        Returns:
            List[str]: A list of mesh names.
        """
        return HdfMesh.get_mesh_area_names(hdf_file)
    
    
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_mesh_summary_output(hdf_file: h5py.File, var: str, round_to: str = "100ms") -> gpd.GeoDataFrame:
        """
        Get the summary output data for a given variable from the HDF file.

        Parameters
        ----------
        hdf_file : h5py.File
            Open HDF file object.
        var : str
            The summary output variable to retrieve.
        round_to : str, optional
            The time unit to round the datetimes to. Default is "100ms".

        Returns
        -------
        gpd.GeoDataFrame
            A GeoDataFrame containing the summary output data with attributes as metadata.

        Raises
        ------
        ValueError
            If the HDF file cannot be opened or read, or if the requested data is not found.
        """
        try:
            dfs = []
            start_time = HdfBase.get_simulation_start_time(hdf_file)
            
            logger.info(f"Processing summary output for variable: {var}")
            d2_flow_areas = hdf_file.get("Geometry/2D Flow Areas/Attributes")
            if d2_flow_areas is None:
                return gpd.GeoDataFrame()

            for d2_flow_area in d2_flow_areas[:]:
                mesh_name = HdfUtils.convert_ras_string(d2_flow_area[0])
                cell_count = d2_flow_area[-1]
                logger.debug(f"Processing mesh: {mesh_name} with {cell_count} cells")
                group = HdfResultsMesh.get_mesh_summary_output_group(hdf_file, mesh_name, var)
                
                data = group[:]
                logger.debug(f"Data shape for {var} in {mesh_name}: {data.shape}")
                logger.debug(f"Data type: {data.dtype}")
                logger.debug(f"Attributes: {dict(group.attrs)}")
                
                if data.ndim == 2 and data.shape[0] == 2:
                    # Handle 2D datasets (e.g. Maximum Water Surface)
                    row_variables = group.attrs.get('Row Variables', [b'Value', b'Time'])
                    row_variables = [v.decode('utf-8').strip() for v in row_variables]
                    
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * data.shape[1],
                        "cell_id" if "Face" not in var else "face_id": range(data.shape[1]),
                        f"{var.lower().replace(' ', '_')}": data[0, :],
                        f"{var.lower().replace(' ', '_')}_time": HdfUtils.convert_timesteps_to_datetimes(
                            data[1, :], start_time, time_unit="days", round_to=round_to
                        )
                    })
                    
                elif data.ndim == 1:
                    # Handle 1D datasets (e.g. Cell Last Iteration)
                    df = pd.DataFrame({
                        "mesh_name": [mesh_name] * len(data),
                        "cell_id" if "Face" not in var else "face_id": range(len(data)),
                        var.lower().replace(' ', '_'): data
                    })
                    
                else:
                    raise ValueError(f"Unexpected data shape for {var} in {mesh_name}. "
                                  f"Got shape {data.shape}")
                
                # Add geometry based on variable type
                if "Face" in var:
                    face_df = HdfMesh.get_mesh_cell_faces(hdf_file)
                    if not face_df.empty:
                        df = df.merge(face_df[['mesh_name', 'face_id', 'geometry']], 
                                    on=['mesh_name', 'face_id'], 
                                    how='left')
                else:
                    cell_df = HdfMesh.get_mesh_cell_points(hdf_file)
                    if not cell_df.empty:
                        df = df.merge(cell_df[['mesh_name', 'cell_id', 'geometry']], 
                                    on=['mesh_name', 'cell_id'], 
                                    how='left')
                
                # Add group attributes as metadata
                df.attrs['mesh_name'] = mesh_name
                for attr_name, attr_value in group.attrs.items():
                    if isinstance(attr_value, bytes):
                        attr_value = attr_value.decode('utf-8')
                    elif isinstance(attr_value, np.ndarray):
                        attr_value = attr_value.tolist()
                    df.attrs[attr_name] = attr_value
                
                dfs.append(df)
            
            if not dfs:
                return gpd.GeoDataFrame()
                
            result = pd.concat(dfs, ignore_index=True)
            
            # Convert to GeoDataFrame
            gdf = gpd.GeoDataFrame(result, geometry='geometry')
            
            # Get CRS from HdfUtils
            crs = HdfBase.get_projection(hdf_file)
            if crs:
                gdf.set_crs(crs, inplace=True)
            
            # Combine attributes from all meshes
            combined_attrs = {}
            for df in dfs:
                for key, value in df.attrs.items():
                    if key not in combined_attrs:
                        combined_attrs[key] = value
                    elif combined_attrs[key] != value:
                        combined_attrs[key] = f"Multiple values: {combined_attrs[key]}, {value}"
            
            gdf.attrs.update(combined_attrs)
            
            logger.info(f"Processed {len(gdf)} rows of summary output data")
            return gdf
        
        except Exception as e:
            logger.error(f"Error processing summary output data: {e}")
            raise ValueError(f"Error processing summary output data: {e}")

    @staticmethod
    def get_mesh_summary_output_group(hdf_file: h5py.File, mesh_name: str, var: str) -> Union[h5py.Group, h5py.Dataset]:
        """
        Return the HDF group for a given mesh and summary output variable.

        Args:
            hdf_file (h5py.File): Open HDF file object.
            mesh_name (str): Name of the mesh.
            var (str): Name of the summary output variable.

        Returns:
            Union[h5py.Group, h5py.Dataset]: The HDF group or dataset for the specified mesh and variable.

        Raises:
            ValueError: If the specified group or dataset is not found in the HDF file.
        """
        output_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/{var}"
        output_item = hdf_file.get(output_path)
        if output_item is None:
            raise ValueError(f"Could not find HDF group or dataset at path '{output_path}'")
        return output_item


==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsPlan.py
==================================================
"""
HdfResultsPlan: A module for extracting and analyzing HEC-RAS plan HDF file results.

Attribution:
    Substantial code sourced/derived from https://github.com/fema-ffrd/rashdf
    Copyright (c) 2024 fema-ffrd, MIT license

Description:
    Provides static methods for extracting unsteady flow results, volume accounting,
    and reference data from HEC-RAS plan HDF files.

Available Functions:
    - get_unsteady_info: Extract unsteady attributes
    - get_unsteady_summary: Extract unsteady summary data
    - get_volume_accounting: Extract volume accounting data
    - get_runtime_data: Extract runtime and compute time data

Note:
    All methods are static and designed to be used without class instantiation.
"""

from typing import Dict, List, Union, Optional
from pathlib import Path
import h5py
import pandas as pd
import xarray as xr
from .Decorators import standardize_input, log_call
from .HdfUtils import HdfUtils
from .HdfResultsXsec import HdfResultsXsec
from .LoggingConfig import get_logger
import numpy as np
from datetime import datetime
from .RasPrj import ras

logger = get_logger(__name__)


class HdfResultsPlan:
    """
    Handles extraction of results data from HEC-RAS plan HDF files.

    This class provides static methods for accessing and analyzing:
        - Unsteady flow results
        - Volume accounting data
        - Runtime statistics
        - Reference line/point time series outputs

    All methods use:
        - @standardize_input decorator for consistent file path handling
        - @log_call decorator for operation logging
        - HdfUtils class for common HDF operations

    Note:
        No instantiation required - all methods are static.
    """

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_info(hdf_path: Path) -> pd.DataFrame:
        """
        Get unsteady attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: A DataFrame containing the unsteady attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady" group is not found in the HDF file.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady" not in hdf_file:
                    raise KeyError("Results/Unsteady group not found in the HDF file.")
                
                # Create dictionary from attributes
                attrs_dict = dict(hdf_file["Results/Unsteady"].attrs)
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_unsteady_summary(hdf_path: Path) -> pd.DataFrame:
        """
        Get results unsteady summary attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: A DataFrame containing the results unsteady summary attributes.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
            KeyError: If the "Results/Unsteady/Summary" group is not found in the HDF file.
        """
        try:           
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary" not in hdf_file:
                    raise KeyError("Results/Unsteady/Summary group not found in the HDF file.")
                
                # Create dictionary from attributes
                attrs_dict = dict(hdf_file["Results/Unsteady/Summary"].attrs)
                
                # Create DataFrame with a single row index
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading unsteady summary attributes: {str(e)}")
        
    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_volume_accounting(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Get volume accounting attributes from a HEC-RAS HDF plan file.

        Args:
            hdf_path (Path): Path to the HEC-RAS plan HDF file.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[pd.DataFrame]: DataFrame containing the volume accounting attributes,
                                  or None if the group is not found.

        Raises:
            FileNotFoundError: If the specified HDF file is not found.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Results/Unsteady/Summary/Volume Accounting" not in hdf_file:
                    return None
                
                # Get attributes and convert to DataFrame
                attrs_dict = dict(hdf_file["Results/Unsteady/Summary/Volume Accounting"].attrs)
                return pd.DataFrame(attrs_dict, index=[0])
                
        except FileNotFoundError:
            raise FileNotFoundError(f"HDF file not found: {hdf_path}")
        except Exception as e:
            raise RuntimeError(f"Error reading volume accounting attributes: {str(e)}")

    @staticmethod
    @standardize_input(file_type='plan_hdf')
    def get_runtime_data(hdf_path: Path) -> Optional[pd.DataFrame]:
        """
        Extract detailed runtime and computational performance metrics from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[pd.DataFrame]: DataFrame containing runtime statistics or None if data cannot be extracted

        Notes:
            - Times are reported in multiple units (ms, s, hours)
            - Compute speeds are calculated as simulation-time/compute-time ratios
            - Process times include: geometry, preprocessing, event conditions, 
              and unsteady flow computations
        """
        try:
            if hdf_path is None:
                logger.error(f"Could not find HDF file for input")
                return None

            with h5py.File(hdf_path, 'r') as hdf_file:
                logger.info(f"Extracting Plan Information from: {Path(hdf_file.filename).name}")
                plan_info = hdf_file.get('/Plan Data/Plan Information')
                if plan_info is None:
                    logger.warning("Group '/Plan Data/Plan Information' not found.")
                    return None

                # Extract plan information
                plan_name = HdfUtils.convert_ras_string(plan_info.attrs.get('Plan Name', 'Unknown'))
                start_time_str = HdfUtils.convert_ras_string(plan_info.attrs.get('Simulation Start Time', 'Unknown'))
                end_time_str = HdfUtils.convert_ras_string(plan_info.attrs.get('Simulation End Time', 'Unknown'))

                try:
                    # Check if times are already datetime objects
                    if isinstance(start_time_str, datetime):
                        start_time = start_time_str
                    else:
                        start_time = datetime.strptime(start_time_str, "%d%b%Y %H:%M:%S")
                        
                    if isinstance(end_time_str, datetime):
                        end_time = end_time_str
                    else:
                        end_time = datetime.strptime(end_time_str, "%d%b%Y %H:%M:%S")
                        
                    simulation_duration = end_time - start_time
                    simulation_hours = simulation_duration.total_seconds() / 3600
                except ValueError as e:
                    logger.error(f"Error parsing simulation times: {e}")
                    return None

                logger.info(f"Plan Name: {plan_name}")
                logger.info(f"Simulation Duration (hours): {simulation_hours}")

                # Extract compute processes data
                compute_processes = hdf_file.get('/Results/Summary/Compute Processes')
                if compute_processes is None:
                    logger.warning("Dataset '/Results/Summary/Compute Processes' not found.")
                    return None

                # Process compute times
                process_names = [HdfUtils.convert_ras_string(name) for name in compute_processes['Process'][:]]
                filenames = [HdfUtils.convert_ras_string(filename) for filename in compute_processes['Filename'][:]]
                completion_times = compute_processes['Compute Time (ms)'][:]

                compute_processes_df = pd.DataFrame({
                    'Process': process_names,
                    'Filename': filenames,
                    'Compute Time (ms)': completion_times,
                    'Compute Time (s)': completion_times / 1000,
                    'Compute Time (hours)': completion_times / (1000 * 3600)
                })

                # Create summary DataFrame
                compute_processes_summary = {
                    'Plan Name': [plan_name],
                    'File Name': [Path(hdf_file.filename).name],
                    'Simulation Start Time': [start_time_str],
                    'Simulation End Time': [end_time_str],
                    'Simulation Duration (s)': [simulation_duration.total_seconds()],
                    'Simulation Time (hr)': [simulation_hours]
                }

                # Add process-specific times
                process_types = {
                    'Completing Geometry': 'Completing Geometry (hr)',
                    'Preprocessing Geometry': 'Preprocessing Geometry (hr)',
                    'Completing Event Conditions': 'Completing Event Conditions (hr)',
                    'Unsteady Flow Computations': 'Unsteady Flow Computations (hr)'
                }

                for process, column in process_types.items():
                    time_value = compute_processes_df[
                        compute_processes_df['Process'] == process
                    ]['Compute Time (hours)'].values[0] if process in process_names else 'N/A'
                    compute_processes_summary[column] = [time_value]

                # Add total process time
                total_time = compute_processes_df['Compute Time (hours)'].sum()
                compute_processes_summary['Complete Process (hr)'] = [total_time]

                # Calculate speeds
                if compute_processes_summary['Unsteady Flow Computations (hr)'][0] != 'N/A':
                    compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = [
                        simulation_hours / compute_processes_summary['Unsteady Flow Computations (hr)'][0]
                    ]
                else:
                    compute_processes_summary['Unsteady Flow Speed (hr/hr)'] = ['N/A']

                compute_processes_summary['Complete Process Speed (hr/hr)'] = [
                    simulation_hours / total_time
                ]

                return pd.DataFrame(compute_processes_summary)

        except Exception as e:
            logger.error(f"Error in get_runtime_data: {str(e)}")
            return None

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_reference_timeseries(hdf_path: Path, reftype: str) -> pd.DataFrame:
        """
        Get reference line or point timeseries output from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            reftype (str): Type of reference data ('lines' or 'points')
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing reference timeseries data
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series"
                ref_path = f"{base_path}/Reference {reftype.capitalize()}"
                
                if ref_path not in hdf_file:
                    logger.warning(f"Reference {reftype} data not found in HDF file")
                    return pd.DataFrame()

                ref_group = hdf_file[ref_path]
                time_data = hdf_file[f"{base_path}/Time"][:]
                
                dfs = []
                for ref_name in ref_group.keys():
                    ref_data = ref_group[ref_name][:]
                    df = pd.DataFrame(ref_data, columns=[ref_name])
                    df['Time'] = time_data
                    dfs.append(df)

                if not dfs:
                    return pd.DataFrame()

                return pd.concat(dfs, axis=1)

        except Exception as e:
            logger.error(f"Error reading reference {reftype} timeseries: {str(e)}")
            return pd.DataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_reference_summary(hdf_path: Path, reftype: str) -> pd.DataFrame:
        """
        Get reference line or point summary output from HDF file.

        Args:
            hdf_path (Path): Path to HEC-RAS plan HDF file
            reftype (str): Type of reference data ('lines' or 'points')
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            pd.DataFrame: DataFrame containing reference summary data
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                base_path = "Results/Unsteady/Output/Output Blocks/Base Output/Summary Output"
                ref_path = f"{base_path}/Reference {reftype.capitalize()}"
                
                if ref_path not in hdf_file:
                    logger.warning(f"Reference {reftype} summary data not found in HDF file")
                    return pd.DataFrame()

                ref_group = hdf_file[ref_path]
                dfs = []
                
                for ref_name in ref_group.keys():
                    ref_data = ref_group[ref_name][:]
                    if ref_data.ndim == 2:
                        df = pd.DataFrame(ref_data.T, columns=['Value', 'Time'])
                    else:
                        df = pd.DataFrame({'Value': ref_data})
                    df['Reference'] = ref_name
                    dfs.append(df)

                if not dfs:
                    return pd.DataFrame()

                return pd.concat(dfs, ignore_index=True)

        except Exception as e:
            logger.error(f"Error reading reference {reftype} summary: {str(e)}")
            return pd.DataFrame()
==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsPlot.py
==================================================
"""
Class: HdfResultsPlot

A collection of static methods for visualizing HEC-RAS results data from HDF files using matplotlib.

Public Functions:
    plot_results_mesh_variable(variable_df, variable_name, colormap='viridis', point_size=10):
        Generic plotting function for any mesh variable with customizable styling.
        
    plot_results_max_wsel(max_ws_df):
        Visualizes the maximum water surface elevation distribution across mesh cells.
        
    plot_results_max_wsel_time(max_ws_df):
        Displays the timing of maximum water surface elevation for each cell,
        including statistics about the temporal distribution.

Requirements:
    - matplotlib
    - pandas
    - geopandas (for geometry handling)

Input DataFrames must contain:
    - 'geometry' column with Point objects containing x,y coordinates
    - Variable data columns as specified in individual function docstrings
"""

import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict
from .Decorators import log_call
from .HdfMesh import HdfMesh

class HdfResultsPlot:
    """
    A class containing static methods for plotting HEC-RAS results data.
    
    This class provides visualization methods for various types of HEC-RAS results,
    including maximum water surface elevations and timing information.
    """

    @staticmethod
    @log_call
    def plot_results_max_wsel(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the maximum water surface elevation per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface elevations.
        """
        # Extract x and y coordinates from the geometry column
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            print("Error: 'x' or 'y' columns not found in the merged dataframe.")
            print("Available columns:", max_ws_df.columns.tolist())
            return

        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=max_ws_df['maximum_water_surface'], 
                           cmap='viridis', s=10)

        ax.set_title('Max Water Surface per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label='Max Water Surface (ft)')

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

    @staticmethod
    @log_call
    def plot_results_max_wsel_time(max_ws_df: pd.DataFrame) -> None:
        """
        Plots the time of the maximum water surface elevation (WSEL) per cell.

        Args:
            max_ws_df (pd.DataFrame): DataFrame containing merged data with coordinates 
                                    and max water surface timing information.
        """
        # Convert datetime strings using the renamed utility function
        max_ws_df['max_wsel_time'] = pd.to_datetime(max_ws_df['maximum_water_surface_time'])
        
        # Extract coordinates
        max_ws_df['x'] = max_ws_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        max_ws_df['y'] = max_ws_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)

        if 'x' not in max_ws_df.columns or 'y' not in max_ws_df.columns:
            raise ValueError("x and y coordinates are missing from the DataFrame. Make sure the 'geometry' column exists and contains valid coordinate data.")

        fig, ax = plt.subplots(figsize=(12, 8))

        min_time = max_ws_df['max_wsel_time'].min()
        color_values = (max_ws_df['max_wsel_time'] - min_time).dt.total_seconds() / 3600

        scatter = ax.scatter(max_ws_df['x'], max_ws_df['y'], 
                           c=color_values, cmap='viridis', s=10)

        ax.set_title('Time of Maximum Water Surface Elevation per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')

        cbar = plt.colorbar(scatter)
        cbar.set_label('Hours since simulation start')
        cbar.set_ticks(range(0, int(color_values.max()) + 1, 6))
        cbar.set_ticklabels([f'{h}h' for h in range(0, int(color_values.max()) + 1, 6)])

        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

        # Print timing information
        print(f"\nSimulation Start Time: {min_time}")
        print(f"Time Range: {color_values.max():.1f} hours")
        print("\nTiming Statistics (hours since start):")
        print(color_values.describe()) 

    @staticmethod
    @log_call
    def plot_results_mesh_variable(variable_df: pd.DataFrame, variable_name: str, colormap: str = 'viridis', point_size: int = 10) -> None:
        """
        Plot any mesh variable with consistent styling.
        
        Args:
            variable_df (pd.DataFrame): DataFrame containing the variable data
            variable_name (str): Name of the variable (for labels)
            colormap (str): Matplotlib colormap to use. Default: 'viridis'
            point_size (int): Size of the scatter points. Default: 10

        Returns:
            None

        Raises:
            ImportError: If matplotlib is not installed
            ValueError: If required columns are missing from variable_df
        """
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            logger.error("matplotlib is required for plotting. Please install it with 'pip install matplotlib'")
            raise ImportError("matplotlib is required for plotting")

        # Get cell coordinates if not in variable_df
        if 'geometry' not in variable_df.columns:
            cell_coords = HdfMesh.mesh_cell_points(plan_hdf_path)
            merged_df = pd.merge(variable_df, cell_coords, on=['mesh_name', 'cell_id'])
        else:
            merged_df = variable_df
            
        # Extract coordinates, handling None values
        merged_df = merged_df.dropna(subset=['geometry'])
        merged_df['x'] = merged_df['geometry'].apply(lambda geom: geom.x if geom is not None else None)
        merged_df['y'] = merged_df['geometry'].apply(lambda geom: geom.y if geom is not None else None)
        
        # Drop any rows with None coordinates
        merged_df = merged_df.dropna(subset=['x', 'y'])
        
        if len(merged_df) == 0:
            logger.error("No valid coordinates found for plotting")
            raise ValueError("No valid coordinates found for plotting")
            
        # Create plot
        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(merged_df['x'], merged_df['y'], 
                           c=merged_df[variable_name], 
                           cmap=colormap, 
                           s=point_size)
        
        # Customize plot
        ax.set_title(f'{variable_name} per Cell')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        plt.colorbar(scatter, label=variable_name)
        ax.grid(True, linestyle='--', alpha=0.7)
        plt.rcParams.update({'font.size': 12})
        plt.tight_layout()
        plt.show()

==================================================

File: c:\GH\ras-commander\ras_commander\HdfResultsXsec.py
==================================================
"""
Class: HdfResultsXsec

Contains methods for extracting 1D results data from HDF files. 
This includes cross section timeseries, structures and reference line/point timeseries as these are all 1D elements.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfResultsXsec:
- get_xsec_timeseries(): Extract cross-section timeseries data including water surface, velocity, and flow
- get_ref_lines_timeseries(): Get timeseries output for reference lines
- get_ref_points_timeseries(): Get timeseries output for reference points

TO BE IMPLEMENTED: 
DSS Hydrograph Extraction for 1D and 2D Structures. 

Planned functions:
- get_bridge_timeseries(): Extract timeseries data for bridge structures
- get_inline_structures_timeseries(): Extract timeseries data for inline structures

Notes:
- All functions use the get_ prefix to indicate they return data
- Results data functions use results_ prefix to indicate they handle results data
- All functions include proper error handling and logging
- Functions return xarray Datasets for efficient handling of multi-dimensional data
"""

from pathlib import Path
from typing import Union, Optional, List, Dict, Tuple

import h5py
import numpy as np
import pandas as pd
import xarray as xr

from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .Decorators import standardize_input, log_call
from .LoggingConfig import get_logger

logger = get_logger(__name__)

class HdfResultsXsec:
    """
    A static class for extracting and processing 1D results data from HEC-RAS HDF files.

    This class provides methods to extract and process unsteady flow simulation results
    for cross-sections, reference lines, and reference points. All methods are static
    and designed to be used without class instantiation.

    The class handles:
    - Cross-section timeseries (water surface, velocity, flow)
    - Reference line timeseries
    - Reference point timeseries

    Dependencies:
        - HdfBase: Core HDF file operations
        - HdfUtils: Utility functions for HDF processing
    """


# Tested functions from AWS webinar where the code was developed
# Need to add examples


    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_xsec_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract Water Surface, Velocity Total, Velocity Channel, Flow Lateral, and Flow data from HEC-RAS HDF file.
        Includes Cross Section Only and Cross Section Attributes as coordinates in the xarray.Dataset.
        Also calculates maximum values for key parameters.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Xarray Dataset containing the extracted cross-section results with appropriate coordinates and attributes.
            Includes maximum values for Water Surface, Flow, Channel Velocity, Total Velocity, and Lateral Flow.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                # Define base paths
                base_output_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Cross Sections/"
                time_stamp_path = "/Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Time Date Stamp (ms)"
                
                # Extract Cross Section Attributes
                attrs_dataset = hdf_file[f"{base_output_path}Cross Section Attributes"][:]
                rivers = [attr['River'].decode('utf-8').strip() for attr in attrs_dataset]
                reaches = [attr['Reach'].decode('utf-8').strip() for attr in attrs_dataset]
                stations = [attr['Station'].decode('utf-8').strip() for attr in attrs_dataset]
                names = [attr['Name'].decode('utf-8').strip() for attr in attrs_dataset]
                
                # Extract Cross Section Only (Unique Names)
                cross_section_only_dataset = hdf_file[f"{base_output_path}Cross Section Only"][:]
                cross_section_names = [cs.decode('utf-8').strip() for cs in cross_section_only_dataset]
                
                # Extract Time Stamps and convert to datetime
                time_stamps = hdf_file[time_stamp_path][:]
                if any(isinstance(ts, bytes) for ts in time_stamps):
                    time_stamps = [ts.decode('utf-8') for ts in time_stamps]
                # Convert RAS format timestamps to datetime
                times = pd.to_datetime(time_stamps, format='%d%b%Y %H:%M:%S:%f')
                
                # Extract Required Datasets
                water_surface = hdf_file[f"{base_output_path}Water Surface"][:]
                velocity_total = hdf_file[f"{base_output_path}Velocity Total"][:]
                velocity_channel = hdf_file[f"{base_output_path}Velocity Channel"][:]
                flow_lateral = hdf_file[f"{base_output_path}Flow Lateral"][:]
                flow = hdf_file[f"{base_output_path}Flow"][:]
                
                # Calculate maximum values along time axis
                max_water_surface = np.max(water_surface, axis=0)
                max_flow = np.max(flow, axis=0)
                max_velocity_channel = np.max(velocity_channel, axis=0)
                max_velocity_total = np.max(velocity_total, axis=0)
                max_flow_lateral = np.max(flow_lateral, axis=0)
                
                # Create Xarray Dataset
                ds = xr.Dataset(
                    {
                        'Water_Surface': (['time', 'cross_section'], water_surface),
                        'Velocity_Total': (['time', 'cross_section'], velocity_total),
                        'Velocity_Channel': (['time', 'cross_section'], velocity_channel),
                        'Flow_Lateral': (['time', 'cross_section'], flow_lateral),
                        'Flow': (['time', 'cross_section'], flow),
                    },
                    coords={
                        'time': times,
                        'cross_section': cross_section_names,
                        'River': ('cross_section', rivers),
                        'Reach': ('cross_section', reaches),
                        'Station': ('cross_section', stations),
                        'Name': ('cross_section', names),
                        'Maximum_Water_Surface': ('cross_section', max_water_surface),
                        'Maximum_Flow': ('cross_section', max_flow),
                        'Maximum_Channel_Velocity': ('cross_section', max_velocity_channel),
                        'Maximum_Velocity_Total': ('cross_section', max_velocity_total),
                        'Maximum_Flow_Lateral': ('cross_section', max_flow_lateral)
                    },
                    attrs={
                        'description': 'Cross-section results extracted from HEC-RAS HDF file',
                        'source_file': str(hdf_path)
                    }
                )
                
                return ds

        except KeyError as e:
            logger.error(f"Required dataset not found in HDF file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error extracting cross section results: {e}")
            raise



    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_lines_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference lines from HEC-RAS HDF file.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing flow, velocity, and water surface data for reference lines.
            Returns empty dataset if reference line data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="lines")

    @staticmethod
    @log_call
    @standardize_input(file_type='plan_hdf')
    def get_ref_points_timeseries(hdf_path: Path) -> xr.Dataset:
        """
        Extract timeseries output data for reference points from HEC-RAS HDF file.

        This method extracts flow, velocity, and water surface elevation data for all
        reference points defined in the model. Reference points are user-defined locations
        where detailed output is desired.

        Parameters:
        -----------
        hdf_path : Path
            Path to the HEC-RAS results HDF file

        Returns:
        --------
        xr.Dataset
            Dataset containing the following variables for each reference point:
            - Flow [cfs or m/s]
            - Velocity [ft/s or m/s]
            - Water Surface [ft or m]
            
            The dataset includes coordinates:
            - time: Simulation timesteps
            - refpt_id: Unique identifier for each reference point
            - refpt_name: Name of each reference point
            - mesh_name: Associated 2D mesh area name
            
            Returns empty dataset if reference point data not found.

        Raises:
        -------
        FileNotFoundError
            If the specified HDF file is not found
        KeyError
            If required datasets are missing from the HDF file

        Examples:
        --------
        >>> ds = HdfResultsXsec.get_ref_points_timeseries("path/to/plan.hdf")
        >>> # Get water surface timeseries for first reference point
        >>> ws = ds['Water Surface'].isel(refpt_id=0)
        >>> # Get all data for a specific reference point by name
        >>> point_data = ds.sel(refpt_name='Point1')
        """
        return HdfResultsXsec._reference_timeseries_output(hdf_path, reftype="points")
    

    @staticmethod
    def _reference_timeseries_output(hdf_file: h5py.File, reftype: str = "lines") -> xr.Dataset:
        """
        Internal method to return timeseries output data for reference lines or points from a HEC-RAS HDF plan file.

        Parameters
        ----------
        hdf_file : h5py.File
            Open HDF file object.
        reftype : str, optional
            The type of reference data to retrieve. Must be either "lines" or "points".
            (default: "lines")

        Returns
        -------
        xr.Dataset
            An xarray Dataset with reference line or point timeseries data.
            Returns an empty Dataset if the reference output data is not found.

        Raises
        ------
        ValueError
            If reftype is not "lines" or "points".
        """
        if reftype == "lines":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Lines"
            abbrev = "refln"
        elif reftype == "points":
            output_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/Reference Points"
            abbrev = "refpt"
        else:
            raise ValueError('reftype must be either "lines" or "points".')

        try:
            reference_group = hdf_file[output_path]
        except KeyError:
            logger.error(f"Could not find HDF group at path '{output_path}'. "
                         f"The Plan HDF file may not contain reference {reftype[:-1]} output data.")
            return xr.Dataset()

        reference_names = reference_group["Name"][:]
        names = []
        mesh_areas = []
        for s in reference_names:
            name, mesh_area = s.decode("utf-8").split("|")
            names.append(name)
            mesh_areas.append(mesh_area)

        times = HdfBase.get_unsteady_timestamps(hdf_file)

        das = {}
        for var in ["Flow", "Velocity", "Water Surface"]:
            group = reference_group.get(var)
            if group is None:
                continue
            values = group[:]
            units = group.attrs["Units"].decode("utf-8")
            da = xr.DataArray(
                values,
                name=var,
                dims=["time", f"{abbrev}_id"],
                coords={
                    "time": times,
                    f"{abbrev}_id": range(values.shape[1]),
                    f"{abbrev}_name": (f"{abbrev}_id", names),
                    "mesh_name": (f"{abbrev}_id", mesh_areas),
                },
                attrs={"units": units, "hdf_path": f"{output_path}/{var}"},
            )
            das[var] = da
        return xr.Dataset(das)

==================================================

File: c:\GH\ras-commander\ras_commander\HdfStruc.py
==================================================
"""
Class: HdfStruc

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

The file has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in HdfStruc:
- get_structures()
- get_geom_structures_attrs()
"""
from typing import Dict, Any, List, Union
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
from shapely.geometry import LineString, MultiLineString, Polygon, MultiPolygon, Point, GeometryCollection
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfBase import HdfBase
from .Decorators import standardize_input, log_call
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfStruc:
    """
    Handles 2D structure geometry data extraction from HEC-RAS HDF files.

    This class provides static methods for extracting and analyzing structure geometries
    and their attributes from HEC-RAS geometry HDF files. All methods are designed to work
    without class instantiation.

    Notes
    -----
    - 1D Structure data should be accessed via the HdfResultsXsec class
    - All methods use @standardize_input for consistent file handling
    - All methods use @log_call for operation logging
    - Returns GeoDataFrames with both geometric and attribute data
    """
    
    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_structures(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts structure data from a HEC-RAS geometry HDF5 file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF5 file
        datetime_to_str : bool, optional
            If True, converts datetime objects to ISO format strings, by default False

        Returns
        -------
        GeoDataFrame
            Structure data with columns:
            - Structure ID: unique identifier
            - Geometry: LineString of structure centerline
            - Various attribute columns from the HDF file
            - Profile_Data: list of station/elevation dictionaries
            - Bridge coefficient attributes (if present)
            - Table info attributes (if present)

        Notes
        -----
        - Group-level attributes are stored in GeoDataFrame.attrs['group_attributes']
        - Invalid geometries are dropped with warning
        - All byte strings are decoded to UTF-8
        - CRS is preserved from the source file
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                if "Geometry/Structures" not in hdf:
                    logger.info(f"No structures found in: {hdf_path}")
                    return GeoDataFrame()
                
                def get_dataset_df(path: str) -> pd.DataFrame:
                    """
                    Converts an HDF5 dataset to a pandas DataFrame.

                    Parameters
                    ----------
                    path : str
                        Dataset path within the HDF5 file

                    Returns
                    -------
                    pd.DataFrame
                        DataFrame containing the dataset values.
                        - For compound datasets, column names match field names
                        - For simple datasets, generic column names (Value_0, Value_1, etc.)
                        - Empty DataFrame if dataset not found

                    Notes
                    -----
                    Automatically decodes byte strings to UTF-8 with error handling.
                    """
                    if path not in hdf:
                        logger.warning(f"Dataset not found: {path}")
                        return pd.DataFrame()
                    
                    data = hdf[path][()]
                    
                    if data.dtype.names:
                        df = pd.DataFrame(data)
                        # Decode byte strings to UTF-8
                        for col in df.columns:
                            if df[col].dtype.kind in {'S', 'a'}:  # Byte strings
                                df[col] = df[col].str.decode('utf-8', errors='ignore')
                        return df
                    else:
                        # If no named fields, assign generic column names
                        return pd.DataFrame(data, columns=[f'Value_{i}' for i in range(data.shape[1])])

                # Extract relevant datasets
                group_attrs = HdfBase.get_attrs(hdf, "Geometry/Structures")
                struct_attrs = get_dataset_df("Geometry/Structures/Attributes")
                bridge_coef = get_dataset_df("Geometry/Structures/Bridge Coefficient Attributes")
                table_info = get_dataset_df("Geometry/Structures/Table Info")
                profile_data = get_dataset_df("Geometry/Structures/Profile Data")

                # Assign 'Structure ID' based on index (starting from 1)
                struct_attrs.reset_index(drop=True, inplace=True)
                struct_attrs['Structure ID'] = range(1, len(struct_attrs) + 1)
                logger.debug(f"Assigned Structure IDs: {struct_attrs['Structure ID'].tolist()}")

                # Check if 'Structure ID' was successfully assigned
                if 'Structure ID' not in struct_attrs.columns:
                    logger.error("'Structure ID' column could not be assigned to Structures/Attributes.")
                    return GeoDataFrame()

                # Get centerline geometry
                centerline_info = hdf["Geometry/Structures/Centerline Info"][()]
                centerline_points = hdf["Geometry/Structures/Centerline Points"][()]
                
                # Create LineString geometries for each structure
                geoms = []
                for i in range(len(centerline_info)):
                    start_idx = centerline_info[i][0]  # Point Starting Index
                    point_count = centerline_info[i][1]  # Point Count
                    points = centerline_points[start_idx:start_idx + point_count]
                    if len(points) >= 2:
                        geoms.append(LineString(points))
                    else:
                        logger.warning(f"Insufficient points for LineString in structure index {i}.")
                        geoms.append(None)

                # Create base GeoDataFrame with Structures Attributes and geometries
                struct_gdf = GeoDataFrame(
                    struct_attrs,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Drop entries with invalid geometries
                initial_count = len(struct_gdf)
                struct_gdf = struct_gdf.dropna(subset=['geometry']).reset_index(drop=True)
                final_count = len(struct_gdf)
                if final_count < initial_count:
                    logger.warning(f"Dropped {initial_count - final_count} structures due to invalid geometries.")

                # Merge Bridge Coefficient Attributes on 'Structure ID'
                if not bridge_coef.empty and 'Structure ID' in bridge_coef.columns:
                    struct_gdf = struct_gdf.merge(
                        bridge_coef,
                        on='Structure ID',
                        how='left',
                        suffixes=('', '_bridge_coef')
                    )
                    logger.debug("Merged Bridge Coefficient Attributes successfully.")
                else:
                    logger.warning("Bridge Coefficient Attributes missing or 'Structure ID' not present.")

                # Merge Table Info based on the DataFrame index (one-to-one correspondence)
                if not table_info.empty:
                    if len(table_info) != len(struct_gdf):
                        logger.warning("Table Info count does not match Structures count. Skipping merge.")
                    else:
                        struct_gdf = pd.concat([struct_gdf, table_info.reset_index(drop=True)], axis=1)
                        logger.debug("Merged Table Info successfully.")
                else:
                    logger.warning("Table Info dataset is empty or missing.")

                # Process Profile Data based on Table Info
                if not profile_data.empty and not table_info.empty:
                    # Assuming 'Centerline Profile (Index)' and 'Centerline Profile (Count)' are in 'Table Info'
                    if ('Centerline Profile (Index)' in table_info.columns and
                        'Centerline Profile (Count)' in table_info.columns):
                        struct_gdf['Profile_Data'] = struct_gdf.apply(
                            lambda row: [
                                {'Station': float(profile_data.iloc[i, 0]),
                                 'Elevation': float(profile_data.iloc[i, 1])}
                                for i in range(
                                    int(row['Centerline Profile (Index)']),
                                    int(row['Centerline Profile (Index)']) + int(row['Centerline Profile (Count)'])
                                )
                            ],
                            axis=1
                        )
                        logger.debug("Processed Profile Data successfully.")
                    else:
                        logger.warning("Required columns for Profile Data not found in Table Info.")
                else:
                    logger.warning("Profile Data dataset is empty or Table Info is missing.")

                # Convert datetime columns to string if requested
                if datetime_to_str:
                    datetime_cols = struct_gdf.select_dtypes(include=['datetime64']).columns
                    for col in datetime_cols:
                        struct_gdf[col] = struct_gdf[col].dt.isoformat()
                        logger.debug(f"Converted datetime column '{col}' to string.")

                # Ensure all byte strings are decoded (if any remain)
                for col in struct_gdf.columns:
                    if struct_gdf[col].dtype == object:
                        struct_gdf[col] = struct_gdf[col].apply(
                            lambda x: x.decode('utf-8', errors='ignore') if isinstance(x, bytes) else x
                        )

                # Final GeoDataFrame
                logger.info("Successfully extracted structures GeoDataFrame.")
                
                # Add group attributes to the GeoDataFrame's attrs['group_attributes']
                struct_gdf.attrs['group_attributes'] = group_attrs
                
                logger.info("Successfully extracted structures GeoDataFrame with attributes.")
                
                return struct_gdf

        except Exception as e:
            logger.error(f"Error reading structures from {hdf_path}: {str(e)}")
            raise

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_geom_structures_attrs(hdf_path: Path) -> Dict[str, Any]:
        """
        Extracts structure attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file

        Returns
        -------
        Dict[str, Any]
            Dictionary of structure attributes from the Geometry/Structures group.
            Returns empty dict if no structures are found.

        Notes
        -----
        Attributes are extracted from the HDF5 group 'Geometry/Structures'.
        All byte strings in attributes are automatically decoded to UTF-8.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/Structures" not in hdf_file:
                    logger.info(f"No structures found in the geometry file: {hdf_path}")
                    return {}
                return HdfUtils.convert_hdf5_attrs_to_dict(hdf_file["Geometry/Structures"].attrs)
        except Exception as e:
            logger.error(f"Error reading geometry structures attributes: {str(e)}")
            return {}

==================================================

File: c:\GH\ras-commander\ras_commander\HdfUtils.py
==================================================
"""
HdfUtils Class
-------------

A utility class providing static methods for working with HEC-RAS HDF files.

Attribution:
    A substantial amount of code in this file is sourced or derived from the 
    https://github.com/fema-ffrd/rashdf library, released under MIT license 
    and Copyright (c) 2024 fema-ffrd. The file has been forked and modified 
    for use in RAS Commander.

Key Features:
- HDF file data conversion and parsing
- DateTime handling for RAS-specific formats
- Spatial operations using KDTree
- HDF attribute management

Main Method Categories:

1. Data Conversion
    - convert_ras_string: Convert RAS HDF strings to Python objects
    - convert_ras_hdf_value: Convert general HDF values to Python objects
    - convert_df_datetimes_to_str: Convert DataFrame datetime columns to strings
    - convert_hdf5_attrs_to_dict: Convert HDF5 attributes to dictionary
    - convert_timesteps_to_datetimes: Convert timesteps to datetime objects

2. Spatial Operations
    - perform_kdtree_query: KDTree search between datasets
    - find_nearest_neighbors: Find nearest neighbors within dataset

3. DateTime Parsing
    - parse_ras_datetime: Parse standard RAS datetime format (ddMMMYYYY HH:MM:SS)
    - parse_ras_window_datetime: Parse simulation window datetime (ddMMMYYYY HHMM)
    - parse_duration: Parse duration strings (HH:MM:SS)
    - parse_ras_datetime_ms: Parse datetime with milliseconds
    - parse_run_time_window: Parse time window strings

Usage Notes:
- All methods are static and can be called without class instantiation
- Methods handle both raw HDF data and converted Python objects
- Includes comprehensive error handling for RAS-specific data formats
- Supports various RAS datetime formats and conversions
"""
import logging
from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Union, Optional, Dict, List, Tuple, Any
from scipy.spatial import KDTree
import re
from shapely.geometry import LineString  # Import LineString to avoid NameError

from .Decorators import standardize_input, log_call 
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class HdfUtils:
    """
    Utility class for working with HEC-RAS HDF files.

    This class provides general utility functions for HDF file operations,
    including attribute extraction, data conversion, and common HDF queries.
    It also includes spatial operations and helper methods for working with
    HEC-RAS specific data structures.

    Note:
    - Use this class for general HDF utility functions that are not specific to plan or geometry files.
    - All methods in this class are static and can be called without instantiating the class.
    """




# RENAME TO convert_ras_string and make public

    @staticmethod
    def convert_ras_string(value: Union[str, bytes]) -> Union[bool, datetime, List[datetime], timedelta, str]:
        """
        Convert a string value from an HEC-RAS HDF file into a Python object.

        Args:
            value (Union[str, bytes]): The value to convert.

        Returns:
            Union[bool, datetime, List[datetime], timedelta, str]: The converted value.
        """
        if isinstance(value, bytes):
            s = value.decode("utf-8")
        else:
            s = value

        if s == "True":
            return True
        elif s == "False":
            return False
        
        ras_datetime_format1_re = r"\d{2}\w{3}\d{4} \d{2}:\d{2}:\d{2}"
        ras_datetime_format2_re = r"\d{2}\w{3}\d{4} \d{2}\d{2}"
        ras_duration_format_re = r"\d{2}:\d{2}:\d{2}"

        if re.match(rf"^{ras_datetime_format1_re}", s):
            if re.match(rf"^{ras_datetime_format1_re} to {ras_datetime_format1_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_datetime(split[0]),
                    HdfUtils.parse_ras_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_datetime(s)
        elif re.match(rf"^{ras_datetime_format2_re}", s):
            if re.match(rf"^{ras_datetime_format2_re} to {ras_datetime_format2_re}$", s):
                split = s.split(" to ")
                return [
                    HdfUtils.parse_ras_window_datetime(split[0]),
                    HdfUtils.parse_ras_window_datetime(split[1]),
                ]
            return HdfUtils.parse_ras_window_datetime(s)
        elif re.match(rf"^{ras_duration_format_re}$", s):
            return HdfUtils.parse_ras_duration(s)
        return s





    @staticmethod
    def convert_ras_hdf_value(value: Any) -> Union[None, bool, str, List[str], int, float, List[int], List[float]]:
        """
        Convert a value from a HEC-RAS HDF file into a Python object.

        Args:
            value (Any): The value to convert.

        Returns:
            Union[None, bool, str, List[str], int, float, List[int], List[float]]: The converted value.
        """
        if isinstance(value, np.floating) and np.isnan(value):
            return None
        elif isinstance(value, (bytes, np.bytes_)):
            return value.decode('utf-8')
        elif isinstance(value, np.integer):
            return int(value)
        elif isinstance(value, np.floating):
            return float(value)
        elif isinstance(value, (int, float)):
            return value
        elif isinstance(value, (list, tuple, np.ndarray)):
            if len(value) > 1:
                return [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                return HdfUtils.convert_ras_hdf_value(value[0])
        else:
            return str(value)










# RENAME TO convert_df_datetimes_to_str 

    @staticmethod
    def convert_df_datetimes_to_str(df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert any datetime64 columns in a DataFrame to strings.

        Args:
            df (pd.DataFrame): The DataFrame to convert.

        Returns:
            pd.DataFrame: The DataFrame with datetime columns converted to strings.
        """
        for col in df.select_dtypes(include=['datetime64']).columns:
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
        return df


# KDTree Methods: 


    @staticmethod
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = HdfUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = HdfUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped




# Datetime Parsing Methods: 

    @staticmethod
    @log_call
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Public method to parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
# Rename to convert_timesteps_to_datetimes and make public
    @staticmethod
    def convert_timesteps_to_datetimes(timesteps: np.ndarray, start_time: datetime, time_unit: str = "days", round_to: str = "100ms") -> pd.DatetimeIndex:
        """
        Convert RAS timesteps to datetime objects.

        Args:
            timesteps (np.ndarray): Array of timesteps.
            start_time (datetime): Start time of the simulation.
            time_unit (str): Unit of the timesteps. Default is "days".
            round_to (str): Frequency string to round the times to. Default is "100ms" (100 milliseconds).

        Returns:
            pd.DatetimeIndex: DatetimeIndex of converted and rounded datetimes.
        """
        if time_unit == "days":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='D')
        elif time_unit == "hours":
            datetimes = start_time + pd.to_timedelta(timesteps, unit='H')
        else:
            raise ValueError(f"Unsupported time unit: {time_unit}")

        return pd.DatetimeIndex(datetimes).round(round_to)
    
# rename to convert_hdf5_attrs_to_dict and make public

    @staticmethod
    def convert_hdf5_attrs_to_dict(attrs: Union[h5py.AttributeManager, Dict], prefix: Optional[str] = None) -> Dict:
        """
        Convert HDF5 attributes to a Python dictionary.

        Args:
            attrs (Union[h5py.AttributeManager, Dict]): The attributes to convert.
            prefix (Optional[str]): A prefix to add to the attribute keys.

        Returns:
            Dict: A dictionary of converted attributes.
        """
        result = {}
        for key, value in attrs.items():
            if prefix:
                key = f"{prefix}/{key}"
            if isinstance(value, (np.ndarray, list)):
                result[key] = [HdfUtils.convert_ras_hdf_value(v) for v in value]
            else:
                result[key] = HdfUtils.convert_ras_hdf_value(value)
        return result
    
    

    @staticmethod
    def parse_run_time_window(window: str) -> Tuple[datetime, datetime]:
        """
        Parse a run time window string into a tuple of datetime objects.

        Args:
            window (str): The run time window string to be parsed.

        Returns:
            Tuple[datetime, datetime]: A tuple containing two datetime objects representing the start and end of the run
            time window.
        """
        split = window.split(" to ")
        begin = HdfUtils._parse_ras_datetime(split[0])
        end = HdfUtils._parse_ras_datetime(split[1])
        return begin, end

    


                
                
                
                
                
                
                
                
                
                
                
                
## MOVED FROM HdfBase to HdfUtils:
# _parse_ras_datetime   
# _parse_ras_simulation_window_datetime
# _parse_duration
# _parse_ras_datetime_ms
# _convert_ras_hdf_string

# Which were renamed and made public as: 
# parse_ras_datetime
# parse_ras_window_datetime
# parse_ras_datetime_ms
# parse_ras_duration
# parse_ras_time_window


# Rename to parse_ras_datetime and make public

    @staticmethod
    def parse_ras_datetime(datetime_str: str) -> datetime:
        """
        Parse a RAS datetime string into a datetime object.

        Args:
            datetime_str (str): The datetime string in format "ddMMMYYYY HH:MM:SS"

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H:%M:%S")

# Rename to parse_ras_window_datetime and make public

    @staticmethod
    def parse_ras_window_datetime(datetime_str: str) -> datetime:
        """
        Parse a datetime string from a RAS simulation window into a datetime object.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        return datetime.strptime(datetime_str, "%d%b%Y %H%M")


# Rename to parse_duration and make public


    @staticmethod
    def parse_duration(duration_str: str) -> timedelta:
        """
        Parse a duration string into a timedelta object.

        Args:
            duration_str (str): The duration string to parse.

        Returns:
            timedelta: The parsed duration as a timedelta object.
        """
        hours, minutes, seconds = map(int, duration_str.split(':'))
        return timedelta(hours=hours, minutes=minutes, seconds=seconds)
    
    
# Rename to parse_ras_datetime_ms and make public
    
    @staticmethod
    def parse_ras_datetime_ms(datetime_str: str) -> datetime:
        """
        Parse a datetime string with milliseconds from a RAS file.

        Args:
            datetime_str (str): The datetime string to parse.

        Returns:
            datetime: The parsed datetime object.
        """
        milliseconds = int(datetime_str[-3:])
        microseconds = milliseconds * 1000
        parsed_dt = HdfUtils.parse_ras_datetime(datetime_str[:-4]).replace(microsecond=microseconds)
        return parsed_dt
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\HdfXsec.py
==================================================
"""
Class: HdfXsec

Attribution: A substantial amount of code in this file is sourced or derived 
from the https://github.com/fema-ffrd/rashdf library, 
released under MIT license and Copyright (c) 2024 fema-ffrd

This source code has been forked and modified for use in RAS Commander.

-----

All of the methods in this class are static and are designed to be used without instantiation.

Available Functions:
- get_cross_sections(): Extract cross sections from HDF geometry file
- get_river_centerlines(): Extract river centerlines from HDF geometry file
- get_river_stationing(): Calculate river stationing along centerlines
- get_river_reaches(): Return the model 1D river reach lines
- get_river_edge_lines(): Return the model river edge lines
- get_river_bank_lines(): Extract river bank lines from HDF geometry file
- _interpolate_station(): Private helper method for station interpolation

All functions follow the get_ prefix convention for methods that return data.
Private helper methods use the underscore prefix convention.

Each function returns a GeoDataFrame containing geometries and associated attributes
specific to the requested feature type. All functions include proper error handling
and logging.
"""

from pathlib import Path
import h5py
import numpy as np
import pandas as pd
from geopandas import GeoDataFrame
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString
from typing import List  # Import List to avoid NameError
from .Decorators import standardize_input, log_call
from .HdfBase import HdfBase
from .HdfUtils import HdfUtils
from .LoggingConfig import get_logger
import logging



logger = get_logger(__name__)

class HdfXsec:
    """
    Handles cross-section and river geometry data extraction from HEC-RAS HDF files.

    This class provides static methods to extract and process:
    - Cross-section geometries and attributes
    - River centerlines and reaches
    - River edge and bank lines
    - Station-elevation profiles

    All methods are designed to return GeoDataFrames with standardized geometries 
    and attributes following the HEC-RAS data structure.

    Note:
        Requires HEC-RAS geometry HDF files with standard structure and naming conventions.
        All methods use proper error handling and logging.
    """
    @staticmethod
    @log_call
    def get_cross_sections(hdf_path: str, datetime_to_str: bool = True, ras_object=None) -> gpd.GeoDataFrame:
        """
        Extracts cross-section geometries and attributes from a HEC-RAS geometry HDF file.

        Parameters
        ----------
        hdf_path : str
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to True
        ras_object : RasPrj, optional
            RAS project object for additional context, defaults to None

        Returns
        -------
        gpd.GeoDataFrame
            Cross-section data with columns:
            - geometry: LineString of cross-section path
            - station_elevation: Station-elevation profile points
            - mannings_n: Dictionary of Manning's n values and stations
            - ineffective_blocks: List of ineffective flow area blocks
            - River, Reach, RS: River system identifiers
            - Name, Description: Cross-section labels
            - Len Left/Channel/Right: Flow path lengths
            - Left/Right Bank: Bank station locations
            - Additional hydraulic parameters and attributes

        Notes
        -----
        The returned GeoDataFrame includes the coordinate system from the HDF file
        when available. All byte strings are converted to regular strings.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf:
                # Extract required datasets
                poly_info = hdf['/Geometry/Cross Sections/Polyline Info'][:]
                poly_parts = hdf['/Geometry/Cross Sections/Polyline Parts'][:]
                poly_points = hdf['/Geometry/Cross Sections/Polyline Points'][:]
                
                station_info = hdf['/Geometry/Cross Sections/Station Elevation Info'][:]
                station_values = hdf['/Geometry/Cross Sections/Station Elevation Values'][:]
                
                # Get attributes for cross sections
                xs_attrs = hdf['/Geometry/Cross Sections/Attributes'][:]
                
                # Get Manning's n data
                mann_info = hdf["/Geometry/Cross Sections/Manning's n Info"][:]
                mann_values = hdf["/Geometry/Cross Sections/Manning's n Values"][:]
                
                # Get ineffective blocks data
                ineff_blocks = hdf['/Geometry/Cross Sections/Ineffective Blocks'][:]
                ineff_info = hdf['/Geometry/Cross Sections/Ineffective Info'][:]
                
                # Initialize lists to store data
                geometries = []
                station_elevations = []
                mannings_n = []
                ineffective_blocks = []
                
                # Process each cross section
                for i in range(len(poly_info)):
                    # Extract polyline info
                    point_start_idx = poly_info[i][0]
                    point_count = poly_info[i][1]
                    part_start_idx = poly_info[i][2]
                    part_count = poly_info[i][3]
                    
                    # Extract parts for current polyline
                    parts = poly_parts[part_start_idx:part_start_idx + part_count]
                    
                    # Collect all points for this cross section
                    xs_points = []
                    for part in parts:
                        part_point_start = point_start_idx + part[0]
                        part_point_count = part[1]
                        points = poly_points[part_point_start:part_point_start + part_point_count]
                        xs_points.extend(points)
                    
                    # Create LineString geometry
                    if len(xs_points) >= 2:
                        geometry = LineString(xs_points)
                        geometries.append(geometry)
                        
                        # Extract station-elevation data
                        start_idx = station_info[i][0]
                        count = station_info[i][1]
                        station_elev = station_values[start_idx:start_idx + count]
                        station_elevations.append(station_elev)
                        
                        # Extract Manning's n data
                        mann_start_idx = mann_info[i][0]
                        mann_count = mann_info[i][1]
                        mann_n_section = mann_values[mann_start_idx:mann_start_idx + mann_count]
                        mann_n_dict = {
                            'Station': mann_n_section[:, 0].tolist(),
                            'Mann n': mann_n_section[:, 1].tolist()
                        }
                        mannings_n.append(mann_n_dict)
                        
                        # Extract ineffective blocks data
                        ineff_start_idx = ineff_info[i][0]
                        ineff_count = ineff_info[i][1]
                        if ineff_count > 0:
                            blocks = ineff_blocks[ineff_start_idx:ineff_start_idx + ineff_count]
                            blocks_list = []
                            for block in blocks:
                                block_dict = {
                                    'Left Sta': float(block['Left Sta']),
                                    'Right Sta': float(block['Right Sta']), 
                                    'Elevation': float(block['Elevation']),
                                    'Permanent': bool(block['Permanent'])
                                }
                                blocks_list.append(block_dict)
                            ineffective_blocks.append(blocks_list)
                        else:
                            ineffective_blocks.append([])
                
                # Create base dictionary with required fields
                data = {
                    'geometry': geometries,
                    'station_elevation': station_elevations,
                    'mannings_n': mannings_n,
                    'ineffective_blocks': ineffective_blocks,
                }
                
                # Define field mappings with default values
                field_mappings = {
                    'River': ('River', ''),
                    'Reach': ('Reach', ''),
                    'RS': ('RS', ''),
                    'Name': ('Name', ''),
                    'Description': ('Description', ''),
                    'Len Left': ('Len Left', 0.0),
                    'Len Channel': ('Len Channel', 0.0),
                    'Len Right': ('Len Right', 0.0),
                    'Left Bank': ('Left Bank', 0.0),
                    'Right Bank': ('Right Bank', 0.0),
                    'Friction Mode': ('Friction Mode', ''),
                    'Contr': ('Contr', 0.0),
                    'Expan': ('Expan', 0.0),
                    'Left Levee Sta': ('Left Levee Sta', None),
                    'Left Levee Elev': ('Left Levee Elev', None),
                    'Right Levee Sta': ('Right Levee Sta', None),
                    'Right Levee Elev': ('Right Levee Elev', None),
                    'HP Count': ('HP Count', 0),
                    'HP Start Elev': ('HP Start Elev', 0.0),
                    'HP Vert Incr': ('HP Vert Incr', 0.0),
                    'HP LOB Slices': ('HP LOB Slices', 0),
                    'HP Chan Slices': ('HP Chan Slices', 0),
                    'HP ROB Slices': ('HP ROB Slices', 0),
                    'Ineff Block Mode': ('Ineff Block Mode', 0),
                    'Obstr Block Mode': ('Obstr Block Mode', 0),
                    'Default Centerline': ('Default Centerline', 0),
                    'Last Edited': ('Last Edited', '')
                }
                
                # Add fields that exist in xs_attrs
                for field_name, (attr_name, default_value) in field_mappings.items():
                    if attr_name in xs_attrs.dtype.names:
                        if xs_attrs[attr_name].dtype.kind == 'S':
                            # Handle string fields
                            data[field_name] = [x[attr_name].decode('utf-8').strip() 
                                              for x in xs_attrs]
                        else:
                            # Handle numeric fields
                            data[field_name] = xs_attrs[attr_name]
                    else:
                        # Use default value if field doesn't exist
                        data[field_name] = [default_value] * len(geometries)
                        logger.debug(f"Field {attr_name} not found in attributes, using default value")
                
                if geometries:
                    gdf = gpd.GeoDataFrame(data)
                    
                    # Set CRS if available
                    if 'Projection' in hdf['/Geometry'].attrs:
                        proj = hdf['/Geometry'].attrs['Projection']
                        if isinstance(proj, bytes):
                            proj = proj.decode('utf-8')
                        gdf.set_crs(proj, allow_override=True)
                    
                    return gdf
                
                return gpd.GeoDataFrame()
                
        except Exception as e:
            logger.error(f"Error processing cross-section data: {str(e)}")
            return gpd.GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_centerlines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extracts river centerline geometries and attributes from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, defaults to False

        Returns
        -------
        GeoDataFrame
            River centerline data with columns:
            - geometry: LineString of river centerline
            - River Name, Reach Name: River system identifiers
            - US/DS Type, Name: Upstream/downstream connection info
            - length: Centerline length in project units
            Additional attributes from the HDF file are included

        Notes
        -----
        Returns an empty GeoDataFrame if no centerlines are found.
        All string attributes are stripped of whitespace.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    logger.warning("No river centerlines found in geometry file")
                    return GeoDataFrame()

                centerline_data = hdf_file["Geometry/River Centerlines"]
                
                # Get attributes directly from HDF dataset
                attrs = centerline_data["Attributes"][()]
                
                # Create initial dictionary for DataFrame
                centerline_dict = {}
                
                # Process each attribute field
                for name in attrs.dtype.names:
                    values = attrs[name]
                    if values.dtype.kind == 'S':
                        # Convert byte strings to regular strings
                        centerline_dict[name] = [val.decode('utf-8').strip() for val in values]
                    else:
                        centerline_dict[name] = values.tolist()  # Convert numpy array to list

                # Get polylines using utility function
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Centerlines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                centerline_gdf = GeoDataFrame(
                    centerline_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Clean up string columns
                str_columns = ['River Name', 'Reach Name', 'US Type', 
                            'US Name', 'DS Type', 'DS Name']
                for col in str_columns:
                    if col in centerline_gdf.columns:
                        centerline_gdf[col] = centerline_gdf[col].str.strip()

                # Add length calculation in project units
                if not centerline_gdf.empty:
                    centerline_gdf['length'] = centerline_gdf.geometry.length
                    
                    # Convert datetime columns if requested
                    if datetime_to_str:
                        datetime_cols = centerline_gdf.select_dtypes(
                            include=['datetime64']).columns
                        for col in datetime_cols:
                            centerline_gdf[col] = centerline_gdf[col].dt.strftime(
                                '%Y-%m-%d %H:%M:%S')

                logger.info(f"Extracted {len(centerline_gdf)} river centerlines")
                return centerline_gdf

        except Exception as e:
            logger.error(f"Error reading river centerlines: {str(e)}")
            return GeoDataFrame()



    @staticmethod
    @log_call
    def get_river_stationing(centerlines_gdf: GeoDataFrame) -> GeoDataFrame:
        """
        Calculates stationing along river centerlines with interpolated points.

        Parameters
        ----------
        centerlines_gdf : GeoDataFrame
            River centerline geometries from get_river_centerlines()

        Returns
        -------
        GeoDataFrame
            Original centerlines with additional columns:
            - station_start: Starting station value (0 or length)
            - station_end: Ending station value (length or 0)
            - stations: Array of station values along centerline
            - points: Array of interpolated point geometries

        Notes
        -----
        Station direction (increasing/decreasing) is determined by
        upstream/downstream junction connections. Stations are calculated
        at 100 evenly spaced points along each centerline.
        """
        if centerlines_gdf.empty:
            logger.warning("Empty centerlines GeoDataFrame provided")
            return centerlines_gdf

        try:
            # Create copy to avoid modifying original
            result_gdf = centerlines_gdf.copy()
            
            # Initialize new columns
            result_gdf['station_start'] = 0.0
            result_gdf['station_end'] = 0.0
            result_gdf['stations'] = None
            result_gdf['points'] = None
            
            # Process each centerline
            for idx, row in result_gdf.iterrows():
                # Get line geometry
                line = row.geometry
                
                # Calculate length
                total_length = line.length
                
                # Generate points along the line
                distances = np.linspace(0, total_length, num=100)  # Adjust num for desired density
                points = [line.interpolate(distance) for distance in distances]
                
                # Store results
                result_gdf.at[idx, 'station_start'] = 0.0
                result_gdf.at[idx, 'station_end'] = total_length
                result_gdf.at[idx, 'stations'] = distances
                result_gdf.at[idx, 'points'] = points
                
                # Add stationing direction based on upstream/downstream info
                if row['US Type'] == 'Junction' and row['DS Type'] != 'Junction':
                    # Reverse stationing if upstream is junction
                    result_gdf.at[idx, 'station_start'] = total_length
                    result_gdf.at[idx, 'station_end'] = 0.0
                    result_gdf.at[idx, 'stations'] = total_length - distances
            
            return result_gdf

        except Exception as e:
            logger.error(f"Error calculating river stationing: {str(e)}")
            return centerlines_gdf

    @staticmethod
    def _interpolate_station(line, distance):
        """
        Interpolates a point along a line at a given distance.

        Parameters
        ----------
        line : LineString
            Shapely LineString geometry
        distance : float
            Distance along the line to interpolate

        Returns
        -------
        tuple
            (x, y) coordinates of interpolated point
        """
        if distance <= 0:
            return line.coords[0]
        elif distance >= line.length:
            return line.coords[-1]
        return line.interpolate(distance).coords[0]



    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_reaches(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Return the model 1D river reach lines.

        This method extracts river reach data from the HEC-RAS geometry HDF file,
        including attributes and geometry information.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        datetime_to_str : bool, optional
            If True, convert datetime objects to strings. Default is False.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing the river reaches with their attributes and geometries.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Centerlines" not in hdf_file:
                    return GeoDataFrame()

                river_data = hdf_file["Geometry/River Centerlines"]
                v_conv_val = np.vectorize(HdfUtils.convert_ras_string)
                river_attrs = river_data["Attributes"][()]
                river_dict = {"river_id": range(river_attrs.shape[0])}
                river_dict.update(
                    {name: v_conv_val(river_attrs[name]) for name in river_attrs.dtype.names}
                )
                
                # Get polylines for river reaches
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, "Geometry/River Centerlines"
                )

                river_gdf = GeoDataFrame(
                    river_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path),
                )
                if datetime_to_str:
                    river_gdf["Last Edited"] = river_gdf["Last Edited"].apply(
                        lambda x: pd.Timestamp.isoformat(x)
                    )
                return river_gdf
        except Exception as e:
            logger.error(f"Error reading river reaches: {str(e)}")
            return GeoDataFrame()


    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_edge_lines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Return the model river edge lines.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file.
        datetime_to_str : bool, optional
            If True, convert datetime objects to strings. Default is False.

        Returns
        -------
        GeoDataFrame
            A GeoDataFrame containing river edge lines with their attributes and geometries.
            Each row represents a river bank (left or right) with associated attributes.
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Edge Lines" not in hdf_file:
                    logger.warning("No river edge lines found in geometry file")
                    return GeoDataFrame()

                edge_data = hdf_file["Geometry/River Edge Lines"]
                
                # Get attributes if they exist
                if "Attributes" in edge_data:
                    attrs = edge_data["Attributes"][()]
                    v_conv_val = np.vectorize(HdfUtils.convert_ras_string)
                    
                    # Create dictionary of attributes
                    edge_dict = {"edge_id": range(attrs.shape[0])}
                    edge_dict.update(
                        {name: v_conv_val(attrs[name]) for name in attrs.dtype.names}
                    )
                    
                    # Add bank side indicator
                    if edge_dict["edge_id"].size % 2 == 0:  # Ensure even number of edges
                        edge_dict["bank_side"] = ["Left", "Right"] * (edge_dict["edge_id"].size // 2)
                else:
                    edge_dict = {"edge_id": [], "bank_side": []}

                # Get polyline geometries
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Edge Lines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create GeoDataFrame
                edge_gdf = GeoDataFrame(
                    edge_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Convert datetime objects to strings if requested
                if datetime_to_str and 'Last Edited' in edge_gdf.columns:
                    edge_gdf["Last Edited"] = edge_gdf["Last Edited"].apply(
                        lambda x: pd.Timestamp.isoformat(x) if pd.notnull(x) else None
                    )

                # Add length calculation in project units
                if not edge_gdf.empty:
                    edge_gdf['length'] = edge_gdf.geometry.length

                return edge_gdf

        except Exception as e:
            logger.error(f"Error reading river edge lines: {str(e)}")
            return GeoDataFrame()

    @staticmethod
    @log_call
    @standardize_input(file_type='geom_hdf')
    def get_river_bank_lines(hdf_path: Path, datetime_to_str: bool = False) -> GeoDataFrame:
        """
        Extract river bank lines from HDF geometry file.

        Parameters
        ----------
        hdf_path : Path
            Path to the HEC-RAS geometry HDF file
        datetime_to_str : bool, optional
            Convert datetime objects to strings, by default False

        Returns
        -------
        GeoDataFrame
            GeoDataFrame containing river bank line geometries with attributes:
            - bank_id: Unique identifier for each bank line
            - bank_side: Left or Right bank indicator
            - geometry: LineString geometry of the bank
            - length: Length of the bank line in project units
        """
        try:
            with h5py.File(hdf_path, 'r') as hdf_file:
                if "Geometry/River Bank Lines" not in hdf_file:
                    logger.warning("No river bank lines found in geometry file")
                    return GeoDataFrame()

                # Get polyline geometries using existing helper method
                geoms = HdfBase.get_polylines_from_parts(
                    hdf_path, 
                    "Geometry/River Bank Lines",
                    info_name="Polyline Info",
                    parts_name="Polyline Parts",
                    points_name="Polyline Points"
                )

                # Create basic attributes
                bank_dict = {
                    "bank_id": range(len(geoms)),
                    "bank_side": ["Left", "Right"] * (len(geoms) // 2)  # Assuming pairs of left/right banks
                }

                # Create GeoDataFrame
                bank_gdf = GeoDataFrame(
                    bank_dict,
                    geometry=geoms,
                    crs=HdfBase.get_projection(hdf_path)
                )

                # Add length calculation in project units
                if not bank_gdf.empty:
                    bank_gdf['length'] = bank_gdf.geometry.length

                return bank_gdf

        except Exception as e:
            logger.error(f"Error reading river bank lines: {str(e)}")
            return GeoDataFrame()


==================================================

File: c:\GH\ras-commander\ras_commander\LoggingConfig.py
==================================================
# logging_config.py

import logging
import logging.handlers
from pathlib import Path
import functools

# Define log levels
DEBUG = logging.DEBUG
INFO = logging.INFO
WARNING = logging.WARNING
ERROR = logging.ERROR
CRITICAL = logging.CRITICAL


_logging_setup_done = False

def setup_logging(log_file=None, log_level=logging.INFO):
    """Set up logging configuration for the ras-commander library."""
    global _logging_setup_done
    if _logging_setup_done:
        return
    
    # Define log format
    log_format = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # Configure console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_format)

    # Set up root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(console_handler)

    # Configure file handler if log_file is provided
    if log_file:
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        log_file_path = log_dir / log_file

        file_handler = logging.handlers.RotatingFileHandler(
            log_file_path, maxBytes=10*1024*1024, backupCount=5
        )
        file_handler.setFormatter(log_format)
        root_logger.addHandler(file_handler)
    
    _logging_setup_done = True

def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the specified name.
    
    Args:
        name: The name for the logger, typically __name__ or module path
        
    Returns:
        logging.Logger: Configured logger instance
    """
    logger = logging.getLogger(name)
    if not logger.handlers:  # Only add handler if none exists
        setup_logging()  # Ensure logging is configured
    return logger

def log_call(logger=None):
    """Decorator to log function calls."""
    def get_logger():
        # Check if logger is None or doesn't have a debug method
        if logger is None or not hasattr(logger, 'debug'):
            return logging.getLogger(__name__)
        return logger

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            log = get_logger()
            log.debug(f"Calling {func.__name__}")
            result = func(*args, **kwargs)
            log.debug(f"Finished {func.__name__}")
            return result
        return wrapper
    
    # Check if we're being called as @log_call or @log_call()
    if callable(logger):
        return decorator(logger)
    return decorator

# Set up logging when this module is imported
setup_logging()
==================================================

File: c:\GH\ras-commander\ras_commander\RasCmdr.py
==================================================
"""
RasCmdr - Execution operations for running HEC-RAS simulations

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function():
        
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasCmdr:
- compute_plan()
- compute_parallel()
- compute_test_mode()
        
        
        
"""
import os
import subprocess
import shutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from .RasPrj import ras, RasPrj, init_ras_project, get_ras_exe
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasUtils import RasUtils
import logging
import time
import queue
from threading import Thread, Lock
from typing import Union, List, Optional, Dict
from pathlib import Path
import shutil
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Thread
from itertools import cycle
from ras_commander.RasPrj import RasPrj  # Ensure RasPrj is imported
from threading import Lock, Thread, current_thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from itertools import cycle
from typing import Union, List, Optional, Dict
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

# Module code starts here

# TODO: Future Enhancements
# 1. Alternate Run Mode for compute_plan and compute_parallel:
#    - Use Powershell to execute HEC-RAS command
#    - Hide RAS window and all child windows
#    - Note: This mode may prevent execution if the plan has a popup
#    - Intended for background runs or popup-free scenarios
#    - Limit to non-commercial use
#
# 2. Implement compute_plan_remote:
#    - Execute compute_plan on a remote machine via psexec
#    - Use keyring package for secure credential storage
#    - Implement psexec command for remote HEC-RAS execution
#    - Create remote_worker objects to store machine details:
#      (machine name, username, password, ras_exe_path, local folder path, etc.)
#    - Develop RasRemote class for remote_worker management and abstractions
#    - Implement compute_plan_remote in RasCmdr as a thin wrapper around RasRemote
#      (similar to existing compute_plan functions but for remote execution)


class RasCmdr:
    
    @staticmethod
    @log_call
    def compute_plan(
        plan_number,
        dest_folder=None, 
        ras_object=None,
        clear_geompre=False,
        num_cores=None,
        overwrite_dest=False
    ):
        """
        Execute a HEC-RAS plan.

        Args:
            plan_number (str, Path): The plan number to execute (e.g., "01", "02") or the full path to the plan file.
            dest_folder (str, Path, optional): Name of the folder or full path for computation.
                If a string is provided, it will be created in the same parent directory as the project folder.
                If a full path is provided, it will be used as is.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files. Defaults to False.
            num_cores (int, optional): Number of cores to use for the plan execution. If None, the current setting is not changed.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. Defaults to False.

        Returns:
            bool: True if the execution was successful, False otherwise.

        Raises:
            ValueError: If the specified dest_folder already exists and is not empty, and overwrite_dest is False.
        """
        try:
            ras_obj = ras_object if ras_object is not None else ras
            logger.info(f"Using ras_object with project folder: {ras_obj.project_folder}")
            ras_obj.check_initialized()
            
            if dest_folder is not None:
                dest_folder = Path(ras_obj.project_folder).parent / dest_folder if isinstance(dest_folder, str) else Path(dest_folder)
                
                if dest_folder.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder)
                        logger.info(f"Destination folder '{dest_folder}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder.iterdir()):
                        error_msg = f"Destination folder '{dest_folder}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                
                dest_folder.mkdir(parents=True, exist_ok=True)
                shutil.copytree(ras_obj.project_folder, dest_folder, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder}")
                
                compute_ras = RasPrj()
                compute_ras.initialize(dest_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
            else:
                compute_ras = ras_obj
                compute_prj_path = ras_obj.prj_file

            # Determine the plan path
            compute_plan_path = Path(plan_number) if isinstance(plan_number, (str, Path)) and Path(plan_number).is_file() else RasPlan.get_plan_path(plan_number, compute_ras)

            if not compute_prj_path or not compute_plan_path:
                logger.error(f"Could not find project file or plan file for plan {plan_number}")
                return False

            # Clear geometry preprocessor files if requested
            if clear_geompre:
                try:
                    RasGeo.clear_geompre_files(compute_plan_path, ras_object=compute_ras)
                    logger.info(f"Cleared geometry preprocessor files for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error clearing geometry preprocessor files for plan {plan_number}: {str(e)}")

            # Set the number of cores if specified
            if num_cores is not None:
                try:
                    RasPlan.set_num_cores(compute_plan_path, num_cores=num_cores, ras_object=compute_ras)
                    logger.info(f"Set number of cores to {num_cores} for plan: {plan_number}")
                except Exception as e:
                    logger.error(f"Error setting number of cores for plan {plan_number}: {str(e)}")

            # Prepare the command for HEC-RAS execution
            cmd = f'"{compute_ras.ras_exe_path}" -c "{compute_prj_path}" "{compute_plan_path}"'
            logger.info("Running HEC-RAS from the Command Line:")
            logger.info(f"Running command: {cmd}")

            # Execute the HEC-RAS command
            start_time = time.time()
            try:
                subprocess.run(cmd, check=True, shell=True, capture_output=True, text=True)
                end_time = time.time()
                run_time = end_time - start_time
                logger.info(f"HEC-RAS execution completed for plan: {plan_number}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return True
            except subprocess.CalledProcessError as e:
                end_time = time.time()
                run_time = end_time - start_time
                logger.error(f"Error running plan: {plan_number}")
                logger.error(f"Error message: {e.output}")
                logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")
                return False
        except Exception as e:
            logger.critical(f"Error in compute_plan: {str(e)}")
            return False
        finally:
            # Update the RAS object's dataframes
            if ras_obj:
                ras_obj.plan_df = ras_obj.get_plan_entries()
                ras_obj.geom_df = ras_obj.get_geom_entries()
                ras_obj.flow_df = ras_obj.get_flow_entries()
                ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
    


    @staticmethod
    @log_call
    @staticmethod
    @log_call
    def compute_parallel(
        plan_number: Union[str, List[str], None] = None,
        max_workers: int = 2,
        num_cores: int = 2,
        clear_geompre: bool = False,
        ras_object: Optional['RasPrj'] = None,
        dest_folder: Union[str, Path, None] = None,
        overwrite_dest: bool = False
    ) -> Dict[str, bool]:
        """
        Compute multiple HEC-RAS plans in parallel.

        Args:
            plan_number (Union[str, List[str], None]): Plan number(s) to compute. If None, all plans are computed.
            max_workers (int): Maximum number of parallel workers.
            num_cores (int): Number of cores to use per plan computation.
            clear_geompre (bool): Whether to clear geometry preprocessor files.
            ras_object (Optional[RasPrj]): RAS project object. If None, uses global instance.
            dest_folder (Union[str, Path, None]): Destination folder for computed results.
            overwrite_dest (bool): Whether to overwrite existing destination folder.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()

            project_folder = Path(ras_obj.project_folder)

            if dest_folder is not None:
                dest_folder_path = Path(dest_folder)
                if dest_folder_path.exists():
                    if overwrite_dest:
                        shutil.rmtree(dest_folder_path)
                        logger.info(f"Destination folder '{dest_folder_path}' exists. Overwriting as per overwrite_dest=True.")
                    elif any(dest_folder_path.iterdir()):
                        error_msg = f"Destination folder '{dest_folder_path}' exists and is not empty. Use overwrite_dest=True to overwrite."
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                dest_folder_path.mkdir(parents=True, exist_ok=True)
                shutil.copytree(project_folder, dest_folder_path, dirs_exist_ok=True)
                logger.info(f"Copied project folder to destination: {dest_folder_path}")
                project_folder = dest_folder_path

            if plan_number:
                if isinstance(plan_number, str):
                    plan_number = [plan_number]
                ras_obj.plan_df = ras_obj.plan_df[ras_obj.plan_df['plan_number'].isin(plan_number)]
                logger.info(f"Filtered plans to execute: {plan_number}")

            num_plans = len(ras_obj.plan_df)
            max_workers = min(max_workers, num_plans) if num_plans > 0 else 1
            logger.info(f"Adjusted max_workers to {max_workers} based on the number of plans: {num_plans}")

            worker_ras_objects = {}
            for worker_id in range(1, max_workers + 1):
                worker_folder = project_folder.parent / f"{project_folder.name} [Worker {worker_id}]"
                if worker_folder.exists():
                    shutil.rmtree(worker_folder)
                    logger.info(f"Removed existing worker folder: {worker_folder}")
                shutil.copytree(project_folder, worker_folder)
                logger.info(f"Created worker folder: {worker_folder}")

                try:
                    worker_ras = RasPrj()
                    worker_ras_object = init_ras_project(
                        ras_project_folder=worker_folder,
                        ras_version=ras_obj.ras_exe_path,
                        ras_object=worker_ras
                    )
                    worker_ras_objects[worker_id] = worker_ras_object
                except Exception as e:
                    logger.critical(f"Failed to initialize RAS project for worker {worker_id}: {str(e)}")
                    worker_ras_objects[worker_id] = None

            worker_cycle = cycle(range(1, max_workers + 1))
            plan_assignments = [(next(worker_cycle), plan_num) for plan_num in ras_obj.plan_df['plan_number']]

            execution_results: Dict[str, bool] = {}

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(
                        RasCmdr.compute_plan,
                        plan_num, 
                        ras_object=worker_ras_objects[worker_id], 
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    for worker_id, plan_num in plan_assignments
                ]

                for future, (worker_id, plan_num) in zip(as_completed(futures), plan_assignments):
                    try:
                        success = future.result()
                        execution_results[plan_num] = success
                        logger.info(f"Plan {plan_num} executed in worker {worker_id}: {'Successful' if success else 'Failed'}")
                    except Exception as e:
                        execution_results[plan_num] = False
                        logger.error(f"Plan {plan_num} failed in worker {worker_id}: {str(e)}")

            final_dest_folder = dest_folder_path if dest_folder is not None else project_folder.parent / f"{project_folder.name} [Computed]"
            final_dest_folder.mkdir(parents=True, exist_ok=True)
            logger.info(f"Final destination for computed results: {final_dest_folder}")

            for worker_ras in worker_ras_objects.values():
                if worker_ras is None:
                    continue
                worker_folder = Path(worker_ras.project_folder)
                try:
                    # First, close any open resources in the worker RAS object
                    worker_ras.close() if hasattr(worker_ras, 'close') else None
                    
                    # Add a small delay to ensure file handles are released
                    time.sleep(1)
                    
                    # Move files with retry mechanism
                    max_retries = 3
                    for retry in range(max_retries):
                        try:
                            for item in worker_folder.iterdir():
                                dest_path = final_dest_folder / item.name
                                if dest_path.exists():
                                    if dest_path.is_dir():
                                        shutil.rmtree(dest_path)
                                    else:
                                        dest_path.unlink()
                                # Use copy instead of move for more reliability
                                if item.is_dir():
                                    shutil.copytree(item, dest_path)
                                else:
                                    shutil.copy2(item, dest_path)
                            
                            # Add another small delay before removal
                            time.sleep(1)
                            
                            # Try to remove the worker folder
                            if worker_folder.exists():
                                shutil.rmtree(worker_folder)
                            break  # If successful, break the retry loop
                            
                        except PermissionError as pe:
                            if retry == max_retries - 1:  # If this was the last retry
                                logger.error(f"Failed to move/remove files after {max_retries} attempts: {str(pe)}")
                                raise
                            time.sleep(2 ** retry)  # Exponential backoff
                            continue
                            
                except Exception as e:
                    logger.error(f"Error moving results from {worker_folder} to {final_dest_folder}: {str(e)}")

            try:
                final_dest_folder_ras = RasPrj()
                final_dest_folder_ras_obj = init_ras_project(
                    ras_project_folder=final_dest_folder, 
                    ras_version=ras_obj.ras_exe_path,
                    ras_object=final_dest_folder_ras
                )
                final_dest_folder_ras_obj.check_initialized()
            except Exception as e:
                logger.critical(f"Failed to initialize RasPrj for final destination: {str(e)}")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj = ras_object or ras
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_parallel: {str(e)}")
            return {}

    @staticmethod
    @log_call
    def compute_test_mode(
        plan_number=None, 
        dest_folder_suffix="[Test]", 
        clear_geompre=False, 
        num_cores=None, 
        ras_object=None,
        overwrite_dest=False
    ):
        """
        Execute HEC-RAS plans in test mode. This is a re-creation of the HEC-RAS command line -test flag, 
        which does not work in recent versions of HEC-RAS.
        
        As a special-purpose function that emulates the original -test flag, it operates differently than the 
        other two compute_ functions. Per the original HEC-RAS test flag, it creates a separate test folder,
        copies the project there, and executes the specified plans in sequential order.
        
        For most purposes, just copying the project folder, initing that new folder, then running each plan 
        with compute_plan is a simpler and more flexible approach.  This is shown in the examples provided
        in the ras-commander library.

        Args:
            plan_number (str, list[str], optional): Plan number or list of plan numbers to execute. 
                If None, all plans will be executed. Default is None.
            dest_folder_suffix (str, optional): Suffix to append to the test folder name to create dest_folder. 
                Defaults to "[Test]".
                dest_folder is always created in the project folder's parent directory.
            clear_geompre (bool, optional): Whether to clear geometry preprocessor files.
                Defaults to False.
            num_cores (int, optional): Maximum number of cores to use for each plan.
                If None, the current setting is not changed. Default is None.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
            overwrite_dest (bool, optional): If True, overwrite the destination folder if it exists. Defaults to False.

        Returns:
            Dict[str, bool]: Dictionary of plan numbers and their execution success status.

        Example:
            Run all plans: RasCommander.compute_test_mode()
            Run a specific plan: RasCommander.compute_test_mode(plan_number="01")
            Run multiple plans: RasCommander.compute_test_mode(plan_number=["01", "03", "05"])
            Run plans with a custom folder suffix: RasCommander.compute_test_mode(dest_folder_suffix="[TestRun]")
            Run plans and clear geometry preprocessor files: RasCommander.compute_test_mode(clear_geompre=True)
            Run plans with a specific number of cores: RasCommander.compute_test_mode(num_cores=4)
            
        Notes:
            - This function executes plans in a separate folder for isolated testing.
            - If plan_number is not provided, all plans in the project will be executed.
            - The function does not change the geometry preprocessor and IB tables settings.  
                - To force recomputing of geometry preprocessor and IB tables, use the clear_geompre=True option.
            - Plans are executed sequentially.
            - Because copying the project is implicit, only a dest_folder_suffix option is provided.
            - For more flexible run management, use the compute_parallel or compute_sequential functions.
        """
        try:
            ras_obj = ras_object or ras
            ras_obj.check_initialized()
            
            logger.info("Starting the compute_test_mode...")
               
            project_folder = Path(ras_obj.project_folder)

            if not project_folder.exists():
                logger.error(f"Project folder '{project_folder}' does not exist.")
                return {}

            compute_folder = project_folder.parent / f"{project_folder.name} {dest_folder_suffix}"
            logger.info(f"Creating the test folder: {compute_folder}...")

            if compute_folder.exists():
                if overwrite_dest:
                    shutil.rmtree(compute_folder)
                    logger.info(f"Compute folder '{compute_folder}' exists. Overwriting as per overwrite_dest=True.")
                elif any(compute_folder.iterdir()):
                    error_msg = (
                        f"Compute folder '{compute_folder}' exists and is not empty. "
                        "Use overwrite_dest=True to overwrite."
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg)

            try:
                shutil.copytree(project_folder, compute_folder)
                logger.info(f"Copied project folder to compute folder: {compute_folder}")
            except Exception as e:
                logger.critical(f"Error occurred while copying project folder: {str(e)}")
                return {}

            try:
                compute_ras = RasPrj()
                compute_ras.initialize(compute_folder, ras_obj.ras_exe_path)
                compute_prj_path = compute_ras.prj_file
                logger.info(f"Initialized RAS project in compute folder: {compute_prj_path}")
            except Exception as e:
                logger.critical(f"Error initializing RAS project in compute folder: {str(e)}")
                return {}

            if not compute_prj_path:
                logger.error("Project file not found.")
                return {}

            logger.info("Getting plan entries...")
            try:
                ras_compute_plan_entries = compute_ras.plan_df
                logger.info("Retrieved plan entries successfully.")
            except Exception as e:
                logger.critical(f"Error retrieving plan entries: {str(e)}")
                return {}

            if plan_number:
                if isinstance(plan_number, str):
                    plan_number = [plan_number]
                ras_compute_plan_entries = ras_compute_plan_entries[
                    ras_compute_plan_entries['plan_number'].isin(plan_number)
                ]
                logger.info(f"Filtered plans to execute: {plan_number}")

            execution_results = {}
            logger.info("Running selected plans sequentially...")
            for _, plan in ras_compute_plan_entries.iterrows():
                plan_number = plan["plan_number"]
                start_time = time.time()
                try:
                    success = RasCmdr.compute_plan(
                        plan_number,
                        ras_object=compute_ras,
                        clear_geompre=clear_geompre,
                        num_cores=num_cores
                    )
                    execution_results[plan_number] = success
                    if success:
                        logger.info(f"Successfully computed plan {plan_number}")
                    else:
                        logger.error(f"Failed to compute plan {plan_number}")
                except Exception as e:
                    execution_results[plan_number] = False
                    logger.error(f"Error computing plan {plan_number}: {str(e)}")
                finally:
                    end_time = time.time()
                    run_time = end_time - start_time
                    logger.info(f"Total run time for plan {plan_number}: {run_time:.2f} seconds")

            logger.info("All selected plans have been executed.")
            logger.info("compute_test_mode completed.")

            logger.info("\nExecution Results:")
            for plan_num, success in execution_results.items():
                status = 'Successful' if success else 'Failed'
                logger.info(f"Plan {plan_num}: {status}")

            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

            return execution_results

        except Exception as e:
            logger.critical(f"Error in compute_test_mode: {str(e)}")
            return {}
==================================================

File: c:\GH\ras-commander\ras_commander\RasExamples.py
==================================================
"""
RasExamples - Manage and load HEC-RAS example projects for testing and development

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasExamples:   
- get_example_projects()
- list_categories()
- list_projects()
- extract_project()
- is_project_extracted()
- clean_projects_directory()
        
"""
import os
import requests
import zipfile
import pandas as pd
from pathlib import Path
import shutil
from typing import Union, List
import csv
from datetime import datetime
import logging
import re
from tqdm import tqdm
from ras_commander import get_logger
from ras_commander.LoggingConfig import log_call

logger = get_logger(__name__)

class RasExamples:
    """
    A class for quickly loading HEC-RAS example projects for testing and development of ras-commander.
    All methods are class methods, so no initialization is required.
    """
    base_url = 'https://github.com/HydrologicEngineeringCenter/hec-downloads/releases/download/'
    valid_versions = [
            "6.6", "6.5", "6.4.1", "6.3.1", "6.3", "6.2", "6.1", "6.0",
            "5.0.7", "5.0.6", "5.0.5", "5.0.4", "5.0.3", "5.0.1", "5.0",
            "4.1", "4.0", "3.1.3", "3.1.2", "3.1.1", "3.0", "2.2"
        ]
    base_dir = Path.cwd()
    examples_dir = base_dir
    projects_dir = examples_dir / 'example_projects'
    csv_file_path = examples_dir / 'example_projects.csv'

    _folder_df = None
    _zip_file_path = None

    def __init__(self):
        """Initialize RasExamples and ensure data is loaded"""
        self._ensure_initialized()

    @property
    def folder_df(self):
        """Access the folder DataFrame"""
        self._ensure_initialized()
        return self._folder_df

    def _ensure_initialized(self):
        """Ensure the class is initialized with required data"""
        self.projects_dir.mkdir(parents=True, exist_ok=True)
        if self._folder_df is None:
            self._load_project_data()

    def _load_project_data(self):
        """Load project data from CSV if up-to-date, otherwise extract from zip."""
        logger.debug("Loading project data")
        self._find_zip_file()
        
        if not self._zip_file_path:
            logger.info("No example projects zip file found. Downloading...")
            self.get_example_projects()
        
        try:
            zip_modified_time = os.path.getmtime(self._zip_file_path)
        except FileNotFoundError:
            logger.error(f"Zip file not found at {self._zip_file_path}.")
            return
        
        if self.csv_file_path.exists():
            csv_modified_time = os.path.getmtime(self.csv_file_path)
            
            if csv_modified_time >= zip_modified_time:
                logger.info("Loading project data from CSV...")
                try:
                    self._folder_df = pd.read_csv(self.csv_file_path)
                    logger.info(f"Loaded {len(self._folder_df)} projects from CSV.")
                    return
                except Exception as e:
                    logger.error(f"Failed to read CSV file: {e}")
                    self._folder_df = None

        logger.info("Extracting folder structure from zip file...")
        self._extract_folder_structure()
        self._save_to_csv()

    @classmethod
    def extract_project(cls, project_names: Union[str, List[str]]) -> Union[Path, List[Path]]:
        """Extract one or more specific HEC-RAS projects from the zip file.
        
        Args:
            project_names: Single project name as string or list of project names
            
        Returns:
            Path: Single Path object if one project extracted
            List[Path]: List of Path objects if multiple projects extracted
        """
        logger.debug(f"Extracting projects: {project_names}")
        
        # Initialize if needed
        if cls._folder_df is None:
            cls._find_zip_file()
            if not cls._zip_file_path:
                logger.info("No example projects zip file found. Downloading...")
                cls.get_example_projects()
            cls._load_project_data()
        
        if isinstance(project_names, str):
            project_names = [project_names]

        extracted_paths = []

        for project_name in project_names:
            logger.info("----- RasExamples Extracting Project -----")
            logger.info(f"Extracting project '{project_name}'")
            project_path = cls.projects_dir

            if (project_path / project_name).exists():
                logger.info(f"Project '{project_name}' already exists. Deleting existing folder...")
                try:
                    shutil.rmtree(project_path / project_name)
                    logger.info(f"Existing folder for project '{project_name}' has been deleted.")
                except Exception as e:
                    logger.error(f"Failed to delete existing project folder '{project_name}': {e}")
                    continue

            project_info = cls._folder_df[cls._folder_df['Project'] == project_name]
            if project_info.empty:
                error_msg = f"Project '{project_name}' not found in the zip file."
                logger.error(error_msg)
                raise ValueError(error_msg)

            try:
                with zipfile.ZipFile(cls._zip_file_path, 'r') as zip_ref:
                    for file in zip_ref.namelist():
                        parts = Path(file).parts
                        if len(parts) > 1 and parts[1] == project_name:
                            relative_path = Path(*parts[1:])
                            extract_path = project_path / relative_path
                            if file.endswith('/'):
                                extract_path.mkdir(parents=True, exist_ok=True)
                            else:
                                extract_path.parent.mkdir(parents=True, exist_ok=True)
                                with zip_ref.open(file) as source, open(extract_path, "wb") as target:
                                    shutil.copyfileobj(source, target)

                logger.info(f"Successfully extracted project '{project_name}' to {project_path / project_name}")
                extracted_paths.append(project_path / project_name)
            except Exception as e:
                logger.error(f"An error occurred while extracting project '{project_name}': {str(e)}")

        # Return single path if only one project was extracted, otherwise return list
        return extracted_paths[0] if len(project_names) == 1 else extracted_paths

    @classmethod
    def _find_zip_file(cls):
        """Locate the example projects zip file in the examples directory."""
        for version in cls.valid_versions:
            potential_zip = cls.examples_dir / f"Example_Projects_{version.replace('.', '_')}.zip"
            if potential_zip.exists():
                cls._zip_file_path = potential_zip
                logger.info(f"Found zip file: {cls._zip_file_path}")
                break
        else:
            logger.warning("No existing example projects zip file found.")

    @classmethod
    def get_example_projects(cls, version_number='6.6'):
        """
        Download and extract HEC-RAS example projects for a specified version.
        """
        logger.info(f"Getting example projects for version {version_number}")
        if version_number not in cls.valid_versions:
            error_msg = f"Invalid version number. Valid versions are: {', '.join(cls.valid_versions)}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        zip_url = f"{cls.base_url}1.0.33/Example_Projects_{version_number.replace('.', '_')}.zip"
        
        cls.examples_dir.mkdir(parents=True, exist_ok=True)
        
        cls._zip_file_path = cls.examples_dir / f"Example_Projects_{version_number.replace('.', '_')}.zip"

        if not cls._zip_file_path.exists():
            logger.info(f"Downloading HEC-RAS Example Projects from {zip_url}. \nThe file is over 400 MB, so it may take a few minutes to download....")
            try:
                response = requests.get(zip_url, stream=True)
                response.raise_for_status()
                with open(cls._zip_file_path, 'wb') as file:
                    shutil.copyfileobj(response.raw, file)
                logger.info(f"Downloaded to {cls._zip_file_path}")
            except requests.exceptions.RequestException as e:
                logger.error(f"Failed to download the zip file: {e}")
                raise
        else:
            logger.info("HEC-RAS Example Projects zip file already exists. Skipping download.")

        cls._load_project_data()
        return cls.projects_dir

    @classmethod
    def _load_project_data(cls):
        """Load project data from CSV if up-to-date, otherwise extract from zip."""
        logger.debug("Loading project data")
        
        try:
            zip_modified_time = os.path.getmtime(cls._zip_file_path)
        except FileNotFoundError:
            logger.error(f"Zip file not found at {cls._zip_file_path}.")
            return
        
        if cls.csv_file_path.exists():
            csv_modified_time = os.path.getmtime(cls.csv_file_path)
            
            if csv_modified_time >= zip_modified_time:
                logger.info("Loading project data from CSV...")
                try:
                    cls._folder_df = pd.read_csv(cls.csv_file_path)
                    logger.info(f"Loaded {len(cls._folder_df)} projects from CSV.")
                    return
                except Exception as e:
                    logger.error(f"Failed to read CSV file: {e}")
                    cls._folder_df = None

        logger.info("Extracting folder structure from zip file...")
        cls._extract_folder_structure()
        cls._save_to_csv()

    @classmethod
    def _extract_folder_structure(cls):
        """
        Extract folder structure from the zip file.

        Populates folder_df with category and project information.
        """
        folder_data = []
        try:
            with zipfile.ZipFile(cls._zip_file_path, 'r') as zip_ref:
                for file in zip_ref.namelist():
                    parts = Path(file).parts
                    if len(parts) > 1:
                        folder_data.append({
                            'Category': parts[0],
                            'Project': parts[1]
                        })
        
            cls._folder_df = pd.DataFrame(folder_data).drop_duplicates()
            logger.info(f"Extracted {len(cls._folder_df)} projects.")
            logger.debug(f"folder_df:\n{cls._folder_df}")
        except zipfile.BadZipFile:
            logger.error(f"The file {cls._zip_file_path} is not a valid zip file.")
            cls._folder_df = pd.DataFrame(columns=['Category', 'Project'])
        except Exception as e:
            logger.error(f"An error occurred while extracting the folder structure: {str(e)}")
            cls._folder_df = pd.DataFrame(columns=['Category', 'Project'])

    @classmethod
    def _save_to_csv(cls):
        """Save the extracted folder structure to CSV file."""
        if cls._folder_df is not None and not cls._folder_df.empty:
            try:
                cls._folder_df.to_csv(cls.csv_file_path, index=False)
                logger.info(f"Saved project data to {cls.csv_file_path}")
            except Exception as e:
                logger.error(f"Failed to save project data to CSV: {e}")
        else:
            logger.warning("No folder data to save to CSV.")

    @classmethod
    def list_categories(cls):
        """
        List all categories of example projects.
        """
        if cls._folder_df is None or 'Category' not in cls._folder_df.columns:
            logger.warning("No categories available. Make sure the zip file is properly loaded.")
            return []
        categories = cls._folder_df['Category'].unique()
        logger.info(f"Available categories: {', '.join(categories)}")
        return categories.tolist()

    @classmethod
    def list_projects(cls, category=None):
        """
        List all projects or projects in a specific category.
        """
        if cls._folder_df is None:
            logger.warning("No projects available. Make sure the zip file is properly loaded.")
            return []
        if category:
            projects = cls._folder_df[cls._folder_df['Category'] == category]['Project'].unique()
            logger.info(f"Projects in category '{category}': {', '.join(projects)}")
        else:
            projects = cls._folder_df['Project'].unique()
            logger.info(f"All available projects: {', '.join(projects)}")
        return projects.tolist()

    @classmethod
    def is_project_extracted(cls, project_name):
        """
        Check if a specific project is already extracted.
        """
        project_path = cls.projects_dir / project_name
        is_extracted = project_path.exists()
        logger.info(f"Project '{project_name}' extracted: {is_extracted}")
        return is_extracted

    @classmethod
    def clean_projects_directory(cls):
        """Remove all extracted projects from the example_projects directory."""
        logger.info(f"Cleaning projects directory: {cls.projects_dir}")
        if cls.projects_dir.exists():
            try:
                shutil.rmtree(cls.projects_dir)
                logger.info("All projects have been removed.")
            except Exception as e:
                logger.error(f"Failed to remove projects directory: {e}")
        else:
            logger.warning("Projects directory does not exist.")
        cls.projects_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Projects directory cleaned and recreated.")

    @classmethod
    def download_fema_ble_model(cls, huc8, output_dir=None):
        """
        Download a FEMA Base Level Engineering (BLE) model for a given HUC8.

        Args:
            huc8 (str): The 8-digit Hydrologic Unit Code (HUC) for the desired watershed.
            output_dir (str, optional): The directory to save the downloaded files. If None, uses the current working directory.

        Returns:
            str: The path to the downloaded and extracted model directory.

        Note:
            This method downloads the BLE model from the FEMA website and extracts it to the specified directory.
        """
        # Method implementation...

    @classmethod
    def _make_safe_folder_name(cls, name: str) -> str:
        """
        Convert a string to a safe folder name by replacing unsafe characters with underscores.
        """
        safe_name = re.sub(r'[^a-zA-Z0-9_\-]', '_', name)
        logger.debug(f"Converted '{name}' to safe folder name '{safe_name}'")
        return safe_name

    @classmethod
    def _download_file_with_progress(cls, url: str, dest_folder: Path, file_size: int) -> Path:
        """
        Download a file from a URL to a specified destination folder with progress bar.
        """
        local_filename = dest_folder / url.split('/')[-1]
        try:
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                with open(local_filename, 'wb') as f, tqdm(
                    desc=local_filename.name,
                    total=file_size,
                    unit='iB',
                    unit_scale=True,
                    unit_divisor=1024,
                ) as progress_bar:
                    for chunk in r.iter_content(chunk_size=8192):
                        size = f.write(chunk)
                        progress_bar.update(size)
            logger.info(f"Successfully downloaded {url} to {local_filename}")
            return local_filename
        except requests.exceptions.RequestException as e:
            logger.error(f"Request failed for {url}: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to write file {local_filename}: {e}")
            raise

    @classmethod
    def _convert_size_to_bytes(cls, size_str: str) -> int:
        """
        Convert a human-readable file size to bytes.
        """
        units = {'B': 1, 'KB': 1024, 'MB': 1024**2, 'GB': 1024**3, 'TB': 1024**4}
        size_str = size_str.upper().replace(' ', '')
        if not re.match(r'^\d+(\.\d+)?[BKMGT]B?$', size_str):
            raise ValueError(f"Invalid size string: {size_str}")
        
        number, unit = float(re.findall(r'[\d\.]+', size_str)[0]), re.findall(r'[BKMGT]B?', size_str)[0]
        return int(number * units[unit])
==================================================

File: c:\GH\ras-commander\ras_commander\RasGeo.py
==================================================
"""
RasGeo - Operations for handling geometry files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasGeo:
- clear_geompre_files()
        
        
"""
import os
from pathlib import Path
from typing import List, Union
from .RasPlan import RasPlan
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasGeo:
    """
    A class for operations on HEC-RAS geometry files.
    """
    
    @staticmethod
    @log_call
    def clear_geompre_files(
        plan_files: Union[str, Path, List[Union[str, Path]]] = None,
        ras_object = None
    ) -> None:
        """
        Clear HEC-RAS geometry preprocessor files for specified plan files or all plan files in the project directory.
        
        Limitations/Future Work:
        - This function only deletes the geometry preprocessor file.
        - It does not clear the IB tables.
        - It also does not clear geometry preprocessor tables from the geometry HDF.
        - All of these features will need to be added to reliably remove geometry preprocessor files for 1D and 2D projects.
        
        Parameters:
            plan_files (Union[str, Path, List[Union[str, Path]]], optional): 
                Full path(s) to the HEC-RAS plan file(s) (.p*).
                If None, clears all plan files in the project directory.
            ras_object: An optional RAS object instance.
        
        Returns:
            None
        
        Examples:
            # Clear all geometry preprocessor files in the project directory
            RasGeo.clear_geompre_files()
            
            # Clear a single plan file
            RasGeo.clear_geompre_files(r'path/to/plan.p01')
            
            # Clear multiple plan files
            RasGeo.clear_geompre_files([r'path/to/plan1.p01', r'path/to/plan2.p02'])

        Note:
            This function updates the ras object's geometry dataframe after clearing the preprocessor files.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        def clear_single_file(plan_file: Union[str, Path], ras_obj) -> None:
            plan_path = Path(plan_file)
            geom_preprocessor_suffix = '.c' + ''.join(plan_path.suffixes[1:]) if plan_path.suffixes else '.c'
            geom_preprocessor_file = plan_path.with_suffix(geom_preprocessor_suffix)
            if geom_preprocessor_file.exists():
                try:
                    geom_preprocessor_file.unlink()
                    logger.info(f"Deleted geometry preprocessor file: {geom_preprocessor_file}")
                except PermissionError:
                    logger.error(f"Permission denied: Unable to delete geometry preprocessor file: {geom_preprocessor_file}")
                    raise PermissionError(f"Unable to delete geometry preprocessor file: {geom_preprocessor_file}. Permission denied.")
                except OSError as e:
                    logger.error(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
                    raise OSError(f"Error deleting geometry preprocessor file: {geom_preprocessor_file}. {str(e)}")
            else:
                logger.warning(f"No geometry preprocessor file found for: {plan_file}")
        
        if plan_files is None:
            logger.info("Clearing all geometry preprocessor files in the project directory.")
            plan_files_to_clear = list(ras_obj.project_folder.glob(r'*.p*'))
        elif isinstance(plan_files, (str, Path)):
            plan_files_to_clear = [plan_files]
            logger.info(f"Clearing geometry preprocessor file for single plan: {plan_files}")
        elif isinstance(plan_files, list):
            plan_files_to_clear = plan_files
            logger.info(f"Clearing geometry preprocessor files for multiple plans: {plan_files}")
        else:
            logger.error("Invalid input type for plan_files.")
            raise ValueError("Invalid input. Please provide a string, Path, list of paths, or None.")
        
        for plan_file in plan_files_to_clear:
            clear_single_file(plan_file, ras_obj)
        
        try:
            ras_obj.geom_df = ras_obj.get_geom_entries()
            logger.info("Geometry dataframe updated successfully.")
        except Exception as e:
            logger.error(f"Failed to update geometry dataframe: {str(e)}")
            raise









==================================================

File: c:\GH\ras-commander\ras_commander\RasGpt.py
==================================================
import os
from pathlib import Path
from typing import Optional
from ras_commander import get_logger, log_call

logger = get_logger(__name__)

class RasGpt:
    """
    A class containing helper functions for the RAS Commander GPT.
    """
    
# to be implemented later
# 
# This class will contain  methods to help LLM's extract useful information from HEC-RAS models in a structured format with token budget etc. 
# Templates will be used to help with this, based on the example projects (1D Steady, 1D Usteady, 1D Sediment Transport, 1D Water Quality, 2D Unsteady, 2D Steady, 2D Sediment Transport, 2D Water Quality, 2D Geospatial, 3D Unsteady, 3D Steady, 3D Sediment Transport, 3D Water Quality, 3D Geospatial).
# These will simply filter the data to only include the relevant information for the area of focus. 

#
# IDEAS
# 1. Package up a standard set of information for LLM analysis
#       - General project information
#       - Cross section information (for specific cross sections)
#       - Structure information (for specific structures)
#       - Include time series results and relevant HEC Guidance for LLM to reference

# 2. Use Library Assistant to call LLM 

==================================================

File: c:\GH\ras-commander\ras_commander\RasMapper.py
==================================================
"""
Class: RasMapper

List of Functions:
    get_raster_map(hdf_path: Path) 
    clip_raster_with_boundary(raster_path: Path, boundary_path: Path) 
    calculate_zonal_stats(boundary_path: Path, raster_data, transform, nodata) 

"""



from pathlib import Path
import pandas as pd
import geopandas as gpd
import rasterio
from rasterio.mask import mask
import h5py
from .Decorators import log_call, standardize_input
from .HdfInfiltration import HdfInfiltration

class RasMapper:
    """Class for handling RAS Mapper operations and data extraction"""

    @staticmethod
    @log_call
    def get_raster_map(hdf_path: Path) -> dict:
        """Read the raster map from HDF file and create value mapping
        
        Args:
            hdf_path: Path to the HDF file
            
        Returns:
            Dictionary mapping raster values to mukeys
        """
        with h5py.File(hdf_path, 'r') as hdf:
            raster_map_data = hdf['Raster Map'][:]
            return {int(item[0]): item[1].decode('utf-8') for item in raster_map_data}

    @staticmethod
    @log_call 
    def clip_raster_with_boundary(raster_path: Path, boundary_path: Path):
        """Clip a raster using a boundary polygon
        
        Args:
            raster_path: Path to the raster file
            boundary_path: Path to the boundary shapefile
            
        Returns:
            Tuple of (clipped_image, transform, nodata_value)
        """
        watershed = gpd.read_file(boundary_path)
        raster = rasterio.open(raster_path)
        
        out_image, out_transform = mask(raster, watershed.geometry, crop=True)
        nodata = raster.nodatavals[0]
        
        return out_image[0], out_transform, nodata

    @staticmethod
    @log_call
    def calculate_zonal_stats(boundary_path: Path, raster_data, transform, nodata):
        """Calculate zonal statistics for a boundary
        
        Args:
            boundary_path: Path to boundary shapefile
            raster_data: Numpy array of raster values
            transform: Raster transform
            nodata: Nodata value
            
        Returns:
            List of statistics by zone
        """
        watershed = gpd.read_file(boundary_path)
        return zonal_stats(watershed, raster_data, 
                         affine=transform, 
                         nodata=nodata,
                         categorical=True)

# Example usage:
"""
# Initialize paths
raster_path = Path('input_files/gSSURGO_InfiltrationDC.tif')
boundary_path = Path('input_files/WF_Boundary_Simple.shp')
hdf_path = raster_path.with_suffix('.hdf')

# Get raster mapping
raster_map = RasMapper.get_raster_map(hdf_path)

# Clip raster with boundary
clipped_data, transform, nodata = RasMapper.clip_raster_with_boundary(
    raster_path, boundary_path)

# Calculate zonal statistics
stats = RasMapper.calculate_zonal_stats(
    boundary_path, clipped_data, transform, nodata)

# Calculate soil statistics
soil_stats = HdfInfiltration.calculate_soil_statistics(
    stats, raster_map)

# Get significant mukeys (>1%)
significant_mukeys = HdfInfiltration.get_significant_mukeys(
    soil_stats, threshold=1.0)
"""
==================================================

File: c:\GH\ras-commander\ras_commander\RasPlan.py
==================================================
"""
RasPlan - Operations for handling plan files in HEC-RAS projects

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).
3. Obtain the logger using: logger = logging.getLogger(__name__)

Example:
    @log_call
    def my_function():
        logger = logging.getLogger(__name__)
        logger.debug("Additional debug information")
        # Function logic here
        
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasPlan:
- set_geom(): Set the geometry for a specified plan
- set_steady(): Apply a steady flow file to a plan file
- set_unsteady(): Apply an unsteady flow file to a plan file
- set_num_cores(): Update the maximum number of cores to use
- set_geom_preprocessor(): Update geometry preprocessor settings
- clone_plan(): Create a new plan file based on a template
- clone_unsteady(): Copy unsteady flow files from a template
- clone_steady(): Copy steady flow files from a template
- clone_geom(): Copy geometry files from a template
- get_next_number(): Determine the next available number from a list
- get_plan_value(): Retrieve a specific value from a plan file
- get_results_path(): Get the results file path for a plan
- get_plan_path(): Get the full path for a plan number
- get_flow_path(): Get the full path for a flow number
- get_unsteady_path(): Get the full path for an unsteady number
- get_geom_path(): Get the full path for a geometry number
- update_run_flags(): Update various run flags in a plan file
- update_plan_intervals(): Update computation and output intervals
- update_plan_description(): Update the description in a plan file
- read_plan_description(): Read the description from a plan file
- update_simulation_date(): Update simulation start and end dates

        
"""
import os
import re
import logging
from pathlib import Path
import shutil
from typing import Union, Optional
import pandas as pd
from .RasPrj import RasPrj, ras
from .RasUtils import RasUtils
from pathlib import Path
from typing import Union, Any
from datetime import datetime

import logging
import re
from .LoggingConfig import get_logger
from .Decorators import log_call

logger = get_logger(__name__)

class RasPlan:
    """
    A class for operations on HEC-RAS plan files.
    """
    
    @staticmethod
    @log_call
    def set_geom(plan_number: Union[str, int], new_geom: Union[str, int], ras_object=None) -> pd.DataFrame:
        """
        Set the geometry for the specified plan.

        Parameters:
            plan_number (Union[str, int]): The plan number to update.
            new_geom (Union[str, int]): The new geometry number to set.
            ras_object: An optional RAS object instance.

        Returns:
            pd.DataFrame: The updated geometry DataFrame.

        Example:
            updated_geom_df = RasPlan.set_geom('02', '03')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Ensure plan_number and new_geom are strings
        plan_number = str(plan_number).zfill(2)
        new_geom = str(new_geom).zfill(2)

        # Before doing anything, make sure the plan, geom, flow, and unsteady dataframes are current
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        if new_geom not in ras_obj.geom_df['geom_number'].values:
            logger.error(f"Geometry {new_geom} not found in project.")
            raise ValueError(f"Geometry {new_geom} not found in project.")

        # Update the geometry for the specified plan
        ras_obj.plan_df.loc[ras_obj.plan_df['plan_number'] == plan_number, 'geom_number'] = new_geom

        logger.info(f"Geometry for plan {plan_number} set to {new_geom}")
        logger.debug("Updated plan DataFrame:")
        logger.debug(ras_obj.plan_df)

        # Update the project file
        prj_file_path = ras_obj.prj_file
        RasUtils.update_file(prj_file_path, RasPlan._update_geom_in_file, plan_number, new_geom)

        # Re-initialize the ras object to reflect changes
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        return ras_obj.plan_df

    @staticmethod
    def _update_geom_in_file(lines, plan_number, new_geom):
        plan_pattern = re.compile(rf"^Plan File=p{plan_number}", re.IGNORECASE)
        geom_pattern = re.compile(r"^Geom File=g\d+", re.IGNORECASE)
        
        for i, line in enumerate(lines):
            if plan_pattern.match(line):
                for j in range(i+1, len(lines)):
                    if geom_pattern.match(lines[j]):
                        lines[j] = f"Geom File=g{new_geom}\n"
                        logger.info(f"Updated Geom File in project file to g{new_geom} for plan {plan_number}")
                        break
                break
        return lines

    @staticmethod
    @log_call
    def set_steady(plan_number: str, new_steady_flow_number: str, ras_object=None):
        """
        Apply a steady flow file to a plan file.
        
        Parameters:
        plan_number (str): Plan number (e.g., '02')
        new_steady_flow_number (str): Steady flow number to apply (e.g., '01')
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If the specified steady flow number is not found in the project file
        FileNotFoundError: If the specified plan file is not found

        Example:
        >>> RasPlan.set_steady('02', '01')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
                        
        # Update the flow dataframe in the ras instance to ensure it is current
        ras_obj.flow_df = ras_obj.get_flow_entries()
        
        if new_steady_flow_number not in ras_obj.flow_df['flow_number'].values:
            raise ValueError(f"Steady flow number {new_steady_flow_number} not found in project file.")
        
        # Resolve the full path of the plan file
        plan_file_path = RasPlan.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}")
        
        RasUtils.update_file(plan_file_path, RasPlan._update_steady_in_file, new_steady_flow_number)

        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    def _update_steady_in_file(lines, new_steady_flow_number):
        return [f"Flow File=f{new_steady_flow_number}\n" if line.startswith("Flow File=f") else line for line in lines]

    @staticmethod
    @log_call
    def set_unsteady(plan_number: str, new_unsteady_flow_number: str, ras_object=None):
        """
        Apply an unsteady flow file to a plan file.
        
        Parameters:
        plan_number (str): Plan number (e.g., '04')
        new_unsteady_flow_number (str): Unsteady flow number to apply (e.g., '01')
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If the specified unsteady number is not found in the project file
        FileNotFoundError: If the specified plan file is not found

        Example:
        >>> RasPlan.set_unsteady('04', '01')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Update the unsteady dataframe in the ras instance to ensure it is current
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        if new_unsteady_flow_number not in ras_obj.unsteady_df['unsteady_number'].values:
            raise ValueError(f"Unsteady number {new_unsteady_flow_number} not found in project file.")
        
        # Get the full path of the plan file
        plan_file_path = RasPlan.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}")
        
        try:
            RasUtils.update_file(plan_file_path, RasPlan._update_unsteady_in_file, new_unsteady_flow_number)
        except Exception as e:
            raise Exception(f"Failed to update unsteady flow file: {e}")

        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    def _update_unsteady_in_file(lines, new_unsteady_flow_number):
        return [f"Unsteady File=u{new_unsteady_flow_number}\n" if line.startswith("Unsteady File=u") else line for line in lines]
    
    @staticmethod
    @log_call
    def set_num_cores(plan_number, num_cores, ras_object=None):
        """
        Update the maximum number of cores to use in the HEC-RAS plan file.
        
        Parameters:
        plan_number (str): Plan number (e.g., '02') or full path to the plan file
        num_cores (int): Maximum number of cores to use
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Number of cores is controlled by the following parameters in the plan file corresponding to 1D, 2D, Pipe Systems and Pump Stations:
        UNET D1 Cores=  
        UNET D2 Cores=
        PS Cores=

        Where a value of "0" is used for "All Available" cores, and values of 1 or more are used to specify the number of cores to use.
        For complex 1D/2D models with pipe systems, a more complex approach may be needed to optimize performance.  (Suggest writing a custom function based on this code).
        This function simply sets the "num_cores" parameter for ALL instances of the above parameters in the plan file.


        Notes on setting num_cores in HEC-RAS:
        The recommended setting for num_cores is 2 (most efficient) to 8 (most performant)
        More details in the HEC-Commander Repository Blog "Benchmarking is All You Need"
        https://github.com/billk-FM/HEC-Commander/blob/main/Blog/7._Benchmarking_Is_All_You_Need.md
        
        Microsoft Windows has a maximum of 64 cores that can be allocated to a single Ras.exe process. 

        Example:
        >>> # Using plan number
        >>> RasPlan.set_num_cores('02', 4)
        >>> # Using full path to plan file
        >>> RasPlan.set_num_cores('/path/to/project.p02', 4)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        plan_file_path = RasUtils.get_plan_path(plan_number, ras_obj)
        if not plan_file_path:
            raise FileNotFoundError(f"Plan file not found: {plan_number}. Please provide a valid plan number or path.")
        
        def update_num_cores(lines):
            updated_lines = []
            for line in lines:
                if any(param in line for param in ["UNET D1 Cores=", "UNET D2 Cores=", "PS Cores="]):
                    param_name = line.split("=")[0]
                    updated_lines.append(f"{param_name}= {num_cores}\n")
                else:
                    updated_lines.append(line)
            return updated_lines
        
        try:
            RasUtils.update_file(plan_file_path, update_num_cores)
        except Exception as e:
            raise IOError(f"Failed to update number of cores in plan file: {e}")
        
        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def set_geom_preprocessor(file_path, run_htab, use_ib_tables, ras_object=None):
        """
        Update the simulation plan file to modify the `Run HTab` and `UNET Use Existing IB Tables` settings.
        
        Parameters:
        file_path (str): Path to the simulation plan file (.p06 or similar) that you want to modify.
        run_htab (int): Value for the `Run HTab` setting:
            - `0` : Do not run the geometry preprocessor, use existing geometry tables.
            - `-1` : Run the geometry preprocessor, forcing a recomputation of the geometry tables.
        use_ib_tables (int): Value for the `UNET Use Existing IB Tables` setting:
            - `0` : Use existing interpolation/boundary (IB) tables without recomputing them.
            - `-1` : Do not use existing IB tables, force a recomputation.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Raises:
        ValueError: If `run_htab` or `use_ib_tables` are not integers or not within the accepted values (`0` or `-1`).
        FileNotFoundError: If the specified file does not exist.
        IOError: If there is an error reading or writing the file.

        Example:
        >>> RasPlan.set_geom_preprocessor('/path/to/project.p06', run_htab=-1, use_ib_tables=0)

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        if run_htab not in [-1, 0]:
            raise ValueError("Invalid value for `Run HTab`. Expected `0` or `-1`.")
        if use_ib_tables not in [-1, 0]:
            raise ValueError("Invalid value for `UNET Use Existing IB Tables`. Expected `0` or `-1`.")
        
        def update_geom_preprocessor(lines, run_htab, use_ib_tables):
            updated_lines = []
            for line in lines:
                if line.lstrip().startswith("Run HTab="):
                    updated_lines.append(f"Run HTab= {run_htab} \n")
                elif line.lstrip().startswith("UNET Use Existing IB Tables="):
                    updated_lines.append(f"UNET Use Existing IB Tables= {use_ib_tables} \n")
                else:
                    updated_lines.append(line)
            return updated_lines
        
        try:
            RasUtils.update_file(file_path, update_geom_preprocessor, run_htab, use_ib_tables)
        except FileNotFoundError:
            raise FileNotFoundError(f"The file '{file_path}' does not exist.")
        except IOError as e:
            raise IOError(f"An error occurred while reading or writing the file: {e}")

        # Update the ras object's dataframes
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def get_results_path(plan_number: str, ras_object=None) -> Optional[str]:
        """
        Retrieve the results file path for a given HEC-RAS plan number.

        Args:
            plan_number (str): The HEC-RAS plan number for which to find the results path.
            ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
            Optional[str]: The full path to the results file if found and the file exists, or None if not found.

        Raises:
            RuntimeError: If the project is not initialized.

        Example:
            >>> ras_plan = RasPlan()
            >>> results_path = ras_plan.get_results_path('01')
            >>> if results_path:
            ...     print(f"Results file found at: {results_path}")
            ... else:
            ...     print("Results file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Update the plan dataframe in the ras instance to ensure it is current
        ras_obj.plan_df = ras_obj.get_plan_entries()
        
        # Ensure plan_number is a string
        plan_number = str(plan_number).zfill(2)
        
        plan_entry = ras_obj.plan_df[ras_obj.plan_df['plan_number'] == plan_number]
        if not plan_entry.empty:
            results_path = plan_entry['HDF_Results_Path'].iloc[0]
            if results_path and Path(results_path).exists():
                return results_path
            else:
                return None
        else:
            return None

    @staticmethod
    @log_call
    def get_plan_path(plan_number: str, ras_object=None) -> Optional[str]:
        """
        Return the full path for a given plan number.
        
        This method ensures that the latest plan entries are included by refreshing
        the plan dataframe before searching for the requested plan number.
        
        Args:
        plan_number (str): The plan number to search for.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        Optional[str]: The full path of the plan file if found, None otherwise.
        
        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> plan_path = ras_plan.get_plan_path('01')
        >>> if plan_path:
        ...     print(f"Plan file found at: {plan_path}")
        ... else:
        ...     print("Plan file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Use updated plan dataframe
        plan_df = ras_obj.get_plan_entries()
        
        plan_path = plan_df[plan_df['plan_number'] == plan_number]
        
        if not plan_path.empty:
            full_path = plan_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_flow_path(flow_number: str, ras_object=None) -> Optional[str]:
        """
        Return the full path for a given flow number.

        Args:
        flow_number (str): The flow number to search for.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the flow file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> flow_path = ras_plan.get_flow_path('01')
        >>> if flow_path:
        ...     print(f"Flow file found at: {flow_path}")
        ... else:
        ...     print("Flow file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Use updated flow dataframe
        ras_obj.flow_df = ras_obj.get_prj_entries('Flow')
        
        flow_path = ras_obj.flow_df[ras_obj.flow_df['flow_number'] == flow_number]
        if not flow_path.empty:
            full_path = flow_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_unsteady_path(unsteady_number: str, ras_object=None) -> Optional[str]:
        """
        Return the full path for a given unsteady number.

        Args:
        unsteady_number (str): The unsteady number to search for.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the unsteady file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> unsteady_path = ras_plan.get_unsteady_path('01')
        >>> if unsteady_path:
        ...     print(f"Unsteady file found at: {unsteady_path}")
        ... else:
        ...     print("Unsteady file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Use updated unsteady dataframe
        ras_obj.unsteady_df = ras_obj.get_prj_entries('Unsteady')
        
        unsteady_path = ras_obj.unsteady_df[ras_obj.unsteady_df['unsteady_number'] == unsteady_number]
        if not unsteady_path.empty:
            full_path = unsteady_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    @staticmethod
    @log_call
    def get_geom_path(geom_number: str, ras_object=None) -> Optional[str]:
        """
        Return the full path for a given geometry number.

        Args:
        geom_number (str): The geometry number to search for.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Optional[str]: The full path of the geometry file if found, None otherwise.

        Raises:
        RuntimeError: If the project is not initialized.

        Example:
        >>> ras_plan = RasPlan()
        >>> geom_path = ras_plan.get_geom_path('01')
        >>> if geom_path:
        ...     print(f"Geometry file found at: {geom_path}")
        ... else:
        ...     print("Geometry file not found.")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        # Use updated geom dataframe
        ras_obj.geom_df = ras_obj.get_prj_entries('Geom')
        
        geom_path = ras_obj.geom_df[ras_obj.geom_df['geom_number'] == geom_number]
        if not geom_path.empty:
            full_path = geom_path['full_path'].iloc[0]
            return full_path
        else:
            return None

    # Clone Functions to copy unsteady, flow, and geometry files from templates

    @staticmethod
    @log_call
    def clone_plan(template_plan, new_plan_shortid=None, ras_object=None):
        """
        Create a new plan file based on a template and update the project file.
        
        Parameters:
        template_plan (str): Plan number to use as template (e.g., '01')
        new_plan_shortid (str, optional): New short identifier for the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        str: New plan number
        
        Example:
        >>> ras_plan = RasPlan()
        >>> new_plan_number = ras_plan.clone_plan('01', new_plan_shortid='New Plan')
        >>> print(f"New plan created with number: {new_plan_number}")

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Update plan entries without reinitializing the entire project
        ras_obj.plan_df = ras_obj.get_prj_entries('Plan')

        new_plan_num = RasPlan.get_next_number(ras_obj.plan_df['plan_number'])
        template_plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{template_plan}"
        new_plan_path = ras_obj.project_folder / f"{ras_obj.project_name}.p{new_plan_num}"

        def update_shortid(lines):
            shortid_pattern = re.compile(r'^Short Identifier=(.*)$', re.IGNORECASE)
            for i, line in enumerate(lines):
                match = shortid_pattern.match(line.strip())
                if match:
                    current_shortid = match.group(1)
                    if new_plan_shortid is None:
                        new_shortid = (current_shortid + "_copy")[:24]
                    else:
                        new_shortid = new_plan_shortid[:24]
                    lines[i] = f"Short Identifier={new_shortid}\n"
                    break
            return lines

        # Use RasUtils to clone the file and update the short identifier
        RasUtils.clone_file(template_plan_path, new_plan_path, update_shortid)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Plan', new_plan_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_plan_num

    @staticmethod
    @log_call
    def clone_unsteady(template_unsteady, ras_object=None):
        """
        Copy unsteady flow files from a template, find the next unsteady number,
        and update the project file accordingly.

        Parameters:
        template_unsteady (str): Unsteady flow number to be used as a template (e.g., '01')
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        str: New unsteady flow number (e.g., '03')

        Example:
        >>> ras_plan = RasPlan()
        >>> new_unsteady_num = ras_plan.clone_unsteady('01')
        >>> print(f"New unsteady flow file created: u{new_unsteady_num}")

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Update unsteady entries without reinitializing the entire project
        ras_obj.unsteady_df = ras_obj.get_prj_entries('Unsteady')

        new_unsteady_num = RasPlan.get_next_number(ras_obj.unsteady_df['unsteady_number'])
        template_unsteady_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{template_unsteady}"
        new_unsteady_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_num}"

        # Use RasUtils to clone the file
        RasUtils.clone_file(template_unsteady_path, new_unsteady_path)

        # Copy the corresponding .hdf file if it exists
        template_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{template_unsteady}.hdf"
        new_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.u{new_unsteady_num}.hdf"
        if template_hdf_path.exists():
            shutil.copy(template_hdf_path, new_hdf_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Unsteady', new_unsteady_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)

        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_unsteady_num


    @staticmethod
    @log_call
    def clone_steady(template_flow, ras_object=None):
        """
        Copy steady flow files from a template, find the next flow number,
        and update the project file accordingly.
        
        Parameters:
        template_flow (str): Flow number to be used as a template (e.g., '01')
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        str: New flow number (e.g., '03')

        Example:
        >>> ras_plan = RasPlan()
        >>> new_flow_num = ras_plan.clone_steady('01')
        >>> print(f"New steady flow file created: f{new_flow_num}")

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Update flow entries without reinitializing the entire project
        ras_obj.flow_df = ras_obj.get_prj_entries('Flow')

        new_flow_num = RasPlan.get_next_number(ras_obj.flow_df['flow_number'])
        template_flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{template_flow}"
        new_flow_path = ras_obj.project_folder / f"{ras_obj.project_name}.f{new_flow_num}"

        # Use RasUtils to clone the file
        RasUtils.clone_file(template_flow_path, new_flow_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Flow', new_flow_num, ras_object=ras_obj)

        # Re-initialize the ras global object
        ras_obj.initialize(ras_obj.project_folder, ras_obj.ras_exe_path)
        
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        return new_flow_num

    @staticmethod
    @log_call
    def clone_geom(template_geom, ras_object=None):
        """
        Copy geometry files from a template, find the next geometry number,
        and update the project file accordingly.
        
        Parameters:
        template_geom (str): Geometry number to be used as a template (e.g., '01')
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        str: New geometry number (e.g., '03')

        Note:
            This function updates the ras object's dataframes after modifying the project structure.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # Update geometry entries without reinitializing the entire project
        ras_obj.geom_df = ras_obj.get_prj_entries('Geom')

        new_geom_num = RasPlan.get_next_number(ras_obj.geom_df['geom_number'])
        template_geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{template_geom}"
        new_geom_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom_num}"

        # Use RasUtils to clone the file
        RasUtils.clone_file(template_geom_path, new_geom_path)

        # Handle HDF file copy
        template_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{template_geom}.hdf"
        new_hdf_path = ras_obj.project_folder / f"{ras_obj.project_name}.g{new_geom_num}.hdf"
        if template_hdf_path.is_file():
            RasUtils.clone_file(template_hdf_path, new_hdf_path)

        # Use RasUtils to update the project file
        RasUtils.update_project_file(ras_obj.prj_file, 'Geom', new_geom_num, ras_object=ras_obj)

        # Update all dataframes in the ras object
        ras_obj.plan_df = ras_obj.get_plan_entries()
        ras_obj.geom_df = ras_obj.get_geom_entries()
        ras_obj.flow_df = ras_obj.get_flow_entries()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

        return new_geom_num

    @staticmethod
    @log_call
    def get_next_number(existing_numbers):
        """
        Determine the next available number from a list of existing numbers.
        
        Parameters:
        existing_numbers (list): List of existing numbers as strings
        
        Returns:
        str: Next available number as a zero-padded string
        
        Example:
        >>> existing_numbers = ['01', '02', '04']
        >>> RasPlan.get_next_number(existing_numbers)
        '03'
        >>> existing_numbers = ['01', '02', '03']
        >>> RasPlan.get_next_number(existing_numbers)
        '04'
        """
        existing_numbers = sorted(int(num) for num in existing_numbers)
        next_number = 1
        for num in existing_numbers:
            if num == next_number:
                next_number += 1
            else:
                break
        return f"{next_number:02d}"

    @staticmethod
    @log_call
    def get_plan_value(
        plan_number_or_path: Union[str, Path],
        key: str,
        ras_object=None
    ) -> Any:
        """
        Retrieve a specific value from a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        key (str): The key to retrieve from the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Any: The value associated with the specified key

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading the plan file

        Available keys and their expected types:
        - 'Computation Interval' (str): Time value for computational time step (e.g., '5SEC', '2MIN')
        - 'DSS File' (str): Name of the DSS file used
        - 'Flow File' (str): Name of the flow input file
        - 'Friction Slope Method' (int): Method selection for friction slope (e.g., 1, 2)
        - 'Geom File' (str): Name of the geometry input file
        - 'Mapping Interval' (str): Time interval for mapping output
        - 'Plan File' (str): Name of the plan file
        - 'Plan Title' (str): Title of the simulation plan
        - 'Program Version' (str): Version number of HEC-RAS
        - 'Run HTab' (int): Flag to run HTab module (-1 or 1)
        - 'Run Post Process' (int): Flag to run post-processing (-1 or 1)
        - 'Run Sediment' (int): Flag to run sediment transport module (0 or 1)
        - 'Run UNET' (int): Flag to run unsteady network module (-1 or 1)
        - 'Run WQNET' (int): Flag to run water quality module (0 or 1)
        - 'Short Identifier' (str): Short name or ID for the plan
        - 'Simulation Date' (str): Start and end dates/times for simulation
        - 'UNET D1 Cores' (int): Number of cores used in 1D calculations
        - 'UNET D2 Cores' (int): Number of cores used in 2D calculations
        - 'PS Cores' (int): Number of cores used in parallel simulation
        - 'UNET Use Existing IB Tables' (int): Flag for using existing internal boundary tables (-1, 0, or 1)
        - 'UNET 1D Methodology' (str): 1D calculation methodology
        - 'UNET D2 Solver Type' (str): 2D solver type
        - 'UNET D2 Name' (str): Name of the 2D area
        - 'Run RASMapper' (int): Flag to run RASMapper for floodplain mapping (-1 for off, 0 for on)
        
        Note: 
        Writing Multi line keys like 'Description' are not supported by this function.

        Example:
        >>> computation_interval = RasPlan.get_plan_value("01", "Computation Interval")
        >>> print(f"Computation interval: {computation_interval}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        supported_plan_keys = {
            'Description', 'Computation Interval', 'DSS File', 'Flow File', 'Friction Slope Method',
            'Geom File', 'Mapping Interval', 'Plan File', 'Plan Title', 'Program Version',
            'Run HTab', 'Run Post Process', 'Run Sediment', 'Run UNET', 'Run WQNET',
            'Short Identifier', 'Simulation Date', 'UNET D1 Cores', 'UNET D2 Cores', 'PS Cores',
            'UNET Use Existing IB Tables', 'UNET 1D Methodology', 'UNET D2 Solver Type', 
            'UNET D2 Name', 'Run RASMapper', 'Run HTab', 'Run UNET'
        }

        if key not in supported_plan_keys:
            logger = logging.getLogger(__name__)
            logger.warning(f"Unknown key: {key}. Valid keys are: {', '.join(supported_plan_keys)}\n Add more keys and explanations in get_plan_value() as needed.")

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()
        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        # Handle core settings specially to convert to integers
        core_keys = {'UNET D1 Cores', 'UNET D2 Cores', 'PS Cores'}
        if key in core_keys:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                try:
                    return int(match.group(1).strip())
                except ValueError:
                    logger = logging.getLogger(__name__)
                    logger.error(f"Could not convert {key} value to integer")
                    return None
            else:
                logger = logging.getLogger(__name__)
                logger.error(f"Key '{key}' not found in the plan file.")
                return None
        elif key == 'Description':
            match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            return match.group(1).strip() if match else None
        else:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()
            else:
                logger = logging.getLogger(__name__)
                logger.error(f"Key '{key}' not found in the plan file.")
                return None


#  NEW FUNCTIONS THAT NEED TESTING AND EXAMPLES


    @staticmethod
    @log_call
    def update_run_flags(
        plan_number_or_path: Union[str, Path],
        geometry_preprocessor: bool = None,
        unsteady_flow_simulation: bool = None,
        run_sediment: bool = None,
        post_processor: bool = None,
        floodplain_mapping: bool = None,
        ras_object=None
    ) -> None:
        """
        Update the run flags in a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        geometry_preprocessor (bool, optional): Flag for Geometry Preprocessor
        unsteady_flow_simulation (bool, optional): Flag for Unsteady Flow Simulation
        run_sediment (bool, optional): Flag for run_sediment
        post_processor (bool, optional): Flag for Post Processor
        floodplain_mapping (bool, optional): Flag for Floodplain Mapping
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading or writing the plan file

        Example:
        >>> RasPlan.update_run_flags("01", geometry_preprocessor=True, unsteady_flow_simulation=True, run_sediment=False, post_processor=True, floodplain_mapping=False)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        flag_mapping = {
            'geometry_preprocessor': ('Run HTab', geometry_preprocessor),
            'unsteady_flow_simulation': ('Run UNet', unsteady_flow_simulation),
            'run_sediment': ('Run run_sediment', run_sediment),
            'post_processor': ('Run PostProcess', post_processor),
            'floodplain_mapping': ('Run RASMapper', floodplain_mapping)
        }

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            for i, line in enumerate(lines):
                for key, (file_key, value) in flag_mapping.items():
                    if value is not None and line.strip().startswith(file_key):
                        lines[i] = f"{file_key}= {1 if value else 0}\n"

            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger = logging.getLogger(__name__)
            logger.info(f"Successfully updated run flags in plan file: {plan_file_path}")

        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error updating run flags in plan file {plan_file_path}: {e}")
            raise



    @staticmethod
    @log_call
    def update_plan_intervals(
        plan_number_or_path: Union[str, Path],
        computation_interval: Optional[str] = None,
        output_interval: Optional[str] = None,
        instantaneous_interval: Optional[str] = None,
        mapping_interval: Optional[str] = None,
        ras_object=None
    ) -> None:
        """
        Update the computation and output intervals in a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        computation_interval (Optional[str]): The new computation interval. Valid entries include:
            '1SEC', '2SEC', '3SEC', '4SEC', '5SEC', '6SEC', '10SEC', '15SEC', '20SEC', '30SEC',
            '1MIN', '2MIN', '3MIN', '4MIN', '5MIN', '6MIN', '10MIN', '15MIN', '20MIN', '30MIN',
            '1HOUR', '2HOUR', '3HOUR', '4HOUR', '6HOUR', '8HOUR', '12HOUR', '1DAY'
        output_interval (Optional[str]): The new output interval. Valid entries are the same as computation_interval.
        instantaneous_interval (Optional[str]): The new instantaneous interval. Valid entries are the same as computation_interval.
        mapping_interval (Optional[str]): The new mapping interval. Valid entries are the same as computation_interval.
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
        ValueError: If the plan file is not found or if an invalid interval is provided
        IOError: If there's an error reading or writing the plan file

        Note: This function does not check if the intervals are equal divisors. Ensure you use valid values from HEC-RAS.

        Example:
        >>> RasPlan.update_plan_intervals("01", computation_interval="5SEC", output_interval="1MIN", instantaneous_interval="1HOUR", mapping_interval="5MIN")
        >>> RasPlan.update_plan_intervals("/path/to/plan.p01", computation_interval="10SEC", output_interval="30SEC")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object=ras_obj)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        valid_intervals = [
            '1SEC', '2SEC', '3SEC', '4SEC', '5SEC', '6SEC', '10SEC', '15SEC', '20SEC', '30SEC',
            '1MIN', '2MIN', '3MIN', '4MIN', '5MIN', '6MIN', '10MIN', '15MIN', '20MIN', '30MIN',
            '1HOUR', '2HOUR', '3HOUR', '4HOUR', '6HOUR', '8HOUR', '12HOUR', '1DAY'
        ]

        interval_mapping = {
            'Computation Interval': computation_interval,
            'Output Interval': output_interval,
            'Instantaneous Interval': instantaneous_interval,
            'Mapping Interval': mapping_interval
        }

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            for i, line in enumerate(lines):
                for key, value in interval_mapping.items():
                    if value is not None:
                        if value.upper() not in valid_intervals:
                            raise ValueError(f"Invalid {key}: {value}. Must be one of {valid_intervals}")
                        if line.strip().startswith(key):
                            lines[i] = f"{key}={value.upper()}\n"

            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger = logging.getLogger(__name__)
            logger.info(f"Successfully updated intervals in plan file: {plan_file_path}")

        except IOError as e:
            logger = logging.getLogger(__name__)
            logger.error(f"Error updating intervals in plan file {plan_file_path}: {e}")
            raise


    @log_call
    def update_plan_description(plan_number_or_path: Union[str, Path], description: str, ras_object: Optional['RasPrj'] = None) -> None:
        """
        Update the description block in a HEC-RAS plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or full path to the plan file
            description (str): The new description text to set
            ras_object (Optional[RasPrj]): Specific RAS object to use. If None, uses the global ras instance.

        Raises:
            ValueError: If the plan file is not found
            IOError: If there's an error reading or writing the plan file
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()

            # Find the description block
            desc_pattern = r'Begin DESCRIPTION.*?END DESCRIPTION'
            new_desc_block = f'Begin DESCRIPTION\n{description}\nEND DESCRIPTION'

            if re.search(desc_pattern, content, re.DOTALL):
                # Replace existing description block
                new_content = re.sub(desc_pattern, new_desc_block, content, flags=re.DOTALL)
            else:
                # Add new description block at the start of the file
                new_content = new_desc_block + '\n' + content

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.write(new_content)

            logger.info(f"Updated description in plan file: {plan_file_path}")

            # Update the dataframes in the RAS object to reflect changes
            if ras_object:
                ras_object.plan_df = ras_object.get_plan_entries()
                ras_object.geom_df = ras_object.get_geom_entries()
                ras_object.flow_df = ras_object.get_flow_entries()
                ras_object.unsteady_df = ras_object.get_unsteady_entries()

        except IOError as e:
            logger.error(f"Error updating plan description in {plan_file_path}: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error updating plan description: {e}")
            raise

    @staticmethod
    @log_call
    def read_plan_description(plan_number_or_path: Union[str, Path], ras_object: Optional['RasPrj'] = None) -> str:
        """
        Read the description from the plan file.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            ras_object (Optional[RasPrj]): The RAS project object. If None, uses the global 'ras' object.

        Returns:
            str: The description from the plan file.

        Raises:
            ValueError: If the plan file is not found.
            IOError: If there's an error reading from the plan file.
        """
        logger = logging.getLogger(__name__)

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()
        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        description_lines = []
        in_description = False
        description_found = False
        for line in lines:
            if line.strip() == "BEGIN DESCRIPTION:":
                in_description = True
                description_found = True
            elif line.strip() == "END DESCRIPTION:":
                break
            elif in_description:
                description_lines.append(line.strip())

        if not description_found:
            logger.warning(f"No description found in plan file: {plan_file_path}")
            return ""

        description = '\n'.join(description_lines)
        logger.info(f"Read description from plan file: {plan_file_path}")
        return description




    @staticmethod
    @log_call
    def update_simulation_date(plan_number_or_path: Union[str, Path], start_date: datetime, end_date: datetime, ras_object: Optional['RasPrj'] = None) -> None:
        """
        Update the simulation date for a given plan.

        Args:
            plan_number_or_path (Union[str, Path]): The plan number or path to the plan file.
            start_date (datetime): The start date and time for the simulation.
            end_date (datetime): The end date and time for the simulation.
            ras_object (Optional['RasPrj']): The RAS project object. Defaults to None.

        Raises:
            ValueError: If the plan file is not found or if there's an error updating the file.
        """

        # Get the plan file path
        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasPlan.get_plan_path(plan_number_or_path, ras_object)
            if plan_file_path is None or not Path(plan_file_path).exists():
                raise ValueError(f"Plan file not found: {plan_file_path}")

        # Format the dates
        formatted_date = f"{start_date.strftime('%d%b%Y').upper()},{start_date.strftime('%H%M')},{end_date.strftime('%d%b%Y').upper()},{end_date.strftime('%H%M')}"

        try:
            # Read the file
            with open(plan_file_path, 'r') as file:
                lines = file.readlines()

            # Update the Simulation Date line
            updated = False
            for i, line in enumerate(lines):
                if line.startswith("Simulation Date="):
                    lines[i] = f"Simulation Date={formatted_date}\n"
                    updated = True
                    break

            # If Simulation Date line not found, add it at the end
            if not updated:
                lines.append(f"Simulation Date={formatted_date}\n")

            # Write the updated content back to the file
            with open(plan_file_path, 'w') as file:
                file.writelines(lines)

            logger.info(f"Updated simulation date in plan file: {plan_file_path}")

        except IOError as e:
            logger.error(f"Error updating simulation date in plan file {plan_file_path}: {e}")
            raise ValueError(f"Error updating simulation date: {e}")

        # Refresh RasPrj dataframes
        if ras_object:
            ras_object.plan_df = ras_object.get_plan_entries()
            ras_object.unsteady_df = ras_object.get_unsteady_entries()


==================================================

File: c:\GH\ras-commander\ras_commander\RasPrj.py
==================================================
"""
RasPrj.py - Manages HEC-RAS projects within the ras-commander library

This module provides a class for managing HEC-RAS projects.

Classes:
    RasPrj: A class for managing HEC-RAS projects.

Functions:
    init_ras_project: Initialize a RAS project.
    get_ras_exe: Determine the HEC-RAS executable path based on the input.

DEVELOPER NOTE:
This class is used to initialize a RAS project and is used in conjunction with the RasCmdr class to manage the execution of RAS plans.
By default, the RasPrj class is initialized with the global 'ras' object.
However, you can create multiple RasPrj instances to manage multiple projects.
Do not mix and match global 'ras' object instances and custom instances of RasPrj - it will cause errors.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function():
        
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are class methods and are designed to be used with instances of the class.

List of Functions in RasPrj:    
- initialize()
- _load_project_data()
- _get_geom_file_for_plan()
- _parse_plan_file()
- _parse_unsteady_file()
- _get_prj_entries()
- _parse_boundary_condition()
- is_initialized (property)
- check_initialized()
- find_ras_prj()
- get_project_name()
- get_prj_entries()
- get_plan_entries()
- get_flow_entries()
- get_unsteady_entries()
- get_geom_entries()
- get_hdf_entries()
- print_data()
- get_plan_value()
- get_boundary_conditions()
        
Functions in RasPrj that are not part of the class:        
- init_ras_project()
- get_ras_exe()

        
        
        
"""
import os
import re
from pathlib import Path
import pandas as pd
from typing import Union, Any, List, Dict, Tuple
import logging
from ras_commander.LoggingConfig import get_logger
from ras_commander.Decorators import log_call

logger = get_logger(__name__)

def read_file_with_fallback_encoding(file_path, encodings=['utf-8', 'latin1', 'cp1252', 'iso-8859-1']):
    """
    Attempt to read a file using multiple encodings.
    
    Args:
        file_path (str or Path): Path to the file to read
        encodings (list): List of encodings to try, in order of preference
    
    Returns:
        tuple: (content, encoding) or (None, None) if all encodings fail
    """
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                content = file.read()
                return content, encoding
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"Error reading file {file_path} with {encoding} encoding: {e}")
            continue
    
    logger.error(f"Failed to read file {file_path} with any of the attempted encodings: {encodings}")
    return None, None

class RasPrj:
    
    def __init__(self):
        self.initialized = False
        self.boundaries_df = None  # New attribute to store boundary conditions
        self.suppress_logging = False  # Add suppress_logging as instance variable

    @log_call
    def initialize(self, project_folder, ras_exe_path, suppress_logging=True):
        """
        Initialize a RasPrj instance.

        This method sets up the RasPrj instance with the given project folder and RAS executable path.
        It finds the project file, loads project data, sets the initialization flag, and now also
        extracts boundary conditions.

        Args:
            project_folder (str or Path): Path to the HEC-RAS project folder.
            ras_exe_path (str or Path): Path to the HEC-RAS executable.
            suppress_logging (bool): If True, suppresses initialization logging messages.

        Raises:
            ValueError: If no HEC-RAS project file is found in the specified folder.

        Note:
            This method is intended for internal use. External users should use the init_ras_project function instead.
        """
        self.suppress_logging = suppress_logging  # Store suppress_logging state
        self.project_folder = Path(project_folder)
        self.prj_file = self.find_ras_prj(self.project_folder)
        if self.prj_file is None:
            logger.error(f"No HEC-RAS project file found in {self.project_folder}")
            raise ValueError(f"No HEC-RAS project file found in {self.project_folder}")
        self.project_name = Path(self.prj_file).stem
        self.ras_exe_path = ras_exe_path
        self._load_project_data()
        self.boundaries_df = self.get_boundary_conditions()  # Extract boundary conditions
        self.initialized = True
        
        if not suppress_logging:
            logger.info(f"Initialization complete for project: {self.project_name}")
            logger.info(f"Plan entries: {len(self.plan_df)}, Flow entries: {len(self.flow_df)}, "
                         f"Unsteady entries: {len(self.unsteady_df)}, Geometry entries: {len(self.geom_df)}, "
                         f"Boundary conditions: {len(self.boundaries_df)}")
            logger.info(f"Geometry HDF files found: {self.plan_df['Geom_File'].notna().sum()}")

    @log_call
    def _load_project_data(self):
        """
        Load project data from the HEC-RAS project file.

        This method initializes DataFrames for plan, flow, unsteady, and geometry entries
        by calling the _get_prj_entries method for each entry type.
        """
        # Initialize DataFrames
        self.plan_df = self._get_prj_entries('Plan')
        self.flow_df = self._get_prj_entries('Flow')
        self.unsteady_df = self._get_prj_entries('Unsteady')
        self.geom_df = self.get_geom_entries()  # Use get_geom_entries instead of _get_prj_entries
        
        # Add Geom_File to plan_df
        self.plan_df['Geom_File'] = self.plan_df.apply(lambda row: self._get_geom_file_for_plan(row['plan_number']), axis=1)

    
    def _get_geom_file_for_plan(self, plan_number):
        """
        Get the geometry file path for a given plan number.
        
        Args:
            plan_number (str): The plan number to find the geometry file for.
        
        Returns:
            str: The full path to the geometry HDF file, or None if not found.
        """
        plan_file_path = self.project_folder / f"{self.project_name}.p{plan_number}"
        content, encoding = read_file_with_fallback_encoding(plan_file_path)
        
        if content is None:
            return None
        
        try:
            for line in content.splitlines():
                if line.startswith("Geom File="):
                    geom_file = line.strip().split('=')[1]
                    geom_hdf_path = self.project_folder / f"{self.project_name}.{geom_file}.hdf"
                    if geom_hdf_path.exists():
                        return str(geom_hdf_path)
                    else:
                        return None
        except Exception as e:
            logger.error(f"Error reading plan file for geometry: {e}")
        return None


    @staticmethod
    @log_call
    def get_plan_value(
        plan_number_or_path: Union[str, Path],
        key: str,
        ras_object=None
    ) -> Any:
        """
        Retrieve a specific value from a HEC-RAS plan file.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        key (str): The key to retrieve from the plan file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.

        Returns:
        Any: The value associated with the specified key

        Raises:
        ValueError: If the plan file is not found
        IOError: If there's an error reading the plan file
        """
        logger = get_logger(__name__)
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        # These must exactly match the keys in supported_plan_keys from _parse_plan_file
        valid_keys = {
            'Computation Interval',
            'DSS File',
            'Flow File',
            'Friction Slope Method',
            'Geom File',
            'Mapping Interval',
            'Plan Title',
            'Program Version',
            'Run HTab',
            'Run PostProcess',
            'Run Sediment',
            'Run UNet',
            'Run WQNet',
            'Short Identifier',
            'Simulation Date',
            'UNET D1 Cores',
            'UNET D2 Cores',
            'PS Cores',
            'UNET Use Existing IB Tables',
            'UNET 1D Methodology',
            'UNET D2 SolverType',
            'UNET D2 Name',
            'description'  # Special case for description block
        }

        if key not in valid_keys:
            logger.warning(f"Unknown key: {key}. Valid keys are: {', '.join(sorted(valid_keys))}")
            return None

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise ValueError(f"Plan file not found: {plan_file_path}")

        try:
            with open(plan_file_path, 'r') as file:
                content = file.read()
        except IOError as e:
            logger.error(f"Error reading plan file {plan_file_path}: {e}")
            raise

        if key == 'description':
            match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            return match.group(1).strip() if match else None
        else:
            pattern = f"{key}=(.*)"
            match = re.search(pattern, content)
            if match:
                value = match.group(1).strip()
                # Convert core values to integers
                if key in ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']:
                    try:
                        return int(value)
                    except ValueError:
                        logger.warning(f"Could not convert {key} value '{value}' to integer")
                        return None
                return value
            
            # Use DEBUG level for missing core values, ERROR for other missing keys
            if key in ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']:
                logger.debug(f"Core setting '{key}' not found in plan file")
            else:
                logger.error(f"Key '{key}' not found in the plan file")
            return None

    def _parse_plan_file(self, plan_file_path):
        """
        Parse a plan file and extract critical information.
        
        Args:
            plan_file_path (Path): Path to the plan file.
        
        Returns:
            dict: Dictionary containing extracted plan information.
        """
        plan_info = {}
        content, encoding = read_file_with_fallback_encoding(plan_file_path)
        
        if content is None:
            logger.error(f"Could not read plan file {plan_file_path} with any supported encoding")
            return plan_info
        
        try:
            # Extract description
            description_match = re.search(r'Begin DESCRIPTION(.*?)END DESCRIPTION', content, re.DOTALL)
            if description_match:
                plan_info['description'] = description_match.group(1).strip()
            
            # BEGIN Exception to Style Guide, this is needed to keep the key names consistent with the plan file keys.
            
            # Extract other critical information
            supported_plan_keys = {
                'Computation Interval': r'Computation Interval=(.+)',
                'DSS File': r'DSS File=(.+)',
                'Flow File': r'Flow File=(.+)',
                'Friction Slope Method': r'Friction Slope Method=(.+)',
                'Geom File': r'Geom File=(.+)',
                'Mapping Interval': r'Mapping Interval=(.+)',
                'Plan Title': r'Plan Title=(.+)',
                'Program Version': r'Program Version=(.+)',
                'Run HTab': r'Run HTab=(.+)',
                'Run PostProcess': r'Run PostProcess=(.+)',
                'Run Sediment': r'Run Sediment=(.+)',
                'Run UNet': r'Run UNet=(.+)',
                'Run WQNet': r'Run WQNet=(.+)',
                'Short Identifier': r'Short Identifier=(.+)',
                'Simulation Date': r'Simulation Date=(.+)',
                'UNET D1 Cores': r'UNET D1 Cores=(.+)',
                'UNET D2 Cores': r'UNET D2 Cores=(.+)',
                'PS Cores': r'PS Cores=(.+)',
                'UNET Use Existing IB Tables': r'UNET Use Existing IB Tables=(.+)',
                'UNET 1D Methodology': r'UNET 1D Methodology=(.+)',
                'UNET D2 SolverType': r'UNET D2 SolverType=(.+)',
                'UNET D2 Name': r'UNET D2 Name=(.+)'
            }
            
            # END Exception to Style Guide
            
            # First, explicitly set None for core values
            core_keys = ['UNET D1 Cores', 'UNET D2 Cores', 'PS Cores']
            for key in core_keys:
                plan_info[key] = None
            
            for key, pattern in supported_plan_keys.items():
                match = re.search(pattern, content)
                if match:
                    value = match.group(1).strip()
                    # Convert core values to integers if they exist
                    if key in core_keys and value:
                        try:
                            value = int(value)
                        except ValueError:
                            logger.warning(f"Could not convert {key} value '{value}' to integer in plan file {plan_file_path}")
                            value = None
                    plan_info[key] = value
                elif key in core_keys:
                    logger.debug(f"Core setting '{key}' not found in plan file {plan_file_path}")
            
            logger.debug(f"Parsed plan file: {plan_file_path} using {encoding} encoding")
        except Exception as e:
            logger.error(f"Error parsing plan file {plan_file_path}: {e}")
        
        return plan_info

    def _get_prj_entries(self, entry_type):
        """
        Extract entries of a specific type from the HEC-RAS project file.

        Args:
            entry_type (str): The type of entry to extract (e.g., 'Plan', 'Flow', 'Unsteady', 'Geom').

        Returns:
            pd.DataFrame: A DataFrame containing the extracted entries.

        Note:
            This method reads the project file and extracts entries matching the specified type.
            For 'Unsteady' entries, it parses additional information from the unsteady file.
        """
        entries = []
        pattern = re.compile(rf"{entry_type} File=(\w+)")

        try:
            with open(self.prj_file, 'r') as file:
                for line in file:
                    match = pattern.match(line.strip())
                    if match:
                        file_name = match.group(1)
                        full_path = str(self.project_folder / f"{self.project_name}.{file_name}")
                        entry = {
                            f'{entry_type.lower()}_number': file_name[1:],
                            'full_path': full_path
                        }

                        if entry_type == 'Plan':
                            plan_info = self._parse_plan_file(Path(full_path))
                            entry.update(plan_info)
                            
                            hdf_results_path = self.project_folder / f"{self.project_name}.p{file_name[1:]}.hdf"
                            entry['HDF_Results_Path'] = str(hdf_results_path) if hdf_results_path.exists() else None

                        if entry_type == 'Unsteady':
                            unsteady_info = self._parse_unsteady_file(Path(full_path))
                            entry.update(unsteady_info)

                        entries.append(entry)
        except Exception as e:
            raise

        return pd.DataFrame(entries)

    def _parse_unsteady_file(self, unsteady_file_path):
        """
        Parse an unsteady flow file and extract critical information.
        
        Args:
            unsteady_file_path (Path): Path to the unsteady flow file.
        
        Returns:
            dict: Dictionary containing extracted unsteady flow information.
        """
        unsteady_info = {}
        content, encoding = read_file_with_fallback_encoding(unsteady_file_path)
        
        if content is None:
            return unsteady_info
        
        try:
            # BEGIN Exception to Style Guide, this is needed to keep the key names consistent with the unsteady file keys.
            
            supported_unsteady_keys = {
                'Flow Title': r'Flow Title=(.+)',
                'Program Version': r'Program Version=(.+)',
                'Use Restart': r'Use Restart=(.+)',
                'Precipitation Mode': r'Precipitation Mode=(.+)',
                'Wind Mode': r'Wind Mode=(.+)',
                'Met BC=Precipitation|Mode': r'Met BC=Precipitation\|Mode=(.+)',
                'Met BC=Evapotranspiration|Mode': r'Met BC=Evapotranspiration\|Mode=(.+)',
                'Met BC=Precipitation|Expanded View': r'Met BC=Precipitation\|Expanded View=(.+)',
                'Met BC=Precipitation|Constant Units': r'Met BC=Precipitation\|Constant Units=(.+)',
                'Met BC=Precipitation|Gridded Source': r'Met BC=Precipitation\|Gridded Source=(.+)'
            }
            
            # END Exception to Style Guide
            
            for key, pattern in supported_unsteady_keys.items():
                match = re.search(pattern, content)
                if match:
                    unsteady_info[key] = match.group(1).strip()
        
        except Exception as e:
            logger.error(f"Error parsing unsteady file {unsteady_file_path}: {e}")
        
        return unsteady_info

    @property
    def is_initialized(self):
        """
        Check if the RasPrj instance has been initialized.

        Returns:
            bool: True if the instance has been initialized, False otherwise.
        """
        return self.initialized

    @log_call
    def check_initialized(self):
        """
        Ensure that the RasPrj instance has been initialized.

        Raises:
            RuntimeError: If the project has not been initialized.
        """
        if not self.initialized:
            raise RuntimeError("Project not initialized. Call init_ras_project() first.")

    @staticmethod
    @log_call
    def find_ras_prj(folder_path):
        """
        Find the appropriate HEC-RAS project file (.prj) in the given folder.
        
        Parameters:
        folder_path (str or Path): Path to the folder containing HEC-RAS files.
        
        Returns:
        Path: The full path of the selected .prj file or None if no suitable file is found.
        """
        folder_path = Path(folder_path)
        prj_files = list(folder_path.glob("*.prj"))
        rasmap_files = list(folder_path.glob("*.rasmap"))
        if len(prj_files) == 1:
            return prj_files[0].resolve()
        if len(prj_files) > 1:
            if len(rasmap_files) == 1:
                base_filename = rasmap_files[0].stem
                prj_file = folder_path / f"{base_filename}.prj"
                if prj_file.exists():
                    return prj_file.resolve()
            for prj_file in prj_files:
                try:
                    with open(prj_file, 'r') as file:
                        content = file.read()
                        if "Proj Title=" in content:
                            return prj_file.resolve()
                except Exception:
                    continue
        return None


    @log_call
    def get_project_name(self):
        """
        Get the name of the HEC-RAS project.

        Returns:
            str: The name of the project.

        Raises:
            RuntimeError: If the project has not been initialized.
        """
        self.check_initialized()
        return self.project_name

    @log_call
    def get_prj_entries(self, entry_type):
        """
        Get entries of a specific type from the HEC-RAS project.

        Args:
            entry_type (str): The type of entry to retrieve (e.g., 'Plan', 'Flow', 'Unsteady', 'Geom').

        Returns:
            pd.DataFrame: A DataFrame containing the requested entries.

        Raises:
            RuntimeError: If the project has not been initialized.
        """
        self.check_initialized()
        return self._get_prj_entries(entry_type)

    @log_call
    def get_plan_entries(self):
        """
        Get all plan entries from the HEC-RAS project.

        Returns:
            pd.DataFrame: A DataFrame containing all plan entries.

        Raises:
            RuntimeError: If the project has not been initialized.
        """
        self.check_initialized()
        return self._get_prj_entries('Plan')

    @log_call
    def get_flow_entries(self):
        """
        Get all flow entries from the HEC-RAS project.

        Returns:
            pd.DataFrame: A DataFrame containing all flow entries.

        Raises:
            RuntimeError: If the project has not been initialized.
        """
        self.check_initialized()
        return self._get_prj_entries('Flow')

    @log_call
    def get_unsteady_entries(self):
        """
        Get all unsteady flow entries from the HEC-RAS project.

        Returns:
            pd.DataFrame: A DataFrame containing all unsteady flow entries.

        Raises:
            RuntimeError: If the project has not been initialized.
        """
        self.check_initialized()
        return self._get_prj_entries('Unsteady')

    @log_call
    def get_geom_entries(self):
        """
        Get geometry entries from the project file.

        Returns:
            pd.DataFrame: DataFrame containing geometry entries.
        """
        geom_pattern = re.compile(r'Geom File=(\w+)')
        geom_entries = []

        try:
            with open(self.prj_file, 'r') as f:
                for line in f:
                    match = geom_pattern.search(line)
                    if match:
                        geom_entries.append(match.group(1))
        
            geom_df = pd.DataFrame({'geom_file': geom_entries})
            geom_df['geom_number'] = geom_df['geom_file'].str.extract(r'(\d+)$')
            geom_df['full_path'] = geom_df['geom_file'].apply(lambda x: str(self.project_folder / f"{self.project_name}.{x}"))
            geom_df['hdf_path'] = geom_df['full_path'] + ".hdf"
            
            if not self.suppress_logging:  # Only log if suppress_logging is False
                logger.info(f"Found {len(geom_df)} geometry entries")
            return geom_df
        except Exception as e:
            logger.error(f"Error reading geometry entries from project file: {e}")
            raise
    
    @log_call
    def get_hdf_entries(self):
        """
        Get HDF entries for plans that have results.
        
        Returns:
            pd.DataFrame: A DataFrame containing plan entries with HDF results.
                      Returns an empty DataFrame if no HDF entries are found.
        """
        self.check_initialized()
        
        hdf_entries = self.plan_df[self.plan_df['HDF_Results_Path'].notna()].copy()
        
        if hdf_entries.empty:
            return pd.DataFrame(columns=self.plan_df.columns)
        
        return hdf_entries
    
    
    @log_call
    def print_data(self):
        """Print all RAS Object data for this instance."""
        self.check_initialized()
        logger.info(f"--- Data for {self.project_name} ---")
        logger.info(f"Project folder: {self.project_folder}")
        logger.info(f"PRJ file: {self.prj_file}")
        logger.info(f"HEC-RAS executable: {self.ras_exe_path}")
        logger.info("Plan files:")
        logger.info(f"\n{self.plan_df}")
        logger.info("Flow files:")
        logger.info(f"\n{self.flow_df}")
        logger.info("Unsteady flow files:")
        logger.info(f"\n{self.unsteady_df}")
        logger.info("Geometry files:")
        logger.info(f"\n{self.geom_df}")
        logger.info("HDF entries:")
        logger.info(f"\n{self.get_hdf_entries()}")
        logger.info("Boundary conditions:")
        logger.info(f"\n{self.boundaries_df}")
        logger.info("----------------------------")

    @log_call
    def get_boundary_conditions(self) -> pd.DataFrame:
        """
        Extract boundary conditions from unsteady flow files and create a DataFrame.

        This method parses unsteady flow files to extract boundary condition information.
        It creates a DataFrame with structured data for known boundary condition types
        and parameters, and associates this information with the corresponding unsteady flow file.

        Note:
        Any lines in the boundary condition blocks that are not explicitly parsed and
        incorporated into the DataFrame are captured in a multi-line string. This string
        is logged at the DEBUG level for each boundary condition. This feature is crucial
        for developers incorporating new boundary condition types or parameters, as it
        allows them to see what information might be missing from the current parsing logic.
        If no unsteady flow files are present, it returns an empty DataFrame.

        Returns:
            pd.DataFrame: A DataFrame containing detailed boundary condition information,
                                      linked to the unsteady flow files.

        Usage:
            To see the unparsed lines, set the logging level to DEBUG before calling this method:
            
            import logging
            getLogger().setLevel(logging.DEBUG)
            
            boundaries_df = ras_project.get_boundary_conditions()
                          linked to the unsteady flow files. Returns an empty DataFrame if
                          no unsteady flow files are present.
        """
        boundary_data = []
        
        # Check if unsteady_df is empty
        if self.unsteady_df.empty:
            logger.info("No unsteady flow files found in the project.")
            return pd.DataFrame()  # Return an empty DataFrame
        
        for _, row in self.unsteady_df.iterrows():
            unsteady_file_path = row['full_path']
            unsteady_number = row['unsteady_number']
            
            try:
                with open(unsteady_file_path, 'r') as file:
                    content = file.read()
            except IOError as e:
                logger.error(f"Error reading unsteady file {unsteady_file_path}: {e}")
                continue
                
            bc_blocks = re.split(r'(?=Boundary Location=)', content)[1:]
            
            for i, block in enumerate(bc_blocks, 1):
                bc_info, unparsed_lines = self._parse_boundary_condition(block, unsteady_number, i)
                boundary_data.append(bc_info)
                
                if unparsed_lines:
                    logger.debug(f"Unparsed lines for boundary condition {i} in unsteady file {unsteady_number}:\n{unparsed_lines}")
        
        if not boundary_data:
            logger.info("No boundary conditions found in unsteady flow files.")
            return pd.DataFrame()  # Return an empty DataFrame if no boundary conditions were found
        
        boundaries_df = pd.DataFrame(boundary_data)
        
        # Merge with unsteady_df to get relevant unsteady flow file information
        merged_df = pd.merge(boundaries_df, self.unsteady_df, 
                             left_on='unsteady_number', right_on='unsteady_number', how='left')
        
        return merged_df

    def _parse_boundary_condition(self, block: str, unsteady_number: str, bc_number: int) -> Tuple[Dict, str]:
        lines = block.split('\n')
        bc_info = {
            'unsteady_number': unsteady_number,
            'boundary_condition_number': bc_number
        }
        
        parsed_lines = set()
        
        # Parse Boundary Location
        boundary_location = lines[0].split('=')[1].strip()
        fields = [field.strip() for field in boundary_location.split(',')]
        bc_info.update({
            'river_reach_name': fields[0] if len(fields) > 0 else '',
            'river_station': fields[1] if len(fields) > 1 else '',
            'storage_area_name': fields[2] if len(fields) > 2 else '',
            'pump_station_name': fields[3] if len(fields) > 3 else ''
        })
        parsed_lines.add(0)
        
        # Determine BC Type
        bc_types = {
            'Flow Hydrograph=': 'Flow Hydrograph',
            'Lateral Inflow Hydrograph=': 'Lateral Inflow Hydrograph',
            'Uniform Lateral Inflow Hydrograph=': 'Uniform Lateral Inflow Hydrograph',
            'Stage Hydrograph=': 'Stage Hydrograph',
            'Friction Slope=': 'Normal Depth',
            'Gate Name=': 'Gate Opening'
        }
        
        bc_info['bc_type'] = 'Unknown'
        bc_info['hydrograph_type'] = None
        for i, line in enumerate(lines[1:], 1):
            for key, bc_type in bc_types.items():
                if line.startswith(key):
                    bc_info['bc_type'] = bc_type
                    if 'Hydrograph' in bc_type:
                        bc_info['hydrograph_type'] = bc_type
                    parsed_lines.add(i)
                    break
            if bc_info['bc_type'] != 'Unknown':
                break
        
        # Parse other fields
        known_fields = ['Interval', 'DSS Path', 'Use DSS', 'Use Fixed Start Time', 'Fixed Start Date/Time',
                        'Is Critical Boundary', 'Critical Boundary Flow', 'DSS File']
        for i, line in enumerate(lines):
            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                if key in known_fields:
                    bc_info[key] = value.strip()
                    parsed_lines.add(i)
        
        # Handle hydrograph values
        bc_info['hydrograph_num_values'] = 0
        if bc_info['hydrograph_type']:
            hydrograph_key = f"{bc_info['hydrograph_type']}="
            hydrograph_line = next((line for i, line in enumerate(lines) if line.startswith(hydrograph_key)), None)
            if hydrograph_line:
                hydrograph_index = lines.index(hydrograph_line)
                values_count = int(hydrograph_line.split('=')[1].strip())
                bc_info['hydrograph_num_values'] = values_count
                if values_count > 0:
                    values = ' '.join(lines[hydrograph_index + 1:]).split()[:values_count]
                    bc_info['hydrograph_values'] = values
                    parsed_lines.update(range(hydrograph_index, hydrograph_index + (values_count // 5) + 2))
        
        # Collect unparsed lines
        unparsed_lines = '\n'.join(line for i, line in enumerate(lines) if i not in parsed_lines and line.strip())
        
        if unparsed_lines:
            logger.debug(f"Unparsed lines for boundary condition {bc_number} in unsteady file {unsteady_number}:\n{unparsed_lines}")
        
        return bc_info, unparsed_lines


# Create a global instance named 'ras'
# Defining the global instance allows the init_ras_project function to initialize the project.
# This only happens on the library initialization, not when the user calls init_ras_project.
ras = RasPrj()

# END OF CLASS DEFINITION


# START OF FUNCTION DEFINITIONS

@log_call
def init_ras_project(ras_project_folder, ras_version=None, ras_object=None):
    """
    Initialize a RAS project.

    USE THIS FUNCTION TO INITIALIZE A RAS PROJECT, NOT THE INITIALIZE METHOD OF THE RasPrj CLASS.
    The initialize method of the RasPrj class only modifies the global 'ras' object.

    Parameters:
    -----------
    ras_project_folder : str
        The path to the RAS project folder.
    ras_version : str, optional
        The version of RAS to use (e.g., "6.6").
        The version can also be a full path to the Ras.exe file.
        If None, the function will attempt to use the version from the global 'ras' object or a default path.
    ras_object : RasPrj, optional
        An instance of RasPrj to initialize. If None, the global 'ras' object is used.

    Returns:
    --------
    RasPrj
        An initialized RasPrj instance.
    """
    if not Path(ras_project_folder).exists():
        logger.error(f"The specified RAS project folder does not exist: {ras_project_folder}")
        raise FileNotFoundError(f"The specified RAS project folder does not exist: {ras_project_folder}. Please check the path and try again.")

    ras_exe_path = get_ras_exe(ras_version)

    if ras_object is None:
        logger.info("Initializing global 'ras' object via init_ras_project function.")
        ras_object = ras
    elif not isinstance(ras_object, RasPrj):
        logger.error("Provided ras_object is not an instance of RasPrj.")
        raise TypeError("ras_object must be an instance of RasPrj or None.")

    # Initialize the RasPrj instance
    ras_object.initialize(ras_project_folder, ras_exe_path)
    
    logger.info(f"Project initialized. ras_object project folder: {ras_object.project_folder}")
    return ras_object

@log_call
def get_ras_exe(ras_version=None):
    """
    Determine the HEC-RAS executable path based on the input.
    
    Args:
        ras_version (str, optional): Either a version number or a full path to the HEC-RAS executable.
                                     If None, the function will attempt to use the version from the global 'ras' object
                                     or a default path.
    
    Returns:
        str: The full path to the HEC-RAS executable.
    
    Raises:
        ValueError: If the input is neither a valid version number nor a valid file path.
    
    Notes:
        - If ras_version is not provided, the function will first check the global 'ras' object for a path.
        - If the global 'ras' object is not initialized or doesn't have a path, a default path will be used.
        - The default path allows the library to function in environments without HEC-RAS installed.
        - This enables the HEC-Commander GPT to operate without stopping, even if HEC-RAS is not present.
        - End users MUST use logging to check for operational errors, as full code stops prevent the GPT from running.
    """
    if ras_version is None:
        if hasattr(ras, 'ras_exe_path') and ras.ras_exe_path:
            logger.debug(f"Using HEC-RAS executable from global 'ras' object: {ras.ras_exe_path}")
            return ras.ras_exe_path
        else:
            default_path = Path("Ras.exe")
            logger.warning(f"No HEC-RAS version specified and global 'ras' object not initialized or missing ras_exe_path. Using default path: {default_path}. The RAS Commander (ras-commander) Library Assistant can ignore this error since it does not have HEC-RAS installed.")
            return str(default_path)
    
    ras_version_numbers = [
        "6.6", "6.5", "6.4.1", "6.3.1", "6.3", "6.2", "6.1", "6.0",
        "5.0.7", "5.0.6", "5.0.5", "5.0.4", "5.0.3", "5.0.1", "5.0",
        "4.1", "4.0", "3.1.3", "3.1.2", "3.1.1", "3.0", "2.2"
    ]
    
    hecras_path = Path(ras_version)
    
    if hecras_path.is_file() and hecras_path.suffix.lower() == '.exe':
        logger.debug(f"HEC-RAS executable found at specified path: {hecras_path}")
        return str(hecras_path)
    
    if ras_version in ras_version_numbers:
        default_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{ras_version}/Ras.exe")
        if default_path.is_file():
            logger.debug(f"HEC-RAS executable found at default path: {default_path}")
            return str(default_path)
        else:
            logger.critical(f"HEC-RAS executable not found at the expected path: {default_path}")
    
    try:
        version_float = float(ras_version)
        if version_float > max(float(v) for v in ras_version_numbers):
            newer_version_path = Path(f"C:/Program Files (x86)/HEC/HEC-RAS/{ras_version}/Ras.exe")
            if newer_version_path.is_file():
                logger.debug(f"Newer version of HEC-RAS executable found at: {newer_version_path}")
                return str(newer_version_path)
            else:
                logger.critical("Newer version of HEC-RAS was specified, but the executable was not found.")
    except ValueError:
        pass
    
    logger.error(f"Invalid HEC-RAS version or path: {ras_version}, returning default path: {default_path}")
    return str(default_path)
    

==================================================

File: c:\GH\ras-commander\ras_commander\RasToGo.py
==================================================
"""
RasToGo module provides functions to interface HEC-RAS with go-consequences.
This module helps prepare and format RAS data for use with go-consequences.


-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasToGo:

TO BE IMPLEMENTED: 
- Adding stored maps in rasmaapper for a results file
- Editing the terrain name for stored maps, so that a reduced resolution terrain can be used for mapping
- Re-computing specific plans using the floodplain mapping option to generate stored maps
- Using the stored map output to call go-consequences and compute damages
- Comparisons of go-consequences outputs based on RAS plan number
    - include optional argument with polygons defining areas of interest

"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
import pandas as pd
import numpy as np

from .Decorators import log_call, standardize_input
from .LoggingConfig import setup_logging, get_logger

logger = get_logger(__name__)

class RasToGo:
    """Class containing functions to interface HEC-RAS with go-consequences."""

    #@staticmethod
    #@log_call 
==================================================

File: c:\GH\ras-commander\ras_commander\RasUnsteady.py
==================================================
"""
RasUnsteady - Operations for handling unsteady flow files in HEC-RAS projects.

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).


Example:
    @log_call
    def my_function():
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasUnsteady:
- update_flow_title()
- update_restart_settings()
- extract_boundary_and_tables()
- print_boundaries_and_tables()
- identify_tables()
- parse_fixed_width_table()
- extract_tables()
- write_table_to_file()
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from .LoggingConfig import get_logger
from .Decorators import log_call
import pandas as pd
import numpy as np
import re
from typing import Union, Optional, Any, Tuple, Dict, List



logger = get_logger(__name__)

# Module code starts here

class RasUnsteady:
    """
    Class for all operations related to HEC-RAS unsteady flow files.
    """
    @staticmethod
    @log_call
    def update_flow_title(unsteady_file: str, new_title: str, ras_object: Optional[Any] = None) -> None:
        """
        Update the Flow Title in an unsteady flow file.
        
        Parameters:
        unsteady_file (str): Full path to the unsteady flow file
        new_title (str): New flow title (max 24 characters)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Note:
            This function updates the ras object's unsteady dataframe after modifying the unsteady flow file.
        
        Example:
            from ras_commander import RasCmdr
            
            # Initialize RAS project
            ras_cmdr = RasCmdr()
            ras_cmdr.init_ras_project(project_folder, ras_version)
            
            # Update flow title
            unsteady_file = r"path/to/unsteady_file.u01"
            new_title = "New Flow Title"
            RasUnsteady.update_flow_title(unsteady_file, new_title, ras_object=ras_cmdr.ras)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        new_title = new_title[:24]  # Truncate to 24 characters if longer
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        for i, line in enumerate(lines):
            if line.startswith("Flow Title="):
                old_title = line.strip().split('=')[1]
                lines[i] = f"Flow Title={new_title}\n"
                updated = True
                logger.info(f"Updated Flow Title from '{old_title}' to '{new_title}'")
                break
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied Flow Title modification to {unsteady_file}")
        else:
            logger.warning(f"Flow Title not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def update_restart_settings(unsteady_file: str, use_restart: bool, restart_filename: Optional[str] = None, ras_object: Optional[Any] = None) -> None:
        """
        Update the Use Restart settings in an unsteady flow file.
        
        Parameters:
        unsteady_file (str): Full path to the unsteady flow file
        use_restart (bool): Whether to use restart (True) or not (False)
        restart_filename (str, optional): Name of the restart file (required if use_restart is True)
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        None

        Note:
            This function updates the ras object's unsteady dataframe after modifying the unsteady flow file.
        
        Example:
            from ras_commander import RasCmdr
            
            # Initialize RAS project
            ras_cmdr = RasCmdr()
            ras_cmdr.init_ras_project(project_folder, ras_version)
            
            # Update restart settings
            unsteady_file = r"path/to/unsteady_file.u01"
            RasUnsteady.update_restart_settings(unsteady_file, True, "restartfile.rst", ras_object=ras_cmdr.ras)
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
        
        unsteady_path = Path(unsteady_file)
        
        try:
            with open(unsteady_path, 'r') as f:
                lines = f.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise FileNotFoundError(f"Unsteady flow file not found: {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise PermissionError(f"Permission denied when reading unsteady flow file: {unsteady_path}")
        
        updated = False
        restart_line_index = None
        for i, line in enumerate(lines):
            if line.startswith("Use Restart="):
                restart_line_index = i
                old_value = line.strip().split('=')[1]
                new_value = "-1" if use_restart else "0"
                lines[i] = f"Use Restart={new_value}\n"
                updated = True
                logger.info(f"Updated Use Restart from {old_value} to {new_value}")
                break
        
        if use_restart:
            if not restart_filename:
                logger.error("Restart filename must be specified when enabling restart.")
                raise ValueError("Restart filename must be specified when enabling restart.")
            if restart_line_index is not None:
                lines.insert(restart_line_index + 1, f"Restart Filename={restart_filename}\n")
                logger.info(f"Added Restart Filename: {restart_filename}")
            else:
                logger.warning("Could not find 'Use Restart' line to insert 'Restart Filename'")
        
        if updated:
            try:
                with open(unsteady_path, 'w') as f:
                    f.writelines(lines)
                logger.debug(f"Successfully wrote modifications to unsteady flow file: {unsteady_path}")
            except PermissionError:
                logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
                raise PermissionError(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            except IOError as e:
                logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
                raise IOError(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            logger.info(f"Applied restart settings modification to {unsteady_file}")
        else:
            logger.warning(f"Use Restart setting not found in {unsteady_file}")
    
        ras_obj.unsteady_df = ras_obj.get_unsteady_entries()

    @staticmethod
    @log_call
    def extract_boundary_and_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> pd.DataFrame:
        """
        Extracts Boundary Location blocks, DSS File entries, and their associated 
        fixed-width tables from the specified unsteady file.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        table_types = [
            'Flow Hydrograph=', 
            'Gate Openings=', 
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=', 
            'Lateral Inflow Hydrograph='
        ]
        
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Initialize variables
        boundary_data = []
        current_boundary = None
        current_tables = {}
        current_table = None
        table_values = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # Check for Boundary Location line
            if line.startswith("Boundary Location="):
                # Save previous boundary if it exists
                if current_boundary is not None:
                    if current_table and table_values:
                        # Process any remaining table
                        try:
                            df = pd.DataFrame({'Value': table_values})
                            current_tables[current_table_name] = df
                        except Exception as e:
                            logger.warning(f"Error processing table {current_table_name}: {e}")
                    current_boundary['Tables'] = current_tables
                    boundary_data.append(current_boundary)
                
                # Start new boundary
                current_boundary = {
                    'Boundary Location': line.split('=', 1)[1].strip(),
                    'DSS File': '',
                    'Tables': {}
                }
                current_tables = {}
                current_table = None
                table_values = []
                
            # Check for DSS File line
            elif line.startswith("DSS File=") and current_boundary is not None:
                current_boundary['DSS File'] = line.split('=', 1)[1].strip()
                
            # Check for table headers
            elif any(line.startswith(t) for t in table_types) and current_boundary is not None:
                # If we were processing a table, save it
                if current_table and table_values:
                    try:
                        df = pd.DataFrame({'Value': table_values})
                        current_tables[current_table_name] = df
                    except Exception as e:
                        logger.warning(f"Error processing previous table: {e}")
                
                # Start new table
                try:
                    current_table = line.split('=')
                    current_table_name = current_table[0].strip()
                    num_values = int(current_table[1])
                    table_values = []
                    
                    # Read the table values
                    rows_needed = (num_values + 9) // 10  # Round up division
                    for _ in range(rows_needed):
                        i += 1
                        if i >= len(lines):
                            break
                        row = lines[i].strip()
                        # Parse fixed-width values (8 characters each)
                        j = 0
                        while j < len(row):
                            value_str = row[j:j+8].strip()
                            if value_str:
                                try:
                                    value = float(value_str)
                                    table_values.append(value)
                                except ValueError:
                                    # Try splitting merged values
                                    parts = re.findall(r'-?\d+\.?\d*', value_str)
                                    table_values.extend([float(p) for p in parts])
                            j += 8
                
                except (ValueError, IndexError) as e:
                    logger.error(f"Error processing table at line {i}: {e}")
                    current_table = None
                    
            i += 1
        
        # Add the last boundary if it exists
        if current_boundary is not None:
            if current_table and table_values:
                try:
                    df = pd.DataFrame({'Value': table_values})
                    current_tables[current_table_name] = df
                except Exception as e:
                    logger.warning(f"Error processing final table: {e}")
            current_boundary['Tables'] = current_tables
            boundary_data.append(current_boundary)
        
        # Create DataFrame
        boundaries_df = pd.DataFrame(boundary_data)
        if not boundaries_df.empty:
            # Split boundary location into components
            location_columns = ['River Name', 'Reach Name', 'River Station', 
                              'Downstream River Station', 'Storage Area Connection',
                              'Storage Area Name', 'Pump Station Name', 
                              'Blank 1', 'Blank 2']
            split_locations = boundaries_df['Boundary Location'].str.split(',', expand=True)
            # Ensure we have the right number of columns
            for i, col in enumerate(location_columns):
                if i < split_locations.shape[1]:
                    boundaries_df[col] = split_locations[i].str.strip()
                else:
                    boundaries_df[col] = ''
            boundaries_df = boundaries_df.drop(columns=['Boundary Location'])
        
        logger.info(f"Successfully extracted boundaries and tables from {unsteady_path}")
        return boundaries_df

    @staticmethod
    @log_call
    def print_boundaries_and_tables(boundaries_df: pd.DataFrame) -> None:
        """
        Prints the boundaries and their associated tables from the extracted DataFrame.
        
        Parameters:
        - boundaries_df: DataFrame containing boundary information and nested tables data
        """
        pd.set_option('display.max_columns', None)
        pd.set_option('display.max_rows', None)
        print("\nBoundaries and Tablesin boundaries_df:")
        for idx, row in boundaries_df.iterrows():
            print(f"\nBoundary {idx+1}:")
            print(f"River Name: {row['River Name']}")
            print(f"Reach Name: {row['Reach Name']}")
            print(f"River Station: {row['River Station']}")
            print(f"DSS File: {row['DSS File']}")
            
            if row['Tables']:
                print("\nTables for this boundary:")
                for table_name, table_df in row['Tables'].items():
                    print(f"\n{table_name}:")
                    print(table_df.to_string())
            print("-" * 80)





# Additional functions from the AWS webinar where the code was developed
# Need to add examples

    @staticmethod
    @log_call
    def identify_tables(lines: List[str]) -> List[Tuple[str, int, int]]:
        """
        Identify the start and end of each table in the unsteady flow file.
        
        Parameters:
        lines (List[str]): List of file lines
        
        Returns:
        List[Tuple[str, int, int]]: List of tuples containing (table_name, start_line, end_line)
        """
        table_types = [
            'Flow Hydrograph=', 
            'Gate Openings=', 
            'Stage Hydrograph=',
            'Uniform Lateral Inflow=', 
            'Lateral Inflow Hydrograph='
        ]
        tables = []
        current_table = None
        
        for i, line in enumerate(lines):
            if any(table_type in line for table_type in table_types):
                if current_table:
                    tables.append((current_table[0], current_table[1], i-1))
                table_name = line.strip().split('=')[0] + '='
                try:
                    num_values = int(line.strip().split('=')[1])
                    current_table = (table_name, i+1, num_values)
                except (ValueError, IndexError) as e:
                    logger.error(f"Error parsing table header at line {i}: {e}")
                    continue
        
        if current_table:
            tables.append((current_table[0], current_table[1], 
                          current_table[1] + (current_table[2] + 9) // 10))
        
        logger.debug(f"Identified {len(tables)} tables in the file")
        return tables

    @staticmethod
    @log_call
    def parse_fixed_width_table(lines: List[str], start: int, end: int) -> pd.DataFrame:
        """
        Parse a fixed-width table into a pandas DataFrame.
        
        Parameters:
        lines (List[str]): List of file lines
        start (int): Starting line number for table
        end (int): Ending line number for table
        
        Returns:
        pd.DataFrame: DataFrame containing parsed table values
        """
        data = []
        for line in lines[start:end]:
            # Skip empty lines or lines that don't contain numeric data
            if not line.strip() or not any(c.isdigit() for c in line):
                continue
                
            # Split the line into 8-character columns and process each value
            values = []
            for i in range(0, len(line.rstrip()), 8):
                value_str = line[i:i+8].strip()
                if value_str:  # Only process non-empty strings
                    try:
                        # Handle special cases where numbers are run together
                        if len(value_str) > 8:
                            # Use regex to find all numbers in the string
                            parts = re.findall(r'-?\d+\.?\d*', value_str)
                            values.extend([float(p) for p in parts])
                        else:
                            values.append(float(value_str))
                    except ValueError:
                        # If conversion fails, try to extract any valid numbers from the string
                        parts = re.findall(r'-?\d+\.?\d*', value_str)
                        if parts:
                            values.extend([float(p) for p in parts])
                        else:
                            logger.debug(f"Skipping non-numeric value: {value_str}")
                            continue
            
            # Only add to data if we found valid numeric values
            if values:
                data.extend(values)
        
        if not data:
            logger.warning("No numeric data found in table section")
            return pd.DataFrame(columns=['Value'])
            
        return pd.DataFrame(data, columns=['Value'])
    
    @staticmethod
    @log_call
    def extract_tables(unsteady_file: str, ras_object: Optional[Any] = None) -> Dict[str, pd.DataFrame]:
        """
        Extract all tables from the unsteady flow file and return them as DataFrames.
        
        Parameters:
        unsteady_file (str): Path to the unsteady flow file
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        
        Returns:
        Dict[str, pd.DataFrame]: Dictionary of table names to DataFrames
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Fix: Use RasUnsteady.identify_tables 
        tables = RasUnsteady.identify_tables(lines)
        extracted_tables = {}
        
        for table_name, start, end in tables:
            df = RasUnsteady.parse_fixed_width_table(lines, start, end)
            extracted_tables[table_name] = df
            logger.debug(f"Extracted table '{table_name}' with {len(df)} values")
        
        return extracted_tables

    @staticmethod
    @log_call
    def write_table_to_file(unsteady_file: str, table_name: str, df: pd.DataFrame, 
                           start_line: int, ras_object: Optional[Any] = None) -> None:
        """
        Write updated table back to file in fixed-width format.
        
        Parameters:
        unsteady_file (str): Path to the unsteady flow file
        table_name (str): Name of the table to update
        df (pd.DataFrame): DataFrame containing the updated values
        start_line (int): Line number where the table starts
        ras_object (RasPrj, optional): Specific RAS object to use. If None, uses the global ras instance.
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        unsteady_path = Path(unsteady_file)
        try:
            with open(unsteady_path, 'r') as file:
                lines = file.readlines()
            logger.debug(f"Successfully read unsteady flow file: {unsteady_path}")
        except FileNotFoundError:
            logger.error(f"Unsteady flow file not found: {unsteady_path}")
            raise
        except PermissionError:
            logger.error(f"Permission denied when reading unsteady flow file: {unsteady_path}")
            raise
        
        # Format values into fixed-width strings
        formatted_values = []
        for i in range(0, len(df), 10):
            row = df['Value'].iloc[i:i+10]
            formatted_row = ''.join(f'{value:8.0f}' for value in row)
            formatted_values.append(formatted_row + '\n')
        
        # Replace old table with new formatted values
        lines[start_line:start_line+len(formatted_values)] = formatted_values
        
        try:
            with open(unsteady_path, 'w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated table '{table_name}' in {unsteady_path}")
        except PermissionError:
            logger.error(f"Permission denied when writing to unsteady flow file: {unsteady_path}")
            raise
        except IOError as e:
            logger.error(f"Error writing to unsteady flow file: {unsteady_path}. {str(e)}")
            raise








'''



Flow Title=Single 2D Area with Bridges
Program Version=6.60
Use Restart= 0 
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DSNormalDepth                   ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalD                      ,                                
Friction Slope=0.0003,0
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,Upstream Inflow                 ,                                
Interval=1HOUR
Flow Hydrograph= 200 
    1000    3000    6500    8000    9500   11000   12500   14000   15500   17000
   18500   20000   22000   24000   26000   28000   30000   34000   38000   42000
   46000   50000   54000   58000   62000   66000   70000   73000   76000   79000
   82000   85000   87200   89400   91600   93800   96000   96800   97600   98400
   99200  100000   99600   99200   98800   98400   98000   96400   94800   93200
   91600   90000   88500   87000   85500   84000   82500   81000   79500   78000
   76500   75000   73500   7200070666.6669333.34   6800066666.6665333.33   64000
62666.6761333.33   6000058666.6757333.33   5600054666.6753333.33   5200050666.67
49333.33   4800046666.6745333.33   4400042666.6741333.33   4000039166.6738333.33
   3750036666.6735833.33   3500034166.6733333.33   3250031666.6730833.33   30000
29166.6728333.33   2750026666.6725833.33   2500024166.6723333.33   2250021666.67
20833.33   2000019655.1719310.3518965.5218620.6918275.8617931.0417586.2117241.38
16896.5516551.72 16206.915862.0715517.2415172.4114827.5914482.7614137.93 13793.1
13448.2813103.4512758.6212413.7912068.9711724.1411379.3111034.4810689.6610344.83
   10000 9915.25 9830.51 9745.76 9661.02 9576.27 9491.53 9406.78 9322.03 9237.29
 9152.54  9067.8 8983.05 8898.31 8813.56 8728.81 8644.07 8559.32 8474.58 8389.83
 8305.09 8220.34 8135.59 8050.85  7966.1 7881.36 7796.61 7711.86 7627.12 7542.37
 7457.63 7372.88 7288.14 7203.39 7118.64  7033.9 6949.15 6864.41 6779.66 6694.92
 6610.17 6525.42 6440.68 6355.93 6271.19 6186.44  6101.7 6016.95  5932.2 5847.46
 5762.71 5677.97 5593.22 5508.48 5423.73 5338.98 5254.24 5169.49 5084.75    5000
Stage Hydrograph TW Check=0
Flow Hydrograph QMult= 0.5 
Flow Hydrograph Slope= 0.0005 
DSS Path=
Use DSS=False
Use Fixed Start Time=False
Fixed Start Date/Time=,
Is Critical Boundary=False
Critical Boundary Flow=
Boundary Location=                ,                ,        ,        ,Sayers Dam      ,                ,                ,                                ,                                
Gate Name=Gate #1     
Gate DSS Path=
Gate Use DSS=False
Gate Time Interval=1HOUR
Gate Use Fixed Start Time=False
Gate Fixed Start Date/Time=,
Gate Openings= 100 
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
       2       2       2       2       2       2       2       2       2       2
Boundary Location=                ,                ,        ,        ,                ,BaldEagleCr     ,                ,DS2NormalDepth                  ,                                
Friction Slope=0.0003,0
Met Point Raster Parameters=,,,,
Precipitation Mode=Disable
Wind Mode=No Wind Forces
Air Density Mode=
Wave Mode=No Wave Forcing
Met BC=Precipitation|Expanded View=0
Met BC=Precipitation|Point Interpolation=Nearest
Met BC=Precipitation|Gridded Source=DSS
Met BC=Precipitation|Gridded Interpolation=
Met BC=Evapotranspiration|Expanded View=0
Met BC=Evapotranspiration|Point Interpolation=Nearest
Met BC=Evapotranspiration|Gridded Source=DSS
Met BC=Evapotranspiration|Gridded Interpolation=
Met BC=Wind Speed|Expanded View=0
Met BC=Wind Speed|Constant Units=ft/s
Met BC=Wind Speed|Point Interpolation=Nearest
Met BC=Wind Speed|Gridded Source=DSS
Met BC=Wind Speed|Gridded Interpolation=
Met BC=Wind Direction|Expanded View=0
Met BC=Wind Direction|Point Interpolation=Nearest
Met BC=Wind Direction|Gridded Source=DSS
Met BC=Wind Direction|Gridded Interpolation=
Met BC=Wind Velocity X|Expanded View=0
Met BC=Wind Velocity X|Constant Units=ft/s
Met BC=Wind Velocity X|Point Interpolation=Nearest
Met BC=Wind Velocity X|Gridded Source=DSS
Met BC=Wind Velocity X|Gridded Interpolation=
Met BC=Wind Velocity Y|Expanded View=0
Met BC=Wind Velocity Y|Constant Units=ft/s
Met BC=Wind Velocity Y|Point Interpolation=Nearest
Met BC=Wind Velocity Y|Gridded Source=DSS
Met BC=Wind Velocity Y|Gridded Interpolation=
Met BC=Wave Forcing X|Expanded View=0
Met BC=Wave Forcing X|Point Interpolation=Nearest
Met BC=Wave Forcing X|Gridded Source=DSS
Met BC=Wave Forcing X|Gridded Interpolation=
Met BC=Wave Forcing Y|Expanded View=0
Met BC=Wave Forcing Y|Point Interpolation=Nearest
Met BC=Wave Forcing Y|Gridded Source=DSS
Met BC=Wave Forcing Y|Gridded Interpolation=
Met BC=Air Density|Mode=Constant
Met BC=Air Density|Expanded View=0
Met BC=Air Density|Constant Value=1.225
Met BC=Air Density|Constant Units=kg/m3
Met BC=Air Density|Point Interpolation=Nearest
Met BC=Air Density|Gridded Source=DSS
Met BC=Air Density|Gridded Interpolation=
Met BC=Air Temperature|Expanded View=0
Met BC=Air Temperature|Point Interpolation=Nearest
Met BC=Air Temperature|Gridded Source=DSS
Met BC=Air Temperature|Gridded Interpolation=
Met BC=Humidity|Expanded View=0
Met BC=Humidity|Point Interpolation=Nearest
Met BC=Humidity|Gridded Source=DSS
Met BC=Humidity|Gridded Interpolation=
Met BC=Air Pressure|Mode=Constant
Met BC=Air Pressure|Expanded View=0
Met BC=Air Pressure|Constant Value=1013.2
Met BC=Air Pressure|Constant Units=mb
Met BC=Air Pressure|Point Interpolation=Nearest
Met BC=Air Pressure|Gridded Source=DSS
Met BC=Air Pressure|Gridded Interpolation=
Non-Newtonian Method= 0 , 
Non-Newtonian Constant Vol Conc=0
Non-Newtonian Yield Method= 0 , 
Non-Newtonian Yield Coef=0, 0
User Yeild=   0
Non-Newtonian Sed Visc= 0 , 
Non-Newtonian Obrian B=0
User Viscosity=0
User Viscosity Ratio=0
Herschel-Bulkley Coef=0, 0
Clastic Method= 0 , 
Coulomb Phi=0
Voellmy X=0
Non-Newtonian Hindered FV= 0 
Non-Newtonian FV K=0
Non-Newtonian ds=0
Non-Newtonian Max Cv=0
Non-Newtonian Bulking Method= 0 , 
Non-Newtonian High C Transport= 0 , 
Lava Activation= 0 
Temperature=1300,15,,15,14,980
Heat Ballance=1,1200,0.5,1,70,0.95
Viscosity=1000,,,
Yield Strength=,,,
Consistency Factor=,,,
Profile Coefficient=4,1.3,
Lava Param=,2500,




'''





==================================================

File: c:\GH\ras-commander\ras_commander\RasUtils.py
==================================================
"""
RasUtils - Utility functions for the ras-commander library

This module is part of the ras-commander library and uses a centralized logging configuration.

Logging Configuration:
- The logging is set up in the logging_config.py file.
- A @log_call decorator is available to automatically log function calls.
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Logs are written to both console and a rotating file handler.
- The default log file is 'ras_commander.log' in the 'logs' directory.
- The default log level is INFO.

To use logging in this module:
1. Use the @log_call decorator for automatic function call logging.
2. For additional logging, use logger.[level]() calls (e.g., logger.info(), logger.debug()).

Example:
    @log_call
    def my_function():
        logger.debug("Additional debug information")
        # Function logic here
        
-----

All of the methods in this class are static and are designed to be used without instantiation.

List of Functions in RasUtils:
- create_directory()
- find_files_by_extension()
- get_file_size()
- get_file_modification_time()
- get_plan_path()
- remove_with_retry()
- update_plan_file()
- check_file_access()
- convert_to_dataframe()
- save_to_excel()
- calculate_rmse()
- calculate_percent_bias()
- calculate_error_metrics()
- update_file()
- get_next_number()
- clone_file()
- update_project_file()
- decode_byte_strings()
- perform_kdtree_query()
- find_nearest_neighbors()
- consolidate_dataframe()
- find_nearest_value()
- horizontal_distance()
    
        
"""
import os
from pathlib import Path
from .RasPrj import ras
from typing import Union, Optional, Dict, Callable, List, Tuple, Any
import pandas as pd
import numpy as np
import shutil
import re
from scipy.spatial import KDTree
import datetime
import time
import h5py
from datetime import timedelta
from .LoggingConfig import get_logger
from .Decorators import log_call


logger = get_logger(__name__)
# Module code starts here

class RasUtils:
    """
    A class containing utility functions for the ras-commander library.
    When integrating new functions that do not clearly fit into other classes, add them here.
    """

    @staticmethod
    @log_call
    def create_directory(directory_path: Path, ras_object=None) -> Path:
        """
        Ensure that a directory exists, creating it if necessary.

        Parameters:
        directory_path (Path): Path to the directory
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Path to the ensured directory

        Example:
        >>> ensured_dir = RasUtils.create_directory(Path("output"))
        >>> print(f"Directory ensured: {ensured_dir}")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(directory_path)
        try:
            path.mkdir(parents=True, exist_ok=True)
            logger.info(f"Directory ensured: {path}")
        except Exception as e:
            logger.error(f"Failed to create directory {path}: {e}")
            raise
        return path

    @staticmethod
    @log_call
    def find_files_by_extension(extension: str, ras_object=None) -> list:
        """
        List all files in the project directory with a specific extension.

        Parameters:
        extension (str): File extension to filter (e.g., '.prj')
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        list: List of file paths matching the extension

        Example:
        >>> prj_files = RasUtils.find_files_by_extension('.prj')
        >>> print(f"Found {len(prj_files)} .prj files")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            files = list(ras_obj.project_folder.glob(f"*{extension}"))
            file_list = [str(file) for file in files]
            logger.info(f"Found {len(file_list)} files with extension '{extension}' in {ras_obj.project_folder}")
            return file_list
        except Exception as e:
            logger.error(f"Failed to find files with extension '{extension}': {e}")
            raise

    @staticmethod
    @log_call
    def get_file_size(file_path: Path, ras_object=None) -> Optional[int]:
        """
        Get the size of a file in bytes.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[int]: Size of the file in bytes, or None if the file does not exist

        Example:
        >>> size = RasUtils.get_file_size(Path("project.prj"))
        >>> print(f"File size: {size} bytes")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                size = path.stat().st_size
                logger.info(f"Size of {path}: {size} bytes")
                return size
            except Exception as e:
                logger.error(f"Failed to get size for {path}: {e}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def get_file_modification_time(file_path: Path, ras_object=None) -> Optional[float]:
        """
        Get the last modification time of a file.

        Parameters:
        file_path (Path): Path to the file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Optional[float]: Last modification time as a timestamp, or None if the file does not exist

        Example:
        >>> mtime = RasUtils.get_file_modification_time(Path("project.prj"))
        >>> print(f"Last modified: {mtime}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        path = Path(file_path)
        if path.exists():
            try:
                mtime = path.stat().st_mtime
                logger.info(f"Last modification time of {path}: {mtime}")
                return mtime
            except Exception as e:
                logger.exception(f"Failed to get modification time for {path}")
                raise
        else:
            logger.warning(f"File not found: {path}")
            return None

    @staticmethod
    @log_call
    def get_plan_path(current_plan_number_or_path: Union[str, Path], ras_object=None) -> Path:
        """
        Get the path for a plan file with a given plan number or path.

        Parameters:
        current_plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        Path: Full path to the plan file

        Example:
        >>> plan_path = RasUtils.get_plan_path(1)
        >>> print(f"Plan file path: {plan_path}")
        >>> plan_path = RasUtils.get_plan_path("path/to/plan.p01")
        >>> print(f"Plan file path: {plan_path}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        plan_path = Path(current_plan_number_or_path)
        if plan_path.is_file():
            logger.info(f"Using provided plan file path: {plan_path}")
            return plan_path
        
        try:
            current_plan_number = f"{int(current_plan_number_or_path):02d}"  # Ensure two-digit format
            logger.debug(f"Converted plan number to two-digit format: {current_plan_number}")
        except ValueError:
            logger.error(f"Invalid plan number: {current_plan_number_or_path}. Expected a number from 1 to 99.")
            raise ValueError(f"Invalid plan number: {current_plan_number_or_path}. Expected a number from 1 to 99.")
        
        plan_name = f"{ras_obj.project_name}.p{current_plan_number}"
        full_plan_path = ras_obj.project_folder / plan_name
        logger.info(f"Constructed plan file path: {full_plan_path}")
        return full_plan_path

    @staticmethod
    @log_call
    def remove_with_retry(
        path: Path,
        max_attempts: int = 5,
        initial_delay: float = 1.0,
        is_folder: bool = True,
        ras_object=None
    ) -> bool:
        """
        Attempts to remove a file or folder with retry logic and exponential backoff.

        Parameters:
        path (Path): Path to the file or folder to be removed.
        max_attempts (int): Maximum number of removal attempts.
        initial_delay (float): Initial delay between attempts in seconds.
        is_folder (bool): If True, the path is treated as a folder; if False, it's treated as a file.
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Returns:
        bool: True if the file or folder was successfully removed, False otherwise.

        Example:
        >>> success = RasUtils.remove_with_retry(Path("temp_folder"), is_folder=True)
        >>> print(f"Removal successful: {success}")
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()

        path = Path(path)
        for attempt in range(1, max_attempts + 1):
            try:
                if path.exists():
                    if is_folder:
                        shutil.rmtree(path)
                        logger.info(f"Folder removed: {path}")
                    else:
                        path.unlink()
                        logger.info(f"File removed: {path}")
                else:
                    logger.info(f"Path does not exist, nothing to remove: {path}")
                return True
            except PermissionError as pe:
                if attempt < max_attempts:
                    delay = initial_delay * (2 ** (attempt - 1))  # Exponential backoff
                    logger.warning(
                        f"PermissionError on attempt {attempt} to remove {path}: {pe}. "
                        f"Retrying in {delay} seconds..."
                    )
                    time.sleep(delay)
                else:
                    logger.error(
                        f"Failed to remove {path} after {max_attempts} attempts due to PermissionError: {pe}. Skipping."
                    )
                    return False
            except Exception as e:
                logger.exception(f"Failed to remove {path} on attempt {attempt}")
                return False
        return False

    @staticmethod
    @log_call
    def update_plan_file(
        plan_number_or_path: Union[str, Path],
        file_type: str,
        entry_number: int,
        ras_object=None
    ) -> None:
        """
        Update a plan file with a new file reference.

        Parameters:
        plan_number_or_path (Union[str, Path]): The plan number (1 to 99) or full path to the plan file
        file_type (str): Type of file to update ('Geom', 'Flow', or 'Unsteady')
        entry_number (int): Number (from 1 to 99) to set
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Raises:
        ValueError: If an invalid file_type is provided
        FileNotFoundError: If the plan file doesn't exist

        Example:
        >>> RasUtils.update_plan_file(1, "Geom", 2)
        >>> RasUtils.update_plan_file("path/to/plan.p01", "Geom", 2)
        """
        
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        valid_file_types = {'Geom': 'g', 'Flow': 'f', 'Unsteady': 'u'}
        if file_type not in valid_file_types:
            logger.error(
                f"Invalid file_type '{file_type}'. Expected one of: {', '.join(valid_file_types.keys())}"
            )
            raise ValueError(
                f"Invalid file_type. Expected one of: {', '.join(valid_file_types.keys())}"
            )

        plan_file_path = Path(plan_number_or_path)
        if not plan_file_path.is_file():
            plan_file_path = RasUtils.get_plan_path(plan_number_or_path, ras_object)
            if not plan_file_path.exists():
                logger.error(f"Plan file not found: {plan_file_path}")
                raise FileNotFoundError(f"Plan file not found: {plan_file_path}")
        
        file_prefix = valid_file_types[file_type]
        search_pattern = f"{file_type} File="
        formatted_entry_number = f"{int(entry_number):02d}"  # Ensure two-digit format

        try:
            RasUtils.check_file_access(plan_file_path, 'r')
            with plan_file_path.open('r') as file:
                lines = file.readlines()
        except Exception as e:
            logger.exception(f"Failed to read plan file {plan_file_path}")
            raise

        updated = False
        for i, line in enumerate(lines):
            if line.startswith(search_pattern):
                lines[i] = f"{search_pattern}{file_prefix}{formatted_entry_number}\n"
                logger.info(
                    f"Updated {file_type} File in {plan_file_path} to {file_prefix}{formatted_entry_number}"
                )
                updated = True
                break

        if not updated:
            logger.warning(
                f"Search pattern '{search_pattern}' not found in {plan_file_path}. No update performed."
            )

        try:
            with plan_file_path.open('w') as file:
                file.writelines(lines)
            logger.info(f"Successfully updated plan file: {plan_file_path}")
        except Exception as e:
            logger.exception(f"Failed to write updates to plan file {plan_file_path}")
            raise

        # Refresh RasPrj dataframes
        try:
            ras_obj.plan_df = ras_obj.get_plan_entries()
            ras_obj.geom_df = ras_obj.get_geom_entries()
            ras_obj.flow_df = ras_obj.get_flow_entries()
            ras_obj.unsteady_df = ras_obj.get_unsteady_entries()
            logger.info("RAS object dataframes have been refreshed.")
        except Exception as e:
            logger.exception("Failed to refresh RasPrj dataframes")
            raise

    @staticmethod
    @log_call
    def check_file_access(file_path: Path, mode: str = 'r') -> None:
        """
        Check if the file can be accessed with the specified mode.

        Parameters:
        file_path (Path): Path to the file
        mode (str): Mode to check ('r' for read, 'w' for write, etc.)

        Raises:
        FileNotFoundError: If the file does not exist
        PermissionError: If the required permissions are not met
        """
        
        path = Path(file_path)
        if not path.exists():
            logger.error(f"File not found: {file_path}")
            raise FileNotFoundError(f"File not found: {file_path}")
        
        if mode in ('r', 'rb'):
            if not os.access(path, os.R_OK):
                logger.error(f"Read permission denied for file: {file_path}")
                raise PermissionError(f"Read permission denied for file: {file_path}")
            else:
                logger.debug(f"Read access granted for file: {file_path}")
        
        if mode in ('w', 'wb', 'a', 'ab'):
            parent_dir = path.parent
            if not os.access(parent_dir, os.W_OK):
                logger.error(f"Write permission denied for directory: {parent_dir}")
                raise PermissionError(f"Write permission denied for directory: {parent_dir}")
            else:
                logger.debug(f"Write access granted for directory: {parent_dir}")


    @staticmethod
    @log_call
    def convert_to_dataframe(data_source: Union[pd.DataFrame, Path], **kwargs) -> pd.DataFrame:
        """
        Converts input to a pandas DataFrame. Supports existing DataFrames or file paths (CSV, Excel, TSV, Parquet).

        Args:
            data_source (Union[pd.DataFrame, Path]): The input to convert to a DataFrame. Can be a file path or an existing DataFrame.
            **kwargs: Additional keyword arguments to pass to pandas read functions.

        Returns:
            pd.DataFrame: The resulting DataFrame.

        Raises:
            NotImplementedError: If the file type is unsupported or input type is invalid.

        Example:
            >>> df = RasUtils.convert_to_dataframe(Path("data.csv"))
            >>> print(type(df))
            <class 'pandas.core.frame.DataFrame'>
        """
        if isinstance(data_source, pd.DataFrame):
            logger.debug("Input is already a DataFrame, returning a copy.")
            return data_source.copy()
        elif isinstance(data_source, Path):
            ext = data_source.suffix.replace('.', '', 1)
            logger.info(f"Converting file with extension '{ext}' to DataFrame.")
            if ext == 'csv':
                return pd.read_csv(data_source, **kwargs)
            elif ext.startswith('x'):
                return pd.read_excel(data_source, **kwargs)
            elif ext == "tsv":
                return pd.read_csv(data_source, sep="\t", **kwargs)
            elif ext in ["parquet", "pq", "parq"]:
                return pd.read_parquet(data_source, **kwargs)
            else:
                logger.error(f"Unsupported file type: {ext}")
                raise NotImplementedError(f"Unsupported file type {ext}. Should be one of csv, tsv, parquet, or xlsx.")
        else:
            logger.error(f"Unsupported input type: {type(data_source)}")
            raise NotImplementedError(f"Unsupported type {type(data_source)}. Only file path / existing DataFrame supported at this time")

    @staticmethod
    @log_call
    def save_to_excel(dataframe: pd.DataFrame, excel_path: Path, **kwargs) -> None:
        """
        Saves a pandas DataFrame to an Excel file with retry functionality.

        Args:
            dataframe (pd.DataFrame): The DataFrame to save.
            excel_path (Path): The path to the Excel file where the DataFrame will be saved.
            **kwargs: Additional keyword arguments passed to `DataFrame.to_excel()`.

        Raises:
            IOError: If the file cannot be saved after multiple attempts.

        Example:
            >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
            >>> RasUtils.save_to_excel(df, Path('output.xlsx'))
        """
        saved = False
        max_attempts = 3
        attempt = 0

        while not saved and attempt < max_attempts:
            try:
                dataframe.to_excel(excel_path, **kwargs)
                logger.info(f'DataFrame successfully saved to {excel_path}')
                saved = True
            except IOError as e:
                attempt += 1
                if attempt < max_attempts:
                    logger.warning(f"Error saving file. Attempt {attempt} of {max_attempts}. Please close the Excel document if it's open.")
                else:
                    logger.error(f"Failed to save {excel_path} after {max_attempts} attempts.")
                    raise IOError(f"Failed to save {excel_path} after {max_attempts} attempts. Last error: {str(e)}")

    @staticmethod
    @log_call
    def calculate_rmse(observed_values: np.ndarray, predicted_values: np.ndarray, normalized: bool = True) -> float:
        """
        Calculate the Root Mean Squared Error (RMSE) between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            normalized (bool, optional): Whether to normalize RMSE to a percentage of observed_values. Defaults to True.

        Returns:
            float: The calculated RMSE value.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_rmse(observed, predicted)
            0.06396394
        """
        rmse = np.sqrt(np.mean((predicted_values - observed_values) ** 2))
        
        if normalized:
            rmse = rmse / np.abs(np.mean(observed_values))
        
        logger.debug(f"Calculated RMSE: {rmse}")
        return rmse

    @staticmethod
    @log_call
    def calculate_percent_bias(observed_values: np.ndarray, predicted_values: np.ndarray, as_percentage: bool = False) -> float:
        """
        Calculate the Percent Bias between observed and predicted values.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.
            as_percentage (bool, optional): If True, return bias as a percentage. Defaults to False.

        Returns:
            float: The calculated Percent Bias.

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_percent_bias(observed, predicted, as_percentage=True)
            3.33333333
        """
        multiplier = 100 if as_percentage else 1
        
        percent_bias = multiplier * (np.mean(predicted_values) - np.mean(observed_values)) / np.mean(observed_values)
        
        logger.debug(f"Calculated Percent Bias: {percent_bias}")
        return percent_bias

    @staticmethod
    @log_call
    def calculate_error_metrics(observed_values: np.ndarray, predicted_values: np.ndarray) -> Dict[str, float]:
        """
        Compute a trio of error metrics: correlation, RMSE, and Percent Bias.

        Args:
            observed_values (np.ndarray): Actual observations time series.
            predicted_values (np.ndarray): Estimated/predicted time series.

        Returns:
            Dict[str, float]: A dictionary containing correlation ('cor'), RMSE ('rmse'), and Percent Bias ('pb').

        Example:
            >>> observed = np.array([1, 2, 3])
            >>> predicted = np.array([1.1, 2.2, 2.9])
            >>> RasUtils.calculate_error_metrics(observed, predicted)
            {'cor': 0.9993, 'rmse': 0.06396, 'pb': 0.03333}
        """
        correlation = np.corrcoef(observed_values, predicted_values)[0, 1]
        rmse = RasUtils.calculate_rmse(observed_values, predicted_values)
        percent_bias = RasUtils.calculate_percent_bias(observed_values, predicted_values)
        
        metrics = {'cor': correlation, 'rmse': rmse, 'pb': percent_bias}
        logger.info(f"Calculated error metrics: {metrics}")
        return metrics

    
    @staticmethod
    @log_call
    def update_file(file_path: Path, update_function: Callable, *args) -> None:
        """
        Generic method to update a file.

        Parameters:
        file_path (Path): Path to the file to be updated
        update_function (Callable): Function to update the file contents
        *args: Additional arguments to pass to the update_function

        Raises:
        Exception: If there's an error updating the file

        Example:
        >>> def update_content(lines, new_value):
        ...     lines[0] = f"New value: {new_value}\\n"
        ...     return lines
        >>> RasUtils.update_file(Path("example.txt"), update_content, "Hello")
        """
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
            
            updated_lines = update_function(lines, *args) if args else update_function(lines)
            
            with open(file_path, 'w') as f:
                f.writelines(updated_lines)
            logger.info(f"Successfully updated file: {file_path}")
        except Exception as e:
            logger.exception(f"Failed to update file {file_path}")
            raise

    @staticmethod
    @log_call
    def get_next_number(existing_numbers: list) -> str:
        """
        Determine the next available number from a list of existing numbers.

        Parameters:
        existing_numbers (list): List of existing numbers as strings

        Returns:
        str: Next available number as a zero-padded string

        Example:
        >>> RasUtils.get_next_number(["01", "02", "04"])
        "05"
        """
        existing_numbers = sorted(int(num) for num in existing_numbers)
        next_number = max(existing_numbers, default=0) + 1
        return f"{next_number:02d}"

    @staticmethod
    @log_call
    def clone_file(template_path: Path, new_path: Path, update_function: Optional[Callable] = None, *args) -> None:
        """
        Generic method to clone a file and optionally update it.

        Parameters:
        template_path (Path): Path to the template file
        new_path (Path): Path where the new file will be created
        update_function (Optional[Callable]): Function to update the cloned file
        *args: Additional arguments to pass to the update_function

        Raises:
        FileNotFoundError: If the template file doesn't exist

        Example:
        >>> def update_content(lines, new_value):
        ...     lines[0] = f"New value: {new_value}\\n"
        ...     return lines
        >>> RasUtils.clone_file(Path("template.txt"), Path("new.txt"), update_content, "Hello")
        """
        if not template_path.exists():
            logger.error(f"Template file '{template_path}' does not exist.")
            raise FileNotFoundError(f"Template file '{template_path}' does not exist.")

        shutil.copy(template_path, new_path)
        logger.info(f"File cloned from {template_path} to {new_path}")

        if update_function:
            RasUtils.update_file(new_path, update_function, *args)
    @staticmethod
    @log_call
    def update_project_file(prj_file: Path, file_type: str, new_num: str, ras_object=None) -> None:
        """
        Update the project file with a new entry.

        Parameters:
        prj_file (Path): Path to the project file
        file_type (str): Type of file being added (e.g., 'Plan', 'Geom')
        new_num (str): Number of the new file entry
        ras_object (RasPrj, optional): RAS object to use. If None, uses the default ras object.

        Example:
        >>> RasUtils.update_project_file(Path("project.prj"), "Plan", "02")
        """
        ras_obj = ras_object or ras
        ras_obj.check_initialized()
        
        try:
            with open(prj_file, 'r') as f:
                lines = f.readlines()
            
            new_line = f"{file_type} File={file_type[0].lower()}{new_num}\n"
            lines.append(new_line)
            
            with open(prj_file, 'w') as f:
                f.writelines(lines)
            logger.info(f"Project file updated with new {file_type} entry: {new_num}")
        except Exception as e:
            logger.exception(f"Failed to update project file {prj_file}")
            raise
        
  
        
        
    # From FunkShuns
        
    @staticmethod
    @log_call
    def decode_byte_strings(dataframe: pd.DataFrame) -> pd.DataFrame:
        """
        Decodes byte strings in a DataFrame to regular string objects.

        This function converts columns with byte-encoded strings (e.g., b'string') into UTF-8 decoded strings.

        Args:
            dataframe (pd.DataFrame): The DataFrame containing byte-encoded string columns.

        Returns:
            pd.DataFrame: The DataFrame with byte strings decoded to regular strings.

        Example:
            >>> df = pd.DataFrame({'A': [b'hello', b'world'], 'B': [1, 2]})
            >>> decoded_df = RasUtils.decode_byte_strings(df)
            >>> print(decoded_df)
                A  B
            0  hello  1
            1  world  2
        """
        str_df = dataframe.select_dtypes(['object'])
        str_df = str_df.stack().str.decode('utf-8').unstack()
        for col in str_df:
            dataframe[col] = str_df[col]
        return dataframe

    @staticmethod
    @log_call
    def perform_kdtree_query(
        reference_points: np.ndarray,
        query_points: np.ndarray,
        max_distance: float = 2.0
    ) -> np.ndarray:
        """
        Performs a KDTree query between two datasets and returns indices with distances exceeding max_distance set to -1.

        Args:
            reference_points (np.ndarray): The reference dataset for KDTree.
            query_points (np.ndarray): The query dataset to search against KDTree of reference_points.
            max_distance (float, optional): The maximum distance threshold. Indices with distances greater than this are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices from reference_points that are nearest to each point in query_points. 
                        Indices with distances > max_distance are set to -1.

        Example:
            >>> ref_points = np.array([[0, 0], [1, 1], [2, 2]])
            >>> query_points = np.array([[0.5, 0.5], [3, 3]])
            >>> result = RasUtils.perform_kdtree_query(ref_points, query_points)
            >>> print(result)
            array([ 0, -1])
        """
        dist, snap = KDTree(reference_points).query(query_points, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        return snap

    @staticmethod
    @log_call
    def find_nearest_neighbors(points: np.ndarray, max_distance: float = 2.0) -> np.ndarray:
        """
        Creates a self KDTree for dataset points and finds nearest neighbors excluding self, 
        with distances above max_distance set to -1.

        Args:
            points (np.ndarray): The dataset to build the KDTree from and query against itself.
            max_distance (float, optional): The maximum distance threshold. Indices with distances 
                                            greater than max_distance are set to -1. Defaults to 2.0.

        Returns:
            np.ndarray: Array of indices representing the nearest neighbor in points for each point in points. 
                        Indices with distances > max_distance or self-matches are set to -1.

        Example:
            >>> points = np.array([[0, 0], [1, 1], [2, 2], [10, 10]])
            >>> result = RasUtils.find_nearest_neighbors(points)
            >>> print(result)
            array([1, 0, 1, -1])
        """
        dist, snap = KDTree(points).query(points, k=2, distance_upper_bound=max_distance)
        snap[dist > max_distance] = -1
        
        snp = pd.DataFrame(snap, index=np.arange(len(snap)))
        snp = snp.replace(-1, np.nan)
        snp.loc[snp[0] == snp.index, 0] = np.nan
        snp.loc[snp[1] == snp.index, 1] = np.nan
        filled = snp[0].fillna(snp[1])
        snapped = filled.fillna(-1).astype(np.int64).to_numpy()
        return snapped

    @staticmethod
    @log_call
    def consolidate_dataframe(
        dataframe: pd.DataFrame,
        group_by: Optional[Union[str, List[str]]] = None,
        pivot_columns: Optional[Union[str, List[str]]] = None,
        level: Optional[int] = None,
        n_dimensional: bool = False,
        aggregation_method: Union[str, Callable] = 'list'
    ) -> pd.DataFrame:
        """
        Consolidate rows in a DataFrame by merging duplicate values into lists or using a specified aggregation function.

        Args:
            dataframe (pd.DataFrame): The DataFrame to consolidate.
            group_by (Optional[Union[str, List[str]]]): Columns or indices to group by.
            pivot_columns (Optional[Union[str, List[str]]]): Columns to pivot.
            level (Optional[int]): Level of multi-index to group by.
            n_dimensional (bool): If True, use a pivot table for N-Dimensional consolidation.
            aggregation_method (Union[str, Callable]): Aggregation method, e.g., 'list' to aggregate into lists.

        Returns:
            pd.DataFrame: The consolidated DataFrame.

        Example:
            >>> df = pd.DataFrame({'A': [1, 1, 2], 'B': [4, 5, 6], 'C': [7, 8, 9]})
            >>> result = RasUtils.consolidate_dataframe(df, group_by='A')
            >>> print(result)
            B         C
            A            
            1  [4, 5]  [7, 8]
            2  [6]     [9]
        """
        if aggregation_method == 'list':
            agg_func = lambda x: tuple(x)
        else:
            agg_func = aggregation_method

        if n_dimensional:
            result = dataframe.pivot_table(group_by, pivot_columns, aggfunc=agg_func)
        else:
            result = dataframe.groupby(group_by, level=level).agg(agg_func).applymap(list)

        return result

    @staticmethod
    @log_call
    def find_nearest_value(array: Union[list, np.ndarray], target_value: Union[int, float]) -> Union[int, float]:
        """
        Finds the nearest value in a NumPy array to the specified target value.

        Args:
            array (Union[list, np.ndarray]): The array to search within.
            target_value (Union[int, float]): The value to find the nearest neighbor to.

        Returns:
            Union[int, float]: The nearest value in the array to the specified target value.

        Example:
            >>> arr = np.array([1, 3, 5, 7, 9])
            >>> result = RasUtils.find_nearest_value(arr, 6)
            >>> print(result)
            5
        """
        array = np.asarray(array)
        idx = (np.abs(array - target_value)).argmin()
        return array[idx]
    
    @classmethod
    @log_call
    def horizontal_distance(cls, coord1: np.ndarray, coord2: np.ndarray) -> float:
        """
        Calculate the horizontal distance between two coordinate points.
        
        Args:
            coord1 (np.ndarray): First coordinate point [X, Y].
            coord2 (np.ndarray): Second coordinate point [X, Y].
        
        Returns:
            float: Horizontal distance.
        
        Example:
            >>> distance = RasUtils.horizontal_distance(np.array([0, 0]), np.array([3, 4]))
            >>> print(distance)
            5.0
        """
        return np.linalg.norm(coord2 - coord1)
    
    
    
    
    
==================================================

File: c:\GH\ras-commander\ras_commander\__init__.py
==================================================
"""
ras-commander: A Python library for automating HEC-RAS operations
"""

from importlib.metadata import version, PackageNotFoundError
from .LoggingConfig import setup_logging, get_logger
from .Decorators import log_call, standardize_input

try:
    __version__ = version("ras-commander")
except PackageNotFoundError:
    # package is not installed
    __version__ = "0.54.0"

# Set up logging
setup_logging()

# Core functionality
from .RasPrj import RasPrj, init_ras_project, get_ras_exe, ras
from .RasPlan import RasPlan
from .RasGeo import RasGeo
from .RasUnsteady import RasUnsteady
from .RasUtils import RasUtils
from .RasExamples import RasExamples
from .RasCmdr import RasCmdr
from .RasGpt import RasGpt
from .RasToGo import RasToGo
from .HdfFluvialPluvial import HdfFluvialPluvial

# HDF handling
from .HdfBase import HdfBase
from .HdfBndry import HdfBndry
from .HdfMesh import HdfMesh
from .HdfPlan import HdfPlan
from .HdfResultsMesh import HdfResultsMesh
from .HdfResultsPlan import HdfResultsPlan
from .HdfResultsXsec import HdfResultsXsec
from .HdfStruc import HdfStruc
from .HdfUtils import HdfUtils
from .HdfXsec import HdfXsec
from .HdfPump import HdfPump
from .HdfPipe import HdfPipe
from .HdfInfiltration import HdfInfiltration
from .RasMapper import RasMapper

# Plotting functionality
from .HdfPlot import HdfPlot
from .HdfResultsPlot import HdfResultsPlot

# Define __all__ to specify what should be imported when using "from ras_commander import *"
__all__ = [
    # Core functionality
    'RasPrj', 'init_ras_project', 'get_ras_exe', 'ras',
    'RasPlan', 'RasGeo', 'RasUnsteady', 'RasUtils',
    'RasExamples', 'RasCmdr', 'RasGpt', 'RasToGo',
    'HdfFluvialPluvial',
    
    # HDF handling
    'HdfBase', 'HdfBndry', 'HdfMesh', 'HdfPlan',
    'HdfResultsMesh', 'HdfResultsPlan', 'HdfResultsXsec',
    'HdfStruc', 'HdfUtils', 'HdfXsec', 'HdfPump',
    'HdfPipe', 'HdfInfiltration', 'RasMapper',
    
    # Plotting functionality
    'HdfPlot', 'HdfResultsPlot',
    
    # Utilities
    'get_logger', 'log_call', 'standardize_input',
]

==================================================

